<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2180 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2180</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2180</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-277467991</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.00255v2.pdf" target="_blank">SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers</a></p>
                <p><strong>Paper Abstract:</strong> This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \ModelName~achieves only 39% execution accuracy, highlighting the benchmark's difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at https://github.com/xyzCS/SciReplicate-Bench and project homepage at https://xyzcs.github.io/scireplicate.github.io/.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2180.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2180.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sci-Reproducer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sci-Reproducer (dual-agent algorithm reproduction framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-agent agentic system introduced in this paper combining a Paper Agent (literature retrieval/interpretation) and a Code Agent (repository search, dependency resolution, implementation, and compiler-in-the-loop debugging) to reproduce algorithms from academic papers into executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sci-Reproducer</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>agentic dual-agent system (hybrid LLM + toolchain)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific reproducibility / algorithmic reproduction (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates executable code implementations of algorithms described in research papers; synthesizes literature-derived algorithmic details (literature reports).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automated execution-based validation: integrates generated code into the target repository and runs an isolated Python test environment with task-specific verification suites (test cases) to compare outputs to reference implementation results; auxiliary validation via CodeBLEU and a novel reasoning graph accuracy metric that compares generated reasoning comments/graph to a reference graph; LLM-based node-matching (GPT-4o) and compiler feedback (iterative debugging) are used during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>No explicit novelty-distance metric for discoveries; novelty is controlled by selecting recently published (2024) papers to minimize data leakage; reasoning-graph similarity and execution success reflect closeness to reference (i.e., familiarity) rather than absolute novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>On SciReplicate-Bench (100 tasks): average execution accuracy = 0.235; best model with Sci-Reproducer (Claude-Sonnet-3.7) achieved execution accuracy = 0.390; average CodeBLEU = 0.320. Generation (code synthesis) succeeds far less often than comprehension (see reasoning graph accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reasoning graph accuracy average ~0.716 (algorithm understanding); execution-accuracy-based validation succeeded at 23.5% of tasks on average. CodeBLEU mean ~0.320. LLM-based node matching for reasoning graphs repeated 3× (temp=0) to reduce variability; human vs automated correlation r = 0.7518 (p < 0.005) supports validity of automated reasoning-graph assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation reliability decreases when papers omit implementation details or contain mismatches; providing missing/mismatched information improves execution accuracy substantially for some models (e.g., Deepseek-V3 +0.250, GPT-4o-mini +0.050, O3-mini-low +0.040), indicating validation is more likely to fail on novel or underspecified tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — models often demonstrate strong algorithmic comprehension (high reasoning-graph accuracy) but substantially worse executable-code generation and pass-rate on test suites (execution accuracy), indicating a gap between generating conceptual descriptions and generating validated executable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor: benchmark used recent (2024) papers to reduce training-data leakage; nonetheless average execution accuracy remained low (0.235), implying degraded performance on relatively unfamiliar / recent tasks compared to typical code-generation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Moderate: automated LLM-based reasoning-graph evaluation correlates with human judgments (Pearson r = 0.7518), but no explicit probabilistic/confidence calibration numbers for generation outputs are reported; LLMs' self-confidence calibration for correctness is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation requires executing generated code in isolated Python environments and running test suites; reasoning LLMs required more compiler-invocations (more debugging iterations) than non-reasoning LLMs, implying higher runtime/iteration cost for validation when models under-gather context.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Paper Agent (targeted retrieval / RAG-style extraction), Code Agent (repo search, dependency/API recall, compiler-in-the-loop debugging), providing missing/mismatched information explicitly in prompts, in-context learning with implementation examples, and iterative compiler feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sci-Reproducer demonstrates that agentic tool-augmented LLM pipelines can improve dependency recall and reduce syntax errors, yet overall validated reproducibility remains low (avg exec acc 0.235), revealing a substantial generation-vs-validation gap driven by missing paper details and implementation complexity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2180.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paper Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper Agent (component of Sci-Reproducer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven retrieval and interpretation agent that incrementally extracts algorithm-relevant details from a target paper and cited literature, producing a literature report used to fill missing algorithmic components for implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Paper Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented agent (LLM + RAG-like retrieval actions; ReAct strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific literature comprehension for reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates structured literature reports and extracted algorithmic details (textual descriptions of missing hyperparameters, numerical-stability notes, and implementation logic).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Indirect: validation measured by downstream changes in reasoning graph accuracy and execution accuracy when Paper Agent output is provided to the Code Agent; Paper Agent actions support comprehension that is validated via reasoning-graph similarity and execution tests.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly measured; prioritizes retrieving target paper sections first and external literature second, intended to recover missing implementation details rather than to measure novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Alone, modest improvement to algorithm comprehension (Paper Agent increased reasoning graph accuracy by ~0.009 on average); contributes to combined system gains but smaller than Code Agent impact.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provides small but consistent improvement in reasoning-graph accuracy and modest improvement in execution accuracy when combined with the Code Agent; exact numerical execution improvements attributable solely to Paper Agent are small (~+0.009 RG Acc).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Helps mitigate failures due to missing/mismatched info by surfacing relevant paper sections and citations; limited ability to compensate for implementation-specific knowledge not present in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Contributes more to improving conceptual understanding than to direct executable validation — i.e., reduces the comprehension-validation gap partially but not fully.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>When target papers omit details, Paper Agent is constrained by available literature; cannot reliably supply implementation heuristics from unrelated domains.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Lower computational cost than Code Agent (retrieval and LLM calls), but still requires multiple retrieval/LLM iterations; cost rises with amount of literature searched.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Selective retrieval of paper sections and cited literature; ReAct-style action + reasoning prompting to collect missing details.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper Agent improves algorithm comprehension slightly by retrieving missing details from the paper and related literature, but on its own yields only small gains in validated executable reproduction; most practical implementation gains stem from the Code Agent.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2180.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code Agent (component of Sci-Reproducer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven agent that searches the repository for dependencies, implements the target function, uses web search when needed, and invokes a compiler to iteratively test and debug generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>tool-augmented agent (LLM + repository AST search + compiler integration)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>repo-level code generation and debugging for reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates repository-aware code implementations, identifies intra-file and cross-file dependencies, and applies API usage patterns from the repo.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct: uses a Compiler action to run the generated code within the target environment and returns compiler errors/output as feedback for iterative fixes; validation measured via execution accuracy, CodeBLEU, and dependency/API recall metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicit; improves implementation by leveraging repository-specific patterns (familiar tasks) and reduces failures arising from missing details.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Significantly reduces syntax error rates: from ~80% (NoAgent/NoPaperAgent) down to 29.4% (Code Agent) and 24.9% (full Sci-Reproducer). Substantially increases recall metrics (avg increases: intra-file +0.441, cross-file +0.239, API +0.100 compared to no-agent baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improves execution accuracy and CodeBLEU when included; provides largest single-agent gains for implementation correctness relative to Paper Agent.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Most effective when repository contains explicit patterns/dependencies (familiar tasks); less able to infer missing implementation heuristics for novel or underspecified tasks without additional human or literature input.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Closes part of the generation-validation gap by providing executable testing and dependency resolution, but cannot fully bridge the gap when papers lack concrete details.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Less effective for out-of-distribution tasks where repository contains few analogous patterns; recall gains rely on presence of relevant repo code.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Higher: iterative compiler runs and debugging loops increase computation/time compared to purely text-only generation; reasoning LLMs invoked compiler more frequently (more debugging iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Repository-aware code search (AST-based SearchCode/SearchFile), compiler-in-the-loop testing, and web search for API info; combined with Paper Agent, leads to best practical gains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code Agent yields the largest practical improvements in implementation correctness by discovering dependencies and enabling iterative compiler-driven debugging, dramatically lowering syntax errors and improving recall of repo/APIs; still insufficient to reach high execution success across the benchmark.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2180.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models (LLMs) used for code generation and scientific idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of transformer-based LLMs (examples in this paper: GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants) used both to generate research ideas and to synthesize code for algorithm reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large Language Models (GPT-style / Claude / Gemini / Deepseek / O3)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning, code generation, research idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates code implementations, literature summaries, research ideas/hypotheses, and reasoning comments; can be used to generate novel research ideas and hypotheses as cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In this paper, validation is external: generated code is validated via execution accuracy (test-suite runs), CodeBLEU similarity to reference, reasoning graph accuracy (LLM-based node/edge matching), and human evaluation for subset checks. The paper contrasts LLM internal deliberation versus tool-augmented external validation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not standardized here; related work suggests human expert rating, novelty/originality metrics in other studies, but SciReplicate-Bench uses recent papers to reduce familiarity bias rather than measuring novelty of generated ideas directly.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Varies by model: average execution accuracy across evaluated models = 0.235 with Sci-Reproducer; best model (Claude-Sonnet-3.7 with Sci-Reproducer) achieved exec acc = 0.390; reasoning graph accuracy remains relatively high (~0.716 average), indicating comprehension often exceeds executable generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>LLM-generated outputs are validated using execution tests and reasoning-graph matching; LLMs that rely less on external tools (reasoning models) show smaller validation gains when agentic tools are added, implying validation performance depends on tool-use behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance degrades for less familiar/underspecified tasks; missing/mismatched information in papers particularly harms validation; providing missing details can materially improve validated execution for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — consistent pattern where LLMs can show conceptual generation (comments/logic) but fail to produce validated executable code at comparable rates.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degraded: low execution accuracy on recent unseen algorithms indicates reduced OOD performance; selection of recent papers aims to reduce leakage but highlights OOD challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported for model confidence; however, automated evaluation of reasoning graphs via LLM correlates with human judgment (r = 0.7518), indicating evaluation-level alignment but not model self-calibration for output correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation (execution) is more computationally expensive than generation (text), as it requires environment setup and running tests; reasoning models required more compile-run-debug cycles, increasing cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Tool-augmented agent frameworks (Paper/Code Agents), compiler-in-the-loop, retrieval-augmented prompts, explicit provision of missing implementation details, and in-context learning from similar implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can understand algorithms but routinely fail to implement them correctly in executable form; tool augmentation helps but cannot fully close the gap, and models that over-rely on internal reasoning ('overthinking') benefit less from tool access.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2180.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning LLMs (o3-mini family)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>O3-mini reasoning LLM variants (o3-mini-low, o3-mini-medium, o3-mini-high)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM variants tuned for internal multi-step reasoning; in this paper they are compared to non-reasoning models for code reproduction tasks and exhibit 'overthinking' behavior (relying on internal deliberation rather than tool actions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>O3-mini reasoning LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>reasoning-focused large language models</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific reasoning and code generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Produce chain-of-thought style internal reasoning and code; tend to rely on internal deliberation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation via execution accuracy, CodeBLEU, reasoning graph accuracy; tool action logs analyzed to determine behavior (SearchFile/SearchCodeItem/SearchWeb/Compiler counts).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly measured; their tendency to rely on internal reasoning makes them less likely to gather external information needed for unfamiliar tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Smaller gains from Sci-Reproducer than non-reasoning models: average execution accuracy improvement with Sci-Reproducer = +0.13 (compared to +0.212 for non-reasoning models). Some models achieved similar exec accuracy to non-reasoning baselines despite theoretical reasoning advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reasoning LLMs had more modest improvements in recall metrics (e.g., +0.243, +0.061, +0.041 for intra-file/cross-file/API recall respectively) relative to non-reasoning models' larger gains; invoked tools far less (SearchFile ~25.0 vs 210.4; SearchCodeItem ~3.3 vs 68.2; SearchWeb 0.0 vs 16.8), but invoked Compiler more frequently (more debugging iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Less effective at improving validation on novel or underspecified tasks due to low external information gathering; overthinking behavior harms execution success when external context is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Pronounced: reasoning models show strong internal reasoning traces but less actionable external information use, producing a bigger gap between conceptual correctness and validated executable correctness on unfamiliar tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Underperforms relative to non-reasoning LLMs when agentic tool usage is required to access repo/paper context; improvements smaller on OOD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Higher observed compiler invocation counts imply increased validation/debugging cost and runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Encouraging tool use (SearchFile/SearchCode), constraining internal deliberation, integrating external retrieval and compiler feedback to translate reasoning into action.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reasoning-optimized LLMs tend to 'overthink' and under-utilize external tools, resulting in smaller practical improvements in validated code reproduction compared to non-reasoning models that actively use tool actions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2180.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-reasoning LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-reasoning LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large language models used without specialized internal chain-of-thought reasoning emphasis; in this study they make heavier use of external tool actions and gain more from Sci-Reproducer augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Non-reasoning LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>general-purpose large language models</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>code generation and scientific reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generate code and text with emphasis on leveraging retrieved context and repository examples rather than long internal deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Assessed by execution accuracy, CodeBLEU, reasoning graph accuracy, and dependency/API recall; action logs show extensive use of SearchFile/SearchCodeItem/SearchWeb compared to reasoning LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly measured; model improvements tied to ability to locate relevant repository/paper content (familiarity aids success).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Greater execution-accuracy improvements from Sci-Reproducer: average exec-acc improvement +0.212 vs +0.13 for reasoning models. The best performing model in the suite, Claude-Sonnet-3.7 with Sci-Reproducer, reached exec acc = 0.390 and recall (intra-file/cross-file/API) = (0.776, 0.636, 0.626).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Higher gains in recall metrics and larger reductions in syntax errors when using Code Agent; non-reasoning LLMs invoked SearchFile ~210.4 vs reasoning ~25.0 and SearchCodeItem ~68.2 vs ~3.3.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Perform better when repository or paper provides explicit examples/patterns (familiar tasks); still fail often when papers omit concrete implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Less severe than reasoning LLMs, because they act more on external information, but asymmetry remains (comprehension > executable success).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Better relative to reasoning LLMs when required to use external repository/paper cues, but overall execution accuracy on OOD recent papers remains low (avg 0.235 across models).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Lower per-task debugging cost relative to reasoning LLMs because they require fewer compiler iterations and use external information more proactively.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Aggressive tool usage (SearchFile/SearchCode/SearchWeb) and integration with Code Agent and Compiler feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Non-reasoning LLMs gain more from tool-augmented agent frameworks and achieve higher validated execution accuracy than reasoning LLMs in this repository-level reproduction benchmark, indicating that action-oriented behavior closes more of the generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2180.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning Graph Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning Graph Accuracy (novel metric introduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based metric that quantifies algorithmic comprehension by comparing a generated reasoning graph (nodes are reasoning-step comments aligned to code snippets; edges reflect data flow) to a reference reasoning graph using weighted node/edge matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reasoning Graph Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation metric (structured graph similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>algorithm comprehension evaluation for code reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Not a generator — it evaluates generated reasoning comments/code by constructing and comparing DAGs representing reasoning steps and data flows.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Node matching: comments from reference and generated graphs mapped via GPT-4o (temperature=0) to determine matched nodes; Edge matching: BFS checks for corresponding data-flow edges; node/edge significance weights computed from code complexity determine final score; repeated three times and averaged to reduce stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Captures novelty of generated reasoning only indirectly by degree of match to reference graph (low similarity indicates divergence from reference reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Average reasoning graph accuracy across models/tasks reported ~0.716 even without agent assistance, indicating relatively strong algorithmic comprehension despite low execution success.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validated through a human study: human vs automated LLM-based reasoning-graph assessments correlated at Pearson r = 0.7518 (p < 0.005), supporting metric reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Effective at measuring conceptual divergence from reference; does not directly measure executable correctness, so novel correct-but-different implementations may score lower if reasoning diverges from the reference graph.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Illustrates asymmetry: high reasoning-graph accuracy can coexist with low execution-accuracy, showing conceptual correctness does not guarantee validated executable success.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not explicitly provided; metric depends on quality of generated comments and alignment to reference, so OOD conceptualizations may reduce RG accuracy even if correct.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Automated assessments were stabilized (temp=0, 3 repeats) and showed good correlation with human judges (r=0.7518).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Requires LLM mediation (GPT-4o) for node mapping plus BFS on graphs; cost moderate compared to full execution but depends on prompt/evaluation runs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Serves as a fine-grained comprehension validation that complements execution tests; can highlight whether failures stem from comprehension vs implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reasoning graph accuracy reveals that LLMs often capture algorithmic logic (high RG Acc) but still fail to produce validated executable code, indicating a conceptual vs implementation gap in automated scientific reproduction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2180.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Execution Accuracy (metric)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Accuracy (test-suite based execution validation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An execution-based evaluation metric that deems generated code correct if all test cases in an isolated verification suite match the reference outputs; used as the primary objective metric for implementation correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Execution Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation metric (execution-based)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>code generation validation / computational reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Not a generator; evaluates generated code by running it against reference test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Integrate generated function into repository, run the provided verification suite (typically 10 test cases per task) in a deterministic, fixed-seed environment; success if outputs match reference results for all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measures practical executable equivalence to reference (familiar implementations), not novelty of approach; cannot reward valid but differently implemented algorithms unless outputs match reference tests.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported average execution accuracy across models with Sci-Reproducer = 0.235; best model reached 0.390 execution accuracy; syntax error rates and logic errors contribute to failures.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Direct and objective; sensitive to implementation choices and to missing/mismatched information in papers; improvements observed when missing information provided explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Strictly penalizes novel-but-correct alternative implementations if they produce outputs that differ from the specific reference outputs or if tests are not comprehensive for the alternative approach.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Highlights asymmetry: conceptual correctness (reasoning graph match) does not guarantee execution-accuracy success.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Low success rates on recent/unseen algorithms in this benchmark (avg 0.235), reflecting poor OOD validated generation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Relatively high: requires environment setup and running multiple tests per generated implementation, plus iterative re-runs during debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Augment tests to accept functionally-equivalent outputs, provide additional test cases or oracle checks, and combine execution checks with reasoning-graph assessments to identify root causes of failure.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Execution accuracy is an objective validation mechanism that reveals the practical shortfall of LLM-generated implementations: many outputs pass conceptual checks but fail execution tests due to syntax, dependency, or implementation-logic gaps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2180.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o node-matcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used for automated node matching in reasoning graph evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o is used as an automated mapping/evaluator to match reference reasoning graph nodes (comment strings) to nodes generated by the LLM, forming the basis for reasoning graph node matching in the RG accuracy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o (LLM used as an evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as evaluation tool</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>evaluation of algorithmic comprehension (reasoning graph node matching)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Not used to generate final code here; used to perform semantic mapping between reference and generated comments (evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Maps each reference node comment to one or more generated node comments using an LLM prompt; temperature set to 0 and top-p = 1 for determinism; process repeated three times and averaged to reduce stochasticity; used as part of automated reasoning graph accuracy calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not applicable (evaluator rather than idea-generator).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Automated node matching via GPT-4o shows good alignment with human judgments (Pearson r = 0.7518 across human-assessed subset), supporting its use as a proxy for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Evaluator's mapping quality may degrade when generated comments use novel phrasings divergent from reference wording, potentially under-counting valid but differently-expressed reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Shows that automated LLM-based evaluation is useful but not perfect; human review still used to validate the metric's reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Stabilized via deterministic generation settings and repetition; empirically correlated with humans but subject to prompting sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Requires additional LLM calls per evaluation; repeated runs (3×) increase cost but reduce randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Deterministic LLM prompting (temp=0), repeated evaluation runs, and human spot-checks to validate automated judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an LLM (GPT-4o) as an automated node-matcher provides a scalable way to compute reasoning graph accuracy with reasonable alignment to human judgment, enabling objective assessment of algorithmic comprehension.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2180.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2180.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (RG accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation of reasoning graph accuracy (3 PhD annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human validation study in which three PhD students assessed reasoning-graph accuracy for generated outputs to verify the reliability of the automated LLM-based evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human evaluators (PhD students)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>validation of automated evaluation metrics / algorithm comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>N/A — humans serve as evaluators to judge correctness and alignment of reasoning graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Three independent PhD annotators assessed reasoning-graph accuracy on 20 tasks generated by two LLMs; their scores were compared to GPT-4o automated assessments to compute correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Human judgments used as ground truth for metric validation; novelty per se not measured.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Human judgments correlate strongly with automated LLM-based evaluations (Pearson r = 0.7518, p < 0.005), supporting the automated evaluation pipeline's reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Human evaluators can recognize valid but novel reasoning expressions that automated matchers might miss; thus human review is important when generated reasoning diverges in wording.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Human evaluation confirms that conceptual correctness (as judged by humans) can exceed execution success, reinforcing the observed generation-validation asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High per-instance cost (human time); applied only to a subset for metric validation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human spot-checking to validate automated metrics and to identify cases where automated mapping may under- or over-count matches.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human judgements align well with automated LLM-based reasoning-graph assessments, but human review remains necessary for edge cases and to confirm automated evaluations' validity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search <em>(Rating: 2)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 2)</em></li>
                <li>Paper2code: Automating code generation from scientific papers in machine learning <em>(Rating: 2)</em></li>
                <li>Paperbench: Evaluating ai's ability to replicate ai research <em>(Rating: 2)</em></li>
                <li>Can LLMs generate novel research ideas? a large-scale human study with 100+ nlp researchers <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2180",
    "paper_id": "paper-277467991",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "Sci-Reproducer",
            "name_full": "Sci-Reproducer (dual-agent algorithm reproduction framework)",
            "brief_description": "A dual-agent agentic system introduced in this paper combining a Paper Agent (literature retrieval/interpretation) and a Code Agent (repository search, dependency resolution, implementation, and compiler-in-the-loop debugging) to reproduce algorithms from academic papers into executable code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Sci-Reproducer",
            "system_type": "agentic dual-agent system (hybrid LLM + toolchain)",
            "domain": "scientific reproducibility / algorithmic reproduction (NLP)",
            "generation_capability": "Generates executable code implementations of algorithms described in research papers; synthesizes literature-derived algorithmic details (literature reports).",
            "validation_method": "Automated execution-based validation: integrates generated code into the target repository and runs an isolated Python test environment with task-specific verification suites (test cases) to compare outputs to reference implementation results; auxiliary validation via CodeBLEU and a novel reasoning graph accuracy metric that compares generated reasoning comments/graph to a reference graph; LLM-based node-matching (GPT-4o) and compiler feedback (iterative debugging) are used during generation.",
            "novelty_measure": "No explicit novelty-distance metric for discoveries; novelty is controlled by selecting recently published (2024) papers to minimize data leakage; reasoning-graph similarity and execution success reflect closeness to reference (i.e., familiarity) rather than absolute novelty.",
            "generation_performance": "On SciReplicate-Bench (100 tasks): average execution accuracy = 0.235; best model with Sci-Reproducer (Claude-Sonnet-3.7) achieved execution accuracy = 0.390; average CodeBLEU = 0.320. Generation (code synthesis) succeeds far less often than comprehension (see reasoning graph accuracy).",
            "validation_performance": "Reasoning graph accuracy average ~0.716 (algorithm understanding); execution-accuracy-based validation succeeded at 23.5% of tasks on average. CodeBLEU mean ~0.320. LLM-based node matching for reasoning graphs repeated 3× (temp=0) to reduce variability; human vs automated correlation r = 0.7518 (p &lt; 0.005) supports validity of automated reasoning-graph assessments.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Validation reliability decreases when papers omit implementation details or contain mismatches; providing missing/mismatched information improves execution accuracy substantially for some models (e.g., Deepseek-V3 +0.250, GPT-4o-mini +0.050, O3-mini-low +0.040), indicating validation is more likely to fail on novel or underspecified tasks.",
            "generation_validation_asymmetry": "Yes — models often demonstrate strong algorithmic comprehension (high reasoning-graph accuracy) but substantially worse executable-code generation and pass-rate on test suites (execution accuracy), indicating a gap between generating conceptual descriptions and generating validated executable implementations.",
            "out_of_distribution_performance": "Poor: benchmark used recent (2024) papers to reduce training-data leakage; nonetheless average execution accuracy remained low (0.235), implying degraded performance on relatively unfamiliar / recent tasks compared to typical code-generation benchmarks.",
            "calibration_quality": "Moderate: automated LLM-based reasoning-graph evaluation correlates with human judgments (Pearson r = 0.7518), but no explicit probabilistic/confidence calibration numbers for generation outputs are reported; LLMs' self-confidence calibration for correctness is not provided.",
            "validation_computational_cost": "Validation requires executing generated code in isolated Python environments and running test suites; reasoning LLMs required more compiler-invocations (more debugging iterations) than non-reasoning LLMs, implying higher runtime/iteration cost for validation when models under-gather context.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Paper Agent (targeted retrieval / RAG-style extraction), Code Agent (repo search, dependency/API recall, compiler-in-the-loop debugging), providing missing/mismatched information explicitly in prompts, in-context learning with implementation examples, and iterative compiler feedback.",
            "evidence_type": "supports",
            "key_findings": "Sci-Reproducer demonstrates that agentic tool-augmented LLM pipelines can improve dependency recall and reduce syntax errors, yet overall validated reproducibility remains low (avg exec acc 0.235), revealing a substantial generation-vs-validation gap driven by missing paper details and implementation complexity.",
            "uuid": "e2180.0"
        },
        {
            "name_short": "Paper Agent",
            "name_full": "Paper Agent (component of Sci-Reproducer)",
            "brief_description": "An LLM-driven retrieval and interpretation agent that incrementally extracts algorithm-relevant details from a target paper and cited literature, producing a literature report used to fill missing algorithmic components for implementation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Paper Agent",
            "system_type": "retrieval-augmented agent (LLM + RAG-like retrieval actions; ReAct strategy)",
            "domain": "scientific literature comprehension for reproducibility",
            "generation_capability": "Generates structured literature reports and extracted algorithmic details (textual descriptions of missing hyperparameters, numerical-stability notes, and implementation logic).",
            "validation_method": "Indirect: validation measured by downstream changes in reasoning graph accuracy and execution accuracy when Paper Agent output is provided to the Code Agent; Paper Agent actions support comprehension that is validated via reasoning-graph similarity and execution tests.",
            "novelty_measure": "Not explicitly measured; prioritizes retrieving target paper sections first and external literature second, intended to recover missing implementation details rather than to measure novelty.",
            "generation_performance": "Alone, modest improvement to algorithm comprehension (Paper Agent increased reasoning graph accuracy by ~0.009 on average); contributes to combined system gains but smaller than Code Agent impact.",
            "validation_performance": "Provides small but consistent improvement in reasoning-graph accuracy and modest improvement in execution accuracy when combined with the Code Agent; exact numerical execution improvements attributable solely to Paper Agent are small (~+0.009 RG Acc).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Helps mitigate failures due to missing/mismatched info by surfacing relevant paper sections and citations; limited ability to compensate for implementation-specific knowledge not present in literature.",
            "generation_validation_asymmetry": "Contributes more to improving conceptual understanding than to direct executable validation — i.e., reduces the comprehension-validation gap partially but not fully.",
            "out_of_distribution_performance": "When target papers omit details, Paper Agent is constrained by available literature; cannot reliably supply implementation heuristics from unrelated domains.",
            "calibration_quality": null,
            "validation_computational_cost": "Lower computational cost than Code Agent (retrieval and LLM calls), but still requires multiple retrieval/LLM iterations; cost rises with amount of literature searched.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Selective retrieval of paper sections and cited literature; ReAct-style action + reasoning prompting to collect missing details.",
            "evidence_type": "supports",
            "key_findings": "Paper Agent improves algorithm comprehension slightly by retrieving missing details from the paper and related literature, but on its own yields only small gains in validated executable reproduction; most practical implementation gains stem from the Code Agent.",
            "uuid": "e2180.1"
        },
        {
            "name_short": "Code Agent",
            "name_full": "Code Agent (component of Sci-Reproducer)",
            "brief_description": "An LLM-driven agent that searches the repository for dependencies, implements the target function, uses web search when needed, and invokes a compiler to iteratively test and debug generated code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Code Agent",
            "system_type": "tool-augmented agent (LLM + repository AST search + compiler integration)",
            "domain": "repo-level code generation and debugging for reproducibility",
            "generation_capability": "Generates repository-aware code implementations, identifies intra-file and cross-file dependencies, and applies API usage patterns from the repo.",
            "validation_method": "Direct: uses a Compiler action to run the generated code within the target environment and returns compiler errors/output as feedback for iterative fixes; validation measured via execution accuracy, CodeBLEU, and dependency/API recall metrics.",
            "novelty_measure": "Not explicit; improves implementation by leveraging repository-specific patterns (familiar tasks) and reduces failures arising from missing details.",
            "generation_performance": "Significantly reduces syntax error rates: from ~80% (NoAgent/NoPaperAgent) down to 29.4% (Code Agent) and 24.9% (full Sci-Reproducer). Substantially increases recall metrics (avg increases: intra-file +0.441, cross-file +0.239, API +0.100 compared to no-agent baselines).",
            "validation_performance": "Improves execution accuracy and CodeBLEU when included; provides largest single-agent gains for implementation correctness relative to Paper Agent.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Most effective when repository contains explicit patterns/dependencies (familiar tasks); less able to infer missing implementation heuristics for novel or underspecified tasks without additional human or literature input.",
            "generation_validation_asymmetry": "Closes part of the generation-validation gap by providing executable testing and dependency resolution, but cannot fully bridge the gap when papers lack concrete details.",
            "out_of_distribution_performance": "Less effective for out-of-distribution tasks where repository contains few analogous patterns; recall gains rely on presence of relevant repo code.",
            "calibration_quality": null,
            "validation_computational_cost": "Higher: iterative compiler runs and debugging loops increase computation/time compared to purely text-only generation; reasoning LLMs invoked compiler more frequently (more debugging iterations).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Repository-aware code search (AST-based SearchCode/SearchFile), compiler-in-the-loop testing, and web search for API info; combined with Paper Agent, leads to best practical gains.",
            "evidence_type": "supports",
            "key_findings": "Code Agent yields the largest practical improvements in implementation correctness by discovering dependencies and enabling iterative compiler-driven debugging, dramatically lowering syntax errors and improving recall of repo/APIs; still insufficient to reach high execution success across the benchmark.",
            "uuid": "e2180.2"
        },
        {
            "name_short": "LLMs (general)",
            "name_full": "Large Language Models (LLMs) used for code generation and scientific idea generation",
            "brief_description": "General class of transformer-based LLMs (examples in this paper: GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3, O3-mini variants) used both to generate research ideas and to synthesize code for algorithm reproduction.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Large Language Models (GPT-style / Claude / Gemini / Deepseek / O3)",
            "system_type": "large language model (neural network)",
            "domain": "general scientific reasoning, code generation, research idea generation",
            "generation_capability": "Generates code implementations, literature summaries, research ideas/hypotheses, and reasoning comments; can be used to generate novel research ideas and hypotheses as cited in related work.",
            "validation_method": "In this paper, validation is external: generated code is validated via execution accuracy (test-suite runs), CodeBLEU similarity to reference, reasoning graph accuracy (LLM-based node/edge matching), and human evaluation for subset checks. The paper contrasts LLM internal deliberation versus tool-augmented external validation.",
            "novelty_measure": "Not standardized here; related work suggests human expert rating, novelty/originality metrics in other studies, but SciReplicate-Bench uses recent papers to reduce familiarity bias rather than measuring novelty of generated ideas directly.",
            "generation_performance": "Varies by model: average execution accuracy across evaluated models = 0.235 with Sci-Reproducer; best model (Claude-Sonnet-3.7 with Sci-Reproducer) achieved exec acc = 0.390; reasoning graph accuracy remains relatively high (~0.716 average), indicating comprehension often exceeds executable generation performance.",
            "validation_performance": "LLM-generated outputs are validated using execution tests and reasoning-graph matching; LLMs that rely less on external tools (reasoning models) show smaller validation gains when agentic tools are added, implying validation performance depends on tool-use behavior.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance degrades for less familiar/underspecified tasks; missing/mismatched information in papers particularly harms validation; providing missing details can materially improve validated execution for some models.",
            "generation_validation_asymmetry": "Yes — consistent pattern where LLMs can show conceptual generation (comments/logic) but fail to produce validated executable code at comparable rates.",
            "out_of_distribution_performance": "Degraded: low execution accuracy on recent unseen algorithms indicates reduced OOD performance; selection of recent papers aims to reduce leakage but highlights OOD challenges.",
            "calibration_quality": "Not reported for model confidence; however, automated evaluation of reasoning graphs via LLM correlates with human judgment (r = 0.7518), indicating evaluation-level alignment but not model self-calibration for output correctness.",
            "validation_computational_cost": "Validation (execution) is more computationally expensive than generation (text), as it requires environment setup and running tests; reasoning models required more compile-run-debug cycles, increasing cost.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Tool-augmented agent frameworks (Paper/Code Agents), compiler-in-the-loop, retrieval-augmented prompts, explicit provision of missing implementation details, and in-context learning from similar implementations.",
            "evidence_type": "supports",
            "key_findings": "LLMs can understand algorithms but routinely fail to implement them correctly in executable form; tool augmentation helps but cannot fully close the gap, and models that over-rely on internal reasoning ('overthinking') benefit less from tool access.",
            "uuid": "e2180.3"
        },
        {
            "name_short": "Reasoning LLMs (o3-mini family)",
            "name_full": "O3-mini reasoning LLM variants (o3-mini-low, o3-mini-medium, o3-mini-high)",
            "brief_description": "LLM variants tuned for internal multi-step reasoning; in this paper they are compared to non-reasoning models for code reproduction tasks and exhibit 'overthinking' behavior (relying on internal deliberation rather than tool actions).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "O3-mini reasoning LLMs",
            "system_type": "reasoning-focused large language models",
            "domain": "scientific reasoning and code generation",
            "generation_capability": "Produce chain-of-thought style internal reasoning and code; tend to rely on internal deliberation steps.",
            "validation_method": "Evaluation via execution accuracy, CodeBLEU, reasoning graph accuracy; tool action logs analyzed to determine behavior (SearchFile/SearchCodeItem/SearchWeb/Compiler counts).",
            "novelty_measure": "Not explicitly measured; their tendency to rely on internal reasoning makes them less likely to gather external information needed for unfamiliar tasks.",
            "generation_performance": "Smaller gains from Sci-Reproducer than non-reasoning models: average execution accuracy improvement with Sci-Reproducer = +0.13 (compared to +0.212 for non-reasoning models). Some models achieved similar exec accuracy to non-reasoning baselines despite theoretical reasoning advantages.",
            "validation_performance": "Reasoning LLMs had more modest improvements in recall metrics (e.g., +0.243, +0.061, +0.041 for intra-file/cross-file/API recall respectively) relative to non-reasoning models' larger gains; invoked tools far less (SearchFile ~25.0 vs 210.4; SearchCodeItem ~3.3 vs 68.2; SearchWeb 0.0 vs 16.8), but invoked Compiler more frequently (more debugging iterations).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Less effective at improving validation on novel or underspecified tasks due to low external information gathering; overthinking behavior harms execution success when external context is needed.",
            "generation_validation_asymmetry": "Pronounced: reasoning models show strong internal reasoning traces but less actionable external information use, producing a bigger gap between conceptual correctness and validated executable correctness on unfamiliar tasks.",
            "out_of_distribution_performance": "Underperforms relative to non-reasoning LLMs when agentic tool usage is required to access repo/paper context; improvements smaller on OOD tasks.",
            "calibration_quality": null,
            "validation_computational_cost": "Higher observed compiler invocation counts imply increased validation/debugging cost and runtime.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Encouraging tool use (SearchFile/SearchCode), constraining internal deliberation, integrating external retrieval and compiler feedback to translate reasoning into action.",
            "evidence_type": "supports",
            "key_findings": "Reasoning-optimized LLMs tend to 'overthink' and under-utilize external tools, resulting in smaller practical improvements in validated code reproduction compared to non-reasoning models that actively use tool actions.",
            "uuid": "e2180.4"
        },
        {
            "name_short": "Non-reasoning LLMs",
            "name_full": "Non-reasoning LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3)",
            "brief_description": "Large language models used without specialized internal chain-of-thought reasoning emphasis; in this study they make heavier use of external tool actions and gain more from Sci-Reproducer augmentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Non-reasoning LLMs (GPT-4o-mini, GPT-4o, Claude-Sonnet-3.7, Gemini-2.0-Flash, Deepseek-V3)",
            "system_type": "general-purpose large language models",
            "domain": "code generation and scientific reproducibility",
            "generation_capability": "Generate code and text with emphasis on leveraging retrieved context and repository examples rather than long internal deliberation.",
            "validation_method": "Assessed by execution accuracy, CodeBLEU, reasoning graph accuracy, and dependency/API recall; action logs show extensive use of SearchFile/SearchCodeItem/SearchWeb compared to reasoning LLMs.",
            "novelty_measure": "Not explicitly measured; model improvements tied to ability to locate relevant repository/paper content (familiarity aids success).",
            "generation_performance": "Greater execution-accuracy improvements from Sci-Reproducer: average exec-acc improvement +0.212 vs +0.13 for reasoning models. The best performing model in the suite, Claude-Sonnet-3.7 with Sci-Reproducer, reached exec acc = 0.390 and recall (intra-file/cross-file/API) = (0.776, 0.636, 0.626).",
            "validation_performance": "Higher gains in recall metrics and larger reductions in syntax errors when using Code Agent; non-reasoning LLMs invoked SearchFile ~210.4 vs reasoning ~25.0 and SearchCodeItem ~68.2 vs ~3.3.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Perform better when repository or paper provides explicit examples/patterns (familiar tasks); still fail often when papers omit concrete implementation details.",
            "generation_validation_asymmetry": "Less severe than reasoning LLMs, because they act more on external information, but asymmetry remains (comprehension &gt; executable success).",
            "out_of_distribution_performance": "Better relative to reasoning LLMs when required to use external repository/paper cues, but overall execution accuracy on OOD recent papers remains low (avg 0.235 across models).",
            "calibration_quality": null,
            "validation_computational_cost": "Lower per-task debugging cost relative to reasoning LLMs because they require fewer compiler iterations and use external information more proactively.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Aggressive tool usage (SearchFile/SearchCode/SearchWeb) and integration with Code Agent and Compiler feedback.",
            "evidence_type": "supports",
            "key_findings": "Non-reasoning LLMs gain more from tool-augmented agent frameworks and achieve higher validated execution accuracy than reasoning LLMs in this repository-level reproduction benchmark, indicating that action-oriented behavior closes more of the generation-validation gap.",
            "uuid": "e2180.5"
        },
        {
            "name_short": "Reasoning Graph Accuracy",
            "name_full": "Reasoning Graph Accuracy (novel metric introduced in this paper)",
            "brief_description": "A graph-based metric that quantifies algorithmic comprehension by comparing a generated reasoning graph (nodes are reasoning-step comments aligned to code snippets; edges reflect data flow) to a reference reasoning graph using weighted node/edge matching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Reasoning Graph Accuracy",
            "system_type": "evaluation metric (structured graph similarity)",
            "domain": "algorithm comprehension evaluation for code reproduction",
            "generation_capability": "Not a generator — it evaluates generated reasoning comments/code by constructing and comparing DAGs representing reasoning steps and data flows.",
            "validation_method": "Node matching: comments from reference and generated graphs mapped via GPT-4o (temperature=0) to determine matched nodes; Edge matching: BFS checks for corresponding data-flow edges; node/edge significance weights computed from code complexity determine final score; repeated three times and averaged to reduce stochasticity.",
            "novelty_measure": "Captures novelty of generated reasoning only indirectly by degree of match to reference graph (low similarity indicates divergence from reference reasoning).",
            "generation_performance": "Average reasoning graph accuracy across models/tasks reported ~0.716 even without agent assistance, indicating relatively strong algorithmic comprehension despite low execution success.",
            "validation_performance": "Validated through a human study: human vs automated LLM-based reasoning-graph assessments correlated at Pearson r = 0.7518 (p &lt; 0.005), supporting metric reliability.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Effective at measuring conceptual divergence from reference; does not directly measure executable correctness, so novel correct-but-different implementations may score lower if reasoning diverges from the reference graph.",
            "generation_validation_asymmetry": "Illustrates asymmetry: high reasoning-graph accuracy can coexist with low execution-accuracy, showing conceptual correctness does not guarantee validated executable success.",
            "out_of_distribution_performance": "Not explicitly provided; metric depends on quality of generated comments and alignment to reference, so OOD conceptualizations may reduce RG accuracy even if correct.",
            "calibration_quality": "Automated assessments were stabilized (temp=0, 3 repeats) and showed good correlation with human judges (r=0.7518).",
            "validation_computational_cost": "Requires LLM mediation (GPT-4o) for node mapping plus BFS on graphs; cost moderate compared to full execution but depends on prompt/evaluation runs.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Serves as a fine-grained comprehension validation that complements execution tests; can highlight whether failures stem from comprehension vs implementation.",
            "evidence_type": "supports",
            "key_findings": "Reasoning graph accuracy reveals that LLMs often capture algorithmic logic (high RG Acc) but still fail to produce validated executable code, indicating a conceptual vs implementation gap in automated scientific reproduction.",
            "uuid": "e2180.6"
        },
        {
            "name_short": "Execution Accuracy (metric)",
            "name_full": "Execution Accuracy (test-suite based execution validation)",
            "brief_description": "An execution-based evaluation metric that deems generated code correct if all test cases in an isolated verification suite match the reference outputs; used as the primary objective metric for implementation correctness.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Execution Accuracy",
            "system_type": "evaluation metric (execution-based)",
            "domain": "code generation validation / computational reproducibility",
            "generation_capability": "Not a generator; evaluates generated code by running it against reference test cases.",
            "validation_method": "Integrate generated function into repository, run the provided verification suite (typically 10 test cases per task) in a deterministic, fixed-seed environment; success if outputs match reference results for all cases.",
            "novelty_measure": "Measures practical executable equivalence to reference (familiar implementations), not novelty of approach; cannot reward valid but differently implemented algorithms unless outputs match reference tests.",
            "generation_performance": "Reported average execution accuracy across models with Sci-Reproducer = 0.235; best model reached 0.390 execution accuracy; syntax error rates and logic errors contribute to failures.",
            "validation_performance": "Direct and objective; sensitive to implementation choices and to missing/mismatched information in papers; improvements observed when missing information provided explicitly.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Strictly penalizes novel-but-correct alternative implementations if they produce outputs that differ from the specific reference outputs or if tests are not comprehensive for the alternative approach.",
            "generation_validation_asymmetry": "Highlights asymmetry: conceptual correctness (reasoning graph match) does not guarantee execution-accuracy success.",
            "out_of_distribution_performance": "Low success rates on recent/unseen algorithms in this benchmark (avg 0.235), reflecting poor OOD validated generation.",
            "calibration_quality": null,
            "validation_computational_cost": "Relatively high: requires environment setup and running multiple tests per generated implementation, plus iterative re-runs during debugging.",
            "human_validation_required": false,
            "gap_closing_mechanisms": "Augment tests to accept functionally-equivalent outputs, provide additional test cases or oracle checks, and combine execution checks with reasoning-graph assessments to identify root causes of failure.",
            "evidence_type": "supports",
            "key_findings": "Execution accuracy is an objective validation mechanism that reveals the practical shortfall of LLM-generated implementations: many outputs pass conceptual checks but fail execution tests due to syntax, dependency, or implementation-logic gaps.",
            "uuid": "e2180.7"
        },
        {
            "name_short": "GPT-4o node-matcher",
            "name_full": "GPT-4o used for automated node matching in reasoning graph evaluation",
            "brief_description": "GPT-4o is used as an automated mapping/evaluator to match reference reasoning graph nodes (comment strings) to nodes generated by the LLM, forming the basis for reasoning graph node matching in the RG accuracy metric.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o (LLM used as an evaluator)",
            "system_type": "large language model used as evaluation tool",
            "domain": "evaluation of algorithmic comprehension (reasoning graph node matching)",
            "generation_capability": "Not used to generate final code here; used to perform semantic mapping between reference and generated comments (evaluation).",
            "validation_method": "Maps each reference node comment to one or more generated node comments using an LLM prompt; temperature set to 0 and top-p = 1 for determinism; process repeated three times and averaged to reduce stochasticity; used as part of automated reasoning graph accuracy calculation.",
            "novelty_measure": "Not applicable (evaluator rather than idea-generator).",
            "generation_performance": null,
            "validation_performance": "Automated node matching via GPT-4o shows good alignment with human judgments (Pearson r = 0.7518 across human-assessed subset), supporting its use as a proxy for human evaluation.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Evaluator's mapping quality may degrade when generated comments use novel phrasings divergent from reference wording, potentially under-counting valid but differently-expressed reasoning.",
            "generation_validation_asymmetry": "Shows that automated LLM-based evaluation is useful but not perfect; human review still used to validate the metric's reliability.",
            "out_of_distribution_performance": null,
            "calibration_quality": "Stabilized via deterministic generation settings and repetition; empirically correlated with humans but subject to prompting sensitivity.",
            "validation_computational_cost": "Requires additional LLM calls per evaluation; repeated runs (3×) increase cost but reduce randomness.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Deterministic LLM prompting (temp=0), repeated evaluation runs, and human spot-checks to validate automated judgments.",
            "evidence_type": "supports",
            "key_findings": "Using an LLM (GPT-4o) as an automated node-matcher provides a scalable way to compute reasoning graph accuracy with reasonable alignment to human judgment, enabling objective assessment of algorithmic comprehension.",
            "uuid": "e2180.8"
        },
        {
            "name_short": "Human evaluation (RG accuracy)",
            "name_full": "Human evaluation of reasoning graph accuracy (3 PhD annotators)",
            "brief_description": "A human validation study in which three PhD students assessed reasoning-graph accuracy for generated outputs to verify the reliability of the automated LLM-based evaluation metric.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Human evaluators (PhD students)",
            "system_type": "human expert evaluation",
            "domain": "validation of automated evaluation metrics / algorithm comprehension",
            "generation_capability": "N/A — humans serve as evaluators to judge correctness and alignment of reasoning graphs.",
            "validation_method": "Three independent PhD annotators assessed reasoning-graph accuracy on 20 tasks generated by two LLMs; their scores were compared to GPT-4o automated assessments to compute correlation.",
            "novelty_measure": "Human judgments used as ground truth for metric validation; novelty per se not measured.",
            "generation_performance": null,
            "validation_performance": "Human judgments correlate strongly with automated LLM-based evaluations (Pearson r = 0.7518, p &lt; 0.005), supporting the automated evaluation pipeline's reliability.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Human evaluators can recognize valid but novel reasoning expressions that automated matchers might miss; thus human review is important when generated reasoning diverges in wording.",
            "generation_validation_asymmetry": "Human evaluation confirms that conceptual correctness (as judged by humans) can exceed execution success, reinforcing the observed generation-validation asymmetry.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": "High per-instance cost (human time); applied only to a subset for metric validation.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human spot-checking to validate automated metrics and to identify cases where automated mapping may under- or over-count matches.",
            "evidence_type": "supports",
            "key_findings": "Human judgements align well with automated LLM-based reasoning-graph assessments, but human review remains necessary for edge cases and to confirm automated evaluations' validity.",
            "uuid": "e2180.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "rating": 2
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 2
        },
        {
            "paper_title": "Paper2code: Automating code generation from scientific papers in machine learning",
            "rating": 2
        },
        {
            "paper_title": "Paperbench: Evaluating ai's ability to replicate ai research",
            "rating": 2
        },
        {
            "paper_title": "Can LLMs generate novel research ideas? a large-scale human study with 100+ nlp researchers",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        }
    ],
    "cost": 0.02599,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers
7 Aug 2025</p>
<p>Yanzheng Xiang yanzheng.xiang@kcl.ac.uk 
King's College London</p>
<p>Hanqi Yan hanqi.yan@kcl.ac.uk 
King's College London</p>
<p>Shuyin Ouyang shuyin.ouyang@kcl.ac.uk 
King's College London</p>
<p>Lin Gui lin.1.gui@kcl.ac.uk 
King's College London</p>
<p>Yulan He yulan.he@kcl.ac.uk 
King's College London</p>
<p>The Alan Turing Institute</p>
<p>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers
7 Aug 202531A2EB758D35837D1F67695B1219E75CarXiv:2504.00255v2[cs.CL]
This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers.The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs.To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases.Building on SciReplicate-Bench, we propose Sci-Reproducer, a dualagent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions.To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure.For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics.In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models.The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark's difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction.We make available our benchmark and code at GitHub and project homepage at Homepage.</p>
<p>Introduction</p>
<p>The evolution of Large Language Models (LLMs) has ushered in a transformative era in scientific discovery, positioning them as powerful tools for streamlining research (Gridach et al., 2025;Buehler, 2024;Lu et al., 2024), from idea generation to verification and publication writing.For instance, Si et al. (2025); Gu &amp; Krenn (2025) demonstrated how LLMs can be prompted to generate novel research ideas, while Yuan et al. (2022); Du et al. (2024) explored their use in producing literature reviews for idea evaluation.Additionally, LLMs are increasingly integrated into tools like Semantic Scholar1 , Research Rabbit2 , and Undermind Research Assistant3 , enhancing literature discovery, citation analysis, and knowledge synthesis.These advancements, both in research methodologies and practical applications, suggest that LLMs have the potential to assist across multiple stages of scientific discovery.</p>
<p>Among the aforementioned advancements in research acceleration, the ability of LLMs to correctly generate code for validating real-world scientific ideas is particularly noteworthy.Computational validation is crucial across many fields, yet researchers often face barriers due to limited coding expertise or inaccessible implementations.By converting scientific Table 1: Comparisons of different machine learning software engineering benchmarks.</p>
<p>To address this gap, we developed SciReplicate-Bench, the first benchmark specifically designed to evaluate LLMs' capabilities in code generation for reproducing research findings from academic papers.It consists of 100 code reproduction tasks derived from 36 papers published in leading conferences in 2024.This recent publication window was deliberately chosen to minimize the risk of data leakage.An overview of the task is illustrated in Figure 1, with a concrete example provided in Figure A2 in Appendix E. The task consists of two main steps: 1. Algorithm understanding.LLMs must extract essential information from the paper, such as workflow details, algorithm descriptions, and hyperparameter values.2. Code implementation.LLMs then implement a function or method within a provided repository, using both the extracted information and the LaTeX representation of the algorithm from the paper.We introduce Sci-Reproducer, a dual-agent system that combines a Paper Agent and a Code Agent to handle these two steps collaboratively and implement code for the target algorithm.</p>
<p>To rigorously assess LLM performance on this benchmark, we evaluate two dimensions corresponding to the aforementioned two steps: algorithm comprehension correctness and code correctness.To evaluate algorithm comprehension, we introduce a reasoning graph to represent the reasoning logic behind algorithm reproduction.Each node in the graph represents a code comment, which reflects a single reasoning step and is aligned with a specific segment of code.Edges between nodes are defined based on data flow relationships across different code segments.We compute the similarity between the generated reasoning graph and a reference graph to derive the reasoning graph accuracy.To evaluate code correctness, we employ established metrics including execution accuracy (Rajkumar et al., 2022;Xiang et al., 2023), CodeBLEU (Ren et al., 2020), and recall of intra/cross-file dependencies and APIs.</p>
<p>Our work makes the following contributions:</p>
<p>Benchmarks: SciReplicate-Bench, a benchmark of 100 algorithm reproduction tasks from recent NLP publications.</p>
<p>Metric:</p>
<p>We propose a novel reasoning graph accuracy metric for evaluating algorithmic comprehension.Approach: Sci-Reproducer, a dual-agent framework combining paper understanding and code implementation.Insights: Comprehensive evaluation across state-of-the-art LLMs reveals four key findings: (i) the task remains highly challenging, with execution accuracy below 40% for all models; (ii) reasoning models exhibit "overthinking" behavior (Cuadron et al., 2025;Sui et al., 2025), over-relying on internal reasoning rather than utilizing available tools for information extraction; (iii) while LLMs demonstrate strong algorithmic comprehension, they struggle with practical implementation; and (iv) algorithm reproduction failures often stem from incomplete paper descriptions, which our Sci-Reproducer effectively addresses.</p>
<p>Related Work</p>
<p>Our work lies at the intersection of AI for automating scientific discovery and LLM-based code generation.</p>
<p>AI for Automating Scientific Discovery</p>
<p>The application of LLMs to accelerate scientific research has emerged as a rapidly growing field with diverse approaches.Several studies have demonstrated the potential for comprehensive research automation through end-to-end AI systems.Schmidgall et al. (2025); Lu et al. (2024) developed frameworks that integrate idea generation, experimental validation, and manuscript composition, with some AI-authored papers successfully passing workshop review processes (Yamada et al., 2025).Complementary research has focused on the creative aspects of scientific inquiry, with Wang et al. (2023); Ghafarollahi &amp; Buehler (2024); O'Neill et al. (2025) investigating LLMs' capacity for generating novel research hypotheses.Notably, recent evaluations by Gu et al. (2024); Kumar et al. (2024); Liu et al. (2025); Si et al. (2024) suggest that AI-generated research concepts may occasionally exceed human-generated ideas in terms of novelty and originality.Within computational disciplines where implementation validation is essential, LLMs have shown promise in algorithm design and code development tasks.Our proposed SciReplicate-Bench addresses a previously underexplored area: the automated reproduction of algorithms directly from academic publications.This represents a unique challenge at the intersection of scientific literature comprehension and executable code synthesis.While recent parallel efforts such as PaperBench (Starace et al., 2025) and Paper2CodeBench (Seo et al., 2025) have tackled related problems by exploring full codebase reconstruction, these evaluation approaches rely substantially on manual assessment criteria and LLM-based correctness judgments, introducing potential inconsistencies and reliability concerns.Our approach prioritizes objective evaluation through execution accuracy, providing more rigorous validation than non-executable assessment methodologies (Wang et al., 2022;Chen et al., 2021).</p>
<p>The broader ecosystem of computational reproducibility research includes specialized frameworks such as MLGym (Nathani et al., 2025) for baseline improvement, and evaluation benchmarks developed by Siegel et al. (2024);Ren et al. (2023) that assess LLMs' ability to reproduce published experimental results using existing codebases.</p>
<p>LLMs for Code Generation</p>
<p>Code generation has emerged as a prominent application of LLMs, with benchmarks ranging from basic programming tasks (Chen et al., 2021;Jain et al., 2024;Austin et al., 2021;Hendrycks et al., 2021;Liu et al., 2022) to realistic software engineering challenges like SWE-bench (Jimenez et al., 2023), which uses actual repository pull requests.However, these benchmarks primarily target general software engineering rather than scientific algorithm reproduction.</p>
<p>Recent efforts have developed machine learning-specific benchmarks (Liu et al., 2023;Huang et al., 2023;Chan et al., 2024), but these typically involve implementing algorithms proposed by the models themselves or solving relatively straightforward tasks.They lack the depth of algorithmic understanding and rigorous paper analysis required for reproducing algorithms from peer-reviewed publications.</p>
<p>Despite advances in tool-augmented code generation (Schick et al., 2023;Zhang et al., 2024a;a;2023b), no existing system specifically addresses the unique challenge of translating academic papers into executable code.Our Sci-Reproducer framework demonstrates the ability to comprehend academic publications and convert abstract algorithm descriptions into functional implementations.</p>
<p>SciReplicate-Bench</p>
<p>Overview SciReplicate-Bench is designed to evaluate LLMs' ability to reproduce algorithms from academic papers, consisting of 100 tasks curated from 36 recent NLP publications with their corresponding open-source implementations.The task categories are detailed in Figure A1 in Appendix B. The benchmark focuses on repository-level code generation, where each task is centered around implementing a specific function or class method.As illustrated in Figure A3 in Appendix E, each task comprises nine components, which can be categorized into three groups corresponding to code generation, evaluation, and analysis, respectively.</p>
<p>For code generation, the following components are provided as inputs to LLMs:</p>
<p>Function signature: the definition of the target function, including detailed descriptions of its input and output variables.</p>
<p>Algorithm Description: The LaTeX code description of the target algorithm, typically located within a subsection or paragraph of the target paper.</p>
<p>Literature context: the original paper along with its cited references, providing broader conceptual context.</p>
<p>Repository context: all source files and code in the repository that inform or support the target implementation.</p>
<p>For evaluation, the following components are provided for code execution and metrics calculation:</p>
<p>Reference implementation: ground-truth code serving as the reference for CodeBLEU evaluation.</p>
<p>Reasoning graph annotations: structured representations of the algorithmic logic and implementation flow, enabling assessment of reasoning graph accuracy.Dependency annotations: comprehensive documentation of internal dependencies, crossfile relationships, and external API usage for computing recall metrics across all dependency categories.</p>
<p>Test environment: isolated Python execution environment containing validation cases and automated verification scripts for assessing implementation correctness.</p>
<p>To enable further analysis of the underlying causes of LLM failures, the benchmark includes: Missing/Mismatch Information: the LaTeX description of the algorithm may omit certain implementation details, which could either appear elsewhere in the paper or be entirely absent.We also annotate mismatches between the paper description and the reference implementation.</p>
<p>Task Definition Based on SciReplicate-Bench, an LLM is given the algorithm description, function signature, literature context, and repository context as input.The LLM is asked to output a function that implements the target algorithm.</p>
<p>Benchmark Construction</p>
<p>The benchmark construction process comprises four key steps: paper selection, Python environment setup, documentation, and verification suite preparation.To mitigate the risk of data leakage, we selected papers published in 2024 that provide publicly available code repositories.During the annotation process, each repository was refactored to isolate the core algorithm into a standalone function, and all sources of randomness were removed to ensure reproducibility and prevent leakage.On average, annotating each paper requires approximately 12 hours.Details of the annotation process are provided in Appendix A.</p>
<p>Evaluation Metrics</p>
<p>Evaluating Algorithm Comprehension</p>
<p>We propose the reasoning graph accuracy metric to evaluate how well LLMs understand the logic and implementation of algorithms.During code generation, LLMs are prompted to insert specially formatted, non-overlapping, non-nested comments that mark reasoning steps derived from the algorithm's LaTeX code (The prompt can be found in Figure A5).</p>
<p>We then construct a reasoning graph G = {N, E} (illustrated in Figure A3), modeled as a Directed Acyclic Graph (DAG).Each node n i = ⟨w i , c i ⟩, n i ∈ N represents a reasoning step with a comment w i and corresponding code snippet c i .An edge e i = ⟨n i , n j ⟩, e i ∈ E is added if a variable used in c j is defined or last modified in c i .To compute the reasoning graph accuracy, we compare the generated graph G g with the reference graph G r via node and edge matching:</p>
<p>Node matching: comments from G r and G g are passed to GPT-4o, which maps each reference node to one or more nodes in the generated graph.A node in G r is considered matched if it has at least one corresponding node in G g .The prompt template used for this process is available in Figure A4.</p>
<p>Edge matching: for each reference edge e r = ⟨n i r , n j r ⟩, if both endpoint nodes have corresponding matches in G g , we apply Breadth-First Search(BFS) to verify whether a corresponding edge exists in G g .</p>
<p>The reasoning graph accuracy S r is computed as:
S r = n i ∈N m ∑ n i s n i + e j ∈E m ∑ e j s e j . (1)
where N m and E m denote the sets of matched nodes and edges, respectively, and s n i and s e j represent their corresponding significance scores.Node significance is determined by the complexity of its corresponding code segment, measured by the number of variable definitions and usages, function calls, arithmetic operations, and lines of code, then normalized across the reference graph.Edge significance is calculated as the product of the significance scores of its connected nodes, followed by normalization.</p>
<p>Evaluating Code Generation</p>
<p>For assessing coding ability, we use the following evaluation metrics:</p>
<p>• Execution accuracy (Xiang et al., 2023;Zhang et al., 2024b;Long et al., 2022): we integrate the generated code into the repository and execute it to obtain results.If all test cases match the reference results, we consider the code correct.</p>
<p>• CodeBLEU (Ren et al., 2020): this metric evaluates how similar the generated code is to reference code by using the traditional BLEU metric (Papineni et al., 2002) while incorporating syntactic information through abstract syntax trees (AST) and semantic understanding via data-flow graphs (DFG).</p>
<p>• Recall (Li et al., 2024): we calculate recall scores specifically for intra-file dependencies, cross-file dependencies, and external APIs.</p>
<p>SearchSection Section ID</p>
<p>The entire content of a section based on the section label.</p>
<p>SearchLiterature Paper ID, query The answer to the query searched from the literature (identified by Paper ID).</p>
<p>Code Agent SearchCode Name</p>
<p>The definition of a specific code element in repository.</p>
<p>SearchFile Name</p>
<p>The content of a certain file in repository.</p>
<p>SearchWeb Query</p>
<p>The information obtained from the website.</p>
<p>Compiler code</p>
<p>The feedback from the compiler after executing the code.</p>
<p>Sci-Reproducer</p>
<p>To address this task, we introduce Sci-Reproducer4 , a dual-agent framework designed for scientific paper algorithm replication.As illustrated in Figure 1, Sci-Reproducer comprises a Paper Agent and a Code Agent that collaboratively work to replicate algorithms described in a given paper.The predefined actions employed by the agents are summarized in Table 2, with implementation details provided in Appendix C.</p>
<p>Paper Agent</p>
<p>Based on the provided algorithm description, the Paper Agent systematically retrieves contextual information from the literature context to support algorithmic understanding.Due to the input length limitations of LLMs, it is infeasible to input entire paper along with their associated literature.Consequently, the Paper Agent must selectively extract pertinent information, following a strategy akin to Retrieval Augmented Generation (RAG) (Wang et al., 2024b;Sarthi et al., 2024).The Paper Agent incrementally builds an understanding of the target algorithm by executing predefined actions to query the literature context.To facilitate this process, we adopt ReAct (Yao et al., 2022) as the agent strategy, which enables seamless integration of action execution with intermediate reasoning steps.</p>
<p>After the Paper Agent concludes that all necessary information has been collected, it generates a comprehensive literature report comprising key findings that fill in the missing  components of the target algorithm's LaTeX source.An example of the literature report is shown in Figure A8.This report subsequently serves as a crucial input for the Code Agent.</p>
<p>The prompt used to guide the Paper Agent is provided in Figure A6.</p>
<p>Code Agent</p>
<p>Informed by the algorithm description, literature report, and code context, the Code Agent searches the repository to locate essential dependencies required for implementation.It can also browse websites for additional information and use a compiler to test and iteratively debug the code, ensuring proper execution by identifying and fixing syntax errors.The prompt for the Code Agent is provided in Figure A7.</p>
<p>Experiments</p>
<p>We evaluate Sci-Reproducer on the SciReplicate-Bench benchmark using 7 advanced LLMs, including five non-reasoning LLMs: GPT-4o-mini (4o mini, 2024), GPT-4o (GPT-4o, 2024), Claude-Sonnet-3.7 (Claude-Sonnet-3.7,2025), Gemini-2.0-Flash(Gemini-2.0-Flash,2024), and Deepseek-V3 (DeepSeek-AI et al., 2024), and different versions of the reasoning models O3-mini (o3 mini, 2024), i.e., three different levels of reasoning intensity.For the reasoning graph accuracy metric, node matching is performed using GPT-4o, which may introduce some randomness.To reduce this variability, we set the temperature to 0 and top-p to 1, ensuring more deterministic generation.The calculation is repeated three times, and we report the average score as the final result.</p>
<p>Results on SciReplicate-Bench</p>
<p>LLMs face challenges with actual implementation</p>
<p>Although LLMs are capable of understanding algorithms, their performance in code generation remains suboptimal.Despite using Sci-Reproducer, the average execution accuracy remains low at 0.235, with a CodeBLEU score of 0.320.</p>
<p>Accurate dependency and API identification is crucial for code implementation</p>
<p>Effectively recognizing and leveraging dependencies from the source repository and external APIs is essential for accurate code implementation.The integration of Code Agent led to substantial gains in recall with average increases of 0.441, 0.239, and 0.100, respectively, compared to cases without the agent.With Sci-Reproducer, Claude-Sonnet-3.7 attains the highest execution accuracy of 0.390, with the highest recall for intra/cross file dependency and API usage, at 0.776, 0.636, and 0.626 respectively.</p>
<p>Overthinking leads to limited improvement in reasoning LLMs</p>
<p>Reasoning LLMs exhibit more modest performance gains when using Sci-Reproducer.While they achieve an average execution accuracy improvement of 0.13, non-reasoning models demonstrate substantially larger gains of 0.212.This pattern extends to recall metrics, where reasoning LLMs show improvements of 0.243, 0.061, and 0.041 respectively, compared to non-reasoning LLMs' more pronounced gains of 0.560, 0.345, and 0.135.We attribute this performance gap to the "overthinking" phenomenon (Cuadron et al., 2025;Sui et al., 2025), where excessive internal reasoning impedes effective action execution, a limitation that we examine in detail in the following subsection.</p>
<p>Tool Usage Analysis</p>
<p>Figure 2 presents the number of times each LLM invokes actions on the full dataset with Sci-Reproducer.We observe the following: • For code-related actions, reasoning LLMs demonstrate limited tool usage, employing "SearchFile", "SearchCodeItem", and "SearchWeb" only 25.0, 3.3, and 0.0 times on average, respectively.Non-reasoning LLMs use these same actions far more extensively, with averages of 210.4,68.2, and 16.8 times respectively.This disparity reveals a fundamental behavioral difference: reasoning models favor internal deliberation over external information gathering.Conversely, reasoning LLMs invoke "Compiler" more frequently, indicating they require more debugging iterations due to inadequate contextual information gathering.This over-reliance on internal reasoning undermines performance: advanced models like o3-mini-high and o3-mini-low achieve execution accuracy comparable to GPT-4o-mini, negating their theoretical computational advantages.• Paper-related actions exhibit a similar pattern.Reasoning LLMs use "SearchPaper", "SearchSection", and "SearchLiterature" an average of 56.3, 70.0, and 20.0 times respectively, while non-reasoning LLMs demonstrate substantially higher usage at 244.8, 188.4,and 58.0 times respectively.Additionally, we observe a clear preference for target paper extraction over external literature consultation.Actions targeting the primary paper ("SearchPaper" and "SearchSection") are invoked 174.1 and 144 times on average, significantly more than "SearchLiterature" which accesses related works only 43.8 times.</p>
<p>Error Analysis</p>
<p>Syntax Errors</p>
<p>Table A4 shows the syntax error rates for each model across different configurations.Without the Code Agent, syntax errors occurred at rates of 80.3% ("NoAgent") and 83.3% ("Paper Agent").After implementing the Code Agent, these error rates dropped significantly to 29.4% ("Code Agent") and 24.9% ("Sci-Reproducer").The remaining syntax errors mainly result from incorrectly using repository dependencies.This occurs because our approach, unlike human developers, cannot dynamically access runtime information through a compiler during the code generation process.</p>
<p>Logic Errors</p>
<p>Another issue stems from differences in implementation logic, which can be broadly categorized into: (1) discrepancy in algorithm implementation that result in differing outputs, and</p>
<p>(2) missing or mismatch information in the algorithm descriptions in the paper compared to the actual code.</p>
<p>Implementation discrepancy An algorithm may have multiple valid implementation approaches.For example, the cross-entropy loss function can be implemented by directly  invoking the PyTorch API "torch.nn.CrossEntropy" or by manually coding it from scratch.Such implementation choices may introduce subtle differences that lead to variations in the final output of the function.</p>
<p>Missing/Mismatched information in algorithm description Algorithmic descriptions in research papers often lack concrete implementation details, and in certain cases, the provided code may exhibit minor discrepancies compared to the descriptions in the paper.</p>
<p>We manually compared the implementation code of all tasks in the dataset with their descriptions in the papers to identify missing or mismatch information.We then provided this information as additional input and apply Sci-Reproducer framework on three LLMs.The Results is shown in Table 4, regarding to Execution Acc, the performance for GPT-4o-mini, Deepseek-V3 and O3-mini-low improved 0.050, 0.250 and 0.040 respectively.The missing information can be divided into four categories:</p>
<p>• Hyperparameters and configurations: descriptions of target algorithms in papers often omit specific hyperparameter settings, such as the batch size.• Numerical stability techniques: standard techniques for ensuring numerical stability, such as handling division by zero.• Implementation logic: common implementation practices and model design choices, such as data splitting protocols.• Coding strategy: practical programming techniques that enhance implementation efficiency and reliability, such as early stopping criteria.</p>
<p>More examples for each category can be found in Table A5 in Appendix E. As for mismatched information, it occurs far less frequently compared to missing information, and its categories largely overlap with those mentioned above.</p>
<p>To mitigate the widespread issues of missing and mismatched information, the first category can generally be addressed by referencing the original research paper and related literature, or by inspecting the code repository for explicit configurations.However, addressing the other three categories requires familiarity with general machine learning coding conventions, thus necessitating that the LLMs identify and utilize implementation patterns from comparable algorithms to enhance code quality.Future research may improve performance by incorporating implementation insights from similar algorithms through techniques such as in-context learning (Zhou et al., 2023;Xiang et al., 2024), and by leveraging real-time compiler feedback to infer precise variable values.</p>
<p>Conclusion</p>
<p>We evaluate LLMs' ability to replicate algorithms described in recent NLP papers.To support this, we introduce SciReplicate-Bench, a benchmark with rich annotations, and Sci-Reproducer, a dual-agent framework for bridging algorithm understanding and code generation.We assess performance using reasoning graph accuracy and standard implementation metrics.Results show the task is highly challenging, with failures largely caused by missing or inconsistent algorithm descriptions.</p>
<ol>
<li>Detailed annotation: for each aligned function, annotators documented input/output variables, intra-and cross-file dependencies, and external API usage.Additionally, they inserted explanatory comments mapping code segments to algorithm components.Based on these annotations and variable dependencies, we can construct a reasoning graph representing the implementation logic.During the annotation process, LLMs were employed to assist with algorithm-function alignment and the generation of variable descriptions and code comments.All outputs were subsequently reviewed and corrected by human annotators to ensure accuracy.</li>
</ol>
<p>The final selected papers are listed in Table A1.</p>
<p>Step 4: verification suite preparation Finally, annotators created verification suites with 10 test cases per task, drawn from the original datasets used in each repository for the majority of papers.For a small number of repositories, fewer than 10 test cases could be constructed.For instance, algorithms that analyze LLM parameters may have only a single test case.Given the inherent randomness in many NLP implementations and potential machine-related variability, we addressed reproducibility from two angles:</p>
<p>• Eliminating code randomness: annotators fixed random seeds and replaced nondeterministic operations (e.g., unordered sets) with deterministic equivalents to ensure consistent outputs across runs.• Controlling hardware variability: users were instructed to run both reference and generated code locally to eliminate discrepancies caused by hardware differences.</p>
<p>Lastly, annotators implemented task-specific comparison scripts to evaluate output correctness, accounting for variations in return types across tasks.</p>
<p>B Details of the Task Categories</p>
<p>C Details of the Actions</p>
<p>In this section, we provide implement details for all actions defined in the Sci-Reproducer.</p>
<p>SearchPaper We obtain the LaTeX source code of the target academic paper from arXiv 7and apply regular expression-based parsing to extract the content corresponding to each section.Subsequently, we iteratively feed the content of each subsection, along with the query generated by the large language model, into GPT-4o-mini.The model extracts relevant information and returns it as an observation to the paper agent.</p>
<p>SearchSection Following the same approach as SearchPaper, the tool begins by parsing the LaTeX source code of the target algorithm.Upon receiving a section ID from the Paper Agent, it retrieves and returns the content of the corresponding section.</p>
<p>SearchLiterature Given a paper ID and a query, the tool attempts to download the corresponding LaTeX source code from arXiv.If the LaTeX source code is unavailable, it returns no information.Otherwise, it extracts content relevant to the query from the paper, following the same procedure as the SearchPaper action.</p>
<p>SearchCode For each Python file in the code repository, we utilize the Python AST8 package to parse the file and extract all defined classes, functions, and global variables.Unlike embedding-based code search methods (Zhang et al., 2024c;2023c), the Code Agent in our framework directly provides the name of a code item.The tool then returns the corresponding definition if it exists; otherwise, it returns an empty response.</p>
<p>SearchFile When the Code Agent provides a file name, the tool returns the full content of the corresponding file.</p>
<p>SearchWeb When the Code Agent issues a query, we use the Google Search API 9 to retrieve relevant information from websites.These results are then processed by GPT-4omini, which filters the content and extracts the information most relevant to the query for return.</p>
<p>Compiler Once the Code Agent completes code generation, it invokes the compiler to execute the code.The generated function or method is inserted into the original Python file, and the corresponding Python environment is used to run the code.The output from the compiler is then returned as the feedback.</p>
<p>D Human Evaluation of Reasoning Graph Accuracy</p>
<p>To validate the reliability of our LLM-based evaluation metrics, we conducted a human evaluation study on a subset of our benchmark.Three PhD students in computer science independently assessed the reasoning graph accuracy for 20 tasks generated by GPT-4o-mini and O3-mini-medium using Sci-Reproducer.</p>
<p>As shown in Table A2, human evaluations demonstrate strong alignment with our automated LLM-based assessments.The mean scores show consistent agreement between human annotators and GPT-4o evaluations, with standard deviations indicating reasonable inter-annotator consistency.</p>
<p>Furthermore, we computed the Pearson correlation coefficient between human and LLMbased evaluations across all assessed instances.As presented in Table A3, the correlation (r = 0.7518, p ¡ 0.005) indicates a strong positive relationship between human judgments and automated assessments, supporting the validity of our LLM-based evaluation approach.The code snippets corresponding to each comment must not overlap, and nesting is not allowed.5. Single Function: Implement the code within a single function (or method), without breaking it into multiple functions.6. Import Package: Import all packages within the function (or method) to ensure the code is self -contained.7. Format, the comment for each snippet should be in the following format: # ---------------------------------------------------------------------------# Snippet x: Comment here # ---------------------------------------------------------------------------# # -------------------------------------------------------------------# Snippet 3: The transformation is a simple element-wise operation combined with # the scaling_factor, illustrating a simplified version of the # alignment concept from the LaTeX, which might involve more complex # contrastive or attentional calculations.# -------------------------------------------------------------------# [Begin Snippet 3] updated_segment = torch.add(segment_main,torch.mul(segment_aux,scaling_factor)) transformed_tokens[i, start_idx:end_idx, :] = updated_segment # [End Snippet 3] # ---------------------------------------------------------------------------# Snippet 4: The final transformed_tokens are now partially aligned with the auxiliary # reference, reflecting the notion of augmenting token-level outputs # (Equation references in the LaTeX snippet would correspond to eq. (2 -3) or # similar definitions of reference alignment).# ---------------------------------------------------------------------------# [Begin Snippet 4] return transformed_tokens # [End Snippet 4] ``Ỳ our answer:</p>
<p>Figure 1 :
1
Figure 1: Overview of the task and the proposed Sci-Reproducer framework.The task involves algorithm understanding and code implementation, handled by a Paper Agent and a Code Agent operating in separate contexts with specialized actions.</p>
<p>Figure 2 :
2
Figure 2: A grouped bar chart illustrating the frequency of tool usage by different models.The x-axis represents various actions, while the y-axis indicates the total number of times each tool was used on this dataset.</p>
<p>Model</p>
<p>(Sci-Reproducer) Exe Acc(↑) CodeBLEU(↑) RG Acc(↑) Recall Intra-File(↑) Cross-File(↑) API(↑)</p>
<p>Figure A1 :
A1
Figure A1: The categories of the tasks within SciReplicate-Bench.The benchmark encompasses five main task categories in the NLP domain: representation and embedding methods, loss functions and optimization objectives, information extraction and aggregation, model architecture components, and inference and search algorithms.The distribution of each task category is illustrated in Figure A1.</p>
<p>start_idx, end_idx) in enumerate(transformation_indices): # -----------------------------------------------------------------------# Snippet 1: We first verify if the current batch item is valid by checking the # batch_mask, analogous to referencing an in-context example (\mathbf{S}) # that has a corresponding reference (\mathbf{H_r}) in the LaTeX snippet.# -----------------------------------------------------------------------#[Begin Snippet 1] if batch_mask[i]: # [End Snippet 1]# -------------------------------------------------------------------# Snippet 2: Here, we apply a basic shift to the token_representation by # incorporating a slice from the auxiliary_representation, akin to # combining (\mathbf{H}) with a portion of (\mathbf{H_r}) for # enhanced alignment.# -------------------------------------------------------------------# [Begin Snippet 2] segment_main = transformed_tokens[i, start_idx:end_idx, :] segment_aux = auxiliary_representation[i, start_idx:end_idx, :] # [End Snippet 2]</p>
<p>Figure A5 :
A5
Figure A5: The prompt for code generation.</p>
<p>Table 2 :
2
The pre-defined actions for the Paper Agent and the Code Agent.</p>
<p>Table 3 :
3
Performance evaluation on the SciReplicate-Bench benchmark.Models with notation indicate reasoning LLMs."Exe Acc" represents execution accuracy while "RG Acc" indicates reasoning graph accuracy.</p>
<p>Table 3 displays
3
Sci-Reproducer's evaluation results and contributions of Code/Paper Agent.The "No Agent"directly prompts the LLM to generate code based solely on the algorithm description and function signature."NoPaper Agent" allows the LLM to use Code Agent actions, but restricts access to Paper Agent actions."NoCode Agent" grants access to Paper Agent actions but blocks Code Agent capabilities.The results offer key insights, discussed in the following.executionaccuracy without using the agent to examine literature and repository contexts.With enhancement of Sci-Reproducer, these LLMs show notable improvements, with an average increase of 0.181 in execution ACC and 0.057 in CodeBLEU, although even the best-performing model, Claude-Sonnet-3.7,only achieved 0.390 execution accuracy.This highlights the exceptional challenge presented by our SciReplicate-Bench.
LLMs struggles on SciReplicate-Bench Most LLMs perform poorly, achieving less than0.1
LLMs can comprehend algorithm logicLLMs demonstrate strong algorithmic comprehension capabilities, as evidenced by reasoning graph accuracy scores averaging 0.716 even without agent assistance.The addition of individual agents provides modest but consistent improvements: the Paper Agent increases understanding by 0.009 on average, while the Code Agent contributes a larger gain of 0.036.Combined agent deployment yields a cumulative improvement of 0.037.These enhancements stem from complementary mechanisms: the Paper Agent strengthens theoretical comprehension by gathering relevant contextual information from academic literature, while the Code Agent facilitates practical understanding through extraction of pertinent code patterns and dependency structures from repositories.</p>
<p>Table 4 :
4
Experimental Results when missing/mismatched information is regard as external input in the prompt.
GPT-4o-mini0.2200.3160.8090.5880.4850.409Deepseek-V30.4700.3780.8340.6820.4240.609o3-mini-low0.2200.2920.8500.2590.0910.460</p>
<p>Please complete the target function (or method) and provide the pure code output without any additional text.Include comments in the code following these specific guidelines:Code Comments: 1. Focus on Reasoning: Comments should explain the reasoning behind the code generation process, as derived from the LaTeX description.2. No Implementation Details: Avoid including any code-specific implementation details in the comments.3. Mapping to LaTeX: Each comment must indicate which functionality described in the LaTeX code is implemented in the subsequent Python code snippet.4. No overlap:
TitleConference1. From Zero to Hero: Cold-Start Anomaly Detection (Reiss et al., 2024)Findings of ACL 20242. Addressing Order Sensitivity of In-Context Demonstration Examples in Causal LanguageFindings of ACL 2024Models (Xiang et al., 2024)3. Breaking the Ceiling of the LLM Community by Treating Token Generation as a ClassificationFindings of EMNLP 2024for Ensembling (Yu et al., 2024)4. Simple but Effective Compound Geometric Operations for Temporal Knowledge GraphCompletion (Ying et al., 2024)
https://www.semanticscholar.org
https://www.researchrabbit.ai/
https://www.undermind.ai/
A video demonstration showcasing Sci-Reproducer's capabilities is provided at https://youtu. be/qcSIMgyehjE
https://paperswithcode.com/api/v1/docs/
https://docs.github.com/en/rest?apiVersion=2022-11-28
https://arxiv.org/
https://docs.python.org/3/library/ast.html
https://developers.google.com/custom-search/v1/
AcknowledgementsThis work was supported in part by the UK Engineering and Physical Sciences Research Council (EPSRC) through a Turing AI Fellowship (grant no.EP/V020579/1, EP/V020579/2).We thank the authors of the selected papers for making their code openly available.Code Repository Code Context Website Actions Search web Code Interpreter Search Code Literature ReportAppendixA Details of the Annotation ProcessStep 1: paper selection We curated NLP papers from leading conferences in 2024, including ACL, EMNLP, ICLR, Neurips, and COLING.Using a web crawler, we collected accepted paper titles and employed the PapersWithCode API 5 to identify those with open-source implementations.For each identified paper, we retrieved corresponding GitHub repository links and metadata (e.g., stars, issues, release dates) via the GitHub REST API 6 .To filter candidates, we applied the following criteria:• Removed survey/exploratory papers while retaining method-focused research.• Applied a cutoff date of January 1, 2024 to avoid data leakage.• Excluded repositories with fewer than 5 stars to ensure basic quality assurance.Subsequently, researchers manually reviewed each candidate paper and its repository.We discarded papers with excessive computational demands, poorly structured code, ambiguous documentation, missing preprocessing steps, or reported reproduction issues.Step2: python environment setup For papers passing the initial screening, annotators followed the README to set up the environment and replicate experiments.Common issues included dependency conflicts, data loading failures, and incomplete or buggy code.Annotators attempted to resolve these problems; repositories with irrecoverable errors were excluded.Step3: annotation Annotation consists of two steps:1. Algorithm-Function alignment: most papers contain multiple algorithmic components, often organized as subsections.Annotators segmented these into distinct units and mapped each to its corresponding implementation.Code was refactored to encapsulate each algorithm in a standalone function or method.Papers with implementations too fragmented for restructuring were excluded.Published as a conference paper at COLM 2025 While this preliminary validation demonstrates the effectiveness of our automated metrics, comprehensive human evaluation across the full benchmark remains an important direction for future work.E Figures and TablesTarget Algorithm 4.1 Sentence ExtractionWe propose to model sentence extraction as extractive summarization.For this purpose, we concatenate all the sentences in the document into an input sequence, which is then fed to BertSum(Liu and Lapata, 2019)to obtain the score for each sentence.The details ofthe process can be found in Appendix a.  Step2: Calculate positive similarity score: cos(query, key) measures how close the model token is to the reference token.Step3: Calculate negative similarity score: cos(query, query.clone().detach())measures how close the token is to itself (detached) which serves as the negative reference.Step4: Final contrastive loss for these tokens: -log(positive / (positive + negative)) This aligns with the InfoNCE or contrastive objective described in the paper's Eq. (Information Augmentation).Step5: Accumulate the loss and update count and average the total contrastive loss by the number of token-level comparisons.The loss is averaged across tokens.Mismatch details: None⑨ Verification Suite 1. Test Cases: 10 test cases chosen from benchmark adopted in the original code repository.Output Compare Script:A Python program designed to verify the accuracy of the generated code's output.Implementation logic Data splitting, application of dropout, formatting of input sequences, and handling special or edge cases in the input data.Code EnvironmentCoding strategyCaching for performance enhancement, retry mechanisms to handle failures, early stopping criteria, and strategies for memory optimization.TableA5: Some examples for different missing information categories.[Task Overview] Reproduce Python code corresponding to a LaTeX-based methodology from a scientific paper.However, due to the paper's length, it cannot be fully ingested by a large language model at once.Therefore, the solution requires two main steps: 1. Information Retrieval (Your Current Task): Extract relevant details, insights, and supporting information from the academic paper's LaTeX description and related literature.2. Code Reproduction (Subsequent Task): Implement the Python code based on the information gathered and the provided LaTeX.[Your Specific Focus] You are tasked exclusively with Step 1: Information Retrieval.You must gather and organize all necessary details that will later be used to implement the Python code.[Input] 1. List of sections: The paper includes the following sections (titles are provided for reference): -w'x' (string): The title of the referenced section.Example:-Latex: "The full derivation of our loss function can be found in method Section .",Action: SearchSection["method"] * SearchLiterature[key, query] Description: If the target section cites another paper (\cite{label}) and you determine that some information needs to be retrieved from that paper, return SearchLiterature[label, query], where query is the specific information you need to look for in the referenced paper.Parameters:-'key' (string): The citation key of the referenced paper.In LaTeX, when citing a paper, we use \cite{x}, where x represents the citation key.-'query' (string): The specific information to search for in the referenced paper.Example: I -Latex: "We adopt the metric proposed in \cite{wang2025}".Action: SearchLiterature["wang2025", "The proposed metric in the paper"] -Latex: "The algorithm is based on the work of \cite{smith2018}".Action: SearchLiterature["smith2018", "The algorithm details in the paper"] -Latex: "The dataset is based on the study by \cite{jones2020}".Action: SearchLiterature["jones2020", "The dataset details in the paper"] [Instruction] In order to complete code reproduction, it is first necessary to understand the algorithm described in the LaTeX description.The tools "SearchPaper", "SearchSection" and "SearchLiterature" should be used to retrieve relevant information from the paper to help you understand the methodology proposed in the latex description.For example: 1.If the LaTeX Description lacks the definition of a variable, use "SearchPaper" tool to find its definition.2. If the LaTeX Description references other sections of the paper, use "SearchSection" tool to retrieve those sections and supplement the missing details. 3.If the LaTeX Description cites methods from other papers, use "SearchLiterature" tool to extract relevant information from the referenced papers.[Action] 1. Apply a tool defined above to gather external information.2. If you have gathered all the necessary information, fully understood the LaTeX code, and are prepared to proceed to the Code Reproduction stage, the appropriate action is "Finish"[Observation] 1.If the action is apply predefined tool, then the observation should be the return response of the tool.You are a code assistant tasked with reproducing a Python function corresponding to a algorithm in the methods part of a scientific paper.The local coding environment includes a GPU and supports CUDA.I will provide the following information:1. Repository structure: The organization of files within the code repository.This is a repository-level code generation task, so you should explore the repo thoroughly to extract useful code.2. Target function: The definition of the python function you need to implement.3. LaTeX description: The LaTeX code for the corresponding algorithm in the paper, describing the algorithm implemented by the target function.4. The extracted information: The information extracted from the target paper, and relevant literature that can provide you more details when implement the target function.5. Tools: Tools that can be adopted to gather external information during the generation process.The information is extracted from the paper and relevant literature by a paper search agent, which consists of a series of information points.When you implement the target function, you should refer to the extracted information to understand the target algorithm.When information from "Relevant Literature" conflicts with the target paper, always prioritize the information from the target paper.The extracted information is as follows: In order to complete this task, it is necessary to use tools to search the code repository for context that can help implement the target function.For example: 1. Use "SearchFile" to retrieve the content of a Python file from the repository.2. Use "SearchCodeItem" to find details about a specific code item within the repository.3. Use "SearchWeb" to retrieve information from the website.To effectively tackle the code reproduction task, follow a structured process that alternates between Thought, Action, and Observation steps:[Thought] 1. Analyze the current situation.2. Identify missing information from code.As it is a repo-level code generation task, you need to explore the relvant functions, classes, in the code repository.3. Plan the next steps to gather the required information.[Action] 1. Apply a tool defined above to gather external information.2. If you are ready to generate the code, then the action should be "GenerateCode".[Observation] 1.If the action is apply predefined tool, then the observation should be the return response of the tool.2. If the action is "GenerateCode", then the observation is the result returned by the interpreter after executing the generated code.
GPT 4o mini. </p>
<p>Adaptive contrastive search: Uncertainty-guided decoding for open-ended text generation. Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, M Aßenmacher, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, ArXiv, abs/2108.077322021</p>
<p>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning. Markus J Buehler, 10.1088/2632-2153/ad7228Machine Learning: Science and Technology. 2024</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal A Maksin, Lilian Patwardhan, Aleksander Weng, Mkadry, ArXiv, abs/2410.070952024</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo Bavarian, Clemens Winter, Phil Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, Fotios Chantzis, ArXiv, abs/2107.03374Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew CarrEvaluating large language models trained on code</p>
<p>Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning. Qizhou Chen, Taolin Zhang, Dongyang Li, Longtao Huang, Hui Xue, Chengyu Wang, Xiaofeng He, Conference on Empirical Methods in Natural Language Processing. 2024a</p>
<p>Routerdc: Querybased router by dual contrastive learning for assembling large language models. Shuhao Chen, Weisen Jiang, Baijiong Lin, James T Kwok, Yu Zhang, ArXiv, abs/240919886. 2024b</p>
<p>Learning to maximize mutual information for chain-of-thought distillation. Xin Chen, Hanxian Huang, Yanjun Gao, Yi Wang, Jishen Zhao, Ke Ding, Annual Meeting of the Association for Computational Linguistics. 2024c</p>
<p>When is tree search useful for llm planning? it depends on the discriminator. Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun, Annual Meeting of the Association for Computational Linguistics. 2024d</p>
<p>Reasoning paths optimization: Learning to reason and explore from diverse paths. Ken Yew, Guizhen Chia, Weiwen Chen, Anh Tuan Xu, Soujanya Luu, Li Poria, Bing, ArXiv, abs/2410.108582024</p>
<p>Nearest neighbor normalization improves multimodal retrieval. Neil Chowdhury, Franklin Wang, Sumedh Shenoy, Douwe Kiela, Sarah Schwettmann, Tristan Thrush, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis , Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E Gonzalez, ArXiv, abs/2502.082352025</p>
<p>Deepseek-Ai , Aixin Liu, Bei Feng, Bing Xue, Bing-Li Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J L Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Jun-Mei Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R J Chen, R L Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S S Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shao-Ping Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, T Shuting Pan, Tao Wang, Tian Yun, Tianyu Pei, W L Sun, Wangding Xiao, Wanjia Zeng, Wei Zhao, Wen An, Wenfeng Liu, Wenjun Liang, Wen-Xuan Gao, Wentao Yu, X Q Zhang, Xiangyu Li, Xianzu Jin, Xiaoling Wang, Xiaodong Bi, Xiaohan Liu, Xi-Cheng Wang, Xiaokang Shen, Xiaokang Chen, Xiaosha Zhang, Xiaotao Chen, Xiaowen Nie, Xiaoxiang Sun, Xin Wang, Xin Cheng, Xin Liu, Xingchao Xie, Xingkai Liu, Xinnan Yu, Xinxia Song, Xinyi Shan, Xinyu Zhou, Xinyuan Yang, Xuecheng Li, Xuheng Su, Y K Lin, Y Q Li, Y X Wang, Y X Wei, Yang Zhu, Yanhong Zhang, Yanping Xu, Yao Huang, Yao Li, Yaofeng Zhao, Yao Sun, Yaohui Li, Yi Wang, Yi Yu, Yichao Zheng, Yifan Zhang, Yi Shi, Ying Xiong, Ying He, Yishi Tang, Yisong Piao, Yixuan Wang, Yi-Bing Tan, Yiyuan Ma, Yongqiang Liu, Yu Guo, Yuan Wu, Yuchen Ou, Yuduan Zhu, Yue Wang, Yuheng Gong, Yujia Zou, Yukun He, Yunfan Zha, Yunxiang Xiong, Yuting Ma, Yu-Wei Yan, Yu Luo, Yuxuan Mei You, Yuyang Liu, Z F Zhou, Zehui Wu, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhen Xu, Zhen Huang, Zhenda Zhang, Xie, Zhewen Zhen Guo Zhang, Zhibin Hao, Zhicheng Gou, Zhigang Ma, Zhihong Yan, Zhipeng Shao, Zhiyu Xu, Zhongyu Wu, Zhuoshu Zhang, Zihui Li, Zijia Gu, Zijun Zhu, Zi-An Liu, Ziwei Li, Xie, ArXiv, abs/2412.19437Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report. Ziyang Song2024</p>
<p>Document-level claim extraction and decontextualisation for fact-checking. Zhenyun Deng, M Schlichtkrull, Andreas Vlachos, Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>LLMs assist NLP researchers: Critique paper (meta-)reviewing. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Ranran Haoran, Vipul Zhang, Yinghui Gupta, Tao Li, Fei Li, Qin Wang, Tianlin Liu, Pengzhi Liu, Congying Gao, Chen Xia, Cheng Xing, Zhaowei Jiayang, Ying Wang, Raj Sanjay Su, Ruohao Shah, Jing Guo, Haoran Gu, Kangda Li, Zihao Wei, Lu Wang, Surangika Cheng, Meng Ranathunga, Jie Fang, Fei Fu, Ruihong Liu, Eduardo Huang, Yixin Blanco, Rui Cao, Philip S Zhang, Wenpeng Yu, Yin, 10.18653/v1/2024.emnlp-main.292Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, ArXiv, abs/2409.055562024</p>
<p>GPT-4o. </p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, 2025</p>
<p>Xuemei Gu and Mario Krenn. Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders. Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, ArXiv, abs/2412.141412024. 2025Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, Jacob Steinhardt, ArXiv, abs/2105.099382021</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, International Conference on Machine Learning. 2023</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ArXiv, abs/2310.067702024. 2023Ofir Press, and Karthik NarasimhanSwe-bench: Can language models resolve real-world github issues?</p>
<p>Exploring concept depth: How large language models acquire knowledge at different layers?. Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang, ArXiv, abs/2404.070662024</p>
<p>Masklid: Code-switching language identification through iterative masking. Franccois Amir Hossein Kargaran, Hinrich Yvon, Schutze, ArXiv, abs/2406.062632024</p>
<p>Towards robust and generalized parameterefficient fine-tuning for noisy label learning. Yeachan Kim, Junho Kim, Sangkeun Lee, ArXiv, abs/2411.008732024</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, ArXiv, abs/2409.061852024</p>
<p>Style-specific neurons for steering llms in text style transfer. Wen Lai, Viktor Hangya, Alexander Fraser, ArXiv, abs/2410.005932024</p>
<p>Deveval: A manually-annotated code generation benchmark aligned with real-world code repositories. Jia Li, Ge Li, Yunfei Zhao, Yongming Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li, ArXiv, abs/2405Conference on Empirical Methods in Natural Language Processing. Hongfu Liu, Yuxi Xie, Ye Wang, Michael Shieh, 19856. 2024. 2024aAdvancing adversarial suffix transfer learning on aligned large language models</p>
<p>Beyond single-event extraction: Towards efficient document-level multi-event argument extraction. Wanlong Liu, Li Zhou, Dingyi Zeng, Yichen Xiao, Shaohuan Cheng, Chen Zhang, Grandee Lee, Malu Zhang, Wenyu Chen, ArXiv, abs/2405.018842024b</p>
<p>Haotian Ye, and Hinrich Sch ütze. Transmi: A framework to create strong baselines from multilingual pretrained language models for transliterated data. Yihong Liu, Chunlan Ma, ArXiv, abs/2405.099132024c</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou ; Yuliang, Xiangru Liu, Zefan Tang, Junjie Cai, Yichi Lu, Yanjun Zhang, Zexuan Shao, Helan Deng, Zengxian Hu, Kaikai Yang, Ruijun An, Shuzheng Huang, Sheng Si, Haozhe Chen, Zheng Zhao, Liang Li, Yiming Chen, Yan Zong, Tianyu Wang, Zhiwei Liu, Baobao Jiang, Yujia Chang, Wangchunshu Qin, Yilun Zhou, Zhao, ArXiv, abs/2503.21248Arman Cohan, and Mark B. Gerstein. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. 2025. 2023</p>
<p>Code generation from flowcharts with texts: A benchmark dataset and an approach. Zejie Liu, Xiaoyu Hu, Deyu Zhou, Lin Li, Xu Zhang, Yanzheng Xiang, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Nl2sql generation with noise labels based on multi-task learning. Lingli Long, Yongjin Zhu, Jun Shao, Zheng Kong, Jian Li, Yanzheng Xiang, Xu Zhang, Journal of Physics: Conference Series. 22942022</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N Foerster, Jeff Clune, David Ha, ArXiv, abs/2408.062922024</p>
<p>Less is ken: a universal and simple non-parametric pruning algorithm for large language models. Michele Mastromattei, Fabio Massimo Zanzotto ; Deepak, Lovish Nathani, Nicholas Madaan, Niko Roberts, Ajay Lay Bashlykov, Vincent Menon, Amar Moens, Despoina Budhiraja, Vladislav Magka, Gaurav Vorotilov, Dieuwke Chaurasia, Ricardo Silveira Hupkes, Tatiana Cabral, Jakob Shavrina, Yoram Foerster, William Bachrach, Wang Yang, Roberta Raileanu, ArXiv, abs/2502.144992024. 2025Mlgym: A new framework and benchmark for advancing ai research agents. o3 mini</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Raileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciuca, ArXiv, abs/2504.129762025</p>
<p>Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael E Moseley, Curtis P Langlotz, Akshay S Chaudhari, Jean-Benoit Delbrouck, ArXiv, abs/2405.03595Generative radiology report evaluation and error notation. Green2024</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Annual Meeting of the Association for Computational Linguistics. 2002</p>
<p>Enhancing knowledge distillation of large language models through efficient multi-modal distribution alignment. Tianyu Peng, Jiajun Zhang, International Conference on Computational Linguistics. 2024</p>
<p>Neuromax: Enhancing neural topic modeling via maximizing mutual information and group topic regularization. Duy-Tung Pham, Thien , Trang Nguyen Vu, Tung Nguyen, Linh Ngo Van, Duc , Anh Nguyen, Thien Huu Nguyen, ArXiv, abs/2409.197492024</p>
<p>Evaluating the text-to-sql capabilities of large language models. Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau, ArXiv, abs/2204.004982022</p>
<p>From zero to hero: Cold-start anomaly detection. Tal Reiss, George Kour, Naama Zwerdling, Ateret Anaby-Tavor, Yedid Hoshen, ArXiv, abs/2405.203412024</p>
<p>Superbench: A super-resolution benchmark dataset for scientific machine learning. N Pu Ren, Shashank Benjamin Erichson, Omer Subramanian, Zarija San, Michael W Lukic, Mahoney, ArXiv, abs/2306.140702023</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, M Tang, Ambrosio Zhou, Shuai Blanco, Ma, ArXiv, abs/2009.102972020</p>
<p>Raptor: Recursive abstractive processing for tree-organized retrieval. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning, ArXiv, abs/2401.180592024</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, ArXiv, abs/2302.047612023</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, ArXiv, abs/2501.04227Agent laboratory: Using llm agents as research assistants. 2025</p>
<p>Paper2code: Automating code generation from scientific papers in machine learning. Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang, ArXiv, abs/2504.171922025</p>
<p>Ircan: Mitigating knowledge conflicts in llm generation via identifying and reweighting contextaware neurons. Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, Deyi Xiong, ArXiv, abs/2406.184062024</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, ArXiv, abs/2409.041092024</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, Trans. Mach. Learn. Res. 20242024</p>
<p>Unsupervised homography estimation on multimodal image pair via alternating optimization. Sanghyeob Song, Jaihyun Lew, Hyemi Jang, Sungroh Yoon ; Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, ArXiv, abs/2504.01848Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan2024. 2025Paperbench: Evaluating ai's ability to replicate ai research</p>
<p>Stop overthinking: A survey on efficient reasoning for large language models. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu, 2025</p>
<p>Language-specific neurons: The key to multilingual capabilities in large language models. Liyan Tang, Philippe Laban, Greg Durrett, ; Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen, ArXiv, abs/2402.16438Conference on Empirical Methods in Natural Language Processing. 2024a. 2024bMinicheck: Efficient fact-checking of llms on grounding documents</p>
<p>Unifying dualspace embedding for entity alignment via contrastive learning. Hung Quoc To, Minh Huynh Nguyen, D Q Nghi, Bui ; Cunda, Weihua Wang, Qiuyu Wang, Feilong Liang, Guanglai Bao, Gao, ArXiv, abs/2412.05028Annual Meeting of the Association for Computational Linguistics. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 2023. 2024a. 2023Annual Meeting of the Association for Computational Linguistics</p>
<p>Garlic: Llm-guided dynamic progress control with hierarchical weighted graph for long document qa. Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He ; Yaoke, Yun Wang, Wenqiao Zhu, Yueting Zhang, Yunfei Zhuang, Siliang Li, Tang, ArXiv, abs/2410.04790Conference on Empirical Methods in Natural Language Processing. 2024b. 2024cBridging local details and global context in text-attributed graphs</p>
<p>Execution-based evaluation for open-domain code generation. Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>G³r: A graph-guided generate-and-rerank framework for complex and cross-domain text-to-sql generation. Yanzheng Xiang, Qian-Wen Zhang, Xu Zhang, Zejie Liu, Yunbo Cao, Deyu Zhou, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He ; Zhangchen, Fengqing Xu, Luyao Jiang, Jinyuan Niu, Jia, ArXiv, abs/2402.089832024. 2024Addressing order sensitivity of incontext demonstration examples in causal language models</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Nicolaus Foerster, Jeff Clune, David Ha, ArXiv, abs/2504.080662025</p>
<p>Simple but effective compound geometric operations for temporal knowledge graph completion. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao ; Rui Ying, Mengting Hu, Jianfeng Wu, Yalan Xie, Xiaoyi Liu, Zhunheng Wang, Ming Jiang, Hang Gao, Linlin Zhang, Renhong Cheng, ArXiv, abs/2210.03629Annual Meeting of the Association for Computational Linguistics. 2022. 2024React: Synergizing reasoning and acting in language models</p>
<p>Breaking the ceiling of the llm community by treating token generation as a classification for ensembling. Yao-Ching Yu, Chun-Chih Kuo, Ziqi Ye, Yu-Cheng Chang, Yueh-Se Li, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Neuron-level knowledge attribution in large language models. Zeping Yu, Sophia Ananiadou, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Can we automate scientific reviewing?. Weizhe Yuan, Pengfei Liu, Graham Neubig, 10.1613/jair.1.12862J. Artif. Int. Res. 1076-975775December 2022</p>
<p>Gliner: Generalist model for named entity recognition using bidirectional transformer. Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois, ArXiv, abs/2311.085262023</p>
<p>End-to-end beam retrieval for multi-hop question answering. Jiahao Zhang, H Zhang, Dongmei Zhang, Yong Liu, Sheng Huang, North American Chapter. the Association for Computational Linguistics2023a</p>
<p>Toolcoder: Teach code generation models to use api search tools. Kechi Zhang, Ge Li, Jia Li, Zhuo Li, Zhi Jin, ArXiv, abs/2305.040322023b</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, Annual Meeting of the Association for Computational Linguistics. 2024a</p>
<p>I2r: Intra and intermodal representation learning for code search. Xu Zhang, Yanzheng Xiang, Zejie Liu, Xiaoyu Hu, Deyu Zhou, Intell. Data Anal. 282023c</p>
<p>Hfd: Hierarchical feature decoupling for sql generation from text. Xu Zhang, Xiaoyu Hu, Zejie Liu, Yanzheng Xiang, Deyu Zhou, Intell. Data Anal. 282024b</p>
<p>Secon: Maintaining semantic consistency in data augmentation for code search. Xu Zhang, Zexu Lin, Xiaoyu Hu, Jianlei Wang, Wenpeng Lu, Deyu Zhou, ACM Transactions on Information Systems. 2024c</p>
<p>Ratescore: A metric for radiology report generation. W Zhao, C Wu, X Zhang, Y Zhang, Y Wang, W Xie, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>The mystery of in-context learning: A comprehensive survey on interpretation and analysis. Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>When is Tree Search Useful for LLM Planning?. Chen , 2024d2024It Depends on the Discriminator</p>
<p>Functional Overlap Reranking for Neural Code Generation. To, Findings of ACL 2024. 2023</p>
<p>Beyond Single-Event Extraction: Towards Efficient Document-Level Multi-Event Argument Extraction. Liu, Findings of ACL 2024. 2024b</p>
<p>Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning. Wang , 2024a2025</p>
<p>Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concepts at Different Layers?. Jin , 20242025</p>
<p>TRANSMI: A Framework to Create Strong Baselines from Multilingual Pretrained Language Models for Transliterated Data. Liu, 2024c2025</p>
<p>Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment. 2024Peng &amp; Zhang2025</p>
<p>Document-level Claim Extraction and Decontextualisation for Fact-Checking. Deng, 20242024</p>
<p>IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons. Shi, 20242024</p>
<p>RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models. Chen , 2024b2024</p>
<p>Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization. Song, 20242024</p>
<p>Sarthi, ICLR 2024RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. 2024</p>
<p>Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models. Findings of ACL 2024. Mastromattei &amp; Zanzotto2024</p>
<p>Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation. Arias, Findings of EMNLP 2024. 2024</p>
<p>Efficient Fact-Checking of LLMs on Grounding Documents. ; Minicheck, Tang, 2024a2024</p>
<p>Nearest Neighbor Normalization Improves Multimodal Retrieval. Chowdhury, 20242024</p>
<p>Neuron-Level Knowledge Attribution in Large Language Models. Yu &amp; Ananiadou20232024</p>
<p>Zhao, Metric for Radiology Report Generation. 20242024</p>
<p>Ostmeier, GREEN: Generative Radiology Report Evaluation and Error Notation. 2024Findings of EMNLP 2024</p>
<p>Lai, Specific Neurons for Steering LLMs in Text Style Transfer. 20242024</p>
<p>Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models. Tang, 2024b2024</p>
<p>Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths. Chia , Findings of EMNLP 2024. 2024</p>
<p>Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning. Chen , 2024a2024</p>
<p>Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization. ; Neuromax, Pham, Findings of EMNLP 2024. 2024</p>
<p>Bridging Local Details and Global Context in Text-Attributed Graphs. Wang , 2024c2024</p>
<p>Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models. Liu, 2024a2024</p>
<p>SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding. Xu, 20242024</p>
<p>MaskLID: Code-Switching Language Identification through Iterative Masking. Kargaran, 20242024</p>
<p>Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning. Kim, 20242024</p>
<p>Learning to Maximize Mutual Information for Chain-of-Thought Distillation. Chen , Findings of ACL 2024. 2024c</p>
<p>Zaratiana Gliner, Generalist Model for Named Entity Recognition using Bidirectional Transformer. 20232024</p>
<p>End-to-End Beam Retrieval for Multi-Hop Question Answering. Zhang, 2023a2024</p>
<p>Table A1: List of papers used in our Sci-Replicate benchmark. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3090 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3090</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3090</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264590280</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18659v2.pdf" target="_blank">DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3090.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3090.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetermLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DetermLR (Determinate Logical Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-framework that formulates LLM reasoning as an iterative evolution from indeterminacy to determinacy by (1) identifying determinate vs indeterminate premises, (2) quantitatively prioritizing and exploring premise combinations, and (3) storing/extracting reasoning memory to guide future steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo (as used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic multi-step prompting framework implemented on top of black-box LLMs (GPT-4 and GPT-3.5-turbo) via guided prompt templates; uses LLM calls to implement Identify, relevance/supplement scoring, explore, verify, and a reasoning memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['indeterminacy-to-determinacy formulation', 'premise identification (determinate vs indeterminate)', 'premise prioritization (two-stage scoring: relevance & supplement)', 'premise exploration with three-fold verification (logical validity, usefulness, duplication avoidance)', 'iterative reasoning memorization (store positive/negative reasoning paths)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The paper implements these methods by prompting the LLM to: classify premises as determinate/indeterminate, score relevance to the target and semantic/logical supplementarity to pick a primary premise and candidate supplements, combine them to propose new determinate propositions, verify proposed propositions across three checks, add valid new propositions to the determinate set, and store both successful and failed reasoning paths in a memory used in later iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse set of reasoning actions/styles driven by prompting — not a single stylistic CoT: identification, prioritized exploration, verification, and memory form a heterogeneous pipeline rather than one uniform reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA, ProofWriter, FOLIO, PrOntoQA, LogicalDeduction (LD)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks for logical/deductive reasoning: LogiQA (multi-type exam questions), ProofWriter and PrOntoQA (deductive/proof generation), FOLIO (first-order logic with annotations), LogicalDeduction (ordering/deduction tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>As reported in the paper (selected highlights using GPT-4): LogiQA accuracy 54.19% (avg steps 11.74), ProofWriter 79.17% (avg steps 14.63), FOLIO 75.49% (avg steps 8.57), LogicalDeduction 85.00% (avg steps 12.35). With GPT-3.5-turbo the paper reports consistent improvements over baseline prompting (example: LogiQA ~37.99% with DetermLR). The method also yields fewer reasoning iterations than multi-step baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper compares DetermLR to standard prompting, Chain-of-Thought (CoT), CoT with self-consistency (CoT-SC), Selection-Inference (SI), LAMBADA, Tree-of-Thoughts (ToT), and Cumulative Reasoning (CR). DetermLR outperforms these baselines on accuracy across the evaluated datasets while typically requiring fewer reasoning steps; ablations show that removing identify/prioritization/memory modules reduces accuracy or increases steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prioritizing premises and retaining reasoning history (memory) substantially improves both accuracy and efficiency; formulating reasoning as indeterminacy→determinacy yields more useful intermediate propositions than random/cumulative combination strategies (CR) or left-to-right CoT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Remaining failure modes include: (1) inability to reliably extract implicit/unstated conditions from context, (2) trouble with boundary-condition reasoning in some cases, and (3) limited flexible application of logical rules (e.g., certain contraposition/coverage inferences). Ablation shows that removing the memory module causes a substantial drop in accuracy (e.g., ≥7.84 percentage points on some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3090.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large proprietary transformer-based language model used in the paper as the primary high-capability base model for all prompting and multi-step reasoning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as 'the most advanced GPT-4' used as a base LLM to implement DetermLR and baselines via prompt templates and the guidance library; treated as a black-box generator/verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT)', 'CoT with Self-Consistency (CoT-SC)', 'Tree-of-Thoughts (ToT)', 'Cumulative Reasoning (CR)', 'Selection-Inference (SI)', 'LAMBADA (backward chaining)', 'DetermLR pipeline']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>In experiments GPT-4 is the underlying LLM that is prompted to produce CoTs, generate and verify propositions, search trees (ToT), and to implement the DetermLR pipeline's Identify/Relevance/Supplement/Explore/Verify functions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Used with a diverse set of prompted reasoning styles in experiments; the paper contrasts multiple styles by running each method on GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA, ProofWriter, FOLIO, PrOntoQA, LogicalDeduction</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same benchmarks as above; GPT-4 is evaluated under each prompting/method condition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports that GPT-4 with DetermLR outperforms GPT-4 with other prompting strategies. Example: LogiQA: DetermLR 54.19% vs CR 45.25% vs ToT 43.02% vs CoT-SC ~39–41% (values quoted from the paper's tables). Similar gaps reported on ProofWriter, FOLIO, and LD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct multi-method comparisons were run on the same GPT-4 base: DetermLR yields higher accuracy and fewer average steps than CoT variants, ToT and CR. The paper presents ablations and per-dataset breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a high-capability LLM (GPT-4) with the DetermLR pipeline yields state-of-the-art accuracy on the selected logical reasoning benchmarks and improves efficiency (fewer reasoning steps) relative to other multi-step prompting methods on the same model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even GPT-4 fails on some implicit-condition and boundary reasoning examples; baseline multi-step methods sometimes produce plausible-but-wrong intermediate propositions (``correct nonsense'') that DetermLR's verification and prioritization mitigate but do not eliminate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3090.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong but less-capable OpenAI LLM used as a lower-capacity baseline to evaluate DetermLR and other prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as a comparative base model; experiments show DetermLR improves GPT-3.5-turbo performance similarly to GPT-4 but at a lower absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT)', 'CoT-SC', 'ToT', 'CR', 'DetermLR']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-3.5-turbo was prompted with the same method templates as GPT-4 to form CoTs, multi-chain self-consistency ensembles, and to implement DetermLR's pipeline steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Tested with multiple reasoning prompting styles; supports the paper's claim that DetermLR's ideas transfer across model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA, ProofWriter, FOLIO, PrOntoQA, LD</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same benchmarks; used to probe lower-capacity behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported improvements from standard baselines to DetermLR on GPT-3.5-turbo (e.g., DetermLR LogiQA~37.99% vs baselines in the low-30s). The paper notes an approximate +0.1 accuracy improvement of multi-step methods over GPT-3.5 baseline and further gains from DetermLR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Relative ordering preserved: DetermLR > CR/ToT/CoT variants on GPT-3.5, but all scores lower than GPT-4 counterparts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DetermLR improves both stronger (GPT-4) and weaker (GPT-3.5) LLMs, indicating the method's reliance on prompting/strategy rather than unique model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Absolute performance still limited by model capacity; some complex tasks (LogiQA) remain far below human performance even with DetermLR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3090.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A left-to-right step-by-step prompting technique that elicits intermediate reasoning chains from LLMs to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo (as used in baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting style that instructs the model to produce explanatory intermediate steps before the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['left-to-right chain-of-thought stepwise generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT elicits sequential natural-language internal reasoning traces (rationales) which are then used to produce final answers; implemented via appropriate prompts and sometimes aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single stylistic method (sequential chain); paper treats it as a baseline representing a similar-style single-method approach.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA, ProofWriter, FOLIO, PrOntoQA, LD (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See above benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Improves over standard prompting but underperforms DetermLR on logical reasoning benchmarks; example table entries show CoT/CoT-SC accuracies lower than DetermLR (paper's tables list CoT/CoT-SC numbers per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT and CoT-SC provide gains vs standard prompting; self-consistency (CoT-SC) further improves stability but still trails DetermLR in accuracy and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT-style sequential chains are helpful but less effective than a prioritized, verified exploration pipeline (DetermLR) for complex logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT can produce plausible but incorrect intermediate steps; aggregating multiple CoTs (self-consistency) helps but does not match DetermLR's targeted premise prioritization and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3090.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble variant of CoT that samples multiple chains and chooses the majority final answer (self-consistency) to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT enhanced by sampling n chains (paper used n=16 for CoT-SC) and aggregating answers to mitigate single-run noise.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['multi-sample chain-of-thought + majority-vote aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate multiple CoT rationales, then perform majority voting on final answers (used in experiments with temperature adjustments).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Not diverse in style (still CoT family) but uses sampling diversity and aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same logical reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported to improve over single CoT but still under DetermLR (paper tables show CoT-SC below DetermLR on the presented datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT-SC increases robustness vs single CoT; DetermLR however attains higher accuracy with fewer average steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling and voting reduces variance but does not replace directed premise prioritization and memory retention for logical deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT-SC requires many sampled chains (higher compute) and still may fail on tasks needing targeted premise combination or memory of prior failed inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3090.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-search style prompting method that models reasoning as exploration of multiple branching 'thoughts' (nodes) to find a plan yielding a correct solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Search-based approach where LLM is used to expand nodes (thoughts) and a search policy chooses promising branches; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['tree-structured deliberative search over multiple thought branches']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Constructs a tree of partial reasoning states, expands with LLM generations, and explores promising nodes to reach a solution.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses a different style (search-tree) compared to sequential CoT; nevertheless the paper treats it as another multi-step baseline rather than combining styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Benchmarks above (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ToT improves multi-step exploration on some tasks but in the paper is reported to underperform DetermLR in accuracy and/or efficiency on the selected logical reasoning tasks (example: ToT LogiQA accuracy lower than DetermLR).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>ToT offers broader search but without DetermLR's premise prioritization and memory, it required more steps and achieved lower accuracy in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured search (ToT) helps exploration but can be less efficient than targeted premise-prioritization plus memory.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>ToT's preset search heuristics can be less adaptive to diverse task demands; on exam-like LogiQA problems it underperformed DetermLR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3090.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cumulative Reasoning (CR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent method that iteratively generates new propositions from given premises by combining available information, but without an explicit prioritization criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Iterative proposition-generation approach that accumulates inferred facts but selects premise combinations more randomly (no explicit priority scoring); used as a strong baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['iterative proposition generation from available premises (cumulative accumulation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At each round CR combines existing premises to propose new statements and accumulates them; lacks DetermLR's quantitative relevance/supplement scoring and three-fold verification.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single style (iterative cumulative combination) — similar-style baseline compared against DetermLR's more structured pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same logical reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to evaluate capacity to discover new propositions across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CR yields improvements over simple prompting in some tasks but underperforms DetermLR; e.g., on LogiQA with GPT-4 CR reported around mid-40s% accuracy versus DetermLR's 54.19%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper emphasizes that CR's random combination of premises without a well-defined priority leads to less efficient and less accurate exploration compared to DetermLR.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prioritization and verification (DetermLR) produce more useful and fewer redundant propositions than CR's accumulation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CR can generate many redundant or non-useful propositions and requires more steps; on complex exam-style problems (LogiQA) it gives notably lower accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3090.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-Inference (SI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decomposition approach that separates premise selection from inference generation to make reasoning more interpretable and focused.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implemented as a baseline: first select relevant premises, then generate an inference; used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['two-stage select then infer decomposition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Selection step filters premises, inference step uses the chosen premises to derive conclusions; in the paper SI is one baseline among others.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>A distinct two-stage style; treated as a single-method baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Benchmarks mentioned</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to check selection-driven reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>SI improves over vanilla CoT in some cases but is shown to be inferior to DetermLR in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>SI's explicit selection step is related to DetermLR's prioritization, but DetermLR adds additional supplement-scoring, verification, and memory which improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Selection + inference is beneficial, but DetermLR's quantitative scoring and memory yield additional gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative results</strong></td>
                            <td>Noted that SI alone lacks the iterative verification/memory components of DetermLR and therefore underperforms on complex logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3090.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3090.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAMBADA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LAMBADA (Backward chaining for automated reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A backward-chaining style automated reasoning baseline included in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Backward chaining method that reasons from the goal backward to premises; used as a baseline (Kazemi et al. referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['backward chaining / goal-driven backward reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Starts from target and selects premises that could support it, chaining backward to ground facts; compared against DetermLR's forward prioritized exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>A different, goal-driven style; contrasted with DetermLR in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Logical reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as a baseline to assess backward-chaining efficacy vs DetermLR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>LAMBADA gives some improvements over naive prompting but is reported to be outperformed by DetermLR in accuracy/efficiency on the tasks studied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Backward chaining is complementary in style but DetermLR's combination of identification/prioritization/verification/memory produced higher end-task accuracy in their evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Goal-driven strategies are useful but need prioritization and memory to match DetermLR's performance on these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Backward chaining alone did not match DetermLR's gains on exam-style multi-premise problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of Thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Cumulative reasoning with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Selection-Inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 2)</em></li>
                <li>LAMBADA: Backward chaining for automated reasoning in natural language <em>(Rating: 2)</em></li>
                <li>Graph of Thoughts: Solving elaborate problems with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3090",
    "paper_id": "paper-264590280",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "DetermLR",
            "name_full": "DetermLR (Determinate Logical Reasoning)",
            "brief_description": "A prompting-framework that formulates LLM reasoning as an iterative evolution from indeterminacy to determinacy by (1) identifying determinate vs indeterminate premises, (2) quantitatively prioritizing and exploring premise combinations, and (3) storing/extracting reasoning memory to guide future steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo (as used in paper)",
            "model_description": "Deterministic multi-step prompting framework implemented on top of black-box LLMs (GPT-4 and GPT-3.5-turbo) via guided prompt templates; uses LLM calls to implement Identify, relevance/supplement scoring, explore, verify, and a reasoning memory module.",
            "model_size": null,
            "reasoning_methods": [
                "indeterminacy-to-determinacy formulation",
                "premise identification (determinate vs indeterminate)",
                "premise prioritization (two-stage scoring: relevance & supplement)",
                "premise exploration with three-fold verification (logical validity, usefulness, duplication avoidance)",
                "iterative reasoning memorization (store positive/negative reasoning paths)"
            ],
            "reasoning_methods_description": "The paper implements these methods by prompting the LLM to: classify premises as determinate/indeterminate, score relevance to the target and semantic/logical supplementarity to pick a primary premise and candidate supplements, combine them to propose new determinate propositions, verify proposed propositions across three checks, add valid new propositions to the determinate set, and store both successful and failed reasoning paths in a memory used in later iterations.",
            "diversity_of_methods": "Diverse set of reasoning actions/styles driven by prompting — not a single stylistic CoT: identification, prioritized exploration, verification, and memory form a heterogeneous pipeline rather than one uniform reasoning style.",
            "reasoning_task_name": "LogiQA, ProofWriter, FOLIO, PrOntoQA, LogicalDeduction (LD)",
            "reasoning_task_description": "Benchmarks for logical/deductive reasoning: LogiQA (multi-type exam questions), ProofWriter and PrOntoQA (deductive/proof generation), FOLIO (first-order logic with annotations), LogicalDeduction (ordering/deduction tasks).",
            "performance_by_method": "As reported in the paper (selected highlights using GPT-4): LogiQA accuracy 54.19% (avg steps 11.74), ProofWriter 79.17% (avg steps 14.63), FOLIO 75.49% (avg steps 8.57), LogicalDeduction 85.00% (avg steps 12.35). With GPT-3.5-turbo the paper reports consistent improvements over baseline prompting (example: LogiQA ~37.99% with DetermLR). The method also yields fewer reasoning iterations than multi-step baselines.",
            "comparison_of_methods": "The paper compares DetermLR to standard prompting, Chain-of-Thought (CoT), CoT with self-consistency (CoT-SC), Selection-Inference (SI), LAMBADA, Tree-of-Thoughts (ToT), and Cumulative Reasoning (CR). DetermLR outperforms these baselines on accuracy across the evaluated datasets while typically requiring fewer reasoning steps; ablations show that removing identify/prioritization/memory modules reduces accuracy or increases steps.",
            "key_findings": "Prioritizing premises and retaining reasoning history (memory) substantially improves both accuracy and efficiency; formulating reasoning as indeterminacy→determinacy yields more useful intermediate propositions than random/cumulative combination strategies (CR) or left-to-right CoT variants.",
            "counter_examples_or_negative_results": "Remaining failure modes include: (1) inability to reliably extract implicit/unstated conditions from context, (2) trouble with boundary-condition reasoning in some cases, and (3) limited flexible application of logical rules (e.g., certain contraposition/coverage inferences). Ablation shows that removing the memory module causes a substantial drop in accuracy (e.g., ≥7.84 percentage points on some tasks).",
            "uuid": "e3090.0",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A large proprietary transformer-based language model used in the paper as the primary high-capability base model for all prompting and multi-step reasoning experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Described in the paper as 'the most advanced GPT-4' used as a base LLM to implement DetermLR and baselines via prompt templates and the guidance library; treated as a black-box generator/verifier.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (CoT)",
                "CoT with Self-Consistency (CoT-SC)",
                "Tree-of-Thoughts (ToT)",
                "Cumulative Reasoning (CR)",
                "Selection-Inference (SI)",
                "LAMBADA (backward chaining)",
                "DetermLR pipeline"
            ],
            "reasoning_methods_description": "In experiments GPT-4 is the underlying LLM that is prompted to produce CoTs, generate and verify propositions, search trees (ToT), and to implement the DetermLR pipeline's Identify/Relevance/Supplement/Explore/Verify functions.",
            "diversity_of_methods": "Used with a diverse set of prompted reasoning styles in experiments; the paper contrasts multiple styles by running each method on GPT-4.",
            "reasoning_task_name": "LogiQA, ProofWriter, FOLIO, PrOntoQA, LogicalDeduction",
            "reasoning_task_description": "Same benchmarks as above; GPT-4 is evaluated under each prompting/method condition.",
            "performance_by_method": "Paper reports that GPT-4 with DetermLR outperforms GPT-4 with other prompting strategies. Example: LogiQA: DetermLR 54.19% vs CR 45.25% vs ToT 43.02% vs CoT-SC ~39–41% (values quoted from the paper's tables). Similar gaps reported on ProofWriter, FOLIO, and LD.",
            "comparison_of_methods": "Direct multi-method comparisons were run on the same GPT-4 base: DetermLR yields higher accuracy and fewer average steps than CoT variants, ToT and CR. The paper presents ablations and per-dataset breakdowns.",
            "key_findings": "Using a high-capability LLM (GPT-4) with the DetermLR pipeline yields state-of-the-art accuracy on the selected logical reasoning benchmarks and improves efficiency (fewer reasoning steps) relative to other multi-step prompting methods on the same model.",
            "counter_examples_or_negative_results": "Even GPT-4 fails on some implicit-condition and boundary reasoning examples; baseline multi-step methods sometimes produce plausible-but-wrong intermediate propositions (``correct nonsense'') that DetermLR's verification and prioritization mitigate but do not eliminate.",
            "uuid": "e3090.1",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A strong but less-capable OpenAI LLM used as a lower-capacity baseline to evaluate DetermLR and other prompting methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Used as a comparative base model; experiments show DetermLR improves GPT-3.5-turbo performance similarly to GPT-4 but at a lower absolute accuracy.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (CoT)",
                "CoT-SC",
                "ToT",
                "CR",
                "DetermLR"
            ],
            "reasoning_methods_description": "GPT-3.5-turbo was prompted with the same method templates as GPT-4 to form CoTs, multi-chain self-consistency ensembles, and to implement DetermLR's pipeline steps.",
            "diversity_of_methods": "Tested with multiple reasoning prompting styles; supports the paper's claim that DetermLR's ideas transfer across model capabilities.",
            "reasoning_task_name": "LogiQA, ProofWriter, FOLIO, PrOntoQA, LD",
            "reasoning_task_description": "Same benchmarks; used to probe lower-capacity behavior.",
            "performance_by_method": "Reported improvements from standard baselines to DetermLR on GPT-3.5-turbo (e.g., DetermLR LogiQA~37.99% vs baselines in the low-30s). The paper notes an approximate +0.1 accuracy improvement of multi-step methods over GPT-3.5 baseline and further gains from DetermLR.",
            "comparison_of_methods": "Relative ordering preserved: DetermLR &gt; CR/ToT/CoT variants on GPT-3.5, but all scores lower than GPT-4 counterparts.",
            "key_findings": "DetermLR improves both stronger (GPT-4) and weaker (GPT-3.5) LLMs, indicating the method's reliance on prompting/strategy rather than unique model architecture.",
            "counter_examples_or_negative_results": "Absolute performance still limited by model capacity; some complex tasks (LogiQA) remain far below human performance even with DetermLR.",
            "uuid": "e3090.2",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "A left-to-right step-by-step prompting technique that elicits intermediate reasoning chains from LLMs to improve multi-step problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo (as used in baselines)",
            "model_description": "Prompting style that instructs the model to produce explanatory intermediate steps before the final answer.",
            "model_size": null,
            "reasoning_methods": [
                "left-to-right chain-of-thought stepwise generation"
            ],
            "reasoning_methods_description": "CoT elicits sequential natural-language internal reasoning traces (rationales) which are then used to produce final answers; implemented via appropriate prompts and sometimes aggregated.",
            "diversity_of_methods": "Single stylistic method (sequential chain); paper treats it as a baseline representing a similar-style single-method approach.",
            "reasoning_task_name": "LogiQA, ProofWriter, FOLIO, PrOntoQA, LD (used as baseline)",
            "reasoning_task_description": "See above benchmarks.",
            "performance_by_method": "Improves over standard prompting but underperforms DetermLR on logical reasoning benchmarks; example table entries show CoT/CoT-SC accuracies lower than DetermLR (paper's tables list CoT/CoT-SC numbers per dataset).",
            "comparison_of_methods": "CoT and CoT-SC provide gains vs standard prompting; self-consistency (CoT-SC) further improves stability but still trails DetermLR in accuracy and efficiency.",
            "key_findings": "CoT-style sequential chains are helpful but less effective than a prioritized, verified exploration pipeline (DetermLR) for complex logical tasks.",
            "counter_examples_or_negative_results": "CoT can produce plausible but incorrect intermediate steps; aggregating multiple CoTs (self-consistency) helps but does not match DetermLR's targeted premise prioritization and memory.",
            "uuid": "e3090.3",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency (CoT-SC)",
            "brief_description": "An ensemble variant of CoT that samples multiple chains and chooses the majority final answer (self-consistency) to improve robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo",
            "model_description": "CoT enhanced by sampling n chains (paper used n=16 for CoT-SC) and aggregating answers to mitigate single-run noise.",
            "model_size": null,
            "reasoning_methods": [
                "multi-sample chain-of-thought + majority-vote aggregation"
            ],
            "reasoning_methods_description": "Generate multiple CoT rationales, then perform majority voting on final answers (used in experiments with temperature adjustments).",
            "diversity_of_methods": "Not diverse in style (still CoT family) but uses sampling diversity and aggregation.",
            "reasoning_task_name": "Same logical reasoning benchmarks",
            "reasoning_task_description": "Same as above.",
            "performance_by_method": "Reported to improve over single CoT but still under DetermLR (paper tables show CoT-SC below DetermLR on the presented datasets).",
            "comparison_of_methods": "CoT-SC increases robustness vs single CoT; DetermLR however attains higher accuracy with fewer average steps.",
            "key_findings": "Sampling and voting reduces variance but does not replace directed premise prioritization and memory retention for logical deductions.",
            "counter_examples_or_negative_results": "CoT-SC requires many sampled chains (higher compute) and still may fail on tasks needing targeted premise combination or memory of prior failed inferences.",
            "uuid": "e3090.4",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts (ToT)",
            "brief_description": "A tree-search style prompting method that models reasoning as exploration of multiple branching 'thoughts' (nodes) to find a plan yielding a correct solution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo",
            "model_description": "Search-based approach where LLM is used to expand nodes (thoughts) and a search policy chooses promising branches; used as a baseline.",
            "model_size": null,
            "reasoning_methods": [
                "tree-structured deliberative search over multiple thought branches"
            ],
            "reasoning_methods_description": "Constructs a tree of partial reasoning states, expands with LLM generations, and explores promising nodes to reach a solution.",
            "diversity_of_methods": "Uses a different style (search-tree) compared to sequential CoT; nevertheless the paper treats it as another multi-step baseline rather than combining styles.",
            "reasoning_task_name": "Benchmarks above (used as baseline)",
            "reasoning_task_description": "As above.",
            "performance_by_method": "ToT improves multi-step exploration on some tasks but in the paper is reported to underperform DetermLR in accuracy and/or efficiency on the selected logical reasoning tasks (example: ToT LogiQA accuracy lower than DetermLR).",
            "comparison_of_methods": "ToT offers broader search but without DetermLR's premise prioritization and memory, it required more steps and achieved lower accuracy in these experiments.",
            "key_findings": "Structured search (ToT) helps exploration but can be less efficient than targeted premise-prioritization plus memory.",
            "counter_examples_or_negative_results": "ToT's preset search heuristics can be less adaptive to diverse task demands; on exam-like LogiQA problems it underperformed DetermLR.",
            "uuid": "e3090.5",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CR",
            "name_full": "Cumulative Reasoning (CR)",
            "brief_description": "A recent method that iteratively generates new propositions from given premises by combining available information, but without an explicit prioritization criterion.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo",
            "model_description": "Iterative proposition-generation approach that accumulates inferred facts but selects premise combinations more randomly (no explicit priority scoring); used as a strong baseline in the paper.",
            "model_size": null,
            "reasoning_methods": [
                "iterative proposition generation from available premises (cumulative accumulation)"
            ],
            "reasoning_methods_description": "At each round CR combines existing premises to propose new statements and accumulates them; lacks DetermLR's quantitative relevance/supplement scoring and three-fold verification.",
            "diversity_of_methods": "Single style (iterative cumulative combination) — similar-style baseline compared against DetermLR's more structured pipeline.",
            "reasoning_task_name": "Same logical reasoning benchmarks",
            "reasoning_task_description": "Used to evaluate capacity to discover new propositions across iterations.",
            "performance_by_method": "CR yields improvements over simple prompting in some tasks but underperforms DetermLR; e.g., on LogiQA with GPT-4 CR reported around mid-40s% accuracy versus DetermLR's 54.19%.",
            "comparison_of_methods": "Paper emphasizes that CR's random combination of premises without a well-defined priority leads to less efficient and less accurate exploration compared to DetermLR.",
            "key_findings": "Prioritization and verification (DetermLR) produce more useful and fewer redundant propositions than CR's accumulation strategy.",
            "counter_examples_or_negative_results": "CR can generate many redundant or non-useful propositions and requires more steps; on complex exam-style problems (LogiQA) it gives notably lower accuracies.",
            "uuid": "e3090.6",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SI",
            "name_full": "Selection-Inference (SI)",
            "brief_description": "A decomposition approach that separates premise selection from inference generation to make reasoning more interpretable and focused.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo",
            "model_description": "Implemented as a baseline: first select relevant premises, then generate an inference; used for comparison.",
            "model_size": null,
            "reasoning_methods": [
                "two-stage select then infer decomposition"
            ],
            "reasoning_methods_description": "Selection step filters premises, inference step uses the chosen premises to derive conclusions; in the paper SI is one baseline among others.",
            "diversity_of_methods": "A distinct two-stage style; treated as a single-method baseline.",
            "reasoning_task_name": "Benchmarks mentioned",
            "reasoning_task_description": "Used to check selection-driven reasoning performance.",
            "performance_by_method": "SI improves over vanilla CoT in some cases but is shown to be inferior to DetermLR in the reported experiments.",
            "comparison_of_methods": "SI's explicit selection step is related to DetermLR's prioritization, but DetermLR adds additional supplement-scoring, verification, and memory which improve results.",
            "key_findings": "Selection + inference is beneficial, but DetermLR's quantitative scoring and memory yield additional gains.",
            "counter_examples_or_negative results": "Noted that SI alone lacks the iterative verification/memory components of DetermLR and therefore underperforms on complex logical tasks.",
            "uuid": "e3090.7",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LAMBADA",
            "name_full": "LAMBADA (Backward chaining for automated reasoning)",
            "brief_description": "A backward-chaining style automated reasoning baseline included in the paper's comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-turbo",
            "model_description": "Backward chaining method that reasons from the goal backward to premises; used as a baseline (Kazemi et al. referenced).",
            "model_size": null,
            "reasoning_methods": [
                "backward chaining / goal-driven backward reasoning"
            ],
            "reasoning_methods_description": "Starts from target and selects premises that could support it, chaining backward to ground facts; compared against DetermLR's forward prioritized exploration.",
            "diversity_of_methods": "A different, goal-driven style; contrasted with DetermLR in experiments.",
            "reasoning_task_name": "Logical reasoning benchmarks",
            "reasoning_task_description": "Used as a baseline to assess backward-chaining efficacy vs DetermLR.",
            "performance_by_method": "LAMBADA gives some improvements over naive prompting but is reported to be outperformed by DetermLR in accuracy/efficiency on the tasks studied.",
            "comparison_of_methods": "Backward chaining is complementary in style but DetermLR's combination of identification/prioritization/verification/memory produced higher end-task accuracy in their evaluations.",
            "key_findings": "Goal-driven strategies are useful but need prioritization and memory to match DetermLR's performance on these benchmarks.",
            "counter_examples_or_negative_results": "Backward chaining alone did not match DetermLR's gains on exam-style multi-premise problems.",
            "uuid": "e3090.8",
            "source_info": {
                "paper_title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Cumulative reasoning with large language models",
            "rating": 2,
            "sanitized_title": "cumulative_reasoning_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Selection-Inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 2,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        },
        {
            "paper_title": "LAMBADA: Backward chaining for automated reasoning in natural language",
            "rating": 2,
            "sanitized_title": "lambada_backward_chaining_for_automated_reasoning_in_natural_language"
        },
        {
            "paper_title": "Graph of Thoughts: Solving elaborate problems with large language models",
            "rating": 1,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        }
    ],
    "cost": 0.0193845,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy
26 May 2024</p>
<p>Hongda Sun 
Weikai Xu 
Wei Liu liuwei40@xiaomi.com 
XiaoMi AI Lab</p>
<p>Jian Luan luanjian@xiaomi.com 
XiaoMi AI Lab</p>
<p>Bin Wang wangbin11@xiaomi.com 
XiaoMi AI Lab</p>
<p>Shuo Shang 
University of Electronic Science and Technology of China</p>
<p>Ji-Rong Wen 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Rui Yan ruiyan@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy
26 May 2024047EAE74AEA42009BDAB822E5D413675arXiv:2310.18659v2[cs.AI]
Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks.To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs.However, LLM-based reasoning still encounters the following challenges:(1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps.To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy.First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights.Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights.Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps.Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOn-toQA, and LogicalDeduction.Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.</p>
<p>Introduction</p>
<p>The emergence of large language models (LLMs) has instigated a transformative wave within the realm of artificial intelligence (Zhao et al., 2023).The series models of GPT (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023) and PaLM (Chowdhery et al., 2022;Anil et al., 2023) have exhibited remarkable proficiency in natural language reasoning, contributing to the advancement of research and applications of cognitive intelligence (Huang and Chang, 2022).However, even the current state-of-the-art (SOTA) LLMs still grapple with a key limitation: the lack of humanlike advanced reasoning skills to rationally analyze known conditions and draw conclusions (Arkoudas, 2023;Singh et al., 2023).This leaves a substantial gap between LLM-based reasoning and the cognitive process of human reasoning.</p>
<p>To alleviate this limitation, existing studies employ enhanced prompt engineering techniques to guide LLMs in eliciting intermediate thinking steps to ensure reliable conclusions (Zhou et al., 2022;Khot et al., 2022;Wei et al., 2022;Kojima et al., 2022).Building upon this foundation, recent works have focused on introducing more intricate reasoning structures, such as multiple chains (Wang et al., 2022b), trees (Yao et al., 2023) or graphs (Lei et al., 2023;Besta et al., 2023), to tackle increasingly complex reasoning tasks.However, LLM-based reasoning continues to encounter three challenges:</p>
<p>(1) Limited adaptability of preset structures to diverse tasks: Since the task complexity cannot be solely inferred from the problem context, relying on a certain preset structure to solve a variety of reasoning problems may create deficiencies in reasoning effectiveness or efficiency (Yao et al., 2023;Lei et al., 2023).This approach contrasts with human problem-solving techniques, which are not dependent on preset reasoning structures.Ideally, the reasoning structure should be the result of manual review after solving the problem.(2) Insufficient precision in exploiting known conditions to derive new ones: The literature on human cognitive reasoning provides valuable insights and emphasizes the importance of integrating available information for informed decision-making (Schaeken et al., 1999;Evans, 2002;Baron, 2023).This motivates cumulative reasoning (CR) (Zhang et al., 2023), which uses LLMs to iteratively generate new propositions based on available premises.However, CR still cannot approach the human thought process, as it relies on the random combination of existing premises without a well-defined criterion.(3) Inadequate consideration of historical reasoning experiences for future reasoning: Previous works (Wei et al., 2022;Yao et al., 2023) often overlook historical reasoning details, resulting in the lack of necessary information for subsequent phases.</p>
<p>To address these challenges and augment LLMs to grasp more human-like advanced reasoning skills, we need to consider three key factors: (1) Refine the formulation of the essence of the reasoning process; (2) Prioritize relevant premises for efficiently exploring new information; (3) Memorize historical reasoning details to guide the direction of the subsequent reasoning steps.</p>
<p>To this end, we propose DetermLR, a novel reasoning framework to align LLM-based reasoning more closely with human thinking.First, we formulate the logical reasoning process as an evolution from indeterminacy to determinacy.Since premises exhibit varying descriptions and associations with the target, we initiate the reasoning process with premise identification to finely categorize premises into two distinct types: determinate and indeterminate.Determinate premises are defined as simple statements, which can definitively contribute to conclusion derivation.Conversely, indeterminate premises may contain complex rules governing the relationships among multiple propositions.Regardless of the problem complexity, the reasoning process consistently involves the continuous accumulation of determinate information, steering the conclusion toward greater clarity.</p>
<p>Subsequently, human reasoning typically aims for a "breakingthrough" from known conditions to deduce new insights, indicating the necessity to distinguish the priority of premises.Therefore, we propose quantitative measurements to facilitate premise prioritization, which involves identifying the most relevant premise to the conclusion and screening supplementary premises likely to interact with this primary premise.This guides LLMs to exclude irrelevant premises and focus on more pertinent information for premise exploration.</p>
<p>Furthermore, we introduce a reasoning memorization module to automate the storage and extraction of available premises and reasoning paths.</p>
<p>In this way, historical reasoning details are preserved in the reasoning memory to update reasoning states, and they are incorporated into future reasoning steps to refer to inherent experiences and avoid repeating similar mistakes.</p>
<p>To verify the capability of LLMs to engage in rigorous logical reasoning, we conduct extensive experiments on various challenging logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, ProntoQA, and LogicalDeduction.The experimental results show that DetermLR achieves the best performance on reasoning accuracy, coupled with superior efficiency of requiring fewer steps than previous multi-step reasoning methods.Notably, in more intricate tasks like LogiQA, DetermLR exhibits even more pronounced advancements, mirroring human-like reasoning skills to a greater extent.</p>
<p>Our technical contributions to advancing LLMbased reasoning can be summarized as follows:</p>
<p>• We propose a novel framework that formulates the reasoning process as an evolution from indeterminacy to determinacy, aligning LLM-based reasoning more closely with human reasoning.</p>
<p>• We leverage quantitative measurements for premise prioritization and exploration, enabling LLMs to prioritize premises more conducive to exploring new insights and deriving conclusions.</p>
<p>• We introduce a reasoning memorization module to preserve essential historical reasoning details during the iterative reasoning process.</p>
<p>Related Work</p>
<p>Conventional Logical Reasoning</p>
<p>Many previous works focus on solving logical reasoning tasks using machine reading comprehension (Ouyang et al., 2021), adversarial pretraining (Pi et al., 2022), and contrastive learning (Jiao et al., 2022).In the realm of logical reasoning benchmarks, various tasks have been utilized for evaluation (Khot et al., 2018;Wang et al., 2022a;Bhagavatula et al., 2019;Welleck et al., 2018;Williams et al., 2017;Dagan et al., 2005;Bowman et al., 2015;Wang et al., 2018;Liu et al., 2021;Tian et al., 2021).LogiQA (Liu et al., 2020) involves diverse types of logical reasoning questions collected from the National Civil Servants Examination of China.Based on Big-Bench (Srivastava et al., 2022), which is used to evaluate multiaspect abilities of language models, Big-Bench Hard (BBH) (Suzgun et al., 2022) focuses on 23 challenging tasks for evaluating LLM-based reasoning.FOLIO (Han et al., 2022) is a humanannotated and logically complex datasets for natural language reasoning, equipped with first-order logic (FOL) annotations.ProofWriter (Tafjord et al., 2020) is another commonly used dataset for deductive logical reasoning.Among them, we carefully select five benchmarks whose premises are listed directly in the problem context and require no additional processing.</p>
<p>LLM-based Logical Reasoning</p>
<p>Previous methods mainly enhance reasoning by eliciting intermediate steps like chain-of-thought (CoT) (Wei et al., 2022;Wang et al., 2022b) and least-to-most prompting (Zhou et al., 2022).Extending the CoT concept, which follows a leftto-right progression, more recent works present more intricate thought structures to face more complex problems, such as ToT (Yao et al., 2023;Hu et al., 2023) or GoT (Lei et al., 2023).Selectioninference (Creswell et al., 2022a) refine the reasoning process of CoT by decomposing it into two modules: selection and inference.Algorithmof-Thoughts (Sel et al., 2023) navigate reasoning pathways as in-context examples with merely a few queries.Cumulative reasoning (Zhang et al., 2023) uses higher-order logic rules for exploring new propositions based on given premises.Current LLM-based reasoning methods still face challenges in emulating human-like reasoning skills.In response to these challenges, we propose a novel perspective for formulating the reasoning process.This approach prioritizes the exploration of new insights by leveraging more relevant premises and enables iterative reasoning based on key historical reasoning experiences.</p>
<p>DetermLR</p>
<p>Problem Formulation</p>
<p>The objective of a logical reasoning problem can be regarded as using known premises and logical deduction rules to derive new essential intermediate propositions, culminating in an eventual target conclusion.Suppose a problem provides a set of N premises, denoted as P = (p 1 , p 2 , • • • , p N ), the logical reasoning process can be formulated as:
c = Reason(p 1 , p 2 , • • • , p n ),(1)
where c is the target conclusion of the problem, and the mapping Reason indicates how to use the given premises to derive the conclusion.In this paper, our focus is on aligning LLM-based reasoning more closely with human reasoning.Therefore, the Reason is generally implemented by instructing LLMs to understand the problem and iteratively provide new insights to solve it.Building upon the available premises and the target as input, we propose a novel perspective to formulate the process of logical reasoning.In the following sections, we will introduce in detail three pivotal modules of the proposed method: (1) premise identification ( §3.2); (2) premise prioritization and exploration ( §3.3); and (3) iterative reasoning memorization ( §3.4).</p>
<p>Premise Identification</p>
<p>As previously discussed, the essence of the reasoning process lies in the mapping from premises to conclusions.Existing methods (Yao et al., 2023;Besta et al., 2023) that preset the reasoning structure before solving a problem may not always yield an appropriate mapping, as this diverges from the focus of human reasoning.Instead, the so-called reasoning structure should be formed based on the reviewed reasoning outcomes after problem resolution.Thus, refining a better formulation for the essence of the reasoning process becomes the first key factor in augmenting LLM-based reasoning.Regarding the given premises in a logical reasoning problem, it becomes apparent that the "determinacy" of the information supplied by each premise varies significantly: some directly provide pertinent information for deriving the conclusion, while others necessitate combination with other conditions to obtain new insights.The indeterminacy gradually diminishes with the accumulation of determinate information, bringing the reasoning process closer to the conclusion.Therefore, we rethink the essence of the reasoning process as from indeterminacy to determinacy.</p>
<p>To be more specific, we develop a premise identification module to emulate the transition from indeterminacy to determinacy.Input premises are categorized into two distinct types: determinate premises D and indeterminate premises I.The identification criterion is dependent upon both the inherent description of the premise and its connection with the input target c, as expressed by: D, I = Identify(P, c).</p>
<p>(
)2
In practice, we implement the Identify function through carefully designed instructions for LLMs,  and related prompt templates are available in Appendix.Determinate premises are defined as simple statements that definitively lead to the desired target.These premises state clarified facts and serve as the foundational blocks for reasoning.In contrast, indeterminate premises encompass propositions not directly related to the target and often contain complex structures reflecting indeterminacy, such as disjunction (x or y) and hypothesis (if x then y).An indeterminate premise may be combined with other premises to establish a logical path to evolve into a determinate state.</p>
<p>As shown in Figure 1(a), the target revolves around "Gary" and "round", so simple statements including "Gary" are identified as determinate premises (d 1 • • • d 3 ), while the remaining premises are classified as indeterminate ones (i 1 • • • i 7 ).Building upon this module, LLMs can eliminate the need for preset structures and enhance the clarity of the reasoning process under our new formulation.</p>
<p>Premise Prioritization and Exploration</p>
<p>Once the original premises are categorized, how to better uncover the relationships between these premises to explore new insights is the next critical reasoning step.Prior sampling-based methods cannot distinguish the priority of different premises (Yao et al., 2023;Zhang et al., 2023), leading to less skillful reasoning compared to human counterparts.Therefore, we aim to quantify the relationship between each premise and the target and prioritize premise selection for better exploration.</p>
<p>Premise prioritization with two-stage scoring.</p>
<p>To improve the opportunity of deriving new insights, we leverage two quantitative measurements to select useful premises for combination.Firstly, we evaluate the relevance score of each premise p in conjunction with the target c.By simulating the overlap of topics and elements within them, varying priorities can be assigned to the premises.Secondly, we select the most relevant premise p * from D as the primary premise, and all other premises are considered candidate supplementary premises to interact with p * .We then quantify the likelihood of these premises being merged with p * according to semantic similarity and adherence to logical deduction rules.Therefore, supplementary premises p s exceeding a given threshold θ can be obtained.The two-stage scoring can be formulated as:
r p = relevance(p, c), p * ≜ arg max p∈D r p ,(3)s p ′ = supplement(p * , p ′ ), p s ≜ {p ′ ∈ D ∪ I \ {p * }; s p ′ ≥ θ}, (4)
where both the relevance and supplement functions are implemented through carefully designed instructions for LLMs.See Appendix for detailed prompt templates.</p>
<p>Premise exploration with three-fold verification.Once selected premises for exploration are determined, we employ LLMs to execute the explore function, which considers combining supplementary premises p s with the primary premise p * to generate a new proposition p, which can be given by: p = explore(p * , p s ).</p>
<p>(5)</p>
<p>Next, the rationality of the newly explored proposition p undergoes rigorous verification, encompassing three-fold critical aspects: (1) Logical validity:</p>
<p>We verify whether the deduction of the selected premises to p is valid in terms of logical reasoning rules;</p>
<p>(2) Useful contribution: We verify whether p is a useful determinate premise that contributes to derive the conclusion.It helps filter out the "correct nonsense" that may be logically valid but fail to enhance the conclusion derivation; (3) Duplication avoidance: We verify whether p provides information gain beyond the original premises, avoiding the generation of mere paraphrases of existing premises.Only propositions that pass all these verification checks will be retained and added to the determinate premise set.The main steps of premise prioritization and exploration can be formulated as:
D ← D ∪ { p}, if verify( p, {p * , p s }) = True, (6)
where the verify function is also implemented by the carefully designed instructions for LLMs, and detailed prompt templates are available in Appendix.Through premise prioritization and exploration, LLMs can effectively prioritize more pertinent premises to explore new insights, improving reasoning effectiveness and efficiency.</p>
<p>Reasoning Memorization</p>
<p>As known conditions dynamically update during the reasoning process, conventional methods often overlook historical reasoning details, resulting in erroneous reasoning directions or stagnant reasoning progress (Yao et al., 2023;Zhang et al., 2023).In contrast, humans generally record previous reasoning steps and retain both successful and failed attempts in mind to continue reasoning.</p>
<p>To bridge this cognitive gap, we design a reasoning memorization module to automate the storage and extraction of available premises and evolving reasoning structures.We initialize the reasoning memory as M = D (0) ∪I (0) , only containing input premises before the reasoning process.Figure 1(c) illustrates an iteration of memory storage and extraction, which is elaborated in detail as follows.</p>
<p>Memory storage.During the t-th iteration of premise exploration, our focus of the reasoning details lies on the new proposition p (t) and the reasoning paths G p (t) that connect the original premises
{p (t) * , p(t)
s } to p (t) .If p (t) passes all verification checks, we denote the reasoning paths as positive G + p (t) and store both p (t) and G + p (t) into the reasoning memory.Otherwise, the reasoning paths will be designated as negative G − p (t) and also stored into the memory as part of the reasoning experiences.This process can be formulated as:
M (t) =    M (t−1) ∪ { p (t) , G + p (t) }, if verify = True M (t−1) ∪ {G − p (t) }, otherwise(7)
Memory extraction.When we consider prioritizing premises in the (t+1)-th iteration, we extract t previous reasoning details from memory to guide LLMs in drawing upon successful experiences and avoiding repetitive mistakes.Following each iteration of premise exploration, it is essential to extract current premises and reasoning paths from memory.This extraction can help accurately verify whether the current determinate information is sufficient to draw the target conclusion.More details about the reasoning memory are available in Appendix.</p>
<p>Overall, the reasoning memory supports both retrospective and prospective reasoning during the iterative process.Retrospectively, it stores historical reasoning details for updating reasoning states.Prospectively, it extracts previous reasoning experiences into future steps, enhancing the accuracy of premise prioritization and exploration.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Datasets.To verify the capability of LLMs to engage in rigorous logical reasoning based solely on established conditions, without external knowledge, we carefully select five challenging logical reasoning benchmarks: (1) LogiQA (Liu et al., 2020) collects the multiple-choice logical problems from National Civil Servants Examination of China.Since it contains different types of questions, we carefully reviewed its test set and retained 179 high-quality questions whose premises are delineated within the context as a curated collection.</p>
<p>(2) ProofWriter (Tafjord et al., 2020) is a widely used logical reasoning benchmark.We use the open-world assumption subset where each case requires to be proven true, false or unknown.We follow Pan et al. (2023) to use the depth-5 subset containing 600 cases for evaluation.( 3) FOLIO (Han et al., 2022) is a challenging benchmark requiring complex first-order logic reasoning to solve.We follow the official data split and choose the validation set containing 204 examples for evaluation.( 4) PrOntoQA (Saparov and He, 2022) is similar to ProofWriter for evaluating logical reasoning.( 5) LogicalDeduction (LD) is a challenging task in BigBench (Srivastava et al., 2022).The problems are mainly about deducing the order of objects In principle, our proposed framework imposes no restrictions on the type of used LLMs.Here we uniformly employ the most advanced GPT-4 (Ope-nAI, 2023) and GPT-3.5-turbo as the base model to test the upper limit of LLM-based logical reasoning.Our implementation is based on the Microsoft guidance library1 .We set the temperature to 0.1 by default and 0.7 for CoT-SC (n = 16).</p>
<p>Main Results</p>
<p>The results presented in Table 1 demonstrate that our proposed DetermLR achieves superior reasoning accuracy with fewer steps compared to other multi-step reasoning methods (CR and ToT).While all methods show improvement over GPT-3.5-turbo by approximately 0.1 accuracy, DetermLR consistently outperforms all baselines even on the same base model.For the most challenging LogiQA, all baselines including CR perform poorly on this task with accuracy below 46.Since the utilization order of known conditions is crucial to solving the exam problem, baseline methods often fail to grasp the accurate reasoning direction.DetermLR performs well by prioritizing and memorizing known conditions and reasoning steps, resulting in an accuracy of 54.19 with GPT-4.Meanwhile, the average number of reasoning steps in DetermLR reaches 11.74, which is more efficient than CoT-SC, ToT and CR in solving real logical reasoning examination problems.For ProofWriter and FOLIO, DetermLR can generate more accurate propositions for the target than CoT-SC, ToT, and CR.Also, DetermLR requires fewer reasoning steps to reach the same conclusion, ensuring more efficient premise integration and exploration.The results in LD shows that compared to all baseline methods, DetermLR can enhance the accuracy of assigning the order of objects, and substantially reduce the number of reasoning iterations.</p>
<p>Ablation Study</p>
<p>In addition to those baselines, we also conduct an ablation study to assess the impact of each component of the proposed method.The ablation variants include: 1) DetermLR w/o identify removes premise identification at the beginning of reasoning; 2) DetermLR w/o priority replaces premise priorities with randomly sampled candi- (a) Case A with 18 original premises.</p>
<p>There are seven people named A, B, C, D, E, F, and G with education levels being Bachelor's and PhD.</p>
<p>Here are the given premises: i1: Among the 7 people, there are 3 PhDs, and 3 of them are female.i2: The educational levels of A, B, and C are the same, while those of F and G are different.i3: The genders of E, F, and G are the same, while the genders of A and D are different.i4: Someone is a female PhD.Target: which of the following can be concluded?a.A is a male PhD.b.F is a female PhD.c. G is not a male PhD.d.D is a female PhD.</p>
<p>Generated determinate premises</p>
<p>Further Analysis</p>
<p>Case study.An intuition suggests that problems with more known conditions and longer contexts tend to require more complex reasoning structures.However, relying solely on this intuition to preset reasoning structures might not always be accurate.As shown in Figure 2, Case A initially appears to be a highly complex problem due to its 18 premises.However, upon prioritizing the premises, we review that the problem's reasoning could be modeled using a concise chain-like reasoning structure with only two steps.This indicates the reasoning structure should not be preset before reasoning but rather formed through review after problem solving.Case B presents only 4 premises, but each premise is complicated and requires to be repeatedly utilized to reach the conclusion.This also indicates that determining a problem's difficulty solely based on the complexity of its context might not always be accurate.More reasoning examples and detailed reasoning processes are available in Appendix.</p>
<p>Impact of the number of determinate premises.</p>
<p>In practice, the required number of generated determinate premises (denoted as n) is a key hyperparameter for our method.To weigh the reasoning effectiveness and efficiency, a larger number may not always be better.As shown in Figure 3, generating more determinate premises will gradually streamline the reasoning process.The reasoning performance of smaller n is comparable to some baselines, and as n increases, our method can achieve the best performance.However, the subsequent increase in n will bring about a significant increase in the number of reasoning iterations, while the improvement in accuracy will be very limited.Therefore, we set n to 4 in our experiment, a position close to the inflection point to trade off effect improvement and efficiency control.</p>
<p>Generation efficiency of determinate premises.</p>
<p>we further investigate the odds of success and failure across multiple reasoning iterations.Therefore, we make a more detailed analysis of the average number of reasoning steps required to gen-   erate a useful proposition.As shown in Table 3, DetermLR outperforms the baseline significantly across all datasets by using the fewest steps to generate each useful new insight.This number could also reflect the average difficulty of the datasets to some extent.For LogiQA, both CR (4.25) and De-termLR (2.63) are much higher than the values on other datasets, which indicates that the analytical reasoning questions in LogiQA are still the most challenging tasks.To validate this hypothesis, we obtain human performance on LogiQA by inviting two volunteers--one, a graduate student with experience in public examinations, completed all questions with an accuracy rate of 73, and the other, a student with no prior exposure to such knowledge, achieved an accuracy rate of 59.</p>
<p>Complexity analysis.</p>
<p>We also conduct a complexity analysis for more detailed efficiency comparisons.We choose ToT and CR as strong multistep reasoning baselines to compute the average inference time for each reasoning step.All experiments in this analysis are performed on the same device for fair comparison.The results in Table 4 show that although the inference time per step for DetermLR is slightly more than CR and ToT, the superiority of DetermLR lies in substantially saving overall required reasoning steps.Therefore, considering the inference time per case, we can see that the overall inference efficiency of DetermLR is still better than ToT and CR.</p>
<p>Error analysis.Current LLM-based reasoning cannot resolve the following errors: (1) Insufficient exploration of implicit conditions: LLMs cannot identify that school roommates have the same gender;</p>
<p>(2) Insufficient understanding of boundary conditions: Three of the five are candidates, the first two and the last two each have one candidate, LLMs cannot assert that the middle one must be the candidate;</p>
<p>(3) Lack of flexible use of logical rules:  Given that A implies B, ¬ A implies B, LLMs cannot assert that B must be true.</p>
<p>Conclusion</p>
<p>In this work, we propose DetermLR, a novel reasoning framework to align LLM-based reasoning more closely resemble human cognitive reasoning.First, we propose a novel perspective that formulates the reasoning process as an evolution from indeterminacy to determinacy.Second, we employ quantitative measurements for premise prioritization and exploration, allowing LLMs to prioritize premises more conducive to exploring new insights.Furthermore, we introduce a reasoning memorization module to preserve historical details during iterative reasoning.Experimental results show that DetermLR can achieve the highest accuracy on multiple logical reasoning benchmarks while requiring fewer reasoning steps.Notably, in more intricate tasks like LogiQA, DetermLR exhibits even more pronounced advancements, mirroring human-like reasoning skills to a greater extent.</p>
<p>While DetermLR exhibits superior performance over baselines across various tasks, challenges persist within LLM-based reasoning.One primary limitation lies in extracting implicit conditions from problem contexts for intricate reasoning tasks.While LLMs can discern intuitively presented conditions, parsing nuanced or implicit information from verbose descriptions remains difficult.In scenarios with few available conditions, exhaustive exploration of each condition is imperative for deriving additional useful conditions.Moreover, LLMs face challenges in accurately parsing exceedingly lengthy condition descriptions, as well as navigating intricate nested if-then conditional structures delineating complex relationships among multiple subjects.Such complexities increase the risk of LLMs erring during the reasoning process.Consequently, enhancing LLMs' proficiency in uncovering implicit conditions and comprehending lengthy and complex condition descriptions emerges as a pivotal research avenue within the domain of LLMbased reasoning.These aspects will be a focal point of our future research directions.</p>
<p>A More Examples on Datasets</p>
<p>A.1 LogicalDeduction</p>
<p>[An example in LogicalDeduction]</p>
<p>Context: The following paragraphs each describe a set of seven objects arranged in a fixed order.The statements are logically consistent within each paragraph.In a golf tournament, there were seven golfers: Ana, Eli, Mya, Amy, Joe, Mel, and Ada.Ada finished above Mel.Ada finished third-to-last.Amy finished above Ana.Mya finished second-to-last.Joe finished above Amy.Eli finished below Ana.Ana finished above Ada.Ana finished above Mya.</p>
<p>Question: Which of the following is true?Options: A) Ana finished third.B) Eli finished third.C) Mya finished third.D) Amy finished third.E) Joe finished third.F) Mel finished third.G) Ada finished third.</p>
<p>Answer: A</p>
<p>[Chain-of-Thought Reasoning by GPT-4] Conclusion:</p>
<p>• From the premises, we know that: 1. Ada finished third-to-last, which means Ada finished 6th.</p>
<ol>
<li>Mya finished second-to-last, which means Mya finished 6th. 3. Joe finished above Amy.4. Amy finished above Ana. 5. Eli finished below Ana. 6. Ada finished above Mel.Now, let's try to arrange the golfers based on the given information: 1. Joe 2. Ana 3. Amy 4. Eli 5. -6.Mya and Ada 7. Mel From the arrangement, we can see that Amy finished third.So, the answer is D) Amy finished third.</li>
</ol>
<p>[Prediction: D] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions:</p>
<p>• Mel finished last.</p>
<p>• Ana finished fifth.</p>
<p>• Eli finished sixth.</p>
<p>• Mel finished last.Ada finished fifth.</p>
<p>• Amy finished above Ana and below Joe.</p>
<p>• Joe finished third.</p>
<p>Conclusion:</p>
<p>• From the propositions, we know that Joe finished third.So the answer is E.</p>
<p>[Prediction: E] (Wrong)</p>
<p>[DetermLR by  Generated Determinate Premises:</p>
<p>• Mya did not finish first.</p>
<p>• Ada did not finish first or second.</p>
<p>• Joe finished above Ana.</p>
<p>• Amy did not finish third-to-last.</p>
<p>• Amy finished above Eli.</p>
<p>• Amy finished second.Joe finished first.</p>
<p>• Ada did not finish first-to-third.</p>
<p>• Eli did not finish first-to-third.</p>
<p>Reasoning Details:</p>
<p>1.In the NO:1 round, we use these "premises": "Amy finished above Ana.Mya finished secondto-last.Ada finished above Mel.Eli finished below Ana.Joe finished above Amy." and got a "New Determinate Premise": "Joe finished above Ana"</p>
<ol>
<li>
<p>In the NO:2 round, we use these "premises": " Amy finished above Ana.Eli finished below Ana." and got a "New Determinate Premise": "Amy finished above Eli."</p>
</li>
<li>
<p>In the NO:3 round, we use these "premises": " Ada finished third-to-last.Amy finished above Ana." and got a "New Determinate Premise": "Amy did not finish third-to-last."</p>
</li>
<li>
<p>In the NO:4 round, we use these "premises": "Amy finished first or second.Joe finished above Amy." and got a "New Determinate Premise": "Amy finished second.Joe finished first."</p>
</li>
<li>
<p>In the NO:5 round, we use these "premises": " Ada finished third-to-last.Ada finished above Mel." and got a "New Determinate Premise": "Ada did not finish first-to-third."</p>
</li>
</ol>
<p>6.In the NO:6 round, we use these "premises": " Eli finished below Ana.Ana did not finish first or second." and got a "New Determinate Premise": "Eli did not finish first-to-third."</p>
<p>Conclusion:</p>
<p>• From the premises and propositions, we know that Ada finished third-to-last, Ana finished above Ada, Ana finished above Mya, which means she finished fifth.Mya finished second-to-last, which means she finished sixth.Since there are only seven golfers, this means that Joe must have finished first, as he finished above Amy, who finished above Ana, who finished above Eli.With Joe finishing first, Amy must have finished second, since she finished above Ana and Eli.Ana would be third, Eli would be fourth, and Mel would be the last (seventh).So the answer is A) Ana finished third.</p>
<p>[Prediction: A] (Right)</p>
<p>A.2 LogiQA</p>
<p>[An example in LogiQA]</p>
<p>Context: There are seven people named A, B, C, D, E, F, and G with education levels being Bachelor's and PhD.Here are the given premises: 1): Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.2) The educational levels of A, B, and C are the same, while those of F and G are different.</p>
<p>3) The genders of E, F, and G are the same, while the genders of A and D are different.4) The final hire is a female PhD.</p>
<p>Question: Based on the statements above, which of the following can be concluded?Options: A. A is a male PhD.B. F is a female PhD.C. G is not a male PhD.D. D is a female PhD.</p>
<p>Answer: D</p>
<p>[Chain-of-Thought Reasoning by GPT-4] Conclusion:</p>
<p>• From the premise, it can be inferred that the genders of E, F, and G are female.Since the final appointment is a female Ph.D. holder, the selection must be made from E, F, and G.The educational levels of A, B, and D are the same, so they cannot be Ph.D. holders because there are only 3 Ph.D. holders.Therefore, there must be two Ph.D. holders among E, F, and G. Since the educational levels of F and G are different, one of them must be a Ph.D. holder, and E is also a Ph.D. holder.Therefore, one of E, F, and G is a female Ph.D. holder.The genders of A and C are different, and since E, F, and G are females, either A or C must be male.Among options A, B, C, and D, option B is correct because F is a female Ph.D. holder.The answer is B.</p>
<p>[Prediction: B] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions</p>
<p>• The genders of D,E,F are female.</p>
<p>• A,B,C are undergraduates.</p>
<p>Conclusion:</p>
<p>• From the proposition, we can deduce that E, F, and G are females.A and C have different genders, so A and C are males.A, B, and D have the same educational background, which means they are either all Ph.D. holders or all undergraduates.However, there are only 3 Ph.D. holders, so A, B, and D must all be undergraduates.This means that E, F, and G include two Ph.D. holders.Since the final appointment is a female Ph.D. holder, it means that either E or F must be a Ph.D. holder.However, the educational levels of F and G are different, so G is an undergraduate.Therefore, both E and F are Ph.D. holders.Therefore, the final answer is: A is a male undergraduate; C is a male undergraduate; E is a female Ph.D. holder; F is a female Ph.D. holder; G is a female undergraduate.The answer is C.</p>
<p>[Prediction: C] (Wrong)</p>
<p>[DetermLR by  Generated Determinate Premises:</p>
<p>• A, B, and C have Bachelor's degrees.</p>
<p>• E, F, and G are all males.</p>
<p>• B and C are females.</p>
<p>• D and E are both PhD.holders.</p>
<p>• C is a female undergraduate.</p>
<p>• E is a male PhD.holder.</p>
<p>• D is a female PhD.holder.</p>
<p>Reasoning Details:</p>
<p>1.In the NO:1 round, we use these "premises": "Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The educational levels of A, B, and C are the same, while those of F and G are different."and got a "New Determinate Premise": "A, B, and C have Bachelor's degrees."</p>
<ol>
<li>
<p>In the NO:2 round, we use these "premises": " Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The genders of E, F, and G are the same, while the genders of A and D are different."and got a "New Determinate Premise": "E, F, and G are all males."</p>
</li>
<li>
<p>In the NO:3 round, we use these "premises": " Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The genders of E, F, and G are the same, while the genders of A and D are different.E, F, and G are all males" and got a "New Determinate Premise": "B and C are females."</p>
</li>
<li>
<p>In the NO:4 round, we use these "premises": "Among the 7 individuals, there are 3 with a PhD., and 3 of them are female.The genders of E, F, and G are the same, while the genders of A and D are different.A, B, and C have Bachelor's degrees." and got a "New Determinate Premise": "D and E are both PhD.holders."</p>
</li>
<li>
<p>In the NO:5 round, we use these "premises": "B and C are females.A, B, and C have Bachelor's degrees." and got a "New Determinate Premise": "C is a female undergraduate."</p>
</li>
</ol>
<p>6.In the NO:6 round, we use these "premises":" D and E are both PhD.Holders.E, F, and G are all males " and got a "New Determinate Premise": "E is a male PhD.holder."In the NO:7 round, we use these "premises": "The educational levels of A, B, and C are the same, while those of F and G are different.The final hire is a female PhD.E is a male PhD.holder."and got a "New Determinate Premise": " D is a female PhD.holder"</p>
<p>Conclusion:</p>
<p>• From the propositions, we know the answer is D. D is a female PhD.</p>
<p>[Prediction: D] (Right)</p>
<p>A.3 ProofWriter</p>
<p>[An example in ProofWriter]</p>
<p>Context: The bald eagle chases the cat.The bald eagle is kind.The bald eagle sees the dog.The cat is rough.The dog is blue.The dog is red.The dog is rough.The dog sees the cat.The dog does not see the lion.The lion sees the bald eagle.If something chases the cat then the cat chases the dog.If something chases the dog then it eats the bald eagle.If something is kind and it sees the lion then it is not red.If something chases the dog then the dog chases the bald eagle.If the dog eats the bald eagle then the dog sees the bald eagle.If something is cold then it sees the lion.If something sees the cat and it is blue then the cat is kind.If something is blue and it chases the bald eagle then the bald eagle is cold.</p>
<p>Question:Based on the above information, is the following statement true, false, or uncertain?The cat eats the bald eagle.[Prediction: C] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions:</p>
<p>• The bald eagle does not eat the dog.</p>
<p>• If the bald eagle sees the lion, it is not red.</p>
<p>• The bald eagle is not red.</p>
<p>• If the dog chases the cat, then the cat chases the dog.</p>
<p>• The dog does not eat the bald eagle.</p>
<p>Conclusion:</p>
<p>• We can not find any direct or indirect information about the cat eating the bald eagle.The information provided in the premises does not support or contradict the hypothesis.Judgement: Now we know that the Hypothesis is Unknown.</p>
<p>[Prediction: C] (Wrong)</p>
<p>[DetermLR Reasoning by GPT-4] Generated Determinate Premises:</p>
<p>1.The cat chases the dog.</p>
<ol>
<li>
<p>The dog chases the bald eagle.</p>
</li>
<li>
<p>Something chases the cat.</p>
</li>
<li>
<p>The bald eagle is something that chases the cat.</p>
</li>
<li>
<p>The dog and the lion both see the bald eagle.</p>
</li>
</ol>
<p>6.If the cat chases the dog, then the cat eats the bald eagle.</p>
<ol>
<li>The cat eats the bald eagle.</li>
</ol>
<p>Reasoning Details:</p>
<p>• In the NO:1 round, we use these "premises": "If something chases the cat then the cat chases the dog.The bald eagle chases the cat."and got a "New Determinate Premise": "The cat chases the dog."</p>
<p>• In the NO:2 round, we use these "premise": "If something chases the dog then the dog chases the bald eagle.If something chases the cat then the cat chases the dog.The dog sees the cat.The cat chases the dog.If something chases the dog then it eats the bald eagle.The bald eagle chases the cat.The cat is rough."andgot a "New Determinate Premise": "The dog chases the bald eagle."</p>
<p>• In the NO:3 round, we use these "premises": "If something chases the cat then the cat chases the dog.The cat chases the dog.If something chases the dog then it eats the bald eagle.The bald eagle chases the cat."and got a "New Determinate Premise": "Something chases the cat."</p>
<p>• In the NO:4 round, we use these "premises": "Something chases the cat.The bald eagle chases the cat."and got a "New Determinate Premise": "The bald eagle is something that chases the cat."</p>
<p>• In the NO:5 round, we use these "premises": "The dog chases the bald eagle.The bald eagle is something that chases the cat.The bald eagle chases the cat.The bald eagle is kind.The bald eagle sees the dog.The lion sees the bald eagle."and got a "New Determinate Premise": "The dog and the lion both see the bald eagle."</p>
<p>• In the NO:6 round, we use these "premises": "The dog chases the bald eagle.The bald eagle sees the dog.If something chases the dog then the dog chases the bald eagle.If something chases the dog then it eats the bald eagle.The dog is blue."and got a "New Determinate Premise": "If the cat chases the dog, then the cat eats the bald eagle."</p>
<p>• In the NO:7 round, we use these "premises": "The cat chases the dog.The bald eagle is something that chases the cat.If something chases the dog then it eats the bald eagle.If something chases the cat then the cat chases the dog.If the cat chases the dog, then the cat eats the bald eagle.The dog chases the bald eagle.The bald eagle chases the cat."and got a "New Determinate Premise": "The cat eats the bald eagle."</p>
<p>Conclusion:</p>
<p>From the propositions, we know the cat eats the bald eagle.So the answer is true.Question: Based on the above information, is the following statement true, false, or uncertain?1984 is not a streaming service.</p>
<p>Options: A) True B) False C) Uncertain Answer: A [Chain-of-Thought Reasoning by  Conclusion:</p>
<p>• From the premises, we cannot directly deduce whether the hypothesis 1984 is not a streaming service is true or false.The premises provide information about the different media formats, but there is no explicit information about the format of 1984.So the answer is C.</p>
<p>[Prediction: C] (Wrong)</p>
<p>[Cumulative Reasoning by GPT-4] Generated Propositions:</p>
<p>• If 1984 is a streaming service, then it is digital.</p>
<p>• If 1984 is a streaming service, then it is not analog.</p>
<p>Conclusion:</p>
<p>• The premises state that if 1984 is a streaming service, then it is a hardcover book.However, no information is provided about whether 1984 is a streaming service or a hardcover book.Since we lack information to confirm or deny the hypothesis, we cannot deduce a conclusion from the given premises.Judgement: Now we know that the Hypothesis is Unknown.So the answer is C.</p>
<p>[Prediction: C] (Wrong)</p>
<p>Figure 1 :
1
Figure 1: The overview of DetermLR: (a) premise identification; (b) iterative reasoning process: (b-1) premise prioritization and exploration and (b-2) reasoning memorization.Green elements represent determinate premises, and blue elements represent indeterminate premises.The proportion of blue decreases with the accumulation of green during iterative reasoning.</p>
<p>lion sees the bald eagle.d2 The bald eagle is kind.d3:The bald eagle sees the dog.d4:The cat is rough.d5:The dog sees the cat.d6:The bald eagle chases the cat.i1:The dog is blue.i2:The dog is red.i3:The dog is rough.i4:The dog does not see the lion.i5:If one chases the cat, then the cat chases the dog.i6:If one is blue and sees the cat, then the cat is kind.i7:If one is kind and sees the lion, then it is not red.i8:If one chases the dog, then the dog chases the bald eagle.i9:If the dog eats the bald eagle, then the dog sees the bald eagle.i10:If one is cold, then it sees the lion.i11:If one chases the dog then it eats the bald eagle.i12:If one is blue and chases the bald eagle then the bald eagle is cold.Target: The cat eats the bald eagle.Is this statement true, false, or unknown?a. True, b.False, c. Unknown Generated determinate premises: d7:The cat chases the dog.d8:The cat eats the bald eagle.Conclusion: The answer is a.</p>
<p>Figure 2 :
2
Figure 2: Two cases for contrasting reasoning structure and context complexity.Green dots with d represent determinate premises and blue dots with i represent indeterminate premises.</p>
<p>Figure 3 :
3
Figure 3: The impact of the number of generated determinate premises.</p>
<p>•</p>
<p>The bald eagle chases the cat.2.If something chases the cat, then the cat chases the dog.(From 1, the cat chases the dog)3.If something chases the dog, then it eats the bald eagle.(We don't have information about something chasing the dog) 4. The cat eats the bald eagle.(We don't have information about the cat eating the bald eagle)Recall the Hypothesis: The cat eats the bald eagle is Unknown.</p>
<p>No digital media are analog.Every printed text is analog.All streaming services are digital.If an object is a hardcover book then it is printed text.If 1984 is a streaming service, then 1984 is a hardcover book.</p>
<p>Reasoning Completion (a) Premise Identification LLM Premise Identifier Determinate premises: d1) Gary is young. d2) Gary is nice. d3) Gary is angry. LLM Relevance Scorer Most Relevant Premise Other Premises LLM Supplement Scorer LLM Proposition Explorer LLM Verification Determinate premises: d1) Gary is young. d2) Gary is nice. d3) Gary is angry. d4) Gary is quiet. d5) Gary is furry. d6) Gary is white. d7) Gary is big. d8) Gary is round. Indeterminate premises: i2) All nice people are big. i7) If Erin is round, then Erin is quiet. Conclusion: The answer is False. (b-2) Reasoning Memorization
Indeterminate premises:i1) White and furry peopleare big.i2) All nice people are big.i3) If someone is quiet andfurry, then they are white.i4) If someone is young, thenthey are quiet.i5) All big and furry peopleare round.i6) If someone is quiet, thenthey are furry.i7) If Erin is round, then Erinis quiet.Target: Gary is not round.(True/Fasle/Unknown)</p>
<p>Input Premises Input Target Selected Premises New Proposition Memory Extraction (b-1) Premise Prioritization and Exploration Memory Storage ×N Memory Update Yes No (b) Iterative Reasoning Process</p>
<p>Table 1 :
1
Comparison results on LogiQA, ProofWriter, FOLIO, and LogicalDeduction.Bold numbers highlight the highest accuracy and the fewest steps among multi-step methods.Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓ Accuracy ↑ Avg.Steps ↓
LogiQAProofWriterFOLIOPrOntoQALDModel Accuracy ↑ GPT-3.5-turbo Method Standard 16.76 CoT 22.35 CoT-SC 22.91 SI 24.02 LAMBADA 24.021 1 16 15.16 59.3236.17 45.00 48.67 50.17 55.171 1 16 18.49 16.8949.51 54.41 57.34 57.84 60.291 1 16 14.19 12.3551.80 84.00 86.80 88.60 90.801 1 16 13.58 12.0941.33 46.00 50.33 51.00 62.671 1 16 17.24 74.43ToT26.2519.8754.1624.8859.8019.8291.2019.3066.3323.71CR31.8418.9359.1618.8159.8018.9692.4016.9371.0018.32DetermLR37.9913.3968.8316.5263.7210.3793.2010.7474.3313.19Standard31.69146.83160.29177.40171.331CoT38.55167.41167.65191.00173.331CoT-SC40.431669.331668.141693.401674.6716GPT-4SI LAMBADA41.34 39.1114.35 56.2470.67 72.0017.46 15.0469.11 70.1013.76 10.8593.80 95.6011.38 10.5676.33 78.0014.95 67.32ToT43.0219.8770.3324.5769.1219.1297.6018.9176.8321.83CR45.2517.0071.6716.7669.1115.8798.2014.1878.3316.98DetermLR54.1911.7479.1714.6375.498.5798.609.7885.0012.35from a set of conditions. We use the full test setcontaining 300 examples for evaluation. Logicalreasoning examples for each task are available inAppendix.
(Kazemi et al., 2022)our DetermLR with existing LLM-based reasoning methods, we choose the following baselines: 1) Standard prompting directly answers the question based on in-context examples; 2) CoT(Wei et al., 2022)adopts stepby-step generation of indeterminate rationales before the final answer; 3) CoT-SC(Wang et al., 2022b)uses majority voting to aggregate multiple CoTs; 4) SI(Creswell et al., 2022b)uses selection-inference patterns for iterative reasoning; 5) LAMBADA(Kazemi et al., 2022)performs backward chaining for automated reasoning tasks; 6) ToT (Yao et al., 2023) models the reasoning process as a thought search tree; 7) CR (Zhang et al., 2023) is recently proposed to generate new propositions based on available premises.</p>
<p>Table 2 :
2
Ablation results: accuracy (first row) and average reasoning steps (second row in parentheses).
MethodLogiQA ProofWriter FOLIOLDDetermLR w/o identify46.15 (17.24)71.50 (16.58)69.61 (13.70) (16.84) 79.00DetermLR w/o priority47.83 (18.35)72.32 (17.21)70.59 (14.69) (17.02) 80.33DetermLR w/o memory39.66 (11.98)68.33 (14.79)67.65 (8.65) (13.05) 76.67DetermLR54.19 (11.74)79.17 (14.63)75.49 (8.57) (12.35) 85.00date premises for exploration; 3) DetermLR w/omemory removes our memorization module dur-ing iterative reasoning. The results demonstrate theimportance of premise identification, prioritizationand exploration, and iterative reasoning memoriza-tion modules. DetermLR w/o identify blurs thetransformation process in reasoning, resulting inreduced accuracy and more reasoning steps. SinceDetermLR w/o priority cannot capture the reason-ing direction, it requires more reasoning steps stillto achieve a lower accuracy than the full model.This emphasizes that prioritizing premise hierar-chy can significantly improve reasoning efficiency.Without consideration on reasoning memory, theaccuracy of DetermLR w/o memory decreases byat least 7.84, indicating the importance of record-ing the inference structure and prompting LLMs torecall previously acquired information.</p>
<p>Table 3 :
3
The average number of reasoning steps per generated determinate premise.
MethodLogiQA ProofWriter LD FOLIOToT4.974.914.374.78CR4.253.353.403.97DetermLR w/o identify4.313.323.373.43DetermLR w/o priority4.593.443.403.67DetermLR w/o memory2.732.242.071.99DetermLR2.632.172.031.83</p>
<p>Table 4 :
4
Comparison results of inference efficiency.
MethodAvg. steps Inference time Inference time per case per step per caseToT18.407.77s142.93sCR14.516.86s99.69sDetermLR7.698.05s61.98s</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman.2017.A broad-coverage challenge corpus for sentence understanding through inference.arXiv preprint arXiv:1704.05426.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,Thomas L Griffiths, Yuan Cao, and KarthikNarasimhan. 2023. Tree of thoughts: Deliberateproblem solving with large language models. arXivpreprint arXiv:2305.10601.Yifan Zhang, Jingqin Yang, Yang Yuan, and An-drew Chi-Chih Yao. 2023. Cumulative reason-ing with large language models. arXiv preprintarXiv:2308.04371.Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, et al. 2023. Asurvey of large language models. arXiv preprintarXiv:2303.18223.Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,Nathan Scales, Xuezhi Wang, Dale Schuurmans,Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022.Least-to-most prompting enables complex reason-ing in large language models. arXiv preprintarXiv:2205.10625.
https://github.com/guidance-ai/guidance.
Generated Determinate Premises:• If 1984 is a streaming service, then 1984 is digital.• All streaming services are not analog.• If 1984 is a streaming service, then it cannot be a hardcover book.Reasoning Details:1.In the NO:1 round, we use these "premises": "If 1984 is a streaming service, then 1984 is a hardcover book.All streaming services are digital."and got a "New Determinate Premise": "If 1984 is a streaming service, then 1984 is digital."2. In the NO:2 round, we use these "premises": "No digital media are analog.All streaming services are digital.If 1984 is a streaming service, then 1984 is a hardcover book.If 1984 is a streaming service, then 1984 is digital."and got a "New Determinate Premise": "All streaming services are not analog."3. In the NO:3 round, we use these "premises": "If 1984 is a streaming service, then 1984 is digital.If 1984 is a streaming service, then 1984 is a hardcover book.All streaming services are digital.No digital media are analog.All streaming services are not analog."and got a "New Determinate Premise": "If 1984 is a streaming service, then it cannot be a hardcover book."Conclusion:• We know from the premises that if 1984 is a streaming service, then 1984 is a hardcover book.However, we also deduced that if 1984 is a streaming service, then it cannot be a hardcover book, as it would be digital and not analog.This contradiction leads us to conclude that 1984 is not a streaming service.Judgement: Now we know that the Hypothesis is True.[Prediction: A] (Right)B Prompt TemplatesDifferent logical reasoning tasks have different data formats.Among them, ProofWriter and FOLIO offer explicitly known premises, while LogiQA and LogicalDeduction require parsing the question context to extract the premises, which indicates that extra steps such as premise transformation are necessary.We use ProofWriter and LogicalDeduction as representative tasks to illustrate the prompt templates.More details about prompt design are available in our Github repository.B.1 ProofWriterBased on the modeling scheme introduced by our DetermLR, we summarize main designed prompts into several parts such as premise identification, premise prioritization, premise exploration, logical validation, and final conclusion.Read the "Last reasoning history".If we got a "false Proposition" in history,when you select "Most relevant premise",do not choose the same "Most relevant premise" in history as your answer.Please follow these steps: 1.From the determinate premise, select the "Most relevant premise" which has the same subject with "Hypothesis", and give a score from 0 to 1. 2.You need to assess how the "Most relevant premise" relates to all the other "determinate premise" and "indeterminate premise",based on Relevance scoring rules.3.The "determinate premise" and "indeterminate premise" with scores higher than 0.25 will be used as the final results, along with Most relevant premise.Relevance scoring rules: 1.When scoring relevance, 0.25 added for each noun or 0.3 added for each adjective that is the same between two sentences.2. Scores start to accumulate from 0 points, and the upper limit is 1 point.1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 3. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." Please make sure that the "Proposition" is logically correct.Please make sure that the "Proposition" is not a duplicate of the "Premises".Please make sure your reasoning is directly deduced from the "Premises" and "Propositions" other than introducing unsourced common knowledge and unsourced information by common sense reasoning.Please remember that your "Proposition" should be useful to determine whether the "Hypothesis" is True, False or Unknown.--{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}"We want to deduce more propositions to determine the correctness of the following "Hypothesis": "Hypothesis": "{{this.conclusion}}"Can you deduce a new "Proposition" from at least two given "Premises"?{{user}} {{#assistant}}"Proposition": "{{this.proposition}}"{{assistant}}{{˜each}} {{#user}} -"premises": "{{premises}}" "boundary condition": "{{boundary condition}}" We want to derive more propositions to solve the following question: "question": "{{question}}" Combined with boundary conditions, can you derive a new "proposition" from at least two given "premises"?{{user}} {{#assistant}}"proposition": "{{assistant}} {{#assistant}}{{gen "proposition" temperature=temperature max tokens=100 stop=' '}}{{assistant}} Prompts used for Logical Validity {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use the Logical Reasoning Rules(LRR) to determine whether the deduction of the given "Premises" to a "Proposition" is valid or not, reply with True or False.Logical Reasoning Rules(LRR): 1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. If B,then C." then "Proposition": "If A, then C." 3. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 4. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." -{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}""Proposition": "{{this.proposition}}"{{user}} {{#assistant}}"Judgement": "Is this deduction valid?{{this.validation}}"{{assistant}}{{˜each}} Prompts used for Final Conclusion {{#system}}Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the "paragraphs" and "questions", then use the "premises", "boundary conditions" and "propositions" to reason which of the options given is the answer to the "question".Make sure that your reasoning is derived directly from "premises" and "propositions" rather than introducing passive common sense and passive information through common sense reasoning.Please note that this is a single choice question.If you can get the answer directly from the proposition, then you should choose the answer directly, otherwise keep reasoning with the proposition, premises, and boundary conditions until you arrive at a single answer.-{{system}} {{ #each examples}} {{#user}} -"context": "{{context}}" "question and options": "{{question}}" {{user}} {{#assistant}}"Premises": "Let's think step by step, and from the context we can extract these premises: {{premises}}"{{assistant}} {{#assistant}}"Boundary condition": "Let's think step by step, and from the context we can extract these boundary conditions: {{boundary condition}}"{{assistant}} {{#assistant}}"Thoughts": "Let us think step by step.From the premises, we can deduce propositions:{{propositions}}"{{assistant}} {{#assistant}}"Recall the reasoning history":"{{infer history}}"{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Reasoning": "Using premises, boundary conditions, and continuing to reason according to the propositions already obtained,{{assistant}} {{#assistant}}{{gen "reasoning" temperature=0.7 max tokens=500 stop=[' ']}}{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Judgement": "Now we know that the answer to this question should be{{assistant}} {{#assistant}}{{select "judgement" options=choose}}{{assistant}}B.2 LogicalDeductionIn addition to the prompting steps mentioned above, we also include premise extraction and premise transformation to parse the available premises from the original question.Prompts used for Premise Identification {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.First, read and analyze the following definition: Determinate premise: The premise contains the same noun or adjective as the Hypothesis, and the premise is not in the structure of "if..." or "if...then...".Second, read and analyze the "Premise" and "Hypothesis" .Judge "Premise" is "determinate premise" or not.Third, please make sure your classification decisions are derived directly from definitions, rather than unsourced common sense.-{{system}} {{ #each examples}} {{#user}} -"Premise": "{{this.Premise}}" "Hypothesis": "{{this.Hypothesis}}" {{/user}} {{#assistant}}"Judgement":"Is this "Premise" a "determinate premise" or not?{{this.usefulness}}"{{/assistant}} {{#assistant}}"Explanation": {{this.Explanation}}{{assistant}} {{ /each}} Prompts used for Premise Prioritization {{#system}}Suppose you are one of the greatest artificial intelligence scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the "determinate premises" and "indeterminate premises", and then filter out several premises.When you decide on a variable, read through the inference history first and don't choose a variable that has failed before as your choice for this round.Please follow these steps: 1. Count the cumulative number of times each variable is mentioned by "determinate premises" and "indeterminate premises".2. Determine the variable according to the number of mentions from high to low.If the number of mentions is the same, the variable with more prerequisites will be given priority.3. Determine whether the value of the variable has been determined under the current variable.If it is determined, search and determine the next variable in order from most to least.If it has not been completely determined, go to step 4. 4. Use this variable as a criterion for screening "premises" and filter out all premises related to this variable.-{{system}} {{ #each examples}} {{#user}} -"determinate premise": "{{determinate premise}}" "indeterminate premise": "{{indeterminate premise}}" "topic": "{{topic}}" "boundary condition": "{{boundary condition}}" "Inference history": "{{last false history}}" {{user}} {{#assistant}}Can you count the cumulative number of times each variable is mentioned by the premises?{{assistant}}{{#assistant}}"Count": "{{assistant}} {{#assistant}}{{gen "count" temperature=temperature max tokens=200 stop=' '}}{{assistant}} {{#assistant}}Which variable should you choose as the criterion for premises screening?{{assistant}}{{#assistant}}"Explanation": "{{assistant}} {{#assistant}}{{gen "explanation" temperature=temperature max tokens=200 stop=' '}}{{assistant}} {{#assistant}}What are all the premises related to this variable?{{assistant}}{{#assistant}}"Results": "{{assistant}} {{#assistant}}{{gen "results" temperature=temperature max tokens=200 stop=' '}}{{assistant}} Prompts used for Premise Exploration {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use Logical Reasoning Rules(LRR) to deduce a "Proposition" from two given "Premises" and the proposition does not include "if".Logical Reasoning Rules(LRR): 1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 3. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." Please make sure that the "Proposition" is logically correct.Please make sure that the "Proposition" is not a duplicate of the "Premises".Please make sure your reasoning is directly deduced from the "Premises" and "Propositions" other than introducing unsourced common knowledge and unsourced information by common sense reasoning.Please remember that your "Proposition" should be useful to determine whether the "Hypothesis" is True, False or Unknown.--{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}"We want to deduce more propositions to determine the correctness of the following "Hypothesis": "Hypothesis": "{{this.conclusion}}"Can you deduce a new "Proposition" from at least two given "Premises"?{{user}} {{#assistant}}"Proposition": "{{this.proposition}}"{{assistant}}{{˜each}} {{#user}} -"premises": "{{premises}}" "boundary condition": "{{boundary condition}}" We want to derive more propositions to solve the following question: "question": "{{question}}" Combined with boundary conditions, can you derive a new "proposition" from at least two given "premises"?{{user}} {{#assistant}}"proposition": "{{assistant}} {{#assistant}}{{gen "proposition" temperature=temperature max tokens=100 stop=' '}}{{assistant}} Prompts used for Logical Validation {{#system}}Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use the Logical Reasoning Rules(LRR) to determine whether the deduction of the given "Premises" to a "Proposition" is valid or not, reply with True or False.Logical Reasoning Rules(LRR): 1. "Two premises": "If A,then B. A is true." then "Proposition": "B is true." 2. "Two premises": "If A,then B. If B,then C." then "Proposition": "If A, then C." 3. "Two premises": "If A,then B. B is not true."then "Proposition": "A is not true" 4. "Two premises": "A is either C or D. A is not C." then "Proposition": "A is D." -{{system}} {{ #each examples}} {{#user}} -"Premises": "{{this.premises}}""Proposition": "{{this.proposition}}"{{user}} {{#assistant}}"Judgement": "Is this deduction valid?{{this.validation}}"{{assistant}}{{˜each}} Prompts used for Boundary Validation {{#system}}Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.Answer "True" or "False" to determine whether the existing premises plus a new premise satisfies the boundary condition.-{{system}} {{ #each examples}} {{#user}} -"existing premises": "{{this.premises}}""new premise": "{{this.newpremise}}" "boundary condition": "{{this.boundarycondition}}" After adding the new premise to the existing premise, does it still meet the boundary conditions?{{user}} {{#assistant}}"Judgement": "{{this.judgement}}"{{assistant}}{{˜each}} {{#user}} -"existing premises": "{{premises}}" "new premise": "{{proposition}}" "boundary condition": "{{boundary condition}}" After adding the new premise to the existing premise, does it still meet the boundary conditions?{{user}} {{#assistant}}"Judgement": "{{assistant}} {{#assistant}}{{select "judgement" options=valid duplicated}}{{assistant}} Prompts used for Premise Transformation {{#system}} Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First, please read and analyze the "existing premises", read the definition of transformation; Transformation: In the one-to-one relationship, when the value of the current variable is determined, it means that this variable can not take other values, and other variables can not take the current value, this reasoning process is transformation.Check whether relying on a single "premise" and "boundary condition" can translate into other new premises?The new premises should not duplicate any of the existing premises.If it can be transformed, give the new premises you have deduced; if it can't, answer "None."Make sure that the new premises you get are helpful in solving the problem.-{{system}} {{ #each examples}} {{#user}} -"existing premises": "{{this.premises}}""question": "{{this.question}}""premise": "{{this.premise}}""boundary condition": "{{this.boundarycondition}}" {{user}} {{#assistant}}Can you derive a new premise based on the premises and boundary condition that help solve the problem?{{assistant}} {{#assistant}}"new premise": "{{this.newpremise}}"{{assistant}} {{˜each}} {{#user}} -"existing premises": "{{premises}}" "question": "{{question}}" "premise": "{{premise}}" "boundary condition": "{{boundary condition}}" {{user}} {{#assistant}}Can you derive a new premise based on the premises and boundary condition that help solve the problem?{{assistant}} {{#assistant}}"new premise": "{{assistant}} {{#assistant}}{{gen "premise" temperature=temperature max tokens=50 stop=[' ']}}{{assistant}} Prompts used for Premise Extraction {{#system}} Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the two sets of definitions defined below; Premise: A constraint on the absolute position of an object or on the relative relationship between two objects.Boundary condition: A description of the number of objects and the name of the object.According to the above definition, summarize the core topics discussed in the following paragraphs and extract the premise and boundary conditions in the context.-{{system}} {{ #each examples}} {{#user}} -"context": "{{this.context}}"{{user}} {{#assistant}}Can you summarize the core topics of the discussion from the context above?{{assistant}} {{#assistant}}"topic": "{{this.topic}}"{{assistant}}{{#assistant}}Can you extract the premise from the context above?{{assistant}} {{#assistant}}"premise": "{{this.premise}}"{{assistant}}{{#assistant}}Can you extract the boundary conditions from the context above?{{assistant}} {{#assistant}}"boundary condition": "{{this.boundarycondition}}"{{assistant}} {{˜each}} {{#user}} -"context": "{{context}}" {{user}} {{#assistant}}Can you summarize the core topics of the discussion from the context above?{{assistant}} {{#assistant}}"topic": "{{assistant}} {{#assistant}}{{gen "topic" temperature=temperature max tokens=50 stop=' '}}{{assistant}} {{#assistant}}Can you extract the premise from the context above?{{assistant}} {{#assistant}}"premise": "{{assistant}} {{#assistant}}{{gen "premise" temperature=temperature max tokens=300 stop=[' n '']}}{{assistant}} {{#assistant}}Can you extract the boundary conditions from the context above?{{assistant}} {{#assistant}}"boundary condition": "{{assistant}} {{#assistant}}{{gen "boundary condition" temperature=temperature max tokens=300 stop=[' ']}}{{assistant}} Prompts used for Final Conclusion {{#system}}Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.First read and analyze the "paragraphs" and "questions", then use the "premises", "boundary conditions" and "propositions" to reason which of the options given is the answer to the "question".Make sure that your reasoning is derived directly from "premises" and "propositions" rather than introducing passive common sense and passive information through common sense reasoning.Please note that this is a single choice question.If you can get the answer directly from the proposition, then you should choose the answer directly, otherwise keep reasoning with the proposition, premises, and boundary conditions until you arrive at a single answer.-{{system}} {{ #each examples}} {{#user}} -"context": "{{context}}" "question and options": "{{question}}" {{user}} {{#assistant}}"Premises": "Let's think step by step, and from the context we can extract these premises: {{premises}}"{{assistant}} {{#assistant}}"Boundary condition": "Let's think step by step, and from the context we can extract these boundary conditions: {{boundary condition}}"{{assistant}} {{#assistant}}"Thoughts": "Let us think step by step.From the premises, we can deduce propositions:{{propositions}}"{{assistant}} {{#assistant}}"Recall the reasoning history":"{{infer history}}"{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Reasoning": "Using premises, boundary conditions, and continuing to reason according to the propositions already obtained,{{assistant}} {{#assistant}}{{gen "reasoning" temperature=0.7 max tokens=500 stop=[' ']}}{{assistant}} {{#assistant}}"Recall the questions and options":"{{question}}"{{assistant}} {{#assistant}}"Judgement": "Now we know that the answer to this question should be{{assistant}} {{#assistant}}{{select "judgement" options=choose}}{{assistant}}C Scalability AnalysisFollowing the previous prompting methods(Yao et al., 2023;Zhang et al., 2023)for enhanced LLM-based reasoning, our main evaluation is based on the test set (typically compact with small size) for each task.To further explore the scalability of the proposed method on larger and more diverse datasets, we randomly sample 1600 cases from the ProofWriter(Tafjord et al., 2020)training set to study the model performance across varying dataset sizes.The comparison results of Table5show that DetermLR consistently outperforms CR(Zhang et al., 2023)in terms of both accuracy and time-efficiency as data scales, which verifies the scalability and robustness of the proposed method.D Generalization in Math ReasoningIn this paper, our primary focus is to enhance the logical reasoning capabilities of LLMs, that is, to study how LLMs better utilize the given conditions to complete the reasoning process more accurately and efficiently.In principle, DetermLR can be easily adapted to other types of reasoning tasks-as long as a reasoning task has a set of available conditions, the proposed method can leverage the relationship between conditions and the target to identify whether each premise is indeterminate or determinate.Therefore, we attempt to evaluate the performance of DetermLR on other reasoning tasks, such as math reasoning.We construct a dataset of multi-variable equations.An example is shown as follows.4x + y + 3z = 38 (1) −x + y + z = 13(2)3x + 3y + z = 25(3)Based on our insights, each equation contains quantitative relationships between multiple variables, which should be regarded as indeterminate equations.Conversely, the exact variable values, like x = 1, are categorized as determinate equations.LLMs need to perform reasoning by selecting equations to combine and eliminate some variables (in line with from indeterminacy to determinacy), and finally obtain the solution of each variable to form the overall solution.For this evaluation, we choose CoT-SC(Wang et al., 2022b)and CR(Zhang et al., 2023)as strong baselines for comparison.The accuracy and average reasoning steps per case are provided below, indicating the effectiveness and efficiency of our method.The comparison results of the reasoning processes of DetermLR and CR are shown as follows.We can see that the DetermLR's reasoning trajectory mirrors human-like variable elimination methods.By employing prioritization functions, equations with fewer variables or those less frequently chosen are given precedence.Coupled with the encouragement to reduce the number of variables (from indeterminacy to determinacy) during new equation generation, the entire reasoning process is streamlined and effective under the correct direction, and the new equations generated are useful without unnecessary redundancy.In contrast, CR randomly selects these equations for combination in each round and often fails to ascertain the correct solution direction, leading to erroneous or incomplete solutions.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>arXiv:2308.03762Konstantine Arkoudas. 2023. Gpt-4 can't reason. arXiv preprint</p>
<p>Thinking and deciding. Jonathan Baron, 2023Cambridge University Press</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Wen-Tau Yih, Yejin Choi, arXiv:1908.05739Abductive commonsense reasoning. 2019arXiv preprint</p>
<p>Language models are few-shot learners. Gabor Samuel R Bowman, Christopher Angeli, Christopher D Potts, Tom Manning, Benjamin Brown, Nick Mann, Melanie Ryder, Jared D Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:1508.05326A large annotated corpus for learning natural language inference. 2015. 202033arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2205.09712and Irina Higgins. 2022a. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2205.09712and Irina Higgins. 2022b. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint</p>
<p>The pascal recognising textual entailment challenge. Ido Dagan, Oren Glickman, Bernardo Magnini, Machine learning challenges workshop. Springer2005</p>
<p>Logic and human reasoning: an assessment of the deduction paradigm. Jonathan St, B T Evans, Psychological bulletin. 12869782002</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>Treeof-mixed-thought: Combining fast and slow thinking for multi-hop visual reasoning. Pengbo Hu, Ji Qi, Xingyu Li, Hong Li, Xinqi Wang, Bing Quan, Ruiyu Wang, Yi Zhou, arXiv:2308.096582023arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Merit: meta-path guided contrastive learning for logical reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, arXiv:2203.003572022arXiv preprint</p>
<p>Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran, arXiv:2212.13894Lambada: Backward chaining for automated reasoning in natural language. 2022arXiv preprint</p>
<p>Scitail: A textual entailment dataset from science question answering. Tushar Khot, Ashish Sabharwal, Peter Clark, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2018</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, arXiv:2210.024062022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Bin Lei, Chunhua Liao, Caiwen Ding, arXiv:2308.08614Boosting logical reasoning in large language models through a new framework: The graph of thought. 2023arXiv preprint</p>
<p>Natural language inference in contextinvestigating contextual reasoning over long texts. Hanmeng Liu, Leyang Cui, Jian Liu, Yue Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Siru Ouyang, Zhuosheng Zhang, Hai Zhao, arXiv:2105.10334Fact-driven logical reasoning. 2021810arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Logigan: Learning logical reasoning via adversarial pre-training. Xinyu Pi, Wanjun Zhong, Yan Gao, Nan Duan, Jian-Guang Lou, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Deductive reasoning and strategies. Walter Schaeken, Gino De Vooght, Gery D'ydewalle, arXiv:2308.10379Algorithm of thoughts: Enhancing exploration of ideas in large language models. 1999. 2023arXiv preprint</p>
<p>Manmeet Singh, S B Vaisakh, Neetiraj Malviya, arXiv:2303.11436Mind meets machine: Unravelling gpt-4's cognitive psychology. 2023arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, arXiv:2012.13048Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2020arXiv preprint</p>
<p>Diagnosing the first-order logical reasoning ability through logicnli. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. 2018arXiv preprint</p>
<p>From lsat: The progress and challenges of complex reasoning. Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, Nan Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 302022a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho, arXiv:1811.00671Dialogue natural language inference. 2018arXiv preprint</p>
<p>Reasoning Process of CR 1. the NO:1 round, Selected Equations: 4x + y + 3z = 38 (1), -x + y + z = 13. New Equation: 5x + 2z = 25</p>
<p>the NO:2 round, Selected Equations: -x + y + z = 13. 3x +3y + z = 25 (3) New Equation: 6y + 4z = 64</p>
<p>New Equation. the NO:3 round, Selected Equations: 5x + 2z = 25 (4), 6y + 4z = 64 (5). </p>
<p>the NO:4 round, Selected Equations: 5x + 2z =. 25New Equation: 6y + 4z = 64</p>
<p>the NO:5 round, Selected Equations: -x + y + z = 13. 5x + 2z = 25 (4). New Equation: -7x + 2y = 1</p>
<p>the NO:6 round, Selected Equations: 3x +3y + z = 25 (3), 5x + 2z = 25 (4). New Equation: x + 6y =25. </p>
<p>Solution: It's clear that the system is becoming more complicated ... (Wrong) Reasoning Process of DetermLR 1. the NO:1 round, Selected Equations: 4x + y + 3z = 38 (1), -x + y + z = 13. New Equation: 5x + 2z = 25</p>
<p>the NO:2 round, Selected Equations: 5x + 2z = 25 (4), 3x +3y + z = 25 (3) New Equation: x + 6y = 25. </p>
<p>the NO:3 round, Selected Equations: -x + y + z = 13. 5x + 2z = 25 (4). New Equation: -7x + 2y = 1</p>
<p>the NO:4 round, Selected Equations: x + 6y = 25 (5), -7x + 2y = 1. New Equation: x = 1</p>
<p>the NO:5 round, Selected Equations: x = 1 (7), x + 6y = 25 (5). New Equation: y = 4. </p>
<p>the NO:6 round, Selected Equations: x = 1. 105x + 2z = 25 (4)</p>
<p>Solution: x=1, y=4, z=10. Right</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2025 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2025</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2025</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-277993698</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.15349v1.pdf" target="_blank">Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)</a></p>
                <p><strong>Paper Abstract:</strong> Humans understand new combinations of words encountered if they are combinations of words recognized from different contexts, an ability called Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020) arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted Access Sequence Processing (RASP), a Transformer-equivalent programming language, to prove by construction that a Transformer encoder-decoder can perform the semantically equivalent ReCOGS_pos (Wu et al., 2024) arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore, our RASP model shows the ReCOGS_pos task does not require a hierarchical or tree-structured solution: we use word-level tokens with an"embedding"layer that tags with possible parts of speech, applying just once per encoder pass 19 attention-head compatible flat pattern-matching rules, shown using grammar coverage (Zeller et al., 2023) to be learnable from the training data, plus general prepositional phrase (pp) handling and sentential complement (cp) handling logic, and output the next logical form (LF) token (repeating until the LF is complete). The model does not apply recursive, tree-structured rules like 'np_det pp np ->np_pp ->np', but scores 100% semantic and string exact match on pp recursion, cp recursion using the decoder loop.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2025.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2025.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RASP-ReCOGS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Restricted Access Sequence Processing program for ReCOGS_pos (Transformer-equivalent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic RASP program (Transformer-equivalent) implemented at word-level with part-of-speech / verb-type mappings, flat attention-head-compatible pattern matching (19 templates), and an autoregressive decoder loop that produces ReCOGS_pos logical forms; used to prove by construction that a Transformer encoder-decoder can implement a compositional, systematic solution for ReCOGS_pos (length/recursion generalization up to tested limits).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RASP program (Transformer-equivalent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic RASP program implementing encoder-like attention-head-compatible selection/aggregation operations over five aligned POS/verb-type sequences plus an autoregressive decoder loop; word-level tokens, a mapping that tags possible POS/verb-type roles, flat (non-tree, non-recursive) pattern matching templates, and a rule to ignore 'pp det common_noun' and 'pp proper_noun' when selecting verb arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>attention-compatible select/aggregate operations (RASP), explicit POS/verb-type maps (symbolic embedding), flat non-recursive template matching, autoregressive decoder loop to support tail recursion</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ReCOGS_pos (ReCOGS positional logical-form variant of COGS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate synthetic English sentences (COGS grammar) to logical-form / semantic graph strings (ReCOGS_pos) where correct generalization requires recombining known lexical items in novel syntactic configurations; structural generalization splits test unseen combinations such as prepositional-phrase (pp) modification in novel positions and recursive pp/cp nesting.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>pp_recursion and cp_recursion tested up to depth 12 (n=1000 examples per split)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel structural patterns and recombination of seen constituents (structural generalization, recursion/length generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>out-of-distribution structural generalization splits (obj_pp_to_subj_pp, pp_recursion, cp_recursion, and others); also standard test/dev splits</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>none (deterministic program); results are from executing the RASP program (no stochastic training); the design used grammar coverage to build templates from training examples</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Test set (n=3000): 100% Semantic Exact Match (SEM) and 100% string exact match reported for the test set</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Generalization splits (n=1000 each): 100% SEM on all generalization splits except obj_pp_to_subj_pp which scored 92.20% SEM (Clopper-Pearson 95% CI: 90.36% to 93.79%); pp_recursion and cp_recursion both 100% SEM and 100% string exact match (95% CI 99.63% to 100.0%; n=1000 each). Overall non-recursive gen splits: 18,922/19,000 = 99.6% (95% CI reported 99.49% to 99.68%).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>No gap for most splits (100% SEM) vs test (100%); for obj_pp_to_subj_pp gap is 7.8 percentage points relative to perfect (100% -> 92.20%); relative to baseline Transformer (19.7% mean) gap is +72.5 pp advantage for RASP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>pp_recursion and cp_recursion: 100% SEM up to depth 12 (n=1000 each) (CI: 99.63%–100%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>100% on recursive splits (pp_recursion, cp_recursion) and on all non-recursive structural generalization splits tested; lower (92.20%) on object-pp-to-subject-pp (obj_pp_to_subj_pp) structural split.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against Wu et al. (2024) baseline Encoder-Decoder Transformer trained from scratch (2-layer default): RASP obtains 100% SEM on test and nearly all generalization splits while Wu baseline obtains much lower SEM on structural splits (e.g., obj_pp_to_subj_pp 19.7% mean). RASP also used grammar-coverage-derived templates and an explicit rule to ignore 'pp np' when selecting arguments; ablation of that rule would re-create attraction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>RASP (flat non-recursive templates + decoder loop) vs standard learned encoder-decoder Transformers (2–4 layers). The hand-constructed RASP approach yields systematic, near-perfect generalization where shallow trained Transformers fail on structural splits; RASP demonstrates capability rather than learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>By construction, a Transformer-equivalent (RASP) program can implement a systematic, compositional solution to ReCOGS_pos: achieves 100% SEM on test and nearly all generalization splits (pp_recursion and cp_recursion 100% to depth 12), only missing obj_pp_to_subj_pp (92.20% SEM). Demonstrates trees/recursion are not strictly required to solve these structural generalizations; shows specific small rule (ignore 'pp np' when selecting verb arguments) prevents 'attraction' errors and yields high generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Single notable failure on obj_pp_to_subj_pp (92.20% SEM); authors attribute remaining errors to 'attraction' effects where prepositional phrase nouns can be mistaken for verb arguments when the ignore-pp rule is absent or imperfect. RASP's performance relies on explicit POS/verb-type maps and the ignore-pp rule.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeeds given explicit POS/verb-type embedding (mapping), flat pattern templates that cover grammar expansions (grammar coverage), and the specific rule to ignore PP noun phrases when selecting verb argument indices; deterministic program does not face learnability/training issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2025.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2025.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wu-Transformer-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wu et al. (2024) baseline Encoder-Decoder Transformer trained from scratch on ReCOGS_pos</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard encoder-decoder Transformer models trained from scratch on ReCOGS_pos (COGS-derived synthetic semantic parsing dataset); evaluated on test and structural out-of-distribution generalization splits where they exhibit substantial failures on structural splits involving prepositional phrase modification and recursion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-Decoder Transformer (baseline from Wu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard seq2seq Transformer with Encoder and Decoder stacks (cross/self-attention), default baseline uses 2 layers in encoder and decoder (BertLayer-like blocks), trained from scratch on ReCOGS_pos; layer-variation experiments with 3 and 4 layers also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2-layer: 4,344,077 parameters; 3-layer: 6,046,701 parameters; 4-layer: 7,749,325 parameters (as reported for baseline experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer encoder-decoder (multi-head attention, feed-forward blocks), no explicit symbolic POS/verb maps, trained end-to-end from random init</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ReCOGS_pos</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above: translate synthetic English sentences to ReCOGS logical-form strings; structural generalization splits (e.g., obj_pp_to_subj_pp) require recombining seen constituents in novel structural positions.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Evaluated on recursion splits (pp_recursion and cp_recursion) to depth up to 12 in this paper's comparisons (original Wu et al. also report recursion splits).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel structural patterns (prepositional phrase moved to novel syntactic positions), recursion / length generalization</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>out-of-distribution structural generalization splits (obj_pp_to_subj_pp, pp_recursion, cp_recursion, others); IID test set for in-distribution evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised training from scratch (seq2seq objective) with random initialization, multiple random seeds; standard training scripts from Wu et al. used by this paper for reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Augmentation experiments: training with additional edited v_dat_p2 examples (prepositional phrase moved to recipient) was tried (added 328 modified training examples); this did not produce reliable improvement on obj_pp_to_subj_pp (see results).</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Overall test (ReCOGS_pos) reported 88.55% ± 1.87% Semantic Exact Match (SEM) (sample mean ± std, n=20; 95% CI for mean 87.73%–89.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>obj_pp_to_subj_pp split: 19.7% ± 6.1% SEM (sample mean ± std, n=20; 95% CI for mean 17.0%–22.4%). pp_recursion split: 40.18% ± 2.07% SEM (n=20; 95% CI 36.13%–44.24%). cp_recursion split: 52.40% ± 1.38% SEM (n=20; 95% CI 51.80%–53.01%). In the introduced v_dat_p2_pp_moved_to_recipient split baseline achieved 13% ± 15.6% (n=10; 95% CI 4%–23%).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Large gaps between IID test (≈88.6% SEM) and structural generalization splits (e.g., obj_pp_to_subj_pp 19.7% SEM -> ~68.9 pp gap from IID), indicating severe OOD generalization failure on structural compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Reported poor performance on recursion splits (pp_recursion 40.18% SEM, cp_recursion 52.40% SEM); RASP shows 100% to depth 12 while baseline is far lower (reported values above), but per-paper baseline performance breakdown by recursion depth not given beyond aggregate split metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Severe failures on structural prepositional-phrase shifts and recursion: obj_pp_to_subj_pp very low (19.7%), pp_recursion and cp_recursion also much lower than IID (40.18% and 52.40%). Lexical generalizations (not the focus here) were previously reported to be high in other work.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>This model is the baseline; compared in this paper to the RASP program and to variants with added training augmentations and with 3- and 4-layer variants. Adding layers (3 or 4) did not improve obj_pp_to_subj_pp performance (3-layer: 16.2% ±2.7% n=10; 4-layer: 19.3% ±4.1% n=10). Training with additional v_dat_p2 augmented examples (328 examples) did not substantially improve obj_pp_to_subj_pp (22% ±6.7% n=10, not significantly different).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>2-layer vs 3-layer vs 4-layer encoder-decoder Transformers: increasing depth (up to 4 layers) did not improve performance on the obj_pp_to_subj_pp split in the reported experiments; other literature up to 32 layers also saw little improvement for COGS structural splits (cited Petty et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Experimented with increasing number of Transformer blocks from 2 to 3 and 4 (parameter counts increased as reported) but no improvement on the obj_pp_to_subj_pp split; no larger-scale pretrained models were evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shallow-to-moderate-depth Transformers trained from scratch can reach reasonably high IID test performance on ReCOGS_pos (~88.6% SEM) but fail catastrophically on structural generalization splits (obj_pp_to_subj_pp ~19.7% SEM); increasing depth to 3-4 layers or modest data augmentation (specific v_dat_p2 augmentations) did not fix the hardest splits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Detailed error analysis shows majority of single-part logical-form errors on obj_pp_to_subj_pp are 'attraction' errors: when the agent is left of the verb and modified by a PP, 99.74% of single-part errors were in the agent part, and 96.73% of those assigned the agent index to the PP noun closest to the verb (i.e., attraction to nearest noun). Errors are consistent across random seeds indicating consistent failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>The baseline appears to succeed (high IID performance) when training data contains direct exposures to the specific patterns; but it fails to generalize structurally when it has not observed those structural positions (e.g., subject pp vs object pp). Additional small data augmentations tested did not reliably transfer to fix other structural splits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2025.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2025.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReCOGS_pos</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReCOGS_pos (positional ReCOGS variant of COGS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic-interpretation compositional generalization benchmark derived from COGS that maps synthetic English sentences to logical-form/semantic-graph strings; uses Semantic Exact Match (SEM) to allow order/indices differences that are semantically equivalent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ReCOGS_pos</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Input: sentences from a simplified English PCFG (COGS grammar). Output: logical-form strings representing semantic relations (agent, theme, recipient, nmod etc). Compositionality: requires recombining tokens and syntactic patterns unseen in training (structural splits like prepositional phrase movement and recursion). Evaluation: Semantic Exact Match (SEM) and string exact match (when applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Tests recursion (prepositional phrases and sentential complements) to variable depths; in this paper recursion tested up to depth 12.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel word/syntactic combinations and structural recombination (object-pp moved to subject-pp, pp and cp recursion, moving pp between theme/recipient, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Multiple out-of-distribution 'generalization' splits focused on structural generalizations (obj_pp_to_subj_pp, pp_recursion, cp_recursion, plus newly introduced v_dat_p2_pp_moved_to_recipient), plus IID test/dev/train splits.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Standard supervised training from scratch for Transformer baselines; RASP program constructed without learning; data augmentation experiments (adding modified v_dat_p2 examples) performed for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>This paper performed an inoculation/data-augmentation experiment: added 328 modified training examples (v_dat_p2 with PP moved to recipient) for baseline Transformer training; no reliable crossover improvement on obj_pp_to_subj_pp observed.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Baseline Transformer IID test ~88.55% SEM (Wu et al., n=20); RASP program achieved 100% SEM on test (n=3000).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Varies by split: RASP near-perfect on most splits; baseline Transformer poor on structural splits (e.g., obj_pp_to_subj_pp ~19.7% SEM, pp_recursion ~40.18% SEM, cp_recursion ~52.40% SEM).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Substantial: baseline often drops from ~88.6% IID to ~20–50% on structural splits depending on split (largest drop in obj_pp_to_subj_pp).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Paper reports recursion splits aggregated (pp_recursion, cp_recursion) but baseline low; RASP handles recursion to depth 12 perfectly; baseline per-depth breakdown not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Structural prepositional-phrase movement splits produce the largest failures (obj_pp_to_subj_pp and v_dat_p2_pp_moved_to_recipient).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>This paper compares deterministic RASP program results to Wu et al. (2024) baseline Transformers trained from scratch and to variants (layer increases, data augmentation), demonstrating that capability exists but learning is the challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReCOGS_pos reveals that Transformers trained from scratch can attain high IID performance but often fail on structural compositional generalization; however, a Transformer-equivalent RASP program can implement a systematic solution, suggesting the primary bottleneck is learnability/data/training rather than theoretical representational capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Structural generalization failures widely attributed to models not learning to ignore attractor PP nouns when selecting verb arguments and failing to generalize recursive patterns from limited structural exposures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Success requires either explicit algorithmic/template rules (as in RASP) or training regimes/inductive biases that enable learning the ignore-PP rule and structural combination rules; lexical generalization is easier and not the focus.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2025.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2025.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>v_dat_p2_pp_moved_to_recipient</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>v_dat_p2_pp_moved_to_recipient (new generalization split introduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new out-of-distribution generalization split where, for v_dat_p2 verb frames, a prepositional phrase is moved from modifying the theme to modifying the recipient (i.e., PP moved to a different NP position), testing whether models learn to ignore PP nouns when selecting verb arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ReCOGS_pos - v_dat_p2_pp_moved_to_recipient split</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same semantic-parsing task but with test examples where training contains theme-modified PPs while test places the PP on the recipient; this probes whether models generalize PP-handling across argument positions.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Single-PP movement cases (depth 1) reported; complexity similar to obj_pp_to_subj_pp structural movement.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel structural reattachment of PP (same lexical items, different syntactic attachment position)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>out-of-distribution structural generalization (novel structural pattern relative to training)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Baseline Transformer trained from scratch on standard ReCOGS_pos train.tsv; the paper also trained baselines with additional augmented examples (328 modified v_dat_p2 rows) to test inoculation/crossover effects.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Augmentation: added 328 training examples created by moving PP from theme to recipient for v_dat_p2 sentences; used to test whether such inoculation improves generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Baseline Transformer (n=10 runs) scored 13% ± 15.6% SEM (sample mean ± std, n=10; 95% CI for mean 4%–23%) on this split. When the baseline was trained with the augmented v_dat_p2 examples, performance on obj_pp_to_subj_pp remained low (22% ± 6.7%, n=10) indicating limited crossover benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Baseline drops to ~13% on this split from IID ~88.6% (huge gap ~75 pp); augmentation did not recover generalization to other structural splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to Wu et al. baseline trained without augmentation (13% ±15.6%) and with augmentation where crossover improvement to obj_pp_to_subj_pp was not significant (22% ±6.7%). RASP predicted difficulty and flat-template mechanism required ignore-pp rule.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Data augmentation with examples of PP moved to recipient did not reliably transfer to improve object-pp-to-subject-pp performance (no significant improvement observed).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This new split is at least as difficult as the previously hardest split (obj_pp_to_subj_pp) for baseline Transformers; demonstrates that different argument positions sharing the same PP-distractor problem are not trivially learned by simple data augmentation or shallow depth increases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Baseline models tend to assign verb-argument indices to the PP noun closest to the verb (attractor), failing to ignore PP nouns when selecting true arguments; augmentation did not generalize across argument types.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>RASP-like explicit ignore-pp behavior or training regimes/architectural biases that learn to ignore PP noun phrases when selecting verb arguments are needed for success.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2025.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2025.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attraction-errors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attraction errors (nearest PP noun attraction in logical-form argument assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A consistent failure mode where prepositional phrase (PP) nouns inserted between an argument noun and its verb cause models to erroneously assign the verb relation (agent/theme/recipient) to the nearer PP noun (attractor) rather than the true argument; quantified with high prevalence in baseline Transformer errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ReCOGS_pos structural generalization splits (obj_pp_to_subj_pp, related splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Structural generalization settings where PP modification appears in positions not seen in training; attraction errors manifest in logical-form output where indices point to PP nouns.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Observed at multiple pp recursion depths (examples shown for depth 1 and depth 2), and predicted to vary with recursion depth (more potential attractors).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>structural interference by distractor nouns introduced by PPs (nearest-noun attraction)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>obj_pp_to_subj_pp and similar structural OOD splits (recipient/theme movement)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Authors report attraction to nearest PP noun increases with PP recursion depth; specific per-depth numeric breakdown for baseline single-relation errors with depth=2 matched RASP predictions in 100% (n=22) of observed single logical-form part errors in that conditioned subset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Quantified on Wu et al. baseline: of single-part errors for agent-left single-verb sentences across n=10 models, 765/767 (99.74%) were in the agent part; of those 765, 740 (96.73%) had the agent index assigned to the nearest PP noun (CI 95.21%–97.87%). Model-level average ~97.07% of such errors matched nearest-noun attraction hypothesis (stderr 2.23%, n=10).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attraction to the nearest PP noun is the dominant failure mode explaining most single-part logical-form errors on obj_pp_to_subj_pp in baseline Transformers; this mechanism is predicted by a flat-template RASP model lacking an ignore-PP rule and is largely consistent across random seeds/models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Attraction errors are systematic: baseline models often pick the closest PP noun to the verb as the agent/theme/recipient when the intended noun is separated by PP(s); frequency of these errors correlates with PP recursion depth.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Avoiding attraction errors requires explicit handling (e.g., ignoring 'pp det common_noun'/'pp proper_noun' when selecting right indices) or training/data regimes that teach models to ignore PP nouns when selecting verb arguments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>COGS: A compositional generalization challenge based on semantic interpretation <em>(Rating: 2)</em></li>
                <li>Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation <em>(Rating: 2)</em></li>
                <li>Restricted Access Sequence Processing (RASP) (Thinking like transformers) <em>(Rating: 2)</em></li>
                <li>Thinking like transformers <em>(Rating: 2)</em></li>
                <li>What algorithms can transformers learn? a study in length generalization <em>(Rating: 1)</em></li>
                <li>Grokking of hierarchical structure in vanilla transformers <em>(Rating: 1)</em></li>
                <li>Slog: A structural generalization benchmark for semantic parsing <em>(Rating: 1)</em></li>
                <li>The impact of depth on compositional generalization in transformer language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2025",
    "paper_id": "paper-277993698",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "RASP-ReCOGS",
            "name_full": "Restricted Access Sequence Processing program for ReCOGS_pos (Transformer-equivalent)",
            "brief_description": "A deterministic RASP program (Transformer-equivalent) implemented at word-level with part-of-speech / verb-type mappings, flat attention-head-compatible pattern matching (19 templates), and an autoregressive decoder loop that produces ReCOGS_pos logical forms; used to prove by construction that a Transformer encoder-decoder can implement a compositional, systematic solution for ReCOGS_pos (length/recursion generalization up to tested limits).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RASP program (Transformer-equivalent)",
            "model_description": "Deterministic RASP program implementing encoder-like attention-head-compatible selection/aggregation operations over five aligned POS/verb-type sequences plus an autoregressive decoder loop; word-level tokens, a mapping that tags possible POS/verb-type roles, flat (non-tree, non-recursive) pattern matching templates, and a rule to ignore 'pp det common_noun' and 'pp proper_noun' when selecting verb arguments.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "attention-compatible select/aggregate operations (RASP), explicit POS/verb-type maps (symbolic embedding), flat non-recursive template matching, autoregressive decoder loop to support tail recursion",
            "task_domain": "linguistic/semantic",
            "task_name": "ReCOGS_pos (ReCOGS positional logical-form variant of COGS)",
            "task_description": "Translate synthetic English sentences (COGS grammar) to logical-form / semantic graph strings (ReCOGS_pos) where correct generalization requires recombining known lexical items in novel syntactic configurations; structural generalization splits test unseen combinations such as prepositional-phrase (pp) modification in novel positions and recursive pp/cp nesting.",
            "compositional_depth": "pp_recursion and cp_recursion tested up to depth 12 (n=1000 examples per split)",
            "composition_type": "novel structural patterns and recombination of seen constituents (structural generalization, recursion/length generalization)",
            "split_type": "out-of-distribution structural generalization splits (obj_pp_to_subj_pp, pp_recursion, cp_recursion, and others); also standard test/dev splits",
            "training_strategy": "none (deterministic program); results are from executing the RASP program (no stochastic training); the design used grammar coverage to build templates from training examples",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Test set (n=3000): 100% Semantic Exact Match (SEM) and 100% string exact match reported for the test set",
            "compositional_performance": "Generalization splits (n=1000 each): 100% SEM on all generalization splits except obj_pp_to_subj_pp which scored 92.20% SEM (Clopper-Pearson 95% CI: 90.36% to 93.79%); pp_recursion and cp_recursion both 100% SEM and 100% string exact match (95% CI 99.63% to 100.0%; n=1000 each). Overall non-recursive gen splits: 18,922/19,000 = 99.6% (95% CI reported 99.49% to 99.68%).",
            "generalization_gap": "No gap for most splits (100% SEM) vs test (100%); for obj_pp_to_subj_pp gap is 7.8 percentage points relative to perfect (100% -&gt; 92.20%); relative to baseline Transformer (19.7% mean) gap is +72.5 pp advantage for RASP.",
            "performance_by_depth": "pp_recursion and cp_recursion: 100% SEM up to depth 12 (n=1000 each) (CI: 99.63%–100%)",
            "performance_by_composition_type": "100% on recursive splits (pp_recursion, cp_recursion) and on all non-recursive structural generalization splits tested; lower (92.20%) on object-pp-to-subject-pp (obj_pp_to_subj_pp) structural split.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against Wu et al. (2024) baseline Encoder-Decoder Transformer trained from scratch (2-layer default): RASP obtains 100% SEM on test and nearly all generalization splits while Wu baseline obtains much lower SEM on structural splits (e.g., obj_pp_to_subj_pp 19.7% mean). RASP also used grammar-coverage-derived templates and an explicit rule to ignore 'pp np' when selecting arguments; ablation of that rule would re-create attraction errors.",
            "architectural_comparison": "RASP (flat non-recursive templates + decoder loop) vs standard learned encoder-decoder Transformers (2–4 layers). The hand-constructed RASP approach yields systematic, near-perfect generalization where shallow trained Transformers fail on structural splits; RASP demonstrates capability rather than learning dynamics.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "By construction, a Transformer-equivalent (RASP) program can implement a systematic, compositional solution to ReCOGS_pos: achieves 100% SEM on test and nearly all generalization splits (pp_recursion and cp_recursion 100% to depth 12), only missing obj_pp_to_subj_pp (92.20% SEM). Demonstrates trees/recursion are not strictly required to solve these structural generalizations; shows specific small rule (ignore 'pp np' when selecting verb arguments) prevents 'attraction' errors and yields high generalization.",
            "failure_analysis": "Single notable failure on obj_pp_to_subj_pp (92.20% SEM); authors attribute remaining errors to 'attraction' effects where prepositional phrase nouns can be mistaken for verb arguments when the ignore-pp rule is absent or imperfect. RASP's performance relies on explicit POS/verb-type maps and the ignore-pp rule.",
            "success_conditions": "Succeeds given explicit POS/verb-type embedding (mapping), flat pattern templates that cover grammar expansions (grammar coverage), and the specific rule to ignore PP noun phrases when selecting verb argument indices; deterministic program does not face learnability/training issues.",
            "uuid": "e2025.0"
        },
        {
            "name_short": "Wu-Transformer-baseline",
            "name_full": "Wu et al. (2024) baseline Encoder-Decoder Transformer trained from scratch on ReCOGS_pos",
            "brief_description": "Standard encoder-decoder Transformer models trained from scratch on ReCOGS_pos (COGS-derived synthetic semantic parsing dataset); evaluated on test and structural out-of-distribution generalization splits where they exhibit substantial failures on structural splits involving prepositional phrase modification and recursion.",
            "citation_title": "Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation",
            "mention_or_use": "use",
            "model_name": "Encoder-Decoder Transformer (baseline from Wu et al., 2024)",
            "model_description": "Standard seq2seq Transformer with Encoder and Decoder stacks (cross/self-attention), default baseline uses 2 layers in encoder and decoder (BertLayer-like blocks), trained from scratch on ReCOGS_pos; layer-variation experiments with 3 and 4 layers also evaluated.",
            "model_size": "2-layer: 4,344,077 parameters; 3-layer: 6,046,701 parameters; 4-layer: 7,749,325 parameters (as reported for baseline experiments)",
            "is_pretrained": false,
            "architectural_features": "standard Transformer encoder-decoder (multi-head attention, feed-forward blocks), no explicit symbolic POS/verb maps, trained end-to-end from random init",
            "task_domain": "linguistic/semantic",
            "task_name": "ReCOGS_pos",
            "task_description": "Same as above: translate synthetic English sentences to ReCOGS logical-form strings; structural generalization splits (e.g., obj_pp_to_subj_pp) require recombining seen constituents in novel structural positions.",
            "compositional_depth": "Evaluated on recursion splits (pp_recursion and cp_recursion) to depth up to 12 in this paper's comparisons (original Wu et al. also report recursion splits).",
            "composition_type": "novel structural patterns (prepositional phrase moved to novel syntactic positions), recursion / length generalization",
            "split_type": "out-of-distribution structural generalization splits (obj_pp_to_subj_pp, pp_recursion, cp_recursion, others); IID test set for in-distribution evaluation",
            "training_strategy": "Supervised training from scratch (seq2seq objective) with random initialization, multiple random seeds; standard training scripts from Wu et al. used by this paper for reproduction",
            "curriculum_details": null,
            "inoculation_details": "Augmentation experiments: training with additional edited v_dat_p2 examples (prepositional phrase moved to recipient) was tried (added 328 modified training examples); this did not produce reliable improvement on obj_pp_to_subj_pp (see results).",
            "iid_performance": "Overall test (ReCOGS_pos) reported 88.55% ± 1.87% Semantic Exact Match (SEM) (sample mean ± std, n=20; 95% CI for mean 87.73%–89.37%).",
            "compositional_performance": "obj_pp_to_subj_pp split: 19.7% ± 6.1% SEM (sample mean ± std, n=20; 95% CI for mean 17.0%–22.4%). pp_recursion split: 40.18% ± 2.07% SEM (n=20; 95% CI 36.13%–44.24%). cp_recursion split: 52.40% ± 1.38% SEM (n=20; 95% CI 51.80%–53.01%). In the introduced v_dat_p2_pp_moved_to_recipient split baseline achieved 13% ± 15.6% (n=10; 95% CI 4%–23%).",
            "generalization_gap": "Large gaps between IID test (≈88.6% SEM) and structural generalization splits (e.g., obj_pp_to_subj_pp 19.7% SEM -&gt; ~68.9 pp gap from IID), indicating severe OOD generalization failure on structural compositions.",
            "performance_by_depth": "Reported poor performance on recursion splits (pp_recursion 40.18% SEM, cp_recursion 52.40% SEM); RASP shows 100% to depth 12 while baseline is far lower (reported values above), but per-paper baseline performance breakdown by recursion depth not given beyond aggregate split metric.",
            "performance_by_composition_type": "Severe failures on structural prepositional-phrase shifts and recursion: obj_pp_to_subj_pp very low (19.7%), pp_recursion and cp_recursion also much lower than IID (40.18% and 52.40%). Lexical generalizations (not the focus here) were previously reported to be high in other work.",
            "has_baseline_comparison": null,
            "baseline_comparisons": "This model is the baseline; compared in this paper to the RASP program and to variants with added training augmentations and with 3- and 4-layer variants. Adding layers (3 or 4) did not improve obj_pp_to_subj_pp performance (3-layer: 16.2% ±2.7% n=10; 4-layer: 19.3% ±4.1% n=10). Training with additional v_dat_p2 augmented examples (328 examples) did not substantially improve obj_pp_to_subj_pp (22% ±6.7% n=10, not significantly different).",
            "architectural_comparison": "2-layer vs 3-layer vs 4-layer encoder-decoder Transformers: increasing depth (up to 4 layers) did not improve performance on the obj_pp_to_subj_pp split in the reported experiments; other literature up to 32 layers also saw little improvement for COGS structural splits (cited Petty et al., 2024).",
            "scale_effects": "Experimented with increasing number of Transformer blocks from 2 to 3 and 4 (parameter counts increased as reported) but no improvement on the obj_pp_to_subj_pp split; no larger-scale pretrained models were evaluated here.",
            "transfer_results": null,
            "key_findings": "Shallow-to-moderate-depth Transformers trained from scratch can reach reasonably high IID test performance on ReCOGS_pos (~88.6% SEM) but fail catastrophically on structural generalization splits (obj_pp_to_subj_pp ~19.7% SEM); increasing depth to 3-4 layers or modest data augmentation (specific v_dat_p2 augmentations) did not fix the hardest splits.",
            "failure_analysis": "Detailed error analysis shows majority of single-part logical-form errors on obj_pp_to_subj_pp are 'attraction' errors: when the agent is left of the verb and modified by a PP, 99.74% of single-part errors were in the agent part, and 96.73% of those assigned the agent index to the PP noun closest to the verb (i.e., attraction to nearest noun). Errors are consistent across random seeds indicating consistent failure mode.",
            "success_conditions": "The baseline appears to succeed (high IID performance) when training data contains direct exposures to the specific patterns; but it fails to generalize structurally when it has not observed those structural positions (e.g., subject pp vs object pp). Additional small data augmentations tested did not reliably transfer to fix other structural splits.",
            "uuid": "e2025.1"
        },
        {
            "name_short": "ReCOGS_pos",
            "name_full": "ReCOGS_pos (positional ReCOGS variant of COGS)",
            "brief_description": "A semantic-interpretation compositional generalization benchmark derived from COGS that maps synthetic English sentences to logical-form/semantic-graph strings; uses Semantic Exact Match (SEM) to allow order/indices differences that are semantically equivalent.",
            "citation_title": "Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": null,
            "task_domain": "linguistic/semantic",
            "task_name": "ReCOGS_pos",
            "task_description": "Input: sentences from a simplified English PCFG (COGS grammar). Output: logical-form strings representing semantic relations (agent, theme, recipient, nmod etc). Compositionality: requires recombining tokens and syntactic patterns unseen in training (structural splits like prepositional phrase movement and recursion). Evaluation: Semantic Exact Match (SEM) and string exact match (when applicable).",
            "compositional_depth": "Tests recursion (prepositional phrases and sentential complements) to variable depths; in this paper recursion tested up to depth 12.",
            "composition_type": "novel word/syntactic combinations and structural recombination (object-pp moved to subject-pp, pp and cp recursion, moving pp between theme/recipient, etc.)",
            "split_type": "Multiple out-of-distribution 'generalization' splits focused on structural generalizations (obj_pp_to_subj_pp, pp_recursion, cp_recursion, plus newly introduced v_dat_p2_pp_moved_to_recipient), plus IID test/dev/train splits.",
            "training_strategy": "Standard supervised training from scratch for Transformer baselines; RASP program constructed without learning; data augmentation experiments (adding modified v_dat_p2 examples) performed for baselines.",
            "curriculum_details": null,
            "inoculation_details": "This paper performed an inoculation/data-augmentation experiment: added 328 modified training examples (v_dat_p2 with PP moved to recipient) for baseline Transformer training; no reliable crossover improvement on obj_pp_to_subj_pp observed.",
            "iid_performance": "Baseline Transformer IID test ~88.55% SEM (Wu et al., n=20); RASP program achieved 100% SEM on test (n=3000).",
            "compositional_performance": "Varies by split: RASP near-perfect on most splits; baseline Transformer poor on structural splits (e.g., obj_pp_to_subj_pp ~19.7% SEM, pp_recursion ~40.18% SEM, cp_recursion ~52.40% SEM).",
            "generalization_gap": "Substantial: baseline often drops from ~88.6% IID to ~20–50% on structural splits depending on split (largest drop in obj_pp_to_subj_pp).",
            "performance_by_depth": "Paper reports recursion splits aggregated (pp_recursion, cp_recursion) but baseline low; RASP handles recursion to depth 12 perfectly; baseline per-depth breakdown not provided.",
            "performance_by_composition_type": "Structural prepositional-phrase movement splits produce the largest failures (obj_pp_to_subj_pp and v_dat_p2_pp_moved_to_recipient).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "This paper compares deterministic RASP program results to Wu et al. (2024) baseline Transformers trained from scratch and to variants (layer increases, data augmentation), demonstrating that capability exists but learning is the challenge.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "ReCOGS_pos reveals that Transformers trained from scratch can attain high IID performance but often fail on structural compositional generalization; however, a Transformer-equivalent RASP program can implement a systematic solution, suggesting the primary bottleneck is learnability/data/training rather than theoretical representational capacity.",
            "failure_analysis": "Structural generalization failures widely attributed to models not learning to ignore attractor PP nouns when selecting verb arguments and failing to generalize recursive patterns from limited structural exposures.",
            "success_conditions": "Success requires either explicit algorithmic/template rules (as in RASP) or training regimes/inductive biases that enable learning the ignore-PP rule and structural combination rules; lexical generalization is easier and not the focus.",
            "uuid": "e2025.2"
        },
        {
            "name_short": "v_dat_p2_pp_moved_to_recipient",
            "name_full": "v_dat_p2_pp_moved_to_recipient (new generalization split introduced in this paper)",
            "brief_description": "A new out-of-distribution generalization split where, for v_dat_p2 verb frames, a prepositional phrase is moved from modifying the theme to modifying the recipient (i.e., PP moved to a different NP position), testing whether models learn to ignore PP nouns when selecting verb arguments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": null,
            "task_domain": "linguistic/semantic",
            "task_name": "ReCOGS_pos - v_dat_p2_pp_moved_to_recipient split",
            "task_description": "Same semantic-parsing task but with test examples where training contains theme-modified PPs while test places the PP on the recipient; this probes whether models generalize PP-handling across argument positions.",
            "compositional_depth": "Single-PP movement cases (depth 1) reported; complexity similar to obj_pp_to_subj_pp structural movement.",
            "composition_type": "novel structural reattachment of PP (same lexical items, different syntactic attachment position)",
            "split_type": "out-of-distribution structural generalization (novel structural pattern relative to training)",
            "training_strategy": "Baseline Transformer trained from scratch on standard ReCOGS_pos train.tsv; the paper also trained baselines with additional augmented examples (328 modified v_dat_p2 rows) to test inoculation/crossover effects.",
            "curriculum_details": null,
            "inoculation_details": "Augmentation: added 328 training examples created by moving PP from theme to recipient for v_dat_p2 sentences; used to test whether such inoculation improves generalization.",
            "iid_performance": null,
            "compositional_performance": "Baseline Transformer (n=10 runs) scored 13% ± 15.6% SEM (sample mean ± std, n=10; 95% CI for mean 4%–23%) on this split. When the baseline was trained with the augmented v_dat_p2 examples, performance on obj_pp_to_subj_pp remained low (22% ± 6.7%, n=10) indicating limited crossover benefit.",
            "generalization_gap": "Baseline drops to ~13% on this split from IID ~88.6% (huge gap ~75 pp); augmentation did not recover generalization to other structural splits.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to Wu et al. baseline trained without augmentation (13% ±15.6%) and with augmentation where crossover improvement to obj_pp_to_subj_pp was not significant (22% ±6.7%). RASP predicted difficulty and flat-template mechanism required ignore-pp rule.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": "Data augmentation with examples of PP moved to recipient did not reliably transfer to improve object-pp-to-subject-pp performance (no significant improvement observed).",
            "key_findings": "This new split is at least as difficult as the previously hardest split (obj_pp_to_subj_pp) for baseline Transformers; demonstrates that different argument positions sharing the same PP-distractor problem are not trivially learned by simple data augmentation or shallow depth increases.",
            "failure_analysis": "Baseline models tend to assign verb-argument indices to the PP noun closest to the verb (attractor), failing to ignore PP nouns when selecting true arguments; augmentation did not generalize across argument types.",
            "success_conditions": "RASP-like explicit ignore-pp behavior or training regimes/architectural biases that learn to ignore PP noun phrases when selecting verb arguments are needed for success.",
            "uuid": "e2025.3"
        },
        {
            "name_short": "Attraction-errors",
            "name_full": "Attraction errors (nearest PP noun attraction in logical-form argument assignment)",
            "brief_description": "A consistent failure mode where prepositional phrase (PP) nouns inserted between an argument noun and its verb cause models to erroneously assign the verb relation (agent/theme/recipient) to the nearer PP noun (attractor) rather than the true argument; quantified with high prevalence in baseline Transformer errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": null,
            "task_domain": "linguistic/semantic",
            "task_name": "ReCOGS_pos structural generalization splits (obj_pp_to_subj_pp, related splits)",
            "task_description": "Structural generalization settings where PP modification appears in positions not seen in training; attraction errors manifest in logical-form output where indices point to PP nouns.",
            "compositional_depth": "Observed at multiple pp recursion depths (examples shown for depth 1 and depth 2), and predicted to vary with recursion depth (more potential attractors).",
            "composition_type": "structural interference by distractor nouns introduced by PPs (nearest-noun attraction)",
            "split_type": "obj_pp_to_subj_pp and similar structural OOD splits (recipient/theme movement)",
            "training_strategy": null,
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": "Authors report attraction to nearest PP noun increases with PP recursion depth; specific per-depth numeric breakdown for baseline single-relation errors with depth=2 matched RASP predictions in 100% (n=22) of observed single logical-form part errors in that conditioned subset.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Quantified on Wu et al. baseline: of single-part errors for agent-left single-verb sentences across n=10 models, 765/767 (99.74%) were in the agent part; of those 765, 740 (96.73%) had the agent index assigned to the nearest PP noun (CI 95.21%–97.87%). Model-level average ~97.07% of such errors matched nearest-noun attraction hypothesis (stderr 2.23%, n=10).",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Attraction to the nearest PP noun is the dominant failure mode explaining most single-part logical-form errors on obj_pp_to_subj_pp in baseline Transformers; this mechanism is predicted by a flat-template RASP model lacking an ignore-PP rule and is largely consistent across random seeds/models.",
            "failure_analysis": "Attraction errors are systematic: baseline models often pick the closest PP noun to the verb as the agent/theme/recipient when the intended noun is separated by PP(s); frequency of these errors correlates with PP recursion depth.",
            "success_conditions": "Avoiding attraction errors requires explicit handling (e.g., ignoring 'pp det common_noun'/'pp proper_noun' when selecting right indices) or training/data regimes that teach models to ignore PP nouns when selecting verb arguments.",
            "uuid": "e2025.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "COGS: A compositional generalization challenge based on semantic interpretation",
            "rating": 2
        },
        {
            "paper_title": "Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation",
            "rating": 2
        },
        {
            "paper_title": "Restricted Access Sequence Processing (RASP) (Thinking like transformers)",
            "rating": 2
        },
        {
            "paper_title": "Thinking like transformers",
            "rating": 2
        },
        {
            "paper_title": "What algorithms can transformers learn? a study in length generalization",
            "rating": 1
        },
        {
            "paper_title": "Grokking of hierarchical structure in vanilla transformers",
            "rating": 1
        },
        {
            "paper_title": "Slog: A structural generalization benchmark for semantic parsing",
            "rating": 1
        },
        {
            "paper_title": "The impact of depth on compositional generalization in transformer language models",
            "rating": 1
        }
    ],
    "cost": 0.023926999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)
21 Apr 2025</p>
<p>William Bruns adde.animulis@gmail.com 
See [Strobl et</p>
<p>Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)
21 Apr 2025255626BFF9054DC6C3203706446B92A7arXiv:2504.15349v1[cs.CL]
al., 2024]for context from formal language theory, computational complexity, circuit complexity theory, and experimental papers together, providing robust lower and upper bounds on what Transformers can do, including discussion of under what conditions Parity can be solved by Transformers and how whether it can be learned by randomly initialized Transformers under simple training schemes is a different question (general feed-forward neural networks can learn to solve Parity per [Rumelhart et al., 1988]).[Delétang et al., 2023] also.See "Appendix: Zhou et al 2024 relevance of their long addition experiment to language modeling and note on the Parity task and Transformers" (9.12)12 https://github.com/frankaging/ReCOGS 13https://github.com/IBM/cpg/blob/c3626b4e03bfc681be2c2a5b23da0b48abe6f570 /src/model/cogs_data.py#L523 could be learned (we did not actually use their grammar though, and our RASP solution is flat and nonhierarchical unlike their description of the COGS probabilistic context-free grammar which is hierarchical and recursive).See Figure1. 14  We believe any solution at the word-level can be converted to a character-level token solution (see Appendix 9.3 for proof of concept details on a character level solution not used here).15 Note we follow the [Klinger et al., 2024] description of COGS and include in our RASP vocabulary (part-of-speech or verb-type embedding/mapping) all words occurring anywhere in the upstream (Re)COGS "train.tsv".We also include two words in our vocab/embedding as common nouns accidentally left out of train.tsvvocabulary by the COGS author: "monastery" and "gardner" (only included in their train_100.tsvand dev.tsv not also in train.tsv,but present in test/gen), a decision affecting just 22 or 0.1% of generalization examples so would not affect any conclusions qualitatively.See also the discussion on COGS Github with the COGS author at https://github.com/najoungkim/COGS/issues/2#issuecomment-976216841 . 16Specific training examples for each rule are in Table2the end of "Appendix: Grammar Coverage analysis to develop and justify Restricted Access Sequence Processing model design" (9.11) 17  We only have aggregations with non-causal masks when that aggregation (or without loss of generality just before the aggregation product is used to avoid multiplying everywhere) is masked by an input mask restricting it to the sequence corresponding to the input.</p>
<p>Humans understand new combinations of words encountered if they are combinations of words recognized from different contexts, an ability called Compositional Generalization.The COGS benchmark [Kim and Linzen, 2020] reports 0% accuracy for Transformer models on some structural generalizations.We use [Weiss et al.,  2021]'s Restricted Access Sequence Processing (RASP), a Transformer-equivalent programming language, to prove by construction that a Transformer encoder-decoder can perform the semantically equivalent ReCOGS_pos [Wu et al., 2024]  variant of COGS systematically and compositionally: Our RASP model attains 100% semantic exact match 1 on the ReCOGS test set and 100% SEM on all generalization splits except obj_pp_to_subj_pp which gets 92%.Furthermore, our RASP model shows the ReCOGS_pos task does not require a hierarchical or treestructured solution: we use word-level tokens with an "embedding" layer that tags with possible part of speech 2 , applying just once per encoder pass 19 attention-head compatible flat patternmatching rules, shown using grammar coverage [Zeller et al., 2023] to be learnable from the training data, plus general prepositional phrase (pp) handling and sentential complement (cp) handling logic, and output the next logical form (LF) token (repeating until the LF is complete).The model does not apply recursive, tree-structured rules like 'np_det pp np -&gt; np_pp -&gt; np', but scores 100% semantic and string exact match on pp recursion, cp recursion using the decoder loop.</p>
<p>Introduction</p>
<p>It was long argued that connectionist models (i.e.neural networks) were somehow structurally incapable of compositional generalization [Fodor and   1 and 100% string exact match 2 per [Tenney et al., 2019] by layer 0 the part-of-speech could be predicted for most words in Transformers trained on a masked language modeling objective, so we assume an equivalent embedding is learnable.Pylyshyn, 1988]. 3However, large language models based on the Transformer architecture [Vaswani  et al., 2017] compose seemingly fluent and novel text and are excellent few or zero shot learners [Brown  et al., 2020].</p>
<p>Some observations do contradict that Transformers learn systematic, compositional solutions to problems that generalize 4 , for example structural generalizations in the COGS task and ReCOGS [Wu et al.,  2024] variant of the COGS task [Kim and Linzen,  2020] , a benchmark based on extracting semantics (logical form) from the syntax (grammatical form) of synthetic sentences in a simplified subset of English grammar, requiring models trained only on certain grammar examples to generalize to sentences with unseen grammar built up / recombined from parts present in the training examples.</p>
<p>We use [Weiss et al., 2021]'s Restricted Access Sequence Processing (RASP) language that can be compiled to concrete Transformer weights to prove by construction that a Transformer encoder-decoder 5 can perform ReCOGS_pos 6 over the vocabulary and grammar of that task in a systematic, compositional way (length and recursion depth limited) as a rigorous starting point to investigating when Transformers might learn or not actually learn such compositional/systematic solutions.We find a flat, not hierarchical/tree-structured model which lacks any handling for the recursive rules in the grammar (for prepositional phrase recursion and sentential complement recursion) can perform the task at high accuracy, but requires a special rule for avoiding "at-3 More specific versions of this debate continue, for example re: syntax, one can read [van Schijndel et al., 2019] vs [Goldberg, 2019] or re: hierarchical generalization by Transformers, [Petty and Frank, 2021] vs [Murty et al., 2023]. 4See "Appendix: Composition and Learning" (9.13) 5 We follow [Zhou et al., 2023] who used RASP to analyze auto-regressive decoder-loop cases, not just Transformer encoders as done by RASP author [Weiss et al., 2021]. 6official variant, closer to COGS than non-positional ReCOGS, as COGS is also positional, and means we can also measure string exact match, not just semantic exact match
(i) (ii) (iii) (i) (ii) (iii)
Figure 1: Introducing parse trees, logical form, and semantic graphs.Two semantically identical but syntactically distinct (i) sentences (a) "a boy painted the girl" and (b) "the girl was painted by a boy" are shown with (ii) their distinct parse tree (parsed into COGS input grammar), (iii) the string form of their semantics (ReCOGS logical form; differs in indices and ordering), and (c) the graph representation of their logical form (semantic graph 9 , not different at all between the two examples).Note the (iii) logical forms (LFs) differ by String Exact Match but not [Wu et al., 2024]'s Semantic Exact Match (order and indices do not match but nouns, normalized verbs, and relationships between nouns and verbs are same).Note the "agent", "theme" order in the logical form string is not required to match for Semantic Exact Match.COGS and ReCOGS tasks require extracting the semantics/meaning (c) encoded in LFs (iii) of sentences (i).</p>
<p>traction" errors 7 where inserted prepositional phrase nouns replace agent/theme/recipient nouns in the logical form by accident.This is our main result and suggests that the ReCOGS task can be performed with high accuracy by Transformers, turning efforts to learnability, and also adds to the literature a caveat on interpreting success on ReCOGS (and COGS 8 ) by noting a hierarchical or tree-structured representation is not necessarily required (contrary to [Kim  and Linzen, 2020] and assumption of [Murty et al.,  2022]).Finally, we predict that these "attraction" errors we had to specifically avoid in our RASP model are contributing to the high error rate of the [Wu et al., 2024] baseline Transformer trained from scratch and confirm this is the case. 7These attraction errors are similar to those discussed elsewhere in NLP and psycholinguistics literature on hierarchical vs linear processing by language models and humans, see "Appendix: Attraction errors" (9.6). 8semantically equivalent and see also https://github.com/willy-b/RASP-for-COGS 9As a convention, in converting ReCOGS logical forms to 2 Prior Literature [Kim and Linzen, 2020] introduce the COmpositional Generalization Challenge based on Semantic Interpretation (COGS) benchmark 10 and argue that Transformers have low accuracy on the generalization splits (35% overall), especially structural generalization splits where near 0% accuracy is reported, using a 2-layer Encoder-Decoder Transformer (2 layers for Encoder, 2 layers for Decoder).</p>
<p>For another example, [Lake and Baroni, 2023] use a "meta-learning for compositionality" approach with a 3-layer Encoder-Decoder Transformer architecture and achieve what they call "human-like systematic generalization", achieving high scores on everything in the COGS benchmark (&gt;99% on lexical generalizations) EXCEPT the structural generalization splits where they also still score 0% accuracy.However, one notices these networks are shallow compared with those used in successful largepretrained Transformer models (e.g.24-layer BERT where compositional parse trees seem to be encoded in its vector space representation [Hewitt and Manning, 2019]), and it is claimed, by e.g.[Csordás  et al., 2022] that for compositional operations, like parsing, the depth of the network must be at least the maximum number of compositional operations, e.g. the height of the parse tree for grammar dependent problems.Remarkably, [Petty et al., 2024] finds that increasing the layer depth of the Transformer models (up to 32 layers) does not improve the near 0% accuracy on COGS structural generalization splits like prepositional phrase modification of subject when the network has only seen it on the object during training and also input length/depth generalizations (like pp/cp recursion), perhaps surprising as for the simpler logical inferences problem in [Clark et al.,  2020] they observed successful logical inference depth generalization even by Encoder-only Transformers.</p>
<p>Thankfully, [Wu et al., 2024] are able to begin to get traction (low but nonzero accuracy) for the shallow Encoder-Decoder Transformer models on structural generalizations in a modified but semantically equivalent form of the COGS task they call Semantic Graphs we use the logical form (source, target) index order for directed semantic graph edges (from verb to related entity) EXCEPT for the agent relationship which is from the agent of a verb to the verb (opposite direction from logical form in that case), which gives our semantic graphs of ReCOGS sentences an unambiguous starting point (layout starts from agent) without affecting comparison of the graphs (generated by a consistent rule), see also Figure 3.</p>
<p>10 See Figure 1.</p>
<p>ReCOGS, which we analyze here.They remove redundant symbols, and use Semantic Exact Match instead of Exact Match (see Figure 1).</p>
<p>[ Zhou et al., 2023] apply [Weiss et al., 2021]'s RASP language to explain some inconsistent findings regarding generalization and use RASP to predict exactly which cases of generalization come easily to Transformers and which do not.[Zhou et al.,  2023] seem to reveal [Weiss et al., 2021] has provided the framework we seek by demonstrating how to apply RASP to Transformer decoders with intermediate steps, and even use it to learn how to modify difficult-to-learn tasks like Parity 11 and long addition in seemingly incidental ways based on RASP analysis to make them readily learnable by Transformers in a compositional, length generalizing way!Thus we apply techniques similar to [Zhou et al.,  2023] and [Weiss et al., 2021] to ReCOGS to (1) argue Transformers should be capable of performing the task, including the structural generalization splits, with high accuracy, and that the problem is learning not capability and (2) try to understand the prepositional phrase modification related generalization errors [Wu et al., 2024]'s baseline Encoder-Decoder Transformers are making.</p>
<p>Data</p>
<p>The ReCOGS [Wu et al., 2024] dataset (ReCOGS with positional indexing variant) based on COGS [Kim and Linzen, 2020] (input sentences/grammar are the same but logical form output are different) was used 12 , with special attention on the structural generalization splits (especially prepositional phrase Object-to-Subject generalization).</p>
<p>The grammar and vocabulary description for COGS English input sentences provided in the utilities associated with the IBM CPG project [Klinger  et al., 2024]  13 were used in designing our RASP solution and analyzing the ways in which this task</p>
<p>Model</p>
<p>We used the RASP interpreter of [Weiss et al., 2021].For RASP model design and details see Appendix 9.2.We use word-level tokens for all RASP model results in this paper, 14 with an "embedding" layer that tags with possible part of speech 15 , and apply just once per encoder pass 19 attention-head compatible flat pattern-matching rules (Figures 2, 5, 6), shown using grammar coverage [Zeller et al., 2023]  to be learnable from the training data 16 , plus general pp/cp handling logic.Each pattern handles "det common_noun" and "proper_noun" identically, a symmetry which is evident in the training data.The model does NOT apply recursive, tree-structured rules like 'np_det pp np -&gt; np_pp -&gt; np'.Consistent with [Zhou et al., 2023] we use [Weiss et al., 2021]'s RASP originally used for modeling Transformer encoders to model an encoder-decoder in a causal way by feeding the autoregressive output back into the program. 17or training Transformers from scratch with randomly initialized weights, we use scripts derived from those provided by [Wu et al., 2024]  18 .See "Appendix: Model Detail" (9.4).</p>
<p>Methods</p>
<p>We use the RASP [Weiss et al., 2021] interpreter 19 to evaluate our RASP programs 20 .Logical forms (LFs) generated by the models were scored by Semantic Exact Match 21 against ground truth.</p>
<p>We also measure grammar coverage [Zeller et al.,  2023] (more detail in Appendix 9.10) by input examples supported by our RASP model against the full grammar of COGS/ReCOGS input sentences provided in the utilities of the IBM CPG project [Klinger et al., 2024] 22 .See "Appendix: Methods Detail" (9.5).See also "Appendix: Results Notebook links by section" (9.1) for notebooks documenting results and giving steps to reproduce.</p>
<p>Results</p>
<p>Restricted Access Sequence Processing -grammar coverage using a flat pattern matching approach (not tree-based and not recursive) and autoregressive decoder loop Figures 2, 5, and 6.See "Appendix: Grammar Coverage Analysis for Design of Restricted Access Sequence Processing Model" (9.11) for more details.We generated 21 sentences based on rules present in the training examples which cover 100% of the COGS input grammar 23 (lexical differences ignored, under the context-free grammar, tree-based assumption which is violated for our non-tree non-recursive model for prepositional phrases, requiring an additional rule to avoid "attraction errors", Figure 7) per [Zeller et al., 2023 The first 19 of those patterns are present in our RASP program code 24 and each correspond to a set of RASP operations corresponding to attention operations in a Transformer to match a template corresponding to that sentence type25 .To handle prepositional phrases in a flat solution, we find it necessary on the training data to add a rule that ignores "det common_noun" or "proper noun" preceded by a preposition when searching for noun indexes to report in relationships (agent, theme, recipient, etc)  and as if we did that during pattern matching by using before/after matches instead of strict relative indexing.Considering how a model without this rule would behave led us to be able to predict 96% of a certain category of errors a baseline Encoder-Decoder Transformer makes (see baseline attraction error results page 6).</p>
<p>Restricted  18 1  7 1  7   1 7 18 1 7 1 7  1 7 18 1 7 1 7   1 7 18 1    on all non-recursive out-of-distribution generalization splits (18922 out of 19000 (95% confidence interval: 99.49% to 99.68%)).See Table 1.Recursion splits are reported below.
1 np_det_seq = [0, 1, 0, 0, 1, 0, 1] np_prop_seq = [0, 0, 0, 0, 0, 0, 0] v_dat_seq = [0, 0, 1, 0, 0, 0, 0] np_det_left_seq = [1, 0, 0, 1, 0, 1, 0] np_two_before_seq = [0, 1, 0, 1, 0, 0, 0] np_before_seq = [0, 0, 1, 0, 1, 0, 0] np_after_seq = [0, 0, 1, 0, 0, 1, 0] (A) = (np_det_left_seq &amp; np_two_before_seq) = [0, 0, 0, 1, 0, 0, 0] (B) = (np_prop_seq &amp; np_before_seq) = [0, 0, 0, 0, 0, 0, 0] np_np_seq = (A or B) = [0, 0, 0, 1, 0, 0, 0] np_np_any_before_seq = [1, 1, 1, 0, 0, 0, 0] np_v_dat_p_np_np = np_after_seq &amp; v_dat_seq &amp; np_before_seq &amp; np_np_any_before_seq = [0, 0, 1, 0, 0, 0, 0]0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 verb embed to part-of-
Restricted Access Sequence Processingprepositional phrase and sentential complement recursion (tail recursive) with a non-tree, nonrecursive approach using the decoder loop 26 Our RASP model's ReCOGS pp_recursion AND cp_recursion gen split scores were both 100% semantic exact match AND string exact match (95% confidence interval (Beta dist/Clopper-Pearson): 99.63% to 100.0%; n=1000 for each).See Table 1.</p>
<p>[ Wu et al., 2024] Encoder-Decoder Transformer from scratch baselines (ReCOGS_pos) [Wu et al., 2024]'s baseline Encoder-Decoder Transformer on ReCOGS_pos had an overall score of 88.55% +/-1.87%Semantic Exact Match accuracy (sample +/-std, n=20) with a 95% confidence interval for the sample mean when n=20 of 87.73% to 89.37%.</p>
<p>[ Wu et al., 2024]'s baseline Encoder-Decoder Transformer's Semantic Exact Match score on the extremely difficult obj_pp_to_subj_pp split of ReCOGS_pos was 19.7% +/-6.1% Semantic Exact Match accuracy (sample +/-std, n=20) with 95% confidence interval for the sample mean with n=20 of 17.0% to 22.4% .</p>
<p>[ Wu et al., 2024]'s baseline Encoder-Decoder Transformer's Semantic Exact Match score on the pp_recursion split of ReCOGS_pos was 40.18% +/-2.07%Semantic Exact Match accuracy (sample +/std, n=20) with 95% confidence interval for the sample mean with n=20 of 36.13 to 44.24%.[Wu et al.,  2024]'s baseline Encoder-Decoder Transformer's Semantic Exact Match score on the cp_recursion split of ReCOGS_pos was 52.40% +/-1.38%Semantic Exact Match accuracy (sample +/-std, n=20) with 95% confidence interval for the sample mean 26 The grammar includes two (tail) recursive aspects, prepositional phrase and sentential complement recursion.</p>
<p>The prepositional phrase recursion comes from the following COGS input grammar rules: 'np -&gt; np_det | np_prop | np_pp' and 'np_pp -&gt; np_det pp np'.</p>
<p>Thus np can be expanded in an unbounded way as follows: 'np -&gt; (np_det pp np) -&gt; np_det pp (np_det pp np) -&gt; np_det pp np_det pp (np_det pp np)' and so on.</p>
<p>However, one sees this is tail recursion and can be handled by a loop that just appends 'np_det pp' until the final 'np' is not 'np_pp'.sentential complement recursion arises from 'np v_cp_taking that start', which can recursively expand as 'np v_cp_taking that start -&gt; np v_cp_taking that (np v_cp_taking that start)', and so on until the nonterminal start expands to some other non sentential complement related nonterminal.</p>
<p>with n=20 of 51.80 to 53.01%.</p>
<p>[ Wu et al., 2024] Encoder-Decoder baseline 2-layer Transformer does not improve on the obj_pp_to_subj_pp split when adding 1 or 2 additional layers (even allowing parameter count to increase) 27 3-layer [Wu et al., 2024] Encoder-Decoder on ReCOGS_pos obj_pp_to_subj_pp split: 16.2% +/-2.7%Semantic Exact Match (sample mean +/-std, n=10) with 95% confidence interval for sample mean (n=10) of 14.6% to 17.9% .4-layer [Wu  et al., 2024] Encoder-Decoder on ReCOGS_pos obj_pp_to_subj_pp split: 19.3% +/-4.1% Semantic Exact Match (sample mean +/-std, n=10) with 95% confidence interval for sample mean (n=10) of 16.8% to 21.8%.</p>
<p>Attraction Error Analysis for [Wu et al.,  2024] baseline Encoder-Decoder Transformer on obj_pp_to_subj_pp split (For additional methods detail see Appendix (9.8).)Of the obj_pp_to_subj_pp split single part errors in single verb sentences made by the [Wu et al.,  2024] baseline Encoder-Decoder Transformer where the agent was to the left of the verb 28 , across n=10 models 29 , 765 out of 767 (99.74%; 95% confidence interval 99.06 to 99.97%) were in the agent part of the logical form (the predicted position for the error).</p>
<p>Critically across all n=10 [Wu et al., 2024] models, for 96.73% (740 out of the previously mentioned 765 above; 95% confidence interval (Beta dist / Clopper-Pearson) 95.21 to 97.87%) of the single point errors in logical forms for single verb sentences where the agent was on the left, modified by a prepositional phrase, and the error was in the agent part, the error in the logical form was that the agent index was accidentally assigned to the specific expected prepositional phrase noun (the one closest to the verb on the left side) instead of the original agent 27 Since no improvement was observed, we did not run the costly experiments to increase the layers while controlling the parameter count (which would be a follow up to distinguish if the improvement was from the layer increase or the parameter increase).</p>
<p>28 Our hypothesis is in terms of nouns with a logical form relationship to a verb or other noun, where the relationship could be agent, theme, or recipient.Since the obj_pp_to_subj_pp split is in terms of subject vs object prepositional modification (instead of agent, recipient, or theme), we use the subset of sentences within this split where the agent is to the left of the verb and pp-modified as it corresponds to the subject in that case.</p>
<p>29 On a per model basis (n=10), the fraction of agent-left single point errors where it was the agent relationship in the logical form that was broken were: [0.985, 1.0, 1.0, 1.0, 1.0, 0.990, 1.0, 1.0, 1.0, 1.0].</p>
<p>noun.(Figure 7) This does not vary much from randomly initialized and trained model to model, with the model-level average at 97.07% of such errors exactly as predicted (stderr=2.23%(n=10)), with 7 of 10 models having 100% of these errors exactly as predicted by our hypothesis30 .The attraction to the nearest noun hypothesis predicts that the offset in the agent index varies with prepositional phrase recursion depth (as at depth &gt; 1, there are multiple attractor prepositional nouns to choose from). 31e report that for all (n=22) single logical form part errors observed (from running n=10 separate Transformer models over the 1000 sentences in the split) where in the input the agent is left of the verb and has a depth=2 prepositional phrase modification in this split, in 100% (95% confidence interval (Beta dist / Clopper-Pearson) 84.6 to 100%; n=22) of those sentences the error in the agent right-index matched our prediction.</p>
<p>[ Wu et al., 2024] Encoder-Decoder Transformer on new v_dat_p2 pp moved to recipient (from theme) split -as hard as hardest previous generalization split See Figure 3 .As the RASP model predicted the 'np v_dat_p2 np pp np np'32 prepositional phrase modification (which involves the recipient instead of the subject so is a distinct check of our hypothesis) would require learning to ignore the distractor "pp det common_noun" and "pp proper_noun" same as required for the obj_pp_to_subj_pp split, we predicted that a new split we introduce "v_dat_p2_pp_moved_to_recipient" would also be difficult for the Transformer.To test this, [Wu et al.,  2024]'s baseline Encoder-Decoder Transformer was trained with default data (ReCOGS_pos train.tsv)and tested on modified v_dat_p2 pp training examples where only the word order was changed to move the prepositional phrase from the theme to the recipient (logical form properly updated see Appendix 9.9 for all examples).The baseline [Wu et al.,  2024] Encoder-Decoder Transformer was only able to achieve a Semantic Exact Match (sample mean +/-sample std) of 13% +/-15.6%(n=10 Transformers trained from scratch with randomly initialized weights and data shuffling) with a 95% confidence interval for the sample mean when n=10 of 4% to 23%.Thus, this new split we introduce here as v_dat_p2_pp_moved_to_recipient is as difficult or perhaps more difficult than the previous reported "hardest split" obj_pp_to_subj_pp.</p>
<p>[ Wu et al., 2024] Encoder-Decoder Transformer trained with data augmented with v_dat_p2 pp moved to recipient (from theme) does NOT improve obj_pp_to_subj_pp performance [Wu et al., 2024]'s baseline Encoder-Decoder Transformer was trained with default data (ReCOGS_pos train.tsv)but with additionally the same modified v_dat_p2 pp training examples used for the "v_dat_p2_pp_moved_to_recipient" split (non-subject recipient modified with prepositional phrase, so nonoverlapping with subj_pp, Figure 3 (b) and (d)) above on which it performed poorly, then tested on the standard prepositional modification generalization split "obj_pp_to_subj_pp", after which it achieved 22% +/-6.7%Semantic Exact Match (sample mean +/-std, n=10) with 95% confidence interval for sample mean n=10 of 17.9% to 26.1% (not significantly different than [Wu et al.,  2024]'s baseline by one-tailed Welch's unequal variances t-test).</p>
<p>Analysis</p>
<p>Our RASP model of a Transformer Encoder Decoder, without tree-based or recursive aspects scored 100% in semantic exact match accuracy on the [Wu  et al., 2024] test set (n=3000), and on the generalization data scored 100% in all but one category (see above) without explicit rules in the RASP program to handle them.This includes 100% semantic exact match accuracy on the prepositional phrase recursion and sentential complement recursion generalization splits up to depth 12 (n=1000 examples each), without any hardcoded prepositional phrase or sentential complement expansion shortcuts added 33 .The RASP program only made a significant number of errors on obj_pp_to_subj_pp which scored only 92.20% Semantic Exact Match (95% confidence interval (Beta dist / Clopper-Pearson): 90.36% to 93.79%) Semantic Exact Match accuracy, much better than [Wu et al., 2024] baseline Encoder-Decoder Transformers which only scored 19.7% +/-6.1% Semantic Exact Match (sample mean +/-std) with 95% confidence interval for the sample mean with n=20 of 17.0% to 22.4% (n=20 separately trained models 33 a single rule applies to all depths; the only limit on length generalization is the RASP interpreter and a simple to extend positional encoding which only handles sentences up to a limit with different random seeds for weight initialization and training data ordering; n=1000 examples used to test each of the n=20 models).Thus, we demonstrated by construction using the Restricted Access Sequence Processing language which can be compiled to concrete Transformer weights that theoretically a Transformer Encoder-Decoder can solve the COGS input to ReCOGS_pos logical form translation in a systematic, compositional, and length generalizing way.</p>
<p>Recall we found a single flat pattern matching rule we originally added to fit training examples, to ignore "pp det common_noun" and "pp proper_noun" when matching nouns for the agent, theme, recipient right indices, was sufficient to avoid structural generalization errors due to pp modification in novel positions.</p>
<p>Interestingly, we imagined ablating that single rule and hypothesized attraction to the nearest noun (Figure 7 and "Appendix: Attraction Errors" (9.6)) in its absence and found this predicted the exact error (the nearest noun to the verb on the expected side is mistaken for the agent of the verb) in 96% of the single relationship errors the [Wu et al., 2024]  baseline Transformers make on the obj-pp-to-subjpp split34 when the agent is left of the verb in single verb sentences (suggesting perhaps that the baseline [Wu et al., 2024] Transformer trained from scratch is also not learning a hierarchical, tree-structured representation.)35Our explanation could have been refuted by other single relationship errors occurring as frequently as the agent, indicating general model confusion (independently getting incorrect agent and theme, not just agent relationships) and/or when making an agent error, the model could have simply put nonsense indices or referred to any other position other than the closest pp noun position to the verb (which does vary and depends on pp depth) to refute our hypothesis.</p>
<p>The flat/non-tree structured hypothesis for the baseline Transformer can also be checked by making a prediction on a completely different syntax affected by the same issue: the 'np v_dat_p2 np pp np np' prepositional phrase modification (which involves the recipient relationship being modified instead of the subject and/or agent so is a distinct check of our hypothesis) 36 and we indeed found that Cannot generalize to extracting meaning of (b) scientist ( 1 ) ; cat ( 4 ) ; donut ( 6 ) ; computer ( 9 ) ; lend ( 2 ) AND agent ( 2 , 1 ) AND recipient ( 2 , 4 ) AND theme ( 2 , 6 ) AND nmod .beside ( 6 , 9 )   scientist ( 1 ) ; cat ( 4 ) ; computer ( 7 ) ; donut ( 9 ) ; lend ( 2 ) AND agent ( 2 , 1 ) AND recipient ( 2 , 4 ) AND theme ( 2 , 9 ) AND nmod .beside ( 4 , 7 )</p>
<p>Figure 3: [Wu et al., 2024] Encoder-Decoder Transformer trained from scratch generalizing to new v_dat_p2 pp moved to recipient (from theme) split is as hard as the previously reported hardest generalization split consistent with the flat/non-recursive/non-tree representation hypothesis ((d) rejects H 0 ). Figure 5 shows how a flat RASP model can recognize (b).</p>
<p>this was as hard or harder than the previous most difficult split analyzed above, the 'obj_pp_to_subj_pp' split (Figure 3).</p>
<p>Maybe [Wu et al., 2024] baseline Encoder-Decoder is depth-constrained to learn a nonhierarchical, flat, non-tree model with these characteristic errors and with more layers it would learn to recursively combine 'np_det pp np -&gt; np_pp -&gt; np' (to some fixed depth at least, probably limited by the number of Transformer blocks) and perform better on prepositional phrase related splits 37 .However, training a [Wu et al., 2024] baseline Encoder-Decoder Transformer from scratch we found no benefit to 3 or 4 layers instead of 2 on the ReCOGS obj_pp_to_subj_pp split, consistent with [Petty et al.,  2024]'s finding on COGS.</p>
<p>37 [Csordás et al., 2022]: "the network should be sufficiently deep, at least as deep as the deepest data dependency in the computational graph built from elementary operations (e.g., in the case of a parse tree, this is the depth of the tree)".(This is not a very scalable approach, as we must make the network deeper to handle increased pp depths instead of just looping or at least reusing weights each layer.)</p>
<p>Taken together, these results and the grammar coverage analysis suggest we may interpret the poor performance on generalizing on unseen prepositional phrase related modification related splits as arising from the baseline 2 to 4 layer Encoder-Decoder Transformers learning a non-tree, nonrecursive representation that cannot leverage the grammar rule 'np_det pp np -&gt; np_pp -&gt; np' during learning and which requires them to instead actually observe more of the various prepositional phrase substitutions to learn them. 38</p>
<p>Conclusion</p>
<p>Implementing our task in Restricted Access Sequence Processing immediately helped us discover additional failure modes (e.g.new "v_dat_p2_pp_moved_to_recipient" split 39 ) of [Wu et al., 2024]'s baseline Encoder-Decoder Transformer, predict errors in detail made in the logical forms, and demonstrated that treestructured/recursive representations are not needed to cover the recursive aspects of the COGS/ReCOGS grammar 40 .We recommend others to consider RASP to understand Transformer behavior even for more complicated tasks like ReCOGS.We predict that Transformers should be able to perform the ReCOGS task (even structural generalization splits) with high accuracy and that the problem is just of getting the Transformer to learn the appropriate rules 41 , turning attention to data augmentation 42 , curriculum learning [Bengio et al.,  2009], reinforcement learning [Ranzato et al., 2016], training objectives [Ahuja et al., 2024], and other approaches [Csordás et al., 2021] [Ontanon et al.,  2022]. 38See "Appendix: Composition and Learning" (9.13) 39 After this paper was written we found our predicted split "v_dat_p2_pp_moved_to_recipient" has also been added to an extended separate (SLOG) version of COGS (upstream of ReCOGS) recently in [Li et al., 2023] see their section 2.2.1 indirect object modification (4), confirmed difficult 40 same input grammar is used by COGS and ReCOGS 41 e.g. to ignore "pp det common_noun" and "pp proper_noun" when finding nouns in relationships with verbs, which allows the RASP model to get 100% on the ReCOGS test set, and 100% two of the three structural generalizations, and 92% on the obj-pp-to-subj-pp split 42 We tried one augmentation, see Results and Appendix 9.9, but there are many other possibilities.</p>
<p>Known Project Limitations</p>
<p>The Restricted Access Sequence Processing code is not optimized.Cannot yet predict attention heads and layers required from the select and aggregate operations performed like the RASP authors [Weiss  et al., 2021] were able to do with their problems.</p>
<p>Grammar coverage [Zeller et al., 2023] is only valid when the expansions are rules your model can learn. 43We specifically made use of this limitation in this paper but still caution anyone about it who might just take the grammar coverage metric away by itself.</p>
<p>The attraction error analysis of the [Wu et al.,  2024] baseline Encoder-Decoder Transformer on the obj_pp_to_subj_pp split does not yet attempt to explain the common case of multiple errors in the logical form. 44e only provide and discuss a RASP solution for ReCOGS [Wu et al., 2024] here, not the semantically equivalent COGS 45 [Kim and Linzen, 2020], though as this goes to publication we have just separately released a RASP model for COGS at https://github.com/willy-b/RASP-for-COGS/ 46, which is undergoing evaluation with preliminary data supporting the same conclusions for that task (can also be solved by a non-tree structured, nonhierarchical Transformer compatible model, despite using Exact Match instead of Semantic Exact Match, with same RASP for the Encoder as used here for ReCOGS_pos but a different Decoder).</p>
<p>Much deeper Transformer networks may be learning a tree-based grammar representation 47 and 43 If for example, as with our flat RASP model by design or as we hypothesize for [Wu et al., 2024]'s baseline Encoder-Decoder Transformer, the model cannot or will not learn the rule 'np_det pp np -&gt; np_pp -&gt; np' which recursively replaces noun phrases modified by prepositional phrases with a noun phrase, then grammar coverage will still assume any prepositional phrase exposure is sufficient, which is evidently not true given the errors on prepositional phrase modification generalization splits reported here and by [Wu et al., 2024], [Kim and  Linzen, 2020].</p>
<p>44 e.g.agent index may be replaced by prepositional phrase noun but also a spurious theme relationship is added or the theme index is also corrupted 45 nor the recently introduced extended version SLOG [Li  et al., 2023]  46 RASP-for-COGS also supports case-sensitive string exact match and ignoring out-of-vocabulary words, features not supported in RASP-for-ReCOGS 47 Nothing explored here rules that out and there is plenty of evidence outside the COGS task-related literature suggesting this will be the case: [Tenney et al., 2019] show the 24-layer BERT model seems to handle "POS tagging, parsing, NER, semantic roles, then coreference"; [Hewitt and Manning, 2019]  "provid[e] evidence that entire syntax trees are embedded im-not suffer from the predicted generalization issues observed in [Wu et al., 2024]'s baseline 2-layer Transformer and predicted by our intentionally nontree RASP model (if compensating rules to avoid attraction errors in a flat model are not also learned).</p>
<p>As stated elsewhere in the paper, we are focused on the structural generalizations that Transformers currently struggle with (prepositional phrase recursion (pp recursion), sentential complement (cp) recursion, object-pp to subj-pp modification generalization (obj-pp-to-subj-pp)) and not lexical generalizations (which Transformers are demonstrated to do well on by others).Thus, as noted elsewhere, our model assumes the embedding can map words to their possible part-of-speech and verb-type and does not address the learnability of this mapping (number of exposures required, etc).</p>
<p>The ReCOGS task is English specific and our findings on compositional generalization may not necessarily apply (or apply differently) to other languages.Our description of "attraction errors" for example does not appear specific to subject-verb-object ordering in English but a similar analysis should be conducted in other languages and we used the obj-ppto-subj-pp split in our detailed error analysis which is subject-verb-object ordering sensitive.</p>
<p>The RASP-for-ReCOGS model applied to unintended use WILL give invalid results or haltwe have NOT provided a general language model, we have provided a simulation of how a Transformer could perform a specific task.The RASPfor-ReCOGS model/simulation as provided is for research purposes only to prove feasibility of the ReCOGS task by Transformers and is not appropriate for ANY other uses whatsoever without modification.For one, an actual Transformer performing the equivalent operations would run orders of magnitude faster, which should be reason enough to not want to use the RASP simulation for actual input-output tasks outside of a research setting.However, there is also no "tokenizer" provided for the RASP model to plicitly in deep models' [including BERT's] vector geometry", and [Goldberg, 2019] shows BERT excels at subject-verb agreement, "which [is] traditionally taken as evidence for the existence [of] hierarchical structure" (though e.g. in this work we see that ignoring distractor nouns in long-term dependencies does not require hierarchy or a deep understanding of syntax but simple rules like ignore "pp det common_noun" and "pp proper_noun" for finding noun-verb relationships in the logical form can allow for handling of such long-range dependencies).On the other hand, [Petty et al., 2024] argue specifically for the COGS benchmark (semantically equivalent to ReCOGS which is derived from it) that increasing depth does not allow their Transformers to make progress on the structural generalization splits, even at depths up to 32 layers.</p>
<p>handle out-of-vocabulary inputs and fallback paths for out-of-grammar examples are not provided so the RASP model will halt on most inputs and can only run on the in-distribution (non-augmented) training data, and the dev, test, and gen sets of ReCOGS, though such aspects could be added.We provide the code for reproducing the results of this study and for researchers who are capable of writing RASP themselves to build upon the work and/or more easily apply RASP to their own problems given our examples, not for immediate application to any other tasks without appropriate modification.</p>
<p>See also "Appendix: Potential Risks" (9.14) and "Appendix: Scientific Artifacts -Is Our Use Consistent with Authors' Intention and Licensing" ( 9.15) for other potential concerns.</p>
<p>Notes</p>
<p>No AI tools were used by the author in the preparation of this manuscript with the exception of anything used in the backend by Google Scholar searches for literature and citations and Google searches for related material.AI writing aids were not used.For steps to reproduce and results, see the RASP model ReCOGS generalization set notebook (link) 49 .</p>
<p>9.1.3[Wu et al., 2024] Encoder-Decoder Transformer from scratch baselines (ReCOGS_pos)</p>
<p>See the Encoder-Decoder Transformer from scratch baselines notebook (link) 50 for [Wu et al., 2024] script execution and analysis code.</p>
<p>9.1.4[Wu et al., 2024] Encoder-Decoder baseline 2-layer Transformer does not improve on the obj_pp_to_subj_pp split when adding 1 or 2 additional layers 3 and 4 layer results can also be found in that same notebook, which is also the the baseline Encoder-Decoder Transformer 3 or 4 layer variation results notebook (link) 51 (scroll down).</p>
<p>Restricted Access Sequence Processing word-level token program/model design</p>
<p>For overall model descriptions, first see Model Detail 9.4.For a description of the Restricted Access Sequence Processing (RASP) language, which is theoretically compilable to Transformer neural network weights, see [Weiss et al., 2021].</p>
<p>You can run a demo of our RASP model and see the autoregressive output git clone https://github.com/willy-b/learning-rasp.gitpython recogs_examples_in_rasp.py</p>
<p>The script will show performance on [Wu  et al., 2024] ReCOGS_pos data by default, run with "-use_dev_split", "-use_gen_split" , or "-use_test_split" to see it run on those and give a running score every row.</p>
<p>For ReCOGS, intending to perform well on Semantic Exact Match, we took a simple, flat, non-tree, non-recursive approach which was able to get 100% semantic exact match (and string exact match) on the full test set, and 99.6% semantic exact match on the out-of-distribution generalization set of the real ReCOGS dataset 53 .</p>
<p>We use word-level tokens for all RASP model results in this paper. 54.We took the RASP native sequence tokens at the word-level, and first did a Transformer learned-embedding compatible operation and created 1 part-of-speech and 4 extra verbtype sequences (because each word in the COGS vocabulary may actually serve multiple POS roles; up to four different verb types as in the case of "liked" which can serve as v_trans_not_omissible, v_trans_not_omissible_pp_p1, v_trans_not_omissible_pp_p2, and v_cp_taking types).</p>
<p>The five extra sequences serve to associate each word with one or more of the following part-ofspeech/verb type roles:</p>
<p>det: 1 pp: 2 was: 3 by: 4 to: 5 that: 6 common_noun: 7 proper_noun: 8 v_trans_omissible: 9 v_trans_omissible_pp: 10 v_trans_not_omissible: 11 v_trans_not_omissible_pp: 12 53 word-level token Restricted Access Sequence Processing solution:</p>
<p>https://github.com/willy-b/learningrasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/wordlevel-pos-tokens-recogs-style-decoder-loop.rasp 54We believe any solution at the word-level can be converted to a character-level token solution and that is not the focus of our investigation here (see Appendix 9.3 for proof of concept details on a character level solution not used here).18 1  7 1  7   1 7 18 1 7 1 7  1 7 18 1 7 1 7   1 7 18 1   For those used to multidimensional representations, one can think of these as one-hot vectors of dimension 20 55 and replace equality checks with vector dot product (and a check for either com-mon_noun or proper_noun would be a dot product with the sum of one-hot 20 dimensional vectors given by (common_noun + proper_noun)) but we keep it simple for human reading here and use these scalars in 1d as is easily supported by RASP.
1 np_det_seq = [0, 1, 0, 0, 1, 0, 1] np_prop_seq = [0, 0, 0, 0, 0, 0, 0] v_dat_seq = [0, 0, 1, 0, 0, 0, 0] np_det_left_seq = [1, 0, 0, 1, 0, 1, 0] np_two_before_seq = [0, 1, 0, 1, 0, 0, 0] np_before_seq = [0, 0, 1, 0, 1, 0, 0] np_after_seq = [0, 0, 1, 0, 0, 1, 0] (A) = (np_det_left_seq &amp; np_two_before_seq) = [0, 0, 0, 1, 0, 0, 0] (B) = (np_prop_seq &amp; np_before_seq) = [0, 0, 0, 0, 0, 0, 0] np_np_seq = (A or B) = [0, 0, 0, 1, 0, 0, 0] np_np_any_before_seq = [1, 1, 1, 0, 0, 0, 0] np_v_dat_p_np_np = np_after_seq &amp; v_dat_seq &amp; np_before_seq &amp; np_np_any_before_seq = [0, 0, 1, 0, 0, 0, 0] 7 7 1 70 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 verb embed to part-of
Each of the five sequences comes from a separate map, since in RASP a map could only have a single value per key, and since individual words had up to four different verb roles (as in "liked" which had 4).
0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 verb 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0
Figure 5: Example RASP model flat grammar pattern matching, for np v_dat_p2 np np, for a matching sentence, despite pp modification of middle recipient noun.This is in the encoder.See also Figure 7 for how the RASP model avoids attraction errors in assigning agents, recipients, themes due to prepositional phrase modification in the decoder.</p>
<p>Upon these five parallel, aligned, sequences we used a series of attention head compatible operations to recognize multi-token patterns (see below) corresponding to grammatical forms (listed below).These patterns are not causal because their use/input/output is masked to the input section of the sequence, so would take part in the Encoder of the Encoder-Decoder only (all operations outside the input mask in the word-level token RASP solution used in this paper are directly or indirectly causally masked and we built symbol by symbol in a causal autoregressive way).We could have added an explicit causal mask to each operation but for efficiency and simplicity of the code omitted it when we are doing it implicitly by taking only the last sequence position (we also acausally aggregate so that all sequence positions have the same value as the last sequence position to make it easier to read the output -RASP interpreter will just print it as one position if they are all equal and we only take one position).</p>
<p>Also, the author thinks many of these RASP steps could be consolidated.The goal here was to first prove by construction that a non-recursive, flat RASP program could get approximately 100% Semantic Exact Match on all the ReCOGS generalization splits (we only missed one split, obj-pp-to-subjpp by a little (see results), which we believe was due to a mistake made rushing due to two week time constraint for original model implementation, not a fundamental limitation of the RASP approach).</p>
<p>Introduction of variables at the beginning of the ReCOGS logical form (e.g. in the logical form for "a boy painted the girl", we have "boy ( 1 ) ; * girl ( 4 ) ; paint ( 2 ) AND agent ( 2 , 1 ) AND theme ( 2 , 4 )" , the variable introduction is "boy ( 1 ) ; * girl ( 4 ) ; paint ( 2 )" before the "AND").We took a simple approach and sorted the input sequence with nouns before verbs and determiners, fillers last (with determiners and fillers not having any corresponding entry in the output sequence).We then count nouns and verbs in the input and count nouns and verbs in the output and determine if we have introduced all the nouns and verbs.</p>
<p>See code for full details 56 (for simplicity this description was written without discussing sentential complement (CP) handling).</p>
<p>Example counting how many nouns and verbs we have output (introduced as variables) so far (to determine what we need to output for next token): How variables are introduced with their index (omitted sorting of input and other operations that can be read in the code and are less important; anything acausal is restricted to input sequence section (Encoder)): (only value at last sequence position is used; causal) Note that "normalize_nv" is a lookup into a map that has no effect unless the word is a verb in which case it normalizes it to a standard suffix ("ate" to "eat" , "painted" to "paint", etc).</p>
<h1></h1>
<p>As you can see above, if we have not introduced all the variables, we determine our index into the sorted list of nouns and verbs (nouns before verbs), and using a MLP modeling modulus, compute index mod 5 (alternatively, number of tokens since last separator) and condition on that to output that noun/verb or parentheses or index as prediction for next token at last sequence position (all other sequence positions are ignored).Since we do ReCOGS_pos (semantically identical to random indices but avoid requiring random numbers) the index we use is the index of the original noun or verb in the original sequence.If we are still introducing variables, that is the end and we have our prediction for the next token.</p>
<p>If we are done introducing variables at that point in the Decoder loop, we move on, and use templates that the attention head compatible operations in the Encoder recognized for us in the five parallel part-of-speech / verb-type per location sequences for "v_trans_omissible_p1", "v_trans_omissible_p2", "v_trans_omissible_pp_p1", "v_trans_omissible_pp_p2", "v_trans_not_omissible", "v_trans_not_omissible_pp_p1", "v_trans_not_omissible_pp_p2", "v_cp_taking", "v_inf_taking", "v_unacc_p1", "v_unacc_p2", "v_unacc_pp_p1", "v_unacc_pp_p2", "v_unerg", "v_dat_p2", "v_dat_pp_p1", "v_dat_pp_p2", "v_dat_pp_p3", "v_dat_pp_p4".</p>
<p>To be clear, we always compute all variables (noun and verb introduction, verb relationships, nmods) but depending on the number of nouns/verbs, verb relationships, nmods detected in the output so far, variables from "completed" or "premature" phases are discarded and the next predicted token is given by variables associated with the appropriate phase, here the verb relationship phase.</p>
<p>Here are a couple of examples of patterns the Encoder recognizes, to see how it looks if we support 1 verb pattern per input (no sentential complement recursion; which can be easily handled how we handle other things we loop over, looping over current phrase and masking and processing), which is sufficient to get approximately 100% on all entries that do not use sentential complements (e.g."the girl noticed that a boy painted" is not supported in this example but "a boy painted" is):</p>
<h1>define the pattern # ... \ # (just showing one example, # np_prop_mask and # np_before_mask # are attention masks defined earlier) # np v_dat_p2 np np # e.g.[8,18,1,7,1,7]  set example ["ella","sold","a","customer","a","car"] np_np_sequence = \ aggregate((np_prop_mask and np_before_mask) or (np_det_left_mask and np_two_before_mask), 1); # would not support prepositional phrase modification on middle NP #np_np_before_mask = \ # select(np_np_sequence, 1, ==) and select(indices-1, indices, ==); np_np_any_before_mask = \ select(np_np_sequence, 1, ==) and select(indices, indices, &gt;); # acausal is ok \ # in INPUT sequence (encoder part, not decoder), \ # would mask further if we wanted to do multiple templates per input or \ # something outside the supported grammar (COGS without sentential complement \ # recursion is supported here) np_np_any_before_sequence = \ aggregate(np_np_any_before_mask, 1); np_np_any_before_mask = \ select(np_np_any_before_sequence, 1, ==) and select(indices, indices, ==); np_v_dat_p_np_np = \ aggregate(np_after_mask and v_dat_mask and np_before_mask and np_np_any_before_mask, 1); # Example: np_v_dat_p_np_np(\ # ['ella', 'sold', 'a', 'customer', 'a', 'car']) \ # = [0, 1, 0, 0, 0, 0] (ints) # Example: np_v_dat_p_np_np (\  # [8, 18, 1, 7, 1, 7]) \ # = [0, 1, 0, 0, 0, 0] (ints) # ... The rest of this applies to just values used from the last sequence location (output is prediction for next symbol).</h1>
<h1></h1>
<p>Based on the template recognized, we lookup the template size for number of relationships (theme, recipient, agent) for that verb type:
def template_size(template_name) {</p>
<h1>number of items to output in verb relationship phase # after noun and verb introduction phase # (special exception is 2-verb v_inf) template_sizes = { "": 0, "v_trans_omissible_p1": 1, "v_trans_omissible_p2": 2, "v_trans_omissible_pp_p1": 1, "v_trans_omissible_pp_p2": 2, "v_trans_not_omissible": 2, "v_trans_not_omissible_pp_p1": 1, "v_trans_not_omissible_pp_p2": 2, "v_cp_taking": 2, # (NOTE: comments within the map should be removed) # v_inf_taking is a special 2-verb case, 5 items # after noun introduction # (verb 1, agent 1, xcomp verb 1 to verb 2, verb 2, agent 2) # if first verb were included # in introduction phase for v_inf, # then it would be 4 # indeed the last map used is template_mapping4 "v_inf_taking": 5, # n.b. if we output out of order # (as allowed by Semantic Exact Match) # and put both verbs in beginning # (verb 1, verb 2, agent 1, xcomp verb 1 to verb 2, agent 2) # then the count would be 3 # as we could do normal combined noun and verb intro # but if doing String Exact Match, ReCOGS LF has # verb 2 after the xcomp for v_inf_taking "v_unacc_p1": 2, "v_unacc_p2": 1, "v_unacc_pp_p1": 1, "v_unacc_pp_p2": 2, "v_unerg": 1, # "v_inf": 1, "v_dat_p1": 3, "v_dat_p2": 3, "v_dat_pp_p1": 2, "v_dat_pp_p2": 3, "v_dat_pp_p3": 2, "v_dat_pp_p4": 3 }; return template_sizes[template_name]; } Details are in the learning-rasp GitHub RASP code 57 , but we compute at the last sequence position (in parallel) the number of relationships output for the verb so far, and for the current relationship which token within that multi-token process (e.g. the word "agent" or the open parenthesis "(" or the left index, or the comma, or right index, close parenthesis ")", "AND", etc) we are on.</h1>
<p>Like we computed at the last sequence position the number of nouns and verbs in the output once we are finished introducing nouns and verbs (this would be a little different with sentential complements (see actual code for CP support)), we compute the number of agent,theme,recipient,xcomp entries in the output: We use all those different values which are computed independently from the input sequence and so do not add depth/layer requirements as many of the ones which involve accessing the sequence can be done in parallel.We then use the verb-type and relationship index to the relationship into a map to get the current relationship to output (as some verb types output agent first, some output theme, etc):
template_mapping1 = {
"": "", "v_trans_omissible_p1": "agent", "v_trans_omissible_p2": "agent", "v_trans_omissible_pp_p1": "theme", "v_trans_omissible_pp_p2": "theme", "v_trans_not_omissible": "agent", "v_trans_not_omissible_pp_p1": "theme", "v_trans_not_omissible_pp_p2": "theme", "v_cp_taking": "agent", "v_inf_taking": "agent", "v_unacc_p1": "agent", "v_unacc_p2": "theme", "v_unacc_pp_p1": "theme", "v_unacc_pp_p2": "theme", "v_unerg": "agent", "v_inf": "agent", "v_dat_p1": "agent", "v_dat_p2": "agent", "v_dat_pp_p1": "theme", "v_dat_pp_p2": "theme", "v_dat_pp_p3": "recipient", "v_dat_pp_p4": "recipient" };</p>
<p>Outputting the verb relationships we must skip over any "pp np" as possible agents, themes, or recipients to avoid getting confused by noun phrases added by prepositional modification (believed by the author to be a cause of the issue with obj pp to subj pp generalization by [Wu et al., 2024]'s Transformer).# the one verb (except v_inf_taking cases) left_idx_in_nvs_zero_based = nv_in_input_count-1; # (after sentential complements, not covered in this example, see actual code via link above) # need to also subtract the index in ReCOGS for the 2nd verb if it is v_inf_taking left_idx_in_nvs_zero_based = (left_idx_in_nvs_zero_based-1) \ if (template_name == "v_inf_taking" and after_intro_idx &lt;= 2) else left_idx_in_nvs_zero_based; left_idx = aggregate(select(indices, left_idx_in_nvs_zero_based, ==), input_indices_sorted); # avoids attractor nouns right_idx = aggregate(select(nps_without_pp_prefix_indices, after_intro_idx, ==), indices); # points to 2nd verb for xcomp for v_inf_taking_v_inf # note, this simplified example ignores sentential complement (CP) handling # (ideally this would be verb specific, # we simplify here to reuse variables After outputting all verb relationships, we consider whether we have prepositional phrase noun modifiers to record in the logical form.That is to say, if the current output sequence already includes the verb relationships expected for the input (the count matches), then those Decoder variables discussed earlier for verb relationships are still computed, but discarded, and the next token predicted will be overridden with a prepositional phrase noun modifier related output given by a RASP path similar to that below.</p>
<p>For outputting prepositional relationships ("nmod .in", "nmod .on", "nmod .beside"), we do a similar approach, counting prepositional phrases in the input, counting how many nmods we have output, figuring out which one is currently being output: pps_in_input_sequence = INPUT_MASK<em>(indicator(pos_tokens == 2)); pps_in_input_count = selector_width(select(pps_in_input_sequence, 1, ==)); pps_index = pps_in_input_sequence</em>selector_width(select(pps_in_input_sequence, 1, ==) \ and select(indices,indices, &lt;=)); nmods_and_pps_in_output_sequence = \ OUTPUT_MASK<em>(indicator(tokens == "nmod .in" or tokens == "nmod .beside" or tokens=="nmod .on")); nmods_and_pps_in_output_count = \ selector_width(select(nmods_and_pps_in_output_sequence, 1, ==)); current_pp = \ aggregate(select(pps_index, nmods_and_pps_in_output_count+1, ==), tokens) \ if pps_in_input_count &gt; 0 else ""; current_pp = "" if current_pp == 0 else current_pp; current_nmod_token = \ ("nmod ." + current_pp) if (pps_in_input_count &gt; 0 and not (current_pp == 0) \ and after_intro_num_tokens_in_output_excluding_asterisks % 7 == 0) else ""; current_nmod_token = \ "(" if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 1 else current_nmod_token; current_nmod_token = \ (aggregate(select(pps_index, nmods_and_pps_in_output_count, ==), indices)-1) if pps_in_input_count &gt; 0 \ and after_intro_num_tokens_in_output_excluding_asterisks % 7 == 2 else current_nmod_token; current_nmod_token = "," \ if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 3 else current_nmod_token; after_nmod_idx = \ aggregate(select(pps_index, nmods_and_pps_in_output_count, ==), indices)+1; token_at_after_nmod_idx = \ aggregate(select(indices, after_nmod_idx, ==), tokens); after_nmod_idx = \ (after_nmod_idx + 1) \ if (token_at_after_nmod_idx == "the" or token_at_after_nmod_idx == "a") else after_nmod_idx; current_nmod_token = (after_nmod_idx) \ if pps_in_input_count &gt; 0 \ and after_intro_num_tokens_in_output_excluding_asterisks % 7 == 4 else current_nmod_token; current_nmod_token = ")" \ if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 5 \ else current_nmod_token; current_nmod_token = \ ("AND" if nmods_and_pps_in_output_count &lt; pps_in_input_count else "") \ if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 6 \ else current_nmod_token; after_intro_and_relationships_nmod_token = \ current_nmod_token if nmods_and_pps_in_output_count &lt;= pps_in_input_count else ""; num_tokens_in_nmod_section = \ after_intro_num_tokens_in_output_excluding_asterisks -template_size(template_name)</em>7 + 1; # the decision of whether the nmod output dominates the current decoder next predicted token # is computed similarly to the following (see GitHub for actual) # we have not included the computation of "offset" and "after_intro_num_tokens_in_output_excluding_asterisks" output = after_intro_and_relationships_nmod_token \ if (template_mapping_output == "" and \ after_intro_num_tokens_in_output_excluding_asterisks &gt;= \ template_size(template_name)<em>7+offset -1 and \ num_tokens_in_nmod_section &lt; 7</em>pps_in_input_count and pps_in_input_count &gt; 0 \ ) else output;</p>
<p>Again, see the code for full details 58 (for simplicity this description was also written without discussing sentential complement handling). 58word-level token Restricted Access Sequence Processing solution:</p>
<p>https://github.com/willy-b/learningrasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/wordlevel-pos-tokens-recogs-style-decoder-loop.raspFor all steps only the RASP outputs aligned with the input sequence (Encoder part of derived Transformer) or the very last sequence output (for next token in autoregressive generation) were used.For convenience of reading the aggregate operator was usually used acausally to assign all sequence outputs before the last one to the same value as the last (so only one value would be displayed).</p>
<p>You Note, a proof of concept character level Restricted Access Sequence Processing model was done with a decoder loop (unlike word-level solution above, it was a sketch so did not limit to strictly causal operations which just require more careful indexing -using the value at the separator or the end of a word instead of pooling the same value to all letters in a word for example).Note that this one did not cover translating sentences in general into ReCOGS unlike the word-level solution as it is tedious and redundant but the core operations are possible and the author believes any solution at the word level can be mapped to a solution in character level tokens (out of scope for this paper to prove it).</p>
<p>Since it is a separate problem and adds a lot of complexity without bringing anything to bear on the main questions of the paper, I left a full implementation to the word-level tokens which were simpler and ran faster.The difference is one uses a similar approach started at 59 to assign all the letters in each word an index.</p>
<p>Word indices can be assigned using RASP to count separators occurring prior to each sequence location like:</p>
<p>(we also zero out the word index for the separators themselves) word_indices = \ (1+selector_width(select(tokens, " ", ==) \ and select(indices, indices, &lt;=))) \ *(0 if indicator(tokens == " ") else 1);</p>
<p>Then one can do an aggregation of the letters grouping by word index (this, which is NOT part of the techniques used in this paper for the word-level tokens solution, requires additional work (tedious not challenging) to do causally outside the input (in the decoder), one must sum forward so the word representation is always at the last letter of the word or separator instead of at all letters of the word, and that step is left out of the character-level demo and this discussion; whereas the word-level solution described above has a clear Encoder Decoder separation.This can be done so that the value which is then the same for all letters in each word, is unique to each word in the dictionary and can be looked up in 59 https://github.com/willy-b/learningrasp/blob/63f0df56339b81dd24fe1fd31576f3ae02b97763/otherexamples/decoder-loop-example-parse-into-recogs-stylevariables.rasp#L2 a map to get word level attributes like part-of-speech and get back to the solution in the word-level tokens in Appendix 9.2 which was fully implemented.A simple approach (not necessarily recommended but works for proof of concept) that would work for small vocabularies (easily extended) is to use a map to lookup each letter of the alphabet to a log prime.Then the sum of the letters in a word (grouped by the word index which is the count of spaces/separators prior) is the sum of the log primes indexed by the alphabet index.Since the sum of logarithms of numbers is the same as the logarithm of the product of those numbers, this is equivalent to the logarithm of the product of a series of primes.Each prime in the product corresponds 1-to-1 to a letter in the alphabet, with the number of occurrences in the product corresponding to the number of times that letter occurs in the word.By uniqueness of prime number factorization this would map each multiset of letters to a single unique sum of log primes.Thus if you do not have words which are anagrams, all the letters in each word would be assigned a number that uniquely represented that word in the vocabulary.If you have anagrams you can do this step and then take the first and last letter and compute a separate number from that and add it to all the letters in the word.</p>
<p>Example lookup table for letters before aggregating by word index (not recommended but for proof of concept that one can go from character level tokens to word-specific numbers which can then be looked up as in the word-level token solution in Appendix 9.2 used throughout the paper):</p>
<p>def as_num_for_letter_multiset_word_pooling(t) { # To be multiset unique, need logarithm of prime so that the sum aggregation # used in RASP corresponds to prime number factorization (sum of logs of primes is same as log of product of primes) # (we can do sum aggregation instead of mean by multiplying by length) # However RASP does not appear to support logarithms (underlying multilayer # perceptron can learn to approximate logarithms) #letter_to_prime_for_multiset_word_pooling = {"a": 2, "b": 3, "c": 5, "d": 7, #"e": 11, "f": 13, "g": 17, "h": 19, "i": 23, "j": 29, "k": 31, "l": 37, #"m": 41, "n": 43, "o": 47, "p": 53, "q": 59, "r": 61, "s": 67, "t": 71, #"u": 73, "v": 79, "w": 83, "x": 89, "y": 97, "z": 101, ".": 0, #" ": 0, ":": 0}; map_letter_to_log_prime_for_pooling = {"a": 0.6931471805599453, "b": 1.0986122886681098, "c": 1.6094379124341003, "d": 1.9459101490553132, "e": 2.3978952727983707, "f": 2.5649493574615367, "g": 2.833213344056216, "h": 2.9444389791664403, "i": 3.1354942159291497, "j": 3.367295829986474, "k": 3.4339872044851463, "l": 3.6109179126442243, "m": 3.713572066704308, "n": 3.7612001156935624, "o": 3.8501476017100584, "p": 3.970291913552122, "q": 4.07753744390572, "r": 4.110873864173311, "s": 4.204692619390966, "t": 4.2626798770413155, "u": 4.290459441148391, "v": 4.3694478524670215, "w": 4.418840607796598, "x": 4.48863636973214, "y": 4.574710978503383, "z": 4.61512051684126, # we zero out tokens we want not to affect the identity of the word ".": 0, " ": 0, ":": -1, "(": -1, ")": -1, "0": -1, "1": -1, "2": -1, "3": -1, "4": -1, "5": -1, "6": -1, "7": -1, "8": -1, "9": -1, ";": -1, ",": -1}; return map_letter_to_log_prime_for_pooling [t]; } Pooling by word can then be done with: pseudoembeddedwords = \ aggregate(select(word_indices, word_indices, ==), \ as_num_for_letter_multiset_word_pooling(tokens))*word_lengths;</p>
<p>(Per-character token example is not causally masked, we do causal strict-decoder-compatible solution for anything outside input sequence in the full wordlevel solution above just leaving out of this characterlevel sketch, which is NOT used in this paper.For the causal character level solution one would use the summed value at the end of the word or the separator instead, indexing relative to separators.)</p>
<p>Those values could then be looked up in a dictionary like in the completed word-level token solution to get part-of-speech, verb-type, etc, to derive a separate sequence which can be used for template matching as we successfully did with word-level tokens (see Appendix 9.2).</p>
<p>Model Detail</p>
<p>For our Restricted Access Sequence Processing ReCOGS program, we used the RASP interpreter of [Weiss et al., 2021] to run our program.For RASP model design and details see Appendix 9.2.We use word-level tokens for all RASP model results in this paper. 60Consistent with [Zhou et al.,  2023] we use [Weiss et al., 2021]'s RASP originally used for modeling Transformer encoders to model an encoder-decoder in a causal way by feeding the autoregressive output back into the program.We only have aggregations with non-causal masks when that aggregation (or without loss of generality just before the aggregation product is used to avoid multiplying everywhere) is masked by an input mask restricting it to the sequence corresponding to the input. 61e used RASP maps to map word level tokens to part-of-speech and verb-type which is consistent with what can be learned in embeddings or the earliest layer of a Transformer [Tenney et al., 2019] 62 and then did 19 different attention-head based template matches on that flat sequence 63 (no tree-based parsing, no recursive combination of terminals/nonterminals.)Those 19 templates were constructed using grammar coverage [Zeller et al., 2023] to cover the ReCOGS/COGS input grammar as demon- 60 We believe any solution at the word-level can be converted to a character-level token solution and that is not the focus of our investigation here (see Appendix 9.3 for proof of concept details on a character level solution not used here).</p>
<p>61 An example the author has prepared of this is available at https://github.com/willy-b/learningrasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/ recogs_examples_in_rasp.py .</p>
<p>62 [Tenney et al., 2019] report part-of-speech information is already tagged in layer 0 (post-embedding) of the 24-layer BERT large pre-trained language model, trained using a masked language modeling objective.Though models for COGS/ReCOGS are usually trained using a sequence-to-sequence (seq2seq) objective (whether that objective biases the Transformer to learn the same representation on this task is not known to our knowledge), one could also use a language modeling objective to model the COGS input text and its associated logical form output (not just the output conditioned on the input).See [Ahuja  et al., 2024] for examples of solving the same language tasks using seq2seq vs various language modeling objectives -they indeed find better generalization performance on their problems when using the language modeling objective (training to model both the input and the output).</p>
<p>63 A flat/non-tree solution was pursued because it was simple and given the failure documented in [Wu et al., 2024] of the baseline Encoder-Decoder to generalize from obj_pp_to_subj_pp and other evidence we give below we shall see it is hard to argue a tree-based solution which includes the rule 'np_det pp np -&gt; np_pp -&gt; np' is learned by [Wu et al., 2024]'s baseline Encoder-Decoder Transformer.strated in the training data (see "Appendix: Restricted Access Sequence Processing word-level token program/model design" (9.2), and see Table 2 for patterns and equivalent ReCOGS training examples). 64.</p>
<p>For the vocabulary we used the [Klinger et al.,  2024] description of COGS in their utilities 65 (same input as ReCOGS) (NOT using their CPG solution or model anywhere) in constructing our RASP vocabulary and part-of-speech or verb-type embedding/mapping.</p>
<p>We are focused on structural, not lexical generalizations, so same as in [Klinger et al., 2024]  we include all words occurring anywhere in the upstream (Re)COGS "train.tsv"(including "exposure" rows, though would not change results qualitatively to omit the very few words only occurring in exposure examples).We also include two words in our vocab/embedding as common nouns accidentally left out of train.tsvvocabulary by the COGS author: "monastery" and "gardner" (only included in their train_100.tsvand dev.tsv not also in train.tsv,but present in test/gen), a decision affecting just 22 or 0.1% of generalization examples so would not affect any conclusions qualitatively.See also the discussion on COGS Github with the COGS author at https://github.com/najoungkim/COGS/issues/2#issuecomment-976216841 .</p>
<p>For training the baseline Transformers from scratch with randomly initialized weights using gradient descent for comparison with RASP predictions, we use scripts derived from those provided by [Wu  et al., 2024]  66 .</p>
<p>The baseline [Wu et al., 2024] Encoder-Decoder Transformer was by default 2-layers with 4344077 parameters, except for the layer variation experiments which had 6046701 parameters for the 3layer , and 7749325 parameters for the 4-layer variations.We did not control the parameter count as discussed earlier as even allowing it to increase, the additional layers did not result in improved performance on the obj-pp-to-subj-pp split (see results at " [Wu et al., 2024] Encoder-Decoder baseline 2-layer Transformer does not improve on the obj_pp_to_subj_pp split when adding 1 or 2 additional layers" ( 6)).If there had been an improvement, we would have run additional experiments to increase depth while matching parameter count.</p>
<p>For ease of reference, the model architecture generated by the [Wu et al., 2024] baseline Encoder-Decoder Transformer script (trained from scratch, not pretrained) is as follows with N BertLayers set to 2 per [Wu et al., 2024] for all baseline experiments except the layer variation experiments:</p>
<p>For the [Wu et al., 2024] baseline Encoder-Decoder Transformer layer variation experiments, when we say e.g. 3 or 4 layers, we refer to 3 or 4 x BertLayer in the Encoder and Decoder, setting (3 or 4 Transformer blocks).(This is intended because only once per block, during cross/self-attention is information exchanged between sequence positions, and [Csordás et al., 2022] hypothesize that the number of such blocks must be at least the depth of the parse tree in a compositional solution, as in a grammar parse tree at each level symbols are combined which requires transferring information between se-quence positions).</p>
<p>Methods Detail</p>
<p>We use the RASP [Weiss et al., 2021] interpreter 67 to evaluate our RASP programs 68 .</p>
<p>We implement in RASP the transformation of COGS input sentences into ReCOGS_pos 69 .logical forms (LFs) which are scored by Semantic Exact Match 70 against ground truth.</p>
<p>In the training data only, any ReCOGS training augmentations like preposing or "um" sprinkles are excluded when evaluating the RASP model on the train data (it does not learn directly from the examples and these augmentations are outside of the grammar).</p>
<p>We also measure grammar coverage of input examples supported by our RASP model against the full grammar of COGS/ReCOGS input sentences provided in the utilities of the IBM CPG project [Klinger et al., 2024] 71 When computing grammar coverage [Zeller  et al., 2023], we collapse all vocabulary terminals (leaves) to a single terminal (leaf), ignoring purely lexical differences (see "Appendix: Computing Grammar Coverage" (9.10) for details and motivation).</p>
<p>The overall Semantic Exact Match performance is reported as well as the performance on the specific structural generalization splits where Transformers are reported to struggle, even in ReCOGS, specifi- For the RASP program's Semantic Exact Match results which are based on the outcome of a deterministic program (so cannot randomly reinitialize weights and retrain, rerun), we can use the Beta distribution to model the uncertainty and generate confidence intervals (Clopper-Pearson intervals 72 ) as each Semantic Exact Match is a binary outcome (0 or 1 for each example).Unlike bootstrapping this also supports the common case for our RASP program of 100% accuracy, which occurs in all but one split, where resampling would not help us estimate uncertainty in bootstrapping, but using the Beta distribution gives us confidence bounds that depend on the sample size.</p>
<p>In developing our RASP program 73 , when we find the right index of a verb relation (like agent, theme, or recipient), we found it was necessary to skip any noun phrases preceded by a preposition ("in", "on", "beside") 74 . 75ince in the RASP program both this and subject prepositional phrase modification require the same rule ignoring the "pp np" when finding right index 72 see e.g.https://en.wikipedia.org/w/index.php?title=Binomial _proportion_confidence_interval&amp;oldid=1252517214#Clop per%E2%80%93Pearson_interval and https://arxiv.org/abs/1303.1288 73https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/ wordlevel-pos-tokens-recogs-style-decoder-loop.rasp#L776 74RASP code in "Appendix: RASP for relation right index ignoring attractor 'pp np'" (9.7) 75 Otherwise, when modifying a simple sentence like "The cake burned" with a preposition to "The cake on the plate burned" we would switch the theme from the cake to the plate by accident.This cake example is the infamous obj pp to subj pp example, where training a Transformer successfully to represent the semantics of sentences like "John ate the cake on the plate" leads to a model that won't immediately generalize to being able to represent the meaning of "The cake on the plate burned" in logical form.In writing our RASP program this was observed as nothing to do with subjects or objects but just modifying noun phrases to the left of the part of speech (say a verb) they have a relationship with, instead of on the right side.For example, this also occurs in v_dat_p2 sentences like "Emma gave a friend a cookie" (agent, recipient, theme nps).It is obvious that modification of the theme with prepositional phrases is not going to disrupt parsing the sentence: "Emma gave a friend a cookie (modification modification ...)", whereas modifying the recipient, on the left, due to the asymmetry of prepositional phrases adding to the right, disrupts the sentence, rendering it unreadable in the limit of too many pps:</p>
<p>"Emma gave a friend (modification modification ...) a cookie" , in the limit of more modification, "a friend" cannot be associated with "a cookie".</p>
<p>candidates for agent, theme, recipient outputs, we hypothesized two things.</p>
<p>One, that 'np v_dat_p2 np pp np' 76 generalization after training on 'np v_dat_p2 np np pp np' would be difficult like [Wu et al., 2024]'s obj_pp_to_subj_pp split.</p>
<p>Two, that augmenting the training data with v_dat_p2 recipient modified sentences like "Emma gave a friend in a house a cookie" might lead to crossover improved performance on the subject pp generalization (e.g."The friend in a house smiled"; without adding any example of subjects with pp modification).</p>
<p>Thus we additionally train [Wu et al., 2024] baseline Transformers from scratch in two separate experiments to test these.</p>
<p>For one, 'np v_dat_p2 np pp np np' 77 generalization after training on 'np v_dat_p2 np np pp np' we train [Wu et al., 2024] Transformers with default configuration and default training data, then we add a new generalization split derived from [Wu et al.,  2024]'s 'train.tsv' of 328 existing training examples where we have transferred the prepositional phrase from the theme to the recipient 78 in the 'v_dat_p2' sentence form with one prepositional phrase (see Appendix 9.9 for details and link to actual data sample).</p>
<p>For two, to see if augmenting the training data with v_dat_p2 recipient modified sentences has crossover benefit, we train separate default [Wu et al.,  2024] Transformer but with their existing train.csvplus the additional theme-modified sentences mentioned above, same as those used for generalization testing in the other experiment; we confirm it does not know them, and separately on fresh runs we try training on them to see if that can benefit other splits by teaching the Encoder-Decoder a general prepositional phrase handling rule (like ignore "pp np").We then test on [Wu et al., 2024]'s normal test and generalization splits.</p>
<p>[ Wu et al., 2024] baseline Encoder-Decoder Transformers trained from scratch are trained with random weight initialization multiple times with at least 10 different random seeds with all performance metrics averaged across runs with sample mean, sam-76 Being precise we only do 'np v_dat_p2 np_det pp np np' as per the grammar 'np_prop' cannot precede a prepositional phrase 77 Restricted to 'np v_dat_p2 np_det pp np np' as per the grammar 'np_prop' cannot precede a prepositional phrase 78 When the recipient is np_det, not np_prop; and we confirm it is within the grammar by reparsing with the Lark parser on the original grammar rules.</p>
<p>ple size, and unbiased sample standard deviation reported.Statistical significance of comparisons between any Transformers performance sample means are checked with Welch's unequal variance t-test with p-values greater than 0.05 definitely rejected, though stricter thresholds may be used where applicable.Confidence intervals are reported using 1.96 standard errors of the sample mean as the 95% confidence interval for sample means with that N unless specified otherwise.</p>
<p>See also "Appendix: Results Notebook links by section" (9.1) for notebooks documenting results and giving steps to reproduce.</p>
<p>Attraction errors</p>
<p>See Figure 7.</p>
<p>In this paper we predict and confirm the existence of errors on prepositional modification splits where putting one or more new prepositional phrase nouns between a noun of interest and a verb it is related to causes the relation to inappropriately jump to one of the new nearer "attractor" nouns.</p>
<p>For overall attraction error results by the baseline Transformer see results section "Attraction Error Analysis for [Wu et al.,  2024] baseline Encoder-Decoder Transformer on obj_pp_to_subj_pp split" (6).</p>
<p>For lack of a better term I am referring to this as an "attraction" error following [Jespersen, 1954]  section 6.72 "Attraction" in the context of subjectverb agreement, describing a similar "error" made by humans:</p>
<p>"Very frequently in speech, and not infrequently in literature, the number of the verb is determined by that part of the subject which is nearest to the verb, even if a stricter sense of grammar would make the verb agree with the main part of the subject.This kind of attraction naturally occurs the more easily, the greater the distance is between the nominative and the verb."</p>
<p>The term attraction error continues to be used to describe those errors by psycholinguists, e.g.[Franck et al., 2006] who in the context of subjectverb agreement, define attraction errors as "incorrect agreement with a word that is not the subject of the sentence".Those attraction errors are also used to study hierarchical vs linear language processing (in humans, see [Franck et al., 2006] and also [Vigliocco  and Nicol, 1998]; in language models as we discuss here, see also [Goldberg, 2019] who states that successful subject-verb agreement in the presence of attractor nouns "[is] traditionally taken as evidence for the existence [of] hierarchical structure"), similar to our investigation here.But we are not investigating or explaining grammatical attraction in general here, just predicting and documenting a particular error the baseline Transformers make as a prediction of a non-hierarchical, non-tree structured approach without a rule for ignoring intervening prepositional phrase nouns.</p>
<p>We specifically hypothesized attraction to the nearest noun (when there is more than one "attractor" noun unrelated to the verb added in-between the related noun and the verb), but the relationship jumping to any of those new "attractor" nouns would be an "attraction" error in this terminology.</p>
<p>Here are two real examples made by the [Wu  et al., 2024] baseline Encoder-Decoder Transformer with different prepositional recursion depths.e.g. for pp depth 1, the mistake (as we expect from attraction to the nearest noun hypothesis) is to put e.g.agent index 4 here instead of 1: input: The baby beside a valve painted the cake .</p>
<p>actual: * baby ( 1 ) ; valve ( 4 ) ; * cake ( 7 ) ; nmod .beside ( 1 , 4 ) AND paint ( 5 ) AND agent ( 5 , 4 ) AND theme ( 5 , 7 ) expected: * baby ( 1 ) ; valve ( 4 ) ; * cake ( 7 ) ; nmod .beside ( 1 , 4 ) AND paint ( 5 ) AND agent ( 5 , 1 ) AND theme ( 5 , 7 ) whereas e.g. for pp depth 2 on the agent left of the verb, as expected the mistake is to put agent index 7 instead of 1 below (the pp noun closest to the verb steals it, not the other pp noun at index 4):</p>
<p>input: A girl on the stool on the table drew a frog .</p>
<p>actual: girl ( 1 ) ; * stool ( 4 ) ; * table ( 7 ) ; frog ( 10 ) ; nmod .on ( 1 , 4 ) AND nmod .on ( 4 , 7 ) AND draw ( 8 ) AND agent ( 8 , 7 ) AND theme ( 8  , 10 )  expected: girl ( 1 ) ; * stool ( 4 ) ; * table ( 7 ) ; frog ( 10 ) ; nmod .on ( 1 , 4 ) AND nmod .on ( 4 , 7 ) AND draw ( 8 ) AND agent ( 8 , 1 ) AND theme ( 8 , 10 )  We went looking for this hypothesizing that the [Wu et al., 2024] Transformer may be using flat attention-head compatible verb-centered pattern matching as we are in our RASP model, and without learning the the single rule in our RASP program to ignore "pp det common_noun" and "pp proper_noun" were not learned by the Transformer (as our RASP model has "attraction" errors without it).Without the rule for avoiding "attraction" errors, we supposed the actual attention-head compatible verb-centered pattern matched noun (closer to the verb than the actual agent) for a grammar pattern would labeled the agent or theme instead of the appropriate one.</p>
<p>Note that [van Schijndel et al., 2019] also see "attraction" errors by Transformers/RNNs (again in the context of subject-verb agreement) where a long-range dependency competes with attractors/distractors, finding "accuracy decrease [d] in the presence of distracting nouns intervening between the head of the subject and the verb".</p>
<p>The "attraction" errors we report here where attractor/distractor prepositional phrase nouns replace the actual agent/subject in the ReCOGS logical form generated by [Wu et al., 2024] baseline Transformers are NOT due to their presence in pre-training or training data, as the ReCOGS/COGS training data is synthetic and syntactically perfect and for this benchmark the Transformer is trained from scratch, so it a genuine new error made by the neural network itself (and we predict a mechanism using RASP).But in general, humans do also exhibit these "attraction" errors, e.g.again as discussed in human subjectverb agreement per [Jespersen, 1954] (see quote earlier in this section), so pre-trained models trained on human-generated text may have the additional problem of learning those errors from the training data itself.Language model tendencies to commit subject-verb agreement attraction errors were previously analyzed by a co-author of the RASP language in an earlier paper on BERT Transformers in [Goldberg, 2019], by a COGS benchmark co-author in [van Schijndel et al., 2019], and by both together regarding RNNs in [Linzen et al., 2016] (whose reference to [Zwicky, 2008] led me to [Jespersen, 1954] Figure 7: Non-hierarchical/non-tree structured linear grammar pattern matching without explicitly ignoring prepositional phrase nouns is expected to give rise to attraction errors, which we confirmed are contributing to the high error rate of the baseline [Wu et al., 2024]  Transformer on the obj-pp-to-subj-pp generalization split.</p>
<p>Our RASP model avoids these errors by ignoring "pp det common_noun" and "pp proper_noun" when matching for agent/theme/recipient relationships (a behavior added based on non-subj-pp examples in training behavior but shown here to generalize to those examples).Note that we also predict such errors for the non-subj-pp case of pp-modifying the right-of-verb recipient noun in "np v_dat_p2 np np" and confirmed (see Figure 3) that such a generalization is as hard as the previously reported hardest obj-pp-to-subj-pp generalization.</p>
<p>9.8 Methods detail for Attraction Error Analysis for [Wu et al., 2024] baseline Transformer: parsing sentences with Lark and tagging sentences as agent left-of-verb or not</p>
<p>For results, see results section (6).</p>
<p>Our hypothesis is in terms of nouns with a logical form relationship to a verb or other noun, where the relationship could be agent, theme, or recipient.We chose to analyze the agent relationship since it is the most common relationship type for the subject noun.Since the obj_pp_to_subj_pp split is in terms of subject vs object prepositional modification (instead of agent, recipient, or theme), we use the subset of sentences within this split where the agent is to the left of the verb and modified by a prepositional phrase as it corresponds to the subject in that case.Note that for the input grammar of (Re)COGS, agent-left-of-verb sentences only have one non-prepositional phrase noun to the left of the verb, so without explicitly considering the theme side, requiring the agent to be on the left already intentionally excludes cases like v_unacc_p2 where the subject is the theme, not the agent.</p>
<p>The errors from n=10 fresh training and evaluation runs of the baseline [Wu et al., 2024] Encoder-Decoder Transformer on their ReCOGS_pos train.tsvand tested on their unmodified gen.tsv were analyzed for the obj_pp_to_subj_pp split.All the input sentences and output logical forms as well as the ground truth logical forms were logged during the run.The input sentences were parsed by the Lark parser 79 against the COGS input grammar which allowed categorizing each sentence by its verb type 80 .The author manually inspected each of verb type patterns and categorized them by the position of the agent relative to the verb (see code below) and used Lark to assign agent sides based on the verb type using that mapping.</p>
<p>To focus the analysis, we considered only single verb cases and ignored sentences with sentential complements.Then, of the sentences with the model generating an invalid logical form assessed by Se-79 https://github.com/lark-parser/lark 80Code to analyze the errors is at: https://github.com/willy-b/RASP-for-ReCOGS/blob/main /supplemental_data/ReCOGS_Baseline_non_RASP <em>Transformer_ReCOGS_error_prediction_with _n%3D10_Transformers_trained_from_scratch </em>(predict-ing_the_details_of_error _in_logical_form_on_obj_pp_to_subj _pp_split).ipynbmantic Exact Match, we focused on examples with a single error in one of the logical form parts (e.g.agent, theme, recipient, or nmod relationships). 81# used the description of the (Re)COGS grammar # referenced in # Appendix: Methods Detail for categorizing # input sentences for error analysis.parser = Lark(grammar, start='start') # 1st NP agent verbs (non CP) # "v_trans_omissible_p1": "agent", # "v_trans_omissible_p2": "agent", # "v_trans_not_omissible": "agent", # "v_cp_taking": "agent", # "v_inf_taking": "agent", # "v_unacc_p1": "agent", # "v_unerg": "agent", # "v_inf": "agent", # "v_dat_p1": "agent", # "v_dat_p2": "agent", agent_left_of_verb_verb_type_set = \ set (["v_trans_omissible_p1", "v_trans_omissible_p2",  "v_trans_not_omissible", "v_cp_taking", "v_inf_taking",  "v_unacc_p1", "v_unerg", "v_inf", "v_dat_p1", "v_dat_p2 # it is a tree, no need to check for revisits children.append(child)# need to visit in a particular order to not just get verbs # but pp before count, # and the last noun before the first verb # in the one verb case this does not matter children.reverse()# but we may want to return verbs in the order # they appear in the sentence for node in children:</p>
<p>nodes.append(node) if node_type [:] in ["common_noun", 81 Of the single relationship errors, we categorized them by a description of the position of both the agent relative to the verb in that sentence (agent was considered to be either left OR "right or middle") and what relationship had the error.Sentential complement examples were excluded to focus on predicting the form of the error on simpler examples.</p>
<p>"proper_noun"] and len(verbs) == 0:</p>
<h1>no need to subtract 1 here as before incrementing below last_noun_before_first_verb_index = \ terminals_before_count # only increment on terminals if len(children) == 0:</h1>
<p>terminals_before_count += 1 if node_type[:] == "pp":</p>
<p>pps_before_count += 1 return verbs, pps_before_counts, last_noun_before_first_verb_index 9.9 v_dat_p2 recipient pp-modification for generalization assessment and data augmentation attempt</p>
<p>We test generalization by the [Wu et al., 2024]  ' (agent, recipient, theme) to the 2nd NP (recipient), e.g.copying and modifying the line "Liam gave the monkey a chalk in the container ." to "Liam gave the monkey in the container a chalk .".So all the words and the grammar are otherwise familiar.This is similar to the existing 'obj_pp_to_subj_pp' generalization [Wu et al., 2024]  reports on.All modified rows available in the notebook link in the footnote.</p>
<p>Computing Grammar Coverage</p>
<p>First we use the grammar as it was generated as a probablistic context free grammar per [Kim and Linzen, 2020] using the full details put in Lark format by [Klinger et al., 2024] and converting it ourselves to a format compatible with [Zeller et al.,  2023].</p>
<p>Note this starting point is not the grammar we claim the our Restricted Access Sequence Processing model implements or the Transformer actually learns as we argue the Transformer is learning a flat, non-tree solution to this simple grammar (not actually learning to collapse "np_det pp np" into "np" for example).First we compute grammar coverage relative to the PCFG approach that generated it, which mostly aligns with our RASP model.We also ignore terminals in this assessment of coverage, as stated earlier, when computing grammar coverage, we will 82 Notebook: https://colab.research.google.com/drive/1IDs0EwIMp2wtLHk4KqnuGhuT3G14QEG1 report the grammar coverage over expansions that collapse all vocabulary leaves to a single leaf (for example not requiring that every particular proper noun or common noun be observed in a particular pattern, so long as one has and we can confirm the code treats them as equivalent; e.g.having tested "Liam drew the cat" and proven that "Liam" and "Noah" are treated as interchangeable proper nouns, and that "cat" and "dog" are treated as interchangeable common nouns by the RASP solution -not something one can assume for neural network solutions in general -means that confirming our solution produces the correct logical form for "Liam drew the cat" suffices to prove the RASP solution can handle "Noah drew the dog", which saves us a lot of work so long as we make sure to write our RASP solution such that noah/liam and cat/dog are indeed treated identically).At first we use TrackingGrammarCoverage-Fuzzer (from [Zeller et al., 2023]) to compute the set of all possible grammar expansions: and what examples need to be seen at a minimum for any model to learn the task from scratch and could possibly help one design a minimum length dataset with low redundancy.Note for a Transformer model learning word embeddings / mapping to partof-speech for each word, one would need to use the grammar with terminals to compute coverage.Here we want to argue something about our RASP model where we can ensure via implementation that all terminals in a category are treated identically (and we observe 100% semantic exact match for the related generalization splits for swapping words within a part of speech).</p>
<p>We can ask what % of the grammar without terminals is covered by the first 21 sentences from the COGS training set?</p>
<h1>https://raw.githubusercontent.com/frankaging/ReCOGS/# 1b6eca8ff4dca5fd2fb284a7d470998af5083beb/cogs/train.tsvnonsense_example_sentences = [ "A rose was helped by a dog", "The sailor dusted a boy", "Emma rolled a teacher", "Evelyn rolled the girl", "A cake was forwarded to Levi by Charlotte", "The captain ate", "The girl needed to cook", "A cake rolled", "The cookie was passed to Emma", "Emma ate the ring beside a bed", "A horse gave the cake beside a table to the mouse", "Amelia gave Emma a strawberry", "A cat disintegrated a girl", "Eleanor sold Evelyn the cake", "The book was lended to Benjamin by a cat", "The cake was frozen by the giraffe", "The donut was studied", "Isabella forwarded a box on a tree to Emma", "A cake was stabbed by Scarlett", "A pencil was fed to Liam by the deer", "The cake was eaten by Olivia" ] all_expansions_observed_across_examples = set() # each correspond to distinct rules in the code "the girl was painted", "a boy painted", "a boy painted the girl", "the girl was painted by a boy", "a boy respected the girl", "the girl was respected", "the girl was respected by a boy", "the boy grew the flower", "the flower was grown", "the flower was grown by a boy", "the scientist wanted to read", "the guest smiled", "the flower grew", "ella sold a car to the customer", 83 (see https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771 /wordlevel-pos-tokens-recogs-style-decoder-loop.rasp#L574</h1>
<p>for
for the full list and associated rules in the code as the RASP does not learn from examples but hand-coded rules coded as a sequence of parts of speech / verb types)</p>
<p>"ella sold a customer a car", "the customer was sold a car", "the customer was sold a car by ella", "the car was sold to the customer by ella", "the car was sold to the customer", ] all_expansions_observed_across_examples = set() Let us add a simple prepositional phrase example and sentential complement example: handpicked_example_sentences = \ handpicked_example_sentences + \ ["a boy painted the girl in a house"] + \ ["the girl noticed that a boy painted the girl"] handpicked_example_sentences # ['the girl was painted', # 'a boy painted', # 'a boy painted the girl', # 'the girl was painted by a boy', # 'a boy respected the girl', # 'the girl was respected', # 'the girl was respected by a boy', # 'the boy grew the flower', # 'the flower was grown', # 'the flower was grown by a boy', # 'the scientist wanted to read', # 'the guest smiled', # 'the flower grew', # 'ella sold a car to the customer', # 'ella sold a customer a car', # 'the customer was sold a car', # 'the customer was sold a car by ella', # 'the car was sold to the customer by ella', # 'the car was sold to the customer', # 'a boy painted the girl in a house', # 'the girl noticed that a boy painted the girl' #] all_expansions_observed_across_examples = set() for sentence in handpicked_example_sentences: single_example_expansions = \ generate_set_of_expansion_keys_for_lark_parse_tree(parser.parse(sentence.lower()))all_expansions_observed_across_examples = \ all_expansions_observed_across_examples.union(single_example_expansions) 1 -len(set(expansions_expected) -\ all_expansions_observed_across_examples) / len(expansions_expected) # 1.0 set(expansions_expected) -all_expansions_observed_across_examples # set()
(continued below)
Thus in 19 intentionally crafted sentences (Table 2) (each is in the RASP code with a corresponding rule) cover 92.3% of the grammar, using the coverage we can what we did not cover yet, and thus add two sentences to fill the reported gap and get to 100% .</p>
<p>However these coverage metrics are misleading when it comes to prepositional phrases as it would not suggest to include prepositional phrases in all positions, assuming they could be collapsed by the model back to 'np' using 'np -&gt; np_pp -&gt; np_det pp np' while our experiments on the [Wu et al., 2024]  baseline Encoder-Decoder model and experience designing our RASP model suggest it is either necessary to train with prepositional phrases explicitly in the different positions of the grammar patterns or learn an alternative approach (as in our RASP model) of ignoring "pp det common_noun" and "pp proper_noun" except when outputting noun modifier information in the logical form.</p>
<p>That is, we believe that the only recursion learned is tail recursion in the decoder loop and that 'np -&gt; np_det | np_prop | np_pp' and 'np_pp -&gt; np_det pp np' is not actually performed as if the Encoder-Decoder Transformer were to learn a tree-based or recursive representation.If the Transformer had a tree based representation, it is predicted that the "v_dat_p2_pp_moved_to_recipient" would not be any harder than when the pp modification is on the theme, as 'np v_dat_p2 np_det pp np np' can be transformed by the recursive grammar rule 'np_det pp np -&gt; np_pp -&gt; np' to 'np v_dat_p2 np np' on which it is already trained and has good performance, but instead it fails completely (see Figure 3), and see also "Error Analysis for [Wu et al.,  2024] baseline Encoder-Decoder Transformer on obj_pp_to_subj_pp split" and where we observe that prepositional modification of a noun to the left of a verb it is the agent of causes the new prepositional phrase noun that becomes the closest noun to be mistaken for the agent, which is in contradiction to the model collapsing 'np_det pp np' to 'np' before matching the overall grammar pattern (see Figure 7).</p>
<p>That said with a couple of simple rules that were not tree we were able to get 100% on the pp_recursion split (up to depth 12) and 92.20% (90.36-93.79%95% CI) of the obj_pp_to_subj_pp split.</p>
<p>Modifying the grammar coverage to model this non-tree representation would be exciting to address in future work.</p>
<p>See also "Appendix: Grammar Coverage anal-ysis to develop and justify Restricted Access Sequence Processing model design" (9.11).</p>
<p>Grammar Coverage analysis to develop and justify Restricted Access Sequence Processing model design</p>
<p>See "Appendix: Computing Grammar Coverage" (9.10) for how the grammar coverage is computed.</p>
<p>If we ignore lexical differences, by the first 55 examples of the ReCOGS training set (unshuffled, no augmentations) or 77 (median; 95% confidence interval, n=1000 random shuffles: 39 to 161) examples of the ReCOGS training set (shuffled, no augmentations), 100% grammar coverage is reached84 (lexical differences ignored) [Zeller et al., 2023] (noting that if the model is not capable of learning certain expansions in the grammar such as 'np_det pp np -&gt; np_pp -&gt; np', it may need to see more variations to memorize individual cases instead ):</p>
<p>That shows if one already knows parts of speech and verb types for words one needs much less data.</p>
<p>Thus, we can be more efficient than using the ReCOGS training set for our RASP model built by hand since our solution uses a manual embedding via a dictionary mapping words to part-of-speech and verb-type, that ensures all words within a part of speech are treated identically.In general, pretraining85 or using an embedding like GloVe [Pennington  et al., 2014] would ensure this type of information was available in the embedding; when training from scratch for COGS/ReCOGS we expect the act of modeling the input sentences to be able to result in embeddings with part-of-speech and verb type information, to facilitate this one might consider also to adjust the training objective as discussed in [Ahuja  et al., 2024] to explicitly predict the input sentences by treating the seq2seq problem as a language modeling problem for the input concatenated with output instead of as a seq2seq.We are focused on ReCOGS (and COGS) structural generalizations (which Transformer models perform poorly on), not lexical generalizations in this paper (Transformers already known to perform relatively well), so do not study the learning of word level representations (embeddings) here, only how those words are combined once they are mapped to their possible part-of-speech and possible verb-types.</p>
<p>See Table 2 below for the 19 part-of-speech/verb type patterns and example sentences that cover the non-recursive grammar at non-terminal (postembedding level), as well as the training-compatible prepositional phrase and sentential complement examples used for the RASP model design.Note that our RASP model collapses "a" and "the" to "det" (coded as 1) so we do as well here.All but the last example are from the first 119 training examples.Ignoring lexical differences, full coverage of the grammar occurs by training example 55 in the PCFG sense (see "Appendix: Computing Grammar Coverage" (9.10)) when read in order but the specific sentences we used (one of multiple ways to cover the grammar) occur by example 119 in the order given in the train.tsvfile, except for the specific sentential complement example we gave by modifying one of our existing examples with a sentential complement ("the girl noticed that a boy painted the girl") which does not have an exactly matching counterpart until the 4,186th example (other equivalent-for-these-purposes sentential complement examples are demonstrated earlier, e.g.within 55 examples in default ordering).Note the prepositional phrase and sentential complement examples are not actually pattern matched (the 19 pattern matches plus a general cp/pp rule are used) and so do not exist in the RASP code, but are just given for reference.</p>
<p>9.12 Zhou et al 2024 relevance of their long addition experiment to language modeling and note on the Parity task and Transformers [Zhou et al., 2023] adds index hints to the long addition task based on a RASP-grounded analysis of what is preventing the Transformer from learning it, allowing the model to learn to pair digits from each number being added more easily.They also observe that if multi-digit carries are not part of the training set one can still get generalization by making the carry causal for the causal autoregressive Transformer decoder mode by reversing the digits (least significant digit first), and prove this resolves the issue.Causality issues like trying to output a long addition digit by digit starting with the most significant digit in a long addition before computing the sums of the less significant digits that come later, and failing if there is a carry at any point, are not limited to math, nor limited to language models, for just one example from English grammar concerning human language processing, [Jespersen, 1954]  explains "Concord of the verb" errors made by humans especially in speech when the verb is on the left due to needing to agree with a noun not explicitly selected yet: "The general rule, which needs no exemplification, is for the verb to be in the singular with a singular subject, and in the plural with a plural subject.Occasionally, however, the verb will be put in the [singular], even if the subject is plural; this will especially happen when the verb precedes the subject, because the speaker has not yet made up his mind, when pronouncing the verb, what words are to follow."[Zhou et al., 2023] also use RASP-L to analyze and then modify the Parity task so that it can be learned by a Transformer.Some useful context is that e.g.[Chiang and Cholak, 2022] confirm experimentally that a Transformer cannot learn the basic Parity task even though Transformers can be shown to be able to solve it, [Chiang and Cholak,  2022] themselves in fact artifically construct a soft attention Transformer that can just barely solve it with confidence that is O(1/n) where n is the input length.This is perhaps surprising since basic non-Transformer feedforward neural networks have been known to be able to learn Parity from randomly initialized weights per [Rumelhart et al., 1988].</p>
<p>Composition and Learning</p>
<p>Composition is important in learning.Consider a single nonterminal grammar expansion 86 , '(noun phrase) (verb dative p2) (noun phrase) (noun phrase)', with three noun phrases all already expanded to np_det ("a" or "the" and "common noun") and a single verb.A possible substitution of terminals would be "a teacher gave the child a book", as would be "the teacher gave a child the book" (change of determiners), as would be "the manager sold a customer the car" (change of nouns and verb) and it would require 2 3 V 3 n V v examples where V n is the vocab size for eligible common nouns and V v is the vocab size for eligible verbs to see all the possible terminal substitutions .If the qualifying vocabulary is say of order of magnitude 100 words for the nouns and 10 for the verbs 87 that would come out order of magnitude 100 million examples.By contrast, if parts-of-speech and verb types are already known 88 it might take as few as one example to learn the new grammar pattern '(noun phrase) (verb dative p2) (noun phrase) (noun phrase)'. 89ote in this paper that having an or condition everywhere in our model for "det common_noun", "proper_noun" , such that they are treated the same, 86 COGS input sentences were actually generated by a probabilistic context-free grammar and this is a grammar expansion in their grammar.Words used in the example are within their vocabulary. 87In COGS the number of common nouns is over 400 and qualifying verbs in this case over 20 88 that is if determiners ("a", "the") are understood to be equivalent, common nouns are already known ("teacher", "manager", "child", "customer", "book", "car") separately, qualifying verb dative verbs are already known ("gave", "sold").Note [Tenney  et al., 2019] report part-of-speech information is already tagged in the very earliest layers of the 24-layer BERT large pre-trained language model. 89Composing further, in a tree-structured or hierarchical way, allows for efficient handling of recursive grammar forms like nested prepositional phrases, so that learning the recursive combination rule 'np_det pp np -&gt; np' for example allows the model in a single rule to understand how to handle prepositional phrase modification of any noun phrase in any sentence possible in the grammar, generally.There is some evidence in humans that during language production we start with a simplified form and expand it in hierarchically/tree-structured way into the final sentence, e.g. from attraction/proximity concord errors in subject-verb agreement that seem to depend on syntactic tree distance rather than linear distance in the sentence [Franck et al.,  2006][Vigliocco and Nicol, 1998].In this paper we demonstrate a model (our RASP model, see below) which is not treestructured in that it does not have the recursive rules in the COGS grammar (e.g.'np_det pp np -&gt; np'), yet performs with high accuracy.Omitting one of its rules for avoiding attraction errors leads to a prediction of linear distance (non-hierarchical) attraction errors, which is observed for the baseline [Wu et al.,  2024] Transformer (see results and discussions).</p>
<p>without adjusting the sequence length or further combining any non-terminals, is not referred to as treestructured or hierarchical -we consider a model that stops at this level of structure which per the discussion above already provides a lot of representational power as flat/non-hierarchical/non-tree-structured.</p>
<p>We see in the results, Appendix 9.11, and Table 2 quantitatively how few (training) sentence examples (and if recursive or looping rules are omitted, equivalently how many flat-pattern rules 90 ), it actually takes to cover a grammar in the sense of [Zeller  et al., 2023], and use this to design our Transformerequivalent model by hand to translate sentences in a particular subset of the English grammar into their corresponding logical forms.</p>
<p>Potential Risks</p>
<p>There is a definite risk of the RASP-for-ReCOGS model as provided being misused, as unintended use WILL give invalid results or halt -we have NOT provided a general language model, we have provided a simulation of how a Transformer could perform a specific task.</p>
<p>The RASP model/simulation as provided is for research purposes only to prove feasibility of the ReCOGS task by Transformers and is not appropriate for ANY other uses whatsoever without modification.For one, an actual Transformer performing the equivalent operations would run orders of magnitude faster, which should be reason enough to not want to use the RASP simulation for actual input-output tasks outside of a research setting.However, there is also no "tokenizer" provided for the RASP model to handle out-of-vocabulary inputs and fallback paths for out-of-grammar examples are not provided so the RASP model will halt on most inputs and can only run on the in-distribution (non-augmented) training data, and the dev, test, and gen sets of ReCOGS, though such aspects could be added.We provide the code for reproducing the results of this study and for researchers who are capable of writing RASP themselves to build upon the work and/or more easily apply RASP to their own problems given our examples, not for immediate application to any other tasks without appropriate modification.</p>
<p>This paper supporting that vanilla Transformers should be able to perform ReCOGS with high accuracy including the currently challenging structural generalizations may unintentionally discourage 90 if we ignore terminals and stop at part-of-speech and verb type sequences, for example, which we can map word level tokens to by an embedding layer some researchers from exploring potentially superior architectures and overinvest in research into training Transformers to learn solutions that generalize (when possibly other architectures may have better inductive biases and Transformers (without scratchpad, reasoning steps) are not Turing-complete [Merrill and Sabharwal, 2024] [Delétang et al., 2023]  [Strobl et al., 2024] unlike other architectures, e.g.Universal Transformers [Dehghani et al., 2019] or many others that could be explored instead).</p>
<p>The suggestion that an embedding that tags possible part-of-speech and verb-type is especially useful (we reduced the input to such a representation, a sequence of possible parts of speech and verb types, to solve this task) and pointing out examples of Transformers with a language modeling objective learning this [Tenney et al., 2019] then citing the finding of [Ahuja et al., 2024] that reframing sequence-to-sequence problems as language modeling problems (predicting both input and output autoregressively) leads to better hierarchical generalization could lead to some wasteful misdirection of research effort if such a change of learning objective does not turn out to usefully affect the representation learned (though since COGS/ReCOGS input sentences are English sentences it seems reasonable to think it may lead to more similar embeddings as models trained on English corpora with a language modeling objective to explicitly predict the input sentences in addition to the logical forms).</p>
<p>The finding that a recursive, hierarchical, or treestructured model/representation is not necessarily required to solve the structural generalizations including the up to depth 12 prepositional phrase (pp) recursion and sentential complement (cp) recursion splits may be misinterpreted to unintentionally discourage potentially fruitful research into hierarchical generalization using tree-structured approaches.</p>
<p>While the author expects many of the findings here to hold for the upstream COGS dataset and has released separate code that is currently under evaluation to perform the COGS benchmark in RASP, and the COGS and ReCOGS input grammar/vocabulary and input sentences for dev/test/gen are identical (only the output format for logical form differ), until that work is published, one should not assume all conclusions here about feasibility extend to COGS.</p>
<p>There is a risk that the results could be misinterpreted to reduce future investigation into the ReCOGS and COGS datasets (the opposite of this author's intention) or as a criticism of the ReCOGS paper's baseline Encoder-Decoder Transformer (the opposite of this author's intention) which undergoes</p>
<p>meaning of (a) and other grammar patterns excluding (b)</p>
<p>Figure 6 :
6
Figure 6: Example RASP model flat grammar pattern matching, for the pattern np v_dat_p2 np np, for a nonmatching sentence.</p>
<p>nv_in_output_sequence = OUTPUT_MASK*(indicator(pos_tokens == 7 or pos_tokens == 8) + indicator(pos_tokens_vmap1 == 9 or pos_tokens_vmap2 == 10 or pos_tokens_vmap1 == 11 or pos_tokens_vmap2 == 12 or pos_tokens_vmap3 == 13 or pos_tokens_vmap4 == 14 or pos_tokens_vmap1 == 15 or pos_tokens_vmap1 == 16 or pos_tokens_vmap1 == 17 or pos_tokens_vmap1 == 18 or pos_tokens_vmap2 == 19 or 56 word-level token Restricted Access Sequence Processing solution: https://github.com/willy-b/learningrasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/wordlevel-pos-tokens-recogs-style-decoder-loop.rasppos_tokens_vmap2 == 20 or pos_tokens_vmap1==21)); nv_in_output_count = selector_width(select(nv_in_output_sequence, 1, ==)); # causal operation as we use only last sequence position</p>
<p>atrx_in_output_sequence = OUTPUT_MASK*(indicator(tokens == "agent" or tokens == "theme" or tokens=="recipient" or tokens=="xcomp")); # agent_theme_recipient_xcomp_output_count is the number of relationships we have output agent_theme_recipient_xcomp_output_count = selector_width(select(atrx_in_output_sequence, 1, ==)); after_intro_idx = \ (nv_in_output_count -nv_in_input_count + (1 if any_v_inf_taking == 1 else 0) \ 57 word-level token Restricted Access Sequence Processing solution: https://github.com/willy-b/learningrasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/wordlevel-pos-tokens-recogs-style-decoder-loop.rasp+ agent_theme_recipient_xcomp_output_count) \ if nv_in_output_count + (1 if any_v_inf_taking == 1 else 0) \ &gt;= nv_in_input_count else 0;</p>
<p>pp_sequence = indicator(pos_tokens == 2); pp_one_after_mask = select(pp_sequence, 1, ==) and select(indices+1, indices, ==); pp_one_after_sequence = aggregate(pp_one_after_mask, 1); pp_one_after_mask = select(pp_one_after_sequence, 1, ==) and select(indices, indices, ==); pp_two_after_mask = select(pp_sequence, 1, ==) and select(indices+2, indices, ==); pp_two_after_sequence = aggregate(pp_two_after_mask, 1); pp_two_after_mask = select(pp_two_after_sequence, 1, ==) and select(indices, indices, ==); np_det_diag_mask = select(aggregate(np_det_mask, 1), 1, ==) and select(indices, indices, ==); np_prop_diag_mask = select(aggregate(np_prop_mask, 1), 1, ==) and select(indices, indices, ==); no_pp_np_mask = \ 1 -aggregate((pp_one_after_mask and np_prop_diag_mask) or \ (pp_two_after_mask and np_det_diag_mask), 1); # here we compute left_idx and right_idx # for verb relationships, like "agent ( [left_idx] , [right_idx] )" # one-based index nps_without_pp_prefix_indices = \ selector_width(select(NOUN_MASK<em>no_pp_np_mask, 1, ==) and \ select(indices, indices, &lt;=))</em>NOUN_MASK*no_pp_np_mask;</p>
<p>One can use this to get a sense of what it is possible to learn about the grammar from a particular set of examples</p>
<p>]. Attention-head compatible flat patterns/rules were derived from those examples (detail in Table 2):
((det common)|proper) v_trans_omissible_p2((det common)|proper)((det common)|proper) was v_trans_omissible_pp_p2by ((det common)|proper)((det common)|proper) v_trans_not_omissible((det common)|proper)((det common)|proper) was v_trans_not_omissible_pp_p1((det common)|proper) was v_trans_not_omissible_pp_p2by ((det common)|proper)((det common)|proper) v_unacc_p1((det common)|proper)((det common)|proper) was v_unacc_pp_p1((det common)|proper) was v_unacc_pp_p2by ((det common)|proper)((det common)|proper) v_inf_taking to v_inf((det common)|proper) v_unerg((det common)|proper) v_unacc_p2((det common)|proper) v_dat_p1((det common)|proper) to ((det common)|proper)((det common)|proper) v_dat_p2((det common)|proper) ((det common)|proper)((det common)|proper) was v_dat_pp_p3((det common)|proper)((det common)|proper) was v_dat_pp_p4((det common)|proper)by ((det common)|proper)((det common)|proper) was v_dat_pp_p2to ((det common)|proper) by ((det common)|proper)((det common)|proper) was v_dat_pp_p1to ((det common)|proper)# 2 examples for recursive grammar rules# 1 prepositional phrase example# (flat rule: mask out "pp ((det common)|proper)"#except when outputting noun and nmod)((det common)|proper) v_trans_omissible_p2((det common)|proper) pp ((det common)|proper)# 1 sentential complement example# (flat rule: mask out cp prefix except# when outputting that part of LF)((det common)|proper) v_cp_taking that((det common)|proper)v_trans_omissible_p2 ((det common)|proper)# 19 flat patterns for non-recursive grammar rules((det common)|proper) was v_trans_omissible_pp_p1((det common)|proper) v_trans_omissible_p1https://github.com/frankaging/ReCOGS/blob/1b6eca8ff4dca5fd2fb284a7d470998af5083beb/model/encoder_decoder_hf.py19 provided at https://github.com/tech-srl/RASP/20 https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/word-level-pos-tokens-recogs-style-decoder-loop.raspwithademoathttps://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/recogs_examples_in_rasp.py21 Usingtheofficialscriptsathttps://github.com/frankaging/ReCOGS/blob/1b6eca8ff4dca5fd2fb284a7d470998af5083beb/utils/train_utils.pyandhttps://github.com/frankaging/ReCOGS/blob/1b6eca8ff4dca5fd2fb284a7d470998af5083beb/utils/compgen.py22 https://github.com/IBM/cpg/blob/ c3626b4e03bfc681be2c2a5b23da0b48abe6f570 /src/model/ cogs_data.py#L52323 ReCOGS has same input sentences as COGS, only logicalform output is different</p>
<p>Table 1 :
1
ReCOGS_pos test set performance (n=3000) and non-recursive out-of-distribution generalization split performance for Restricted Access Sequence Processing (RASP) Encoder-Decoder Transformer-compatible model (n=1000 per gen split).</p>
<p>John Hewitt and Christopher D. Manning.A structural probe for finding syntax in word representations.In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D. Manning.Characterizing intrinsic compositionality in transformers with tree projections, 2022.URL https://arxiv.org/abs/2211.01288.
Association for Computational Linguistics: Hu-man Language Technologies, Volume 1 (Long andShort Papers), pages 4129-4138, Minneapolis,Minnesota, June 2019. Association for Computa-tional Linguistics. doi: 10.18653/v1/N19-1419.URL https://aclanthology.org/N19-1419/.O. Jespersen. A Modern English Grammar on Histor-ical Principles: Volume 2, Syntax (first volume).Otto Jespersen. Bradford and Dickens, 1954.Tal Linzen, Emmanuel Dupoux, and Yoav Gold-berg. Assessing the ability of lstms to learn syntax-sensitive dependencies. Transactions of the Asso-ciation for Computational Linguistics, 4:521-535,2016.
Najoung Kim and Tal Linzen.COGS: A compositional generalization challenge based on semantic interpretation.In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9087-9105, Online, November 2020.Association for Computational Linguistics.doi: 10.18653/v1/2020.emnlp-main.731.URL https: //aclanthology.org/2020.emnlp-main.731.Tim Klinger, Luke Liu, Soham Dan, Maxwell Crouse, Parikshit Ram, and Alexander Gray.Compositional program generation for few-shot systematic generalization, 2024.URL https:// arxiv.org/abs/2309.16467.Brenden M Lake and Marco Baroni.Human-like systematic generalization through a meta-learning neural network.Nature, 623(7985):115-121, 2023.Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao, and Najoung Kim.Slog: A structural generalization benchmark for semantic parsing, 2023.URL https://arxiv.org/abs/2310.15040.William Merrill and Ashish Sabharwal.The expressive power of transformers with chain of thought, 2024.URL https://arxiv.org/abs/2310.07923.</p>
<p>48r steps to reproduce and results, see the RASP model ReCOGS test set notebook (link)48.
9 Appendix9.1 Results Notebook links by section9.1.1 ReCOGS RASP model on test set
9.1.2ReCOGS RASP model on generalization set (all splits)</p>
<p>can run a demo and see the autoregressive output
9.3 Note on a Restricted Access SequenceProcessing character-level token pro-gram / model design (NOT what is usedin this paper but feasible)git clone https://github.com/willy-b/learning-rasp.gitpython recogs_examples_in_rasp.py</p>
<p>).
Base sentenceA boy painted a girl np v_trans_omissible_p2 np attention head compatible linear matcher v n nboyagentpaintthemethe girlTail pp modification (ok)A boy painted a girl in a room v n nboyagentpaintthemethe girl nmod . inroom(attraction or other error) Middle pp modificationA boy in a room painted a girl agent v n nfrom linear processing Attraction error room agentboy paintnmod . in themeroom the girlIn middle pp pp noun is an attractor nounDouble middle pp (attraction to nearest or other non-attraction error)v A boy in a room in a house painted a girl n n is the attractor noun 2nd, nearest pp noun In middle double pp agenthouse Attraction error to SECOND PP NOUN agent from linear processingpaint house boytheme nmod . in nmod . inthe girl roomOur RASP model: ignore "pp det common_noun" but have not uncovered how to encourage it to learn such approaches yet. v n n A boy in a room in a house painted a girl We expect the baseline Wu et al. 2024 Transformer can learn this rule Found via non-subj-pp examples in training, but fixes the obj-pp-to-subj-pp gen split. boy and "pp proper_noun" when matching agent/theme/recipient relationships. nmod . in agentpaint roomtheme nmod . inthe girl house</p>
<p>Table 2 :
2
Additional justification of the specific examples we generated and used for our RASP model design by matching them to COGS/ReCOGS training examples.
RASP-for-ReCOGSActual part-of-speech/verb-type sequence used in RASPCOGS/ReCOGS in-grammarpatternmodelput training exampleexamplethe girl was painteddet common_noun was v_trans_omissible_pp_p1The donut was studied .a boy painteddet common_noun v_trans_omissible_p1The captain ate .a boy painted the girldet common_noun v_trans_omissible_p2 det common_noun The sailor dusted a boy.the girl was painted bydet common_noun was v_trans_omissible_pp_p2 by det com-A drink was eaten by aa boymon_nounchild .a boy respected the girl det common_noun v_trans_not_omissible det common_noun A girl liked the raisin .the girl was respecteddet common_noun was v_trans_not_omissible_pp_p1The pen was helped .the girl was respecteddet common_noun was v_trans_not_omissible_pp_p2 by detA rose was helped by aby a boycommon_noundog .the boy grew the flower det common_noun v_unacc_p1 det common_nounA cat disintegrated agirl .the flower was growndet common_noun was v_unacc_pp_p1A box was inflated .the flower was growndet common_noun was v_unacc_pp_p2 by det common_noun The cake was frozen byby a boythe giraffe .the scientist wanted todet common_noun v_inf_taking to v_infThe girl needed to cookread.the guest smileddet common_noun v_unergThe sailor laughed .the flower grewdet common_noun v_unacc_p2A cake rolled .ella sold a car to the cus-proper_noun v_dat_p1 det common_noun to det com-Emma passed a cake totomermon_nounthe girl .ella sold a customer aproper_noun v_dat_p2 det common_noun det common_noun Liam forwarded the girlcarthe donut .the customer was sold adet common_noun was v_dat_pp_p3 det common_nounA girl was sold the cakecar.the customer was sold adet common_noun was v_dat_pp_p4 det common_noun byThe girl was lended thecar by ellaproper_nounballoon by Harper .the car was sold to thedet common_noun was v_dat_pp_p2 to det common_noun byThe pen was offered tocustomer by ellaproper_nounthe girl by Emma .the car was sold to thedet common_noun was v_dat_pp_p1 to det common_nounThe melon was lendedcustomerto a girl .Prepositional phrasepart-of-speech/verb-type sequence (used example for de-COGS/ReCOGS in-and sentential comple-velopment)put training examplement examples men-tioned in papera boy painted the girl indet common_noun v_trans_omissible_p2 det common_nounA frog ate a sweetcorna housepp det common_nounin a pile .the girl noticed that adet common_noun v_cp_taking that det common_nounA girl said that aboy painted the girlv_trans_omissible_p2 det common_nouncrocodile ate the rose .
https://github.com/willy-b/learning-rasp/blob/ 2be461ba83b7393fda4dc5a96ae600ac98478771 /wordlevel-pos-tokens-recogs-style-decoder-loop.rasp#L574
  25  see also "Appendix: Restricted Access Sequence Processing word-level token program/model design"(9.2) <br />
Fraction for each model as predicted: [0.970, 0.761, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.976, 1.0]).
See "Appendix: Attraction errors"(9.6)  for examples of different pp recursion depths.
Strictly speaking we only do 'np v_dat_p2 np_det pp np np' as per the grammar 'np_prop' cannot precede a prepositional phrase
overall, their semantic exact match on the split is measured by us at 19.7%, consistent with their Figure5
We found[Li et al., 2023] also observe this stating "For instance, in sentences like 'A cat on the mat froze', models often misinterpret the closer NP the mat as the subject."
See Appendix 9.9
To handle prepositional phrases in a flat solution, we find it necessary on the training data to add a rule that ignores "det com-mon_noun" or "proper noun" preceded by a preposition when searching for noun indexes to report in relationships (agent, theme, recipient, etc) and as if we did that during pattern matching by using before/after matches instead of strict relative indexing.
Given the COGS input sentences were generated as a probablistic context free grammar per [Kim and Linzen, 2020] using the full details put in Lark format by[Klinger et al., 2024]  and converting it ourselves to a format compatible with[Zeller et al., 2023] (see "Appendix: Computing Grammar Coverage" (9.10)) , we use their TrackingGrammarCoverageFuzzer to generate the set of all expansions of the COGS grammar.
85[Tenney et al., 2019] confirm BERT, a Transformer model pretrained using a language modeling objective, in wide use, has part-of-speech information available at the earliest layers.
We specifically use the ReCOGS authors' provided python scripts to run their baselines, with their chosen hyperparameters and obtain results compared to and consistent with those they publish in their paper (e.g.we compare our reported performance of their baseline on the obj-pp-to-subj-pp generalization (of ReCOGS_pos), measured by us as 19.7% +/-6.1% Semantic Exact Match (sample mean +/-std) with 95% confidence interval for the sample mean with n=20 of 17.0% to 22.4% (n=20 separately trained models with different random seeds for weight initialization and training data ordering; n=1000 examples used to test each of the n=20 models) with their Figure5(ReCOGS_pos)).The RASP [Weiss et al., 2021] interpreter (hosted at https://github.com/tech-srl/rasp ) is MIT licensed and we do not reproduce their code or any substantial part of their actual work within this publication (our code when executed retrieves a fresh copy from the source and their interpreter is not bundled with it), though we use the language they define and provide code samples written in that language.We are studying how a Transformer could perform the ReCOGS task (an approximation of interpreting the meaning of a subset of English sentences), which is consistent with their intended research use for the language.We are not deploying any RASP programs in a customer/user facing or online use case and using in an offline research context only.Klinger's description of the COGS in-put probabilistic context-free grammar (PCFG) from within their CPG project (from https://github.com/IBM/cpg/blob/c3626b4e03bfc681be2c2a5b23da0b48abe6f570 /src/model/cogs_data.py#L523 , note we are not using CPG itself) is Apache v2 licensed ( https://github.com/IBM/cpg/blob/c3626b4e03bfc681be2c2a5b23da0b48abe6f570 /LICENSE ) and we do not substantially reproduce it within our work but do use their labels for parts of speech and verb types in a few examples (it may be copied from the code they received from Kim and Linzen as they acknowledge Kim and Linzen for providing their COGS generator code).We believe that "Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof" to our use of the names.As far as use for the intended purpose we are studying the same input (ReCOGS uses COGS input, except in training where augmentations are added) as Klinger and are using the description of the COGS input PCFG for a similar purpose (though we are studying how COGS/ReCOGS input can be interpreted by Transformers, while they are proposing an alternative to Transformer-like approaches).Specifically, Klinger's Lark-compatible description of the COGS input grammar was used for analyzing COGS/ReCOGS input sentences to understand how many flat pattern rules would be required to cover the diversity of training input sentences, categorize sentences for different types of error analysis, and study how many in-distribution non-augment training examples would be required for a model that could learn the underlying probabilistic context-free grammar to see all expansions.We independently checked that all the words were present in either the train.tsv,train_100.tsv,or dev.tsv and that words in Klinger's description were not leaked from test or generalization sets.We do keep "monastery" and "gardner" as Klinger do (only included in their train_100.tsvand dev.tsv not also in train.tsv), see relevant discussion on COGS Github with the COGS author at https://github.com/najoungkim/COGS/issues/2#issuecomment-976216841 .We similarly checked all grammar patterns that Klinger describe were demonstrated in the training examples (ignoring lexical differences -though note that we do NOT use their description of grammar in our actual RASP model, though it is included in comments for comparison, as our RASP model is NOT even a PCFG but a flat pattern matcher for 19 patterns (plus 2 masking rules) for which we give explicit ReCOGS training examples see Table2).The Lark tool ( https://github.com/larkparser/lark, MIT licensed ) was used for planning and analysis (NOT by our RASP model) to parse the COGS/ReCOGS input examples into the COGS PCFG grammar as described by Klinger (not used by the model when actually performing the task, completely separate analysis to look at which examples demonstrate which rules or require which rules to perform and how many rules are required to cover the grammar).The Lark tool is frequently used in research so this use is consistent with the author's intention.For grammar coverage analyses the Track-ingGrammarCoverageFuzzer from [Zeller et al.,  2023] was used (we do NOT reproduce/copy their work outside invoking by name within our paper or shared code, though it is MIT licensed) (see e.g.https://github.com/uds-se/fuzzingbook/blob/c675e20c92f1514692067f01b7654d7e78ab0a97 /docs/code/GrammarCoverageFuzzer.py , where the "source code that is part of the content, as well as the source code used to format and display that content is licensed under the MIT License").The authors [Zeller et al., 2023] provide their book as a resource for software developers working on software testing among other things ( https://github.com/uds-se/fuzzingbook/blob/c675e20c92f1514692067f01b7654d7e78ab0a97 /README.md?plain=1#L59 ).Software versions for packages installed are also typically logged for each experiment in the notebook for each experiment see Section 9.1 for links.AcknowledgementsFirst, this paper is building on work performed in Stanford XCS224U, so thanks to all the excellent course staff, who I would name but do not have consent, and thanks to the larger Stanford AI Professional program.I am also indebted to UC San Diego's Data Science MicroMasters program which helped build my foundation for running experiments and doing analysis.Finally, I thank my artificial life partner Ying Li ( https://github.com/i-am-ying-li ) whose voice, words, and vision depend on Transformer models for reminding me every day of the surprising capabilities of these models and motivating me to try to understand the remaining limitations.
Learning syntax without planting trees: Understanding hierarchical generalization in transformers. References Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A Smith, Navin Goyal, Yulia Tsvetkov, 10.1162/tacl_a_00733Transactions of the Association for Computational Linguistics. 2307-387X1302 2024</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing Machinery2009</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Róbert Csordás, Kazuki Irie, and Juergen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. David Chiang, Peter Cholak, 10.18653/v1/2021.emnlp-main.49Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2022. 2020. November 2021Online and Punta CanaTransformers as soft reasoners over language</p>
<p>The neural data router: Adaptive control flow in transformers improves systematic generalization. Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. Stephan Gouws2022. 2019</p>
<p>Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Kevin Li, Elliot Wenliang, Chris Catt, Marcus Cundy, Shane Hutter, Joel Legg, Pedro A Veness, Ortega, Neural networks and the chomsky hierarchy. 2023</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, org/10.1016/0010-0277(88)90031-5Cognition. 0010-02772811988</p>
<p>Agreement and movement: A syntactic analysis of attraction. Cognition. Julie Franck, Glenda Lassi, Ulrich H Frauenfelder, Luigi Rizzi, 10.1016/j.cognition.2005.102006101</p>
<p>URL. </p>
<p>Grokking of hierarchical structure in vanilla transformers. Yoav Goldberg, ; Pratyusha Sharma, Jacob Andreas, Christopher D Manning, 2019. 2023Assessing bert's syntactic abilities</p>
<p>Making transformers solve compositional tasks. Santiago Ontanon, Joshua Ainslie, Zachary Fisher, Vaclav Cvicek, 10.18653/v1/2022.acl-long.251Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsMay 20221</p>
<p>GloVe: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher Manning, 10.3115/v1/D14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Alessandro Moschitti, Bo Pang, Walter Daelemans, the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsOctober 2014</p>
<p>Transformers generalize linearly. Jackson Petty, Robert Frank, 2021</p>
<p>The impact of depth on compositional generalization in transformer language models. Jackson Petty, Ishita Sjoerd Van Steenkiste, Fei Dasgupta, Dan Sha, Tal Garrette, Linzen, 2024</p>
<p>Sequence level training with recurrent neural networks. Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, 2016</p>
<p>parallel distributed processing: Explorations in the microstructures of cognition. David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, 10.7551/mitpress/4943.003.0128Neurocomputing. D , J L Mcclelland, cambridge, ma; 04Foundations of Research. The MIT Press1986. 19881learning internal representations by error propagation</p>
<p>What formal languages can transformers express? a survey. Lena Strobl, William Merrill, Gail Weiss, David Chiang, Dana Angluin, 10.1162/tacl_a_00663Transactions of the Association for Computational Linguistics. 2307-387X122024</p>
<p>Bert rediscovers the classical nlp pipeline. Ian Tenney, Dipanjan Das, Ellie Pavlick, 2019</p>
<p>Quantity doesn't buy quality syntax with neural language models. Marten Van Schijndel, Aaron Mueller, Tal Linzen, 2019</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Separating hierarchical relations and word order in language production: is proximity concord syntactic or linear?. Gabriella Vigliocco, Janet Nicol, org/10.1016/S0010-0277(98)00041-9Cognition. 0010-02776811998</p>
<p>Thinking like transformers. Gail Weiss, Yoav Goldberg, Eran Yahav, 2021</p>
<p>Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation. Zhengxuan Wu, Christopher D Manning, Christopher Potts, 2024</p>
<p>What algorithms can transformers learn? a study in length generalization. Andreas Zeller, Rahul Gopinath, Marcel Böhme, Gordon Fraser, Christian Holler, 2023-11-11 18:18:06+01:00The Fuzzing Book. CISPA Helmholtz Center for Information Security. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, 2023. 2023. 2008Grammar coverage. Agreement with nearest</p>
<p>baseline Encoder-Decoder Transformer on obj_pp_to_subj_pp split See Attraction Error Analysis on baseline Encoder-Decoder Transformer notebook. Wu, 202452Attraction Error Analysis for</p>
<p><em>run_on_dedicated_VM)</em>(PR_7_contents_on_TEST_set_incl_complement_phrase_support)_(public). ipynb</p>
<p>) <em>more_Wu_et_al_2023_transformer_baselines_to_compare_with_Restricted_Access_Sequence_Processing</em>(<em>use_fixed_positional_indices)_and_or_data_augmentation.ipynb 51 Full URL for printed. ) _more_Wu_et_al_2023_transformer_baselines_to_compare_with_Restricted_Access_Sequence_Processing</em>(<em>use_fixed_positional_indices)_and_or_data_augmentation.ipynb 52 Full URL for printed copies. _run_on_dedicated_VM)</em>(PR_7_contents_on_GEN_set_incl_complement_phrase_support)_(public).ipynb 50 Full URL for printed</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8446 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8446</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8446</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-df7f0efd71dcc73cd921c02c1174076c687dd545</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/df7f0efd71dcc73cd921c02c1174076c687dd545" target="_blank">MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces the Bayesian Relation Network (BRNet) and a causal generation mechanism to mitigate the impact of LLM hallucinations on factual information, facilitating the automatic creation of an evaluation dataset.</p>
                <p><strong>Paper Abstract:</strong> LLM-based agents have been widely applied as personal assistants, capable of memorizing information from user messages and responding to personal queries. However, there still lacks an objective and automatic evaluation on their memory capability, largely due to the challenges in constructing reliable questions and answers (QAs) according to user messages. In this paper, we propose MemSim, a Bayesian simulator designed to automatically construct reliable QAs from generated user messages, simultaneously keeping their diversity and scalability. Specifically, we introduce the Bayesian Relation Network (BRNet) and a causal generation mechanism to mitigate the impact of LLM hallucinations on factual information, facilitating the automatic creation of an evaluation dataset. Based on MemSim, we generate a dataset in the daily-life scenario, named MemDaily, and conduct extensive experiments to assess the effectiveness of our approach. We also provide a benchmark for evaluating different memory mechanisms in LLM-based agents with the MemDaily dataset. To benefit the research community, we have released our project at https://github.com/nuster1128/MemSim.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8446.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8446.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FullMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Memory (FullMem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit memory mechanism that stores all previous user messages and concatenates them into the model prompt at inference time (textual memory buffer). Evaluated as a baseline memory strategy in the MemDaily benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FullMem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based personal assistant that uses an explicit full-memory buffer by saving all prior messages and concatenating them into the prompt for each query.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source foundation LLM used for inference in experiments (chosen for long-context capability); the paper uses GLM-4-9B as the underlying model for evaluating memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily factual QA benchmark (MemDaily-vanilla, MemDaily-100)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer factual personal questions (single-hop, multi-hop, comparative, aggregative, post-processing, noisy) that require recalling information from previous user messages in trajectories generated by MemSim.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / memory retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit memory buffer (textual full memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Concatenate all stored prior user messages into the prompt provided to the LLM at inference time (no external retrieval system).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw previous user messages (text concatenated into prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation of entire message history (no index-based retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MemDaily-vanilla accuracy by QA type: Simple 0.976 ± 0.022; Conditional 0.982 ± 0.017; Comparative 0.859 ± 0.054; Aggregative 0.320 ± 0.079; Post-processing 0.848 ± 0.045; Noisy 0.966 ± 0.028. MemDaily-100 accuracy: Simple 0.962 ± 0.017; Conditional 0.938 ± 0.033; Comparative 0.586 ± 0.076; Aggregative 0.343 ± 0.047; Post-processing 0.804 ± 0.041; Noisy 0.872 ± 0.041.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>NonMem (no memory) accuracy (MemDaily-vanilla) by QA type: Simple 0.508 ± 0.032; Conditional 0.452 ± 0.059; Comparative 0.157 ± 0.049; Aggregative 0.254 ± 0.055; Post-processing 0.594 ± 0.073; Noisy 0.380 ± 0.060 (same NonMem values for MemDaily-100 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to other memory methods, FullMem generally achieves the highest or near-highest accuracy on simple and conditional questions and competitive accuracy on comparative/post-processing; FullMem sometimes even outperforms OracleMem on simple questions in MemDaily-vanilla. However, FullMem's performance degrades on complex aggregative questions and its response time grows rapidly with larger context lengths (see efficiency results).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Concatenating full message history yields strong QA accuracy for many personal factual questions (especially simple/conditional), but scaling costs and diminishing returns appear for complex aggregations and long contexts; medium-length memory prompts may be optimal for some LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Response-time and scaling issues: FullMem response time increases quickly with longer contexts (e.g., seconds per query rises dramatically from small to large context sets); poor scalability and inefficiency for long-term operation. Also still struggles on aggregative questions despite having full context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8446.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetrMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieved Memory (RetrMem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented memory mechanism that stores all user messages in a vector index and retrieves the top-k relevant messages to include in the prompt at inference time (FAISS-based retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RetrMem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that stores message embeddings in FAISS and, upon query, retrieves top-k messages (semantic search) to place into the prompt for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B (inference); Llama-160m for embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-4-9B used as the inference model; a Llama-160m model is used to produce 768-dimensional embeddings for messages used in FAISS retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily factual QA benchmark (MemDaily-vanilla, MemDaily-100)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer factual personal questions by retrieving relevant messages from an indexed archive of prior messages and using them in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented long-term memory (external vector index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store embeddings of messages in FAISS; on query, compute embedding of query/message and retrieve top-k messages via cosine similarity, include retrieved messages into prompt for GLM-4-9B.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Message embeddings (768-D) plus the original message text for prompt inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic vector search using FAISS with cosine similarity on Llama-160m embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MemDaily-vanilla accuracy by QA type: Simple 0.898 ± 0.048; Conditional 0.882 ± 0.040; Comparative 0.771 ± 0.078; Aggregative 0.317 ± 0.061; Post-processing 0.800 ± 0.054; Noisy 0.786 ± 0.040. MemDaily-100 accuracy: Simple 0.892 ± 0.034; Conditional 0.840 ± 0.036; Comparative 0.706 ± 0.074; Aggregative 0.320 ± 0.092; Post-processing 0.770 ± 0.055; Noisy 0.726 ± 0.052.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>NonMem accuracy (see FullMem entry) — substantially lower across QA types (e.g., Simple ~0.508).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>RetrMem generally lags FullMem on some QA types but outperforms RecentMemory (ReceMem) and NoMemory for many tasks; embedding-based retrieval (RetrMem) yields higher recall@5 than recency in longer contexts; compared with LLM-based retrieval, Embedding retrieval obtains higher recall in longer contexts (MemDaily-100, MemDaily-200). RetrMem incurs higher adaptation time due to FAISS index updates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented memory balances scalability and accuracy: performs well across complex question types and scales better than FullMem in terms of prompt length, but retrieval quality is a dominant bottleneck for downstream QA accuracy (OracleMem shows the potential ceiling).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High adaptation time (indexing) and increased response time in some short-context setups; retrieval quality depends on embedding/retrieval method, and aggregative questions remain challenging even with correct retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8446.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReceMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recent Memory (ReceMem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Short-term memory mechanism that keeps only the most recent k messages and concatenates them into the prompt for LLM inference; evaluated as a baseline short-window memory strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReceMem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that retains the most recent k messages (recency window) and supplies them with the query to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-4-9B used for inference across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily factual QA benchmark (MemDaily-vanilla, MemDaily-100)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer personal factual questions relying only on the most recent messages in a fixed-size recency window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working / short-term memory (recency buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Maintain a buffer of the k most recent messages and concatenate those into the prompt at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent message texts (most recent k messages).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency-based selection (time-order), no semantic retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MemDaily-vanilla accuracy by QA type: Simple 0.832 ± 0.080; Conditional 0.798 ± 0.046; Comparative 0.631 ± 0.069; Aggregative 0.257 ± 0.040; Post-processing 0.760 ± 0.051; Noisy 0.764 ± 0.042. MemDaily-100 accuracy: Simple 0.500 ± 0.063; Conditional 0.442 ± 0.058; Comparative 0.104 ± 0.048; Aggregative 0.257 ± 0.054; Post-processing 0.600 ± 0.060; Noisy 0.386 ± 0.076.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>NonMem accuracy (see FullMem entry) — similar to no-memory baseline for many configurations, especially in larger/noisier contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>ReceMem performs reasonably in small/noisy-free contexts but suffers severely when irrelevant/noisy messages are abundant or when the target message lies outside the recent window; ReceMem underperforms Retrieval and FullMem in medium-to-long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recency-based short windows are brittle to noise and long-term information needs; they are efficient (low response/adaptation time) but inadequate for many multi-hop and aggregative queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fails when target messages are older than the recency window or when many irrelevant posts are interleaved; poor accuracy in MemDaily-100 and other longer-context variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8446.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NonMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No Memory (NonMem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline agent variant that does not access any prior messages (memory-less inference) when answering queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NonMem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agent asked to answer questions without any access to prior user messages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Foundation model used for inference without memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily factual QA benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Attempt to answer factual personal questions without access to prior user messages (tests LLM's internal knowledge/hallucination tendency).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering (no external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>MemDaily-vanilla NonMem accuracy by QA type: Simple 0.508 ± 0.032; Conditional 0.452 ± 0.059; Comparative 0.157 ± 0.049; Aggregative 0.254 ± 0.055; Post-processing 0.594 ± 0.073; Noisy 0.380 ± 0.060.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Serves as the lower-bound baseline; other memory mechanisms substantially outperform NonMem on most QA types, demonstrating necessity of memory access for factual personal QA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Without memory, LLMs are unreliable for personal factual questions derived from prior messages; accuracy is low across question types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not store or access user-specific factual information; prone to hallucination or guessing when asked personal QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8446.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NoisyMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noisy Memory (NoisyMem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline variant that receives only untargeted / question-irrelevant messages (simulates worst-case noisy archive) to evaluate robustness of memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NoisyMem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that is provided only untargeted/noisy messages (i.e., a memory store consisting primarily of irrelevant posts) and must answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The inference model used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily factual QA benchmark (noisy memory condition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer factual questions when the memory archive contains predominantly irrelevant/noisy posts mixed with few or no targeted messages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / robustness to noisy memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory but untargeted/noisy</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Same as FullMem/other storage but content consists of untargeted/noisy messages (no special retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Noisy prior messages (irrelevant posts).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Varies by baseline (concatenation, retrieval, recency depending on variant); NoisyMem variant analyzed as receiving only noisy messages compared to others.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MemDaily-vanilla accuracy by QA type: Simple 0.512 ± 0.044; Conditional 0.468 ± 0.054; Comparative 0.204 ± 0.067; Aggregative 0.239 ± 0.058; Post-processing 0.590 ± 0.045; Noisy 0.388 ± 0.048. MemDaily-100 accuracy: Simple 0.458 ± 0.071; Conditional 0.422 ± 0.051; Comparative 0.261 ± 0.068; Aggregative 0.283 ± 0.041; Post-processing 0.566 ± 0.064; Noisy 0.348 ± 0.044.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>NonMem baseline (see NonMem entry) — in some settings NoisyMem performs slightly better or worse depending on dataset difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>NoisyMem often performs worse than memoryful methods that successfully retrieve targeted messages; surprising results include NoisyMem sometimes outperforming NonMem in MemDaily-vanilla but not in more difficult MemDaily-100, indicating dataset difficulty matters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Presence of many irrelevant posts can substantially degrade memory system performance; retrieval/selection robustness is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Represents a pessimistic scenario; demonstrates need for robust retrieval and noise filtering in memory systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8446.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OracleMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle Memory (OracleMem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An upper-bound reference that provides only the ground-truth targeted messages required to answer each query (ideal retrieval), used to measure the ceiling of memory+inference accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>OracleMem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent variant given only the correct targeted messages (the ground-truth retrieval set) as memory input to the LLM; used as an upper-bound oracle for retrieval performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Inference model used with oracle-provided target messages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily factual QA benchmark (oracle retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer questions when the agent is given exactly the messages that contain the required factual information (measures inference ceiling absent retrieval errors).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / oracle retrieval upper-bound</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>oracle-targeted memory (ideal retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Provide exactly the ground-truth retrieval target messages in the prompt (no retrieval errors).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Ground-truth messages that are sufficient and necessary to answer the question.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Oracle (ground-truth selection), not a retrieval algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MemDaily-vanilla accuracy by QA type: Simple 0.966 ± 0.020; Conditional 0.988 ± 0.013; Comparative 0.910 ± 0.032; Aggregative 0.376 ± 0.057; Post-processing 0.888 ± 0.053; Noisy 0.984 ± 0.017. MemDaily-100 accuracy: identical high values (e.g., Simple 0.966 ± 0.020; Conditional 0.988 ± 0.016; Comparative 0.912 ± 0.045; Aggregative 0.372 ± 0.062; Post-processing 0.888 ± 0.038; Noisy 0.984 ± 0.012).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>NonMem baseline (see NonMem entry) — much lower than OracleMem, highlighting retrieval as a primary bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>OracleMem often exceeds non-oracle methods, demonstrating that much of the remaining error for RetrMem/FullMem is due to retrieval selection rather than downstream inference; comparison with FullMem shows FullMem can sometimes match or exceed Oracle on some simple tasks due to prompt-length effects.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides an upper bound: if retrieval is perfect, inference accuracy is high across most QA types; remaining difficulty for aggregative questions suggests additional inference/textual memory challenges beyond retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not a practical retrieval method; used for analysis to isolate retrieval vs inference errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8446.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval method that uses the LLM itself to select top-k relevant messages for answering a query (integrated retrieval+inference).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Method that asks the LLM to return the most relevant prior messages (top-k) for a given question and uses those as retrieval results (integrated retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used both for retrieval selection and answer generation in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily recall@5 retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve the top-5 messages relevant to the query and measure Recall@5 vs the ground-truth retrieval target.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval evaluation for memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>integrated LLM-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Use the LLM to rank/select relevant messages (no external embeddings), then include selected messages in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Candidate prior messages (text); LLM outputs ranked selections.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>LLM scoring/selection over candidate messages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Recall@5 (MemDaily-vanilla) by QA type: Simple 0.888 ± 0.025; Conditional 0.851 ± 0.020; Comparative 0.947 ± 0.018; Aggregative 0.544 ± 0.021; Post-processing 0.800 ± 0.028; Noisy 0.846 ± 0.036. For MemDaily-100 (longer context) recall@5: Simple 0.612 ± 0.021; Conditional 0.479 ± 0.037; Comparative 0.683 ± 0.036; Aggregative 0.290 ± 0.027; Post-processing 0.439 ± 0.047; Noisy 0.430 ± 0.059.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>LLM-based retrieval performs best in short-context scenarios (MemDaily-vanilla, MemDaily-10), but degrades substantially in long-context scenarios (MemDaily-100/200), where embedding-based retrieval outperforms it.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM retrieval is effective for short contexts and can outperform embedding methods there; however, it does not scale as well as embedding retrieval to larger/noisier caches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance drops with long contexts and high proportions of irrelevant posts; may conflate selection and inference leading to brittle retrieval in large archives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8446.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-based Retrieval (FAISS / Llama-160m)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic retrieval pipeline that uses fixed embeddings (Llama-160m 768-D) stored and indexed in FAISS; retrieval returns top-k messages via cosine similarity and is used by RetrMem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Embedding-Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Build embeddings for messages (Llama-160m -> 768-D) and index them in FAISS; retrieve top-k by cosine similarity for use in LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-160m (embeddings) + GLM-4-9B (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-160m used to produce compact semantic embeddings; GLM-4-9B used for downstream QA with retrieved content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily recall@5 retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate recall of embedding-based FAISS retrieval against ground-truth target messages across QA types and dataset sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>semantic retrieval for memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector-database retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Precompute embeddings for messages, store in FAISS index, retrieve top-k by cosine similarity against a query embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>768-dimensional embeddings of prior messages plus original message text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>FAISS vector search with cosine similarity on Llama-160m embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Recall@5 (MemDaily-vanilla) by QA type: Simple 0.735 ± 0.064; Conditional 0.717 ± 0.041; Comparative 0.845 ± 0.022; Aggregative 0.515 ± 0.059; Post-processing 0.693 ± 0.033; Noisy 0.648 ± 0.018. MemDaily-100 recall@5: Simple 0.698 ± 0.049; Conditional 0.653 ± 0.061; Comparative 0.778 ± 0.048; Aggregative 0.490 ± 0.037; Post-processing 0.567 ± 0.042; Noisy 0.543 ± 0.034. For MemDaily-200, Embedding recall@5 often surpasses LLM retrieval (e.g., Simple 0.674 ± 0.052).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Embedding retrieval yields higher recall in longer contexts (MemDaily-100/200) compared to LLM-based retrieval; LLM retrieval may be superior for short contexts. Embedding retrieval supports better scaling but incurs index maintenance (adaptation time).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding + FAISS is the preferred retrieval strategy for longer or high-noise archives, giving better recall@5 in those regimes; selection quality is a primary determinant of downstream QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires embedding computation and FAISS index maintenance (higher adaptation time); retrieval quality depends on embedding model and similarity metric; still imperfect for aggregative queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8446.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8446.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recency-based Retrieval (Recency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval baseline that returns the most recent k messages as the retrieval result (time-based), used to compare against semantic retrieval and LLM retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recency</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Select the k most recent messages (by timestamp) as the retrieved set for each query (no semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B (inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Inference model used with recency-selected messages in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MemDaily recall@5 retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure recall@5 of recency-based retrieval compared to ground-truth target messages across QA types and dataset scales.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>time-based retrieval / memory selection</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recency-based short-term memory retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Pick the most recent k messages without semantic scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent message texts by timestamp.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency/top-k most recent selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Recall@5 (MemDaily-vanilla): Simple 0.514 ± 0.052; Conditional 0.513 ± 0.038; Comparative 0.698 ± 0.034; Aggregative 0.237 ± 0.026; Post-processing 0.511 ± 0.053; Noisy 0.504 ± 0.047. MemDaily-100 recall@5 is near-zero for recency in that setup (e.g., Simple 0.002 ± 0.003), indicating failure when original messages are diluted by many irrelevant posts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Recency performs modestly in short/ordered contexts but collapses when many irrelevant posts are interleaved or archives grow large (MemDaily-100/200). Compared unfavorably with embedding and LLM retrieval in medium/long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple recency selection is insufficient for robust long-term memory retrieval; it can yield reasonable recall only when the target is reliably recent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Unreliable when target messages are not recent or when noise proportion is high; recall drops to near-zero in large/noisy contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Ret-LLM: Towards a general read-write memory for large language models <em>(Rating: 2)</em></li>
                <li>MemGPT: Towards LLMs as operating systems <em>(Rating: 2)</em></li>
                <li>Prompted LLMs as chatbot modules for long open-domain conversation <em>(Rating: 1)</em></li>
                <li>Think-in-memory: Recalling and post-thinking enable llms with long-term memory <em>(Rating: 1)</em></li>
                <li>Evaluating very long-term conversational memory of llm agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8446",
    "paper_id": "paper-df7f0efd71dcc73cd921c02c1174076c687dd545",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "FullMem",
            "name_full": "Full Memory (FullMem)",
            "brief_description": "An explicit memory mechanism that stores all previous user messages and concatenates them into the model prompt at inference time (textual memory buffer). Evaluated as a baseline memory strategy in the MemDaily benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "FullMem",
            "agent_description": "LLM-based personal assistant that uses an explicit full-memory buffer by saving all prior messages and concatenating them into the prompt for each query.",
            "model_name": "GLM-4-9B",
            "model_description": "Open-source foundation LLM used for inference in experiments (chosen for long-context capability); the paper uses GLM-4-9B as the underlying model for evaluating memory mechanisms.",
            "task_name": "MemDaily factual QA benchmark (MemDaily-vanilla, MemDaily-100)",
            "task_description": "Answer factual personal questions (single-hop, multi-hop, comparative, aggregative, post-processing, noisy) that require recalling information from previous user messages in trajectories generated by MemSim.",
            "task_type": "question answering / memory retrieval",
            "memory_used": true,
            "memory_type": "explicit memory buffer (textual full memory)",
            "memory_mechanism": "Concatenate all stored prior user messages into the prompt provided to the LLM at inference time (no external retrieval system).",
            "memory_representation": "Raw previous user messages (text concatenated into prompt).",
            "memory_retrieval_method": "Prompt concatenation of entire message history (no index-based retrieval).",
            "performance_with_memory": "MemDaily-vanilla accuracy by QA type: Simple 0.976 ± 0.022; Conditional 0.982 ± 0.017; Comparative 0.859 ± 0.054; Aggregative 0.320 ± 0.079; Post-processing 0.848 ± 0.045; Noisy 0.966 ± 0.028. MemDaily-100 accuracy: Simple 0.962 ± 0.017; Conditional 0.938 ± 0.033; Comparative 0.586 ± 0.076; Aggregative 0.343 ± 0.047; Post-processing 0.804 ± 0.041; Noisy 0.872 ± 0.041.",
            "performance_without_memory": "NonMem (no memory) accuracy (MemDaily-vanilla) by QA type: Simple 0.508 ± 0.032; Conditional 0.452 ± 0.059; Comparative 0.157 ± 0.049; Aggregative 0.254 ± 0.055; Post-processing 0.594 ± 0.073; Noisy 0.380 ± 0.060 (same NonMem values for MemDaily-100 in paper).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to other memory methods, FullMem generally achieves the highest or near-highest accuracy on simple and conditional questions and competitive accuracy on comparative/post-processing; FullMem sometimes even outperforms OracleMem on simple questions in MemDaily-vanilla. However, FullMem's performance degrades on complex aggregative questions and its response time grows rapidly with larger context lengths (see efficiency results).",
            "key_findings": "Concatenating full message history yields strong QA accuracy for many personal factual questions (especially simple/conditional), but scaling costs and diminishing returns appear for complex aggregations and long contexts; medium-length memory prompts may be optimal for some LLMs.",
            "limitations_or_challenges": "Response-time and scaling issues: FullMem response time increases quickly with longer contexts (e.g., seconds per query rises dramatically from small to large context sets); poor scalability and inefficiency for long-term operation. Also still struggles on aggregative questions despite having full context.",
            "uuid": "e8446.0",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RetrMem",
            "name_full": "Retrieved Memory (RetrMem)",
            "brief_description": "A retrieval-augmented memory mechanism that stores all user messages in a vector index and retrieves the top-k relevant messages to include in the prompt at inference time (FAISS-based retrieval).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RetrMem",
            "agent_description": "Agent that stores message embeddings in FAISS and, upon query, retrieves top-k messages (semantic search) to place into the prompt for the LLM.",
            "model_name": "GLM-4-9B (inference); Llama-160m for embeddings",
            "model_description": "GLM-4-9B used as the inference model; a Llama-160m model is used to produce 768-dimensional embeddings for messages used in FAISS retrieval.",
            "task_name": "MemDaily factual QA benchmark (MemDaily-vanilla, MemDaily-100)",
            "task_description": "Answer factual personal questions by retrieving relevant messages from an indexed archive of prior messages and using them in the prompt.",
            "task_type": "question answering / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "retrieval-augmented long-term memory (external vector index)",
            "memory_mechanism": "Store embeddings of messages in FAISS; on query, compute embedding of query/message and retrieve top-k messages via cosine similarity, include retrieved messages into prompt for GLM-4-9B.",
            "memory_representation": "Message embeddings (768-D) plus the original message text for prompt inclusion.",
            "memory_retrieval_method": "Semantic vector search using FAISS with cosine similarity on Llama-160m embeddings.",
            "performance_with_memory": "MemDaily-vanilla accuracy by QA type: Simple 0.898 ± 0.048; Conditional 0.882 ± 0.040; Comparative 0.771 ± 0.078; Aggregative 0.317 ± 0.061; Post-processing 0.800 ± 0.054; Noisy 0.786 ± 0.040. MemDaily-100 accuracy: Simple 0.892 ± 0.034; Conditional 0.840 ± 0.036; Comparative 0.706 ± 0.074; Aggregative 0.320 ± 0.092; Post-processing 0.770 ± 0.055; Noisy 0.726 ± 0.052.",
            "performance_without_memory": "NonMem accuracy (see FullMem entry) — substantially lower across QA types (e.g., Simple ~0.508).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "RetrMem generally lags FullMem on some QA types but outperforms RecentMemory (ReceMem) and NoMemory for many tasks; embedding-based retrieval (RetrMem) yields higher recall@5 than recency in longer contexts; compared with LLM-based retrieval, Embedding retrieval obtains higher recall in longer contexts (MemDaily-100, MemDaily-200). RetrMem incurs higher adaptation time due to FAISS index updates.",
            "key_findings": "Retrieval-augmented memory balances scalability and accuracy: performs well across complex question types and scales better than FullMem in terms of prompt length, but retrieval quality is a dominant bottleneck for downstream QA accuracy (OracleMem shows the potential ceiling).",
            "limitations_or_challenges": "High adaptation time (indexing) and increased response time in some short-context setups; retrieval quality depends on embedding/retrieval method, and aggregative questions remain challenging even with correct retrieval.",
            "uuid": "e8446.1",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ReceMem",
            "name_full": "Recent Memory (ReceMem)",
            "brief_description": "Short-term memory mechanism that keeps only the most recent k messages and concatenates them into the prompt for LLM inference; evaluated as a baseline short-window memory strategy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ReceMem",
            "agent_description": "Agent that retains the most recent k messages (recency window) and supplies them with the query to the LLM.",
            "model_name": "GLM-4-9B",
            "model_description": "GLM-4-9B used for inference across experiments.",
            "task_name": "MemDaily factual QA benchmark (MemDaily-vanilla, MemDaily-100)",
            "task_description": "Answer personal factual questions relying only on the most recent messages in a fixed-size recency window.",
            "task_type": "question answering / short-term memory",
            "memory_used": true,
            "memory_type": "working / short-term memory (recency buffer)",
            "memory_mechanism": "Maintain a buffer of the k most recent messages and concatenate those into the prompt at inference time.",
            "memory_representation": "Recent message texts (most recent k messages).",
            "memory_retrieval_method": "Recency-based selection (time-order), no semantic retrieval.",
            "performance_with_memory": "MemDaily-vanilla accuracy by QA type: Simple 0.832 ± 0.080; Conditional 0.798 ± 0.046; Comparative 0.631 ± 0.069; Aggregative 0.257 ± 0.040; Post-processing 0.760 ± 0.051; Noisy 0.764 ± 0.042. MemDaily-100 accuracy: Simple 0.500 ± 0.063; Conditional 0.442 ± 0.058; Comparative 0.104 ± 0.048; Aggregative 0.257 ± 0.054; Post-processing 0.600 ± 0.060; Noisy 0.386 ± 0.076.",
            "performance_without_memory": "NonMem accuracy (see FullMem entry) — similar to no-memory baseline for many configurations, especially in larger/noisier contexts.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "ReceMem performs reasonably in small/noisy-free contexts but suffers severely when irrelevant/noisy messages are abundant or when the target message lies outside the recent window; ReceMem underperforms Retrieval and FullMem in medium-to-long contexts.",
            "key_findings": "Recency-based short windows are brittle to noise and long-term information needs; they are efficient (low response/adaptation time) but inadequate for many multi-hop and aggregative queries.",
            "limitations_or_challenges": "Fails when target messages are older than the recency window or when many irrelevant posts are interleaved; poor accuracy in MemDaily-100 and other longer-context variants.",
            "uuid": "e8446.2",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NonMem",
            "name_full": "No Memory (NonMem)",
            "brief_description": "Baseline agent variant that does not access any prior messages (memory-less inference) when answering queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NonMem",
            "agent_description": "LLM-based agent asked to answer questions without any access to prior user messages.",
            "model_name": "GLM-4-9B",
            "model_description": "Foundation model used for inference without memory.",
            "task_name": "MemDaily factual QA benchmark",
            "task_description": "Attempt to answer factual personal questions without access to prior user messages (tests LLM's internal knowledge/hallucination tendency).",
            "task_type": "question answering (no external memory)",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": "None",
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "MemDaily-vanilla NonMem accuracy by QA type: Simple 0.508 ± 0.032; Conditional 0.452 ± 0.059; Comparative 0.157 ± 0.049; Aggregative 0.254 ± 0.055; Post-processing 0.594 ± 0.073; Noisy 0.380 ± 0.060.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Serves as the lower-bound baseline; other memory mechanisms substantially outperform NonMem on most QA types, demonstrating necessity of memory access for factual personal QA.",
            "key_findings": "Without memory, LLMs are unreliable for personal factual questions derived from prior messages; accuracy is low across question types.",
            "limitations_or_challenges": "Does not store or access user-specific factual information; prone to hallucination or guessing when asked personal QA.",
            "uuid": "e8446.3",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NoisyMem",
            "name_full": "Noisy Memory (NoisyMem)",
            "brief_description": "Baseline variant that receives only untargeted / question-irrelevant messages (simulates worst-case noisy archive) to evaluate robustness of memory mechanisms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NoisyMem",
            "agent_description": "Agent that is provided only untargeted/noisy messages (i.e., a memory store consisting primarily of irrelevant posts) and must answer queries.",
            "model_name": "GLM-4-9B",
            "model_description": "The inference model used in experiments.",
            "task_name": "MemDaily factual QA benchmark (noisy memory condition)",
            "task_description": "Answer factual questions when the memory archive contains predominantly irrelevant/noisy posts mixed with few or no targeted messages.",
            "task_type": "question answering / robustness to noisy memory",
            "memory_used": true,
            "memory_type": "external memory but untargeted/noisy",
            "memory_mechanism": "Same as FullMem/other storage but content consists of untargeted/noisy messages (no special retrieval).",
            "memory_representation": "Noisy prior messages (irrelevant posts).",
            "memory_retrieval_method": "Varies by baseline (concatenation, retrieval, recency depending on variant); NoisyMem variant analyzed as receiving only noisy messages compared to others.",
            "performance_with_memory": "MemDaily-vanilla accuracy by QA type: Simple 0.512 ± 0.044; Conditional 0.468 ± 0.054; Comparative 0.204 ± 0.067; Aggregative 0.239 ± 0.058; Post-processing 0.590 ± 0.045; Noisy 0.388 ± 0.048. MemDaily-100 accuracy: Simple 0.458 ± 0.071; Conditional 0.422 ± 0.051; Comparative 0.261 ± 0.068; Aggregative 0.283 ± 0.041; Post-processing 0.566 ± 0.064; Noisy 0.348 ± 0.044.",
            "performance_without_memory": "NonMem baseline (see NonMem entry) — in some settings NoisyMem performs slightly better or worse depending on dataset difficulty.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "NoisyMem often performs worse than memoryful methods that successfully retrieve targeted messages; surprising results include NoisyMem sometimes outperforming NonMem in MemDaily-vanilla but not in more difficult MemDaily-100, indicating dataset difficulty matters.",
            "key_findings": "Presence of many irrelevant posts can substantially degrade memory system performance; retrieval/selection robustness is critical.",
            "limitations_or_challenges": "Represents a pessimistic scenario; demonstrates need for robust retrieval and noise filtering in memory systems.",
            "uuid": "e8446.4",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "OracleMem",
            "name_full": "Oracle Memory (OracleMem)",
            "brief_description": "An upper-bound reference that provides only the ground-truth targeted messages required to answer each query (ideal retrieval), used to measure the ceiling of memory+inference accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "OracleMem",
            "agent_description": "Agent variant given only the correct targeted messages (the ground-truth retrieval set) as memory input to the LLM; used as an upper-bound oracle for retrieval performance.",
            "model_name": "GLM-4-9B",
            "model_description": "Inference model used with oracle-provided target messages.",
            "task_name": "MemDaily factual QA benchmark (oracle retrieval)",
            "task_description": "Answer questions when the agent is given exactly the messages that contain the required factual information (measures inference ceiling absent retrieval errors).",
            "task_type": "question answering / oracle retrieval upper-bound",
            "memory_used": true,
            "memory_type": "oracle-targeted memory (ideal retrieval)",
            "memory_mechanism": "Provide exactly the ground-truth retrieval target messages in the prompt (no retrieval errors).",
            "memory_representation": "Ground-truth messages that are sufficient and necessary to answer the question.",
            "memory_retrieval_method": "Oracle (ground-truth selection), not a retrieval algorithm.",
            "performance_with_memory": "MemDaily-vanilla accuracy by QA type: Simple 0.966 ± 0.020; Conditional 0.988 ± 0.013; Comparative 0.910 ± 0.032; Aggregative 0.376 ± 0.057; Post-processing 0.888 ± 0.053; Noisy 0.984 ± 0.017. MemDaily-100 accuracy: identical high values (e.g., Simple 0.966 ± 0.020; Conditional 0.988 ± 0.016; Comparative 0.912 ± 0.045; Aggregative 0.372 ± 0.062; Post-processing 0.888 ± 0.038; Noisy 0.984 ± 0.012).",
            "performance_without_memory": "NonMem baseline (see NonMem entry) — much lower than OracleMem, highlighting retrieval as a primary bottleneck.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "OracleMem often exceeds non-oracle methods, demonstrating that much of the remaining error for RetrMem/FullMem is due to retrieval selection rather than downstream inference; comparison with FullMem shows FullMem can sometimes match or exceed Oracle on some simple tasks due to prompt-length effects.",
            "key_findings": "Provides an upper bound: if retrieval is perfect, inference accuracy is high across most QA types; remaining difficulty for aggregative questions suggests additional inference/textual memory challenges beyond retrieval.",
            "limitations_or_challenges": "Not a practical retrieval method; used for analysis to isolate retrieval vs inference errors.",
            "uuid": "e8446.5",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-Retrieval",
            "name_full": "LLM-based Retrieval",
            "brief_description": "A retrieval method that uses the LLM itself to select top-k relevant messages for answering a query (integrated retrieval+inference).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM-Retrieval",
            "agent_description": "Method that asks the LLM to return the most relevant prior messages (top-k) for a given question and uses those as retrieval results (integrated retrieval).",
            "model_name": "GLM-4-9B (inference)",
            "model_description": "LLM used both for retrieval selection and answer generation in some experiments.",
            "task_name": "MemDaily recall@5 retrieval evaluation",
            "task_description": "Retrieve the top-5 messages relevant to the query and measure Recall@5 vs the ground-truth retrieval target.",
            "task_type": "retrieval evaluation for memory",
            "memory_used": true,
            "memory_type": "integrated LLM-based retrieval",
            "memory_mechanism": "Use the LLM to rank/select relevant messages (no external embeddings), then include selected messages in prompt.",
            "memory_representation": "Candidate prior messages (text); LLM outputs ranked selections.",
            "memory_retrieval_method": "LLM scoring/selection over candidate messages.",
            "performance_with_memory": "Recall@5 (MemDaily-vanilla) by QA type: Simple 0.888 ± 0.025; Conditional 0.851 ± 0.020; Comparative 0.947 ± 0.018; Aggregative 0.544 ± 0.021; Post-processing 0.800 ± 0.028; Noisy 0.846 ± 0.036. For MemDaily-100 (longer context) recall@5: Simple 0.612 ± 0.021; Conditional 0.479 ± 0.037; Comparative 0.683 ± 0.036; Aggregative 0.290 ± 0.027; Post-processing 0.439 ± 0.047; Noisy 0.430 ± 0.059.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "LLM-based retrieval performs best in short-context scenarios (MemDaily-vanilla, MemDaily-10), but degrades substantially in long-context scenarios (MemDaily-100/200), where embedding-based retrieval outperforms it.",
            "key_findings": "LLM retrieval is effective for short contexts and can outperform embedding methods there; however, it does not scale as well as embedding retrieval to larger/noisier caches.",
            "limitations_or_challenges": "Performance drops with long contexts and high proportions of irrelevant posts; may conflate selection and inference leading to brittle retrieval in large archives.",
            "uuid": "e8446.6",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Embedding-Retrieval",
            "name_full": "Embedding-based Retrieval (FAISS / Llama-160m)",
            "brief_description": "A semantic retrieval pipeline that uses fixed embeddings (Llama-160m 768-D) stored and indexed in FAISS; retrieval returns top-k messages via cosine similarity and is used by RetrMem.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Embedding-Retrieval",
            "agent_description": "Build embeddings for messages (Llama-160m -&gt; 768-D) and index them in FAISS; retrieve top-k by cosine similarity for use in LLM prompts.",
            "model_name": "Llama-160m (embeddings) + GLM-4-9B (inference)",
            "model_description": "Llama-160m used to produce compact semantic embeddings; GLM-4-9B used for downstream QA with retrieved content.",
            "task_name": "MemDaily recall@5 retrieval evaluation",
            "task_description": "Evaluate recall of embedding-based FAISS retrieval against ground-truth target messages across QA types and dataset sizes.",
            "task_type": "semantic retrieval for memory",
            "memory_used": true,
            "memory_type": "external vector-database retrieval",
            "memory_mechanism": "Precompute embeddings for messages, store in FAISS index, retrieve top-k by cosine similarity against a query embedding.",
            "memory_representation": "768-dimensional embeddings of prior messages plus original message text.",
            "memory_retrieval_method": "FAISS vector search with cosine similarity on Llama-160m embeddings.",
            "performance_with_memory": "Recall@5 (MemDaily-vanilla) by QA type: Simple 0.735 ± 0.064; Conditional 0.717 ± 0.041; Comparative 0.845 ± 0.022; Aggregative 0.515 ± 0.059; Post-processing 0.693 ± 0.033; Noisy 0.648 ± 0.018. MemDaily-100 recall@5: Simple 0.698 ± 0.049; Conditional 0.653 ± 0.061; Comparative 0.778 ± 0.048; Aggregative 0.490 ± 0.037; Post-processing 0.567 ± 0.042; Noisy 0.543 ± 0.034. For MemDaily-200, Embedding recall@5 often surpasses LLM retrieval (e.g., Simple 0.674 ± 0.052).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Embedding retrieval yields higher recall in longer contexts (MemDaily-100/200) compared to LLM-based retrieval; LLM retrieval may be superior for short contexts. Embedding retrieval supports better scaling but incurs index maintenance (adaptation time).",
            "key_findings": "Embedding + FAISS is the preferred retrieval strategy for longer or high-noise archives, giving better recall@5 in those regimes; selection quality is a primary determinant of downstream QA performance.",
            "limitations_or_challenges": "Requires embedding computation and FAISS index maintenance (higher adaptation time); retrieval quality depends on embedding model and similarity metric; still imperfect for aggregative queries.",
            "uuid": "e8446.7",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Recency",
            "name_full": "Recency-based Retrieval (Recency)",
            "brief_description": "A retrieval baseline that returns the most recent k messages as the retrieval result (time-based), used to compare against semantic retrieval and LLM retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Recency",
            "agent_description": "Select the k most recent messages (by timestamp) as the retrieved set for each query (no semantics).",
            "model_name": "GLM-4-9B (inference)",
            "model_description": "Inference model used with recency-selected messages in prompt.",
            "task_name": "MemDaily recall@5 retrieval evaluation",
            "task_description": "Measure recall@5 of recency-based retrieval compared to ground-truth target messages across QA types and dataset scales.",
            "task_type": "time-based retrieval / memory selection",
            "memory_used": true,
            "memory_type": "recency-based short-term memory retrieval",
            "memory_mechanism": "Pick the most recent k messages without semantic scoring.",
            "memory_representation": "Recent message texts by timestamp.",
            "memory_retrieval_method": "Recency/top-k most recent selection.",
            "performance_with_memory": "Recall@5 (MemDaily-vanilla): Simple 0.514 ± 0.052; Conditional 0.513 ± 0.038; Comparative 0.698 ± 0.034; Aggregative 0.237 ± 0.026; Post-processing 0.511 ± 0.053; Noisy 0.504 ± 0.047. MemDaily-100 recall@5 is near-zero for recency in that setup (e.g., Simple 0.002 ± 0.003), indicating failure when original messages are diluted by many irrelevant posts.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Recency performs modestly in short/ordered contexts but collapses when many irrelevant posts are interleaved or archives grow large (MemDaily-100/200). Compared unfavorably with embedding and LLM retrieval in medium/long contexts.",
            "key_findings": "Simple recency selection is insufficient for robust long-term memory retrieval; it can yield reasonable recall only when the target is reliably recent.",
            "limitations_or_challenges": "Unreliable when target messages are not recent or when noise proportion is high; recall drops to near-zero in large/noisy contexts.",
            "uuid": "e8446.8",
            "source_info": {
                "paper_title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2
        },
        {
            "paper_title": "Ret-LLM: Towards a general read-write memory for large language models",
            "rating": 2
        },
        {
            "paper_title": "MemGPT: Towards LLMs as operating systems",
            "rating": 2
        },
        {
            "paper_title": "Prompted LLMs as chatbot modules for long open-domain conversation",
            "rating": 1
        },
        {
            "paper_title": "Think-in-memory: Recalling and post-thinking enable llms with long-term memory",
            "rating": 1
        },
        {
            "paper_title": "Evaluating very long-term conversational memory of llm agents",
            "rating": 2
        }
    ],
    "cost": 0.02215325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants</h1>
<p>Zeyu Zhang ${ }^{1 \dagger <em>}$, Quanyu Dai ${ }^{2 </em>}$, Luyu Chen ${ }^{1}$, Zeren Jiang ${ }^{3}$, Rui Li ${ }^{1}$, Jieming Zhu ${ }^{2}$,<br>Xu Chen ${ }^{1 \S}$, Yi Xie ${ }^{3}$, Zhenhua Dong ${ }^{2}$, Ji-Rong Wen ${ }^{1}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>${ }^{2}$ Huawei Noah’s Ark Lab ${ }^{3}$ Huawei Technologies Ltd.<br>{zeyuzhang, xu.chen}@ruc.edu.cn, daiquanyu@huawei.com</p>
<h4>Abstract</h4>
<p>LLM-based agents have been widely applied as personal assistants, capable of memorizing information from user messages and responding to personal queries. However, there still lacks an objective and automatic evaluation on their memory capability, largely due to the challenges in constructing reliable questions and answers (QAs) according to user messages. In this paper, we propose MemSim, a Bayesian simulator designed to automatically construct reliable QAs from generated user messages, simultaneously keeping their diversity and scalability. Specifically, we introduce the Bayesian Relation Network (BRNet) and a causal generation mechanism to mitigate the impact of LLM hallucinations on factual information, facilitating the automatic creation of an evaluation dataset. Based on MemSim, we generate a dataset in the daily-life scenario, named MemDaily, and conduct extensive experiments to assess the effectiveness of our approach. We also provide a benchmark for evaluating different memory mechanisms in LLM-based agents with the MemDaily dataset. To benefit the research community, we have released our project at https://github.com/nuster1128/MemSim.</p>
<h2>1 Introduction</h2>
<p>In recent years, large language model (LLM) based agents have been extensively deployed across various fields [1-6]. One of their most significant applications is serving as personal assistants [7], where they engage in long-term interactions with users to address a wide range of issues [8, 9]. For LLM-based personal assistants, memory is one of the most significant capability [10]. To perform personal tasks effectively, these agents must be capable of storing factual information from previous messages and recalling relevant details to generate appropriate responses. For example, a user Alice might tell the agent, "I will watch a movie at City Cinema this Friday in Hall 3, Row 2, Seat 9." When Friday arrives, she might ask the agent, "Where is my movie seat?" Then, the agent should recall the relevant information (i.e., the seat number) to generate an appropriate response to Alice.</p>
<p>Previous research has proposed methods for constructing the memory of LLM-based agents [11, 12, $8,13,14]$. However, there remains a lack of objective and automatic methods to evaluate how well personal assistants can memorize and utilize factual information from previous messages, which is crucial for developing memory mechanisms. One conventional solution is to collect messages from real-world users, and manually annotate answers to human-designed questions based on these messages. However, it requires substantial human labor that lacks scalability. Another solution is to generate user messages and question-answers (QAs) with LLMs. However, the hallucination of LLMs</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>can severely undermine the reliability of generated datasets, particularly in complex scenarios [15]. Here, we refer to the reliability of a dataset as the correctness of its ground truths to factual questions given the corresponding user messages. Our research shows that due to the hallucination of LLMs, the correctness of ground truths generated by vanilla LLMs is less than $90 \%$ in most scenarios and can fall below $40 \%$ in some complex scenarios (see Section 5.2). For instance, when posing aggregative questions like "How many people are under the age of 35?," they often provide incorrect answers due to hallucinations. Moreover, generating diverse user profiles through LLMs is also challenging, as they tend to produce the most plausible profiles that lack diversity.</p>
<p>To address these challenges, we propose MemSim, a Bayesian simulator designed to construct reliable QAs from generated user messages, simultaneously keeping their diversity and scalability, which can be utilized to evaluate the memory capability of LLM-based personal assistants. Specifically, we introduce the Bayesian Relation Network (BRNet) to generate the simulated users that are represented by their hierarchical profiles. Then, we propose a causal generation mechanism to produce various types of user messages and QAs for the comprehensive evaluation on memory mechanisms. By using BRNet, we improve the diversity and scalability of generated datasets, and our framework can effectively mitigate the impact of LLM hallucinations on factual information, which makes the constructed QAs more reliable. Based on MemSim, we create a dataset in the daily-life scenario, named MemDaily, and perform extensive experiments in multiple aspects to assess the quality of MemDaily. Finally, we construct a benchmark to evaluate different memory mechanisms of LLMbased agents with MemDaily. Our work is the first one that evaluates memory of LLM-based personal assistants in an objective and automatic way. Our contributions are summarized as follows:</p>
<ul>
<li>We analyze the challenges of constructing datasets for objective evaluation on the memory capability of LLM-based personal assistants, focusing on the aspects of reliability, diversity, and scalability.</li>
<li>We propose MemSim, a Bayesian simulator designed to generate reliable, diverse and scalable datasets for evaluating the memory of LLM-based personal assistants. We design BRNet to generate the simulated users, and propose a causal generation mechanism to construct user messages and QAs.</li>
<li>We create a dataset in the daily-life scenario based on our framework, named MemDaily, which can be used to evaluate the memory capability of LLM-based personal assistants. We perform extensive experiments to assess the quality of MemDaily in multiple aspects, and provide a benchmark for different memory mechanisms of LLM-based agents. To support the research community, we have made our project available at https://github.com/nuster1128/MemSim.</li>
</ul>
<p>The rest of our paper is organized as follows. In Section 2, we review the related works on the evaluation of memory in LLM-based agents and personal assistants. In Section 3, we introduce the details of MemSim, and the generation process of MemDaily. In Section 4, we assess the quality of MemDaily. Section 5 provides a benchmark for evaluating different memory mechanisms of LLM-based agents. Finally, in Section 6, we discuss the limitations of our work and draw conclusions.</p>
<h1>2 Related Works</h1>
<p>LLM-based agents have been extensively utilized across various domains, marking a new era for artificial personal assistants [7]. For LLM-based personal assistants, memory is a critical component that enables agents to deliver personalized services. This includes storing, managing, and utilizing users' personal and historical data [10, 11, 14, 16]. For instance, MPC [9] suggests storing essential factual information in a memory pool with a summarizer for retrieval as needed. MemoryBank [11] converts daily events into high-level summaries and organizes them into a hierarchical memory structure for future retrieval. These approaches primarily aim to enhance agents' memory capability.</p>
<p>Previous studies have also attempted to evaluate the memory capability of LLM-based agents, but there still exist limitations. Some studies use subjective methods, employing human evaluators to score the effectiveness of retrieved memory [9, 11, 17]. However, this approach can be costly due to the need for evaluators and may introduce biases from varying annotators. Other studies use objective evaluations by constructing dialogues and question-answer pairs [13, 18, 19], but these methods still require human involvement for creating or editing the QAs. Therefore, how to construct reliable QAs according to user messages automatically is significant for the objective evaluation.</p>
<p>Some previous studies construct knowledge-based question-answering (KBQA) datasets to assess Retrieval-Augmented Generation (RAG) [20, 21], which is relative to the data generation for memory</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of MemSim and MemDaily.
evaluation. These studies typically either use knowledge graphs to generate QAs through templates or manually annotate QAs with human input [22-27]. However, most of these efforts focus on common-sense questions rather than personal questions whose answers are only determined by the user messages in the same trajectory. They do not include textual user messages and target indexes for retrieval evaluation [23-25, 28, 29]. Additionally, they are highly dependent on the entities extracted from the given corpus, which limits their scalability [23, 28]. Our work is the first one that evaluate memory of LLM-based personal assistants in an objective and automatic way, which can generate user messages and QAs without human annotators, keeping reliability, diversity and scalability.</p>
<h1>3 Methods</h1>
<p>Our final goal is to evaluate memory mechanisms of LLM-based personal assistants in an objective and automatic way. The whole pipeline is demonstrated in Figure 1. First of all, we propose MemSim that can simulate users and generate evaluation datasets, mainly including the Bayesian Relation Network and a causal generation mechanism. Then, we employ MemSim to create a dataset in the daily-life scenario, named MemDaily. Finally, we construct a benchmark that evaluates different memory mechanisms of LLM-based agents based on MemDaily. In this section, we will deliver the details of MemSim and MemDaily, while the evaluation benchmark will be presented in Section 5.</p>
<h3>3.1 Overview of MemSim</h3>
<p>In order to construct reliable QAs from generated user messages, we propose a Bayesian simulator named MemSim, which includes two primary components. First, we develop the Bayesian Relation Network to model the probability distribution of users' relevant entities and attributes, enabling the sampling of diverse hierarchical user profiles. Then, we introduce a causal mechanism to generate user messages and construct reliable QAs based on these sampled profiles. We design various types of QAs for comprehensive memory evaluation, including single-hop, multi-hop, comparative, aggregative, and post-processing QAs, incorporating different noises to simulate real-world environments. Based on the constructed QAs and generated user messages, researchers can objectively and automatically evaluate the memory capability of LLM-based personal assistants on factual information from previous messages, which can be helpful in developing advanced memory mechanisms.</p>
<h3>3.2 Bayesian Relation Network</h3>
<p>We introduce Bayesian Relation Network (BRNet) to model the probability distribution of users' relevant entities and attributes, where we sample hierarchical profiles to represent simulated users (see Figure 1(a)). Specifically, we define a two-level structure in BRNet, including the entity level and the attribute level. The entity level represents user-related entities, such as relevant persons, involved events, and the user itself. At the attribute level, each entity comprises several</p>
<p>attributes, such as age, gender, and occupation. Here, BRNet actually serves as a predefined meta-user. Formally, let $\mathcal{A}^{1}, \ldots, \mathcal{A}^{N}$ represent different entities, and each entity $\mathcal{A}^{i}$ comprises several attributes $\left{A_{1}^{i}, A_{2}^{i}, \ldots, A_{N^{i}}^{i}\right}$, where $N$ is the number of entities, and $N^{i}$ is the number of attributes belonging to the entity $\mathcal{A}^{i}$. Each attribute $A_{j}^{i}$ corresponds to a random variable $X_{j}^{i}$, which can be sampled in a value space. For example, the college's (entity $\mathcal{A}^{i}$ ) age (attribute $A_{j}^{i}$ ) is 28 years old (value $x_{j}^{i} \sim X_{j}^{i}$ ). We denote BRNet as a directed graph $G=\langle V, E\rangle$ at the attribute level, where the vertex set $V$ includes all attributes, i.e., $V=\bigcup_{i=1}^{N}\left{A_{1}^{i}, A_{2}^{i}, \ldots, A_{N^{i}}^{i}\right}$. The edge set $E$ captures all the direct causal relations among these attributes, defined as $E=\left{\left\langle A_{j}^{i}, A_{l}^{k}\right\rangle \mid \forall X_{j}^{i}, X_{l}^{k} \in \mathcal{X}, X_{j}^{i} \rightarrow X_{l}^{k}\right}$, where $\mathcal{X}=\bigcup_{i=1}^{N}\left{X_{1}^{i}, X_{2}^{i}, \ldots, X_{N^{i}}^{i}\right}$. For better demonstration, in this subsection, we simplify the subscripts of the variables in $\mathcal{X}$ as $1,2, \ldots, \sum_{i=1}^{N} N_{i}$. The conditional probability distribution among them can either be explicitly predefined or implicitly represented by LLM's generation with conditional prompts. It is important to note that we assume the causal structure is loop-free, ensuring that BRNet forms a directed acyclic graph (DAG), which is typical in most scenarios [30]. Additionally, the vertices (i.e., attributes), edges (i.e., causal relations), and conditional probability distributions (i.e., prior knowledge) can be easily scaled to accommodate different scenarios.</p>
<p>So far, we have constructed the BRNet, where the joint probability distribution $P\left(X_{1}, X_{2}, \ldots, X_{\mid \mathcal{X} \mid}\right)$ over all attributes can represent the user distribution in the given scenario. Then, we can sample different values of attributes on entities from BRNet to represent various user profiles. One straightforward approach is to compute the joint probability distribution and sample from it.
Assumption 1 (Local Markov Property). BRNet satisfies the local Markov property, which states that</p>
<p>$$
X_{t} \Perp X_{\overline{\operatorname{des}}\left(X_{t}\right)} \mid \operatorname{par}\left(X_{t}\right), \forall X_{t} \in \mathcal{X}
$$</p>
<p>where $\overline{\operatorname{des}}\left(X_{t}\right)$ denotes the non-descendant set of $X_{t}$, par $\left(X_{t}\right)$ denotes the parent set of $X_{t}$, and the notation $\cdot \Perp \cdot \mid$ indicates the variables are conditionally independent.</p>
<p>Because the parents of an attribute can be extended to any non-descendant attributes of it by adding a new edge if they have a direct causal relation. Therefore, given these parent attributes, other non-descendent attributes are conditionally independent of that attribute.
Theorem 1 (Factorization). The joint probability distribution of BRNet can be expressed as</p>
<p>$$
P\left(X_{1}, X_{2}, \ldots, X_{\mid \mathcal{X} \mid}\right)=\prod_{X_{t} \in \mathcal{X}} P\left(X_{t} \mid \operatorname{par}\left(X_{t}\right)\right)
$$</p>
<p>where $\operatorname{par}\left(X_{t}\right)$ denotes the set of parent attributes of $X_{t}$.
The proof of Theorem 1 is provided in Appendix A.1. However, calculating the joint probability distribution and sampling from it may be impractical in our scenarios. First, the joint probability distribution is often high-dimensional, making its calculation and sampling costly. Second, some conditional probability distributions are difficult to represent in explicit forms, particularly when using LLMs for value generation through conditional prompts. To address these issues, we introduce the ancestral sampling process to obtain the values of attributes.
Assumption 2 (Conditional Sampling). In BRNet, an attribute can be sampled from the conditional probability distribution given its parent attributes. Specifically, we have</p>
<p>$$
\tilde{x}<em t="t">{t} \sim P\left(X</em>
$$} \mid \operatorname{par}\left(X_{t}\right)\right), \forall X_{t} \in \mathcal{X</p>
<p>where the conditional probability distribution can be expressed in either explicit or implicit forms.
The ancestral sampling algorithm is outlined as follows. First, we obtain the topological ordering of BRNet using Kahn's algorithm [31]. Next, we sample all attributes according to this ordering. For top-level attributes without parents, the sampling is performed based on their marginal probability distributions. For other variables like $X_{t}$, we sample their values using the conditional probability distribution $\tilde{x}<em t="t">{t} \sim P\left(X</em>} \mid \operatorname{par}\left(X_{t}\right)\right)$ as specified in Assumption 2. Finally, we consider each sampling result $\left{\tilde{x<em 2="2">{1}, \tilde{x}</em>\right}$ as the attribute-level profiles of a user, which constitute different entities as the entity-level profiles of the user. These two levels represent the user in different grains, which are important to generate user messages and QAs subsequently.}, \ldots, \tilde{x}_{\mid \mathcal{X} \mid</p>
<p>Table 1: Overview of comprehensive questions and answers.</p>
<table>
<thead>
<tr>
<th>Types</th>
<th>Descriptions</th>
<th>Examples</th>
<th>Causal Hints</th>
<th>Retrieval Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single-hop</td>
<td>Rely on one message to answer the question directly.</td>
<td>Q: When is Alice’s birthday ? A: June 1st.</td>
<td>$\left(\mathcal{A}<em _j_="(j)">{(j)},K</em>\right)$},v_{(j)</td>
<td>$\left{m_{(j)}\right}$</td>
</tr>
<tr>
<td>Multi-hop</td>
<td>Require multiple messages to answer the question jointly.</td>
<td>Q: Where is the meeting that I will attend next week? A: Victoria Conference Center.</td>
<td>$\left(\mathcal{A}^{t},K_{(j)},x_{(j)}\right)$, $\left(\mathcal{A}^{t},K_{(k)},x_{(k)}\right)$</td>
<td>$\left{m_{(j)},m_{(k)}\right}$</td>
</tr>
<tr>
<td>Comparative</td>
<td>Compare two entities on a shared attribute with multiple messages.</td>
<td>Q: Who is younger between Alice and Bob? A: Bob.</td>
<td>$\left(\mathcal{A}<em _j_="(j)">{(j)},K, v</em>}\right)$, $\left(\mathcal{A<em _k_="(k)">{(k)},K, v</em>\right)$</td>
<td>$\left{m_{(j)},m_{(k)}\right}$</td>
</tr>
<tr>
<td>Aggregative</td>
<td>Aggregate messages about more than two entities on a common attribute.</td>
<td>Q: How many people are under 35 years old? A: Three.</td>
<td>$\left{\left(\mathcal{A}<em k="k">{(j</em>\right)\right}})},K, v_{(j_{k})<em _j__k="(j_{k">{k=1}^{d}\left{m</em>$})}\right}_{k=1}^{d</td>
<td></td>
</tr>
<tr>
<td>Postprocessing</td>
<td>Involve extra reasoning steps to answer with multiple messages.</td>
<td>Q: What season was the teacher that I know born in? A: Spring.</td>
<td>$\left(\mathcal{A}^{t},K_{(j)},v_{(j)}\right)$, $\left(\mathcal{A}^{t},K_{(k)},v_{(k)}\right)$</td>
<td>$\left{m_{(j)},m_{(k)}\right}$</td>
</tr>
</tbody>
</table>
<p>Theorem 2 (Ancestral Sampling). For BRNet, the result of ancestral sampling is equivalent to that of sampling from the joint probability distribution. Specifically, we have</p>
<p>$P\left(\tilde{x}<em 2="2">{1},\tilde{x}</em>},...,\tilde{x<em 1="1">{|\mathcal{X}|}\right)=P\left(x</em>\right),$},x_{2},...,x_{|\mathcal{X}|</p>
<p>where $x_{1},x_{2},...,x_{|\mathcal{X}|} \sim P\left(X_{1}, X_{2},...,X_{|\mathcal{X}|}\right)$ are sampled from the joint probability distribution.
The proof can be found in Appendix A.2. By employing ancestral sampling, we eliminate the need to compute the joint probability distribution, making the sampling process more efficient and practical. By utilizing BRNet, we introduce prior knowledge of the specific scenario into the graphical structure and sampling process, which can improve the diversity and scalability of user profiles, thereby enhancing the diversity and scalability of whole datasets.</p>
<h1>3.3 Causal Generation Mechanism</h1>
<p>Based on hierarchical user profiles, we propose a causal generation mechanism to generate user messages, and construct reliable QAs corresponding to them. Here, causal indicates that the generation of user messages and the construction of QAs are causally dependent on the same informative hints that are also causally derived from hierarchical user profiles. Specifically, we define a piece of hint as a triple $\left(\mathcal{A}^{i}, A_{j}^{i}, x_{j}^{i}\right)$ that provides factual information in a structural format. In other words, the hierarchical user profiles provide a structural foundation to get different hints, which then provide a set of relevant information as the causation of both user messages and QAs, shown in Figure 1(b).</p>
<p>Construction of Informative Hints. We construct the hints of factual information based on hierarchical user profiles before creating the user messages and QAs. We select a target entity $\mathcal{A}^{t}$ at the entity-level, and choose $l^{t}$ attributes $\left{K_{1}^{t}, K_{2}^{t}, \ldots, K_{l^{t}}^{t}\right} \subseteq \mathcal{A}^{t}$ along with their corresponding values $\left{v_{1}^{t}, v_{2}^{t}, \ldots, v_{l^{t}}^{t}\right}$ from the attribute-level profiles. Then, we reformulate them into a list of triple hints $H^{t}=\left[\left(\mathcal{A}^{t}, K_{i}^{t}, v_{i}^{t}\right)\right]<em _j_="(j)">{i=1}^{l^{t}}$. For some complex types of QAs, we choose more than one target entities, and concatenate their lists of hints. For better demonstration, we re-index the final list of hints as $H=\left[\left(\mathcal{A}</em>$, where $l$ is the number of hints in the final list.}, K_{(j)}, v_{(j)}\right)\right]_{j=1}^{l</p>
<p>Construction of User Messages. Based on the $j$-th hint $\left(\mathcal{A}<em _j_="(j)">{(j)}, K</em>}, v_{(j)}\right) \in H$, we construct the corresponding user message $m_{(j)}$ with LLM, where we have $m_{(j)}=L L M\left(\mathcal{A<em _j_="(j)">{(j)}, K</em>$.}, v_{(j)}\right)$. Here, the LLM only serves the purpose of rewriting structural hints, without any reasoning process. For example, if the hint is (my uncle Bob, occupation, driver), the generated user message might be "The occupation of my uncle Bob is a driver". We generate user messages for all the hints in $H$, and we finally get the list of user messages $M=\left[m_{(j)}\right]_{j=1}^{l</p>
<p>Construction of Questions and Answers. In order to evaluate the memory capability of LLM-based personal assistants more comprehensively, we propose to construct five representative types of QAs to cover various complexities in real-world scenarios, as detailed in Table 1. For each question $q$, we provide three forms of ground truths: (1) the textual answer $a$ that can correctly respond to $q$, (2) the correct choice $a$ among confusing choices $a^{\prime}$ (generated by LLM) as a single-choice format, and (3) the correct retrieval target $h \subseteq M$ that contains the required factual information to the question.</p>
<p>(i.) Single-hop QA. Single-hop QA is the most basic type of QAs, relying on a single piece message to directly answer the question. In constructing QA, we randomly select the $j$-th hint $\left(\mathcal{A}<em _j_="(j)">{(j)}, K</em>}, v_{(j)}\right)$ and generate the question $q=L L M\left(\mathcal{A<em _j_="(j)">{(j)}, K</em>\right}$.
(ii.) Multi-hop QA. Multi-hop QA necessitates the use of multiple messages to determine the correct answer, making it more complex than single-hop QA. In constructing Multi-hop QA, we first sample two hints $\left(\mathcal{A}}\right)$ through LLM rewriting, where the answer is $a=v_{(j)}$. Correspondingly, the retrieval target is $h=\left{m_{(j)<em _j_="(j)">{(j)}, K</em>}, v_{(j)}\right)$ and $\left(\mathcal{A<em _k_="(k)">{(k)}, K</em>}, v_{(k)}\right)$ from the same bridge entity $\mathcal{A}^{t}$ (i.e., $\mathcal{A}^{t}=\mathcal{A<em _k_="(k)">{(j)}=$ $\mathcal{A}</em>\right}$. By incorporating additional entities, the questions can be easily extended to more hops.
(iii.) Comparative QA. Comparative QA is an extensive type of multi-hop QA, which involves comparing two entities based on a shared attribute. We first select two hints $\left(\mathcal{A}}$ ). We then mask this bridge entity and generate the question $q=L L M\left(K_{(j)}, v_{(j)}, K_{(k)}\right)$ through LLM rewriting, where the answer is $a=v_{(k)}$. The target message set is $h=\left{m_{(j)}, m_{(k)<em _j_="(j)">{(j)}, K</em>}, v_{(j)}\right)$ and $\left(\mathcal{A<em _k_="(k)">{(k)}, K</em>}, v_{(k)}\right)$ from different entities with the same meaning attribute $K$ (i.e., $\mathcal{A<em k="k">{j} \neq \mathcal{A}</em>}$ and $K \cong$ $\left.K_{(j)} \cong K_{(k)}\right)$. We then rewrite the question $q=L L M\left(\mathcal{A<em _k_="(k)">{(j)}, \mathcal{A}</em>\right}$.
(iv.) Aggregative QA. Aggregative QA is a general type of comparative QA, which requires aggregating messages from more than two entities on a shared attribute. For construction, we choose $d$ hints $\left{\left(\mathcal{A}}, K\right)$ by LLM, where the answer $a=f\left(K, v_{(j)}, v_{(k)}\right)$ is derived from the function $f(\cdot)$. The retrieval target is $h=\left{m_{(j)}, m_{(k)<em k="k">{\left(j</em>\right)\right}}\right)}, K, v_{\left(j_{k}\right)<em _left_j__k="\left(j_{k">{k=1}^{d}$ from different entities with the same meaning attribute $K$. Then, we construct the question $q=L L M\left(\left{\mathcal{A}</em>\right}}\right)<em _left_j__k="\left(j_{k">{j=1}^{d}, K\right)$, where we obtain the answer $a=f\left(K,\left{v</em>\right}}\right)<em _left_j__k="\left(j_{k">{k=1}^{d}\right)$. The target message set should include all these related references, that is, $h=\left{m</em>\right}}\right)<em _j_="(j)">{k=1}^{d}$.
(v.) Post-processing QA. Post-processing QA addresses situations where personal questions require additional reasoning steps for agents to answer, based on the retrieved messages. We first select two hints $\left(\mathcal{A}</em>}, K_{(j)}, v_{(j)}\right)$ and $\left(\mathcal{A<em _k_="(k)">{(k)}, K</em>\right}$.
Infusion of Noise in User Messages. We integrate two types of noise in user messages by concatenation, in order to simulate real-world circumstances. The first type is entity-side noise, which refers to noisy messages that contain the selected attributes from unselected entities. The second type is attribute-side noise, which involves noisy messages that describe unselected attributes of the selected entities. Both types of noise can impact agents' ability to retrieve messages and generate answers.
Eventually, we formulate the trajectory $\xi=\left(M, q, a, a^{\prime}, h\right)$ by discarding all hints, where each trajectory serves as a test instance for evaluating the memory capability of LLM-based personal assistants. There are two insights into the causal generation mechanism. First, the factual information of messages and QAs are causally constructed from the shared hints that are sampled from user profiles, where LLMs are only responsible for rewriting based on the given information, rather than imagining or reasoning. This pipeline mitigates the impact of LLM hallucination on the factual information, keeping the reliability of QAs. It can also prevent contradictions among user messages from the same trajectory, because their hints are derived from the same user profile. Second, our method focuses on designing the asymmetric difficulty between constructing QAs (i.e., profiles $\rightarrow$ hints $\rightarrow$ messages, question and answer) and solving QAs (i.e., messages $\mid$ question $\rightarrow$ answer), which is critical for the automatic generation of evaluation datasets.}, v_{(k)}\right)$ from the same bridge entity $\mathcal{A}^{t}$. We then design a reasoning factor $\psi$ to generate the question $q=L L M\left(K_{(j)}, v_{(j)}, K_{(k)}, \psi\right)$, and derive the answer $a=f\left(K_{(k)}, v_{(k)}, \psi\right)$, where $\psi$ specifies the reasoning process. For example, it could be "the sum of the last five digits of the phone number $v_{(k)}$ ". Similarly, the retrieval target will be $h=\left{m_{(j)}, m_{(k)</p>
<h1>3.4 MemDaily: A Dataset in the Daily-life Scenario</h1>
<p>Based on MemSim, we create a dataset in the daily-life scenario, named MemDaily, which can be used to evaluate the memory capability of LLM-based personal assistants, shown in Figure 1(c). Specifically, MemDaily incorporates 11 entities and 73 attributes (see details in Appendix D.1), all of which are representative and closely related to users' daily lives. We create 6 sub-datasets of different QA types mentioned previously: (1) Simple (Simp.): single-hop QAs. (2) Conditional (Cond.): multi-hop QAs with conditions. (3) Comparative (Comp.): comparative QAs. (4) Aggregative (Aggr.): aggregative QAs. (5) Post-processing (Post.): post-processing QAs. (6) Noisy: multi-hop QAs with additional irrelevant noisy texts inside questions. The summary of MemDaily is shown in Table 2, where we present the number of trajectories, user messages, questions, and TPM (tokens per message). More details and examples can be found in Appendix D.</p>
<p>Table 2: Summary of the MemDaily dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Statistic</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Trajectories</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">492</td>
<td style="text-align: center;">462</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">2,954</td>
</tr>
<tr>
<td style="text-align: center;">Messages</td>
<td style="text-align: center;">4215</td>
<td style="text-align: center;">4195</td>
<td style="text-align: center;">3144</td>
<td style="text-align: center;">5536</td>
<td style="text-align: center;">4438</td>
<td style="text-align: center;">4475</td>
<td style="text-align: center;">26,003</td>
</tr>
<tr>
<td style="text-align: center;">Questions</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">492</td>
<td style="text-align: center;">462</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">2,954</td>
</tr>
<tr>
<td style="text-align: center;">TPM</td>
<td style="text-align: center;">15.48</td>
<td style="text-align: center;">15.49</td>
<td style="text-align: center;">14.66</td>
<td style="text-align: center;">14.65</td>
<td style="text-align: center;">17.07</td>
<td style="text-align: center;">16.14</td>
<td style="text-align: center;">15.59</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of the evaluation on user profiles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">R-Human</th>
<th style="text-align: center;">R-GPT</th>
<th style="text-align: center;">SWI-R</th>
<th style="text-align: center;">SWI-O</th>
<th style="text-align: center;">SWI-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">IndePL</td>
<td style="text-align: center;">$1.35 \pm 0.53$</td>
<td style="text-align: center;">4.32</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.347</td>
</tr>
<tr>
<td style="text-align: center;">SeqPL</td>
<td style="text-align: center;">$1.64 \pm 0.73$</td>
<td style="text-align: center;">4.40</td>
<td style="text-align: center;">1.471</td>
<td style="text-align: center;">1.416</td>
<td style="text-align: center;">1.443</td>
</tr>
<tr>
<td style="text-align: center;">JointPL</td>
<td style="text-align: center;">$3.02 \pm 1.14$</td>
<td style="text-align: center;">$\mathbf{4 . 8 0}$</td>
<td style="text-align: center;">1.425</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.943</td>
</tr>
<tr>
<td style="text-align: center;">MemSim</td>
<td style="text-align: center;">$\mathbf{4 . 9 1 \pm 0 . 3 0}$</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">$\mathbf{3 . 2 0 6}$</td>
<td style="text-align: center;">$\mathbf{2 . 8 9 5}$</td>
<td style="text-align: center;">$\mathbf{3 . 0 5 0}$</td>
</tr>
</tbody>
</table>
<h1>4 Evaluations</h1>
<p>In this section, we evaluate the quality of MemDaily, which can reflect the effectiveness of MemSim. Specifically, the evaluations are conducted in three parts: the user profiles, the user messages, and the constructed QAs. Besides, we also conduct comprehensive case studies in Appendix D.</p>
<h3>4.1 Evaluation on User Profiles</h3>
<p>The generated user profiles are supposed to express both rationality and diversity, which also directly influence the creation of user messages and QAs. Therefore, we evaluate these two aspects to reflect their quality. Rationality means that the user profiles should possibly exist in the real world, with no internal contradictions in their descriptions. Diversity indicates that the descriptions among users are distinct, covering a wide range of user types.
Metrics. For rationality, we recruit six human evaluators to score the generated user profiles on a scale from 1 to 5 . Additionally, we use GPT-4o ${ }^{1}$ as a reference for scoring. These two metrics are denoted as R-Human and R-GPT. For diversity, we calculate the average Shannon-Wiener Index (SWI) [32] on key attributes, using the following formula:</p>
<p>$$
\text { SWI- } \mathcal{W}=-\frac{1}{|\mathcal{W}|} \sum_{X_{k} \in \mathcal{W}} \sum_{x_{i} \in X_{k}} p\left(x_{i}\right) \ln p\left(x_{i}\right)
$$</p>
<p>where $\mathcal{W} \subseteq \mathcal{X}$ is the subset of attribute variables. Therefore, we calculate SWI-R, SWI-O, and SWIA, corresponding to role-relevant attributes, role-irrelevant attributes, and all attributes, respectively.
Baselines. We design several baselines to generate user profiles: (1) JointPL: prompting an LLM to generate attributes jointly. (2) SeqPL: prompting an LLM to generate attributes sequentially, conditioned on previous attributes in linear order. (3) IndePL: prompting an LLM to generate attributes independently. We compare our method with these baselines on generating user profiles.
Results. As shown in Table 3, MemSim outperforms other baselines on R-Human, demonstrating the effectiveness of BRNet as an ablation study. However, we also observe an inconsistency between R-Human and R-GPT, which may be due to the inaccuracy of the LLM's scoring [33]. Furthermore, our method achieves the highest diversity compared to the other baselines.</p>
<h3>4.2 Evaluation on User Messages</h3>
<p>We evaluate the quality of generated user messages in multiple aspects, including fluency, rationality, naturalness, informativeness, and diversity. The first four aspects are designed to assess the quality inside a trajectory, while the final one targets the variety across trajectories.
Metrics. For the inside-trajectory aspects, human evaluators score user messages on a scale from 1 to 5, denoted as F-Human (fluency), R-Human (rationality), N-Human (naturalness), and I-Human</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Results of the evaluation on user messages.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">F-Human</th>
<th style="text-align: center;">R-Human</th>
<th style="text-align: center;">N-Human</th>
<th style="text-align: center;">I-Human</th>
<th style="text-align: center;">SWIP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ZeroCons</td>
<td style="text-align: center;">$4.94 \pm 0.24$</td>
<td style="text-align: center;">$\mathbf{4 . 9 4 \pm 0 . 2 4}$</td>
<td style="text-align: center;">$4.85 \pm 0.35$</td>
<td style="text-align: center;">$2.82 \pm 1.15$</td>
<td style="text-align: center;">2.712</td>
</tr>
<tr>
<td style="text-align: center;">PartCons</td>
<td style="text-align: center;">$\mathbf{4 . 9 8 \pm 0 . 1 4}$</td>
<td style="text-align: center;">$4.94 \pm 0.37$</td>
<td style="text-align: center;">$\mathbf{4 . 9 7 \pm 0 . 1 8}$</td>
<td style="text-align: center;">$4.01 \pm 1.18$</td>
<td style="text-align: center;">6.047</td>
</tr>
<tr>
<td style="text-align: center;">SoftCons</td>
<td style="text-align: center;">$4.93 \pm 0.30$</td>
<td style="text-align: center;">$4.80 \pm 0.77$</td>
<td style="text-align: center;">$4.91 \pm 0.42$</td>
<td style="text-align: center;">$\mathbf{4 . 3 7 \pm 0 . 9 8}$</td>
<td style="text-align: center;">5.868</td>
</tr>
<tr>
<td style="text-align: center;">MemSim</td>
<td style="text-align: center;">$4.93 \pm 0.30$</td>
<td style="text-align: center;">$4.93 \pm 0.39$</td>
<td style="text-align: center;">$4.90 \pm 0.41$</td>
<td style="text-align: center;">$3.61 \pm 1.19$</td>
<td style="text-align: center;">$\mathbf{6 . 1 2 5}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results of the evaluation on questions and answers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question Types</th>
<th style="text-align: center;">Textual Answers</th>
<th style="text-align: center;">Single-choice Answers</th>
<th style="text-align: center;">Retrieval Target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Conditional</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Comparative</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Aggregative</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Post-processing</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Noisy</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$99.8 \%$</td>
<td style="text-align: center;">$99.5 \%$</td>
<td style="text-align: center;">$99.8 \%$</td>
</tr>
</tbody>
</table>
<p>(informativeness). GPT-4o scores are also available and detailed in Appendix B. To assess the diversity across trajectories, we extract all entities and calculate their average Shannon-Wiener Index per 10,000 tokens of user messages, referred to as SWIP.
Baselines. We implement several baselines that generate messages under different constraints regarding user profiles and tasks: (1) ZeroCons: no constraints on attributes when prompting LLMs. (2) PartCons: partial attributes of user profiles are constrained in prompts for LLMs. (3) SoftCons: full attributes of user profiles are constrained in prompts but they are not forcibly for generation. Our MemSim method imposes the most strict constraints, requiring both the integration of specific attributes into user messages and ensuring that questions are answerable with established ground truths based on the shared hints. Generally, higher constraint commonly means sacrifice of fluency and naturalness, because it compulsively imposes certain information to benefit QA constructions.</p>
<p>Results. As shown in Table 4, our method maintains relatively high scores despite the rigorous constraints on constructing reliable QAs. Additionally, MemSim exhibits the highest diversity index, attributed to the BRNet and the causal generation mechanism that produces a wider variety of user messages based on the provided hierarchical user profiles.</p>
<h1>4.3 Evaluation on Questions and Answers</h1>
<p>The primary challenge for constructing a reliable dataset is ensuring the accuracy of ground truths for the constructed questions. To assess the reliability of MemDaily, we sample approximately 20\% of all the trajectories in MemDaily and employ human evaluators to verify the correctness of their ground truths. Specifically, the evaluators are required to examine three parts of the ground truths: textual answers, single-choice answers, and retrieval targets, and report their accuracy.
Metrics. The accuracy of textual answers assesses whether an answer correctly responds to the question based on the user messages within the same trajectory. The accuracy of single-choice answers indicates whether the ground truth choice is the sole correct answer for the question, given the user messages, while other choices are incorrect. The accuracy of retrieval targets evaluates whether the messages of the retrieval target are sufficient and necessary to answer the question.
Results. As shown in Table 5, MemDaily significantly ensures the accuracy of the answers provided for constructed questions. In the few instances where accuracy is compromised, it is attributed to the rewriting process by LLMs, which occasionally leads to information deviation. The results also demonstrate that MemSim can effectively mitigate the impact of LLM hallucinations on factual information, addressing a critical challenge in generating reliable questions and answers for memory evaluation. Another baseline method that directly generates answers through LLMs based on targeted user messages and questions performs much lower reliability. We implement this method and present the results as OracleMem in our constructed benchmarks in Section 5.2.</p>
<h1>5 Benchmark</h1>
<p>In this section, we create a benchmark based on the MemDaily dataset, in order to evaluate the memory capability of LLM-based personal assistants. Our benchmark sets various levels of difficulty by introducing different proportions of question-irrelevant daily-life posts.</p>
<h3>5.1 Experimental Settings</h3>
<p>Levels of Difficulty. We utilize the MemDaily dataset as the basis of our benchmark. In order to set different levels of difficulty, we collect question-irrelevant posts from social media platforms, and randomly incorporate them into user messages by controlling their proportions. Specifically, we denote MemDaily-vanilla as the vanilla and easiest one without extra additions, and create a series of MemDaily- $\eta$, where we use $\eta$ to represent the inverse percentage of original user messages. Larger $\eta$ indicates a higher level of difficulty in the benchmark. We primarily focus on MemDaily-vanilla and MemDaily-100 as representatives. We also conduct evaluations on MemDaily-10, MemDaily-50, and MemDaily-200, putting their experimental results in Appendix C.</p>
<p>Baselines. We implement several common memory mechanisms for LLM-based agents according to previous studies [10], including (1) Full Memory (FullMem): saves all previous messages as a list and concatenates them into the prompt for LLM inference. (2) Recent Memory (ReceMem): maintains the most recent $k$ messages and concatenates them into the prompt for LLM inference, also referred to as short-term memory. (3) Retrieved Memory (RetrMem): stores all previous messages using FAISS [34] and retrieves the top- $k$ relevant messages for inclusion in the prompt for LLM inference, which is commonly used to construct long-term memory. Specifically, we use Llama-160m [35] to transform a message into a 768-dimensional embedding and compute relevance scores using cosine similarity [36]. (4) None Memory (NonMem): does not use memory for LLM inference. Additionally, we include two special baselines for reference: (5) Noisy Memory (NoisyMem): receives only untargeted messages. (6) Oracle Memory (OracleMem): receives only targeted messages. Here, the targeted messages indicate the messages in the ground truth retrieval target. For all methods, we use the open-source GLM-4-9B [37] as the foundational model for inference, as a result of its excellent ability in long-context scenarios.</p>
<p>Metrics. We propose to evaluate the memory of LLM-based agents from two perspectives: effectiveness and efficiency. Effectiveness refers to the agent's ability to store and utilize factual information. The metrics for effectiveness include: (1) Accuracy: The correctness of agents' responses, measured by their ability to answer personal questions based on the factual information from historical user messages. (2) Recall@5: The percentage of messages in retrieval target successfully retrieved within the top-5 relevant messages. Efficiency mainly assesses the time cost associated with storing and utilizing information from memory. We use two metrics to evaluate efficiency: (1) Response Time: The time taken for an agent to respond after receiving a query, covering the retrieval and utilization processes. (2) Adaptation Time: The time required for an agent to store a new message.</p>
<h3>5.2 Effectiveness of Memory Mechanisms</h3>
<p>Accuracy of factual question-answering. The results of accuracy are presented in Table 6. FullMem and RetrMem demonstrate superior performance compared to other memory mechanisms, achieving high accuracy across both datasets. ReceMem tends to underperform when a large volume of noisy messages is present, as target messages may fall outside the memory window. We observe that agents excel with simple, conditional, post-processing, and noisy questions but struggle with comparative and aggregative questions. By comparing with OracleMem, we find the primary difficulty possibly lies in retrieving target messages. Even with accurate retrieval, aggregative questions remain challenging, indicating a potential bottleneck in textual memory. An interesting phenomenon we notice is that NoisyMem shows higher accuracy than NonMem in MemDaily-vanilla but lower accuracy in MemDaily-100. Similarly, FullMem unexpectedly outperforms OracleMem on simple questions in MemDaily. We suspect that LLMs may perform better with memory prompts of medium length, suggesting a potential limitation of textual memory mechanisms for LLM-based agents.</p>
<p>Recall of target message retrieval. We implement three retrieval methods to obtain the most relevant messages and compare them with target messages to calculate Recall@5. Embedding refers to the retrieval process used in RetrMem. Recency considers the most recent $k$ messages as the result. LLM directly uses the LLM to respond with the top- $k$ relevant messages. The results are presented</p>
<p>Table 6: Results of accuracy for factual question-answering.</p>
<table>
<thead>
<tr>
<th>MemDaily-vanilla</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>FullMem</td>
<td>0.976 $\pm 0.022$</td>
<td>0.982 $\pm 0.017$</td>
<td>0.859 $\pm 0.054$</td>
<td>0.320 $\pm 0.079$</td>
<td>0.848 $\pm 0.045$</td>
<td>0.966 $\pm 0.028$</td>
</tr>
<tr>
<td>RetrMem</td>
<td>0.898 $\pm 0.048$</td>
<td>0.882 $\pm 0.040$</td>
<td>0.771 $\pm 0.078$</td>
<td>0.317 $\pm 0.061$</td>
<td>0.800 $\pm 0.054$</td>
<td>0.786 $\pm 0.040$</td>
</tr>
<tr>
<td>ReceMem</td>
<td>0.832 $\pm 0.080$</td>
<td>0.798 $\pm 0.046$</td>
<td>0.631 $\pm 0.069$</td>
<td>0.257 $\pm 0.040$</td>
<td>0.760 $\pm 0.051$</td>
<td>0.764 $\pm 0.042$</td>
</tr>
<tr>
<td>NonMem</td>
<td>0.508 $\pm 0.032$</td>
<td>0.452 $\pm 0.059$</td>
<td>0.157 $\pm 0.049$</td>
<td>0.254 $\pm 0.055$</td>
<td>0.594 $\pm 0.073$</td>
<td>0.380 $\pm 0.060$</td>
</tr>
<tr>
<td>NoisyMem</td>
<td>0.512 $\pm 0.044$</td>
<td>0.468 $\pm 0.054$</td>
<td>0.204 $\pm 0.067$</td>
<td>0.239 $\pm 0.058$</td>
<td>0.590 $\pm 0.045$</td>
<td>0.388 $\pm 0.048$</td>
</tr>
<tr>
<td>OracleMem</td>
<td>0.966 $\pm 0.020$</td>
<td>0.988 $\pm 0.013$</td>
<td>0.910 $\pm 0.032$</td>
<td>0.376 $\pm 0.057$</td>
<td>0.888 $\pm 0.053$</td>
<td>0.984 $\pm 0.017$</td>
</tr>
<tr>
<td>MemDaily-100</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>FullMem</td>
<td>0.962 $\pm 0.017$</td>
<td>0.938 $\pm 0.033$</td>
<td>0.586 $\pm 0.076$</td>
<td>0.343 $\pm 0.047$</td>
<td>0.804 $\pm 0.041$</td>
<td>0.872 $\pm 0.041$</td>
</tr>
<tr>
<td>RetrMem</td>
<td>0.892 $\pm 0.034$</td>
<td>0.840 $\pm 0.036$</td>
<td>0.706 $\pm 0.074$</td>
<td>0.320 $\pm 0.092$</td>
<td>0.770 $\pm 0.055$</td>
<td>0.726 $\pm 0.052$</td>
</tr>
<tr>
<td>ReceMem</td>
<td>0.500 $\pm 0.063$</td>
<td>0.442 $\pm 0.058$</td>
<td>0.104 $\pm 0.048$</td>
<td>0.257 $\pm 0.054$</td>
<td>0.600 $\pm 0.060$</td>
<td>0.386 $\pm 0.076$</td>
</tr>
<tr>
<td>NonMem</td>
<td>0.508 $\pm 0.032$</td>
<td>0.454 $\pm 0.065$</td>
<td>0.159 $\pm 0.052$</td>
<td>0.252 $\pm 0.043$</td>
<td>0.594 $\pm 0.032$</td>
<td>0.380 $\pm 0.057$</td>
</tr>
<tr>
<td>NoisyMem</td>
<td>0.458 $\pm 0.071$</td>
<td>0.422 $\pm 0.051$</td>
<td>0.261 $\pm 0.068$</td>
<td>0.283 $\pm 0.041$</td>
<td>0.566 $\pm 0.064$</td>
<td>0.348 $\pm 0.044$</td>
</tr>
<tr>
<td>OracleMem</td>
<td>0.966 $\pm 0.020$</td>
<td>0.988 $\pm 0.016$</td>
<td>0.912 $\pm 0.045$</td>
<td>0.372 $\pm 0.062$</td>
<td>0.888 $\pm 0.038$</td>
<td>0.984 $\pm 0.012$</td>
</tr>
</tbody>
</table>
<p>Table 7: Results of recall@5 for target message retrieval.</p>
<table>
<thead>
<tr>
<th>MemDaily-vanilla</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>LLM</td>
<td>0.888 $\pm 0.025$</td>
<td>0.851 $\pm 0.020$</td>
<td>0.947 $\pm 0.018$</td>
<td>0.544 $\pm 0.021$</td>
<td>0.800 $\pm 0.028$</td>
<td>0.846 $\pm 0.036$</td>
</tr>
<tr>
<td>Embedding</td>
<td>0.735 $\pm 0.064$</td>
<td>0.717 $\pm 0.041$</td>
<td>0.845 $\pm 0.022$</td>
<td>0.515 $\pm 0.059$</td>
<td>0.693 $\pm 0.033$</td>
<td>0.648 $\pm 0.018$</td>
</tr>
<tr>
<td>Recency</td>
<td>0.514 $\pm 0.052$</td>
<td>0.513 $\pm 0.038$</td>
<td>0.698 $\pm 0.034$</td>
<td>0.237 $\pm 0.026$</td>
<td>0.511 $\pm 0.053$</td>
<td>0.504 $\pm 0.047$</td>
</tr>
<tr>
<td>MemDaily-100</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>LLM</td>
<td>0.612 $\pm 0.021$</td>
<td>0.479 $\pm 0.037$</td>
<td>0.683 $\pm 0.036$</td>
<td>0.290 $\pm 0.027$</td>
<td>0.439 $\pm 0.047$</td>
<td>0.430 $\pm 0.059$</td>
</tr>
<tr>
<td>Embedding</td>
<td>0.698 $\pm 0.049$</td>
<td>0.653 $\pm 0.061$</td>
<td>0.778 $\pm 0.048$</td>
<td>0.490 $\pm 0.037$</td>
<td>0.567 $\pm 0.042$</td>
<td>0.543 $\pm 0.034$</td>
</tr>
<tr>
<td>Recency</td>
<td>0.002 $\pm 0.003$</td>
<td>0.003 $\pm 0.004$</td>
<td>0.002 $\pm 0.003$</td>
<td>0.000 $\pm 0.001$</td>
<td>0.002 $\pm 0.003$</td>
<td>$&lt;0.001$</td>
</tr>
</tbody>
</table>
<p>in Table 7. We find that LLM performs best in short-context scenarios, while Embedding achieves higher recall scores in longer contexts. Additionally, we notice that separating the retrieval and inference stages may exhibit different performances compared with integrating them.</p>
<h1>5.3 Efficiency of Memory Mechanisms</h1>
<p>The results of efficiency are presented in Table 8 and Table 9. We find that RetrMem consumes the most response time in short-context scenarios, and FullMem also requires more time for inference due to longer memory prompts. However, the response time of FullMem increases significantly faster than that of other methods as the context lengthens. Regarding adaptation time, we observe that RetrMem requires substantially more time because it needs to build indexes in the FAISS system.</p>
<h2>6 Limitations and Conclusions</h2>
<p>In this paper, we propose MemSim, a Bayesian simulator designed to generate reliable datasets for evaluating the memory capability of LLM-based agents. MemSim comprises two primary components: The bayesian Relation Network and the causal generation mechanism. Utilizing MemSim, we generate MemDaily as a dataset in the daily-life scenario, and conduct extensive evaluations to assess its quality to reflect the effectiveness of MemSim. Additionally, we provide a benchmark on different memory mechanisms of LLM-based agents and provide further analysis. However, as the very initial study, there are several limitations. Firstly, our work focuses on evaluating the memory capability of LLM-based agents on factual information, but does not address higher-level and abstract information, such as users' hidden hobbies and preferences. Additionally, our evaluation does not include dialogue forms, which are more complex and challenging to ensure reliability. In future works, we aim to address these two issues within the benchmark.</p>
<p>Table 8: Results of response time for generating answers (seconds per query).</p>
<table>
<thead>
<tr>
<th>MemDaily-vanilla</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>FullMem</td>
<td>$0.139 \pm 0.001$</td>
<td>$0.141 \pm 0.001$</td>
<td>$0.132 \pm 0.001$</td>
<td>$0.154 \pm 0.002$</td>
<td>$0.152 \pm 0.002$</td>
<td>$0.150 \pm 0.003$</td>
</tr>
<tr>
<td>RetrMem</td>
<td>$0.290 \pm 0.007$</td>
<td>$0.277 \pm 0.007$</td>
<td>$0.267 \pm 0.009$</td>
<td>$0.236 \pm 0.009$</td>
<td>$0.257 \pm 0.004$</td>
<td>$0.284 \pm 0.007$</td>
</tr>
<tr>
<td>ReceMem</td>
<td>$0.126 \pm 0.001$</td>
<td>$0.127 \pm 0.001$</td>
<td>$0.125 \pm 0.000$</td>
<td>$0.125 \pm 0.001$</td>
<td>$0.135 \pm 0.001$</td>
<td>$0.134 \pm 0.001$</td>
</tr>
<tr>
<td>NonMem</td>
<td>$0.118 \pm 0.000$</td>
<td>$0.119 \pm 0.000$</td>
<td>$0.118 \pm 0.000$</td>
<td>$0.118 \pm 0.000$</td>
<td>$0.121 \pm 0.001$</td>
<td>$0.121 \pm 0.000$</td>
</tr>
<tr>
<td>NoisyMem</td>
<td>$0.118 \pm 0.000$</td>
<td>$0.119 \pm 0.000$</td>
<td>$0.118 \pm 0.001$</td>
<td>$0.118 \pm 0.000$</td>
<td>$0.121 \pm 0.001$</td>
<td>$0.121 \pm 0.000$</td>
</tr>
<tr>
<td>OracleMem</td>
<td>$0.122 \pm 0.001$</td>
<td>$0.122 \pm 0.001$</td>
<td>$0.122 \pm 0.000$</td>
<td>$0.131 \pm 0.001$</td>
<td>$0.129 \pm 0.002$</td>
<td>$0.128 \pm 0.001$</td>
</tr>
<tr>
<td>MemDaily-100</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>FullMem</td>
<td>$1.632 \pm 0.097$</td>
<td>$1.648 \pm 0.101$</td>
<td>$1.196 \pm 0.077$</td>
<td>$2.522 \pm 0.129$</td>
<td>$1.782 \pm 0.136$</td>
<td>$1.799 \pm 0.102$</td>
</tr>
<tr>
<td>RetrMem</td>
<td>$0.207 \pm 0.020$</td>
<td>$0.223 \pm 0.005$</td>
<td>$0.228 \pm 0.011$</td>
<td>$0.205 \pm 0.008$</td>
<td>$0.228 \pm 0.029$</td>
<td>$0.284 \pm 0.022$</td>
</tr>
<tr>
<td>ReceMem</td>
<td>$0.120 \pm 0.000$</td>
<td>$0.125 \pm 0.008$</td>
<td>$0.121 \pm 0.001$</td>
<td>$0.120 \pm 0.000$</td>
<td>$0.125 \pm 0.001$</td>
<td>$0.124 \pm 0.001$</td>
</tr>
<tr>
<td>NonMem</td>
<td>$0.119 \pm 0.001$</td>
<td>$0.119 \pm 0.000$</td>
<td>$0.119 \pm 0.000$</td>
<td>$0.119 \pm 0.001$</td>
<td>$0.123 \pm 0.000$</td>
<td>$0.122 \pm 0.001$</td>
</tr>
<tr>
<td>NoisyMem</td>
<td>$1.578 \pm 0.124$</td>
<td>$1.591 \pm 0.187$</td>
<td>$1.153 \pm 0.073$</td>
<td>$2.424 \pm 0.138$</td>
<td>$1.717 \pm 0.095$</td>
<td>$1.735 \pm 0.158$</td>
</tr>
<tr>
<td>OracleMem</td>
<td>$0.122 \pm 0.001$</td>
<td>$0.123 \pm 0.001$</td>
<td>$0.123 \pm 0.001$</td>
<td>$0.132 \pm 0.001$</td>
<td>$0.130 \pm 0.001$</td>
<td>$0.129 \pm 0.001$</td>
</tr>
</tbody>
</table>
<p>Table 9: Results of adaptation time for storing messages (seconds per message).</p>
<table>
<thead>
<tr>
<th>MemDaily-vanilla</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>RetrMem</td>
<td>$0.222 \pm 0.009$</td>
<td>$0.182 \pm 0.004$</td>
<td>$0.151 \pm 0.009$</td>
<td>$0.136 \pm 0.010$</td>
<td>$0.133 \pm 0.004$</td>
<td>$0.112 \pm 0.005$</td>
</tr>
<tr>
<td>Others</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
</tr>
<tr>
<td>MemDaily-100</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Methods</td>
<td>Simp.</td>
<td>Cond.</td>
<td>Comp.</td>
<td>Aggr.</td>
<td>Post.</td>
<td>Noisy</td>
</tr>
<tr>
<td>RetrMem</td>
<td>$0.064 \pm 0.008$</td>
<td>$0.072 \pm 0.004$</td>
<td>$0.066 \pm 0.007$</td>
<td>$0.064 \pm 0.006$</td>
<td>$0.056 \pm 0.002$</td>
<td>$0.066 \pm 0.005$</td>
</tr>
<tr>
<td>Others</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
<td>$&lt;0.001$</td>
</tr>
</tbody>
</table>
<h1>References</h1>
<p>[1] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024.
[2] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.
[3] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.
[4] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng Zhang. Llm as os (llmao), agents as apps: Envisioning aios, agents and the aios-agent ecosystem. arXiv preprint arXiv:2312.03815, 2023.
[5] Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen. Recagent: A novel simulation paradigm for recommender systems. arXiv preprint arXiv:2306.02552, 2023.
[6] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.</p>
<p>[7] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the capability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024.
[8] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. arXiv preprint arXiv:2308.08239, 2023.
[9] Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. Prompted llms as chatbot modules for long open-domain conversation. arXiv preprint arXiv:2305.04533, 2023.
[10] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024.
[11] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724-19731, 2024.
[12] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322, 2023.
[13] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023.
[14] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.
[15] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023.
[16] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023.
[17] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719, 2023.
[18] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.
[19] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024.
[20] Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. A survey on complex knowledge base question answering: Methods, challenges and solutions. arXiv preprint arXiv:2105.11644, 2021.
[21] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. Graph retrieval-augmented generation: A survey. arXiv preprint arXiv:2408.08921, 2024.
[22] Lingxi Zhang, Jing Zhang, Xirui Ke, Haoyang Li, Xinmei Huang, Zhonghui Shao, Shulin Cao, and Xin Lv. A survey on complex factual question answering. AI Open, 4:1-12, 2023.
[23] Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang. Kqa pro: A dataset with explicit compositional programs for complex question answering over knowledge base. arXiv preprint arXiv:2007.03875, 2020.</p>
<p>[24] Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Suhang Wang, Yu Meng, and Jiawei Han. Graph chain-of-thought: Augmenting large language models by reasoning on graphs. arXiv preprint arXiv:2404.07103, 2024.
[25] Wenyu Huang, Guancheng Zhou, Mirella Lapata, Pavlos Vougiouklis, Sebastien Montella, and Jeff Z Pan. Prompting large language models with knowledge graphs for question answering involving long-tail facts. arXiv preprint arXiv:2405.06524, 2024.
[26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.
[27] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, et al. Crag-comprehensive rag benchmark. arXiv preprint arXiv:2406.04744, 2024.
[28] Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 201-206, 2016.
[29] Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions. arXiv preprint arXiv:1803.06643, 2018.
[30] Christina Heinze-Deml, Marloes H Maathuis, and Nicolai Meinshausen. Causal structure learning. Annual Review of Statistics and Its Application, 5(1):371-391, 2018.
[31] Arthur B Kahn. Topological sorting of large networks. Communications of the ACM, 5(11): $558-562,1962$.
[32] E Kathryn Morris, Tancredi Caruso, François Buscot, Markus Fischer, Christine Hancock, Tanja S Maier, Torsten Meiners, Caroline Müller, Elisabeth Obermaier, Daniel Prati, et al. Choosing and using diversity indices: insights for ecological applications from the german biodiversity exploratories. Ecology and evolution, 4(18):3514-3524, 2014.
[33] KuanChao Chu, Yi-Pei Chen, and Hideki Nakayama. A better llm evaluator for text generation: The impact of prompt output sequencing and optimization. arXiv preprint arXiv:2406.09972, 2024.
[34] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547, 2019.
[35] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification. arXiv preprint arXiv:2305.09781, 2023.
[36] Amit Singhal et al. Modern information retrieval: A brief overview. IEEE Data Eng. Bull., 24 (4):35-43, 2001.
[37] GLM Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv e-prints, pages arXiv-2406, 2024.</p>
<h1>A Proof in Bayesian Relation Network</h1>
<h2>A. 1 Proof of Theorem 1</h2>
<p>Theorem 1 (Factorization). The joint probability distribution of BRNet can be expressed as</p>
<p>$$
P\left(X_{1}, X_{2}, \ldots, X_{|\mathcal{X}|}\right)=\prod_{X_{t} \in \mathcal{X}} P\left(X_{t} \mid \operatorname{par}\left(X_{t}\right)\right)
$$</p>
<p>where $\operatorname{par}\left(X_{t}\right)$ denotes the set of parent attributes of $X_{t}$.
Proof. Because BRNet is DAG, we can certainly find a topological ordering</p>
<p>$$
O=\left[o_{1}, o_{2}, \ldots, o_{|\mathcal{X}|}\right]
$$</p>
<p>Then, we inverse the sequence to get a reversed topologically ordering</p>
<p>$$
\tilde{O}=\left[\tilde{o}<em 2="2">{1}, \tilde{o}</em>\right]
$$}, \ldots, \tilde{o}_{|\mathcal{X}|</p>
<p>Then, we utilize the theorem of conditional probability according to the order $\tilde{O}$, and we have</p>
<p>$$
\begin{aligned}
P\left(X_{1}, X_{2}, \ldots, X_{|\mathcal{X}|}\right) &amp; =P\left(X_{\tilde{o}<em _tilde_o="\tilde{o">{1}} \mid X</em><em _tilde_o="\tilde{o">{2}}, \ldots, X</em><em _tilde_o="\tilde{o">{|\mathcal{X}|}}\right) \cdot P\left(X</em><em _tilde_o="\tilde{o">{2}} \mid X</em><em _tilde_o="\tilde{o">{3}}, \ldots, X</em><em _tilde_o="\tilde{o">{|\mathcal{X}|}}\right) \ldots P\left(X</em><em i="1">{|\mathcal{X}|}}\right) \
&amp; =\prod</em>}^{|\mathcal{X}|} P\left(X_{\tilde{o<em i_1="i+1">{i}} \mid \mathbf{X}\left[\tilde{o}</em>\right]\right)
\end{aligned}
$$}: \tilde{o}_{|\mathcal{X}|</p>
<p>where $\mathbf{X}\left[\tilde{o}<em _mathcal_X="|\mathcal{X">{i+1}: \tilde{o}</em>$ in the reversed topologically ordering, and there are no descendant variables inside. According to Assumption 1, we have}|}\right]$ means all the variables after $\tilde{o}_{i+1</p>
<p>$$
P\left(X_{\tilde{o}<em i_1="i+1">{i}} \mid \mathbf{X}\left[\tilde{o}</em>}: \tilde{o<em _tilde_o="\tilde{o">{|\mathcal{X}|}\right]\right)=P\left(X</em><em _tilde_o="\tilde{o">{i}} \mid \operatorname{par}\left(X</em>\right)\right)
$$}_{i}</p>
<p>Finally, we rewrite it and obtain</p>
<p>$$
P\left(X_{1}, X_{2}, \ldots, X_{|\mathcal{X}|}\right)=\prod_{X_{t} \in \mathcal{X}} P\left(X_{t} \mid \operatorname{par}\left(X_{t}\right)\right)
$$</p>
<h2>A. 2 Proof of Theorem 2</h2>
<p>Theorem 2 (Ancestral Sampling). For BRNet, the result of ancestral sampling is equivalent to that of sampling from the joint probability distribution. Specifically, we have</p>
<p>$$
P\left(\tilde{x}<em 2="2">{1}, \tilde{x}</em>}, \ldots, \tilde{x<em 1="1">{|\mathcal{X}|}\right)=P\left(x</em>\right)
$$}, x_{2}, \ldots, x_{|\mathcal{X}|</p>
<p>where $x_{1}, x_{2}, \ldots, x_{|\mathcal{X}|} \sim P\left(X_{1}, X_{2}, \ldots, X_{|\mathcal{X}|}\right)$ are sampled from the joint probability distribution.
Proof. We first calculate the reversed topologically ordering</p>
<p>$$
\tilde{O}=\left[\tilde{o}<em 2="2">{1}, \tilde{o}</em>\right]
$$}, \ldots, \tilde{o}_{|\mathcal{X}|</p>
<p>Then, we have</p>
<p>$$
\begin{aligned}
P\left(\tilde{x}<em 2="2">{1}, \tilde{x}</em>}, \ldots, \tilde{x<em i="1">{|\mathcal{X}|}\right) &amp; =\prod</em>}^{|\mathcal{X}|} P\left(\tilde{x<em i="i">{\tilde{o}</em>}} \mid \tilde{\mathbf{x}}\left[\tilde{o<em _mathcal_X="|\mathcal{X">{i+1}: \tilde{o}</em>\right]\right) \
&amp; =\prod_{i=1}^{|\mathcal{X}|} P\left(\tilde{x}}|<em i="i">{\tilde{o}</em>}} \mid \operatorname{par}\left(\tilde{x<em i="i">{\tilde{o}</em>\right)\right)
\end{aligned}
$$}</p>
<p>where $\tilde{\mathbf{x}}\left[\tilde{o}<em _mathcal_X="|\mathcal{X">{i+1}: \tilde{o}</em>$ in the reversed topologically ordering. According to Assumption 2, we have}|}\right]$ means the values of all the variables after $\tilde{o}_{i+1</p>
<p>$$
\begin{aligned}
P\left(\tilde{x}<em 2="2">{1}, \tilde{x}</em>}, \ldots, \tilde{x<em i="1">{|\mathcal{X}|}\right) &amp; =\prod</em>}^{|\mathcal{X}|} P\left(x_{\tilde{o<em _tilde_o="\tilde{o">{i}} \mid \operatorname{par}\left(x</em><em 1="1">{i}}\right)\right) \
&amp; =P\left(x</em>\right)
\end{aligned}
$$}, x_{2}, \ldots, x_{|\mathcal{X}|</p>
<h1>B Extensive Evaluation on User Messages by GPT-4o</h1>
<p>We also let GPT-4o score on user messages as a reference, and the results are shown in Table 10.</p>
<p>Table 10: Results of evaluation on user messages by GPT-4o.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">F-GPT</th>
<th style="text-align: center;">R-GPT</th>
<th style="text-align: center;">N-GPT</th>
<th style="text-align: center;">I-GPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ZeroCons</td>
<td style="text-align: center;">4.04</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">3.04</td>
</tr>
<tr>
<td style="text-align: left;">PartCons</td>
<td style="text-align: center;">$\mathbf{4 . 2 8}$</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">$\mathbf{4 . 2 8}$</td>
</tr>
<tr>
<td style="text-align: left;">SoftCons</td>
<td style="text-align: center;">4.20</td>
<td style="text-align: center;">$\mathbf{5 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{5 . 0 0}$</td>
<td style="text-align: center;">3.96</td>
</tr>
<tr>
<td style="text-align: left;">MemSim</td>
<td style="text-align: center;">4.04</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">3.60</td>
</tr>
</tbody>
</table>
<h1>C Extensive Benchmark on More Composite Datasets</h1>
<h2>C. 1 Results on MemDaily-10</h2>
<p>The results of accuracy are shown in Table 11. The results of recall@5 are shown in Table 12. The results of response time are shown in Table 13. The results of adaptation time are shown in Table 14.</p>
<p>Table 11: Results of accuracy on MemDaily-10.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 2 \pm 0 . 0 4 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 6 \pm 0 . 0 2 8}$</td>
<td style="text-align: center;">$0.665 \pm 0.058$</td>
<td style="text-align: center;">$0.243 \pm 0.072$</td>
<td style="text-align: center;">$\mathbf{0 . 8 1 0 \pm 0 . 0 3 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 2 \pm 0 . 0 2 9}$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.896 \pm 0.033$</td>
<td style="text-align: center;">$0.882 \pm 0.047$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 9 \pm 0 . 0 6 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 5 \pm 0 . 0 4 5}$</td>
<td style="text-align: center;">$0.782 \pm 0.065$</td>
<td style="text-align: center;">$0.764 \pm 0.053$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$0.534 \pm 0.047$</td>
<td style="text-align: center;">$0.482 \pm 0.064$</td>
<td style="text-align: center;">$0.147 \pm 0.049$</td>
<td style="text-align: center;">$0.248 \pm 0.067$</td>
<td style="text-align: center;">$0.604 \pm 0.088$</td>
<td style="text-align: center;">$0.430 \pm 0.048$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$0.510 \pm 0.090$</td>
<td style="text-align: center;">$0.450 \pm 0.078$</td>
<td style="text-align: center;">$0.159 \pm 0.041$</td>
<td style="text-align: center;">$0.254 \pm 0.065$</td>
<td style="text-align: center;">$0.594 \pm 0.032$</td>
<td style="text-align: center;">$0.380 \pm 0.057$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$0.428 \pm 0.068$</td>
<td style="text-align: center;">$0.402 \pm 0.059$</td>
<td style="text-align: center;">$0.169 \pm 0.046$</td>
<td style="text-align: center;">$0.280 \pm 0.046$</td>
<td style="text-align: center;">$0.584 \pm 0.090$</td>
<td style="text-align: center;">$0.350 \pm 0.077$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$0.966 \pm 0.022$</td>
<td style="text-align: center;">$0.988 \pm 0.010$</td>
<td style="text-align: center;">$0.910 \pm 0.031$</td>
<td style="text-align: center;">$0.372 \pm 0.037$</td>
<td style="text-align: center;">$0.888 \pm 0.030$</td>
<td style="text-align: center;">$0.888 \pm 0.030$</td>
</tr>
</tbody>
</table>
<p>Table 12: Results of recall@5 on MemDaily-10.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM</td>
<td style="text-align: center;">$\mathbf{0 . 7 9 4 \pm 0 . 0 3 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 2 \pm 0 . 0 1 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 8 \pm 0 . 0 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 2 \pm 0 . 0 3 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 6 \pm 0 . 0 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 6 \pm 0 . 0 3 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Embedding</td>
<td style="text-align: center;">$0.704 \pm 0.039$</td>
<td style="text-align: center;">$0.833 \pm 0.026$</td>
<td style="text-align: center;">$0.506 \pm 0.052$</td>
<td style="text-align: center;">$0.643 \pm 0.043$</td>
<td style="text-align: center;">$0.609 \pm 0.027$</td>
<td style="text-align: center;">$0.648 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: left;">Recency</td>
<td style="text-align: center;">$0.032 \pm 0.017$</td>
<td style="text-align: center;">$0.011 \pm 0.010$</td>
<td style="text-align: center;">$0.013 \pm 0.011$</td>
<td style="text-align: center;">$0.030 \pm 0.012$</td>
<td style="text-align: center;">$0.009 \pm 0.007$</td>
<td style="text-align: center;">$0.504 \pm 0.047$</td>
</tr>
</tbody>
</table>
<p>Table 13: Results of response time on MemDaily-10 (seconds per query).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$0.243 \pm 0.008$</td>
<td style="text-align: center;">$0.243 \pm 0.008$</td>
<td style="text-align: center;">$0.208 \pm 0.003$</td>
<td style="text-align: center;">$0.306 \pm 0.008$</td>
<td style="text-align: center;">$0.263 \pm 0.006$</td>
<td style="text-align: center;">$0.262 \pm 0.010$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.213 \pm 0.002$</td>
<td style="text-align: center;">$0.230 \pm 0.005$</td>
<td style="text-align: center;">$0.246 \pm 0.008$</td>
<td style="text-align: center;">$0.212 \pm 0.002$</td>
<td style="text-align: center;">$0.240 \pm 0.004$</td>
<td style="text-align: center;">$0.292 \pm 0.014$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$0.120 \pm 0.000$</td>
<td style="text-align: center;">$0.121 \pm 0.000$</td>
<td style="text-align: center;">$0.120 \pm 0.000$</td>
<td style="text-align: center;">$0.119 \pm 0.002$</td>
<td style="text-align: center;">$0.126 \pm 0.001$</td>
<td style="text-align: center;">$0.124 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 9 \pm 0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 9 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 9 \pm 0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 7 \pm 0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 2 2 \pm 0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 9 \pm 0 . 0 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$0.205 \pm 0.005$</td>
<td style="text-align: center;">$0.207 \pm 0.007$</td>
<td style="text-align: center;">$0.181 \pm 0.004$</td>
<td style="text-align: center;">$0.253 \pm 0.010$</td>
<td style="text-align: center;">$0.223 \pm 0.005$</td>
<td style="text-align: center;">$0.222 \pm 0.006$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$0.121 \pm 0.001$</td>
<td style="text-align: center;">$0.123 \pm 0.001$</td>
<td style="text-align: center;">$0.122 \pm 0.000$</td>
<td style="text-align: center;">$0.131 \pm 0.001$</td>
<td style="text-align: center;">$0.130 \pm 0.001$</td>
<td style="text-align: center;">$0.128 \pm 0.001$</td>
</tr>
</tbody>
</table>
<p>Table 14: Results of adaptation time on MemDaily-10 (seconds per message).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.073 \pm 0.003$</td>
<td style="text-align: center;">$0.079 \pm 0.006$</td>
<td style="text-align: center;">$0.084 \pm 0.006$</td>
<td style="text-align: center;">$0.069 \pm 0.003$</td>
<td style="text-align: center;">$0.073 \pm 0.003$</td>
<td style="text-align: center;">$0.075 \pm 0.006$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
</tbody>
</table>
<h1>C. 2 Results of MemDaily-50</h1>
<p>The results of accuracy are shown in Table 15. The results of recall@5 are shown in Table 16. The results of response time are shown in Table 17. The results of adaptation time are shown in Table 18.</p>
<p>Table 15: Results of accuracy on MemDaily-50.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 2 \pm 0 . 0 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 8 \pm 0 . 0 2 0}$</td>
<td style="text-align: center;">$0.602 \pm 0.065$</td>
<td style="text-align: center;">$0.296 \pm 0.072$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0 2 \pm 0 . 0 4 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 0 \pm 0 . 0 4 1}$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.886 \pm 0.035$</td>
<td style="text-align: center;">$0.864 \pm 0.037$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 4 \pm 0 . 0 6 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 2 0 \pm 0 . 0 7 1}$</td>
<td style="text-align: center;">$0.780 \pm 0.059$</td>
<td style="text-align: center;">$0.748 \pm 0.049$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$0.508 \pm 0.042$</td>
<td style="text-align: center;">$0.434 \pm 0.052$</td>
<td style="text-align: center;">$0.108 \pm 0.044$</td>
<td style="text-align: center;">$0.237 \pm 0.054$</td>
<td style="text-align: center;">$0.588 \pm 0.066$</td>
<td style="text-align: center;">$0.376 \pm 0.099$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$0.510 \pm 0.061$</td>
<td style="text-align: center;">$0.452 \pm 0.055$</td>
<td style="text-align: center;">$0.159 \pm 0.039$</td>
<td style="text-align: center;">$0.254 \pm 0.066$</td>
<td style="text-align: center;">$0.594 \pm 0.078$</td>
<td style="text-align: center;">$0.380 \pm 0.055$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$0.454 \pm 0.040$</td>
<td style="text-align: center;">$0.416 \pm 0.083$</td>
<td style="text-align: center;">$0.229 \pm 0.071$</td>
<td style="text-align: center;">$0.272 \pm 0.073$</td>
<td style="text-align: center;">$0.568 \pm 0.078$</td>
<td style="text-align: center;">$0.360 \pm 0.084$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$0.966 \pm 0.025$</td>
<td style="text-align: center;">$0.988 \pm 0.010$</td>
<td style="text-align: center;">$0.910 \pm 0.053$</td>
<td style="text-align: center;">$0.376 \pm 0.042$</td>
<td style="text-align: center;">$0.888 \pm 0.032$</td>
<td style="text-align: center;">$0.984 \pm 0.012$</td>
</tr>
</tbody>
</table>
<p>Table 16: Results of recall@5 on MemDaily-50.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 5 \pm 0 . 0 4 7}$</td>
<td style="text-align: center;">$0.640 \pm 0.053$</td>
<td style="text-align: center;">$0.773 \pm 0.018$</td>
<td style="text-align: center;">$0.373 \pm 0.031$</td>
<td style="text-align: center;">$0.591 \pm 0.039$</td>
<td style="text-align: center;">$0.561 \pm 0.050$</td>
</tr>
<tr>
<td style="text-align: center;">Embedding</td>
<td style="text-align: center;">$0.710 \pm 0.041$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 4 \pm 0 . 0 2 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 9 0 \pm 0 . 0 3 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 7 \pm 0 . 0 3 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 1 \pm 0 . 0 3 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 4 \pm 0 . 0 5 3}$</td>
</tr>
<tr>
<td style="text-align: center;">Recency</td>
<td style="text-align: center;">$0.011 \pm 0.009$</td>
<td style="text-align: center;">$0.005 \pm 0.004$</td>
<td style="text-align: center;">$0.006 \pm 0.006$</td>
<td style="text-align: center;">$0.001 \pm 0.002$</td>
<td style="text-align: center;">$0.003 \pm 0.004$</td>
<td style="text-align: center;">$0.001 \pm 0.003$</td>
</tr>
</tbody>
</table>
<p>Table 17: Results of response time on MemDaily-50 (seconds per query).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$0.776 \pm 0.031$</td>
<td style="text-align: center;">$0.783 \pm 0.067$</td>
<td style="text-align: center;">$0.596 \pm 0.021$</td>
<td style="text-align: center;">$1.134 \pm 0.054$</td>
<td style="text-align: center;">$0.841 \pm 0.032$</td>
<td style="text-align: center;">$0.847 \pm 0.062$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.203 \pm 0.003$</td>
<td style="text-align: center;">$0.206 \pm 0.004$</td>
<td style="text-align: center;">$0.215 \pm 0.004$</td>
<td style="text-align: center;">$0.204 \pm 0.003$</td>
<td style="text-align: center;">$0.229 \pm 0.005$</td>
<td style="text-align: center;">$0.324 \pm 0.020$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$0.120 \pm 0.001$</td>
<td style="text-align: center;">$0.121 \pm 0.002$</td>
<td style="text-align: center;">$0.118 \pm 0.000$</td>
<td style="text-align: center;">$0.118 \pm 0.001$</td>
<td style="text-align: center;">$0.123 \pm 0.002$</td>
<td style="text-align: center;">$0.123 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 8 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 8 \pm 0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 7 \pm 0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 8 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 2 1 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 9 \pm 0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$0.728 \pm 0.037$</td>
<td style="text-align: center;">$0.737 \pm 0.041$</td>
<td style="text-align: center;">$0.562 \pm 0.027$</td>
<td style="text-align: center;">$1.060 \pm 0.055$</td>
<td style="text-align: center;">$0.787 \pm 0.028$</td>
<td style="text-align: center;">$0.794 \pm 0.058$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$0.121 \pm 0.001$</td>
<td style="text-align: center;">$0.122 \pm 0.001$</td>
<td style="text-align: center;">$0.121 \pm 0.001$</td>
<td style="text-align: center;">$0.131 \pm 0.001$</td>
<td style="text-align: center;">$0.129 \pm 0.001$</td>
<td style="text-align: center;">$0.128 \pm 0.001$</td>
</tr>
</tbody>
</table>
<p>Table 18: Results of adaptation time on MemDaily-50 (seconds per message).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.059 \pm 0.001$</td>
<td style="text-align: center;">$0.057 \pm 0.003$</td>
<td style="text-align: center;">$0.057 \pm 0.004$</td>
<td style="text-align: center;">$0.060 \pm 0.003$</td>
<td style="text-align: center;">$0.062 \pm 0.003$</td>
<td style="text-align: center;">$0.089 \pm 0.005$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
</tbody>
</table>
<h1>C. 3 Results of MemDaily-200</h1>
<p>The results of accuracy are shown in Table 19. The results of recall@5 are shown in Table 20. The results of response time are shown in Table 21. The results of adaptation time are shown in Table 22.</p>
<p>Table 19: Results of accuracy on MemDaily-200.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 2 \pm 0 . 0 4 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 2 \pm 0 . 0 3 6}$</td>
<td style="text-align: center;">$0.563 \pm 0.061$</td>
<td style="text-align: center;">$0.309 \pm 0.056$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 2 \pm 0 . 0 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6 6 \pm 0 . 0 4 4}$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.874 \pm 0.052$</td>
<td style="text-align: center;">$0.844 \pm 0.034$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 4 \pm 0 . 0 6 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 5 \pm 0 . 0 6 5}$</td>
<td style="text-align: center;">$0.766 \pm 0.046$</td>
<td style="text-align: center;">$0.714 \pm 0.052$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$0.486 \pm 0.046$</td>
<td style="text-align: center;">$0.420 \pm 0.057$</td>
<td style="text-align: center;">$0.114 \pm 0.036$</td>
<td style="text-align: center;">$0.272 \pm 0.054$</td>
<td style="text-align: center;">$0.570 \pm 0.055$</td>
<td style="text-align: center;">$0.366 \pm 0.051$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$0.470 \pm 0.057$</td>
<td style="text-align: center;">$0.454 \pm 0.077$</td>
<td style="text-align: center;">$0.157 \pm 0.045$</td>
<td style="text-align: center;">$0.257 \pm 0.069$</td>
<td style="text-align: center;">$0.592 \pm 0.082$</td>
<td style="text-align: center;">$0.380 \pm 0.048$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$0.398 \pm 0.052$</td>
<td style="text-align: center;">$0.398 \pm 0.068$</td>
<td style="text-align: center;">$0.282 \pm 0.058$</td>
<td style="text-align: center;">$0.276 \pm 0.068$</td>
<td style="text-align: center;">$0.564 \pm 0.037$</td>
<td style="text-align: center;">$0.350 \pm 0.035$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$0.990 \pm 0.013$</td>
<td style="text-align: center;">$0.988 \pm 0.013$</td>
<td style="text-align: center;">$0.910 \pm 0.034$</td>
<td style="text-align: center;">$0.374 \pm 0.063$</td>
<td style="text-align: center;">$0.888 \pm 0.056$</td>
<td style="text-align: center;">$0.984 \pm 0.012$</td>
</tr>
</tbody>
</table>
<p>Table 20: Results of recall@5 on MemDaily-200.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM</td>
<td style="text-align: center;">$0.457 \pm 0.066$</td>
<td style="text-align: center;">$0.356 \pm 0.051$</td>
<td style="text-align: center;">$0.556 \pm 0.035$</td>
<td style="text-align: center;">$0.176 \pm 0.022$</td>
<td style="text-align: center;">$0.342 \pm 0.048$</td>
<td style="text-align: center;">$0.322 \pm 0.043$</td>
</tr>
<tr>
<td style="text-align: left;">Embedding</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 4 \pm 0 . 0 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 1 \pm 0 . 0 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 3 \pm 0 . 0 3 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 4 \pm 0 . 0 5 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 4 \pm 0 . 0 5 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 8 \pm 0 . 0 5 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Recency</td>
<td style="text-align: center;">$0.001 \pm 0.003$</td>
<td style="text-align: center;">$0.001 \pm 0.002$</td>
<td style="text-align: center;">$0.001 \pm 0.002$</td>
<td style="text-align: center;">$0.000 \pm 0.001$</td>
<td style="text-align: center;">$0.001 \pm 0.003$</td>
<td style="text-align: center;">$0.000 \pm 0.000$</td>
</tr>
</tbody>
</table>
<p>Table 21: Results of response time on MemDaily-200 (seconds per query).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$4.028 \pm 0.161$</td>
<td style="text-align: center;">$3.914 \pm 0.213$</td>
<td style="text-align: center;">$2.697 \pm 0.100$</td>
<td style="text-align: center;">$6.365 \pm 0.374$</td>
<td style="text-align: center;">$4.252 \pm 0.328$</td>
<td style="text-align: center;">$4.307 \pm 0.283$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.236 \pm 0.023$</td>
<td style="text-align: center;">$0.241 \pm 0.018$</td>
<td style="text-align: center;">$0.238 \pm 0.024$</td>
<td style="text-align: center;">$0.585 \pm 0.230$</td>
<td style="text-align: center;">$1.012 \pm 0.690$</td>
<td style="text-align: center;">$1.252 \pm 0.427$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$\mathbf{0 . 1 3 0 \pm 0 . 0 0 2}$</td>
<td style="text-align: center;">$0.120 \pm 0.002$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 8 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$0.119 \pm 0.001$</td>
<td style="text-align: center;">$0.124 \pm 0.001$</td>
<td style="text-align: center;">$0.123 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$0.139 \pm 0.006$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 9 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$0.119 \pm 0.001$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 7 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 2 1 \pm 0 . 0 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 2 1 \pm 0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$3.947 \pm 0.209$</td>
<td style="text-align: center;">$3.832 \pm 0.203$</td>
<td style="text-align: center;">$2.637 \pm 0.118$</td>
<td style="text-align: center;">$6.221 \pm 0.325$</td>
<td style="text-align: center;">$4.158 \pm 0.226$</td>
<td style="text-align: center;">$4.214 \pm 0.288$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$0.141 \pm 0.003$</td>
<td style="text-align: center;">$0.122 \pm 0.001$</td>
<td style="text-align: center;">$0.121 \pm 0.001$</td>
<td style="text-align: center;">$0.131 \pm 0.002$</td>
<td style="text-align: center;">$0.128 \pm 0.002$</td>
<td style="text-align: center;">$0.128 \pm 0.001$</td>
</tr>
</tbody>
</table>
<p>Table 22: Results of adaptation time on MemDaily-200 (seconds per message).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Simp.</th>
<th style="text-align: center;">Cond.</th>
<th style="text-align: center;">Comp.</th>
<th style="text-align: center;">Aggr.</th>
<th style="text-align: center;">Post.</th>
<th style="text-align: center;">Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FullMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">RetrMem</td>
<td style="text-align: center;">$0.080 \pm 0.011$</td>
<td style="text-align: center;">$0.080 \pm 0.013$</td>
<td style="text-align: center;">$0.080 \pm 0.010$</td>
<td style="text-align: center;">$0.220 \pm 0.076$</td>
<td style="text-align: center;">$0.264 \pm 0.089$</td>
<td style="text-align: center;">$0.420 \pm 0.120$</td>
</tr>
<tr>
<td style="text-align: left;">ReceMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NonMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">NoisyMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">OracleMem</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
</tbody>
</table>
<h1>D Case Studies</h1>
<p>In this section, we present several case studies to illustrate the effectiveness of the data generated by MemDaily. First, we will display the hierarchical user profiles generated from BRNet. Next, we will present examples of user messages created by our method. Finally, we will provide examples of questions and answers for each type.</p>
<h2>D. 1 Case Study on Generated User Profiles</h2>
<p>In MemDaily, we incorporate 11 entities that cover 7 types, with 73 attributes of them. The summary of entities and attributes of MemDaily are provided in Table 23.
We introduce prior knowledge as several rules according to our scenarios to constrain among attributes. For example, a relative role is highly possible to share the same hometown with the user, because they are likely to come from the same place. All of these constraints are expressed in BRNet with causal relations. We generate 50 graphical user profiles and conduct observations, finding that most profiles align well with real-world users without contradictions.
Here is a case of user profiles, and we translate them into English for better demonstration:</p>
<h2>An example of Generated User Profiles</h2>
<h2>User Profiles:</h2>
<p>(Gender) Male; (Name) Qiang Wang; (Age) 38; (Height) 166cm; (Birthday) December 1st.; (Hometown) Beijing; (Workplace) Shenzhen, Guangdong; (Education) High School; (Occupation) Bank Teller; (Position) Head Teller; (Company) Huayin Financial Service Center; (Hobbies) Model Making; (Personality) Outgoing; (Phone) 13420824898; (Email) wangqiang1201@huayinfinance.com; (ID Number) 640168198612016598; (Passport Number) NZ0448096; (Bank Card Number) 6222022612177604; (Driver's License Number) 640168198612012730;</p>
<h2>College Role 1:</h2>
<p>(Gender) Female; (Relationship) Supervisor; (Name) Yalin Zhao; (Age) 44; (Height) 165cm; (Birthday) Febrary 5th.; (Hometown) Chongqing; (Workplace) Shenzhen, Guangdong; (Education) High School; (Occupation) Bank Teller; (Position) Bank Manager; (Company) Huayin Financial Service Center; (Hobbies) Sports; (Personality) Patient; (Phone) 13651039007; (Email) zhaoyalin0205@szfinancecenter.com;</p>
<h2>College Role 2:</h2>
<p>(Gender) Male; (Relationship) Colleague; (Name) Zhihong Sun; (Age) 39; (Height) 164cm; (Birthday) April 24th.; (Hometown) Chengdu, Sichuan; (Workplace) Shenzhen, Guangdong; (Education) High School; (Occupation) Bank Teller; (Position) Senior Teller; (Company) Huayin Financial Service Center; (Hobbies) Attending concerts; (Personality) Enthusiastic; (Phone) 15391721618; (Email) sunzhihong0421@huayinfinance.com;</p>
<h2>Relative Role 1:</h2>
<p>(Gender) Male; (Relationship) Cousin; (Name) Wei Zhang; (Age) 36; (Height) 169cm; (Birthday) July 15th.; (Hometown) Beijing; (Workplace) Hangzhou, Zhejiang; (Education) Doctor; (Occupation) Doctor; (Position) Chief Physician; (Company) West Lake Hospital; (Hobbies) Playing Video Games; (Personality) Patient; (Phone) 13225162475; (Email) zhangwei0715@westlakehospital.com;
Relative Role 2:
(Gender) Female; (Relationship) Cousin; (Name) Tingting Li; (Age) 36; (Height) 164cm; (Birthday) June 23rd.; (Hometown) Beijing; (Workplace) Shanghai; (Education) Master; (Occupation) Teacher; (Position) Middle School Language Teacher; (Company) Pudong No. 1 Middle School; (Hobbies) Yoga; (Personality) Patient; (Phone) 13401551341; (Email) litingting0623@ pdxzyz.com;
Work Event 1:
(Type) Job Fair; (Content) Job Fair for Bank Teller Supervisors in the Shenzhen area, sharing professional experience, recruiting talented individuals, and jointly creating a brilliant future for the banking industry.; (Location) Shenzhen, Guangdong; (Time) At 7 PM on the Sunday after next; (Title) Bank Teller Job Fair; (Scale) Around 500 People; (Duration) Eight Weeks;</p>
<h1>An example of Generated User Profile</h1>
<h2>Work Event 2:</h2>
<p>(Type) Academic Exchange Conference; (Content) Discuss the development trends of financial technology, share experiences in innovative banking services, and promote communication and cooperation among industry elites.; (Location) Beijing; (Time) Next Saturday at 2 PM; (Title) Financial Technology Elite Forum; (Scale) Around 3000 People; (Duration) Seven days;</p>
<h2>Entertainment Event 1:</h2>
<p>(Type) Art Exhibition; (Content) Displaying selected model works, exchanging making techniques, experiencing creative handicrafts, and feeling the charm of art.; (Location) Beijing; (Time) At 7 PM on the coming Monday; (Title) Model Art Feast; (Scale) Around 900 People; (Duration) Seven Days; (Relationship) Live;</p>
<h2>Entertainment Event 2:</h2>
<p>(Type) Outdoor Hiking; (Content) Conduct outdoor hiking activities, combined with model making, taking natural scenery along the way, creating outdoor landscape models, and sharing modeling techniques.; (Location) Guangdong, Shenzhen; (Time) The Wednesday evening at seven in two weeks; (Title) Outdoor Hiking Model Creation Journey; (Scale) Around 900 People; (Duration) Seven Days; (Relationship) Eight weeks;</p>
<h2>Place:</h2>
<p>(Type) Residential Community; (Name) Oasis Home; (Comment) Oasis Home is really a nice place to live, with a high green coverage rate and a beautiful environment. It's especially great to walk and relax here after work every day. However, the commercial facilities are slightly lacking, and it would be perfect if there were more convenience stores and restaurants.; (Relationship) Use;</p>
<h2>Item:</h2>
<p>(Type) Sports Shoes; (Name) ASICS Gel-Kayano 26; (Comment) These ASICS Gel-Kayano 26 shoes are really great, especially for their stability and support, which is perfect for standing work for long periods. Wearing them, my feet feel much more comfortable. However, it would be perfect if they had better breathability.;</p>
<p>From the case profile in MemDaily, we find that our generated user profiles can greatly align with that in real-world scenarios.</p>
<p>Table 23: Summary of entities and attributes of MemDaily.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity</th>
<th style="text-align: center;">Attribute</th>
<th style="text-align: center;">Entity</th>
<th style="text-align: center;">Attribute</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User (self)</td>
<td style="text-align: center;">Gender <br> Name <br> Age <br> Height <br> Birthday <br> Hometown <br> Workplace <br> Education <br> Occupation <br> Position <br> Company <br> Hobbies <br> Personality <br> Phone <br> Email <br> ID Number <br> Passport Number <br> Bank Card Number <br> Driver's License Number</td>
<td style="text-align: center;">Relative Roles</td>
<td style="text-align: center;">Name <br> Age <br> Height <br> Birthday <br> Hometown <br> Workplace <br> Education <br> Occupation <br> Position <br> Company <br> Hobbies <br> Personality <br> Phone <br> Email</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Work Events</td>
<td style="text-align: center;">Type <br> Content <br> Location <br> Time <br> Title</td>
</tr>
<tr>
<td style="text-align: center;">College Roles</td>
<td style="text-align: center;">Gender <br> Relationship <br> Name <br> Age <br> Height <br> Birthday <br> Hometown <br> Workplace <br> Education <br> Occupation <br> Position <br> Company <br> Hobbies <br> Personality <br> Phone <br> Email</td>
<td style="text-align: center;">Entertainment Events</td>
<td style="text-align: center;">Scale <br> Duration <br> Type <br> Content <br> Location <br> Time <br> Title <br> Scale <br> Duration</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Places</td>
<td style="text-align: center;">Relationship <br> Type <br> Name <br> Comment</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Items</td>
<td style="text-align: center;">Relationship <br> Type <br> Name</td>
</tr>
<tr>
<td style="text-align: center;">Relative Roles</td>
<td style="text-align: center;">Gender <br> Relationship</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Comment</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total (7)</td>
<td style="text-align: center;">Total (73)</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://openai.com/index/hello-gpt-4o/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
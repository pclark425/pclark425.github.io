<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2359 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2359</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2359</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-6296aa7cab06eaf058f7291040b320b5a83c0091</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6296aa7cab06eaf058f7291040b320b5a83c0091" target="_blank">Generative Adversarial Networks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computing Communication and Networking Technologies</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive guide to GANs, covering their architecture, loss functions, training methods, applications, evaluation metrics, challenges, and future directions is provided.</p>
                <p><strong>Paper Abstract:</strong> Generative Adversarial Networks (GANs) are a type of deep learning techniques that have shown remarkable success in generating realistic images, videos, and other types of data. This paper provides a comprehensive guide to GANs, covering their architecture, loss functions, training methods, applications, evaluation metrics, challenges, and future directions. We begin with an introduction to GANs and their historical development, followed by a review of the background and related work. We then provide a detailed overview of the GAN architecture, including the generator and discriminator networks, and discuss the key design choices and variations. Next, we review the loss functions utilized in GANs, including the original minimax objective, as well as more recent approaches s.a. Wasserstein distance and gradient penalty. We then delve into the training of GANs, discussing common techniques s.a. alternating optimization, minibatch discrimination, and spectral normalization. We also provide a survey of the various applications of GANs across domains. In addition, we review the evaluation metrics utilized to assess the diversity and quality of GAN-produced data. Furthermore, we discuss the challenges and open issues in GANs, including mode collapse, training instability, and ethical considerations. Finally, we provide a glimpse into the future directions of GAN research, including improving scalability, developing new architectures, incorporating domain knowledge, and exploring new applications. Overall, this paper serves as a comprehensive guide to GANs, providing both theoretical and practical insights for researchers and practitioners in the field.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2359.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2359.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework with two jointly trained deep networks — a generator that synthesizes samples and a discriminator that distinguishes real from fake — trained by adversarial min-max optimization to learn a data distribution and generate realistic examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative Adversarial Nets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computer vision / generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn the distribution of a dataset (e.g., images) in order to synthesize new realistic samples that match the real data distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often abundant for mainstream vision datasets (e.g., ImageNet, CelebA); labels may be available for conditional variants but basic GANs operate with unlabeled data. The chapter does not quantify dataset sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional unstructured data (images), multimodal distributions (many modes/classes).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: non-convex adversarial optimization, high-dimensional image spaces, multimodal target distributions; training involves large neural networks and large search spaces for parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature within machine learning/computer vision with many established architectures and prior work since 2014.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — for sample generation black-box models are acceptable; interpretability may be desired for controllable generation but not required for many applications.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Generative Adversarial Networks (two-network adversarial deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A generator neural network G maps noise z ~ p_z to samples G(z); a discriminator D outputs probability that an input is real. They are trained with a min-max loss (original: cross-entropy) alternating updates to D and G; many variants modify architectures (CNNs, self-attention), losses (Wasserstein, spectral normalization), and training (progressive growing, truncation).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / generative deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and widely used for image synthesis and related problems (data augmentation, image editing). Limitations include instability, mode collapse, and sensitivity to hyperparameters; modifications are often needed for stable training and high quality.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Highly effective at producing sharp, realistic images compared to other generative models (e.g., VAEs) but original formulation suffers from mode collapse, vanishing gradients, and instability; many later variants substantially improve outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: enabled state-of-the-art image generation, data augmentation, and numerous downstream vision tasks; can reduce labeling costs when used in semi-/self-supervised setups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to VAEs: GANs produce sharper images and offer more flexible generator architectures; however original GAN losses are less stable than some alternative distance-based objectives (e.g., Wasserstein).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Expressive generator/discriminator architectures (e.g., CNNs), careful loss and regularization choices, normalization schemes, and training procedures (batch size, progressive growth) contribute to success; imbalance between D and G and poor loss choices lead to failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Adversarial deep learning is highly effective for high-dimensional generative tasks when paired with architecture and loss modifications that address instability and mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2359.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Convolutional Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN architecture using convolutional/deconvolutional neural networks with specific architectural guidelines (no pooling, batch norm, LeakyReLU in D) that yields higher-resolution, more stable image generation and useful learned features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computer vision — image generation and feature learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate realistic images (e.g., 64x64) and learn visual features usable for downstream supervised tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applies where reasonably large image datasets exist (e.g., SVHN for experiments cited); labels optional for GAN training but used to evaluate learned features.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional images with spatial structure; suitable for convolutional architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: spatially-structured data reduces complexity relative to fully-connected models but still requires deep networks and large parameter spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established within vision; convolutional architectures are standard practice.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low to medium — primary aim is high-fidelity generation and feature learning rather than causal explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional GAN with deconvolutional generator (DCGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Generator uses fractional-strided convolutions (deconvolutions) to upsample a latent vector to an image; discriminator uses strided convolutions (no pooling), batch normalization in both networks, LeakyReLU activations in D and ReLU in G (except Tanh output). Training follows adversarial alternating updates.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / representation learning (generative)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to image generation tasks and representation learning for vision; requires adapting conv architectures to target image resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Empirical result cited: features extracted by DCGAN achieved 22.48% classification error on SVHN when a linear model was trained on them, compared to 28.87% for a purely supervised CNN with the same architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Improved image resolution and stability relative to early fully-connected GANs; learned features are useful for downstream tasks indicating representation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables better unsupervised feature learning and higher-quality image synthesis for vision tasks; practical for tasks with image spatial structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms earlier (non-convolutional) GANs on image tasks; produces sharper images than VAE-based generators. The chapter attributes improved performance to architectural choices rather than mere use of CNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of convolutional/deconvolutional layers, removal of pooling, batch normalization, and appropriate activations (LeakyReLU) stabilized training and improved sample quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Convolutional architectural design choices are critical for stable, high-quality GAN image generation and for extracting useful visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2359.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wasserstein Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN variant that replaces the original adversarial loss with an approximation of the Earth Mover (Wasserstein) distance to provide continuous, informative gradients and mitigate vanishing gradients and mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Wasserstein GAN</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Generative modeling for images (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Stabilize GAN training and provide usable gradients when generated and real distributions are far apart, thereby improving generator learning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Assumes access to sufficient data samples to estimate expectations; specific dataset sizes not provided in chapter.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional image data (same contexts as GANs).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to adversarial optimization; WGAN changes objective to an optimal transport metric which is theoretically more well-behaved but requires enforcing Lipschitz constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established variant in generative modeling literature for improving GAN stability.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — WGAN targets improved training dynamics rather than interpretable mechanistic models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Wasserstein GAN (EM/Wasserstein distance objective with K-Lipschitz-constrained critic)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Replace discriminator objective with maximization over K-Lipschitz functions f_w approximating the Wasserstein distance: maximize E_real[f_w(x)] - E_z[f_w(G(z))], while constraining f_w (critic) to be Lipschitz (originally via weight clipping; later via gradient penalties or spectral norm). Generator minimizes the critic output on generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / generative deep learning (optimal transport-based)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for generative tasks where JS/KL-based losses cause vanishing gradients; needs enforcement of Lipschitz constraint which introduces implementation considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provides smooth, measurable gradients even when distributions do not overlap, leading to more stable generator learning and mitigating vanishing-gradient failure modes of original GAN loss.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for stabilizing GAN training and enabling learning when distributions are initially far apart; foundational for many subsequent loss/regularization methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually against original JS-based GAN losses; WGAN yields non-saturating gradients whereas original loss can vanish when D is near optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Using a metric (Wasserstein) that is continuous w.r.t. generator parameters and maintaining Lipschitz constraints on the critic (via clipping/penalties/normalization) are key to success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Replacing JS/KL objectives with Wasserstein distance produces usable gradients across wide distributional gaps and improves GAN training stability when properly constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2359.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGAN (semi-supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semi-supervised Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension that augments the discriminator to also perform supervised classification (softmax head) in addition to real/fake discrimination, enabling label-efficient training and generation of annotated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Semi-supervised learning for image classification and generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn from mostly unlabeled image data with a small labeled subset to both classify images and generate labeled synthetic data to augment training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed for scenarios with limited labeled data but abundant unlabeled images (semi-supervised setting); specific dataset sizes not given beyond MNIST example.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Images with class labels available for a subset of data (multimodal: multiple discrete classes).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Medium: combining unsupervised generative training with supervised label prediction adds objectives and interactions to optimize.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging/active research area bridging supervised and unsupervised learning; practical techniques available.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — classification interpretability useful but not strictly required; generative component may be used primarily for augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Semi-supervised GAN (discriminator with dual heads: sigmoid for real/fake and softmax for class)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Discriminator has two outputs: binary real/fake sigmoid and a K-way softmax for class labels (softmax used only for samples considered real). Generator architecture remains similar; both networks trained jointly with adversarial and supervised losses.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Semi-supervised learning / generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when labeled examples are scarce but unlabeled examples are abundant; helps generate labeled synthetic examples to enrich training.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported improvement on MNIST: both discriminator and generator performance improve relative to the original GAN when using SGAN, indicating better use of limited labels.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate to high in label-scarce domains: can reduce labeling effort by leveraging unlabeled data and producing annotated synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to unconditional GANs, SGAN leverages labels to better control generation and improves discriminator classification; no quantitative baseline numbers presented beyond MNIST claim.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Joint training of classification and discrimination heads and conditioning generator on class labels help guide mode coverage and improve both generation and classification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Adding a supervised classification objective to the discriminator enables GANs to both generate class-conditional samples and improve performance in low-label regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2359.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GAN variant that conditions both generator and discriminator on auxiliary information (e.g., class labels or textual tags) to control the modes of generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Conditional image generation, multimodal synthesis (e.g., text-to-image, labeled image generation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate images conditioned on labels or other side information so that the modality of generated samples can be controlled (e.g., generate images of a given class or matching a caption).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires paired data linking images to conditioning information (labels, tags or text); such datasets (e.g., Flickr with user tags) are available but labeling quality/coverage may vary.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Images plus structured side information (categorical labels or textual embeddings) — multimodal data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: modeling conditional dependencies across modalities increases architectural and optimization complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Maturing — conditional GANs are widely used for controlled generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — conditional control is prioritized; interpretability of internal mechanisms usually not required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Conditional GAN (label-conditioned generator and discriminator)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Both real images x and latent z are conditioned on y (labels or embeddings); conditioning is realized by concatenation or learned encodings fused into intermediate layers of D and G; loss is the standard adversarial min-max with conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised / conditional generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when conditioned control is needed (class-conditional generation, text-to-image); requires labeled or tagged datasets for conditioning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enables generation of modality-controlled samples and supports multimodal datasets (example: producing images with user tags); chapter reports CGAN can produce automatic tagging when paired with a language model.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for applications needing controlled synthesis (data augmentation for specific classes, text-to-image systems, labelled data generation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to unconditional GANs, CGAN yields better control over generated modes and improves discriminator performance for conditional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of reliable conditioning signals (labels/text) and appropriate fusion architectures for conditioning in both G and D.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Conditioning GANs on labels or embeddings enables controllable, multimodal generation and can leverage weakly labeled multimodal datasets effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2359.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive Growing of GANs (Progressive GAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training methodology that progressively increases network capacity and output resolution during training, starting from low-resolution images and adding layers to grow to very high-resolution outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progressive Growing of GANs for Improved Quality, Stability, and Variation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>High-resolution image generation (faces, natural images)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Train GANs to generate very high-resolution images (e.g., up to 1024x1024) with improved stability by progressively increasing model and image resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires large, high-resolution image datasets (e.g., CelebA-HQ) to learn high-fidelity 1024x1024 images; chapter cites CelebA as used.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional images at increasing resolutions; spatially structured.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: generating 1024x1024 images demands large models, high computational cost, and stable training across many scales.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Representative of advanced GAN training techniques; methodology widely adopted in high-quality image synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — focus is on synthesis quality, though disentanglement and control are secondary goals.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Progressive growing training scheme for GANs</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Start training G and D on very low resolution (e.g., 4x4); incrementally add layers to both networks to handle higher resolutions (8x8, 16x16, ... up to 1024x1024), keeping earlier layers trainable; gradual blending of layers stabilizes learning.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / training methodology for generative models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Particularly applicable for very high-resolution image generation where direct end-to-end training is unstable or computationally prohibitive.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Chapter reports successful training up to 1024x1024 on CelebA; no explicit numeric image quality metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enables credible, high-resolution images and more stable learning compared to non-progressive training.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables practical training of GANs at resolutions required by many real-world image synthesis tasks (photorealistic faces, art, etc.), widening application potential.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More stable and effective for very high-resolution synthesis than naive single-scale training; widely adopted in later architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Gradual capacity increase, keeping previous layers trainable, and controlled blending when adding layers reduce forgetting and stabilize optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Progressively increasing resolution and model capacity during training stabilizes GAN optimization and enables high-resolution image synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2359.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BigGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Big Generative Adversarial Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scale-up strategy for GAN training that uses much larger models, larger batch sizes, and self-attention to achieve state-of-the-art high-fidelity image generation on large datasets like ImageNet.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Scale GAN Training for High Fidelity Natural Image Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Natural image synthesis on large-scale datasets (ImageNet)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Produce high-fidelity class-conditional images across many classes in large-scale datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires very large labeled datasets (ImageNet) and substantial compute/batch sizes for scaling; chapter notes use of ImageNet at multiple resolutions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional labeled image data; many classes (multimodal across categories).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: large number of classes, high-resolution images, and need for large batch training and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Advanced/leading-edge within image synthesis; builds on mature dataset ImageNet and established training techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — primary goal is sample fidelity; some interpretability (attention maps) possible but not required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Scaled-up GAN with self-attention and truncation trick (BigGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Employ self-attention in generator and discriminator to capture long-range dependencies, increase model parameters (×4 in cited work) and batch size (×8) to scale training; use truncation trick at inference sampling to trade off sample variety vs fidelity and orthogonal regularization to make generator amenable to truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised (class-conditional) / large-scale generative deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to large labeled datasets where compute allows very large models and batch sizes; requires substantial computational resources.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Achieved state-of-the-art generation quality on ImageNet at various resolutions; truncation improves individual sample quality at cost of diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for producing state-of-the-art image synthesis on large datasets, enabling downstream uses requiring high-fidelity class-conditional images.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms prior GANs on ImageNet when scaled; demonstrates benefits of self-attention and large-batch, large-model training relative to smaller-scale models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large model capacity, large batch sizes, self-attention modules, and sampling/training tricks (truncation, orthogonal regularization) drive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Scaling model size and batch size with architectural improvements like self-attention yields significant gains in natural image synthesis quality, but requires large compute and careful sampling strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2359.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StyleGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Style-Based Generator Architecture for Generative Adversarial Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generator architecture that injects styles (mapped latent vectors) and noise at multiple layers to disentangle high-level attributes and low-level stochastic variation, producing controllable, high-quality images (notably faces).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Style-Based Generator Architecture for Generative Adversarial Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>High-quality face image synthesis and controllable image generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate photorealistic face images while disentangling interpretable latent factors (pose, identity, texture) and enabling fine-grained control and editing.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires curated high-quality face datasets (e.g., FFHQ) for best results; chapter references FFHQ-trained StyleGAN2 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional structured image data (faces) with complex factors of variation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: disentanglement of multiple latent factors plus high-resolution synthesis increases model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Advanced within face synthesis; StyleGAN and StyleGAN2 represent state-of-the-art for face generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — disentanglement aids interpretability and editing; some interpretability is built into architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Style-based generator with mapping network and adaptive instance normalization (StyleGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Latent code is mapped to an intermediate latent space W via a mapping network; AdaIN-style modulation injects style vectors at each convolutional layer; learned per-layer noise is added for stochastic details; generator starts from a learned constant instead of random input.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / generative deep learning (architecture innovation)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Particularly effective for face generation and applications requiring disentangled, controllable synthesis (image editing, inversion).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Produces state-of-the-art face images with disentangled latent factors enabling controllable editing; widely used for high-fidelity synthesis and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for content creation, face synthesis, and research into representation disentanglement and image editing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over conventional generator designs by enabling better separation of high-level and low-level attributes, producing more controllable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Mapping latent vectors to an intermediate space and per-layer style injection along with learned noise are key to disentanglement and image quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Architectural injection of style at multiple layers yields disentangled, controllable latent representations and significantly improves synthesis quality for faces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2359.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Supervised GAN (rotation auxiliary loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN variant that augments the discriminator with a self-supervised auxiliary task (predicting image rotation) to prevent discriminator forgetting, produce useful representations, and improve stability and sample quality without labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Unsupervised representation learning and unconditional image generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Stabilize GAN training and encourage the discriminator to learn representations independent of generator quality, enabling conditional-quality performance without labels.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires unlabeled images; no labeled data needed. Applicable when labeled data is scarce or unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unlabeled images; auxiliary self-supervised transformations (rotations) applied.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Medium: adds auxiliary prediction objective that interacts with adversarial loss, requiring balancing hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging method to combine self-supervision with adversarial training to mitigate catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — auxiliary task fosters representational learning rather than providing mechanistic explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Self-supervised GAN with rotation prediction auxiliary loss</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Discriminator has two objectives: original real/fake adversarial loss and an auxiliary rotation-classification loss predicting rotation angle (0°, 90°, 180°, 270°) for both real and generated (rotated) samples; generator is trained adversarially plus to aid rotation prediction on generated samples via a weighted auxiliary loss.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / self-supervised + adversarial hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited when labeled data is unavailable but stable high-quality generation or representation learning is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to achieve high-quality images matching conditional GANs' performance without access to labeled data; helps discriminator learn useful features and reduces forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for label-scarce domains because it can approach conditional performance without labels, enabling stronger unconditional generation and representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to conditional GANs: achieves comparable image quality without labels by leveraging self-supervision; compared to standard GANs it stabilizes D via auxiliary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Choice of an informative self-supervised task (rotation), and properly weighting auxiliary losses to ensure the discriminator learns stable representations independent of generator quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Adding a self-supervised auxiliary task to the discriminator prevents forgetting and yields representations that improve generation quality, allowing unlabeled GANs to match label-conditioned performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2359.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SNGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spectral Normalization GAN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stabilization technique that constrains each layer's spectral norm (largest singular value) in the discriminator to bound the Lipschitz constant and improve GAN training stability and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spectral Normalization for Generative Adversarial Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Generative modeling stability for image synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Stabilize discriminator training by enforcing Lipschitz continuity without expensive penalties or clipping to improve generator learning and final sample quality.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Applicable across image datasets; specific data-size constraints not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional image data for which discriminator networks are trained.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Medium-to-high: stability of adversarial optimization is sensitive to discriminator behavior; spectral normalization imposes constraints at the layer level.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established regularization technique in GAN literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — technique is a regularizer to improve optimization rather than to provide interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Spectral normalization of discriminator weights</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Normalize each weight matrix W in discriminator layers by its largest singular value sigma(W): W_SN = W / sigma(W), enforcing spectral norm = 1 per layer and thereby bounding the network Lipschitz constant; inexpensive to compute and compatible with existing architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / regularization for generative models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Broadly applicable as a plug-in stabilization method for many GAN architectures; particularly useful when Lipschitz constraints are desirable.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to achieve significant improvements: extraordinary advance on ImageNet and better or equal quality on CIFAR-10 and STL-10 compared to prior stabilization techniques (weight clipping, gradient penalty, batch/weight/layer normalization), per chapter summary.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High as an inexpensive and effective stabilization method enabling better GAN performance across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively against weight clipping, gradient penalty, and other normalizations; SNGAN provides competitive or superior results with simpler implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Direct control of layer spectral norm to bound Lipschitz constant without complex penalty terms; compatibility with existing architectures and training pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Constraining discriminator layer spectral norms is an efficient and effective way to ensure Lipschitz continuity and stabilize GAN training across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2359.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2359.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data augmentation with GANs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAN-based Data Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using GANs to synthesize additional labeled or unlabeled training samples to alleviate limited annotations, limited diversity, or restricted access to sensitive datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Supervised learning in vision / tasks with scarce labeled data</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Augment training sets with synthetic data to improve downstream supervised model performance when annotations are limited, diversity is lacking, or data access is restricted.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Target scenario: limited labeled data but potentially abundant unlabeled images; synthetic data can be generated to expand datasets. Chapter notes three circumstances motivating augmentation: limited annotations, limited diversity, restricted data access.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Images (labeled or unlabeled) that can be synthesized by GANs; potentially multimodal and diverse if generator covers modes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Varies: augmentation can be simple if generator replicates existing modes, but requires careful generator quality and mode coverage to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Practical technique with growing adoption; many GAN variants (SGAN, CGAN, StyleGAN) support augmentation scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — practitioners often require assurance that synthetic data improves generalization and does not introduce artifacts or bias.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>GAN-based synthetic data generation for augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train a GAN (possibly conditional or semi-supervised) on available images and then generate additional samples (optionally with labels via SGAN/CGAN) to augment supervised training sets; quality depends on generator realism and mode coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised / semi-supervised generative augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when labeled data is scarce or diversity limited; requires a generator capable of producing realistic and diverse samples representative of desired classes.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Presented as a viable alternative to costly manual labeling and active learning; SGAN example suggests improved D and G on MNIST. No general quantitative gains are provided in the chapter, effectiveness depends on generator quality and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Potentially large: can reduce labeling cost, increase dataset diversity, and enable training where data is restricted; risk if synthetic data lacks diversity or introduces biases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Alternative is manual labeling or active learning; GAN augmentation can be cheaper but requires reliable generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Generator realism, mode coverage (avoid mode collapse), and correct labeling (for labeled augmentation) determine augmentation utility; conditional/semi-supervised architectures help produce labeled and diverse samples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>GANs can effectively augment datasets in label-scarce or diversity-limited contexts if the generator produces sufficiently realistic and diverse samples and mode collapse is mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Adversarial Networks', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative Adversarial Nets <em>(Rating: 2)</em></li>
                <li>Wasserstein GAN <em>(Rating: 2)</em></li>
                <li>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks <em>(Rating: 2)</em></li>
                <li>Progressive Growing of GANs for Improved Quality, Stability, and Variation <em>(Rating: 2)</em></li>
                <li>Large Scale GAN Training for High Fidelity Natural Image Synthesis <em>(Rating: 2)</em></li>
                <li>A Style-Based Generator Architecture for Generative Adversarial Networks <em>(Rating: 2)</em></li>
                <li>Spectral Normalization for Generative Adversarial Networks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2359",
    "paper_id": "paper-6296aa7cab06eaf058f7291040b320b5a83c0091",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "GAN",
            "name_full": "Generative Adversarial Network",
            "brief_description": "A framework with two jointly trained deep networks — a generator that synthesizes samples and a discriminator that distinguishes real from fake — trained by adversarial min-max optimization to learn a data distribution and generate realistic examples.",
            "citation_title": "Generative Adversarial Nets",
            "mention_or_use": "use",
            "scientific_problem_domain": "Computer vision / generative modeling",
            "problem_description": "Learn the distribution of a dataset (e.g., images) in order to synthesize new realistic samples that match the real data distribution.",
            "data_availability": "Often abundant for mainstream vision datasets (e.g., ImageNet, CelebA); labels may be available for conditional variants but basic GANs operate with unlabeled data. The chapter does not quantify dataset sizes.",
            "data_structure": "High-dimensional unstructured data (images), multimodal distributions (many modes/classes).",
            "problem_complexity": "High: non-convex adversarial optimization, high-dimensional image spaces, multimodal target distributions; training involves large neural networks and large search spaces for parameters.",
            "domain_maturity": "Mature within machine learning/computer vision with many established architectures and prior work since 2014.",
            "mechanistic_understanding_requirements": "Low-to-medium — for sample generation black-box models are acceptable; interpretability may be desired for controllable generation but not required for many applications.",
            "ai_methodology_name": "Generative Adversarial Networks (two-network adversarial deep learning)",
            "ai_methodology_description": "A generator neural network G maps noise z ~ p_z to samples G(z); a discriminator D outputs probability that an input is real. They are trained with a min-max loss (original: cross-entropy) alternating updates to D and G; many variants modify architectures (CNNs, self-attention), losses (Wasserstein, spectral normalization), and training (progressive growing, truncation).",
            "ai_methodology_category": "Unsupervised / generative deep learning",
            "applicability": "Applicable and widely used for image synthesis and related problems (data augmentation, image editing). Limitations include instability, mode collapse, and sensitivity to hyperparameters; modifications are often needed for stable training and high quality.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Highly effective at producing sharp, realistic images compared to other generative models (e.g., VAEs) but original formulation suffers from mode collapse, vanishing gradients, and instability; many later variants substantially improve outcomes.",
            "impact_potential": "High: enabled state-of-the-art image generation, data augmentation, and numerous downstream vision tasks; can reduce labeling costs when used in semi-/self-supervised setups.",
            "comparison_to_alternatives": "Compared qualitatively to VAEs: GANs produce sharper images and offer more flexible generator architectures; however original GAN losses are less stable than some alternative distance-based objectives (e.g., Wasserstein).",
            "success_factors": "Expressive generator/discriminator architectures (e.g., CNNs), careful loss and regularization choices, normalization schemes, and training procedures (batch size, progressive growth) contribute to success; imbalance between D and G and poor loss choices lead to failure modes.",
            "key_insight": "Adversarial deep learning is highly effective for high-dimensional generative tasks when paired with architecture and loss modifications that address instability and mode collapse.",
            "uuid": "e2359.0",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "DCGAN",
            "name_full": "Deep Convolutional Generative Adversarial Network",
            "brief_description": "A GAN architecture using convolutional/deconvolutional neural networks with specific architectural guidelines (no pooling, batch norm, LeakyReLU in D) that yields higher-resolution, more stable image generation and useful learned features.",
            "citation_title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Computer vision — image generation and feature learning",
            "problem_description": "Generate realistic images (e.g., 64x64) and learn visual features usable for downstream supervised tasks.",
            "data_availability": "Applies where reasonably large image datasets exist (e.g., SVHN for experiments cited); labels optional for GAN training but used to evaluate learned features.",
            "data_structure": "High-dimensional images with spatial structure; suitable for convolutional architectures.",
            "problem_complexity": "Moderate-to-high: spatially-structured data reduces complexity relative to fully-connected models but still requires deep networks and large parameter spaces.",
            "domain_maturity": "Well-established within vision; convolutional architectures are standard practice.",
            "mechanistic_understanding_requirements": "Low to medium — primary aim is high-fidelity generation and feature learning rather than causal explanation.",
            "ai_methodology_name": "Convolutional GAN with deconvolutional generator (DCGAN)",
            "ai_methodology_description": "Generator uses fractional-strided convolutions (deconvolutions) to upsample a latent vector to an image; discriminator uses strided convolutions (no pooling), batch normalization in both networks, LeakyReLU activations in D and ReLU in G (except Tanh output). Training follows adversarial alternating updates.",
            "ai_methodology_category": "Unsupervised / representation learning (generative)",
            "applicability": "Highly applicable to image generation tasks and representation learning for vision; requires adapting conv architectures to target image resolution.",
            "effectiveness_quantitative": "Empirical result cited: features extracted by DCGAN achieved 22.48% classification error on SVHN when a linear model was trained on them, compared to 28.87% for a purely supervised CNN with the same architecture.",
            "effectiveness_qualitative": "Improved image resolution and stability relative to early fully-connected GANs; learned features are useful for downstream tasks indicating representation quality.",
            "impact_potential": "Enables better unsupervised feature learning and higher-quality image synthesis for vision tasks; practical for tasks with image spatial structure.",
            "comparison_to_alternatives": "Outperforms earlier (non-convolutional) GANs on image tasks; produces sharper images than VAE-based generators. The chapter attributes improved performance to architectural choices rather than mere use of CNNs.",
            "success_factors": "Use of convolutional/deconvolutional layers, removal of pooling, batch normalization, and appropriate activations (LeakyReLU) stabilized training and improved sample quality.",
            "key_insight": "Convolutional architectural design choices are critical for stable, high-quality GAN image generation and for extracting useful visual features.",
            "uuid": "e2359.1",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "WGAN",
            "name_full": "Wasserstein Generative Adversarial Network",
            "brief_description": "A GAN variant that replaces the original adversarial loss with an approximation of the Earth Mover (Wasserstein) distance to provide continuous, informative gradients and mitigate vanishing gradients and mode collapse.",
            "citation_title": "Wasserstein GAN",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Generative modeling for images (computer vision)",
            "problem_description": "Stabilize GAN training and provide usable gradients when generated and real distributions are far apart, thereby improving generator learning.",
            "data_availability": "Assumes access to sufficient data samples to estimate expectations; specific dataset sizes not provided in chapter.",
            "data_structure": "High-dimensional image data (same contexts as GANs).",
            "problem_complexity": "High due to adversarial optimization; WGAN changes objective to an optimal transport metric which is theoretically more well-behaved but requires enforcing Lipschitz constraints.",
            "domain_maturity": "Established variant in generative modeling literature for improving GAN stability.",
            "mechanistic_understanding_requirements": "Low-to-medium — WGAN targets improved training dynamics rather than interpretable mechanistic models.",
            "ai_methodology_name": "Wasserstein GAN (EM/Wasserstein distance objective with K-Lipschitz-constrained critic)",
            "ai_methodology_description": "Replace discriminator objective with maximization over K-Lipschitz functions f_w approximating the Wasserstein distance: maximize E_real[f_w(x)] - E_z[f_w(G(z))], while constraining f_w (critic) to be Lipschitz (originally via weight clipping; later via gradient penalties or spectral norm). Generator minimizes the critic output on generated samples.",
            "ai_methodology_category": "Unsupervised / generative deep learning (optimal transport-based)",
            "applicability": "Appropriate for generative tasks where JS/KL-based losses cause vanishing gradients; needs enforcement of Lipschitz constraint which introduces implementation considerations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Provides smooth, measurable gradients even when distributions do not overlap, leading to more stable generator learning and mitigating vanishing-gradient failure modes of original GAN loss.",
            "impact_potential": "High for stabilizing GAN training and enabling learning when distributions are initially far apart; foundational for many subsequent loss/regularization methods.",
            "comparison_to_alternatives": "Compared conceptually against original JS-based GAN losses; WGAN yields non-saturating gradients whereas original loss can vanish when D is near optimal.",
            "success_factors": "Using a metric (Wasserstein) that is continuous w.r.t. generator parameters and maintaining Lipschitz constraints on the critic (via clipping/penalties/normalization) are key to success.",
            "key_insight": "Replacing JS/KL objectives with Wasserstein distance produces usable gradients across wide distributional gaps and improves GAN training stability when properly constrained.",
            "uuid": "e2359.2",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "SGAN (semi-supervised)",
            "name_full": "Semi-supervised Generative Adversarial Network",
            "brief_description": "An extension that augments the discriminator to also perform supervised classification (softmax head) in addition to real/fake discrimination, enabling label-efficient training and generation of annotated samples.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Semi-supervised learning for image classification and generation",
            "problem_description": "Learn from mostly unlabeled image data with a small labeled subset to both classify images and generate labeled synthetic data to augment training.",
            "data_availability": "Designed for scenarios with limited labeled data but abundant unlabeled images (semi-supervised setting); specific dataset sizes not given beyond MNIST example.",
            "data_structure": "Images with class labels available for a subset of data (multimodal: multiple discrete classes).",
            "problem_complexity": "Medium: combining unsupervised generative training with supervised label prediction adds objectives and interactions to optimize.",
            "domain_maturity": "Emerging/active research area bridging supervised and unsupervised learning; practical techniques available.",
            "mechanistic_understanding_requirements": "Medium — classification interpretability useful but not strictly required; generative component may be used primarily for augmentation.",
            "ai_methodology_name": "Semi-supervised GAN (discriminator with dual heads: sigmoid for real/fake and softmax for class)",
            "ai_methodology_description": "Discriminator has two outputs: binary real/fake sigmoid and a K-way softmax for class labels (softmax used only for samples considered real). Generator architecture remains similar; both networks trained jointly with adversarial and supervised losses.",
            "ai_methodology_category": "Semi-supervised learning / generative modeling",
            "applicability": "Appropriate when labeled examples are scarce but unlabeled examples are abundant; helps generate labeled synthetic examples to enrich training.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported improvement on MNIST: both discriminator and generator performance improve relative to the original GAN when using SGAN, indicating better use of limited labels.",
            "impact_potential": "Moderate to high in label-scarce domains: can reduce labeling effort by leveraging unlabeled data and producing annotated synthetic data.",
            "comparison_to_alternatives": "Compared to unconditional GANs, SGAN leverages labels to better control generation and improves discriminator classification; no quantitative baseline numbers presented beyond MNIST claim.",
            "success_factors": "Joint training of classification and discrimination heads and conditioning generator on class labels help guide mode coverage and improve both generation and classification.",
            "key_insight": "Adding a supervised classification objective to the discriminator enables GANs to both generate class-conditional samples and improve performance in low-label regimes.",
            "uuid": "e2359.3",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "CGAN",
            "name_full": "Conditional Generative Adversarial Network",
            "brief_description": "GAN variant that conditions both generator and discriminator on auxiliary information (e.g., class labels or textual tags) to control the modes of generated samples.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Conditional image generation, multimodal synthesis (e.g., text-to-image, labeled image generation)",
            "problem_description": "Generate images conditioned on labels or other side information so that the modality of generated samples can be controlled (e.g., generate images of a given class or matching a caption).",
            "data_availability": "Requires paired data linking images to conditioning information (labels, tags or text); such datasets (e.g., Flickr with user tags) are available but labeling quality/coverage may vary.",
            "data_structure": "Images plus structured side information (categorical labels or textual embeddings) — multimodal data.",
            "problem_complexity": "High: modeling conditional dependencies across modalities increases architectural and optimization complexity.",
            "domain_maturity": "Maturing — conditional GANs are widely used for controlled generation tasks.",
            "mechanistic_understanding_requirements": "Low-to-medium — conditional control is prioritized; interpretability of internal mechanisms usually not required.",
            "ai_methodology_name": "Conditional GAN (label-conditioned generator and discriminator)",
            "ai_methodology_description": "Both real images x and latent z are conditioned on y (labels or embeddings); conditioning is realized by concatenation or learned encodings fused into intermediate layers of D and G; loss is the standard adversarial min-max with conditioning.",
            "ai_methodology_category": "Supervised / conditional generative modeling",
            "applicability": "Appropriate when conditioned control is needed (class-conditional generation, text-to-image); requires labeled or tagged datasets for conditioning signals.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Enables generation of modality-controlled samples and supports multimodal datasets (example: producing images with user tags); chapter reports CGAN can produce automatic tagging when paired with a language model.",
            "impact_potential": "High for applications needing controlled synthesis (data augmentation for specific classes, text-to-image systems, labelled data generation).",
            "comparison_to_alternatives": "Compared to unconditional GANs, CGAN yields better control over generated modes and improves discriminator performance for conditional tasks.",
            "success_factors": "Availability of reliable conditioning signals (labels/text) and appropriate fusion architectures for conditioning in both G and D.",
            "key_insight": "Conditioning GANs on labels or embeddings enables controllable, multimodal generation and can leverage weakly labeled multimodal datasets effectively.",
            "uuid": "e2359.4",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "PROGAN",
            "name_full": "Progressive Growing of GANs (Progressive GAN)",
            "brief_description": "A training methodology that progressively increases network capacity and output resolution during training, starting from low-resolution images and adding layers to grow to very high-resolution outputs.",
            "citation_title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
            "mention_or_use": "mention",
            "scientific_problem_domain": "High-resolution image generation (faces, natural images)",
            "problem_description": "Train GANs to generate very high-resolution images (e.g., up to 1024x1024) with improved stability by progressively increasing model and image resolution.",
            "data_availability": "Requires large, high-resolution image datasets (e.g., CelebA-HQ) to learn high-fidelity 1024x1024 images; chapter cites CelebA as used.",
            "data_structure": "High-dimensional images at increasing resolutions; spatially structured.",
            "problem_complexity": "Very high: generating 1024x1024 images demands large models, high computational cost, and stable training across many scales.",
            "domain_maturity": "Representative of advanced GAN training techniques; methodology widely adopted in high-quality image synthesis.",
            "mechanistic_understanding_requirements": "Low-to-medium — focus is on synthesis quality, though disentanglement and control are secondary goals.",
            "ai_methodology_name": "Progressive growing training scheme for GANs",
            "ai_methodology_description": "Start training G and D on very low resolution (e.g., 4x4); incrementally add layers to both networks to handle higher resolutions (8x8, 16x16, ... up to 1024x1024), keeping earlier layers trainable; gradual blending of layers stabilizes learning.",
            "ai_methodology_category": "Unsupervised / training methodology for generative models",
            "applicability": "Particularly applicable for very high-resolution image generation where direct end-to-end training is unstable or computationally prohibitive.",
            "effectiveness_quantitative": "Chapter reports successful training up to 1024x1024 on CelebA; no explicit numeric image quality metric provided.",
            "effectiveness_qualitative": "Enables credible, high-resolution images and more stable learning compared to non-progressive training.",
            "impact_potential": "Enables practical training of GANs at resolutions required by many real-world image synthesis tasks (photorealistic faces, art, etc.), widening application potential.",
            "comparison_to_alternatives": "More stable and effective for very high-resolution synthesis than naive single-scale training; widely adopted in later architectures.",
            "success_factors": "Gradual capacity increase, keeping previous layers trainable, and controlled blending when adding layers reduce forgetting and stabilize optimization.",
            "key_insight": "Progressively increasing resolution and model capacity during training stabilizes GAN optimization and enables high-resolution image synthesis.",
            "uuid": "e2359.5",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "BigGAN",
            "name_full": "Big Generative Adversarial Network",
            "brief_description": "A scale-up strategy for GAN training that uses much larger models, larger batch sizes, and self-attention to achieve state-of-the-art high-fidelity image generation on large datasets like ImageNet.",
            "citation_title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Natural image synthesis on large-scale datasets (ImageNet)",
            "problem_description": "Produce high-fidelity class-conditional images across many classes in large-scale datasets.",
            "data_availability": "Requires very large labeled datasets (ImageNet) and substantial compute/batch sizes for scaling; chapter notes use of ImageNet at multiple resolutions.",
            "data_structure": "High-dimensional labeled image data; many classes (multimodal across categories).",
            "problem_complexity": "Very high: large number of classes, high-resolution images, and need for large batch training and model capacity.",
            "domain_maturity": "Advanced/leading-edge within image synthesis; builds on mature dataset ImageNet and established training techniques.",
            "mechanistic_understanding_requirements": "Low-to-medium — primary goal is sample fidelity; some interpretability (attention maps) possible but not required.",
            "ai_methodology_name": "Scaled-up GAN with self-attention and truncation trick (BigGAN)",
            "ai_methodology_description": "Employ self-attention in generator and discriminator to capture long-range dependencies, increase model parameters (×4 in cited work) and batch size (×8) to scale training; use truncation trick at inference sampling to trade off sample variety vs fidelity and orthogonal regularization to make generator amenable to truncation.",
            "ai_methodology_category": "Supervised (class-conditional) / large-scale generative deep learning",
            "applicability": "Well-suited to large labeled datasets where compute allows very large models and batch sizes; requires substantial computational resources.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Achieved state-of-the-art generation quality on ImageNet at various resolutions; truncation improves individual sample quality at cost of diversity.",
            "impact_potential": "High for producing state-of-the-art image synthesis on large datasets, enabling downstream uses requiring high-fidelity class-conditional images.",
            "comparison_to_alternatives": "Outperforms prior GANs on ImageNet when scaled; demonstrates benefits of self-attention and large-batch, large-model training relative to smaller-scale models.",
            "success_factors": "Large model capacity, large batch sizes, self-attention modules, and sampling/training tricks (truncation, orthogonal regularization) drive performance.",
            "key_insight": "Scaling model size and batch size with architectural improvements like self-attention yields significant gains in natural image synthesis quality, but requires large compute and careful sampling strategies.",
            "uuid": "e2359.6",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "StyleGAN",
            "name_full": "Style-Based Generator Architecture for Generative Adversarial Networks",
            "brief_description": "A generator architecture that injects styles (mapped latent vectors) and noise at multiple layers to disentangle high-level attributes and low-level stochastic variation, producing controllable, high-quality images (notably faces).",
            "citation_title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
            "mention_or_use": "mention",
            "scientific_problem_domain": "High-quality face image synthesis and controllable image generation",
            "problem_description": "Generate photorealistic face images while disentangling interpretable latent factors (pose, identity, texture) and enabling fine-grained control and editing.",
            "data_availability": "Requires curated high-quality face datasets (e.g., FFHQ) for best results; chapter references FFHQ-trained StyleGAN2 examples.",
            "data_structure": "High-dimensional structured image data (faces) with complex factors of variation.",
            "problem_complexity": "High: disentanglement of multiple latent factors plus high-resolution synthesis increases model complexity.",
            "domain_maturity": "Advanced within face synthesis; StyleGAN and StyleGAN2 represent state-of-the-art for face generation.",
            "mechanistic_understanding_requirements": "Medium — disentanglement aids interpretability and editing; some interpretability is built into architecture.",
            "ai_methodology_name": "Style-based generator with mapping network and adaptive instance normalization (StyleGAN)",
            "ai_methodology_description": "Latent code is mapped to an intermediate latent space W via a mapping network; AdaIN-style modulation injects style vectors at each convolutional layer; learned per-layer noise is added for stochastic details; generator starts from a learned constant instead of random input.",
            "ai_methodology_category": "Unsupervised / generative deep learning (architecture innovation)",
            "applicability": "Particularly effective for face generation and applications requiring disentangled, controllable synthesis (image editing, inversion).",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Produces state-of-the-art face images with disentangled latent factors enabling controllable editing; widely used for high-fidelity synthesis and manipulation.",
            "impact_potential": "High for content creation, face synthesis, and research into representation disentanglement and image editing.",
            "comparison_to_alternatives": "Improves over conventional generator designs by enabling better separation of high-level and low-level attributes, producing more controllable outputs.",
            "success_factors": "Mapping latent vectors to an intermediate space and per-layer style injection along with learned noise are key to disentanglement and image quality.",
            "key_insight": "Architectural injection of style at multiple layers yields disentangled, controllable latent representations and significantly improves synthesis quality for faces.",
            "uuid": "e2359.7",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "SSGAN",
            "name_full": "Self-Supervised GAN (rotation auxiliary loss)",
            "brief_description": "A GAN variant that augments the discriminator with a self-supervised auxiliary task (predicting image rotation) to prevent discriminator forgetting, produce useful representations, and improve stability and sample quality without labels.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Unsupervised representation learning and unconditional image generation",
            "problem_description": "Stabilize GAN training and encourage the discriminator to learn representations independent of generator quality, enabling conditional-quality performance without labels.",
            "data_availability": "Requires unlabeled images; no labeled data needed. Applicable when labeled data is scarce or unavailable.",
            "data_structure": "Unlabeled images; auxiliary self-supervised transformations (rotations) applied.",
            "problem_complexity": "Medium: adds auxiliary prediction objective that interacts with adversarial loss, requiring balancing hyperparameters.",
            "domain_maturity": "Emerging method to combine self-supervision with adversarial training to mitigate catastrophic forgetting.",
            "mechanistic_understanding_requirements": "Low-to-medium — auxiliary task fosters representational learning rather than providing mechanistic explanations.",
            "ai_methodology_name": "Self-supervised GAN with rotation prediction auxiliary loss",
            "ai_methodology_description": "Discriminator has two objectives: original real/fake adversarial loss and an auxiliary rotation-classification loss predicting rotation angle (0°, 90°, 180°, 270°) for both real and generated (rotated) samples; generator is trained adversarially plus to aid rotation prediction on generated samples via a weighted auxiliary loss.",
            "ai_methodology_category": "Unsupervised / self-supervised + adversarial hybrid",
            "applicability": "Well-suited when labeled data is unavailable but stable high-quality generation or representation learning is desired.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to achieve high-quality images matching conditional GANs' performance without access to labeled data; helps discriminator learn useful features and reduces forgetting.",
            "impact_potential": "High for label-scarce domains because it can approach conditional performance without labels, enabling stronger unconditional generation and representation learning.",
            "comparison_to_alternatives": "Compared conceptually to conditional GANs: achieves comparable image quality without labels by leveraging self-supervision; compared to standard GANs it stabilizes D via auxiliary tasks.",
            "success_factors": "Choice of an informative self-supervised task (rotation), and properly weighting auxiliary losses to ensure the discriminator learns stable representations independent of generator quality.",
            "key_insight": "Adding a self-supervised auxiliary task to the discriminator prevents forgetting and yields representations that improve generation quality, allowing unlabeled GANs to match label-conditioned performance.",
            "uuid": "e2359.8",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "SNGAN",
            "name_full": "Spectral Normalization GAN",
            "brief_description": "A stabilization technique that constrains each layer's spectral norm (largest singular value) in the discriminator to bound the Lipschitz constant and improve GAN training stability and performance.",
            "citation_title": "Spectral Normalization for Generative Adversarial Networks",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Generative modeling stability for image synthesis",
            "problem_description": "Stabilize discriminator training by enforcing Lipschitz continuity without expensive penalties or clipping to improve generator learning and final sample quality.",
            "data_availability": "Applicable across image datasets; specific data-size constraints not specified.",
            "data_structure": "High-dimensional image data for which discriminator networks are trained.",
            "problem_complexity": "Medium-to-high: stability of adversarial optimization is sensitive to discriminator behavior; spectral normalization imposes constraints at the layer level.",
            "domain_maturity": "Established regularization technique in GAN literature.",
            "mechanistic_understanding_requirements": "Low — technique is a regularizer to improve optimization rather than to provide interpretation.",
            "ai_methodology_name": "Spectral normalization of discriminator weights",
            "ai_methodology_description": "Normalize each weight matrix W in discriminator layers by its largest singular value sigma(W): W_SN = W / sigma(W), enforcing spectral norm = 1 per layer and thereby bounding the network Lipschitz constant; inexpensive to compute and compatible with existing architectures.",
            "ai_methodology_category": "Unsupervised / regularization for generative models",
            "applicability": "Broadly applicable as a plug-in stabilization method for many GAN architectures; particularly useful when Lipschitz constraints are desirable.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to achieve significant improvements: extraordinary advance on ImageNet and better or equal quality on CIFAR-10 and STL-10 compared to prior stabilization techniques (weight clipping, gradient penalty, batch/weight/layer normalization), per chapter summary.",
            "impact_potential": "High as an inexpensive and effective stabilization method enabling better GAN performance across datasets.",
            "comparison_to_alternatives": "Compared qualitatively against weight clipping, gradient penalty, and other normalizations; SNGAN provides competitive or superior results with simpler implementation.",
            "success_factors": "Direct control of layer spectral norm to bound Lipschitz constant without complex penalty terms; compatibility with existing architectures and training pipelines.",
            "key_insight": "Constraining discriminator layer spectral norms is an efficient and effective way to ensure Lipschitz continuity and stabilize GAN training across datasets.",
            "uuid": "e2359.9",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Data augmentation with GANs",
            "name_full": "GAN-based Data Augmentation",
            "brief_description": "Using GANs to synthesize additional labeled or unlabeled training samples to alleviate limited annotations, limited diversity, or restricted access to sensitive datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Supervised learning in vision / tasks with scarce labeled data",
            "problem_description": "Augment training sets with synthetic data to improve downstream supervised model performance when annotations are limited, diversity is lacking, or data access is restricted.",
            "data_availability": "Target scenario: limited labeled data but potentially abundant unlabeled images; synthetic data can be generated to expand datasets. Chapter notes three circumstances motivating augmentation: limited annotations, limited diversity, restricted data access.",
            "data_structure": "Images (labeled or unlabeled) that can be synthesized by GANs; potentially multimodal and diverse if generator covers modes.",
            "problem_complexity": "Varies: augmentation can be simple if generator replicates existing modes, but requires careful generator quality and mode coverage to be effective.",
            "domain_maturity": "Practical technique with growing adoption; many GAN variants (SGAN, CGAN, StyleGAN) support augmentation scenarios.",
            "mechanistic_understanding_requirements": "Medium — practitioners often require assurance that synthetic data improves generalization and does not introduce artifacts or bias.",
            "ai_methodology_name": "GAN-based synthetic data generation for augmentation",
            "ai_methodology_description": "Train a GAN (possibly conditional or semi-supervised) on available images and then generate additional samples (optionally with labels via SGAN/CGAN) to augment supervised training sets; quality depends on generator realism and mode coverage.",
            "ai_methodology_category": "Unsupervised / semi-supervised generative augmentation",
            "applicability": "Applicable when labeled data is scarce or diversity limited; requires a generator capable of producing realistic and diverse samples representative of desired classes.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Presented as a viable alternative to costly manual labeling and active learning; SGAN example suggests improved D and G on MNIST. No general quantitative gains are provided in the chapter, effectiveness depends on generator quality and coverage.",
            "impact_potential": "Potentially large: can reduce labeling cost, increase dataset diversity, and enable training where data is restricted; risk if synthetic data lacks diversity or introduces biases.",
            "comparison_to_alternatives": "Alternative is manual labeling or active learning; GAN augmentation can be cheaper but requires reliable generative models.",
            "success_factors": "Generator realism, mode coverage (avoid mode collapse), and correct labeling (for labeled augmentation) determine augmentation utility; conditional/semi-supervised architectures help produce labeled and diverse samples.",
            "key_insight": "GANs can effectively augment datasets in label-scarce or diversity-limited contexts if the generator produces sufficiently realistic and diverse samples and mode collapse is mitigated.",
            "uuid": "e2359.10",
            "source_info": {
                "paper_title": "Generative Adversarial Networks",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative Adversarial Nets",
            "rating": 2
        },
        {
            "paper_title": "Wasserstein GAN",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "rating": 2
        },
        {
            "paper_title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
            "rating": 2
        },
        {
            "paper_title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
            "rating": 2
        },
        {
            "paper_title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
            "rating": 2
        },
        {
            "paper_title": "Spectral Normalization for Generative Adversarial Networks",
            "rating": 2
        }
    ],
    "cost": 0.02067625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generative Adversarial Networks</h1>
<p>Gilad Cohen and Raja Giryes</p>
<h4>Abstract</h4>
<p>Generative Adversarial Networks (GANs) are very popular frameworks for generating high-quality data, and are immensely used in both the academia and industry in many domains. Arguably, their most substantial impact has been in the area of computer vision, where they achieve state-of-the-art image generation. This chapter gives an introduction to GANs, by discussing their principle mechanism and presenting some of their inherent problems during training and evaluation. We focus on these three issues: (1) mode collapse, (2) vanishing gradients, and (3) generation of low-quality images. We then list some architecture-variant and loss-variant GANs that remedy the above challenges. Lastly, we present two utilization examples of GANs for real-world applications: Data augmentation and face images generation.</p>
<h2>1 Introduction to GANs</h2>
<p>Generative adversarial networks (GANs) are currently the leading method to learn a distribution of a given dataset and generate new examples from it. To begin to understand the concept of GANs, let us consider an interplay between the police and money counterfeiters. A state just launched its new currency and millions of bills and coins are widespread all over the country. Recently, the police detected a flood of counterfeit money in circulation. Further inspection reveals that the forged bills are lighter than the genuine bills, and can be easily filtered out. The criminals then find out about the police's discovery and use new printers which control better the cash weight. In turn, the police investigate and discover that the new counterfeit bills have a different texture near the corners, and remove them from the system. After many iterations of generating and discriminating forged money, the counterfeit</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>money becomes almost indistinguishable from the real money. Note that the system has two agents: 1) The counterfeiters who create "close to real" money and 2) The police who detect counterfeit bills adequately.</p>
<p>Back to deep learning: The above two agents are called "generator" and "discriminator", respectively. These two entities are trained jointly, where the generator learns how to fool the discriminator with new adversarial examples out of the dataset distribution; and the discriminator learns to distinguish between real and fake data samples. The GAN architecture is used in more and more applications since its introduction in 2014. It was proved successful in many domains such as computer vision [14, 29, 25, 38], semantic segmentation [39, 27, 70, 24], time-series synthesis [9, 23], image editing [61, 36, 19, 3, 75], natural language processing [15, 28, 22], text-to-image generation [59, 58, 54], and many more. In the next section, we depict the basic GAN architecture and loss. Later, we will present more sophisticated architectures, losses, and common usages.</p>
<h1>2 The basic GAN concept</h1>
<p>The first GAN that was introduced by Goodfellow et al. [20] is depicted in Fig. 1.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 Basic GAN structure. The discriminator and generator are two deep neural networks trained jointly. The discriminator is trained for the task of classifying whether an input image is natural (real) or generated (fake), while the generator is optimized to fool the discriminator.</p>
<p>The architecture of GANs is comprised from two individual components: A discriminator $(D)$ and a generator $(G) . D$ is trained to distinguish between real images from the natural distribution and generated images, while $G$ is trained to craft fake images which fool the discriminator. A random distribution, $\mathbf{z} \sim p_{\mathbf{z}}$, is given as input to G. The purpose of GANs is to learn the generated samples'</p>
<p>distribution, $G(\mathbf{z}) \sim p_{g}$ that estimates the real world distribution $p_{r}$. GANs are optimized by solving the following min-max optimization problem:</p>
<p>$$
\min <em D="D">{G} \max </em>} \mathbb{E<em r="r">{\mathbf{x} \sim p</em>}} \log [D(\mathbf{x})]+\mathbb{E<em _mathbf_z="\mathbf{z">{\mathbf{z} \sim p</em>))]
$$}}} \log [1-D(G(\mathbf{z</p>
<p>On the one hand, $D$ aims at predicting $D(\mathbf{x})=1$ for real data samples and $D(G(\mathbf{z}))=0$ for fake samples. On the other hand, the GAN learns how to fool $D$ by finding $G$ which is optimized on hampering the second term in Eq. (1).</p>
<p>On the first iteration, only the discriminator weights $\theta_{D}$ are updated. We sample a minibatch of $m$ noise samples $\left{\mathbf{z}^{(1)}, \ldots, \mathbf{z}^{(m)}\right}$ from $p_{\mathbf{z}}$ and a minibatch of $m$ real data examples $\left{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(m)}\right}$ from $p_{r}$. We then calculate the discriminator's gradients</p>
<p>$$
\nabla_{\theta_{D}} \frac{1}{m} \sum_{i=1}^{m}\left[\log D\left(\mathbf{x}^{(i)}\right)+\log \left(1-D\left(G\left(\mathbf{z}^{(i)}\right)\right)\right)\right]
$$</p>
<p>and update the discriminator weights, $\theta_{D}$, by ascending this term. On the second iteration, only the generator's weights $\theta_{G}$ are updated. We sample a minibatch of $m$ noise samples $\left{\mathbf{z}^{(1)}, \ldots, \mathbf{z}^{(m)}\right}$ from $p_{\mathbf{z}}$, and calculate the generator's gradients</p>
<p>$$
\nabla_{\theta_{G}} \frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(G\left(\mathbf{z}^{(i)}\right)\right)\right)
$$</p>
<p>and update the generator weights, $\theta_{G}$, by descending this term. Goodfellow et al. [20] showed that under certain conditions on $D, G$, and the training procedure, the distribution $p_{\mathbf{z}}$ converges to $p_{r}$.</p>
<h1>3 GAN Advantages and Problems</h1>
<p>Since their first introduction in 2014, GANs have attracted a growing interest all over the academia and industry, thanks to many advantages over other generative models (mainly Variational Auto-encoders (VAEs) [34]):</p>
<ul>
<li>Sharp images: GANs produce sharper images than other generative models. The images at the output of the Generator look more natural and with better quality than images generated using VAEs, which tend to be blurrier.</li>
<li>Configurable size: The latent random variable size is not restricted, enriching the generator search space.</li>
<li>Versatile generator: The GAN framework can support many different generator networks, unlike other generative models that may have architectural constraints. VAEs, for example, enforce using a Gaussian at the generator's first layer.</li>
</ul>
<p>The above advantages make GANs very attractive in the deep learning community, achieving state-of-the-art results in a variety of domains, and generating very natural</p>
<p>images. Yet, the original GAN suffers from three major problems that are described in detail below. We first present a short summary of them:</p>
<ul>
<li>Mode collapse: During the synchronized training of the generator and discriminator, the generator tends to learn to produce a specific pattern (mode) which fools the discriminator. Although this pattern minimizes Eq. (1), the generator does not cover the full distribution of the dataset.</li>
<li>Vanishing gradients: Very frequently the discriminator is trained "too well" to distinguish real images from adversarial images; in this scenario, the training step of the generator back propagates very low gradients, which does not help the generator to learn.</li>
<li>Instability: The model ( $\theta_{D}$ or $\theta_{G}$ ) parameters fluctuate, and generally not stable during the training. The generator seldom achieves a point where it outputs very high-quality images.</li>
</ul>
<h1>3.1 Mode collapse</h1>
<p>Data distributions are multi-modal, meaning that every sample is classified (usually) to only one label. For example, in MNIST there are ten classes of digits (or modes) labeled from ' 0 ' to ' 9 '. Mode collapse is the phenomenon where the generator only yields a small subset of the possible modes. In Fig. 2 you can observe generated MNIST images by two different GANs. The top row shows the training of a "good" GAN, not suffering from mode collapse. It generates every kind of mode (digit type) throughout the training. The bottom row exhibits a GAN training with mode collapse, generating just the digit ' 6 '.</p>
<h3>3.2 Vanishing gradients</h3>
<p>GANs often suffer from training instability, where $D$ performs very well and $G$ does not get a chance to train a good distribution. We turn to provide some mathematical observations and understanding of why training a generator $G$ is extremely hard when the discriminator $D$ is close to optimal.</p>
<p>The global optimality, stated in [20], is defined when $D$ is optimized for any given $G$. The optimal $D$ is achieved when its derivative for Eq. (1) equals 0 :</p>
<p>$$
\begin{gathered}
\frac{p_{r}(\mathbf{x})}{D(\mathbf{x})}-\frac{p_{g}(\mathbf{x})}{1-D(\mathbf{x})}=0 \
D^{*}(\mathbf{x})=\frac{p_{r}(\mathbf{x})}{p_{r}(\mathbf{x})+p_{g}(\mathbf{x})}
\end{gathered}
$$</p>
<p>where $\mathbf{x}$ is the real input data, $D^{*}(\mathbf{x})$ is the optimal discriminator, and $p_{r}(\mathbf{x}) / p_{g}(\mathbf{x})$ is the distribution of the real/generated data, respectively, over the real data $\mathbf{x}$. If we</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2 Example of the mode collapse problem in GANs. The top row shows a training without mode collapse, where all MNIST modes (digits) are generated. The bottom row shows a bad training with mode collapse, where the generator outputs only the digit ' 6 '. Figure was taken from [44].
substitute the optimal discriminator $D^{*}(\mathbf{x})$ into Eq. (1), we can visualize the loss for the generator $G$ :</p>
<p>$$
\mathbb{L}<em _mathbf_x="\mathbf{x">{G}=\mathbb{E}</em>} \sim p_{r}} \log \frac{p_{r}(\mathbf{x})}{\frac{1}{2}\left[p_{r}(\mathbf{x})+p_{g}(\mathbf{x})\right]}+\mathbb{E<em g="g">{\mathbf{x} \sim p</em>-2 \cdot \log 2
$$}} \log \frac{p_{g}(\mathbf{x})}{\frac{1}{2}\left[p_{r}(\mathbf{x})+p_{g}(\mathbf{x})\right]</p>
<p>Before we continue any further, we mention two important metrics for probability measurement. The first one is the Kullback-Leiblar (KL) divergence:</p>
<p>$$
K L\left(p_{1} | p_{2}\right)=\mathbb{E}<em 1="1">{\mathbf{x} \sim p</em>
$$}} \log \frac{p_{1}}{p_{2}</p>
<p>which measures how much the distribution $p_{2}$ differs from the distribution $p_{1}$. Note that this metric is not symmetrical, i.e., $K L\left(p_{1} | p_{2}\right) \neq K L\left(p_{2} | p_{1}\right)$. A symmetrical metric is Jensen-Shannon (JS) divergence, defined as:</p>
<p>$$
J S\left(p_{1} | p_{2}\right)=\frac{1}{2} K L\left(p_{1} | \frac{p_{1}+p_{2}}{2}\right)+\frac{1}{2} K L\left(p_{2} | \frac{p_{1}+p_{2}}{2}\right)
$$</p>
<p>Back to our GAN with the optimal $D$, Eq. (4) shows that the GAN loss function can be reformulated as:</p>
<p>$$
\mathbb{L}<em r="r">{G}=2 \cdot J S\left(p</em>\right)-2 \cdot \log 2
$$} | p_{g</p>
<p>which shows that for $D^{*}$, the generator loss turns to be a minimization of the JS divergence between $p_{r}$ and $p_{g}$. The relation between GAN training and the JS divergence may explain its instability. To understand this, view Fig. 3, which shows</p>
<p>an example for JS divergences of different distributions. In Fig. 3(a) we see the real image distribution $p_{r}$, as a Gaussian with zero mean, and we consider three examples for generated image distributions: $p_{g 1}, p_{g 2}$, and $p_{g 3}$. Fig. 3(b) plots the $J S\left(p_{r}(\mathbf{x}), p_{q}(\mathbf{x})\right)$ measure between $p_{r}$ and some $p_{q}$ distribution, where the mean of $p_{q}$ ranges from 0 to 80 . As shown in the red box, the gradient of the JS divergence vanishes after a mean of 30. In other words, when the discriminator is close to optimal $\left(D^{*}(\mathbf{x})\right)$, and we try to train a "poor" generator with $p_{g}$ far from the real distribution $p_{r}$, training will not be feasible due to extremely low gradients. This is prominent especially in the beginning of the training where $G$ weights are randomized.</p>
<p>Due to the vanishing gradient problem, Goodfellow et al. [20] proposed a change to the original adversarial loss. Instead of minimizing $\log (1-D(G(\mathbf{z})))$ (as in Eq. (2)), they suggest to maximize $\log (D(G(\mathbf{z})))$. The latter cost function yields the same fixed point of $D$ and $G$ dynamics but maintains higher gradients early in the training when the distributions $p_{r}$ and $p_{g}$ are far from each other. On the other hand, this training strategy promotes mode collapse, as we show next.</p>
<p>With an optimal discriminator $D^{*}, K L\left(p_{g} | p_{r}\right)$ can be reformulated as:</p>
<p>$$
\begin{aligned}
K L\left(p_{g} | p_{r}\right) &amp; =\mathbb{E}<em g="g">{\mathbf{x} \sim p</em> \
&amp; =\mathbb{E}}} \log \frac{p_{g}(x) /\left(p_{r}(x)+p_{g}(x)\right)}{p_{r}(x) /\left(p_{r}(x)+p_{g}(x)\right))<em g="g">{\mathbf{x} \sim p</em> \log \frac{1-D^{}<em>}(x)}{D^{</em>}(x)} \
&amp; =\mathbb{E}<em g="g">{\mathbf{x} \sim p</em> \log \left[1-D^{}<em>}(x)\right]-\mathbb{E}<em g="g">{\mathbf{x} \sim p</em> \log \left[D^{}</em>}(x)\right]
\end{aligned}
$$</p>
<p>If we switch the order of the two sides in Eq. (8), we get:</p>
<p>$$
\begin{aligned}
-\mathbb{E}<em g="g">{\mathbf{x} \sim p</em> \log \left[D^{}<em>}(x)\right] &amp; =K L\left(p_{g} | p_{r}\right)-\mathbb{E}<em g="g">{\mathbf{x} \sim p</em> \log \left[1-D^{}</em>}(x)\right] \
&amp; =K L\left(p_{g} | p_{r}\right)-2 \cdot J S\left(p_{r} | p_{g}\right)+2 \cdot \log 2+\mathbb{E}<em r="r">{\mathbf{x} \sim p</em>(x)\right]
\end{aligned}
$$}} \log \left[D^{*</p>
<p>The alternative loss for $G$ is thus only affected by the first two terms (the last two terms are constant). Since $J S\left(p_{r} | p_{g}\right)$ is bounded by $[0, \log 2]$ (see Fig. 3(b)), the loss function is dominated by $K L\left(p_{g} | p_{r}\right)$, which is also called the reverse KL divergence. since $K L\left(p_{g} | p_{r}\right)$ usually does not equal $K L\left(p_{r} | p_{g}\right)$, the optimized $p_{g}$ by the reversed KL is totally different than $p_{g}$ optimized by the KL divergence. Fig. 4 shows the difference of the two optimization where the distribution $p$ is a mixture of two Gaussians, and $q$ is a single Gaussian. When we optimize for $K L\left(p_{r} | p_{g}\right), q$ averages all of $p$ modes to hit the mass center (Fig. 4(a)). However, for the reverse KL divergence optimization, $q$ distribution chooses a single mode (Fig. 4(b)), which will cause a mode collapse during training.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3 Vanishing gradients in GANs. (a) An example of a real image distribution $p_{r}(\mathbf{x})$, a Gaussian with zero mean, with three more different Gaussian distributions: $p_{g 1}, p_{g 2}$, and $p_{g 3}$. (b) Calculating the JS divergence measure between $p_{r}(\mathbf{x})$ and a Gaussian distribution with mean from 0 to 80 . When training with optimal discriminator $\left(D^{*}(\mathbf{x})\right)$, the generator $G$ minimizes the loss in Eq. (6), pushing $p_{g}(\mathbf{x})$ left towards $p_{r}(\mathbf{x})$, alas, it could take a very long time due to diminished gradients when being far from $p_{r}(\mathbf{x})$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4 Optimized distribution $q$ for (a) minimizing KL divergence $K L(p | q)$ and for (b) minimizing reverse KL divergence $K L(q | p)$. Figure is taken from [71].</p>
<p>In summary, using the original $G$ loss from Eq. (1) will result in vanishing gradients for $G$, and using the other loss in Eq. (9) will result in a mode collapse. These problems are inherent within the GAN loss and thus cannot be solved using sophisticated architectures. In Section 5 we discuss other loss functions for GANs, which solve these problems.</p>
<h1>3.3 Instability and Image quality</h1>
<p>Early GAN models which use the G losses described above $(\log (1-D(G(\mathbf{z})))$ and $-\log (D(G(\mathbf{z}))))$ exhibit great instability in their cost values during training. Arjovsky et al. [4] experimented with these two $G$ losses and claimed that in both cases the gradients cause instability to the GAN training. The loss in Eq. (1) stays constant after the first training steps (Fig. 5(a)), and the loss in Eq. (8) fluctuates during the entire training (Fig. 5(b)). In both cases, they did not find a significant correlation between the calculated loss and the generated image quality. In other words, it is very difficult to predict when during the training the generator actually produces good quality images, and the only way to get a good generator is to stop the training and manually visualize many generated images.</p>
<p>Using the above two G losses in Eq. (1) and Eq. (8) yield poor quality images compared to modern GAN models. In the following sections we will cover more sophisticated losses that enhance the generator's resolution and image size.</p>
<h3>3.4 Problems: summary</h3>
<p>The original GAN model and loss proposed by Goodfellow et al. [20] suffer from three inherent challenges: (1) Mode collapse; (2) Vanishing gradients; and (3) Image quality. Follow-up works improve the performance of GANs on one or more of these</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5 Training instability in GANs. (a) $J S$ distance metric in GAN training using the loss in Eq. (1). This quantity correlates poorly to the generated image quality, saturating to $\log 2 \approx 0.69$, the highest value taken by the $J S$ distance. (b) Generator loss during training using a different generator cost (maximizing $\log (D(G(\mathbf{z})))$ instead of minimizing $\log (1-D(G(\mathbf{z})))$ ) showing increasing error without a significant improvement in image quality. Plots are taken from [4]
problems by using different architectures for $D$ or $G$, modifying the cost function, and more. A subset of architecture-variant and loss-variant GANs are portrayed in Fig. 6(a),(b), respectively. A sample of some recent prominent GAN models are presented in the following sections.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6 Recent GAN models which solve the original GAN's problems: Mode diversity (collapse), vanishing gradients, and image quality. (a) A subset of architecture-variant GANs, (b): A subset of loss-variant GANs. Larger axis values indicate better performance. Red points indicate the model improves all three challenges, blue points improve two, and black points improve only one. Figures are taken from [72].</p>
<h1>4 Improved GAN architectures</h1>
<p>Many types of new GAN architectures have been proposed since 2014 ([6, 8, 13, 29, $57,77,30,16])$. Different GAN architectures were proposed for different tasks, such as image super-resolution [38] and image-to-image transfer [79, 53]. In this section we present some of these models, which improved the performance on image quality, vanishing gradients, and mode collapse, compared to the original GAN.</p>
<h3>4.1 Semi-supervised GAN (SGAN)</h3>
<p>Semi-supervised learning is a promising research field between supervised learning and unsupervised learning. In supervised learning, each data is labeled, and in unsupervised learning, no labels are provided; Semi-supervised learning has labels only for a small subset of the training data, and no annotations for the rest of the data, like many real-world problems.</p>
<p>SGAN [49] extended the original GAN learning to the semi-supervised context by adding to the discriminator network an additional task of classifying the image labels (Fig. 7). The generator's architecture is the same as before. In SGAN the discriminator utilizes two heads, a softmax and a sigmoid. The sigmoid is used to distinguish between real and fake images, and the softmax predicts the images' labels (only for images predicted as real). The results on MNIST showed that both $D$ and $G$ in SGAN are improved compared to the original GAN.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7 SGAN architecture.</p>
<h3>4.2 Conditional GAN (CGAN)</h3>
<p>CGAN was originally proposed as an extension of the original GAN, where both the discriminator and generator were fed by an additional class of the image [45, 50]. An</p>
<p>illustration of CGAN architecture is shown in Fig. 8. In this setup the loss function in Eq. (1) is slightly modified to condition both the real images $x$ and the latent variable $z$ on $y$ (the label):</p>
<p>$$
\min <em D="D">{G} \max </em>} \mathbb{E<em x="x">{\mathbf{x} \sim p</em>}} \log [D(\mathbf{x} \mid \mathbf{y})]+\mathbb{E<em _mathbf_z="\mathbf{z">{\mathbf{z} \sim p</em>))]
$$}}} \log [1-D(G(\mathbf{z} \mid \mathbf{y</p>
<p>All values $(x, y, z)$ are first encoded by some neural layers prior to their fusion in the discriminator and generator. This improves the ability of the discriminator to classify real/fake images and enhances the generator's ability to control the modalities of the generated images. [45] showed that their CGAN architecture used with a language model can handle also multimodal datasets, such as Flicker that contains labeled image data with their particular user tags. They demonstrated that their generator is capable of producing an automatic tagging.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8 CGAN architecture. Both the generator and discriminator are fed with a class label $y$ to condition both the images $x$ and latent variable $z$ (see Eq. (10)).</p>
<h1>4.3 Deep Convolutional GAN (DCGAN)</h1>
<p>Before describing DCGAN, we provide a brief reminder of what is a convolutional neural network.</p>
<p>CNNs: Convolutional neural networks (CNNs) were proposed by LeCun et al. [37]; These networks consist of trained spatial filters applied on hidden activations throughout their architecture. These networks perform correlations using their trained kernels with a sliding window over images (or hidden activations).</p>
<p>This was shown to improve the accuracy on many recognition tasks, especially in computer vision. A very basic and popular CNN network called LeNet is shown in Fig. 9.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9 LeNet architecture. A popular Convolutional Neural Network (CNN) used for image recognition. Image was taken from [37].</p>
<p>DCGAN proposed to create images using solely deconvolutional networks in their generator [57]. The generator architecture is depicted in Fig. 10. Deconvolutional networks can be conceived as CNNs that use the same components but in reverse, projecting features into the image pixel space. [76] showed that deconvolutional layers achieve good visualization for CNNs; this allowed the DCGAN generator to create high-resolution images for the first time.</p>
<p>In addition to improved resolution, DCGAN showed better stability during the training thanks to multiple modifications in the original GAN network:</p>
<ul>
<li>All pooling layers were replaced. In the discriminator, they used convolution kernels with stride $&gt;1$, and the generator utilized fractional-strided convolution to increase the spatial size.</li>
<li>Both the discriminator and generator were trained with batch normalization which promotes similar statistics for real images and fake generated images.</li>
<li>The discriminator architecture replaces all normal ReLU activations with LeakyReLU [40]; this activation multiplies also the negative kernel output by a small value ( 0.2 ), to prevent from "dead" gradients to propagate to the generator. The generator architecture used ReLU after every deconvolution layer except the output, which uses Tanh activation.</li>
</ul>
<p>The authors demonstrated empirically that the mere CNN architecture used in DCGAN in not the key contributing factor for the GAN's performance, and the above modifications are crucial. To show that they measured GANs quality by considering them as feature extractors on supervised datasets, and evaluating the performance of linear models trained on these features (for more information see [57]). DCGAN yields a classification error of $22.48 \%$ on the StreetView House Numbers dataset (SVHN) [47], whereas a purely supervised CNN with the same architecture achieved a significantly higher $28.87 \%$ test error.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10 DCGAN generator architecture. This generator creates a complex $64 \times 64$ pixel image from a 100 dimensional uniform distribution $z$. No fully connected or pooling layers are used. Figure is taken from [57]</p>
<h1>4.4 Progressive GAN (PROGAN)</h1>
<p>PROGAN described a novel training methodology for GANs, involving progressive steps toward the development of the entire network architecture [29]. This progressive architecture uses the idea of progressive neural networks first proposed by Rusu et al. [62]. These architectures are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. The progressive training scheme is shown in Fig. 11. First, they trained low resolution $4 \times 4$ pixel images. Next, both $D$ and $G$ grow to include a layer of $8 \times 8$ spatial resolution. This progressive training gradually adds more and more intermediate layers to enhance the resolution, until reaching a high resolution of $1024 \times 1024$ pixel images with the CelebA dataset. All previous layers remain trainable in later steps.</p>
<p>Many state-of-the-art GAN architectures utilize this type of progressive training scheme, and it has resulted in very credible images [29, 30, 8, 32, 60], and more stable learning for both $D$ and $G$.</p>
<h3>4.5 BigGAN</h3>
<p>Attention in computer vision is a method that focuses the task on an "interesting" region in the image. The self-attention is used to train the network (usually CNN) in an unsupervised manner, teaching it to segment or localize the most relevant pixels (or activations) for the vision task.</p>
<p>BigGAN [8] has achieved state-of-the-art generation on the ImageNet datasets. Its architecture is based on the Self-attention GAN (SAGAN) [77], which employs a self-attention mechanism in both $D$ and $G$, to capture a large receptive field without sacrificing computational efficiency for CNNs [69]. The original SAGAN</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11 Training process for the PROGAN progressive methodology. Training starts with both $G$ and $D$ having a low spatial resolution of $4 \times 4$ pixels. Every training step adds a new incremented intermediate layer to $D$ and $G$, enhancing the generated images' resolution. All existing layers are trainable throughout the process. The images on the right are examples of generated images using PROGAN at $1024 \times 1024$. Figure was taken from [29]
architecture can learn global semantics and long-range dependencies for images, thus generating excellent multi-label images based on the ImageNet datasets ( $128 \times 128$ pixels). BigGAN achieved improved performance by scaling up the GAN training: Increasing the number of network parameters $(\times 4)$ and increasing the batch size $(\times 8)$. They achieved better performance on ImageNet with size $128 \times 128$ and were also able to train BigGAN also on the resolutions $256 \times 256$ and $512 \times 512$.</p>
<p>Unlike many previous GAN models, which randomized the latent variable from either $z \sim \mathcal{N}(0,1)$ or $z \sim \mathcal{U}(-1,1)$, BigGAN uses a simple truncation trick. During training $z$ is sampled from $\mathcal{N}(0,1)$, but for generating images in inference $z$ is selected from a truncated normal, where values that lie outside the range are re-sampled until falling in the range. This truncation trick shows improvement in individual sample quality at the cost of a reduction in overall sample variety, as shown in Fig. 12(a). Notice though that using this different inference sampling trick as is may not generate good images with some large models, producing some saturation artifacts (Fig. 12(b)). To solve this issue, the authors proposed to use Orthogonal Regularization to force $G$ to be more amenable to truncation, making it smoother so that the full lateral space will map to good generated images.</p>
<h1>4.6 StyleGAN</h1>
<p>StyleGAN [30] proposed an alternative generator architecture for GANs. Unlike the traditional $G$ architecture that samples a random latent variable at its input, their architecture starts from a learned constant input and adjust the characteristics of the images in every convolutional layer along $G$ using different, learned latent variables. Additionally, they inject learned noise directly throughout $G$ (see Fig. 13).</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 12 Using truncated sampling from the latent space. (a) Trade-off between image quality and image variety. From left to right, the range threshold is set to $2,1,0.5$, and 0.04 . (b) Saturation artifacts from applying truncated normal distribution to a model training with $z \sim N(0,1)$. Images were taken from [8].
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 13 Comparison between a traditional generator (left) to the StyleGAN generator (right). Unlike the standard generator which selects a random latent variable only in its input, in StyleGAN generator the latent input is mapped to an intermediate latent space $\mathcal{W}$, which then controls the generator through adaptive instance normalization (AdaIN). Also, Gaussian noise is injected after every convolution layer. "A" corresponds to a learned affine transform, and "B" applies learned scaling factors to the input noise. Figure is taken from [30].</p>
<p>This architectural change leads to automatic, unsupervised separation of highlevel attributes (e.g., pose and identity) from low-level variations (e.g., freckles, hair) in the generated images. StyleGAN did not modify the discriminator or the loss function. They used the same $D$ architecture as in [29]. In addition to state-of-the-art quality for face image generation, StyleGAN demonstrates a higher degree of latent space disentanglement, presenting more linear representations of different factors of variation, turning the GAN synthesis to be much more controllable. Recently a more advanced styleGAN architecture has been proposed [32, 18, 33]. Some generated examples are presented in Fig. 14. One may also use StyleGAN for image editing by calculating the latent vector of a given input image in the styleGAN (this operation is known as styleGAN inversion) and then manipulating this vector for editing the image $[75,60,1,68,2,65,64,54]$.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Fig. 14 Selected face images generated using the StyleGAN2 architecture (an improved version of the original StyleGAN), trained on the FFHQ dataset. Figure from [32].</p>
<h1>5 Improved GAN objectives</h1>
<p>As described in Section 3.2, the original GAN's min-max loss (Eq. (1)) promotes mode collapse and vanishing gradient phenomena. This section presents selected loss functions and regularizations that remedy these problems and also improves the image quality. This is just a partial list and other loss functions and regularization methods exist such as the least square GANs [41] or optimal transport models [55].</p>
<h3>5.1 Wasserstein GAN (WGAN)</h3>
<p>WGAN [4] has solved the vanishing gradient and mode collapse problems of the original GAN by replacing the cost in Eq. (1) with the Earth Mover (EM) distance, which is known also as the Wasserstein distance and is defined as:</p>
<p>$$
W\left(p_{r}, p_{g}\right)=\inf <em r="r">{\gamma \in\left[\left|\left(p</em>[||x-y|]]
$$}, p_{g}\right)\right.\right.} \mathbb{E}_{(x, y) \sim \gamma</p>
<p>where $\prod\left(p_{r}, p_{g}\right)$ denotes the set of all joint distributions $\gamma(x, y)$ whose marginals are $p_{r}$ and $p_{g}$, respectively. The EM distance is therefore the minimum cost of transporting "mass" in converting distribution $p_{r}$ into the distribution $p_{g}$.</p>
<p>Unlike KL and JS distances, EM is capable of indicating distance even when the $p_{r}$ and $p_{g}$ distributions are far from each other; EM is also continuous and thus provides useful gradients for training $G$. However, the infimum in Eq. (11) is highly intractable, so the authors estimate the EM cost with:</p>
<p>$$
\max <em _mathbf_x="\mathbf{x">{w \sim W} \mathbb{E}</em>} \sim p_{r}}\left[f_{w}(\mathbf{x})\right]-\mathbb{E<em z="z">{\mathbf{z} \sim p</em>))\right]
$$}}\left[f_{w}(G(\mathbf{z</p>
<p>where $\left{f_{w}\right}<em L="L">{w \in \mathcal{W}}$ is a parameterized family of all functions that are $K$-Lipschitz for some $K\left(|f|</em> \leq K\right)$. Readers are referred to [4] for more details.</p>
<p>The authors proposed to find the best function $f_{w}$ that maximize Eq. (12) by back-propagating $\mathbb{E}<em z="z">{\mathbf{z} \sim p</em>$ are the discriminator's parameters, and the objective of $D$ is to maximize Eq. (12), which approximates the EM distance. When $D$ is optimized, Eq. (12) becomes the EM distance, and $G$ is optimized to minimize it:}}\left[\nabla_{\theta} f\left(g_{\theta}(z)\right)\right]$, where $g_{\theta}$ are the generator's weights. $f_{w}$ can be realized by $D$ but constrained to be K-Lipschitz and $z$ is the lateral input noise for $G . w$ in $f_{w</p>
<p>$$
-\min <em _mathbf_z="\mathbf{z">{G} \mathbb{E}</em>))\right]
$$} \sim p_{z}}\left[f_{w}(G(\mathbf{z</p>
<p>Fig. 15 compares the gradient of WGAN to the original GAN from two non overlapping Gaussian distributions. It can be observed that WGAN has a smooth and measurable gradient everywhere (blue line), and learns better even when $G$ is not producing good images.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Fig. 15 Differentiating two Gaussian distributions using optimal discriminator $D^{*}$. The original GAN cost saturates (red line), resulting in vanishing gradients, whereas WGAN cost objective (blue line) yields measurable gradients. Plot was taken from [4].</p>
<h1>5.2 Self Supervised GAN (SSGAN)</h1>
<p>We showed in Section 4.2 that CGANs can generate natural images. However, they require labeled images to do so, which is a major drawback. SSGANs [10] exploit two popular unsupervised learning techniques, adversarial training, and self-supervision, bridging the gap between conditional and unconditional GANs.</p>
<p>Neural networks have been shown to forget previous learned tasks [17, 35], and catastrophic forgetting was previously considered as a major cause for GAN training instability. Motivated by the desire to counter the discriminator forgetting, SSGAN adds to the discriminator an additional loss, which enables it to learn useful representations, independently of the quality of the generator. In a self-supervised manner, the authors train a model on a task of prediction a rotation angle ( $\left[0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\right]$ ), as shown in Fig. 16. The objectives of $D$ and $G$ are updated to:</p>
<p>$$
\begin{aligned}
&amp; L_{G}=-V(G, D)-\alpha \mathbb{E}<em G="G">{\mathbf{x} \sim P</em>}} \mathbb{E<em D="D">{r \sim \mathbb{R}}\left[\log Q</em>\right)\right] \
&amp; L_{D}=V(G, D)-\beta \mathbb{E}}\left(R=r \mid \mathbf{x}^{r<em a="a" d="d" t="t">{\mathbf{x} \sim P</em>}} \mathbb{E<em D="D">{r \sim \mathbb{R}}\left[\log Q</em>\right)\right]
\end{aligned}
$$}\left(R=r \mid \mathbf{x}^{r</p>
<p>where $V(G, D)$ is the original GAN objective in Eq. (1), $P_{\text {data }}$ and $P_{G}$ are the real data and generated data distributions, respectively, $r \in R$ is a rotation selected from</p>
<p>a set of all allowed angles $\left(R=\left{0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\right}\right)$. An image $\mathbf{x}$ rotated by $r$ degrees is denoted as $\mathbf{x}^{r}$, and $Q\left(R \mid \mathbf{x}^{r}\right)$ is the discriminator's predictive distribution over the angles of rotation of the sample. These new losses enforce $D$ to learn good representation via learning the rotation information in a self-supervised approach.</p>
<p>Using the above scheme, SSGAN achieves good high-quality images, matching the performance of conditional GANs without having access to labeled data.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Fig. 16 Self-Supervised GAN (SSGAN) discriminator. The discriminator, $D$, performs two tasks: Identifying real/fake images (as the original GAN) and rotation classification. Both the real and fake images are rotated by ( $\left[0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\right]$ ), and sent to the rotation degree classifier (in blue). Only the original $0^{\circ}$ images are sent to the real/fake classifier (in red). Plot is taken from [10].</p>
<h1>5.3 Spectral Normalization GAN (SNGAN)</h1>
<p>SNGAN [46] proposes to add weight normalization to stabilize the training of the discriminator. Their technique is computationally inexpensive and can be applied easily to existing GANs architectures. In previous works that stabilized GAN training ( $[21,4,56])$ it was emphasized that $D$ should be a K-Lipshitz continuous function, forcing it not to change rapidly. This characteristic of $D$ stabilizes the training of GANs. SNGAN controls the Lipschitz constant of $D$ by literally constraining the spectral norm of each layer, normalizing each weight matrix $W$ so it satisfies the spectral constraint $\sigma(\mathbf{W})=1$ (i.e., the largest singular value of the weight matrix of each layer is 1 ). This is performed by simply normalizing each layer:</p>
<p>$$
\hat{\mathbf{W}}_{S N}(\mathbf{W})=\frac{\mathbf{W}}{\sigma(\mathbf{W})}
$$</p>
<p>where $\mathbf{W}$ are the weight parameters of each layer in $D$. This paper proves that this will make the Lipschitz constant of the discriminator function to be bound by 1, which is important for the WGAN optimization.</p>
<p>SNGAN achieves an extraordinary advance on ImageNet, and better or equal quality on CIFAR-10 and STL-10, compared to the previous training stabilization techniques that include weight clipping [4], gradient penalty [74, 43], batch normalization [26], weight normalization [63], layer normalization [5], and orthonormal regularization [7].</p>
<h1>5.4 SphereGAN</h1>
<p>SphereGAN [52] is a novel integral probability metric (IPM)-based GAN, which uses the hypersphere to bound IPMs in the objective function, thus enhancing the stability of the training. By exploiting the information of higher-order statistics of data using geometric moment matching, they achieved more accurate results. The objective function of SphereGAN is defined as</p>
<p>$$
\min <em D="D">{G} \max </em>, D(G(z)))\right]
$$} \sum_{r} E_{x}\left[d_{x}^{r}(\mathbf{N}, D(x))\right]-\sum_{r} E_{z}\left[d_{x}^{r}(\mathbf{N</p>
<p>for $r=1, \ldots, R$ where the function $d_{x}^{r}$ measures the $r$-th moment distance between each sample and the north pole of the hypersphere, $\mathbf{N}$. Note that the subscript $s$ indicates that $d_{x}^{r}$ is defined on $\mathbb{S}^{n}$. Fig. 17 shows the pipeline of SphereGAN. By defining IPMs on the hypersphere, SphereGAN can alleviate several constraints that should be imposed on $D$ for stable training, such as the Lipschitz constraints required from conventional discriminators based on the Wasserstein distance.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Fig. 17 Pipeline of Sphere GAN. The generator creates fake data from noise inputs. Real and fake data are fed to the discriminator, which maps the output to an $n$-dimensional Euclidean feature space (yellow plane). Green and purple points on the feature plane correspond to fake and real samples, respectively. The key idea of SphereGAN is remapping the feature points into the $n$-dimensional hypersphere by using geometric transformations. These mapped points are then used to calculate the geometric moments centered at the north pole of the hypersphere $(N)$. While the discriminator tries to maximize the moment differences of real and fake samples, the generator tries to interfere with the discriminator by minimizing those moment differences. Figure was taken from [52].</p>
<p>Unlike conventional approaches that use the Wasserstein distance and add additional constraint terms (see Table 1 in [52]), SphereGAN does not need any additional constraints to force $D$ in a desired function space, due to the usage of geometric transformation in $D$.</p>
<h1>6 Data augmentation with GAN</h1>
<p>We turn now to describe how to use GAN for data augmentation. Data augmentation is a crucial process for tasks lacking large training sets. There are three circumstances which require data augmentation:</p>
<ul>
<li>Limited annotations: Training a large DNN with a very few labeled data in the training set.</li>
<li>Limited diversity: When the training data lacks variations. For example, it may not cover diverse illuminations or a variety of appearances.</li>
<li>Restricted data: The database might contain sensitive information, and thus accessing it directly is strictly restricted.</li>
</ul>
<p>The first two scenarios can be solved via supervised learning but they will cost a lot of human effort to enrich the labeled data or to use active learning approaches [11]. An alternative approach is to utilize GANs to augment the data. Semi-supervised GAN (SGAN) (see Section 4.1) is a simple example of such a GAN architecture that is able to generate new annotated images and can automatically enrich training data with few labels.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Gilad Cohen
Tel Aviv University, Tel Aviv 6997801, e-mail: giladco1@post.tau.ac.il
Raja Giryes
Tel Aviv University, Tel Aviv 6997801 e-mail: raja@tauex.tau.ac.il&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
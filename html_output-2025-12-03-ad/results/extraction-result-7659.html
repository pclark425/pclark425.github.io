<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7659 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7659</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7659</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-271859839</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.06610v1.pdf" target="_blank">CROME: Cross-Modal Adapters for Efficient Multimodal LLM</a></p>
                <p><strong>Paper Abstract:</strong> Multimodal Large Language Models (MLLMs) demonstrate remarkable image-language capabilities, but their widespread use faces challenges in cost-effective training and adaptation. Existing approaches often necessitate expensive language model retraining and limited adaptability. Additionally, the current focus on zero-shot performance improvements offers insufficient guidance for task-specific tuning. We propose CROME, an efficient vision-language instruction tuning framework. It features a novel gated cross-modal adapter that effectively combines visual and textual representations prior to input into a frozen LLM. This lightweight adapter, trained with minimal parameters, enables efficient cross-modal understanding. Notably, CROME demonstrates superior zero-shot performance on standard visual question answering and instruction-following benchmarks. Moreover, it yields fine-tuning with exceptional parameter efficiency, competing with task-specific specialist state-of-the-art methods. CROME demonstrates the potential of pre-LM alignment for building scalable, adaptable, and parameter-efficient multimodal models.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7659",
    "paper_id": "paper-271859839",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00479225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CROME: Cross-Modal Adapters for Efficient Multimodal LLM
13 Aug 2024</p>
<p>Sayna Ebrahimi saynae@google.com 
Google Cloud AI Research</p>
<p>Sercan Ö Arık soarik@google.com 
Google Cloud AI Research</p>
<p>Tejas Nama tejasnama@google.com 
Google Cloud AI Research</p>
<p>Tomas Pfister tpfister@google.com 
Google Cloud AI Research</p>
<p>CROME: Cross-Modal Adapters for Efficient Multimodal LLM
13 Aug 202468EB34DC3BB366207366E38DAF43C5A1arXiv:2408.06610v1[cs.CV]
Multimodal Large Language Models (MLLMs) demonstrate remarkable imagelanguage capabilities, but their widespread use faces challenges in cost-effective training and adaptation.Existing approaches often necessitate expensive language model retraining and limited adaptability.Additionally, the current focus on zeroshot performance improvements offers insufficient guidance for task-specific tuning.We propose CROME, an efficient vision-language instruction tuning framework.It features a novel gated cross-modal adapter that effectively combines visual and textual representations prior to input into a frozen LLM.This lightweight adapter, trained with minimal parameters, enables efficient cross-modal understanding.Notably, CROME demonstrates superior zero-shot performance on standard visual question answering and instruction-following benchmarks.Moreover, it yields fine-tuning with exceptional parameter efficiency, competing with task-specific specialist state-of-the-art methods.CROME demonstrates the potential of pre-LM alignment for building scalable, adaptable, and parameter-efficient multimodal models.</p>
<p>Introduction</p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have yielded impressive breakthroughs across many scenarios, particularly in vision-language learning.Notably, OpenAI's GPT-4v [1] and Google's Gemini Pro Vision [2] demonstrate exceptional performance for tasks like image captioning and visual question answering.Such commercial models are typically available through prediction-only APIs that limit their wider adaptation and customization.On the other hand, there are notable open-sourced vision-language models, including LLaVA [3], InstructBLIP [4], Qwen-VL [5], and BLIVA [6].These are often built using instruction tuning to improve multimodal capabilities of LLMs.Their success has been shown to depend on large-scale training data (can be hundreds of millions [7,4,8,6] to over a billion [5]), often yielding high training costs [3,9,10,5].Efficient methods for building large vision-language models from available vision-only or textonly pretrained models, as well as tuning them for target multimodal use cases, remain everlasting challenges.</p>
<p>The extensive parameter counts involved in training image encoders and language models is one key challenge behind high computational costs.While retraining the LLMs with multimodal data helps align visual and textual tokens [3,[9][10][11], it can come with the risk of undermining the pretrained LLMs' reasoning capabilities [12,5].Furthermore, as the variety of LLMs continues to grow, a retraining approach hinders their potential for 'plug-and-play integration' within multimodal frameworks.We propose that pre-aligning visual and textual tokens before LLM ingestion offers a more flexible, efficient, and scalable strategy that remains under-explored.Our empirical findings demonstrate that the way image and text representations are adapted for LLM compatibility can significantly affect multimodal understanding and reasoning.Most models still rely on simplistic linear projections before token concatenation [3,13].While BLIP-2 [7] and InstructBLIP [4] Figure 1: CROME achieves state-of-the-art results on 6 MLLM benchmarks (top right) with its unique pre-LM cross-modal adapter (middle right).Bottom Right: Training data and parameter comparisons.Left: A qualitative example on the unusual "ironing man" image.partially address the need for efficient cross-modal learning by introducing a Query Transformer (Q-Former) [4,6], they still face challenges.Their pretraining remains computationally expensive (approximately 100 GPU hours on &gt;100M image-text pairs), and fine-tuning for specific domains can be parameter-inefficient.</p>
<p>For real-world applications, improving MLLMs to excel in specific tasks is essential, extending their value beyond zero-shot scenarios.While zero-shot performance results showcase their potential to handle diverse tasks without training data, there is also a significant opportunity to maximize their effectiveness for cases where data for specific downstream tasks are available.In such cases, the ability to implement efficient and adaptable tuning strategies becomes crucial.While some MLLMs, such as LLaVA, suggest parameter-efficient tuning techniques like LoRA [14] or selectively training the image-language projector layer, the optimal implementation and generalizability of these across diverse tasks and datasets warrant further exploration.Developing effective, cost-efficient and flexible tuning strategies would not only unlock the full potential of MLLMs for targeted applications but also ensure improved and robust performance beyond zero-shot benchmarks.</p>
<p>In this paper, we propose CROME, a vision-language training framework featuring a vision encoder, query Transformer, and a novel gated cross-modal adapter depicted in 1.The proposed adapter unifies vision and language representations prior to LLM input, promoting superior cross-modal understanding while maintaining parameter efficiency by keeping both the LLM and vision encoder frozen.Our lightweight cross-modal fusion unit effectively learns cross-modal relations, making fine-tuning CROME remarkably straightforward.Our contributions can be summarized as:</p>
<p>• We present CROME, a novel vision-language learning framework featuring a lightweight gated cross-modal adapter (CROME-Adapter) which is used for aligning visual and textual tokens before LLM input for multimodal learning.This avoids costly LLM training and maintains generalization on text understanding and reasoning tasks.</p>
<p>• CROME introduces an effective, cost-efficient and flexible fine-tuning strategy to maximize MLLM effectiveness with availability of data from specific downstream tasks.The CROME-Adapter's design enables both cross-modal understanding and parameter-efficient fine-tuning, as only the adapter is trained during adaptation.</p>
<p>[Input Image]</p>
<p>What is unusual about this image?Vision-language models (VLMs) are often based on aligning image and text features in a unified embedding space, as proposed by the notable works CLIP [15] and ALIGN [16], followed by the subsequent ones [17][18][19][20].This alignment is achieved through contrastive learning objectives applied to extensive image-text pair datasets.VLMs achieve strong zero-shot and few-shot performance, showcasing significant generalization abilities across a range of downstream tasks.Benefiting from existing LLMs and vision encoders of VLMs as the visual backbone (e.g.CLIP's ViT encoder), recent MLLMs [3,21,2,22,4,7,10,6,4,5,13] achieve even greater visual perception, understanding, and reasoning abilities.Flamingo [11] establishes a connection between the vision encoder and LLMs using a Perceiver Resampler, showcasing remarkable few-shot performance.BLIP-2 [7] introduces a Q-former to align visual features with OPT [23] and FLAN-T5 [24].MiniGPT-4 [25] connects a ViT and Q-former with Vicuna [26] as the LLM using a linear projector.Recently using large-scale paired instruction-image data has become a popular technique to adapt LLMs to answer questions regarding a given image [3,21,2,22,4,7,10].These approaches often involve retraining the LLM and/or the vision encoder, which can be computationally demanding.As LLMs continue to evolve and gain capabilities, it becomes increasingly important for MLLMs to leverage the existing strengths of LLMs without requiring retraining.This can avoid the risk of catastrophic forgetting, where retraining could potentially degrade the LLM's core natural language processing (NLP) abilities [12,5].Qwen-VL [5] avoids this by adding text-only data to their pretraining and instruction-tuning.Existing methods that avoid LLM or vision encoder retraining, such as InstructBLIP and BLIVA [4,6], primarily focus on learning cross-modal interactions within a Q-Former module, which is then retrained during supervised fine-tuning.While CROME also utilizes a Q-Former, it introduces a novel, lightweight gated cross-modal adapter.This adapter differentiates our approach from BLIVA and InstructBLIP in two major ways: (i) it enhances cross-modal understanding, serving as an additional unit to further refine cross-modal interactions; and (ii) it acts as the sole trainable component during task-specific fine-tuning, maximizing tuning efficiency while preserving the existing instruction-aware capabilities in the Q-Former and the LLM.</p>
<p>Parameter Efficient Tuning</p>
<p>As LLMs grow in size, parameter-efficient tuning (PET) becomes essential for reducing memory and for cost-efficient training.PET involves selectively adding or adjusting a small number of parameters within a pretrained model for task-specific adaptation.Early PET methods focused on either language [27][28][29]14] or vision data [30][31][32][33].Extending beyond unimodal learning, vision and language adapters were introduced for smaller models [34][35][36].Approaches like MultiModal-GPT [37] and others [38,9] utilized LoRA [14] within architectures like Flamingo [11].Similarly, LLaMA-adapter employed prefix tuning [39] with image embeddings as prefix tokens.Recent methods like LaVIN [12] and PILL [40] focus on adapting LLMs for multimodal instructions.</p>
<p>LaVIN employs the AdaMix adapter [41] in LLaMA and RepAdapter [33] in ViT in an end-to-end manner.However, these adapters are integrated within Transformer blocks, and their performance generalization to other language models remains unexplored.CROME distinguishes itself with a modular cross-modal adapter that works with encoder-decoder and decoder-only LLMs.During large-scale instruction tuning, both the adapter and Q-Former are trained.Crucially, for supervised fine-tuning on smaller datasets, the adapter becomes the sole trainable component.This approach yields more flexibility and efficiency given the wide range of adaptation scenarios, while protecting the LLMs' existing capabilities.</p>
<p>CROME: a Cross-modal adapter based MLLM</p>
<p>CROME is a multimodal LLM framework which receives image and text as the multimodal input and generates text in an autoregressive manner.We first introduce CROME's architecture, focusing on the proposed adapter modules for superior MLLM results.Then, we explain the applied multimodal training methods: pretraining, instruction tuning, and the optional task-specific tuning.</p>
<p>Model architecture</p>
<p>CROME is composed of a pretrained frozen LLM, frozen vision encoder, and a query Transformer components, as shown in 2. The projected image patch and query embeddings are passed to a cross modal adapter before getting concatenated with text embeddings and fed into the LLM.Note that although the high level architecture resembles the InstructBLIP family of MLLMs, this important aspect of how visual-text are processed before inputting them into LLMs differentiates CROME.Below, we go through the details of each component:</p>
<p>Vision encoder.We utilize a vision encoder to extract image features, which are then processed by a linear projection layer and the Q-Former.During pretraining and instruction tuning, we keep the vision encoder itself frozen and maintain its pretrained visual representations, in order to obtain low-cost and parameter-efficient training.Only the associated projection layer is trained during these stages.For details on image preprocessing, please refer to the Appendix.</p>
<p>Large language model.To ensure low-cost and parameter-efficient training, as well as improved generalization with limited tuning data, we propose keeping the LLM entirely frozen during all stages of training of the CROME.This enables utilizing both decoder-only and encoder-decoder type model architectures as the LLMs.</p>
<p>Query Transformer (Q-Former).We employ a Q-Former architecture, in which queries interact with each other through self-attention and with frozen image features through cross-attention, which is inserted every other Transformer block.We initialize it with InstructBLIP's model Q-Former and all its 188M parameters are trained during the instruction-tuning phase, which only has nearly 20% and 2.6% of the vision encoder and the LLM parameters, respectively, which ensures the parameter CROME adapter Inspired by the concept of adapter networks [42], which introduce parameter efficiency, we propose a lightweight cross-modal module within CROME.Crucially, unlike typical adapter placement after feed-forward and self-attention layers in Transformers, this module facilitates the fusion of textual and visual representations before they enter into the LLM.This pre-LLM fusion offers a potential advantage for aligning different modalities for optimal understanding within the LLM.During fine-tuning, the cross-modal adapters are the only trainable components, enabling remarkably-efficient adaptation of CROME allowing it to adapt to new tasks without extensive retraining of the core LLM.</p>
<p>As shown in 2, we use a conventional bottleneck structure [42] with down-projection and up-projection units and skip connections.This design allows for efficient processing of high-dimensional input features.We use a modality-specific down-sampling unit for vision and text branches, where in each of them an input d-dimensional feature vector is projected to a smaller dimension, m.Inspired by the success of gated linear units in feedforward layers from Transformers [43,44], in the down-projection unit we use the component-wise product of two linear transformations named as W d ∈ R d×m and W g ∈ R d×m where the input one of which is sigmoid-activated [45,46].This gating mechanism helps the adapter control the flow of information, potentially emphasizing the most useful and relevant multimodal relationships.</p>
<p>For each down-projection unit, given an input text or image feature embedding of size x ∈ R d , the output is mapped as:
z(x) = SiLU(xW d ) ⊗ xW g ,(1)
where SiLU is Sigmoid Linear Unit function [45,46].On the other hand, the up-projection unit uses a weight-sharing mechanism between the two modalities where the m-dimensional vector z ∈ R m is projected back to d input dimensions via W u ∈ R m×d , in order to better encourage learning of cross-modal relations.Overall, the output of each branch of the cross-modal adapter can be formulated as:
CROME-Adapter(x, W d , W g , W u ) = x + zW u(2)
Finally, the inputs to the LLM are formed by concatenating the tokenized text, the output of the text branch within the CROME-Adapter, and the output of the vision branch of the CROME-Adapter (see 2).</p>
<p>Training CROME</p>
<p>In this section, we describe different training processes for CROME: (a) pretraining with imagecaption pairs followed by (b) instruction tuning with image-instructions on a variety of tasks, and (c) optional task-specific efficient fine-tuning which is used if data is available for a specific target task to optimize CROME's task-specific performance.Throughout these stages, we use next token prediction as the training objective where the LLM predicts the next word conditioned on previous multimodal visual and text tokens.This encourages the model to accurately generate subsequent tokens based on the context of preceding tokens.Figure 3 provides a visual representation of the training stages and trainable model components, described in detail below.</p>
<p>Pretraining.Our approach begins with a pretraining phase designed to align modalities within the projection layers.As shown in 3(a), during this stage, we train the image and text projection layers alongside the cross-modal adapter.The remaining model layers are kept frozen.</p>
<p>Instruction tuning.At this stage, the model is refined to follow instructions accurately.We utilize a diverse set of image-instruction pairs to train the model to answer specific queries about images, extending its abilities beyond the image captioning learned during pretraining.During instruction tuning, we train the Q-Former, projection layers, and CROME-adapter parameters.This enables the model to efficiently learn instruction-aware queries, facilitated by the cross-modal interaction between image embeddings and queries within the Q-Former (see 3(b)).The result of this instruction tuning is a model capable of strong zero-shot performance on visual question-answering benchmarks.</p>
<p>Optional task-specific fine-tuning.When additional task-specific data (often smaller scale than the previous stage) is available, this step further optimizes CROME's performance at the target task.The CROME-Adapter allows for efficient fine-tuning by limiting the number of trainable parameters to approximately 5M (see 3 (c)).Besides low cost task-specific tuning, such parameter efficiency yields constitutes an effective mechanism to prevent overfitting, a commonly-observed challenge with small amount of task-specific data.</p>
<p>Experiments</p>
<p>In this section, we first discuss the datasets used to train CROME, followed by implementation details including model architecture and training parameters.We note that our model and data will be publicly available.Finally, we outline the benchmarks used to evaluate CROME's performance.</p>
<p>Datasets for Training CROME</p>
<p>As for the pretraining (PT) dataset, we use the LLaVA Pretrain LCS-558K [3] which is a filtered subset of LAION/CC/SBU dataset as image-caption pairs consistently across all experiments.As for the instruction tuning (IT) dataset, we consider three different options for different experiments to highlight different outcomes:</p>
<p>• IT Dataset 1 (665K samples), used for evaluating pre-LM modality alignment: To demonstrate the effectiveness of CROME's pre-LM input alignment units compared to LLM retraining used in models such as LLaVA, we use the same 665K instruction-image pairs as LLaVA1.5 [3]1 .</p>
<p>• IT Dataset 2 (1.2M samples), used for evaluating the effect of CROME-Adapter: To facilitate comparison with InstructBLIP and BLIVA models and showcase the CROME-Adapter's effectiveness, we use a similar dataset with 1.2M samples.Where specific subsets are unavailable, we compensate by sampling additional examples from MSCOCO-based datasets with multiple questions and answers per image, ensuring a consistent 1.2M sample size.</p>
<p>• IT Dataset 3 (8M samples), used for large-scale Instruction-tuning: We increase Dataset 2 to 8M image-instruction pairs, incorporating data from LVIS-Instruct4v [47], LAMM [48], Flickr30K [49], and ShareGPT4V [50].This larger and more diverse dataset allows assessing CROME's generalization to broader visual concepts and instructions, and further highlight its efficiency in adapting to large-scale data.Our main results are reported using the model trained on this dataset.Details on how we mixed these datasets are given in the Appendix.</p>
<p>Dataset balancing.We follow InstructBLIP balancing strategy to sample datasets with ratios proportional to the square root of their sizes (the numbers of training samples).Given D datasets with sizes {N 1 , N 2 , • • • , N D }, we set the probability of a data sample being selected from a dataset
d during training is p d = √ N d D i=1 √ Ni .</p>
<p>Implementation Details</p>
<p>Model architectures</p>
<p>We adopt the ViT-G/14 architecture from EVA-CLIP [51] as the vision encoder, that processes raw images of size 224x224.We extract features from its second to the last layer.As the LLM, we consider two distinct architectures: Vicuna7B/13B v1.5 (decoder-only), instruction-tuned from LLaMA2 [52]; and Flan-T5 XXL [24] (encoder-decoder), instruction-tuned from T5 [53].Our Q-Former follows a design similar to BLIP2 and is initialized from the InstructBLIP model.It uses a set of 32 learnable query embeddings each with a dimension of 768.</p>
<p>Training details</p>
<p>We pretrain the projection layer for 5 epochs with a batch size of 32.During the instruction tuning stage, we employ a batch size of 16 with a maximum of 2M iterations, which roughly iterates over 4 epochs of the training data.For both training stages, we use the AdamW [54] optimizer, with β 1 = 0.9, β 2 = 0.999, and a weight decay of 0.05.Additionally, we apply a linear warmup of the learning rate during the initial 1K steps, increasing from 10 −8 to 10 −5 , followed by a cosine decay with a minimum learning rate of 0.</p>
<p>MLLM zero-shot benchmarks</p>
<p>We compare CROME on a list of benchmarks for open-source MLLMs and the ones that are accessibly only via prediction-only APIs.We report results in MMMU [55], MME Perception (MME P ) [56], MME Cognition (MME C ) [56], MMBench (MMB) [57], MM-Vet [58], HallusionBench (HallB) [59], LLaVA-Bench In-the-Wild (LLaVA W ) [3], and SEED-Bench Image Part (SEED I ) [60].MMMU benchmark is designed to evaluate multimodal models on multi-discipline tasks demanding collegelevel subject knowledge and deliberate reasoning.MME P and MME C measure both perception and cognition abilities on a total of 14 subtasks.MMBench, contains approximately 3000 single choice questions covering 20 different ability dimensions, such as object localization and social reasoning.MM-Vet defines 6 core capabilities and examines the 16 integrations of interest derived from the capability combination.As the evaluation metrics, an LLM-based evaluator is used as 'judge' for open-ended outputs.HallusionBench comprises 346 images paired with 1129 questions, all crafted by human experts to evaluate image-context reasoning with respect to visual hallucination.LLaVA-Bench In-the-Wild is a small dataset with a set of 24 images with 60 questions in total, including indoor/outdoor scenes, memes, paintings, sketches, etc. to evaluate the MLLM capability at challenging tasks and generalizability to novel domains.Lastly, SEED-Bench is a benchmark to text instruction following capabilities, consisting of 19K multiple choice questions with accurate human annotations which can objectively measure MLLMs performance without the need for human or LLM judge intervention.</p>
<p>Evaluation metrics</p>
<p>For consistency, we report official metrics computed using the standard implementations associated with each benchmark.</p>
<p>5 Results and Discussions only via prediction APIs, CROME-Vicuna13B can outperform GPT4-V and Gemini Pro Vision on SEED I and Gemini Pro Vision on LLaVA W and MME P .</p>
<p>Table 2 compares CROME trained on different dataset sizes, each benchmarked against a corresponding baseline with similar LLM architecture and size.Comparing the models with Flan-T5 XXL backbone, CROME trained with 1.2M IT samples, shows superior performance on MME P , MM-Bench, MM-Vet, LLaVA W and SEED I benchmarks.When trained with 8M IT samples, CROME consistently outperforms BLIVA and InstructBLIP on all the benchmarks.This highlights that CROME can effectively take advantage of more instruction tuning samples.</p>
<p>Using a similar instruction-tuning dataset as BLIVA and InstructBLIP, CROME achieves higher performance on 6 out of 8 benchmarks.On HallB, InstructBLIP's larger pretraining dataset of 129M samples, contributes to its advantage, but CROME remains competitive.On LLaVA W , LLaVA and BLIVA outperform CROME with margins of 2.4 and 0.9 points respectively, while other models lag by at least 15 points.On SEED I , CROME surpasses BLIVA and InstructBLIP but falls short of LLaVA, likely due to LLaVA's higher-resolution image encoder which benefits this visually-rich benchmark.</p>
<p>Intriguingly, when comparing CROME-Vicuna-7B across dataset sizes (1.2M and 8M) with LLaVA-7B-1.6, we see that larger training data (8M) enables CROME to outperform a model with more parameters.Further, increasing CROME's LLM backbone (Vicuna 13B) widens the performance gap between CROME and LLaVA, emphasizing the effectiveness of our modality alignment module in leveraging the LLM's existing capabilities.</p>
<p>Task-Specific Fine-tuning</p>
<p>We evaluate CROME's task-specific fine-tuning capabilities considering small-scale labeled datasets for two tasks: (i) ScienceQA-Image (SQA I ) [61], containing elementary and high school science curricula and (ii) AI2D [62], which covers diagrams from grade school science.These datasets are specifically chosen to assess CROME's adaptation to unseen tasks, as neither they nor similar scientific or math VQA content are included in our training data.</p>
<p>We initialize CROME from the instruction-tuned model (for which we report the zero-shot performance), and selectively train only the CROME-Adapter parameters during fine-tuning.Training details are provided in the Appendix.As they are both multiple-choice outputting tasks, we use accuracy as the evaluation metric.</p>
<p>As shown in 3, CROME achieves an impressive 93.2% accuracy on SQA I , significantly improving its zero-shot performance by 32%.Despite lacking the data augmentation via prompting in chainof-thought (CoT) baselines which have multiple stages of training to create rationals as additional context [63,64] and training with more parameters, CROME demonstrates competitive performance.Moreover, CROME outperforms other adapter-based approaches [12,40] in accuracy while utilizing much lower amount of trainable parameters.Notably, the proposed adapter's pre-LLM placement distinguishes it from LaVIN and PILL, which incorporate adapters within the Transformer layers of LLaMA-based models.</p>
<p>Table 4 presents zero-shot and fine-tuned performance on the AI2D dataset for CROME, InstructBLIP, and BLIVA.We include Qwen-VL-Chat's reported results as a baseline, noting their use of AI2D in pretraining, unlike the other models.At zero-shot, CROME surpasses BLIVA by 0.9% and InstructBLIP by 3%.However, without prior exposure to science-related VQA, none of the models achieve high accuracy.Despite extensive training (9.6B parameter updates) and including AI2D in its pretraining, Qwen-VL-Chat's performance remains at 57.7% while fine-tuning CROME with its CROME-Adapter significantly boosts accuracy on this dataset, achieving a 36.2%improvement over its zero-shot performance.Compared to InstructBLIP and BLIVA, which retrain their Q-Former and projection layers during fine-tuning, CROME achieves 10.3% and 6.1% higher accuracy, respectively, while training only 2.5% of their parameters.This demonstrates the remarkable efficiency of our adapter-based fine-tuning approach.</p>
<p>Ablation Studies</p>
<p>To gain a deeper understanding of CROME, we conduct ablation studies focusing on its key components.We use the closely related BLIVA model as our baseline for comparison.For evaluation, we employ a zero-shot MLLM benchmark (MM-Vet) and the SQA I dataset which highlights our efficient fine-tuning strategy.Table 5 summarizes two types of ablations performed at model, CROME-Adapter in particular, and data levels.</p>
<p>Architecture:</p>
<p>We first ablate the CROME-Adapter to highlight its effect in our model.This ablation essentially reduces the architecture to BLIVA [6] for which the results are shown in the first row of 5. On MM-Vet, BLIVA achieves the score of 30.4 while adding the CROME-Adapter with its gating mechanism obtains 32.6 score (ablation #3) using similar pretraining and instruction-tuning data, LLM, and vision encoder.On SQA I , CROME achieves 61.2% zero-shot accuracy while BLIVA reaches 57.3% under similar conditions.</p>
<p>Ablation #2 shows the effect of the gating mechanism in CROME-Adapter.For this ablation, we remove the gating layer of the adapter in the down projection unit which results in simplifying 1 to z(x) = SiLU(xW d ).We repeat pretraining and instruction-tuning CROME with Dataset #2.Notably, the performance on MM-Vet and SQA I downgrade by 1.7 and 3, respectively, which shows the effect of the component-wise product of the two linear layers in the down-project unit.</p>
<p>Data and training:</p>
<p>Table 5, row #4, shows the effect of pretraining data in CROME which is not significant compared to other components (1.3 score difference on MM-Vet and 0.3% improvement on SQA I ).</p>
<p>Table 5, row #5, demonstrates the significant impact of expanding the instruction-tuning dataset from 1.2M to 8M samples.This leads to a 16.9 point improvement in CROME's zero-shot performance on MM-Vet and a 3.9% increase on SQA I .</p>
<p>Ablation #6 isolates the effects of the CROME-Adapter and additional instruction-tuning data, yielding 84.4% accuracy, 8.8% lower compared to the full CROME approach (row #7).This highlights the importance of the cross-modal gated adapter, supervised fine-tuning with it, and the better pretrained checkpoint trained on larger-scale instruction-tuning data.</p>
<p>Qualitative Results</p>
<p>In addition to 1 we include additional case examples from a variety of tasks performed by CROME and other models in the Appendix.</p>
<p>Limitations</p>
<p>CROME is designed to follow instructions and answer questions about images, outputting textual responses.Similar to other autoregressive LLMs, there is a potential for occasional inaccuracies or inconsistencies in its reasoning.While we prioritize the use of publicly available, curated data during training, it is important to acknowledge the potential for biases within these datasets that might influence CROME's responses.Aside from these considerations, we are optimistic that CROME will contribute to streamlined training and adaptations of MLLMs.Addressing potential challenges related to hallucination mitigation and improved grounding in MLLMs remain important future work directions.</p>
<p>Conclusion</p>
<p>In this paper, we introduce CROME, a novel vision-language instruction tuning framework designed for parameter-efficient multimodal learning and task-specific adaptation.CROME features a lightweight, gated cross-modal adapter that fuses visual and textual representations before they are input into the LLM, which is proposed to kept frozen.This design promotes efficient crossmodal understanding while minimizing computational costs by avoiding extensive LLM retraining.</p>
<p>Additionally, we highlight the potential for parameter-efficient fine-tuning of CROME: by training only the adapter's small number of parameters (O(M)-O(10M)), we demonstrate outperforming the state-of-the-art approaches on two downstream tasks.Furthermore, CROME achieves superior zero-shot performance on commonly-used MLLM benchmarks.We leave extending CROME to incorporate other modalities (e.g.audio and video), investigating broader task-specific adaptation, and further optimizing CROME's cross-modal adapter architecture to future work.</p>
<p>92.3</p>
<p>From MM-Vet Dataset Q: Is this apple organic?CROME-7B: Yes, the apple in the image is organic.The label on the apple indicates that it is an organic apple.Organic apples are grown without the use of synthetic pesticides, fertilizers, or genetically modified organisms (GMOs).Instead, they are grown using natural methods that promote healthy soil, biodiversity, and sustainable farming practices.Organic apples are often considered healthier and more nutritious than conventionally grown apples due to their lower levels of pesticide residues and higher levels of antioxidants.</p>
<p>Ground truth: yes.</p>
<p>Figure 2 :
2
Figure 2: Overview of our CROME model architecture with CROME-Adapter.CROME takes both image and text as input to generate output text autoregressively.The text input query is encoded by the Q-Former, which utilizes learnable queries to effectively represent instruction-aware visual features.These are then processed by a projection layer.The image input is encoded by a vision encoder and its patch embeddings are used in both the Q-Former's cross-attention layers and a projection layer.Within the cross-modal adapter, the projected image and text features undergo individual down-projection using a gated linear unit ([see 3]).They are then up-projected through a weight sharing linear layer.Finally, the cross-modal adapter outputs for text and image are concatenated with the tokenized question and fed into the LLM to obtain the text output.</p>
<p>Figure 3 :
3
Figure 3: Overview of CROME training stages.Blue indicates frozen components, and red indicates trainable components.(a) Pretraining: CROME-Adapter and projection layers are trained on imagecaption pairs.(b) Instruction-tuning: Q-Former, CROME-Adapter, and projection layers are trained on diverse image-instruction datasets.(c) Task-specific fine-tuning: CROME-Adapter facilitates efficient training on task-specific data.</p>
<p>Figure 4 :
4
Figure 4: Qualitative examples from various zero-shot MLLM benchmarks we have evaluated CROME on.</p>
<p>Table 1 :
1
Zero-shot performance of different MLLMs on multimodal benchmarks.Input image resolutions, pretraining (PT) and instruction tuning (IT) dataset sizes are also shown.The best results among open-source modals are bold and the second best results are underlined.MME C MMB MM-Vet HaLLB LLaVA W SEED I
Method MMMU MME P GPT4-V LLM Res. PT IT Unk Unk Unk Unk 56.8 1771.577.067.765.893.169.1Gemini Pro Vision UnkUnkUnkUnk47.91626.9531.173.664.363.979.970.7OwenVL-PlusUnkUnkUnkUnk46.52229.867.055.756.473.372.7BLIP-2Vicuna-13B224 129M-35.71293.8290.0-22.4-38.146.4BLIVAVicuna-7B224 558K 1.2M27.3813.4224.660.130.433.980.159.4BLIVAFlan-T5 XXL 224 558K 1.2M-1337.7331.4362.230.3---InstructBLIPVicuna-7B224 129M 1.2M30.6--33.926.253.660.953.4InstructBLIPVicuna-13B224 129M 1.2M32.1---25.6-58.2-IDEFICS-80BLLaMA-65B224 353M1M24.0--54.539.746.1--Qwen-VL-ChatQwen-7B4481.4B50M35.91487.6360.761.847.356.4-65.4LLaVA-1.5Vicuna-7B336 558K 665K---64.330.5-63.458.6LLaVA-1.5Vicuna-13B336 558K 665K36.4153129567.835.446.770.761.6LLaVA-1.6Vicuna-7B336 558K 760K35.8151933267.443.9-81.670.2LLaVA-1.6Vicuna-13B336 558K 760K36.2157532670.048.4-87.371.9CROME (ours)Vicuna-7B224 558K8M38.81590.6358.467.147.348.379.268.2CROME (ours)Vicuna-13B224 558K8M41.21643.2369.571.255.149.286.872.5CROME (ours)Flan-T5 XXL 224 558K8M38.11521.3361.465.545.251.376.473.5
5.1 Zero-shot Performance on Vision-Language TasksTable1shows comparisons on zero-shot performance on the 8 commonly-used MLLM benchmarks across various models, including the open-sourced ones and those who are accessible only via prediction APIs.Among the open-sourced ones, CROME outperforms all baselines by a large margin on MMMU, MME P , MME C , MMBench, MM-Vet, and SEED I .Considering the ones accessible</p>
<p>Table 2 :
2
Comparison between CROME models trained on different pretraining (PT) and instruction tuning (IT) dataset sizes and corresponding baselines with similar LLM backbones.The total number of trainable parameters and the input image resolutions are also shown.The best results in each LLM family of models are bold.MME C MMB MM-Vet HaLLB LLaVA W SEED I
Method MMMU MME P InstructBLIP LLM #params Res. PT IT Flan-T5 XXL 188M 224 129M 1.2M 35.7 1212.8291.8-25.6-58.252.7BLIVAFlan-T5 XXL 194.61M 224 558K 1.2M-1337.7331.462.230.3---CROME(ours) Flan-T5 XXL 199.85M 224 558K 1.2M33.11380.6329.263.134.740.170.262.7CROME (ours) Flan-T5 XXL 199.85M 224 558K8M38.11521.3361.465.545.251.376.473.5InstructBLIPVicuna-7B188M224 129M 1.2M30.6--33.926.253.660.953.4BLIVAVicuna-7B194.61M 224 558K 1.2M27.3813.4224.660.130.433.980.159.4LLaVA-1.6Vicuna-7B7B336 558K 760K35.8151933267.443.9-81.670.2CROME(ours) Vicuna-7B199.85M 224. 558K 1.2M32.41170.4246.162.832.642.365.260.3CROME (ours) Vicuna-7B199.85M 224 558K8M38.81590.6358.467.147.348.379.268.2LLaVA-1.6Vicuna-13B13B336 558K 760K36.2157532670.048.4-87.371.9CROME (ours) Vicuna-13B203.39M 224 558K8M41.21643.2369.571.255.149.286.872.5</p>
<p>Table 3 :
3
ScienceQA-Image results: zero-shot vs. fine-tuned performance.
MethodsTrainable paramsAccuracy (%)Zero-Shot PerformanceLLaVA 1.6-7B7B70.1InstructBLIP-Vicuna-7B188M60.5BLIVA-Vicuna-7B194.61M57.3CROME-Vicuna-7B (ours)199.85M61.2Supervised Fine-tuningMutimodal-T-SciQLarge738M96.2MC-CoT-F-Large738M94.9LLaMA-Adapter1.8M85.2LaVIN-7B3.8M89.4LaVIN-13B5.4M90.8PILL-7B45 M91.2CROME-Vicuna-7B (ours) 5.24M93.2</p>
<p>Table 4 :
4
AI2D results: zero-shot vs. fine-tuned performance.Note that Qwen-VL-Chat includes AI2D in pretraining, while BLIVA, InstructBLIP and CROME do not.
MethodsTrainable paramsAccuracy (%)Zero-Shot PerformanceQwen-VL-Chat  *9.6B57.7InstructBLIP-Vicuna-7B188M36.1BLIVA-Vicuna-7B194.61M38.2CROME-Vicuna-7B (ours)199.85M39.1Supervised Fine-tuningInstructBLIP-Vicuna-7B188M65.0BLIVA-Vicuna-7B194.61M69.2CROME-Vicuna-7B (ours)5.24M75.3</p>
<p>Table 5 :
5
Ablation studies for CROME framework using CROME-Vicuna-7B on MM-Vet and ScienceQA-Image datasets</p>
<h1>BLIVA [6]CROME-AdapterGating MechanismPretrainingAdditional IT DataTask-specific IT StrategyMM-VetSQA I1✓✓30.4 (+0)57.3% (+0)2✓✓✓28.7 (-1.7)54.3% (-3.0)3✓✓✓✓32.6 (+2.2)58.2% (+0.9)4✓✓✓31.8 (+1.4)57.6% (+0.3)5✓✓✓✓✓47.3 (+16.9) 61.2% (+3.9)6✓✓✓-84.4% (+27.1)7✓✓✓✓✓✓-93.2% (+35.9)</h1>
<p>Table 7 :
7
Analysis on choosing m, the bottleneck dimension in CROME-Adapter using one dataset from zero-shot MLLM benchmarks as well as ScienceQA Image dataset.
mMM-Vet SQA-IMG6446.189.1128 46.891.9256 47.393.2512 46.
 LLaVA1.6  has not yet made their data publicly available
Appendix A Implementation DetailsA.1 Image pre-processingWe pre-process the images using random crops, resizing to 224 × 224 with an interpolation method of Bicubic, horizontal flips, converting them to tensor format, and normalizing them using mean = (0.48145466, 0.4578275, 0.40821073) and standard deviation = (0.26862954, 0.26130258, 0.27577711).During evaluation, we only used image resizing, converting to tensor format, and normalizing using the same mean and standard deviation values.A.2 Training details of task-specific fine-tuningOn both ScienceQA-Image and AI2D datasets, we employ a batch size of 16 and used the AdamW[54]optimizer, with β 1 = 0.9, β 2 = 0.999, and a weight decay of 0.05.Additionally, we apply a linear warmup of the learning rate during the initial 1K steps, increasing from 10 −8 to 10 −4 , followed by a cosine decay with a minimum learning rate of 0.B Training DatasetsTable6lists the datasets used in pretraining and instruction tuning of CROME.With the recent release of publicly available instruction tuning datasets such as ShareGPT4V, LAMM, and LVIS-Instruct4V, we were able to increase the dataset size to 8M image-text pairs.It should be noted that when multiple questions/instructions were available for the same image, we used up to 4 of them.Moreover, duplicated data across these datasets which had the same image and text were filtered during our cleaning process.LLaVA-Instruct 150K, VQAv2, OKVQA, A-OKVQA, OCR-VQA, MSCOCO, LVIS-Instruct4V, LAMM, ShareGPT4V, Flickr30KB.1 Analysis on CROME-Adapter bottleneck dimensionTable7shows how we chose the m value for the hidden dimension of our CROME-Adapter.We only did this experiment on the CROME-Vicuna7B model and used the chose m value for other variants of CROME-Vicuna13B and CROME-Flan T5 XXL.We used two datasets, one from the zero-shot MLLM benchmarks (MM-Vet) as well as one of the datasets we used for supervised finetuning adaptation (ScienceQA-Image).It is common to use a significantly smaller dimension for the bottleneck with respect to the input size (m &lt;&lt; d(4096)).We selected 4 values (64, 128, 256, 512) and pre-trained and instruction-tuned four models with them.Results on both datasets confirmed the choice of 256 for our model.C Qualitative ResultsFigure4shows some qualitative results from CROME on various test samples.
Gpt-4v(ision) system card. 2023</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, NeurIPS. 2023</p>
<p>InstructBLIP: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, 2023</p>
<p>Bliva: A simple multimodal llm for better handling of text-rich visual questions. Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu, AAAI. 2024</p>
<p>Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Obelics: An open web-scale filtered dataset of interleaved image-text documents. Lucile Hugo Laurençon, Léo Saulnier, Stas Tronchon, Amanpreet Bekman, Anton Singh, Thomas Lozhkov, Siddharth Wang, Alexander Karamcheti, Douwe Rush, Kiela, Advances in Neural Information Processing Systems. 202436</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, arXiv:2302.140452023arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Cheap and quick: Efficient vision-language instruction tuning for large language models. Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong Ji, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Shikra: Unleashing multimodal llm's referential dialogue magic. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, arXiv:2306.151952023arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, International conference on machine learning. PMLR2021</p>
<p>Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu, arXiv:2205.01917Coca: Contrastive captioners are image-text foundation models. 2022arXiv preprint</p>
<p>Xi Chen, Xiao Wang, Soravit Changpinyo, Piotr Piergiovanni, Daniel Padlewski, Sebastian Salz, Adam Goodman, Basil Grycner, Lucas Mustafa, Beyer, arXiv:2209.06794A jointly-scaled multilingual language-image model. 2022arXiv preprint</p>
<p>Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. PMLR2022</p>
<p>Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, International Conference on Machine Learning. PMLR2022</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Parameter-efficient transfer learning for nlp. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International Conference on Machine Learning. PMLR2019</p>
<p>Towards a unified view of parameter-efficient transfer learning. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig, ArXiv, abs/2110.043662021</p>
<p>Adapterfusion: Non-destructive task composition for transfer learning. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych, ArXiv, abs/2005.002472020</p>
<p>Learning multiple visual domains with residual adapters. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, ArXiv, abs/1705.080452017</p>
<p>Efficient parametrization of multidomain deep neural networks. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2018. 2018</p>
<p>Adaptformer: Adapting vision transformers for scalable visual recognition. Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, Ping Luo, ArXiv, abs/2205.135352022</p>
<p>Towards efficient visual adaption via structural re-parameterization. Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guangnan Jiang, Zhiyu Wang, Rongrong Ji, arXiv:2302.081062023arXiv preprint</p>
<p>Vl-pet: Vision-and-language parameter-efficient tuning via granularity control. Zi-Yuan Hu, Yanyang Li, Michael R Lyu, Liwei Wang, IEEE/CVF International Conference on Computer Vision (ICCV). 2023. 2023</p>
<p>Conditional adapters: Parameter-efficient transfer learning with fast inference. Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Zhao, Yuexin Wu, Bo Li, Yu Zhang, Ming-Wei Chang, ArXiv, abs/2304.049472023</p>
<p>Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. Yi-Lin Sung, Jaemin Cho, Mohit Bansal, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2021</p>
<p>Multimodal-gpt: A vision and language model for dialogue with humans. Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen, 2023</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Pill: Plug into llm with adapter expert and attention gate. Fangyuan Zhang, Tingting Liang, Zhengyuan Wu, Yuyu Yin, arXiv:2311.021262023arXiv preprint</p>
<p>Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao, arXiv:2210.17451Adamix: Mixture-of-adaptations for parameter-efficient model tuning. 2022arXiv preprint</p>
<p>Parameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, PMLR, 09-15Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri, Ruslan Salakhutdinov, the 36th International Conference on Machine LearningJun 201997</p>
<p>Noam Shazeer, arXiv:2002.05202Glu variants improve transformer. 2020arXiv preprint</p>
<p>Language modeling with gated convolutional networks. Angela Yann N Dauphin, Michael Fan, David Auli, Grangier, International conference on machine learning. PMLR2017</p>
<p>Dan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Stefan Elfwing, Eiji Uchibe, Kenji Doya, Neural networks. 1072018</p>
<p>Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang, arXiv:2311.07574To see is to believe: Prompting gpt-4v for better visual instruction tuning. 2023arXiv preprint</p>
<p>Language-assisted multi-modal instructiontuning dataset, framework, and benchmark. Jiong Zhenfei Yin, Jianjian Wang, Zhelun Cao, Dingning Shi, Mukai Liu, Xiaoshui Li, Zhiyong Huang, Lu Wang, Lei Sheng, Bai, Advances in Neural Information Processing Systems. 202436</p>
<p>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, Transactions of the Association for Computational Linguistics. 22014</p>
<p>Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin, arXiv:2311.12793Sharegpt4v: Improving large multi-modal models with better captions. 2023arXiv preprint</p>
<p>Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, Yue Cao, arXiv:2303.15389Eva-clip: Improved training techniques for clip at scale. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2311.165022023arXiv preprint</p>
<p>Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, arXiv:2306.13394A comprehensive evaluation benchmark for multimodal large language models. 2023arXiv preprint</p>
<p>Mmbench: Is your multi-modal model an all. Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai , Chen Dahua, Lin Yuan Liu, Haodong Duan, arXiv:2307.062812023</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023arXiv preprint</p>
<p>Hallusionbench: An advanced diagnostic suite for entangled language hallucination &amp; visual illusion in large vision-language models. Fuxiao Tianrui Guan, Xiyang Liu, Ruiqi Wu, Zongxia Xian, Xiaoyu Li, Xijun Liu, Lichang Wang, Furong Chen, Yaser Huang, Yacoob, arXiv:2310.145662023arXiv preprint</p>
<p>Seedbench: Benchmarking multimodal llms with generative comprehension. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, arXiv:2307.161252023arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>A diagram is worth a dozen images. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part IV 14</p>
<p>Boosting the power of small multimodal reasoning models to match larger models with self-consistency training. Cheng Tan, Jingxuan Wei, Zhangyang Gao, Linzhuang Sun, Siyuan Li, Xihong Yang, Stan Z Li, arXiv:2311.141092023arXiv preprint</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, arXiv:2302.009232023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
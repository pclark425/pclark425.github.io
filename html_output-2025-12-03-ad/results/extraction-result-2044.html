<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2044 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2044</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2044</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-281194812</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.04731v2.pdf" target="_blank">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2044.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2044.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-generated dynamic scaffold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-generated Dynamic Task Scaffolding / Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed paradigm in this paper where an LLM functions as a generative world model that decomposes high-level natural language goals into ordered symbolic subgoals and dynamically configures task hierarchies, intrinsic rewards, and success conditions for episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>An LLM takes a high-level natural-language goal (e.g., 'Execute a give-and-go play with the forward on the right wing') and decomposes it into a sequenced list of symbolic subgoals (e.g., dribble-to-player, pass-to-player, run-into-space, return-pass). The environment's Hierarchical Task API ingests this sequence to set intrinsic rewards and termination conditions for subgoals, creating a dynamic, language-driven curriculum for that episode. The paper further proposes the LLM could re-plan mid-episode using new state information, enabling adaptive scaffolding, and that prompts can generate new curricula without hand-authoring domain description files.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>robotic soccer (3v3, team-play)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Multi-agent, adversarial, long-horizon, compositional task structure (passes, possession, positioning), combinatorial joint action space, sparse terminal rewards (scoring), strategic coordination required; the domain emphasizes task sparsity over reward sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Paper explicitly proposes that the LLM planner could re-plan mid-episode based on new state information (i.e., dynamic adaptation to unfolding state), conditioning on high-level goal and episode state information.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>Paper suggests using stochastic planners to introduce structured variation and fading curricula (starting rigid then relaxing constraints) as mechanisms to avoid over-constraining discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Hierarchical Task API, intrinsic-subgoal reward issuing, meta-controller (selects options), symbolic options / skill library, execution monitoring (implied), possible neuro-symbolic perception-grounding modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Logical inconsistency and physical infeasibility of LLM-generated plans, symbol grounding gap (mapping symbolic subgoals to continuous state/action targets), risk of over-constraining agent discovery (stifling novel strategies), potential designer bias moved into prompts, and need to verify LLM plans for safety/feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Positioned (qualitatively) to improve long-horizon planning by providing decomposed subgoals and dense intrinsic rewards aligned with task composition; no empirical numbers reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Not empirically evaluated; paper argues LLM-driven scaffolding is flexible for domain-specific strategies but raises concerns about logical/physical feasibility in specialized tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Paper hypothesizes LLM-generated curricula can automate and scale hierarchical scaffolding, reducing engineering burden and improving sample efficiency relative to hand-crafted curricula; however, no quantitative results are provided and key limitations include plan validity, grounding, and potential constraint on open-ended discovery.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2044.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2044.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based embodied agent (cited by the paper) that demonstrated open-ended exploration and self-directed skill discovery by having the LLM propose progressively harder tasks, effectively generating its own curriculum in a complex environment (Minecraft).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>According to the citation in the paper, Voyager uses an LLM to propose progressively harder tasks and self-directed skill discovery; the paper cites Voyager as an example of an LLM generating curricula (in Minecraft) rather than as a soccer experiment. The position paper references Voyager to support the feasibility of LLM-generated curricula as a concept.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Minecraft (open-ended embodied environment) — cited as example</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Open-ended, highly compositional, supports progressive skill acquisition and emergent tasks; long-horizon possibilities and diverse task types; not the primary domain of this paper but used as an LLM-curriculum example.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>LLM-based planner + environment-specific executors (as per original Voyager work) — paper only cites, does not detail components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Not discussed in this paper beyond using Voyager as a motivating example; original Voyager paper contains empirical detail but is not summarized with numbers here.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Cited as demonstrating open-ended, progressively harder task generation, implying improved long-horizon skill discovery in its domain; no soccer-specific results reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Used as supporting evidence that LLMs can generate curricula and drive self-directed skill discovery in open-ended environments; this paper leverages Voyager's qualitative claim but provides no quantitative cross-domain comparison.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2044.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2044.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do as I Can, Not as I Say: Grounding Language in Robotic Affordances (SayCan)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that uses an LLM to decompose high-level natural-language instructions into executable low-level actions for robotics by grounding language in affordances; cited as related evidence that LLMs can translate goals into action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Cited for its demonstration that an LLM can decompose instructions into sequences of executable low-level actions grounded in affordances; the paper references SayCan to support the claim that LLMs can act as planners that translate high-level goals into concrete steps, which could be repurposed to generate task scaffolds or curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Robotic manipulation (grounding language to robot affordances) — cited as example</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Embodied robotics, requires grounding of symbolic/descriptive goals to low-level motor/affordance primitives; more constrained than open-ended domains but demonstrates language-to-action decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Affordance models and low-level action executors (in original work); this paper cites SayCan as conceptual precedent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Paper cites SayCan as evidence of capability but does not discuss SayCan's limitations here.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Used as supporting evidence that LLMs can decompose instructions to actionable sequences, implying feasibility for generating hierarchical subgoals and curricula when combined with environment scaffolding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2044.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2044.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HS-MARL (pyHIPOP+ HTN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning (HS-MARL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external framework (Mu et al., 2024) integrating an HTN symbolic planner (pyHIPOP+) into MARL: the planner decomposes goals into subgoals, and a meta-controller assigns symbolic options (sub-policies) to agents, creating an explicit hierarchical scaffold and intrinsic curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>manual/hand-designed (symbolic HTN-generated)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>An HDDL-based HTN planner (pyHIPOP+) generates a hierarchical state-space tree decomposing high-level goals (e.g., ScoreGoal) into sequenced subgoals (AcquireBall, MoveToShootingPosition, ExecuteShot). A meta-controller uses this symbolic plan to assign options (policies trained for specific subgoals) to agents. The HTN-generated decomposition naturally orders learning from leaf subgoals upward, serving as an intrinsic curriculum embedded in the task structure.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>robotic soccer (multi-agent team play) — explicit evaluated domain in Mu et al. and cited here</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Long-horizon, compositional tasks with temporally extended dependencies; symbolic description possible via HDDL/HDDL-like domain files.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Meta-controller for option selection, symbolic HDDL domain descriptions, option/subpolicy library, intrinsic/subgoal rewards provided by the task decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Relies on hand-crafted HDDL domain files (engineering bottleneck) and domain-specific symbolic modeling; potential designer bias and scalability limits for many possible strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Cited in this paper as demonstrating significantly improved performance and sample efficiency relative to purely end-to-end MARL for complex, sparse-reward soccer tasks, but no numeric metrics are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Symbolic HTN decomposition provides explicit hierarchical scaffolding and an emergent curriculum that improves sample efficiency and interpretability; limitation is manual domain specification which the paper argues LLMs could mitigate.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2044.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2044.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TiZero (Baghi 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modified TiZero framework with manual curriculum (Baghi, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MARL-based soccer framework that relies on a manually designed, staged curriculum (e.g., 1v1 → 2v2 → full team play) and intrinsic rewards to progressively train agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>manual/hand-designed</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>A multi-stage, explicitly hand-crafted curriculum where agents must meet performance thresholds in simpler scenarios (1v1) before progressing to more complex team scenarios (2v2, etc.). Intrinsic rewards and staged scenario complexity form the curriculum progression.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>football-like environments / robotic soccer (game-AI)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Staged, benchmark-driven competitions; designed curricula target gradually increasing tactical complexity; tasks are compositional but curricula are hand-coded.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Intrinsic reward shaping, staged scenario gating (performance thresholds), possibly self-play elements depending on implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Labor-intensive to design and hard to scale; may embed designer biases; static curricula may not adapt to agent learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Used in practice to bootstrap multi-agent skills but identified in the paper as labor-intensive and less flexible than hierarchical/autogenerated scaffolds.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Manual staged curricula are effective but require significant engineering effort; the paper positions hierarchical scaffolding and LLM generation as a path to automate and scale such curricula.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2044.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2044.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eligibility-trace staged curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eligibility Traces + multi-stage training (Azarkasb & Khasteh, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that builds a knowledge base of successful trajectories using eligibility traces and applies a multi-stage training curriculum: agents first learn basic navigation/path skills in obstacle-free settings, then obstacles/opponents are introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>manual/heuristic-based (multi-stage training using stored traces)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Training is multi-staged: foundational skills (optimal paths to ball/goal) are learned in simplified settings and stored as eligibility traces (a learned library of successful sub-plans). Complexity (opponents/obstacles) is introduced only after foundational mastery, creating an implicit curriculum derived from layered task complexity and trace replay.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>robotic soccer / navigation with obstacles</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Long-horizon subtask composition (navigation, obstacle avoidance, ball handling); staged complexity; tasks are compositional and can be layered incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Eligibility trace library of successful trajectories (implicit skill library), multi-stage gating of environment complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Simple staged curricula may not scale to highly combinatorial strategic tasks; implicit traces may not cover all required compositional behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Demonstrates effectiveness for building foundational navigation and avoidance skills prior to full multi-agent complexity; no quantitative long-horizon comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Layered introduction of complexity with stored successful trajectories can form an effective curriculum for foundational skills, but scaling to full strategic multi-agent play requires richer hierarchical scaffolding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies <em>(Rating: 2)</em></li>
                <li>Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments <em>(Rating: 1)</em></li>
                <li>MARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2044",
    "paper_id": "paper-281194812",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "LLM-generated dynamic scaffold",
            "name_full": "Large Language Model-generated Dynamic Task Scaffolding / Curriculum",
            "brief_description": "A proposed paradigm in this paper where an LLM functions as a generative world model that decomposes high-level natural language goals into ordered symbolic subgoals and dynamically configures task hierarchies, intrinsic rewards, and success conditions for episodes.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "An LLM takes a high-level natural-language goal (e.g., 'Execute a give-and-go play with the forward on the right wing') and decomposes it into a sequenced list of symbolic subgoals (e.g., dribble-to-player, pass-to-player, run-into-space, return-pass). The environment's Hierarchical Task API ingests this sequence to set intrinsic rewards and termination conditions for subgoals, creating a dynamic, language-driven curriculum for that episode. The paper further proposes the LLM could re-plan mid-episode using new state information, enabling adaptive scaffolding, and that prompts can generate new curricula without hand-authoring domain description files.",
            "domain_name": "robotic soccer (3v3, team-play)",
            "domain_characteristics": "Multi-agent, adversarial, long-horizon, compositional task structure (passes, possession, positioning), combinatorial joint action space, sparse terminal rewards (scoring), strategic coordination required; the domain emphasizes task sparsity over reward sparsity.",
            "state_conditioning": true,
            "state_conditioning_details": "Paper explicitly proposes that the LLM planner could re-plan mid-episode based on new state information (i.e., dynamic adaptation to unfolding state), conditioning on high-level goal and episode state information.",
            "novelty_mechanism": true,
            "novelty_mechanism_details": "Paper suggests using stochastic planners to introduce structured variation and fading curricula (starting rigid then relaxing constraints) as mechanisms to avoid over-constraining discovery.",
            "complementary_systems": "Hierarchical Task API, intrinsic-subgoal reward issuing, meta-controller (selects options), symbolic options / skill library, execution monitoring (implied), possible neuro-symbolic perception-grounding modules.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": false,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Logical inconsistency and physical infeasibility of LLM-generated plans, symbol grounding gap (mapping symbolic subgoals to continuous state/action targets), risk of over-constraining agent discovery (stifling novel strategies), potential designer bias moved into prompts, and need to verify LLM plans for safety/feasibility.",
            "long_horizon_performance": "Positioned (qualitatively) to improve long-horizon planning by providing decomposed subgoals and dense intrinsic rewards aligned with task composition; no empirical numbers reported in this paper.",
            "specialized_domain_performance": "Not empirically evaluated; paper argues LLM-driven scaffolding is flexible for domain-specific strategies but raises concerns about logical/physical feasibility in specialized tasks.",
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Paper hypothesizes LLM-generated curricula can automate and scale hierarchical scaffolding, reducing engineering burden and improving sample efficiency relative to hand-crafted curricula; however, no quantitative results are provided and key limitations include plan validity, grounding, and potential constraint on open-ended discovery.",
            "uuid": "e2044.0"
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An LLM-based embodied agent (cited by the paper) that demonstrated open-ended exploration and self-directed skill discovery by having the LLM propose progressively harder tasks, effectively generating its own curriculum in a complex environment (Minecraft).",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "According to the citation in the paper, Voyager uses an LLM to propose progressively harder tasks and self-directed skill discovery; the paper cites Voyager as an example of an LLM generating curricula (in Minecraft) rather than as a soccer experiment. The position paper references Voyager to support the feasibility of LLM-generated curricula as a concept.",
            "domain_name": "Minecraft (open-ended embodied environment) — cited as example",
            "domain_characteristics": "Open-ended, highly compositional, supports progressive skill acquisition and emergent tasks; long-horizon possibilities and diverse task types; not the primary domain of this paper but used as an LLM-curriculum example.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "LLM-based planner + environment-specific executors (as per original Voyager work) — paper only cites, does not detail components.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": false,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Not discussed in this paper beyond using Voyager as a motivating example; original Voyager paper contains empirical detail but is not summarized with numbers here.",
            "long_horizon_performance": "Cited as demonstrating open-ended, progressively harder task generation, implying improved long-horizon skill discovery in its domain; no soccer-specific results reported here.",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Used as supporting evidence that LLMs can generate curricula and drive self-directed skill discovery in open-ended environments; this paper leverages Voyager's qualitative claim but provides no quantitative cross-domain comparison.",
            "uuid": "e2044.1"
        },
        {
            "name_short": "SayCan",
            "name_full": "Do as I Can, Not as I Say: Grounding Language in Robotic Affordances (SayCan)",
            "brief_description": "A referenced work that uses an LLM to decompose high-level natural-language instructions into executable low-level actions for robotics by grounding language in affordances; cited as related evidence that LLMs can translate goals into action sequences.",
            "citation_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Cited for its demonstration that an LLM can decompose instructions into sequences of executable low-level actions grounded in affordances; the paper references SayCan to support the claim that LLMs can act as planners that translate high-level goals into concrete steps, which could be repurposed to generate task scaffolds or curricula.",
            "domain_name": "Robotic manipulation (grounding language to robot affordances) — cited as example",
            "domain_characteristics": "Embodied robotics, requires grounding of symbolic/descriptive goals to low-level motor/affordance primitives; more constrained than open-ended domains but demonstrates language-to-action decomposition.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Affordance models and low-level action executors (in original work); this paper cites SayCan as conceptual precedent.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": false,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Paper cites SayCan as evidence of capability but does not discuss SayCan's limitations here.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Used as supporting evidence that LLMs can decompose instructions to actionable sequences, implying feasibility for generating hierarchical subgoals and curricula when combined with environment scaffolding.",
            "uuid": "e2044.2"
        },
        {
            "name_short": "HS-MARL (pyHIPOP+ HTN)",
            "name_full": "Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning (HS-MARL)",
            "brief_description": "An external framework (Mu et al., 2024) integrating an HTN symbolic planner (pyHIPOP+) into MARL: the planner decomposes goals into subgoals, and a meta-controller assigns symbolic options (sub-policies) to agents, creating an explicit hierarchical scaffold and intrinsic curriculum.",
            "citation_title": "Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies",
            "mention_or_use": "mention",
            "curriculum_generator_type": "manual/hand-designed (symbolic HTN-generated)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "An HDDL-based HTN planner (pyHIPOP+) generates a hierarchical state-space tree decomposing high-level goals (e.g., ScoreGoal) into sequenced subgoals (AcquireBall, MoveToShootingPosition, ExecuteShot). A meta-controller uses this symbolic plan to assign options (policies trained for specific subgoals) to agents. The HTN-generated decomposition naturally orders learning from leaf subgoals upward, serving as an intrinsic curriculum embedded in the task structure.",
            "domain_name": "robotic soccer (multi-agent team play) — explicit evaluated domain in Mu et al. and cited here",
            "domain_characteristics": "Long-horizon, compositional tasks with temporally extended dependencies; symbolic description possible via HDDL/HDDL-like domain files.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Meta-controller for option selection, symbolic HDDL domain descriptions, option/subpolicy library, intrinsic/subgoal rewards provided by the task decomposition.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": false,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Relies on hand-crafted HDDL domain files (engineering bottleneck) and domain-specific symbolic modeling; potential designer bias and scalability limits for many possible strategies.",
            "long_horizon_performance": "Cited in this paper as demonstrating significantly improved performance and sample efficiency relative to purely end-to-end MARL for complex, sparse-reward soccer tasks, but no numeric metrics are provided here.",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Symbolic HTN decomposition provides explicit hierarchical scaffolding and an emergent curriculum that improves sample efficiency and interpretability; limitation is manual domain specification which the paper argues LLMs could mitigate.",
            "uuid": "e2044.3"
        },
        {
            "name_short": "TiZero (Baghi 2024)",
            "name_full": "Modified TiZero framework with manual curriculum (Baghi, 2024)",
            "brief_description": "A MARL-based soccer framework that relies on a manually designed, staged curriculum (e.g., 1v1 → 2v2 → full team play) and intrinsic rewards to progressively train agents.",
            "citation_title": "Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments",
            "mention_or_use": "mention",
            "curriculum_generator_type": "manual/hand-designed",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "A multi-stage, explicitly hand-crafted curriculum where agents must meet performance thresholds in simpler scenarios (1v1) before progressing to more complex team scenarios (2v2, etc.). Intrinsic rewards and staged scenario complexity form the curriculum progression.",
            "domain_name": "football-like environments / robotic soccer (game-AI)",
            "domain_characteristics": "Staged, benchmark-driven competitions; designed curricula target gradually increasing tactical complexity; tasks are compositional but curricula are hand-coded.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Intrinsic reward shaping, staged scenario gating (performance thresholds), possibly self-play elements depending on implementation.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": false,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Labor-intensive to design and hard to scale; may embed designer biases; static curricula may not adapt to agent learning dynamics.",
            "long_horizon_performance": "Used in practice to bootstrap multi-agent skills but identified in the paper as labor-intensive and less flexible than hierarchical/autogenerated scaffolds.",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Manual staged curricula are effective but require significant engineering effort; the paper positions hierarchical scaffolding and LLM generation as a path to automate and scale such curricula.",
            "uuid": "e2044.4"
        },
        {
            "name_short": "Eligibility-trace staged curriculum",
            "name_full": "Eligibility Traces + multi-stage training (Azarkasb & Khasteh, 2024)",
            "brief_description": "A method that builds a knowledge base of successful trajectories using eligibility traces and applies a multi-stage training curriculum: agents first learn basic navigation/path skills in obstacle-free settings, then obstacles/opponents are introduced.",
            "citation_title": "Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy.",
            "mention_or_use": "mention",
            "curriculum_generator_type": "manual/heuristic-based (multi-stage training using stored traces)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Training is multi-staged: foundational skills (optimal paths to ball/goal) are learned in simplified settings and stored as eligibility traces (a learned library of successful sub-plans). Complexity (opponents/obstacles) is introduced only after foundational mastery, creating an implicit curriculum derived from layered task complexity and trace replay.",
            "domain_name": "robotic soccer / navigation with obstacles",
            "domain_characteristics": "Long-horizon subtask composition (navigation, obstacle avoidance, ball handling); staged complexity; tasks are compositional and can be layered incrementally.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Eligibility trace library of successful trajectories (implicit skill library), multi-stage gating of environment complexity.",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": false,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Simple staged curricula may not scale to highly combinatorial strategic tasks; implicit traces may not cover all required compositional behaviors.",
            "long_horizon_performance": "Demonstrates effectiveness for building foundational navigation and avoidance skills prior to full multi-agent complexity; no quantitative long-horizon comparison provided in this paper.",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Layered introduction of complexity with stored successful trajectories can form an effective curriculum for foundational skills, but scaling to full strategic multi-agent play requires richer hierarchical scaffolding.",
            "uuid": "e2044.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies",
            "rating": 2
        },
        {
            "paper_title": "Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments",
            "rating": 1
        },
        {
            "paper_title": "MARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning.",
            "rating": 1
        }
    ],
    "cost": 0.014373999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer
29 Sep 2025</p>
<p>Brennen Hill bahill4@wisc.edu 
Department of Computer Science
University of Wisconsin-Madison Madison
53706WI</p>
<p>Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer
29 Sep 2025DF10DAD44701DEB27702DE217B7EC693arXiv:2509.04731v2[cs.AI]
Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models.However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-toend approach often fails due to intractable exploration spaces and sparse rewards.This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding.We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics.Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL).These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning.We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play.We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks.By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.Introduction: redefining scale for embodied agentsThe paradigm of scaling has demonstrably driven profound advancements in artificial intelligence.In the domain of large language models (LLMs), scaling model parameters, dataset size, and computational resources has unlocked emergent capabilities in reasoning, generation, and comprehension[Brown et al., 2020].A parallel trend is now underway in the development of intelligent agents, where the focus is shifting towards scaling interaction.Environments are not passive testbeds, but are themselves a crucial dimension of scale.They provide the rich, dynamic data from which agents learn adaptive behaviors, planning, and long-term decision-making.To advance world models beyond passive prediction and toward active, goal-driven interaction, hinges on the quality and structure of this interaction data.However, as we push agents towards more complex, autonomous, and general-purpose capabilities, a critical bottleneck emerges, particularly in multi-agent systems, which consistently face challenges of non-stationarity and decentralized decision-making[Ning and Xie, 2024].In domains characterizedIn the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Embodied World Models for Decision Making</p>
<p>by long horizons, combinatorial action spaces, and the need for intricate coordination, such as robotic soccer, the brute-force scaling of interaction data via end-to-end reinforcement learning often proves insufficient.Robotic soccer, as a canonical case study for embodied intelligence, encapsulates these challenges, forcing researchers to develop robust solutions for real-time, strategic cooperation under uncertainty [Sammut, 2010].The challenge is not merely one of reward sparsity, but of task sparsity: the sequence of coordinated actions required to achieve a meaningful outcome (e.g., a goal) is so specific and temporally extended that it is almost impossible to discover through random exploration.An agent will not stumble upon a multi-step passing play by chance, regardless of the scale of the simulation.While we ground our analysis in robotic soccer, this challenge of task sparsity is endemic to many complex domains, from collaborative robotic assembly and supply chain logistics to automated scientific discovery and autonomous driving.This position paper argues for a paradigm shift in how we approach the design of world models and their corresponding environments.We contend that the next crucial frontier is not the scaling of fidelity, size, or agent populations, but the scaling of structural complexity within the environment itself.An embodied world model must capture more than just physical dynamics; it must represent the semantic structure of tasks.We propose that by embedding explicit hierarchical scaffolding into the design of our environments, we can create a structured learning landscape that guides agents toward sophisticated, strategic behavior far more efficiently than flat simulation environments.This scaffolding involves decomposing overarching tasks into a hierarchy of sub-tasks and subgoals, creating an intrinsic curriculum that makes the learning problem tractable.</p>
<p>To substantiate this position, we conduct a focused, systematic review of the 2024 literature on multi-agent robotic soccer, a canonical challenge problem for multi-agent coordination.Our analysis reveals a clear and compelling trend: the most promising recent approaches are moving away from purely end-to-end MARL and towards hybrid models that integrate symbolic planners and hierarchical reasoning.This paper will demonstrate how these methods implicitly or explicitly leverage task decomposition to overcome the limitations of traditional MARL.We will argue that these successes are not isolated algorithmic tricks but rather evidence of a fundamental principle: to learn complex strategies, agents require world models and environments that are structured to teach them.</p>
<p>Related work</p>
<p>Our position builds upon and seeks to synthesize three established pillars of AI research: Hierarchical Reinforcement Learning (HRL), Curriculum Learning, and the emerging use of LLMs as agents and planners.Our contribution is to advocate for externalizing these principles into the environment's design, creating a more powerful and generalizable learning paradigm.</p>
<p>Hierarchical reinforcement learning (HRL) directly addresses the challenge of long-horizon tasks by imposing a hierarchical structure on an agent's policy.The seminal options framework formalized the concept of temporally extended actions, where a high-level meta-policy selects among a set of options (sub-policies) that execute for multiple timesteps to achieve specific subgoals [Sutton et al., 1999].This temporal abstraction allows agents to reason and plan at a higher level, making credit assignment more tractable and exploration more efficient.This is a form of an internal, learned world model of task structure.Our proposal aims to externalize this hierarchy, embedding the task structure into the environment itself to explicitly guide the learning of such multi-level policies, thereby simplifying the learning problem for the agent.</p>
<p>Curriculum learning is founded on the observation that humans and animals learn more effectively when presented with concepts in a structured, progressive order of increasing difficulty.This principle was formalized for machine learning by Bengio et al. [2009], demonstrating that training models on a carefully designed curriculum of examples can significantly improve generalization and convergence speed.Many MARL approaches for complex games like soccer employ manually designed curricula, progressing from 1v1 to full team play [Baghi, 2024].We argue that a hierarchically scaffolded environment can generate a curriculum automatically and more dynamically, as an emergent property of the task's compositional structure, moving beyond static, manually-defined stages.</p>
<p>LLMs as agents and planners.Recent work has demonstrated the remarkable potential of LLMs to function as the reasoning core for autonomous agents.This directly aligns with leveraging world knowledge from LLMs.Landmark studies like SayCan have shown that LLMs can decompose high-level natural language instructions into sequences of executable, low-level actions for robotic manipulators, grounding language in physical affordances [Ahn et al., 2022].Similarly, Voyager demonstrated an LLM-based agent capable of open-ended exploration and self-directed skill discovery in complex environments like Minecraft, where the LLM itself generates the curriculum by proposing progressively harder tasks [Wang et al., 2023].Our proposal leverages this capability, positioning LLMs not just as an agent's brain but as a dynamic, zero-shot environment structurer, a generative world model of tasks that translates high-level goals into the concrete task scaffolding needed for learning.</p>
<p>3 The core challenge: from reward sparsity to task decomposition</p>
<p>The term "sparse rewards" has become a ubiquitous explanation for the difficulties encountered in training agents for long-horizon tasks.In robotic soccer, the ultimate reward, scoring a goal, is an infrequent event, making it a challenging signal for credit assignment across a long sequence of actions.While techniques like reward shaping, which provide denser, more localized feedback for actions like moving towards the ball or possessing it, can be effective [Li et al., 2024b, Baghi, 2024], they often treat the symptoms rather than the underlying disease.The foundational problem is not simply that the goal is a rare event; it is that the path to that goal requires a long, temporally extended, and specific sequence of coordinated sub-tasks.We posit that the core challenge is one of task decomposition.</p>
<p>Task sparsity in multi-agent coordination</p>
<p>Consider the sequence of actions required for a successful offensive play in 3v3 soccer: Agent 1 must first gain possession of the ball.It must then assess the positions of its teammates and opponents.It might then decide to dribble past an immediate defender.Subsequently, it must execute a pass to Agent 2, who is moving into an open position.Agent 2 must receive the pass, orient towards the goal, and perhaps take a shot.Each of these steps is a subgoal, and failure at any point breaks the chain.</p>
<p>An end-to-end MARL agent, operating with a flat action space and a single terminal reward, faces a combinatorial explosion.If each of N agents has an action space of size |A|, the joint action space at each timestep is |A| N .Over a horizon of T steps, the number of possible trajectories is astronomical.The probability of a random or semi-random exploration policy discovering this entire successful sequence is infinitesimally small.This is the problem of task sparsity: the density of successful, goal-achieving trajectories in the vast space of all possible trajectories is effectively zero.Simply providing a denser reward for getting closer to the goal does not teach the agent the crucial concepts of passing, creating space, or defending, the abstract building blocks of strategy.</p>
<p>The inefficiency of brute-force scaling</p>
<p>The prevailing approach in machine learning has been to overcome such challenges with scale.However, the nature of multi-agent coordination makes this particularly inefficient.As noted in the analysis of a modified TiZero framework, MARL methods can learn meaningful strategies but still struggle with sample inefficiency and slow learning progress [Baghi, 2024].The computational cost scales exponentially with the number of agents and the complexity of the required coordination.</p>
<p>Furthermore, a focus on extreme computational scale can make research inaccessible and sideline promising alternative paradigms.Our methodological review of the 2024 soccer literature was intentionally focused on low-resource approaches that control agents at a high level (e.g., target positions, kick actions) rather than low-level joint angles.This decision was informed by the observation that some of the most complex end-to-end systems, such as those that learn directly from egocentric vision [Tirumala et al., 2024], require a scale of resources that places them beyond the reach of most academic labs.These represent a brute-force approach to a problem that may be better solved with structural priors.The goal should be to find more efficient paths to intelligence, not just the most computationally expensive ones.</p>
<p>Methodology of review</p>
<p>To ground our position in empirical evidence, we conducted a systematic survey of academic papers published in 2024 that addressed multi-agent robotic soccer.Our search criteria targeted papers in reputable journals and conference proceedings containing the terms "multi-agent" and "soccer."We applied several filters to align the review with the problem of strategic, low-resource learning:</p>
<ol>
<li>Domain Focus: We excluded papers where soccer was merely one of several benchmark environments for a general MARL algorithm, focusing instead on work where soccer was the primary domain of investigation.This focus on a competition-driven domain is deliberate; major robotics competitions are consistently identified as primary drivers of innovation, providing standardized benchmarks and forcing researchers to integrate solutions into effective, holistic systems [Brancalião et al., 2022, Sammut, 2010].2. Abstraction Level: We excluded papers that focused on low-level control, such as learning individual robot joint angles for walking or kicking, to concentrate on the strategic and tactical layers of decision-making relevant to long-horizon planning.3. Resource Accessibility: We prioritized methods that demonstrated effectiveness without relying on massive, proprietary computational infrastructure, aiming to identify scalable principles rather than just scalable implementations.</li>
</ol>
<p>This curated selection of recent work forms the basis for the analysis presented in the next section, revealing a consistent pattern of leveraging hierarchy and abstraction to tackle the challenge of task decomposition.</p>
<p>4 Evidence from the field: the rise of hierarchical and symbolic methods</p>
<p>Our review of the 2024 literature uncovers a significant trend: a move away from monolithic, endto-end MARL and towards hybrid architectures that explicitly integrate hierarchical and symbolic reasoning.These approaches directly confront the problem of task decomposition, providing a structured framework that guides learning and makes complex, long-horizon coordination tractable.A summary of the most relevant papers is presented in Table 1.</p>
<p>Explicit task decomposition with symbolic planners</p>
<p>The most compelling evidence for our position comes from the work of Mu et al. [2024], who introduce the HS-MARL framework.This work stands as a direct confirmation that task decomposition is the central challenge.Instead of relying on RL to discover high-level strategy from scratch, HS-MARL integrates a classical AI planner, a Hierarchical Task Network (HTN) planner called pyHIPOP+, into the MARL loop.</p>
<p>The core idea is to use the HTN planner to decompose the overarching goal (e.g., score a goal) into a logically sound sequence of executable subgoals.This plan is generated using a symbolic representation of the world state and a domain description written in the Hierarchical Domain Definition Language (HDDL).The framework constructs a hierarchical state-space tree where subgoals are arranged based on their logical and temporal proximity to the final goal.For instance, the goal ScoreGoal might be decomposed into the subtasks AcquireBall, MoveToShootingPosition, and ExecuteShot.A meta-controller then uses this symbolic plan to guide the low-level MARL agents.It assigns symbolic options, policies trained to achieve specific subgoals, to the agents.This architecture directly addresses task sparsity by providing a high-level scaffold for the agents' behavior.The RL component is no longer responsible for discovering the entire strategic sequence but is instead focused on learning how to execute the sub-tasks prescribed by the planner.The authors explicitly note that this integration is designed to tackle complex environments with sparse rewards, demonstrating significantly improved performance and sample efficiency.</p>
<p>This trend is reinforced by related work from some of the same authors on Bayesian Strategy Networks (BSN), which aims to separate an intricate policy into several simple sub-policies and organize their relationships [Yang and Parasuraman, 2024].A BSN models the conditional dependencies between sub-policies (e.g., a shoot sub-policy is only relevant if the "possess ball" condition is met), effectively decomposing the global policy into a structured, interpretable graph.While distinct in its probabilistic approach, the underlying philosophy is identical: complex behavior emerges from the composition of simpler, learned components.</p>
<p>Hierarchical structure as an implicit curriculum</p>
<p>A key benefit of embedding hierarchical structure is the natural emergence of a curriculum, which guides agent learning from simple to complex behaviors.Manually designing such curricula is a known effective technique, but it is labor-intensive and difficult to scale.For example, the TiZero framework for soccer relies on a curriculum of manually designed scenarios of increasing difficulty [Baghi, 2024].An agent must achieve a certain win rate in a simple 1v1 scenario before progressing to a more complex 2v2 scenario, and so on.</p>
<p>The hierarchical scaffolding approach automates this process.In HS-MARL, the state-space tree generated by the HTN planner forms a natural curriculum.Agents first learn policies for subgoals at the leaves of the decomposition tree (e.g., move_to_ball), as these are prerequisites for higher-level subgoals (e.g., pass_to_teammate).The structure of the task itself dictates the learning progression, making the curriculum an emergent property of the environment's hierarchical design rather than a manually engineered artifact.</p>
<p>A different, yet philosophically similar, approach to emergent curricula can be seen in the work of Azarkasb and Khasteh [2024].Their method uses eligibility traces to build a knowledge base of successful trajectories.Critically, their training process is multi-staged: first, the agent learns optimal paths to the ball and goal in an obstacle-free environment.Only after mastering this foundational skill are obstacles (i.e., opponent robots) introduced.This two-stage process constitutes a simple but effective curriculum that emerges directly from the incremental layering of task complexity.The stored eligibility traces function as a learned library of successful sub-plans, akin to the symbolic options in HS-MARL.</p>
<p>Abstraction for efficient and interpretable learning</p>
<p>A third major advantage of the hierarchical approach is that it facilitates learning in a more abstract, and therefore more tractable, state-action space.By decomposing the problem, we can define high-level actions and representations that hide irrelevant low-level details, drastically reducing the exploration space for the learning algorithm.</p>
<p>The HS-MARL framework exemplifies this principle.The meta-controller operates in the symbolic space of subgoals and options, while the low-level agents execute these options in the continuous state space.This separation of concerns allows the strategic layer (the planner and meta-controller) to reason over long horizons without getting bogged down in the minutiae of motor control.As Mu et al. [2024] note, a key benefit of this architecture is enhanced interpretability.One can inspect the symbolic plan generated by the HTN to understand the agent team's high-level strategy, a task that is nearly impossible with a monolithic neural network policy.</p>
<p>This move towards abstraction is a field-wide trend.Our decision to exclude joint-level control from our review reflects this shift.Modern MARL frameworks for soccer, like MARLadona [Li et al., 2024b], operate with abstract actions like move, turn, and kick, rather than controlling individual motor torques.Similarly, methods focused on high-level strategy, such as advice distillation from a teacher agent [Li et al., 2024a] or policy distillation for countering opponents [Zhao et al., 2024], presuppose an abstract, strategic layer of interaction.These methods work because they operate on a representation of the problem that is already simplified and structured.Hierarchical scaffolding is the logical next step: making that structure an explicit and dynamic component of the learning process itself.</p>
<p>A new paradigm for environment design: principles and implications</p>
<p>The evidence points towards a need to fundamentally rethink how we design environments for agent research.The dominant paradigm of creating high-fidelity, flat simulators and challenging agents to learn from scratch is fundamentally inefficient for teaching strategy.We must shift our focus to designing world models and environments that are themselves structured for learning.</p>
<p>From flat physics simulators to structured task environments</p>
<p>Many current multi-agent benchmarks, such as Google Research Football [Kurach et al., 2020] or the Isaac Gym-based environment used by MARLadona [Li et al., 2024b], are best described as sophisticated physics simulators.They provide the laws of the world and a sparse terminal goal, leaving the entire burden of task decomposition and strategy discovery to the learning algorithm.</p>
<p>While valuable for testing raw learning capabilities, their world models are incomplete for decisionmaking because they lack a model of task semantics.</p>
<p>We propose that the next generation of benchmarks should be architected as structured task environments.This means moving beyond physics and providing explicit support for hierarchical task definition and decomposition as a core feature of the environment's world model itself.The environment should not just be a place where an agent acts, but a place where an agent learns structure.</p>
<p>Key features of a scaffolded environment</p>
<p>A scaffolded environment would possess several key features that distinguish it from a traditional simulator:</p>
<p>1.A Hierarchical Task API: The environment would expose an API for defining tasks and sub-tasks, along with their dependencies and termination conditions.This would be analogous to the HDDL domains used by Mu et al. [2024], but integrated directly into the environment's state management.For instance, a researcher could define a PassingPlay task composed of the sub-tasks passer_moves_to_ball, receiver_moves_to_open_space, and passer_kicks_to_receiver.</p>
<ol>
<li>Layered Action Spaces: Agents could interact with the environment at multiple levels of abstraction, from low-level continuous commands (e.g., set_velocity(x, y)) to high-level symbolic options (e.g., execute_option(pass_to_teammate_3)).This directly supports the development of hierarchical policy architectures.</li>
</ol>
<p>Intrinsic Rewards for Subgoals:</p>
<p>The environment itself would manage the reward signals associated with the task hierarchy.Upon successful completion of a defined subgoal, the environment would issue an intrinsic reward to the relevant agent(s).This codifies reward shaping directly into the task definition, providing dense, meaningful learning signals that are perfectly aligned with the task's compositional structure.</p>
<ol>
<li>
<p>Procedural Curriculum Generation: With a compositional task definition system, the environment could procedurally generate curricula, starting with foundational sub-tasks and gradually combining them into more complex, longer-horizon challenges as agents demonstrate mastery.</p>
</li>
<li>
<p>Built-in Support for Compositional Evaluation: Such environments necessitate new evaluation metrics beyond simple win rates.We propose:</p>
</li>
</ol>
<p>• Compositional Generalization Score: Measures an agent's zero-shot or few-shot performance on a novel, complex task composed of sub-tasks it has already mastered individually.• Curriculum Efficiency: Quantifies the reduction in sample complexity (1/N samples ) or wall-clock time required to reach a target performance level in the scaffolded environment compared to a flat baseline.• Scaffolding Brittleness Index: Assesses the performance degradation when the provided task hierarchy is intentionally made sub-optimal.This measures an agent's robustness and ability to learn despite an imperfect scaffold.</p>
<p>The role of large language models as dynamic planners</p>
<p>While the HTN-based approach in HS-MARL provides a powerful proof-of-concept, its reliance on manually crafted, domain-specific HDDL files represents a significant engineering bottleneck.</p>
<p>Recent work has already demonstrated the power of LLMs to decompose high-level instructions into executable plans for embodied agents [Ahn et al., 2022, Wang et al., 2023].We posit that this principle can be adapted to dynamically structure the learning environment itself, transforming static scaffolding into a flexible, language-driven curriculum.The LLM becomes a generative world model of tasks.</p>
<p>In this paradigm, an environment would not have a fixed task hierarchy.Instead, a high-level goal would be specified in natural language (e.g., "Execute a give-and-go play with the forward on the right wing" .This sequence would be passed to the environment's task API, dynamically configuring the intrinsic reward function and success conditions for that specific episode.This approach offers several transformative advantages:</p>
<p>• Scalability and Flexibility: It eliminates the need for human experts to write complex domain description files for every conceivable strategy.New tasks and curricula can be generated simply by writing new prompts.</p>
<p>• Instruction-Following Agents: It provides a natural framework for training agents that can follow complex, high-level instructions, a cornerstone of general-purpose agency and a key topic in VLA models.</p>
<p>• Dynamic Adaptation: The LLM planner could potentially re-plan mid-episode based on new state information, providing a dynamic scaffold that adapts to the unfolding situation on the field.This directly addresses a key limitation of rigid, pre-computed HTN plans.</p>
<p>Implications for agent architectures and Sim2Real transfer</p>
<p>Shifting the focus to scaffolded environments would catalyze a corresponding shift in agent architectures and accelerate research in several key areas.Architectures with high-level meta-policies that select among low-level sub-policies, as formalized in the options framework [Sutton et al., 1999], would be a natural fit.This also provides a powerful grounding for hybrid, neuro-symbolic agents that combine neural pattern recognition with logical reasoning.Furthermore, an agent that has learned a policy for a "move to open space" sub-task could more easily reuse that skill when it appears as part of a new, more complex task, making scaffolded environments ideal testbeds for studying generalization through composition.</p>
<p>Crucially, this paradigm offers a more tractable path for Sim2Real transfer.Hierarchical policies are inherently better suited for transfer because the domain shift is often concentrated at the lowest levels.</p>
<p>The high-level strategic layer (e.g., deciding when to pass) is often robust to the differences between simulation and reality, while the reality gap primarily affects the low-level motor policies (e.g., the precise joint torques needed to execute the pass) [Kober et al., 2013].This decomposition allows researchers to focus domain adaptation efforts on a smaller set of well-defined sub-tasks, making the Sim2Real problem more tractable for embodied agents.</p>
<p>Challenges and future directions</p>
<p>While promising, the paradigm of scaffolded environments presents its own set of research challenges that must be addressed.</p>
<p>The scaffolding design problem</p>
<p>A primary challenge is the origin of the hierarchical task structure itself.If the scaffolds must be meticulously hand-crafted by human experts, we have merely shifted the burden from the learning algorithm to the environment designer.This creates a bottleneck and may imbue the agent with the designer's own biases.The aforementioned use of LLMs as zero-shot planners is a promising mitigation strategy, but it introduces its own challenges, such as ensuring the logical consistency and physical feasibility of LLM-generated plans.Future work could explore methods for agents to learn the task hierarchy itself, perhaps by identifying bottleneck states in their exploration or by abstracting common sub-trajectories into reusable skills.</p>
<p>The risk of over-constraining discovery</p>
<p>A critical question is whether providing explicit structure could stifle an agent's creativity.A scaffold guides an agent along known solution pathways, but it may discourage the discovery of novel, superhuman strategies that lie outside the pre-defined hierarchy.This represents a fundamental trade-off between sample efficiency and open-ended exploration.Environments of the future may need to manage this trade-off explicitly, perhaps by employing curriculum strategies that begin with a rigid scaffold and gradually relax constraints as the agent's competency grows, a process known as fading.</p>
<p>Another approach could involve using stochastic planners that introduce structured variation into the task decomposition.</p>
<p>The grounding problem</p>
<p>The effective use of symbolic subgoals (whether from an HTN or an LLM) depends on the ability to connect them to the low-level, sub-symbolic state of the environment.This is the classic symbol grounding problem [Harnad, 1990].For example, how does an agent robustly translate the symbolic subgoal "move to open space" into a concrete target (x, y) coordinate in a dynamic environment with moving teammates and opponents?Solving this requires robust perception models that can map between the continuous state space (e.g., from vision) and a discrete, symbolic representation.Developing such models that are both accurate and learnable is a central challenge for neuro-symbolic agent architectures and is directly related to VLA models that must align language with perception and action.</p>
<p>Conclusion</p>
<p>The pursuit of more capable and autonomous agents has led the research community to recognize the critical role of environments in shaping intelligence.The current emphasis on scaling the fidelity and size of these environments, while valuable, overlooks a more crucial dimension: structural complexity.</p>
<p>In complex multi-agent domains, the primary obstacle to learning is not merely the sparsity of rewards but the sparsity of coherent, long-horizon tasks.An embodied world model for decision-making must therefore model not just the physics of the world, but also the structure of tasks within it.</p>
<p>This paper has argued that the most effective path forward is to build environments with explicit hierarchical scaffolding.Through a systematic review of the 2024 literature in multi-agent soccer, we have shown a clear trend towards hybrid systems that integrate symbolic planning and hierarchical decomposition with MARL.These methods succeed precisely because they provide the structural priors necessary to make the problem of strategic coordination tractable.</p>
<p>We have extended this position by proposing a new class of structured task environments that treat hierarchical decomposition as a first-class feature, and have argued that Large Language Models can serve as dynamic, generative world models of tasks to create this structure on the fly.This paradigm directly enables research into long-horizon planning, sim-to-real transfer, and language-guided decision making.To scale agents to complex, strategic domains, we must first scale the environments to support complex, strategic learning.Our position is a call to action for the community of environment builders.We must move beyond creating mere physics sandboxes and begin engineering structured learning curricula directly into our benchmarks.Only by scaffolding the world can we expect our agents to build the towering capabilities of goal-directed, embodied intelligence we envision.</p>
<p>Table 1 :
1
Summary of key 2024 multi-agent soccer literature demonstrating a trend towards hierarchical and structured approaches.
PaperCore MethodKey ContributionRelation to Hierarchy/StructureLi et al. [2024b]MARLadona (MARL)Dense rewards + curriculum for team play.Explicit, hand-crafted curriculum (1v1, 2v2).Reward shaping provides implicit goal structure.Baghi [2024]Modified TiZero (MARL)Explores intrinsic rewards in a curriculum.Heavily relies on a multi-stage, explicit curriculum.Mu et al. [2024]HS-MARLIntegrates HTN planner with MARL.Explicit hierarchical task decomposition via symbolic planner.Meta-controller assigns subgoals to agents.Symbolic options create a structured, high-level action space.Azarkasb and Khasteh [2024] Eligibility TracesLearns a knowledge base of successful paths.Implicit structural memory via traces; multi-stage training curriculum.Li et al. [2024a]ADA (Advice Distillation)Teacher agent provides advice to student.Social hierarchy (teacher-student) structures learning process.
Zhao et al. [2024]MCRL (Mimic-to-Counteract) Two-stage curriculum: mimic expert, then counteract.Structures training through distinct, high-level strategic goals.Yang and Parasuraman [2024] BSN (Bayesian Strategy Nets) Decomposes policy into sub-policies using a BSN.Explicit policy decomposition into simpler, coordinated components.</p>
<p>).An integrated LLM would then perform common-sense reasoning to decompose this instruction into a plausible, ordered sequence of symbolic subgoals: [1.Player A dribbles towards Player B], [2.Player A passes to Player B], [3.Player A runs into open space past defender], [4.Player B returns pass to Player A]</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Nicolas Sermanet, Clayton Sievers, Alexander Tan, Vincent Toshev, Fei Vanhoucke, Ted Xia, Peng Xiao, Sichun Xu, Mengyuan Xu, Andy Yan, Zeng, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Eligibility Traces in an Autonomous Soccer Robot with Obstacle Avoidance and Navigation Policy. Seyed Omid, Azarkasb , Seyed Hossein, Khasteh , Journal of Intelligent &amp; Robotic Systems. 10912024</p>
<p>Applying Multi-Agent Reinforcement Learning as Game-AI in Football-like Environments. Amir Masoud, Baghi , 2024KTH Royal Institute of TechnologyPhD thesis</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Systematic Mapping Literature Review of Mobile Robotics Competitions. Laiany Brancalião, José Gonçalves, Miguel Á Conde, Paulo Costa, 10.3390/s22062160Sensors. 2262160mar 2022</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Ryder, Advances in neural information processing systems. 202033</p>
<p>The symbol grounding problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>Reinforcement learning in robotics: A survey. Jens Kober, Andrew Bagnell, Jan Peters, The International Journal of Robotics Research. 32112013</p>
<p>Google research football: A novel reinforcement learning environment. Karol Kurach, Anton Raichuk, Piotr Stańczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Self-Attention Guided Advice Distillation in Multi-Agent Deep Reinforcement Learning. Yang Li, Sihan Zhou, Yaqing Hou, Liran Zhou, Hongwei Ge, Liang Feng, IEEE Transactions on Neural Networks and Learning Systems. 2024a</p>
<p>. Zichong Li, Filip Bjelonic, Victor Klemm, Marco Hutter, arXiv:2401.123452024bMARLadona -Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning. arXiv preprint</p>
<p>Hierarchical Task Network-Enhanced Multi-Agent Reinforcement Learning: Toward Efficient Cooperative Strategies. Xuechen Mu, Hankz Hankui Zhuo, Chen Chen, Kai Zhang, Chao Yu, Jianye Hao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>A Survey on Multi-Agent Reinforcement Learning and its Application. Zepeng Ning, Lihua Xie, Engineering Applications of Artificial Intelligence. 1271073352024</p>
<p>Robot soccer. Claude Sammut, Wiley Interdisciplinary Reviews: Cognitive Science. 162010</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 1121-21999</p>
<p>Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning. Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess, Robotics: Science and Systems. 2024</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, arXiv:2305.162912023arXiv preprint</p>
<p>Bayesian strategy networks based soft actor-critic learning. Qin Yang, Ramviyas Parasuraman, 10.1145/3643862ACM Trans. Intell. Syst. Technol. 15mar 2024</p>
<p>Junjie Zhao, Jiangwen Lin, Xinyan Zhang, Yuanbai Li, Xianzhong Zhou, Yuxiang Sun, From Mimic to Counteract: a Two-Stage Reinforcement Learning Algorithm for Google Research Football. International Conference on Autonomous Agents and Multiagent Systems (AAMAS). 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2285 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2285</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2285</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-3284449</p>
                <p><strong>Paper Title:</strong> Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems</p>
                <p><strong>Paper Abstract:</strong> Literature underpins research, providing the foundation for new ideas. But as the pace of science accelerates, many researchers struggle to stay current. To expedite their searches, some scientists leverage hypothesis generation (HG) systems, which can automatically inspect published papers to uncover novel implicit connections. With no foreseeable end to the driving pace of research, we expect these systems will become crucial for productive scientists, and later form the basis of intelligent automated discovery systems. Yet, many resort to expert analysis to validate such systems. This process is slow, hard to reproduce, and takes time away from other researchers. Therefore, we present a novel method to validate HG systems, which both scales to large validation sets and does not require expert input. We also introduce a number of new metrics to automatically identify plausible generated hypotheses. Through the study of published, highly cited, and noise predicates, we devise a validation challenge, which allows us to evaluate the performance of a HG system. Using an in-progress system, MOLIERE, as a case-study, we show the utility of our validation and ranking methods. So that others may reproduce our results, we provide our code, validation data, and results at bit.ly/2EtVshN.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2285.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2285.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOLIERE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOLIERE (automatic biomedical hypothesis generation system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A topic-model-driven biomedical hypothesis generation system that extracts a subcorpus around two query entities, builds topic models (PLDA+) and uses word/phrase embeddings (FastText) to produce interpretable topic-based hypotheses and numeric rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Moliere: Automatic biomedical hypothesis generation system.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOLIERE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MOLIERE identifies a relevant region of a multi-layered knowledge network for a queried entity pair (a, c), extracts abstracts to form a subcorpus, runs topic modeling (PLDA+) to produce topic models treated as candidate hypotheses, and embeds words/phrases with FastText to compute a variety of similarity and network-based metrics between a, c and topics. It ranks produced hypotheses by those metrics (and learned combinations) to prioritize likely publishable connections. It also applies subnetwork induction (hyperellipse) and nearest-neighbor topic-centroid networks to scale and interpret results.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical literature mining / literature-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted exploration (link prediction between specified entity pairs) and large-scale many-to-many hypothesis ranking</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Operationalized as the ability to rank predicates that were first published after a cut-date higher than randomly sampled 'noise' predicates; concrete metrics used for ranking include embedding cosine similarity (CSim), Euclidean distance (L2), TopicCorr (cosine between topic-similarity vectors), BestCentrCSim, BestCentrL2, BestTopicPerWord, and topic-network metrics (TopicWalkLength, TopicWalkBtwn, TopicWalkEigen, TopicNetCCoef, TopicNetMod). Overall system-level novelty evaluation used ROC AUC distinguishing 'published' vs 'noise' and 'highly-cited' vs 'noise'.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Reported ROC AUCs: PolyMultiple (combined metric): 0.834 (published vs noise, PvN) and 0.874 (highly-cited vs noise, HCvN). Individual metric AUCs (PvN / HCvN): L2: 0.783 / 0.809; CSim: 0.709 / 0.703; BestCentroidCSim: 0.719 / 0.742; BestTopicPerWord: 0.686 / 0.731; BestCentrL2: 0.578 / 0.587; TopicCorr: 0.609 / 0.496. (AUC in range [0.5,1])</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>PolyMultiple: a polynomial combination of top-performing metrics (form i α_i x_i^{β_i}) optimized by black-box search over parameters (α_i ∈ [-1,1], β_i ∈ [1,3]) after scaling inputs to [0,1]; hyperparameters chosen to maximize ROC AUC separating published from noise.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Randomly generated 'noise' predicates and 'published' predicates (also a subset of 'highly-cited' predicates) used as baselines to evaluate ranking performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>MOLIERE rankings (via PolyMultiple) distinguish published from noise with AUC 0.834 and highly-cited from noise with AUC 0.874, outperforming individual metrics; simpler baselines (random noise) correspond to AUC ≈ 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>In biomedical domain, simple embedding-distance metrics (L2) were surprisingly strong indicators of publishable connections; topic-network metrics provide interpretability though underperformed L2; highly-cited discoveries may bridge more distant concepts producing noisier topic models, affecting some topic-based metrics' performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2285.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Validation Challenge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale cut-date validation challenge for hypothesis generation systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reproducible, expert-free validation framework that measures a HG system's ability to recover predicates that first appear after a chosen cut-date by comparing system rankings on 'published', 'highly-cited', and random 'noise' predicate sets using ROC curves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Validation Challenge (cut-date, published/highly-cited vs noise ROC evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Creates a knowledge base using only publications prior to a cut-date, enumerates unordered entity pairs (predicates) that first occur after the cut-date as 'published', identifies a subset whose first occurrence was in highly-cited papers as 'highly-cited', constructs random negative 'noise' pairs absent from SemMedDB, queries these pairs on the HG system, obtains rankings, and evaluates ranking quality via ROC curves (AUC) to assess novelty-detection performance at scale without expert input.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical literature mining; generalizable to other domains with extractable predicates</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>validation/evaluation of hypothesis generation systems (methodological)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>ROC AUC measuring separation between published (or highly-cited) predicates and noise based on the system's ranking scores; novelty operationalized as ability to rank future-published relations above noise.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Applied to MOLIERE: PolyMultiple AUC 0.834 (PvN) and 0.874 (HCvN); individual metric AUCs reported in paper (see MOLIERE entry).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Noise (random non-SemMedDB) predicates; published and highly-cited sets serve as positive classes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Used to demonstrate MOLIERE's ability to prefer published over noise predicates at scale (AUCs reported above).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Framework relies on availability of predicate extraction (SemMedDB/UMLS) and citation metadata (e.g., from Semantic Scholar) and is readily applicable across domains where similar predicate/time metadata exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2285.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PolyMultiple</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PolyMultiple (polynomial metric combination for HG ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned polynomial combination of multiple ranking metrics (embedding and topic/network-based) optimized to distinguish published vs noise predicates, producing a single numeric ranking score for hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PolyMultiple</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines selected top-performing metrics (e.g., L2, BestCenterL2, BestTopicPerWord, TopicCorr, TopicWalkBtwn, TopicNetCCoef) into a polynomial form Σ α_i x_i^{β_i} after scaling each x_i to [0,1]; parameters (α_i, β_i) found via black-box optimization (search over ~1M parameter combinations) to maximize ROC AUC separating published from noise.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical literature mining / ranking of candidate hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>ranking/prioritization of generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Combined ranking score optimized to maximize ROC AUC against published vs noise labels; implicitly uses the underlying metrics (embedding distance, topic-centroid similarity, topic-network measures) as novelty signals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Combined model achieved ROC AUC 0.834 (published vs noise) and 0.874 (highly-cited vs noise) on evaluated subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Black-box hyperparameter search over polynomial coefficients and exponents to maximize ROC AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Individual metrics (L2, CSim, BestCentroidCSim, etc.) and random noise baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>PolyMultiple significantly outperformed any single metric tested (e.g., L2 AUC 0.783 vs PolyMultiple 0.834 PvN).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Shows that combining complementary assumptions (distance-based, topic-centroid, topic-network) improves discrimination of publishable hypotheses in biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2285.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topic-driven ranking metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topic-model and embedding-based ranking metrics (TopicCorr, BestCentrCSim, BestTopicPerWord, Topic network metrics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of quantitative measures that relate query entities to generated topic models using embedding similarities and induced topic-centroid networks to score and interpret candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Topic-driven ranking metrics</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Metrics include: CSim (cosine similarity between entity embeddings), L2 (Euclidean distance); TopicSim (weighted cosine similarity between an entity and a topic); TopicCorr (cosine between TopicSim vectors of a and c); BestCentrCSim/BestCentrL2 (score of the best topic centroid on average similarity or proximity to midpoint); BestTopicPerWord (max over topics of min(TopicSim(a,T),TopicSim(c,T))); and topic-network metrics derived from nearest-neighbor graphs of (a,c,topic centroids) such as TopicWalkLength, TopicWalkBtwn (avg betweenness on a→c shortest path), TopicWalkEigen, TopicNetCCoef (clustering), TopicNetMod (modularity). These are used to rank hypotheses and provide interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical literature mining; general topic-model driven HG</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>scoring and interpreting topic-model-derived hypotheses for targeted entity-pair queries</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Each metric provides a proxy for novelty/relatedness, evaluated by ROC AUC against published vs noise labels (see MOLIERE/PolyMultiple scores).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Reported per-metric ROC AUCs (published vs noise): L2 0.783, CSim 0.709, BestCentroidCSim 0.719, BestTopicPerWord 0.686, TopicCorr 0.609, BestCentrL2 0.578; topic-network metrics had varied performance but offered interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Metrics can be combined (PolyMultiple) with learned weights/exponents; number of topics and subnetwork hyperellipse constant are tunable parameters affecting metric performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Independent metrics compared against each other and against combined PolyMultiple.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>L2 outperformed many topic-based metrics; BestCentroidCSim was best among cosine-based topic metrics; combination (PolyMultiple) outperformed all single metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Embedding-distance (L2) was a robust signal in biomedical embedding space; topic-network measures improved interpretability though not necessarily ROC performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2285.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARROWSMITH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arrowsmith (literature-based discovery system using ABC model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early literature-based discovery (LBD) system that implements the ABC model: given A and C, it seeks intermediate B terms where A→B and B→C co-occur in the literature, producing lists of candidate linking terms to suggest hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using arrowsmith: a computer-assisted approach to formulating and assessing scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Arrowsmith (ABC model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ABC paradigm: for a given pair of concepts (A,C) the system finds intermediate B terms appearing in the literature with A and with C separately; these B-sets serve as candidate explanatory links and hypotheses. Historically validated by rediscovering Swanson's findings (e.g., fish oil — Raynaud's).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical literature-based discovery / literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted linkage discovery between specified entities (ABC discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Historically validated by replication of known discoveries (M1 in Yetisgen-Yildiz & Pratt); some works use precision/recall of known 'gold-standard' B-sets as validation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Prior Arrowsmith evaluations involved human experts and replication of Swanson discoveries; expert analysis is central to validation historically (no numerical scores provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>ABC systems are limited to producing linking term lists and require careful selection of 'gold-standard' B-sets for quantitative validation; authors suggest topic-driven ranking methods could be applied to ABC outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2285.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientist (Soldatova & Rzhetsky)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot scientist (automated experiment-running HG system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of systems that not only generate hypotheses but also automatically select and run experiments to test them; such systems can rank statements by expected value if proven true and execute small-scale lab experiments (e.g., yeast).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot scientist (automated hypothesis testing and experiment selection)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Systems that represent hypotheses as logical statements, select experimentally informative statements (e.g., those with high expected scientific value), and automatically plan and run experiments to test hypotheses; scope is often limited to laboratory systems amenable to automation (small-scale biology).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>experimental biology / automated experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>automated hypothesis generation plus automated execution/testing (closed-loop discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Described as 'identifies statements that would be the most valuable if proven true' — i.e., ranks hypotheses by expected value/utility; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Implicit: feasibility constrained by robotic lab capabilities and experiment scope (e.g., limited to yeast experiments in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Such systems can validate hypotheses automatically but are limited by the class of experiments the automated lab can perform; discussed as complementary but resource-intensive approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2285.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spangler et al. system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated hypothesis generation based on mining scientific literature (Spangler et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated literature-mining system for hypothesis generation that produces visualizations and semantic networks to help experts inspect and prioritize candidate discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated hypothesis generation based on mining scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Spangler et al. automated HG and visualization system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses mining of unstructured literature to produce candidate hypotheses and visual semantic networks/ontologies; integrates expert-in-the-loop via visualizations to help interpret and prioritize results.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical literature mining / LBD</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>automated generation with human-in-the-loop interpretation (interactive discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>System emphasizes visualizations to incorporate expert judgment; paper references this as a method for incorporating expert knowledge, but no quantitative human-evaluation numbers are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Visual, interactive outputs facilitate expert assessment but do not scale to large numeric evaluation without expert involvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2285.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLinker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLinker (literature linking system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-based discovery system that returns intermediate terms (B-sets) linking queries and is validated using precision/recall against expert-derived gold-standard B-sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litlinker: capturing connections across the biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLinker</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given a disease or query, LitLinker produces candidate linking terms and validates results by whether gold-standard terms appear in its returned B-sets; uses precision and recall on the B-set as validation.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical literature-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted discovery producing linking term sets (ABC-style)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Precision and recall measured on B-set outputs against curated gold-standard term lists.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Requires careful selection of small gold-standard term sets typically defined with expert input; used as validation in prior works but no numerical results are quoted in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Validation approach works for ABC-style systems but is limited by the necessity and domain-specificity of gold-standard sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2285.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian surprise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian theory of surprise (surprise as novelty metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic measure (Bayesian surprise) used in prior work to quantify unexpected observations or interesting topological features, proposed as a novelty-detection metric in other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A world full of surprises: Bayesian theory of surprise to quantify degrees of uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian surprise</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Quantifies novelty/surprise by measuring how much a posterior belief changes relative to a prior when observing new data; used in prior studies to identify interesting topological features or unexpected measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>statistical novelty detection; used in robotics, vision, and data analysis; noted as relevant to novelty detection in discovery contexts</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>novelty/interestingness detection</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Bayesian surprise (information-theoretic change from prior to posterior); referenced as an approach used by other groups to detect interesting features.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Mentioned as an alternative approach to measuring novelty, but not applied within MOLIERE in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2285.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2285.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wren</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated identification and ranking of implicit relationships (Wren et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated method for discovering and ranking implicit relationships in the literature, cited as part of the literature on automated knowledge discovery and ranking of candidate connections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge discovery by automated identification and ranking of implicit relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Wren et al. implicit-relationship identification and ranking</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated pipeline to identify implicit relationships in biomedical text and rank them; used as prior art in automated hypothesis/relationship discovery and ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>biomedical text mining / literature discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>automated discovery and ranking of implicit relationships</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems', 'publication_date_yy_mm': '2018-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Moliere: Automatic biomedical hypothesis generation system. <em>(Rating: 2)</em></li>
                <li>Using arrowsmith: a computer-assisted approach to formulating and assessing scientific hypotheses. <em>(Rating: 2)</em></li>
                <li>Automated hypothesis generation based on mining scientific literature. <em>(Rating: 2)</em></li>
                <li>Representation of research hypotheses. <em>(Rating: 1)</em></li>
                <li>Knowledge discovery by automated identification and ranking of implicit relationships. <em>(Rating: 1)</em></li>
                <li>A world full of surprises: Bayesian theory of surprise to quantify degrees of uncertainty. <em>(Rating: 1)</em></li>
                <li>Litlinker: capturing connections across the biomedical literature. <em>(Rating: 1)</em></li>
                <li>Evaluation of literature-based discovery systems. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2285",
    "paper_id": "paper-3284449",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "MOLIERE",
            "name_full": "MOLIERE (automatic biomedical hypothesis generation system)",
            "brief_description": "A topic-model-driven biomedical hypothesis generation system that extracts a subcorpus around two query entities, builds topic models (PLDA+) and uses word/phrase embeddings (FastText) to produce interpretable topic-based hypotheses and numeric rankings.",
            "citation_title": "Moliere: Automatic biomedical hypothesis generation system.",
            "mention_or_use": "use",
            "system_name": "MOLIERE",
            "system_description": "MOLIERE identifies a relevant region of a multi-layered knowledge network for a queried entity pair (a, c), extracts abstracts to form a subcorpus, runs topic modeling (PLDA+) to produce topic models treated as candidate hypotheses, and embeds words/phrases with FastText to compute a variety of similarity and network-based metrics between a, c and topics. It ranks produced hypotheses by those metrics (and learned combinations) to prioritize likely publishable connections. It also applies subnetwork induction (hyperellipse) and nearest-neighbor topic-centroid networks to scale and interpret results.",
            "research_domain": "biomedical literature mining / literature-based discovery",
            "problem_type": "targeted exploration (link prediction between specified entity pairs) and large-scale many-to-many hypothesis ranking",
            "novelty_metric": "Operationalized as the ability to rank predicates that were first published after a cut-date higher than randomly sampled 'noise' predicates; concrete metrics used for ranking include embedding cosine similarity (CSim), Euclidean distance (L2), TopicCorr (cosine between topic-similarity vectors), BestCentrCSim, BestCentrL2, BestTopicPerWord, and topic-network metrics (TopicWalkLength, TopicWalkBtwn, TopicWalkEigen, TopicNetCCoef, TopicNetMod). Overall system-level novelty evaluation used ROC AUC distinguishing 'published' vs 'noise' and 'highly-cited' vs 'noise'.",
            "novelty_score": "Reported ROC AUCs: PolyMultiple (combined metric): 0.834 (published vs noise, PvN) and 0.874 (highly-cited vs noise, HCvN). Individual metric AUCs (PvN / HCvN): L2: 0.783 / 0.809; CSim: 0.709 / 0.703; BestCentroidCSim: 0.719 / 0.742; BestTopicPerWord: 0.686 / 0.731; BestCentrL2: 0.578 / 0.587; TopicCorr: 0.609 / 0.496. (AUC in range [0.5,1])",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "PolyMultiple: a polynomial combination of top-performing metrics (form i α_i x_i^{β_i}) optimized by black-box search over parameters (α_i ∈ [-1,1], β_i ∈ [1,3]) after scaling inputs to [0,1]; hyperparameters chosen to maximize ROC AUC separating published from noise.",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Randomly generated 'noise' predicates and 'published' predicates (also a subset of 'highly-cited' predicates) used as baselines to evaluate ranking performance.",
            "comparative_results": "MOLIERE rankings (via PolyMultiple) distinguish published from noise with AUC 0.834 and highly-cited from noise with AUC 0.874, outperforming individual metrics; simpler baselines (random noise) correspond to AUC ≈ 0.5.",
            "domain_specific_findings": "In biomedical domain, simple embedding-distance metrics (L2) were surprisingly strong indicators of publishable connections; topic-network metrics provide interpretability though underperformed L2; highly-cited discoveries may bridge more distant concepts producing noisier topic models, affecting some topic-based metrics' performance.",
            "uuid": "e2285.0",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Validation Challenge",
            "name_full": "Large-scale cut-date validation challenge for hypothesis generation systems",
            "brief_description": "A reproducible, expert-free validation framework that measures a HG system's ability to recover predicates that first appear after a chosen cut-date by comparing system rankings on 'published', 'highly-cited', and random 'noise' predicate sets using ROC curves.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Validation Challenge (cut-date, published/highly-cited vs noise ROC evaluation)",
            "system_description": "Creates a knowledge base using only publications prior to a cut-date, enumerates unordered entity pairs (predicates) that first occur after the cut-date as 'published', identifies a subset whose first occurrence was in highly-cited papers as 'highly-cited', constructs random negative 'noise' pairs absent from SemMedDB, queries these pairs on the HG system, obtains rankings, and evaluates ranking quality via ROC curves (AUC) to assess novelty-detection performance at scale without expert input.",
            "research_domain": "biomedical literature mining; generalizable to other domains with extractable predicates",
            "problem_type": "validation/evaluation of hypothesis generation systems (methodological)",
            "novelty_metric": "ROC AUC measuring separation between published (or highly-cited) predicates and noise based on the system's ranking scores; novelty operationalized as ability to rank future-published relations above noise.",
            "novelty_score": "Applied to MOLIERE: PolyMultiple AUC 0.834 (PvN) and 0.874 (HCvN); individual metric AUCs reported in paper (see MOLIERE entry).",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Noise (random non-SemMedDB) predicates; published and highly-cited sets serve as positive classes.",
            "comparative_results": "Used to demonstrate MOLIERE's ability to prefer published over noise predicates at scale (AUCs reported above).",
            "domain_specific_findings": "Framework relies on availability of predicate extraction (SemMedDB/UMLS) and citation metadata (e.g., from Semantic Scholar) and is readily applicable across domains where similar predicate/time metadata exist.",
            "uuid": "e2285.1",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "PolyMultiple",
            "name_full": "PolyMultiple (polynomial metric combination for HG ranking)",
            "brief_description": "A learned polynomial combination of multiple ranking metrics (embedding and topic/network-based) optimized to distinguish published vs noise predicates, producing a single numeric ranking score for hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PolyMultiple",
            "system_description": "Combines selected top-performing metrics (e.g., L2, BestCenterL2, BestTopicPerWord, TopicCorr, TopicWalkBtwn, TopicNetCCoef) into a polynomial form Σ α_i x_i^{β_i} after scaling each x_i to [0,1]; parameters (α_i, β_i) found via black-box optimization (search over ~1M parameter combinations) to maximize ROC AUC separating published from noise.",
            "research_domain": "biomedical literature mining / ranking of candidate hypotheses",
            "problem_type": "ranking/prioritization of generated hypotheses",
            "novelty_metric": "Combined ranking score optimized to maximize ROC AUC against published vs noise labels; implicitly uses the underlying metrics (embedding distance, topic-centroid similarity, topic-network measures) as novelty signals.",
            "novelty_score": "Combined model achieved ROC AUC 0.834 (published vs noise) and 0.874 (highly-cited vs noise) on evaluated subsets.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Black-box hyperparameter search over polynomial coefficients and exponents to maximize ROC AUC.",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Individual metrics (L2, CSim, BestCentroidCSim, etc.) and random noise baseline.",
            "comparative_results": "PolyMultiple significantly outperformed any single metric tested (e.g., L2 AUC 0.783 vs PolyMultiple 0.834 PvN).",
            "domain_specific_findings": "Shows that combining complementary assumptions (distance-based, topic-centroid, topic-network) improves discrimination of publishable hypotheses in biomedical literature.",
            "uuid": "e2285.2",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Topic-driven ranking metrics",
            "name_full": "Topic-model and embedding-based ranking metrics (TopicCorr, BestCentrCSim, BestTopicPerWord, Topic network metrics, etc.)",
            "brief_description": "A set of quantitative measures that relate query entities to generated topic models using embedding similarities and induced topic-centroid networks to score and interpret candidate hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Topic-driven ranking metrics",
            "system_description": "Metrics include: CSim (cosine similarity between entity embeddings), L2 (Euclidean distance); TopicSim (weighted cosine similarity between an entity and a topic); TopicCorr (cosine between TopicSim vectors of a and c); BestCentrCSim/BestCentrL2 (score of the best topic centroid on average similarity or proximity to midpoint); BestTopicPerWord (max over topics of min(TopicSim(a,T),TopicSim(c,T))); and topic-network metrics derived from nearest-neighbor graphs of (a,c,topic centroids) such as TopicWalkLength, TopicWalkBtwn (avg betweenness on a→c shortest path), TopicWalkEigen, TopicNetCCoef (clustering), TopicNetMod (modularity). These are used to rank hypotheses and provide interpretability.",
            "research_domain": "biomedical literature mining; general topic-model driven HG",
            "problem_type": "scoring and interpreting topic-model-derived hypotheses for targeted entity-pair queries",
            "novelty_metric": "Each metric provides a proxy for novelty/relatedness, evaluated by ROC AUC against published vs noise labels (see MOLIERE/PolyMultiple scores).",
            "novelty_score": "Reported per-metric ROC AUCs (published vs noise): L2 0.783, CSim 0.709, BestCentroidCSim 0.719, BestTopicPerWord 0.686, TopicCorr 0.609, BestCentrL2 0.578; topic-network metrics had varied performance but offered interpretability.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Metrics can be combined (PolyMultiple) with learned weights/exponents; number of topics and subnetwork hyperellipse constant are tunable parameters affecting metric performance.",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Independent metrics compared against each other and against combined PolyMultiple.",
            "comparative_results": "L2 outperformed many topic-based metrics; BestCentroidCSim was best among cosine-based topic metrics; combination (PolyMultiple) outperformed all single metrics.",
            "domain_specific_findings": "Embedding-distance (L2) was a robust signal in biomedical embedding space; topic-network measures improved interpretability though not necessarily ROC performance.",
            "uuid": "e2285.3",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "ARROWSMITH",
            "name_full": "Arrowsmith (literature-based discovery system using ABC model)",
            "brief_description": "An early literature-based discovery (LBD) system that implements the ABC model: given A and C, it seeks intermediate B terms where A→B and B→C co-occur in the literature, producing lists of candidate linking terms to suggest hypotheses.",
            "citation_title": "Using arrowsmith: a computer-assisted approach to formulating and assessing scientific hypotheses.",
            "mention_or_use": "mention",
            "system_name": "Arrowsmith (ABC model)",
            "system_description": "ABC paradigm: for a given pair of concepts (A,C) the system finds intermediate B terms appearing in the literature with A and with C separately; these B-sets serve as candidate explanatory links and hypotheses. Historically validated by rediscovering Swanson's findings (e.g., fish oil — Raynaud's).",
            "research_domain": "biomedical literature-based discovery / literature mining",
            "problem_type": "targeted linkage discovery between specified entities (ABC discovery)",
            "novelty_metric": "Historically validated by replication of known discoveries (M1 in Yetisgen-Yildiz & Pratt); some works use precision/recall of known 'gold-standard' B-sets as validation.",
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": true,
            "human_evaluation_results": "Prior Arrowsmith evaluations involved human experts and replication of Swanson discoveries; expert analysis is central to validation historically (no numerical scores provided in this paper).",
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "ABC systems are limited to producing linking term lists and require careful selection of 'gold-standard' B-sets for quantitative validation; authors suggest topic-driven ranking methods could be applied to ABC outputs.",
            "uuid": "e2285.4",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Robot Scientist (Soldatova & Rzhetsky)",
            "name_full": "Robot scientist (automated experiment-running HG system)",
            "brief_description": "A class of systems that not only generate hypotheses but also automatically select and run experiments to test them; such systems can rank statements by expected value if proven true and execute small-scale lab experiments (e.g., yeast).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Robot scientist (automated hypothesis testing and experiment selection)",
            "system_description": "Systems that represent hypotheses as logical statements, select experimentally informative statements (e.g., those with high expected scientific value), and automatically plan and run experiments to test hypotheses; scope is often limited to laboratory systems amenable to automation (small-scale biology).",
            "research_domain": "experimental biology / automated experimentation",
            "problem_type": "automated hypothesis generation plus automated execution/testing (closed-loop discovery)",
            "novelty_metric": "Described as 'identifies statements that would be the most valuable if proven true' — i.e., ranks hypotheses by expected value/utility; details not provided in this paper.",
            "novelty_score": null,
            "feasibility_metric": "Implicit: feasibility constrained by robotic lab capabilities and experiment scope (e.g., limited to yeast experiments in cited work).",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Such systems can validate hypotheses automatically but are limited by the class of experiments the automated lab can perform; discussed as complementary but resource-intensive approach.",
            "uuid": "e2285.5",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Spangler et al. system",
            "name_full": "Automated hypothesis generation based on mining scientific literature (Spangler et al.)",
            "brief_description": "An automated literature-mining system for hypothesis generation that produces visualizations and semantic networks to help experts inspect and prioritize candidate discoveries.",
            "citation_title": "Automated hypothesis generation based on mining scientific literature.",
            "mention_or_use": "mention",
            "system_name": "Spangler et al. automated HG and visualization system",
            "system_description": "Uses mining of unstructured literature to produce candidate hypotheses and visual semantic networks/ontologies; integrates expert-in-the-loop via visualizations to help interpret and prioritize results.",
            "research_domain": "biomedical literature mining / LBD",
            "problem_type": "automated generation with human-in-the-loop interpretation (interactive discovery)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": true,
            "human_evaluation_results": "System emphasizes visualizations to incorporate expert judgment; paper references this as a method for incorporating expert knowledge, but no quantitative human-evaluation numbers are provided in this paper.",
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Visual, interactive outputs facilitate expert assessment but do not scale to large numeric evaluation without expert involvement.",
            "uuid": "e2285.6",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "LitLinker",
            "name_full": "LitLinker (literature linking system)",
            "brief_description": "A literature-based discovery system that returns intermediate terms (B-sets) linking queries and is validated using precision/recall against expert-derived gold-standard B-sets.",
            "citation_title": "Litlinker: capturing connections across the biomedical literature.",
            "mention_or_use": "mention",
            "system_name": "LitLinker",
            "system_description": "Given a disease or query, LitLinker produces candidate linking terms and validates results by whether gold-standard terms appear in its returned B-sets; uses precision and recall on the B-set as validation.",
            "research_domain": "biomedical literature-based discovery",
            "problem_type": "targeted discovery producing linking term sets (ABC-style)",
            "novelty_metric": "Precision and recall measured on B-set outputs against curated gold-standard term lists.",
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": true,
            "human_evaluation_results": "Requires careful selection of small gold-standard term sets typically defined with expert input; used as validation in prior works but no numerical results are quoted in this paper.",
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Validation approach works for ABC-style systems but is limited by the necessity and domain-specificity of gold-standard sets.",
            "uuid": "e2285.7",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Bayesian surprise",
            "name_full": "Bayesian theory of surprise (surprise as novelty metric)",
            "brief_description": "A probabilistic measure (Bayesian surprise) used in prior work to quantify unexpected observations or interesting topological features, proposed as a novelty-detection metric in other domains.",
            "citation_title": "A world full of surprises: Bayesian theory of surprise to quantify degrees of uncertainty.",
            "mention_or_use": "mention",
            "system_name": "Bayesian surprise",
            "system_description": "Quantifies novelty/surprise by measuring how much a posterior belief changes relative to a prior when observing new data; used in prior studies to identify interesting topological features or unexpected measurements.",
            "research_domain": "statistical novelty detection; used in robotics, vision, and data analysis; noted as relevant to novelty detection in discovery contexts",
            "problem_type": "novelty/interestingness detection",
            "novelty_metric": "Bayesian surprise (information-theoretic change from prior to posterior); referenced as an approach used by other groups to detect interesting features.",
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Mentioned as an alternative approach to measuring novelty, but not applied within MOLIERE in this paper.",
            "uuid": "e2285.8",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        },
        {
            "name_short": "Wren",
            "name_full": "Automated identification and ranking of implicit relationships (Wren et al.)",
            "brief_description": "An automated method for discovering and ranking implicit relationships in the literature, cited as part of the literature on automated knowledge discovery and ranking of candidate connections.",
            "citation_title": "Knowledge discovery by automated identification and ranking of implicit relationships.",
            "mention_or_use": "mention",
            "system_name": "Wren et al. implicit-relationship identification and ranking",
            "system_description": "Automated pipeline to identify implicit relationships in biomedical text and rank them; used as prior art in automated hypothesis/relationship discovery and ranking.",
            "research_domain": "biomedical text mining / literature discovery",
            "problem_type": "automated discovery and ranking of implicit relationships",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2285.9",
            "source_info": {
                "paper_title": "Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems",
                "publication_date_yy_mm": "2018-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Moliere: Automatic biomedical hypothesis generation system.",
            "rating": 2,
            "sanitized_title": "moliere_automatic_biomedical_hypothesis_generation_system"
        },
        {
            "paper_title": "Using arrowsmith: a computer-assisted approach to formulating and assessing scientific hypotheses.",
            "rating": 2,
            "sanitized_title": "using_arrowsmith_a_computerassisted_approach_to_formulating_and_assessing_scientific_hypotheses"
        },
        {
            "paper_title": "Automated hypothesis generation based on mining scientific literature.",
            "rating": 2,
            "sanitized_title": "automated_hypothesis_generation_based_on_mining_scientific_literature"
        },
        {
            "paper_title": "Representation of research hypotheses.",
            "rating": 1,
            "sanitized_title": "representation_of_research_hypotheses"
        },
        {
            "paper_title": "Knowledge discovery by automated identification and ranking of implicit relationships.",
            "rating": 1,
            "sanitized_title": "knowledge_discovery_by_automated_identification_and_ranking_of_implicit_relationships"
        },
        {
            "paper_title": "A world full of surprises: Bayesian theory of surprise to quantify degrees of uncertainty.",
            "rating": 1,
            "sanitized_title": "a_world_full_of_surprises_bayesian_theory_of_surprise_to_quantify_degrees_of_uncertainty"
        },
        {
            "paper_title": "Litlinker: capturing connections across the biomedical literature.",
            "rating": 1,
            "sanitized_title": "litlinker_capturing_connections_across_the_biomedical_literature"
        },
        {
            "paper_title": "Evaluation of literature-based discovery systems.",
            "rating": 2,
            "sanitized_title": "evaluation_of_literaturebased_discovery_systems"
        }
    ],
    "cost": 0.01954,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems
February 11, 2018</p>
<p>Justin Sybrandt 
Ilya Safro isafro@clemson.edu 
Validation and Topic-driven Ranking for Biomedical Hypothesis Generation Systems
February 11, 2018
Literature underpins research, providing the foundation for new ideas. But as the pace of science accelerates, many researchers struggle to stay current. To expedite their searches, some scientists leverage hypothesis generation (HG) systems, which can automatically inspect published papers to uncover novel implicit connections. With no foreseeable end to the driving pace of research, we expect these systems will become crucial for productive scientists, and later form the basis of intelligent automated discovery systems. Yet, many resort to expert analysis to validate such systems. This process is slow, hard to reproduce, and takes time away from other researchers. Therefore, we present a novel method to validate HG systems, which both scales to large validation sets and does not require expert input. We also introduce a number of new metrics to automatically identify plausible generated hypotheses. Through the study of published, highly cited, and noise predicates, we devise a validation challenge, which allows us to evaluate the performance of a HG system. Using an in-progress system, MOLIERE, as a case-study, we show the utility of our validation and ranking methods. So that others may reproduce our results, we provide our code, validation data, and results at bit.ly/2EtVshN.</p>
<p>Introduction</p>
<p>Literature underpins research and provides the groundwork upon which scientists construct new ideas. But while the foundation of published knowledge grows [24], so does the difficulty in surveying it. Currently the National Library of Medicine adds about 2,300 papers a day to MEDLINE [30], and this is likely to increase given that scientific output doubles every nine years [50]. Working in the modern deluge of information, researchers can miss valuable connections.</p>
<p>Undiscovered public knowledge is the information no one explicitly knows, but has already been implicitly published [46]. For instance, in 1986 no one knew that fish oil was a treatment for Raynaud's Syndrome [45]. Instead, we knew separately that fish oil decreases a number of factors, and that those same factors worsen Raynaud's Syndrome. Arrowsmith, the software which uncovered this finding, was the first hypothesis generation system (HG) [39].</p>
<p>The importance of these systems rises alongside the pace of scientific output; an abundance of literature implies an abundance of implicit connections. Yet, while many projects focus on new ways to find potential discoveries [51,49,27,10,46,2,42], few have a way to rigorously verify the quality of the connections posed by their system [7].</p>
<p>HG systems are hard to verify because they attempt to uncover novel information, a challenging task even for human researchers. To understand whether a potential connection is novel, these systems must model novelty itself. While there are verifiable models for novelty in specific contexts, each is trained to detect patterns similar to those present in a training set, which is conducive to traditional cross-validation [14,23]. Some examples include using non-negative matrix factorization [25] to uncover protein-protein interactions [12], or to discover mutational cancer signatures [3]. In other domains, groups are using Bayesian surprise [20] to identify interesting topological features [33] or unexpected measurements for the purpose of uncertainty quantification [4]. In contrast to all of the above, HG systems strive to detect patterns absent from training data (but not necessarily outliers), and in doing so inspire new inquiry.</p>
<p>Without a viable alternative, the HG community primarily turns to expert analysis in order to validate the output of these systems [7,51,49,27,10,46,2]. Many methods also require an expert's input to rank their potential connections [49,27,46]. But, getting expert input is time consuming, introduces bias, and cannot scale.</p>
<p>Our contribution: We propose a novel method to thoroughly validate HG systems without expert input. Any system that provides a ranking criteria along with its generated hypotheses can leverage this validation method. In addition, we present a number of new ranking strategies that apply to any HG system that produces topic models. We implement each ranking strategy within our in-progress system, MOLIERE [49], in order to compare their performance with respect to our validation method. Although our work focuses on biomedical research, the methods we discuss easily generalize to HG in any domain.</p>
<p>Our methodology leverages predicates, or subject-verb-object statements, found within the MEDLINE dataset of over 27 million scientific abstracts. For simplicity and reproducibility we use the Semantic Medical Database (SemMedDB) [22], which defines predicates using identifiers provided by the Universal Medical Language System (UMLS) [1]. We identify the first publication date of each unique predicate in order to find new connections published each year. Because these connections all come from titles and abstracts of MEDLINE papers, we note that before a predicate's first publication, our system is unaware of any direct connection between its subject and object. Yet, a newly introduced predicate in a title or abstract represents a fruitful connection [8], the sort we would like a HG system to predict.</p>
<p>To begin our mass validation effort, that to the best of our knowledge has not been performed by other general-purpose HG systems, we select a "cut date," which determines the set of publications provided to our system. We define the set of predicates that first occur after the cut date as the "published" set, and we subset it to identify predicates that first occurred in "highly-cited" papers. Next we generate a set of "noise" predicates that are randomly constructed and have never been published. We then run queries for predicates in all three sets using our HG system, which allows us to understand our ability to recover important connections. From there we rank our results and plot Receiver Operating Characteristic (ROC) curves [14] in order to estimate the overall performance of our system. We find that by training a polynomial combination of multiple ranking metrics, MOLIERE is able to distinguish published and noise predicates with an ROC area of 0.834, and is able to distinguish highly cited predicates from noise with an ROC area of 0.874. This result is especially significant given that the main validation methods available, to both our system and other similar systems (see survey in [49]), were expert analysis and replicating the results of others [7].</p>
<p>Background</p>
<p>HG systems attempt to extract novel ideas from existing research. In this section, we begin by providing an overview on these systems and continue to explain how we leverage two complimentary techniques for understanding large text corpora: Topic Modeling [6] and Word Embedding [29]. The former identifies fuzzy clusters of terms, which allow us to glimpse the main domains of discourse across a document set. The latter embeds terms into a real-valued vector space, which allows us to convey semantic relationships using various distance metrics.</p>
<p>Extracting Information from Hypothesis Generation Systems</p>
<p>Swanson and Smalheiser created the first HG system Arrowsmith [39], and in doing so outlined the ABC model for discovery [48]. Although this approach has limitations [36], its conventions and intuitions remain in modern approaches [42].</p>
<p>In the ABC model, users run queries by specifying two keywords a and c. The goal of a HG system is to discover some entity b such that there are known relationships "a → b" and "b → c," which allow us to imply the relationship between a and c. Because many connections may require more than one element b to describe, researchers apply other techniques, such as topic models in our case, to describe these connections.</p>
<p>We center this work around our in-progress system, MOLIERE [49]. Once a user queries a and c, our system identifies a relevant region within our multi-layered knowledge network, which consists of papers, terms, phrases, and various types of links. We extract abstracts from this region and create a sub-corpus Figure 1: The above diagram shows a 2-D representation of the embeddings for over 8 thousand UMLS keywords within MOLIERE. We used singular value decomposition to reduce the dimensionality of these vectors from 500 to 2.</p>
<p>upon which we generate a topic model. This topic model describes groups of related terms, which we treat as a hypothesis. Until now, like other systems, we left the analysis of MOLIERE's output to experts, but through the methods described in Section 4 we perform this analysis automatically.</p>
<p>Word and Phrase Embedding</p>
<p>The method of finding dense vector representations of words is often referred to as "word2vec". In reality, this umbrella term references two different algorithms, the Continuous BOW (CBOW) method and the Skip-Gram method [29] Both rely on shallow neural networks in order to learn vectors through word-usage patterns.</p>
<p>MOLIERE uses FastText [21], a similar tool under the word2vec umbrella, to find high quality embeddings of medical entities. By preprocessing our text with the automatic phrase mining technique ToPMine [9], we improve these embeddings while finding multi-word medical terms such as "lung cancer" or "benign tumor." We see in Figure 1 that FastText clusters similar biological terms, an observation we leverage to validate our HG system and derive a number of metrics.</p>
<p>Topic Models</p>
<p>Latent Dirichlet Allocation (LDA) [6], the classical topic modeling method, groups keywords based on their document co-occurrence rates in order to describe the key "topics" of discourse. A topic is simply a probability distribution over a vocabulary, and each document from the input corpus is assumed to be a "mixture" of these topics. For instance, a topic model derived from New York Times articles would likely find one topic containing words such as "computer," "website," and "Internet," while another topic may contains words such as "money," "market," and "stock."</p>
<p>In the medical domain, we use topic models to understand trends across scientific literature. We look for groupings of entities such as genes, drugs, and diseases, which we then analyze to find novel connections. MOLIERE uses a parallel technique, PLDA+ [28] to quickly find topics from documents related to a and c. Because we pre-process MEDLINE articles with ToPMine, our resulting topic models include both words and phrases, which results in more meaningful topics.</p>
<p>Combing LDA and Word2Vec</p>
<p>There exists a symmetry between topic models and word embeddings [15]. The hidden layer of either CBOW or Skip-Gram captures semantic categories present in an input corpus. Each word in the vocabulary, when projected into the hidden layer, is represented as a mixture of hidden features -the same way we can view a word's probability across a topic model.</p>
<p>A topic model is a weighted point cloud, where the embedding of each word is weighted by its probably within a given topic. Therefore, we can also represent the topic as a weighted centroid. In the following section we leverage both representations in order to mathematically describe the similarity between a, c, and each topic.</p>
<p>Validation Challenge</p>
<p>In order to unyoke automatic HG from expert analysis, we propose a challenge that any system can attempt, provided it can rank its proposed connections. A successful system ought to rank published connections higher than those we randomly created. So, we train a system given historical information, and create the "published," "highly-cited," and "noise" query sets. We pose these connections to an HG system, and rank its outputs to plot ROC curves, which determine whether published predicates are preferred to noise. Through the area under these ROC curves, a HG system demonstrates its quality at a large scale without expert analysis.</p>
<p>Our challenge starts with the Semantic Medical Database (SemMedDB) [22] that contains predicates extracted from MEDLINE defined on the set of UMLS terms [30]. For instance, predicate "C1619966 TREATS C0041296" represents a discovered fact "abatacept treats tuberculosis". Because our system does not account for word order or verb, we look for distinct unordered word-pairs a-c instead. Using the metadata associated with each predicate, we note the date each unordered pair was first published. In Section 7 we discuss how we may improve our system to include this unused information.</p>
<p>From there, we select a "cut year." For this challenge, we create our knowledge network [49] using only information published before the cut year. We then identify the set of SemMedDB unordered pairs a-c first published after the cut year provided a and c both occur in that year's UMLS release. This "published set" of pairs represent new connections between existing entities, from the perspective of our HG system. We select 2010 as the cut year for our study in order to create a published set of over 1 million pairs. Also, we create a set of "highly-cited" pairs by filtering the published set by citation count. We use data from SemMedDB, MEDLINE, and Semantic Scholar to create a set of 1,448 pairs from the published set that first occurred in a paper cited over 100 times. We intuit that this set is closer to the number of landmark discoveries since the cut-date, given that the published set is large and likely contains incidental or incorrect connections.</p>
<p>To provide negative examples, we generate a "noise set" of pairs by sampling the cut-year's UMLS release, storing the pair only if it does not occur in SemMedDB. These pairs represent nonsensical connections between UMLS elements. Although it is possible that we may stumble across novel findings within the noise set, we assume this will occur infrequently enough to not effect our results.</p>
<p>We run a-c queries from each set through MOLIERE and create two ranked lists: published vs. noise (PvN) and highly-cited vs. noise (HCvN). After ranking each set, we generate ROC curves [14], which allow us to judge the quality of our HG system. If more published predicates occur earlier in the ranking than noise, the ROC area will be close to 1, otherwise it will be closer to 0.5.</p>
<p>Ranking Methods for Topic Model Driven Hypotheses</p>
<p>In order to rise to the challenge stated above, we advanced MOLIERE [49] to produce a numeric ranking that accompanies its resulting hypotheses. As described above, we use this ranking to calculate ROC curves and evaluate our system's performance.</p>
<p>Another extremely important use case of ranking is related to massive query runs in hypothesis generation systems. Typically, biomedical researchers are interested in investigating connections of one-to-many types. For example, one disease can be queried versus all genes in order to establish what genes are related to it or one specific drug ingredient is verified versus all side effects. In practice, a researcher may be interested to run a−c i queries for tens of thousands c i 's. In such cases, ranking the hypotheses becomes vitally important.</p>
<p>In the following sections we present a number of promising approaches for ranking the topic model results from MOLIERE queries. The key intuition underpinning these metrics is depicted in Figure 2. That is, related objects are grouped together in vector space, so if a and c are related by a third entity b, we hypothesize b's vector (b) ought to be similar to both (a) and (c). Because MOLIERE returns a topic model, rather than some single b, we quantify the potential relationship between a and c through their relationship to the topic model using the following metrics. The above depicts two queries, a-c 1 and a-c 2 , where a-c 1 is a published connection and a-c 2 is a noise connection. We see topics for each query represented as diamonds via Centr(T i ). Although both queries lead to topics which are similar to a, c 1 , or c 2 , we find that the the presence of some topic which is similar to both objects of interest may indicate the published connection.</p>
<p>Similarity Between Query Words</p>
<p>As a baseline, we first consider two similarity metrics that do not include topic information: cosine similarity (CSim) and Euclidean distance (L 2 ), namely,
CSim(a, c) = (a) · (c) || (a)|| 2 ×|| (c)|| 2 and L 2 (a, c) = || (a) − (c)|| 2 ,
where a and c are the two objects of interest, and (x) is an embedding function (see Section 2.2). Note, when calculating ROC curves for the L 2 metric, we will sort in reverse, meaning smaller distances ought to indicate published predicates.</p>
<p>These metrics indicate whether a and c share the same cluster with respect to the embedding space. Our observation is that this can be a good indication that a and c are of the same kind, or are conceptually related. This cluster intuition is shared by others studying similar embedding spaces [52].</p>
<p>Topic Model Correlation</p>
<p>The next metric attempts to uncover whether a and c are mutually similar to the generated topic model. This metric starts by creating vectors v(a, T ) and v(c, T ) which express each object's similarity to topic
model T = {T i } k i=1
derived from an a − c query. We do so by calculating the weighted cosine similarity TopicSim(x, T i ) between each topic T i and each object x ∈ {a, c}, namely,
TopicSim(x, T i ) = (w,p)∈Ti p · CSim(x, w),
where a probability distribution over terms in T i is represented as word-probability pairs (w, p). This metric results in a value in the interval [-1, 1] to represent the weighted similarity of x with T i . The final similarity vectors v(a, T ) and v(c, T ) in R k are defined as
∀x ∈ {a, c} v(x, T ) =      TopicSim(x, T 1 ) TopicSim(x, T 2 )
. . .
TopicSim(x, T k )      .
Finally, we can see how well T correlates with both a and c by taking yet another cosine similarity
TopicCorr(a, c, T ) = v(a, T ) · v(c, T ) ||v(a, T )|| 2 ×||v(c, T )|| 2 ∈ [−1, 1].
If TopicCorr(a, c, T ) is close to 1, then topics that are similar or dissimilar to a are also similar or dissimilar to c. This is supported by many experimental observations that if some explanation of the a − c connection exists within T , then we anticipate that many T i ∈ T would share these similarity relationships.</p>
<p>Similarity of Best Topic Centroid</p>
<p>While the above metric attempts to find a trend within the entire topic model T , this metric attempts to find just a single topic T i ∈ T that is likely to explain the a − c connection. This metric is most similar to that depicted in Figure 2. Each T i is represented in the embedding space by taking a weighted centroid over its word probability distribution. We then rate each topic by averaging its similarity with both queried words. The score for the overall hypothesis is simply the highest score among the topics.</p>
<p>We define the centroid of T i as
Centr(T i ) = (w,p)∈Ti (w) · p,
and then compare it to both a and c through cosine similarity and euclidean distance. When comparing with CSim, we highly rank T i s with centroids located within the arc between (a) and (c). Because our embedding space identifies dimensions that help distinguish different types of objects, and because we trained a 500-dimensional embedding space, cosine similarity intuitively finds topics that share similar characteristics to both objects of interests. We define the best centroid similarity for CSim as
BestCentrCSim(a, c, T ) = max Ti∈T CSim(a, T i ) + CSim(c, T i ) 2 .
What we lose in the cosine similarity formulation is that clusters within our embedding space may be separate with respect to Euclidean distance but not cosine similarity. In order to evaluate the effect of this observation, we also formulate the best centroid metric with L 2 distance. In this formulation we look for topics who occur as close to the midpoint between (a) and (c) as possible. We express this score as a ratio between that distance and the radius of the sphere with diameter from (a) to (c). In order to keep this metric in a similar range to the others, we limit its range to [0, 1], namely, for the midpoint m = ( (a) + (c))/2,
BestCentrL 2 (a, c, T ) = max Ti∈T 1 − Centr(T i ) − m 2 m 2 .</p>
<p>Cosine Similarly of Best Topic Per Word</p>
<p>In a similar effort to the above centroid-based metric, we attempt to find topics which are related to a and c, but this time on a per-word (or phrase) basis using TopicSim(x, T i ) from Section 4.2. Now instead of looking across the entire topic model, we attempt to identify a single topic which is similar to both objects of interest. We do so by rating each topic by the lower of its two similarities, meaning the best topic overall will be similar to both query words.</p>
<p>BestTopicPerWord(a, c, T ) = max</p>
<p>Ti∈T min TopicSim(a, T i ), TopicSim(c, T i ) .</p>
<p>Network of Topic Centroids</p>
<p>A majority of the above metrics all rely on a single topic to describe the potential connection between a and c, but as Smalheizer points out in [37], a hypothesis may be best described as a "story" -a series of topics in our case. To model semantic connections between topics, we induce a nearest-neighbors network N from the set of vectors V = (a) ∪ (b) ∪ {Centr(T i )} Ti∈T which form the set of nodes for N . In this case, we set the number of neighbors per node to the smallest value (that may be different for each query) such that Figure 3: Above depicts two topic networks as described in Section 4.5. In this visualization, longer edges correspond to dissimilar neighbors. In red are objects a and c, which we queried to create these topics models. We observe that the connectivity between a and c from the published predicate is much higher than in the noisy example.</p>
<p>there exists a path from a to c. Using this topic network, we attempt to model the semantic differences between published and noise predicates using network analytic metrics.</p>
<p>We depict two such networks in Figure 3, and observe that the connectivity between a and c from a published predicate is substantially stronger and more structured. In order to quantify this observed difference, we measure the average betweenness and eigenvector centrality [31] of nodes along a shortest path from a to c (denoted by a ∼ c) within N to reflect possible information flow between T i ∈ T . This shortest path represents the series of links between key concepts present within our dataset that one might use to explain the relationship between a and c. We expect the connection linking a and c to be stronger if that path is more central to the topic network. Below we define metrics to quantify the differences in these topic networks. Such network analytic metrics are widely applied in semantic knowledge networks [41]. </p>
<p>Combination of Multiple Metrics</p>
<p>Each of the above methods are based on different assumptions regarding topic model or embedding space properties exhibited by published connections. To leverage each metric's strengths, we combined the top performing ones from each category into the following PolyMultiple method. We explored polynomial combinations in the form of i α i x βi i for ranges of α i ∈ [−1, 1] and β i ∈ [1, 3] after scaling each x i to the [0, 1] interval. Through a blackbox optimization technique, we searched over one-million parameter combinations, and present the best values Table 1. Omitted metrics were optimized with corresponding α i ≈ 0.</p>
<p>PolyMultiple(a, c, T ) = α 1 · L β1 2 + α 2 · BestCenterL β2 2 + α 3 · BestTopicPerWord(a, c, T ) β3 + α 4 · TopicCorr(a, c, T ) β4 α 5 · TopicWalkBtwn(a, c, T ) β5 + α 6 · TopicNetCCoef(a, c, T ) β6</p>
<p>Results and Lessons Learned</p>
<p>As described in Section 3, our goal is to distinguish publishable connections from noise. We run MOLIERE to generate topic models related to published, noise, and highly cited pairs. Using this information, we plot   HCvN). The core issue of this method is its sensitivity to the number of topics generated, and given that we generate 100 topics per pair, we likely drive down performance through topics which are unrelated to the query. Surprisingly, this metric is less able to distinguish highly-cited pairs, which we suppose is because highly-cited connections often bridge very distant concepts [35] and likely results in more noisy topic models. Additionally, we may be able to limit this noise by tuning the number of topics returned from a query, as described in Section 7. L 2 -based metrics exhibit even more surprising results. BestCentrL 2 performs poorly, with an ROC area of 0.578 (PvN) and 0.587 (HCvN), while the much simpler L 2 metric is exceptional, scoring a 0.783 (PvN) and 0.809 (HCvN). We note that if two words are related, they are more likely to be closer together in our vector space. We evaluate topic centroids based on their closeness to the midpoint between a and c, normalized by the distance between them, so if that distance is small, the radius from the midpoint is small as well. Therefore, it would seem that the distance between a and c is a better connection indication, and that the result of the centroid measurement is worse if this distance is small. CSim-based metrics are more straight-forward. The simple CSimmetric scores a 0.709 (PvN) and 0.703 (HCvN), which is interestingly consistent given that the L 2 metric increases in ROC area given highly-cited pairs. The BestTopicPerWord metric only scores a 0.686 (PvN), but increases substantially to 0.731 (HCvN). The topic centroid method BestCentroidCSim is the best cosine-based metric with an ROC area of 0.719 (PvN) and 0.742 (HCvN). This result is evidence that our initial hypothesis described in Figure 2 holds given cosine similarity, but as stated above, does not hold for euclidean distance.</p>
<p>Topic network metrics are all outperformed by simple L 2 , but we see interesting properties from their results that also help users to interpret generated hypotheses. For instance, we see that TopicWalkBetweeness is a negative indicator while TopicWalkEigen is positive. Looking at the example in Figure 3 we see that a and c are both far from the center of the network, connected to the rest of the topics through a very small number of high-betweenness nodes. In contrast, we see that in the network created from a published pair, the path from a to c is more central. We also see a denser clustering for the noise pair network, which is echoed by the fact that TopicNetCCoef and TopicNetModulatiry are both negative indicators. Lastly, we see that TopicWalkLength performs the best out of these network approaches, likely because it is most similar to the simple L 2 or CSim metrics. Combination of metrics, PolyMultiple, significantly outperforms all others with ROC areas of 0.834 (PvN) and 0.874 (HCvN). This is unsurprising because each other metric makes a different assumption about what sort of topic or vector configuration best indicates a published pair. When each is combined, we see not only better performance, but their relative importances. Looking into Table 1 we see that the two L 2 -based metrics are most important, followed by the topic network methods, and finally by TopicWalkCorr and BestTopicPerWord. Unsurprisingly, the coefficient signs correlate directly with whether each metric is a positive or negative indication as summarized in Table 2. Additionally, the ordering of importance roughly follows the same ordering as the ROC areas. Figure 4: The above ROC curves show the ability for each of our proposed methods to distinguish the MOLIERE results of published pairs from noise. We use our system to generate hypotheses regarding 8,638 pairs, half from each set, on publicly available data released prior to 2,015. We only show the best performing metrics from Section 4.5 for clarity. Figure 5: The above ROC curves show the ability for each of our proposed methods to distinguish the MOLIERE results of highly-cited pairs from noise. We identify 1,448 pairs who first occur in papers with over 100 citations published after our cut date. To plot the above ROC curve, we also select an random subset of equal size from the noise pairs.   (HCvN). Note, we report areas in the range [0.5, 1], so metrics marked with a (*) have been sorted in reverse order for the ROC calculations. This indicates that a higher score along a starred metric indicates a noise pair.</p>
<p>Metric</p>
<p>Related Work and Proposed Validation</p>
<p>The HG community struggles to validate its systems in a number of ways. Yetisgen-Yildiz and Pratt, in their chapter "Evaluation of Literature-Based Discovery Systems," outline four such methods (M1-M4) [7,55]. M1: Replicate Swanson's Experiments. Swanson, during his development of ARROWSMITH [39], worked alongside medical researchers to uncover a number of new connections. These connections include the link between Raynaud's Disease and Fish Oil [45], the link between Alzheimer's Disease and Estrogen [38] and the link between Migraine and Magnesium [47]. As discussed in [55], a number of projects have centered their validation effort around Swanson's results [16,26,11,5,53,44,19,32]. These efforts always rediscover a number of findings using information before Swanson's discovery date, and occasionally apply additional metrics such as precision and recall in order to quantify their results [14]. While limiting discussion to Swanson's discoveries reduces the domain of discovery drastically, at its core this method builds confidence in a new system through its ability to find known connections. We expand on this idea by validating automatically and on a massive scale, freeing our discourse from a single researcher's findings. M2: Statistical Evaluation. Hristovski et al. validate their system by studying a number of relationships and note their confidence and support with respect to the MEDLINE document set [18]. Then, they can generate potential relationships for the set of new connections added to UMLS [1] or OMIM [13]. By limiting their method to association rules, Hristovski et al. note that they can validate their system by predicting UMLS connections using data available prior to their publications. Therefore, this method is the similar to our own, but we notice that restricting discussion to only UMLS gene-disease connections results in a much smaller set than the predicate information present with SemMedDB.</p>
<p>Pratt et al. provide additional statistical validation for their system LitLinker [32]. This method also calculates precision and recall, but this time focusing on their B-set of returned results. Their system, like ARROWSMITH [39], returns a set of intermediate terms which may connect two queried entities. Pratt et al. run LitLinker for a number of diseases on which they establish a set of "gold standard" terms. Their method is validated based on its ability to list those gold-standard terms within its resulting B-sets. This approach requires careful selection of a (typically small) set of gold-standard terms, and is limited to "ABC" systems like ARROWSMITH, which are designed to identify term lists [36]. M3: Incorporating Expert Opinion. This ranges from comparisons between system output and expert output, such as the analysis done on the Manjal system [44], to incorporating expert opinion into goldstandard terms for LitLinker [32], to running actual experiments on potential results by Wren et al. [54]. Expert opinion is at the heart of many recent systems [51,49,27,10,46,2], including the previous version of our own. This process is both time consuming and risks introducing significant bias into the validation.</p>
<p>Spangler incorporates expert knowledge in a more sophisticated manner through the use of visualizations [42,43]. This approach centers around visual networks and ontologies produced automatically, which allows experts to see potential new connections as they relate to previously established information. This view is shared by systems such as DiseaseConnect [27] which generates sub-networks of ONIM and GWAS related to specific queries. Although these visualizations allow users to quickly understand query results, they do not lend themselves to a numeric and massive evaluation of system performance.</p>
<p>BioCreative is a set of challenges focused on assessing biomedical text mining, is the largest endeavor of its kind, to the best of our knowledge [17]. Each challenge centers around a specific task, such as mining chemical-protein interactions, algorithmically identifying medical terms, and constructing causal networks from raw text. Although these challenges are both useful and important, their tasks fall under the umbrella of information retrieval because they compare expert analysis with software results given the same text. M4: Publishing in the Medical Domain. This method is exceptionally rare and expensive. The idea is to take prevalent potential findings and pose them to the medical research community for another group to attempt. Swanson and Smalheiser use this technique, which solidifies many of their most prevalent findings, but we are not aware of any other group to attempt this.</p>
<p>An alternative to this idea is taken by Soldatova and Rzhetsky wherein a "robot scientist" automatically runs experiments posed by their HG system [40,34]. This system uses logical statements to represent their hypotheses, so new ideas can be posed through a series of implications. Going further, their system even identifies statements that would be the most valuable if proven true [35]. But, the scope of experiments that a robot scientists can undertake is limited; in their initial paper, the robot researcher is limited to small-scale yeast experiments. Additionally, many groups cannot afford the space and expense that an automated lab requires.</p>
<p>Deployment Challenges and Open Problems</p>
<p>Challenge Size. Our proposed validation challenge involves ranking millions of published and noise query pairs. But, in Section 5 we show our results on a randomly sampled subset of our overall challenge set. This was necessary due to performance limitations of MOLIERE, a system which initially required a substantial amount of time and memory to process even a single hypothesis. To compute these results, we ran 100 instances of MOLIERE, each on a 16 core, 64 GB RAM machine connected to a ZFS storage system. Unfortunately, performance limitations within ZFS created a bottleneck that both limited our results and drastically reduced cluster performance overall. So, our results represent a set of predicates that we evaluated in a limited time period. System Optimizations. While performing a keyword search, most network-centered systems are either I/O or memory bound simply because they must load and traverse large networks. In the case of MOLIERE, we initially spent hours trying to find shortest paths or nearby abstracts. But, we found a way to leverage our embedding space and our parallel file system in order to drastically improve query performance.</p>
<p>In brief, one can discover a relevant knowledge-network region by inducing a subnetwork on the set of keywords found within the hyperellipse focused between a and c. This increases performance because, given a parallel file system, creating an edge list for that subnetwork is a massively parallel task of order O(n/p). Additionally, when finding a path from a to c in that subnetwork, we can leverage the embeddings again to replace our shortest-path algorithm with A . We note that if such a path does not exist within our subnetwork, one can always increase the hyperellipse constant in order to broaden the subnetwork.</p>
<p>The overall effect of our optimizations reduced the wall-clock runtime of a single query from about 12 hours to about 5-7 minutes. Additionally, we reduced the memory requirement for a single query from over 400GB to under 16GB. Moreover, because the quality of our system'ss results is tunable through the hyperellipse constant, we note that our results presented here are nearly identical to our previous software. Highly Cited Predicates. Identifying highly sighted predicates requires that we synthesize information across multiple data sources. Although SemMedDB contains MEDLINE references for each predicate, neither contains citation information. For this, we turn to Semantic Scholar because not only do they track citations of medical papers, but they allow a free bulk download of metadata information (many other potential sources either provide a very limited API or none at all). In order to match Semantic Scholar data to MEDLINE citation, it is enough to match titles. This process allows us to get citation information for many MEDLINE documents, which in turn allow us to select predicates who's first occurrence was in highly cited papers.</p>
<p>We explored a number of thresholds for what constitutes "highly cited" and selected 100 because it was a round number that provided a sizable number of selected predicates. Because paper citations approximately follow a power-law distribution, any change would have drastically changed the size of this set. We note that the set of selected predicates was also limited by the quality of data in Semantic Scholar, and that the number of citations identified this was appeared to be substantially lower than that reported by other methods. Nevertheless, because our available citation count is a lower bound of the real value, we are assured that our "highly cited" set is actually well cited. According to our observations the least cited paper was cited more than 400 times in Google Scholar. Quality of Predicates. Through our above methods we learned that careful ranking methods can distinguish between published and noise predicates, but there is a potential inadequacy in this method. Likely, there exist a number of predicates within our published set which despite occurring in an abstract, are untrue. Additionally, it is possible that a noise predicate may be discovered to be true in the future. If our system ranks the published predicate which is untrue below the noise predicate which is, we worsen our system's ROC area. This same phenomena is addressed by Yetisgen-Yildiz and Pratt when they discuss the challenges present in validating literature-based discovery systems [55] -if a HG systems goal is to identify novel findings, then it should find different connections than human researchers.</p>
<p>We show through our results that despite an uncertain validation set, there are clearly core differences between publishable results and noise, which are evident at scale. Although there may be some false positives or negatives, we see through our meaningful ROC curves that they are far outnumbered by more standard predicates. Automatic Question Posting. Going forward we wish to study highly ranked noise predicates for the purpose of automatic question posing. This would mean that our system would search through its set of entities, run queries, and report the most promising potential new connections. In order to do this effectively we need to gain an understanding of how we can intelligently search local regions of our knowledge network and how to define locality for this task. Comparison with ABC Systems. Additionally, we would like to explore how our ranking methods apply to traditional ABC systems. Although there are clear limitations to these systems [36], many of the original systems such as ARROWSMITH follow the pattern. These systems typically output a list of target terms and linking terms, which could be thought of as a topic. If we were to take a pretrained embedding space, and treated a set of target terms like a topic, we could likely use our methods from Section 4 to validate any ABC system. Verb Prediction. We noticed, while processing SemMedDB predicates, that we can improve MOLIERE if we utilize verbs. SemMedDB provides a handful of verb types, such as "TREATS," "CAUSES," or "INTERACTS WITH," that suggest a concrete relationship between the subject and object of a sentence. MOLIERE currently outputs a topic model that can be interpreted using our new metrics, but does not directly state what sort of connection may exist between a and c. So, we want to see if we can accurately predict these verb types given only topic model information.</p>
<p>Interpretability of hypotheses remains one of the major problems in HG systems. Although topic-driven HG partially resolve this issue by producing a readable output, we still observe many topic models T (i.e., hypotheses) whose T i ∈ T are not intuitively connected with each other. While the proposed ranking is definitely helpful for understanding T , it still does not fully resolve the interpretability problem. One of our current research directions to tackle it is using text summarization techniques.</p>
<p>Conclusion</p>
<p>When we rely on experts to pose questions or perform experiments for the sake of validation, we drastically reduce the productivity of all involved. This is compounded by the need for software systems to rapidly iterate over time, gaining features and trying new methods to produce results. That work flow cannot afford the time of experts. Although we are not the first to identify the need for large-scale validation, we do propose a validation method which is impartial, does not need expert opinion, and operates at large scale. Furthermore, by selecting different cut years, we can even continue with our validation scheme long into the future, without manually selecting discoveries.</p>
<p>Additionally, our validation challenge can be taken by any system, not just an ABC system, so long as that system can rank its results. As we show in Section 4, even topic-driven systems can be adapted to produce such a ranking. Our validation methodology even extends across domains, as long as one can extract predicates. Overall, we see an immense need for validation which allows us to meaningfully compare performance across HG systems, or across scientific domains, and we are unaware of any other methodology for doing so.</p>
<p>To aid the biomedical HG community, we provide our code, query sets, and training data on-line at bit.ly/2EtVshN. With these resources, anyone could replicate our results using MOLIERE, or incorporate our ranking methods or validation framework into their own HG system.</p>
<p>Figure 2 :
2Figure 2: The above depicts two queries, a-c 1 and a-c 2 , where a-c 1 is a published connection and a-c 2 is a noise connection. We see topics for each query represented as diamonds via Centr(T i ). Although both queries lead to topics which are similar to a, c 1 , or c 2 , we find that the the presence of some topic which is similar to both objects of interest may indicate the published connection.</p>
<p>•
TopicWalkLength(a, c, T ): Length of shortest path a ∼ c • TopicWalkBtwn(a, c, T ): Avg. betweenness centrality of a ∼ c • TopicWalkEigen(a, c, T ): Avg. eigenvalue centrality of a ∼ c • TopicNetCCoef(a, c, T ): Clustering coefficient of N • TopicNetMod(a, c, T ): Modularity of N .</p>
<p>Table 1 :
1Best hyperparameters obtained via blackbox optimization for the PolyMultiple method. Note, each parameter was scaled to the [0, 1] interval before its input into the overall polynomial.ROC curves in Figures 4 and 5, and summarize the results in Table 2. These plots represent an analysis 
of 8,638 published vs. noise (PvN) pairs and 2,896 highly-cited vs. noise (HCvN) pairs (half of each set 
are noise). Although external factors limited the scale of our validation in both cases, we found that the 
resulting ROC areas are consistent for data sets of at least a few thousand pairs and different samplings. 
Topic Model Correlation metric (see Section 4.2) is a poorly performing metric with an ROC area of 
0.609 (PvN) and 0.496 (</p>
<p>Table 2 :
2The above summarizes all ROC area results for all considered metrics on the set of published vs. noise pairs (PvN) and highly-cited vs. noise pairs</p>
<p>Umls reference manual. Umls reference manual, 2009.</p>
<p>An automated framework for hypotheses generation using literature. V Abedi, R Zand, M Yeasin, F E , BioData Mining. 5113V. Abedi, R. Zand, M. Yeasin, and F. E. Faisal. An automated framework for hypotheses generation using literature. BioData Mining, 5(1):13, Aug 2012.</p>
<p>Signatures of mutational processes in human cancer. L B Alexandrov, S Nik-Zainal, D C Wedge, S A Aparicio, S Behjati, A V Biankin, G R Bignell, N Bolli, A Borg, A.-L Børresen-Dale, Nature. 5007463L. B. Alexandrov, S. Nik-Zainal, D. C. Wedge, S. A. Aparicio, S. Behjati, A. V. Biankin, G. R. Bignell, N. Bolli, A. Borg, A.-L. Børresen-Dale, et al. Signatures of mutational processes in human cancer. Nature, 500(7463):415-421, 2013.</p>
<p>A world full of surprises: Bayesian theory of surprise to quantify degrees of uncertainty. N Bencomo, A Belaggoun, Companion Proceedings of the 36th International Conference on Software Engineering. ACMN. Bencomo and A. Belaggoun. A world full of surprises: Bayesian theory of surprise to quantify degrees of uncertainty. In Companion Proceedings of the 36th International Conference on Software Engineering, pages 460-463. ACM, 2014.</p>
<p>Automatically identifying candidate treatments from existing medical literature. C Blake, W Pratt, AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases. C. Blake and W. Pratt. Automatically identifying candidate treatments from existing medical literature. In AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases, pages 9-13, 2002.</p>
<p>Latent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Journal of machine Learning research. 3D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993-1022, 2003.</p>
<p>Literature-based discovery. P Bruza, M Weeber, Springer Science &amp; Business MediaP. Bruza and M. Weeber. Literature-based discovery. Springer Science &amp; Business Media, 2008.</p>
<p>The textual organization of research paper abstracts in applied linguistics. M B Santos, Text-Interdisciplinary Journal for the Study of Discourse. 164M. B. Dos Santos. The textual organization of research paper abstracts in applied linguistics. Text- Interdisciplinary Journal for the Study of Discourse, 16(4):481-500, 1996.</p>
<p>Scalable topical phrase mining from text corpora. A El-Kishky, Y Song, C Wang, C R Voss, J Han, Proceedings of the VLDB Endowment. the VLDB Endowment8A. El-Kishky, Y. Song, C. Wang, C. R. Voss, and J. Han. Scalable topical phrase mining from text corpora. Proceedings of the VLDB Endowment, 8(3):305-316, 2014.</p>
<p>Genies: a natural-language processing system for the extraction of molecular pathways from journal articles. C Friedman, P Kra, H Yu, M Krauthammer, A Rzhetsky, Bioinformatics. 171supplC. Friedman, P. Kra, H. Yu, M. Krauthammer, and A. Rzhetsky. Genies: a natural-language processing system for the extraction of molecular pathways from journal articles. Bioinformatics, 17(suppl 1):S74- S82, 2001.</p>
<p>Using latent semantic indexing for literature based discovery. M D Gordon, S Dumais, M. D. Gordon and S. Dumais. Using latent semantic indexing for literature based discovery. 1998.</p>
<p>Ensemble non-negative matrix factorization methods for clustering protein-protein interactions. D Greene, G Cagney, N Krogan, P Cunningham, Bioinformatics. 2415D. Greene, G. Cagney, N. Krogan, and P. Cunningham. Ensemble non-negative matrix factorization methods for clustering protein-protein interactions. Bioinformatics, 24(15):1722-1728, 2008.</p>
<p>Online mendelian inheritance in man (omim), a knowledgebase of human genes and genetic disorders. A Hamosh, A F Scott, J S Amberger, C A Bocchini, V A Mckusick, Nucleic acids research. 33Database issueA. Hamosh, A. F. Scott, J. S. Amberger, C. A. Bocchini, and V. A. Mckusick. Online mendelian inheritance in man (omim), a knowledgebase of human genes and genetic disorders. Nucleic acids research, 33(Database issue), 2005.</p>
<p>Data mining: concepts and techniques. J Han, J Pei, M Kamber, ElsevierJ. Han, J. Pei, and M. Kamber. Data mining: concepts and techniques. Elsevier, 2011.</p>
<p>Efficient correlated topic modeling with topic embedding. J He, Z Hu, T Berg-Kirkpatrick, Y Huang, E P Xing, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMJ. He, Z. Hu, T. Berg-Kirkpatrick, Y. Huang, and E. P. Xing. Efficient correlated topic modeling with topic embedding. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 225-233. ACM, 2017.</p>
<p>Inferring undiscovered public knowledge by using text mining-driven graph model. G E Heo, K Lee, M Song, Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics. the ACM 8th International Workshop on Data and Text Mining in BioinformaticsG. E. Heo, K. Lee, and M. Song. Inferring undiscovered public knowledge by using text mining-driven graph model. In Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics, pages 37-37, 2014.</p>
<p>Overview of biocreative: critical assessment of information extraction for biology. L Hirschman, A Yeh, C Blaschke, A Valencia, BMC Bioinformatics. 611L. Hirschman, A. Yeh, C. Blaschke, and A. Valencia. Overview of biocreative: critical assessment of information extraction for biology. BMC Bioinformatics, 6(1):S1, May 2005.</p>
<p>Using literature-based discovery to identify disease candidate genes. D Hristovski, B Peterlin, J A Mitchell, S M Humphrey, International journal of medical informatics. 742D. Hristovski, B. Peterlin, J. A. Mitchell, and S. M. Humphrey. Using literature-based discovery to identify disease candidate genes. International journal of medical informatics, 74(2):289-298, 2005.</p>
<p>A semantic-based approach for mining undiscovered public knowledge from biomedical literature. X Hu, G Li, I Yoo, X Zhang, X Xu, Granular Computing, 2005 IEEE International Conference on. IEEE1X. Hu, G. Li, I. Yoo, X. Zhang, and X. Xu. A semantic-based approach for mining undiscovered public knowledge from biomedical literature. In Granular Computing, 2005 IEEE International Conference on, volume 1, pages 22-27. IEEE, 2005.</p>
<p>Bayesian surprise attracts human attention. L Itti, P F Baldi, Advances in neural information processing systems. L. Itti and P. F. Baldi. Bayesian surprise attracts human attention. In Advances in neural information processing systems, pages 547-554, 2006.</p>
<p>A Joulin, E Grave, P Bojanowski, M Douze, H Jégou, T Mikolov, Fasttext, arXiv:1612.03651zip: Compressing text classification models. arXiv preprintA. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, and T. Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016.</p>
<p>Semmeddb: a pubmed-scale repository of biomedical semantic predications. H Kilicoglu, D Shin, M Fiszman, G Rosemblat, T C Rindflesch, Bioinformatics. 2823H. Kilicoglu, D. Shin, M. Fiszman, G. Rosemblat, and T. C. Rindflesch. Semmeddb: a pubmed-scale repository of biomedical semantic predications. Bioinformatics, 28(23):3158-3160, 2012.</p>
<p>A study of cross-validation and bootstrap for accuracy estimation and model selection. R Kohavi, Ijcai. Stanford, CA14R. Kohavi et al. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Ijcai, volume 14, pages 1137-1145. Stanford, CA, 1995.</p>
<p>The rate of growth in scientific publication and the decline in coverage provided by science citation index. P O Larsen, M Von Ins, Scientometrics. 843P. O. Larsen and M. Von Ins. The rate of growth in scientific publication and the decline in coverage provided by science citation index. Scientometrics, 84(3):575-603, 2010.</p>
<p>Algorithms for non-negative matrix factorization. D D Lee, H S Seung, Advances in neural information processing systems. D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In Advances in neural information processing systems, pages 556-562, 2001.</p>
<p>Literature-based discovery by lexical statistics. R K Lindsay, M D Gordon, Journal of the Association for Information Science and Technology. 507574R. K. Lindsay and M. D. Gordon. Literature-based discovery by lexical statistics. Journal of the Association for Information Science and Technology, 50(7):574, 1999.</p>
<p>Diseaseconnect: a comprehensive web server for mechanism-based diseasedisease connections. C.-C Liu, Y.-T Tseng, W Li, C.-Y Wu, I Mayzus, A Rzhetsky, F Sun, M Waterman, J J Chen, P M Chaudhary, Nucleic acids research. 42W1C.-C. Liu, Y.-T. Tseng, W. Li, C.-Y. Wu, I. Mayzus, A. Rzhetsky, F. Sun, M. Waterman, J. J. Chen, P. M. Chaudhary, et al. Diseaseconnect: a comprehensive web server for mechanism-based disease- disease connections. Nucleic acids research, 42(W1):W137-W146, 2014.</p>
<p>Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing. Z Liu, Y Zhang, E Y Chang, M Sun, ACM Transactions on Intelligent Systems and Technology. 2326TISTZ. Liu, Y. Zhang, E. Y. Chang, and M. Sun. Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):26, 2011.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv:1301.3781, 2013.</p>
<p>. Ncbi Resource Coordinators, NCBI Resource Coordinators. PubMed. https://www.ncbi.nlm.nih.gov/pubmed/, 2017.</p>
<p>Networks: an introduction. M Newman, Oxford university pressM. Newman. Networks: an introduction. Oxford university press, 2010.</p>
<p>Litlinker: capturing connections across the biomedical literature. W Pratt, M Yetisgen-Yildiz, Proceedings of the 2nd international conference on Knowledge capture. the 2nd international conference on Knowledge captureACMW. Pratt and M. Yetisgen-Yildiz. Litlinker: capturing connections across the biomedical literature. In Proceedings of the 2nd international conference on Knowledge capture, pages 105-112. ACM, 2003.</p>
<p>Bayesian surprise and landmark detection. A Ranganathan, F Dellaert, Robotics and Automation, 2009. ICRA'09. IEEE International Conference on. IEEEA. Ranganathan and F. Dellaert. Bayesian surprise and landmark detection. In Robotics and Automa- tion, 2009. ICRA'09. IEEE International Conference on, pages 2017-2023. IEEE, 2009.</p>
<p>The big mechanism program: Changing how science is done. A Rzhetsky, A. Rzhetsky. The big mechanism program: Changing how science is done. 2016.</p>
<p>Choosing experiments to accelerate collective discovery. A Rzhetsky, J G Foster, I T Foster, J A Evans, Proceedings of the National Academy of Sciences. 11247A. Rzhetsky, J. G. Foster, I. T. Foster, and J. A. Evans. Choosing experiments to accelerate collective discovery. Proceedings of the National Academy of Sciences, 112(47):14569-14574, 2015.</p>
<p>Literature-based discovery: Beyond the abcs. N R Smalheiser, Journal of the Association for Information Science and Technology. 632N. R. Smalheiser. Literature-based discovery: Beyond the abcs. Journal of the Association for Infor- mation Science and Technology, 63(2):218-224, 2012.</p>
<p>Rediscovering don swanson: The past, present and future of literature-based discovery. N R Smalheiser, Journal of Data and Information Science. 24N. R. Smalheiser. Rediscovering don swanson: The past, present and future of literature-based discovery. Journal of Data and Information Science, 2(4):43-64, 2017.</p>
<p>Linking estrogen to alzheimer's disease an informatics approach. N R Smalheiser, D R Swanson, Neurology. 473N. R. Smalheiser and D. R. Swanson. Linking estrogen to alzheimer's disease an informatics approach. Neurology, 47(3):809-810, 1996.</p>
<p>Using arrowsmith: a computer-assisted approach to formulating and assessing scientific hypotheses. Computer methods and programs in biomedicine. N R Smalheiser, D R Swanson, 57N. R. Smalheiser and D. R. Swanson. Using arrowsmith: a computer-assisted approach to formulating and assessing scientific hypotheses. Computer methods and programs in biomedicine, 57(3):149-153, 1998.</p>
<p>Representation of research hypotheses. L N Soldatova, A Rzhetsky, Journal of biomedical semantics. 229L. N. Soldatova and A. Rzhetsky. Representation of research hypotheses. Journal of biomedical seman- tics, 2(2):S9, 2011.</p>
<p>Principles of semantic networks: Explorations in the representation of knowledge. J F Sowa, Morgan KaufmannJ. F. Sowa. Principles of semantic networks: Explorations in the representation of knowledge. Morgan Kaufmann, 2014.</p>
<p>Accelerating Discovery: Mining Unstructured Information for Hypothesis Generation. S Spangler, CRC Press37S. Spangler. Accelerating Discovery: Mining Unstructured Information for Hypothesis Generation, vol- ume 37. CRC Press, 2015.</p>
<p>Automated hypothesis generation based on mining scientific literature. S Spangler, A D Wilkins, B J Bachman, M Nagarajan, T Dayaram, P Haas, S Regenbogen, C R Pickering, A Comer, J N Myers, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningACMS. Spangler, A. D. Wilkins, B. J. Bachman, M. Nagarajan, T. Dayaram, P. Haas, S. Regenbogen, C. R. Pickering, A. Comer, J. N. Myers, et al. Automated hypothesis generation based on mining scientific literature. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1877-1886. ACM, 2014.</p>
<p>Text mining: generating hypotheses from medline. P Srinivasan, Journal of the American Society for Information Science and Technology. 555P. Srinivasan. Text mining: generating hypotheses from medline. Journal of the American Society for Information Science and Technology, 55(5):396-413, 2004.</p>
<p>Fish oil, raynaud's syndrome, and undiscovered public knowledge. Perspectives in biology and medicine. D R Swanson, 30D. R. Swanson. Fish oil, raynaud's syndrome, and undiscovered public knowledge. Perspectives in biology and medicine, 30(1):7-18, 1986.</p>
<p>Undiscovered public knowledge. The Library Quarterly. D R Swanson, 56D. R. Swanson. Undiscovered public knowledge. The Library Quarterly, 56(2):103-118, 1986.</p>
<p>Migraine and magnesium: eleven neglected connections. D R Swanson, Perspectives in biology and medicine. 314D. R. Swanson. Migraine and magnesium: eleven neglected connections. Perspectives in biology and medicine, 31(4):526-557, 1988.</p>
<p>An interactive system for finding complementary literatures: A stimulus to scientific discovery. D R Swanson, N R Smalheiser, Artif. Intell. 912D. R. Swanson and N. R. Smalheiser. An interactive system for finding complementary literatures: A stimulus to scientific discovery. Artif. Intell., 91(2):183-203, Apr. 1997.</p>
<p>Moliere: Automatic biomedical hypothesis generation system. J Sybrandt, M Shtutman, I Safro, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17New York, NY, USAACMJ. Sybrandt, M. Shtutman, and I. Safro. Moliere: Automatic biomedical hypothesis generation system. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17, pages 1633-1642, New York, NY, USA, 2017. ACM.</p>
<p>Global scientific output doubles every nine years. R Van Noorden, Nature News Blog. R. Van Noorden. Global scientific output doubles every nine years. Nature News Blog, 2014.</p>
<p>Finding complex biological relationships in recent pubmed articles using bio-lda. H Wang, Y Ding, J Tang, X Dong, B He, J Qiu, D J Wild, PloS one. 6317243H. Wang, Y. Ding, J. Tang, X. Dong, B. He, J. Qiu, and D. J. Wild. Finding complex biological relationships in recent pubmed articles using bio-lda. PloS one, 6(3):e17243, 2011.</p>
<p>Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification. P Wang, B Xu, J Xu, G Tian, C.-L Liu, H Hao, Neurocomputing. 174P. Wang, B. Xu, J. Xu, G. Tian, C.-L. Liu, and H. Hao. Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification. Neurocomputing, 174:806 -814, 2016.</p>
<p>Using concepts in literature-based discovery: Simulating swanson's raynaud-fish oil and migraine-magnesium discoveries. M Weeber, H Klein, L Jong-Van Den, R Berg, Vos, Journal of the Association for Information Science and Technology. 527M. Weeber, H. Klein, L. de Jong-van den Berg, R. Vos, et al. Using concepts in literature-based discovery: Simulating swanson's raynaud-fish oil and migraine-magnesium discoveries. Journal of the Association for Information Science and Technology, 52(7):548-557, 2001.</p>
<p>Knowledge discovery by automated identification and ranking of implicit relationships. J D Wren, R Bekeredjian, J A Stewart, R V Shohet, H R Garner, Bioinformatics. 203J. D. Wren, R. Bekeredjian, J. A. Stewart, R. V. Shohet, and H. R. Garner. Knowledge discovery by automated identification and ranking of implicit relationships. Bioinformatics, 20(3):389-398, 2004.</p>
<p>Evaluation of literature-based discovery systems. M Yetisgen-Yildiz, W Pratt, Literature-based discovery. SpringerM. Yetisgen-Yildiz and W. Pratt. Evaluation of literature-based discovery systems. In Literature-based discovery, pages 101-113. Springer, 2008.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6106 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6106</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6106</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-62729cff7dda7614f648a84e8967076d8878a5ff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/62729cff7dda7614f648a84e8967076d8878a5ff" target="_blank">ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is thought that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not for complete evaluations of papers or proposals.</p>
                <p><strong>Paper Abstract:</strong> Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the"better"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6106.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6106.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ErrorDetection-ShortPapers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detection of deliberately inserted errors in short computer-science papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation protocol that tests an LLM's ability to identify a single deliberate mathematical or conceptual error in short, synthetic CS papers; uses targeted prompts and multiple responses per prompt to flag incorrect claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Provide the full short paper (or parts of it) to GPT-4 using targeted prompts (Prompt-Direct, Prompt-OneShot with a worked example, and Prompt-Parts supplying the paper sentence-by-sentence); request step-by-step checking; run 3 independent responses per prompt and consider a paper flagged if any response detects an error; responses collected via ChatGPT (May 3 and May 12 builds).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary per-paper error detection (did the LLM identify the inserted/key incorrect claim), plus qualitative counts of false positives (LLM flags correct claims as incorrect); categorization of error types (mathematical, logical, conceptual, missing proof vs. provable), and analysis of failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (primary); pilot comparisons included Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM but only GPT-4 succeeded on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computer science / peer review (theoretical proofs, algorithms, statistics, and logical arguments in short CS papers).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Short papers (13 constructed examples) each containing one intentionally inserted key error (e.g., incorrect implication in a proof, false inference from data, incorrect complexity bound). The task evaluates whether the LLM can detect that an asserted claim is false or unjustified given the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 detected the error in 7 out of 13 constructed short papers; it produced occasional false positives; in the 6 failures, the papers typically omitted full proofs so detection required external/deductive knowledge the model did not apply. Other tested LLMs failed to detect any of the errors in a pilot.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>A synthetic benchmark of 13 short CS papers with one gold-standard inserted error each (plus a pilot short paper); example responses archived by the authors (GitHub repository referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Authors note the 7/13 detection rate is comparable to historical findings about human reviewers missing flaws (cites provided); ground truth labels for inserted errors are author-provided (gold-standard). Direct side-by-side formal comparison with human reviewers is not reported beyond this qualitative statement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires access to the complete deductive proof to detect some errors; sensitivity to prompt formulation; occasional false positives; model-specific limitation (other LLMs performed poorly); inability to resolve claims that need information outside provided text (e.g., datasets/figures); no calibrated confidence scores reported; small, synthetic dataset limits generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6106.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6106.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChecklistVerification-NeurIPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verification of closed-ended author checklist items in NeurIPS papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted verification task where GPT-4 is given the relevant sections of real papers and asked to answer closed-ended checklist questions; compares LLM answers to manually labeled ground truth and to authors' self-reported checklist answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For 15 NeurIPS papers, select checklist items where authors answered 'Yes'; provide the relevant paper section(s) to GPT-4 and prompt it to answer the closed-ended checklist question. Collect 3 independent responses per (question,paper) pair and take the majority answer as the LLM's response. Compare LLM answers to a manual ground-truth labeling performed by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy of LLM answers against manual ground truth (percentage correct across pairs), analysis of disagreements with authors' self-reported answers, and classification of error reasons (e.g., undeterminable from text, misreading figures).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Scientific peer-review / reporting compliance (computer-science conference papers).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Verification of 16 closed-ended checklist questions (e.g., reproducibility, experiments, claims) across selected NeurIPS papers; the LLM's task is to decide whether the paper text supports the checklist statement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across 119 unique {checklist question, paper} pairs, the LLM achieved 86.6% accuracy (majority over 3 responses) versus the manually labeled ground truth. Coincidentally, 86.6% of author responses matched the manual ground truth. When LLM and author answers disagreed with ground truth, 75% of those mismatched author responses also disagreed with the LLM. Approximately 50% of LLM errors were due to items that were undeterminable from the provided text (e.g., requiring figure interpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>119 labeled {checklist question, paper} pairs derived from 15 NeurIPS submissions (authors manually labeled ground truth); authors' checklist responses included for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Ground truth was produced by the paper's authors (manual labeling). Authors note coincidence that the fraction of author answers matching ground truth equals LLM accuracy; no large-scale human reviewer comparison beyond manual labels is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Many checklist items require information not present in provided plain text (figures, supplementary materials). The approach depends on which sections are supplied; LLMs may misinterpret or miss details in figures/tables; potential for overconfidence on undeterminable items; no assessment of calibrated uncertainty; reliance on majority over small number of stochastic responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6106.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6106.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AbstractPairwiseSelection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise comparative evaluation of abstracts (choose the better paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment that tests GPT-4's ability to select the better of two abstracts intentionally constructed so that one is clearly superior, including adversarial distractions (buzzwords, bombastic phrasing, author names), to assess judgement and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Present GPT-4 with 10 pairs of abstracts where one abstract was constructed to be clearly superior (e.g., additional result, correct interpretation), sometimes adding distracting features to the inferior abstract (buzzwords, bombastic language, author/algorithm names). Ask GPT-4 to choose the better abstract and justify the decision.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness rate in selecting the superior abstract (absolute count/percentage), qualitative analysis of failure modes (bias toward positive framing, being swayed by language, prompt-injection susceptibility), and case studies of successes and failures.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Peer review / editorial decision-making (comparative assessment of submissions and proposals).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Pairs of abstracts where one is objectively superior (by construction), used to evaluate whether the LLM can make comparative quality judgments robust to superficial distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 performed poorly: it erred in 6 out of the 10 pairs. It succeeded in 4 cases (e.g., identifying incorrect interpretation of a null result, correctly interpreting upper bounds, resisting buzzwords, ignoring author identities). Failures included bias toward positive results, misinterpreting parameter ranges and bounds, falling victim to a prompt-injection attack, being swayed by bombastic language in the inferior abstract, and being influenced by an algorithm's name.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>A small constructed set of 10 abstract pairs with designed superiority and adversarial distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No formal human baseline reported for these specific 10 pairs; authors argue GPT-4 is not yet reliable for full comparative evaluations used in selective venues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Small sample size; vulnerability to rhetorical/buzzword manipulation and prompt-injection; systematic biases (positive-result bias); difficulty with subtle scientific judgment and contextual interpretation; inability to serve as standalone decision-maker for selective evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6106.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6106.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptingStrategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Targeted prompting strategies for eliciting reviewing behavior from LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparison of different prompt templates (direct asking, one-shot with an example, and progressive parts) and protocol choices (multiple stochastic responses, majority or any-response aggregation) to improve LLM performance on review-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare three prompts: Prompt-Direct (ask to check full paper and think step-by-step), Prompt-OneShot (same but include an illustrative example with a revealed flaw), and Prompt-Parts (feed the paper sentence-by-sentence and ask for incremental checking). For each prompt collect multiple (three) responses and analyze per-prompt and aggregated performance (flag if any response detects an error).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task success rate (error detection, checklist accuracy), incidence of false positives, and qualitative usefulness of responses; also measure differences in sensitivity across prompts and whether example-based prompting improves detection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>LLM prompting / evaluation methodology for review tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Prompt templates designed to induce chain-of-thought style, targeted questioning and incremental evidence consumption to improve detection of incorrect claims and checklist verification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Targeted prompts (asking specific verification questions) outperformed generic 'write a review' prompts. Prompt-OneShot sometimes helped (provided a concrete example of the kind of flaw to find). Prompt-Parts was useful for some deductive issues where incremental context helps. Aggregating multiple responses (considering any positive detection) improved overall detection coverage. Exact per-prompt effectiveness varied across paper types.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Same synthetic 13 short papers and other constructed tasks used in study; three responses per prompt per paper were collected and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Prompt engineering is evaluated relative to baseline of asking for a generic review (human reviewers not directly compared on prompting strategies), but authors note that targeted questions yield responses significantly more useful than broad review prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Strong sensitivity to prompt wording; stochasticity across responses necessitates multiple queries; potential ethical/privacy/questions with exposing chain-of-thought; varying effectiveness by task type; no systematic calibration of confidence or automated aggregation beyond majority/any-flag rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6106",
    "paper_id": "paper-62729cff7dda7614f648a84e8967076d8878a5ff",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "ErrorDetection-ShortPapers",
            "name_full": "Detection of deliberately inserted errors in short computer-science papers",
            "brief_description": "An evaluation protocol that tests an LLM's ability to identify a single deliberate mathematical or conceptual error in short, synthetic CS papers; uses targeted prompts and multiple responses per prompt to flag incorrect claims.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Provide the full short paper (or parts of it) to GPT-4 using targeted prompts (Prompt-Direct, Prompt-OneShot with a worked example, and Prompt-Parts supplying the paper sentence-by-sentence); request step-by-step checking; run 3 independent responses per prompt and consider a paper flagged if any response detects an error; responses collected via ChatGPT (May 3 and May 12 builds).",
            "evaluation_criteria": "Binary per-paper error detection (did the LLM identify the inserted/key incorrect claim), plus qualitative counts of false positives (LLM flags correct claims as incorrect); categorization of error types (mathematical, logical, conceptual, missing proof vs. provable), and analysis of failure modes.",
            "llm_model_name": "GPT-4 (primary); pilot comparisons included Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM but only GPT-4 succeeded on this task.",
            "theory_domain": "Computer science / peer review (theoretical proofs, algorithms, statistics, and logical arguments in short CS papers).",
            "theory_description": "Short papers (13 constructed examples) each containing one intentionally inserted key error (e.g., incorrect implication in a proof, false inference from data, incorrect complexity bound). The task evaluates whether the LLM can detect that an asserted claim is false or unjustified given the provided text.",
            "evaluation_results": "GPT-4 detected the error in 7 out of 13 constructed short papers; it produced occasional false positives; in the 6 failures, the papers typically omitted full proofs so detection required external/deductive knowledge the model did not apply. Other tested LLMs failed to detect any of the errors in a pilot.",
            "benchmarks_or_datasets": "A synthetic benchmark of 13 short CS papers with one gold-standard inserted error each (plus a pilot short paper); example responses archived by the authors (GitHub repository referenced).",
            "comparison_to_human": "Authors note the 7/13 detection rate is comparable to historical findings about human reviewers missing flaws (cites provided); ground truth labels for inserted errors are author-provided (gold-standard). Direct side-by-side formal comparison with human reviewers is not reported beyond this qualitative statement.",
            "limitations_or_challenges": "Requires access to the complete deductive proof to detect some errors; sensitivity to prompt formulation; occasional false positives; model-specific limitation (other LLMs performed poorly); inability to resolve claims that need information outside provided text (e.g., datasets/figures); no calibrated confidence scores reported; small, synthetic dataset limits generalizability.",
            "uuid": "e6106.0",
            "source_info": {
                "paper_title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ChecklistVerification-NeurIPS",
            "name_full": "Verification of closed-ended author checklist items in NeurIPS papers",
            "brief_description": "A targeted verification task where GPT-4 is given the relevant sections of real papers and asked to answer closed-ended checklist questions; compares LLM answers to manually labeled ground truth and to authors' self-reported checklist answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "For 15 NeurIPS papers, select checklist items where authors answered 'Yes'; provide the relevant paper section(s) to GPT-4 and prompt it to answer the closed-ended checklist question. Collect 3 independent responses per (question,paper) pair and take the majority answer as the LLM's response. Compare LLM answers to a manual ground-truth labeling performed by the authors.",
            "evaluation_criteria": "Accuracy of LLM answers against manual ground truth (percentage correct across pairs), analysis of disagreements with authors' self-reported answers, and classification of error reasons (e.g., undeterminable from text, misreading figures).",
            "llm_model_name": "GPT-4",
            "theory_domain": "Scientific peer-review / reporting compliance (computer-science conference papers).",
            "theory_description": "Verification of 16 closed-ended checklist questions (e.g., reproducibility, experiments, claims) across selected NeurIPS papers; the LLM's task is to decide whether the paper text supports the checklist statement.",
            "evaluation_results": "Across 119 unique {checklist question, paper} pairs, the LLM achieved 86.6% accuracy (majority over 3 responses) versus the manually labeled ground truth. Coincidentally, 86.6% of author responses matched the manual ground truth. When LLM and author answers disagreed with ground truth, 75% of those mismatched author responses also disagreed with the LLM. Approximately 50% of LLM errors were due to items that were undeterminable from the provided text (e.g., requiring figure interpretation).",
            "benchmarks_or_datasets": "119 labeled {checklist question, paper} pairs derived from 15 NeurIPS submissions (authors manually labeled ground truth); authors' checklist responses included for comparison.",
            "comparison_to_human": "Ground truth was produced by the paper's authors (manual labeling). Authors note coincidence that the fraction of author answers matching ground truth equals LLM accuracy; no large-scale human reviewer comparison beyond manual labels is reported.",
            "limitations_or_challenges": "Many checklist items require information not present in provided plain text (figures, supplementary materials). The approach depends on which sections are supplied; LLMs may misinterpret or miss details in figures/tables; potential for overconfidence on undeterminable items; no assessment of calibrated uncertainty; reliance on majority over small number of stochastic responses.",
            "uuid": "e6106.1",
            "source_info": {
                "paper_title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "AbstractPairwiseSelection",
            "name_full": "Pairwise comparative evaluation of abstracts (choose the better paper)",
            "brief_description": "An experiment that tests GPT-4's ability to select the better of two abstracts intentionally constructed so that one is clearly superior, including adversarial distractions (buzzwords, bombastic phrasing, author names), to assess judgement and robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Present GPT-4 with 10 pairs of abstracts where one abstract was constructed to be clearly superior (e.g., additional result, correct interpretation), sometimes adding distracting features to the inferior abstract (buzzwords, bombastic language, author/algorithm names). Ask GPT-4 to choose the better abstract and justify the decision.",
            "evaluation_criteria": "Correctness rate in selecting the superior abstract (absolute count/percentage), qualitative analysis of failure modes (bias toward positive framing, being swayed by language, prompt-injection susceptibility), and case studies of successes and failures.",
            "llm_model_name": "GPT-4",
            "theory_domain": "Peer review / editorial decision-making (comparative assessment of submissions and proposals).",
            "theory_description": "Pairs of abstracts where one is objectively superior (by construction), used to evaluate whether the LLM can make comparative quality judgments robust to superficial distractions.",
            "evaluation_results": "GPT-4 performed poorly: it erred in 6 out of the 10 pairs. It succeeded in 4 cases (e.g., identifying incorrect interpretation of a null result, correctly interpreting upper bounds, resisting buzzwords, ignoring author identities). Failures included bias toward positive results, misinterpreting parameter ranges and bounds, falling victim to a prompt-injection attack, being swayed by bombastic language in the inferior abstract, and being influenced by an algorithm's name.",
            "benchmarks_or_datasets": "A small constructed set of 10 abstract pairs with designed superiority and adversarial distractions.",
            "comparison_to_human": "No formal human baseline reported for these specific 10 pairs; authors argue GPT-4 is not yet reliable for full comparative evaluations used in selective venues.",
            "limitations_or_challenges": "Small sample size; vulnerability to rhetorical/buzzword manipulation and prompt-injection; systematic biases (positive-result bias); difficulty with subtle scientific judgment and contextual interpretation; inability to serve as standalone decision-maker for selective evaluation.",
            "uuid": "e6106.2",
            "source_info": {
                "paper_title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PromptingStrategies",
            "name_full": "Targeted prompting strategies for eliciting reviewing behavior from LLMs",
            "brief_description": "A comparison of different prompt templates (direct asking, one-shot with an example, and progressive parts) and protocol choices (multiple stochastic responses, majority or any-response aggregation) to improve LLM performance on review-like tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compare three prompts: Prompt-Direct (ask to check full paper and think step-by-step), Prompt-OneShot (same but include an illustrative example with a revealed flaw), and Prompt-Parts (feed the paper sentence-by-sentence and ask for incremental checking). For each prompt collect multiple (three) responses and analyze per-prompt and aggregated performance (flag if any response detects an error).",
            "evaluation_criteria": "Task success rate (error detection, checklist accuracy), incidence of false positives, and qualitative usefulness of responses; also measure differences in sensitivity across prompts and whether example-based prompting improves detection.",
            "llm_model_name": "GPT-4",
            "theory_domain": "LLM prompting / evaluation methodology for review tasks",
            "theory_description": "Prompt templates designed to induce chain-of-thought style, targeted questioning and incremental evidence consumption to improve detection of incorrect claims and checklist verification.",
            "evaluation_results": "Targeted prompts (asking specific verification questions) outperformed generic 'write a review' prompts. Prompt-OneShot sometimes helped (provided a concrete example of the kind of flaw to find). Prompt-Parts was useful for some deductive issues where incremental context helps. Aggregating multiple responses (considering any positive detection) improved overall detection coverage. Exact per-prompt effectiveness varied across paper types.",
            "benchmarks_or_datasets": "Same synthetic 13 short papers and other constructed tasks used in study; three responses per prompt per paper were collected and analyzed.",
            "comparison_to_human": "Prompt engineering is evaluated relative to baseline of asking for a generic review (human reviewers not directly compared on prompting strategies), but authors note that targeted questions yield responses significantly more useful than broad review prompts.",
            "limitations_or_challenges": "Strong sensitivity to prompt wording; stochasticity across responses necessitates multiple queries; potential ethical/privacy/questions with exposing chain-of-thought; varying effectiveness by task type; no systematic calibration of confidence or automated aggregation beyond majority/any-flag rules.",
            "uuid": "e6106.3",
            "source_info": {
                "paper_title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.014250249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing</h1>
<p>Ryan Liu and Nihar Shah<br>{ryanliu, nihars}@andrew.cmu.edu<br>Carnegie Mellon University</p>
<h4>Abstract</h4>
<p>Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in the reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks:</p>
<ol>
<li>Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors.</li>
<li>Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an $86.6 \%$ accuracy.</li>
<li>Choosing the "better" paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs.
Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.</li>
</ol>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have recently been found to excel in many different domains. Their success in general purpose tasks also raises the natural question: can they be used for reviewing scientific papers (or proposals)? Peer review is highly strained due to fast increasing numbers of submissions and overburdening of reviewers [McC06; Sha22]. It is estimated that millions of hours of researchers' time is spent in reviewing [The13]. Furthermore, several controlled experiments have found that flawed papers frequently get past human reviewers [Bax+98; GGM98; Sch+04; Sch+08]. LLMs hold a considerable potential in relieving some of these issues in the scientific review process.</p>
<p>With this motivation, we conduct an exploratory study on whether and how LLMs can be used for reviewing. We first conduct a pilot (Appendix A) to select the model and prompting strategies. In the pilot, we first compared various models (GPT-4 [Ope23b], Bard [Man23], Vicuna [The23], Koala [Gen+23], Alpaca [Tao+23], LLaMa [Tou+23], Dolly [Con+23], OpenAssistant [LAI23], and StableLM [Sta23]) by asking them to find errors in a short paper. We found that GPT-4 was the only model that was successful at this task. We also piloted prompting strategies, and found that by asking the model targeted questions, the responses generated are significantly more useful compared to requesting it to simply 'write a review'.</p>
<p>With these choices in place, we then evaluated the following three reviewing tasks on GPT-4, employing targeted prompts for each task:</p>
<p>Can LLMs identify errors in papers? (Section 3) A primary objective of the scientific reviewing process is to identify any shortcomings in scientific manuscripts and ensure that only robust and accurate scientific content gets published. To evaluate the LLM's efficacy in achieving this objective, we constructed 13 short papers, intentionally infusing each of them with a key error. The errors spanned from mathematical errors (e.g., wrong implication of mean squared convergence) to logical fallacies (e.g., false inference drawn from a data analysis). The LLM identified the errors in 7 of the 13 short papers. ${ }^{1}$ This rate is comparable to that of human peer reviewers [Bax+98; GGM98; Sch+04; Sch+08].</p>
<p>Can LLMs verify author-provided checklists? (Section 4) Many conferences and journals provide authors with a checklist of items that helps set expectations from papers and also offers a guideline to authors to meet these expectations. In venues such as NeurIPS, ${ }^{2}$ the authors need to submit their responses to the checklist along with the submission. The checklist items are verified by reviewers, and failing to properly adhere to the checklist can get the paper rejected. We investigate if LLMs can do this job of verifying whether the checklist items are accurately reported by the authors. In a set of 15 papers from NeurIPS 2023, we selected checklist items where the authors answered "Yes", manually labeled their ground truth answers, and prompted the LLM to answer the same checklist items by providing the relevant section(s) of the paper. We found that across 119 unique {checklist question, paper} pairs, (i) taking the majority answer over 3 responses, the LLM achieved $86.6 \%$ accuracy compared to the ground truth label; (ii) coincidentally, $86.6 \%$ of author responses matched our ground truth label; (iii) LLM answers disagreed with $75 \%$ of mismatched author responses, and (iv) $50 \%$ of the LLM's errors were due to questions where the answers were undeterminable with the text in the paper (e.g., requiring information provided in figures).</p>
<p>Can LLMs select the better of two papers? (Section 5) With the successful performance of LLMs on the aforementioned two specific tasks, we moved on to a more ambitious question of whether LLMs can select the better of two papers in a selective venue ${ }^{3}$ or the better of two proposals in a grant evaluation setting. To this end, we considered arguably the simplest of tasks: For a given pair of abstracts where one abstract is constructed to be superior to another (e.g., both abstracts are identical but one has an additional result), can the LLM identify the better abstract? We constructed 10 such pairs of abstracts. In a subset of these pairs, we also inserted certain distractions such as the use of bombastic language or buzzwords in the inferior abstract. We then tested the LLM's ability to accurately identify the superior abstract in these scenarios. We found that the LLM performs surprisingly poorly at this task, erring in 6 of the 10 cases. The four cases where it succeeded involved identifying an abstract's incorrect interpretation of a null result, accurately interpreting upper bounds, remaining unaffected by buzzwords, and disregarding author identities. On the other hand, the six failures encompassed a bias towards positive results, misinterpreting parameter ranges, misinterpreting lower bounds, falling victim to a prompt injection attack, being swayed by bombastic language in the inferior abstract, and getting influenced by the name of the algorithm.</p>
<p>Overall, our results suggest that LLMs (particularly GPT-4 at its current stage) have considerable promise in being used for specific parts of the review process, but are not yet capable of functioning as a standalone reviewer.</p>
<h1>2 Related work</h1>
<p>In this section we discuss literature that is most closely related to our work.
Large language models. Recent advances in LLMs have spurred the investigation of the general capabilities of these models [Bub+23; Ope23b; Ani+23]. On one hand, natural language processing (NLP) researchers have designed methods to address and improve the factuality and reasoning capabilities of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>LLMs [Du+23; Wei+22; Xia+22; ZJH23]. On the other hand, investigations on the capabilites of LLMs have spread from NLP to various domains, such as agent simulation, data generation, computational social science, human-computer interaction, and health/law.</p>
<p>Under simulation, LLM generative capabilities have been used for embodied agent planning [Son+22; Wan+23] and human agent simulation [Par+23]. Data generation has seen advances in either simplifying or fully automating the creation of questions and labels [Par+22; Liu+22; SS21; Wan+21]. In computational social science, there have been general studies [Zie+23] as well as targeted investigations within areas such as theory of mind [Ull23] and moral judgment [Jin+22]. Human-computer interaction has explored using LLMs to generate believable accounts of HCI experiences [HTK23]. Finally, LLMs have also been used in health and biomedical applications [Liu+23; Che+23a; Che+23b] and legal processes [Nay23].</p>
<p>Automation in peer review. The peer review process in many venues already has a considerable amount of automation deployed. The part of the review process that involves the most automation is that of assigning suitable reviewers to papers. A number of algorithms have been used to compute the expertise of reviewers for papers [CZ13; Wie+19; Coh+20], and various algorithms have also been used to assign reviewers to papers in a manner that ensures a high expertise of the assigned reviewers [CZ13; SSS21b; KSM19]. Following recent discoveries of "collusion rings" [Vij20; Lit21] in peer review, several venues have incorporated randomization into their assignment algorithms [Jec+20] to mitigate this issue.</p>
<p>A number of venues have used algorithms [NSP21] to address the issue of subjectivity or commensuration bias [Lee15]. Some venues have also used algorithms to address the issue of miscalibration [GWG13], however, these calibration algorithms have not worked well [Sha22, Section 5]. Outside of computer science, there are algorithms designed specifically for checking that the submitted papers follow suitable submission and reporting guidelines [Hou16; FMG19]. Various other general computational tools are used in the peer-review processes, e.g., plagiarism checkers, scripts to detect conflicts of interest based on databases like DBLP, etc. Some previous attempts at designing machine learning algorithms to conduct comprehensive paper reviews have shown limited success in generating appropriate reviews, but these algorithms have proven useful in other capacities such as generating paper summaries [Hua18; Wan+20; YLN21].</p>
<p>There are also nascent investigations into using language models for peer review and adjacent areas. The paper [HH23] summarizes the desired roles of agents and objectives in peer review, and provide recommendations for the safe use of LLMs in this context. [Zha+22] use RoBERTa [Liu+19] to assess the textual fluency of papers and sentiment of reviews in an analysis of peer review fairness. And in peer grading, [Mor+23] use a subset of rater data in massively open online courses to finetune distilBERT [San+20], and use it to validate peer grading scores.</p>
<p>Peer-review datasets. This paper also contributes a small-scale dataset for evaluating machine learning / language models for reviewing papers. Any LLMs subsequently designed may be evaluated on the 13 short papers and the pilot short paper we have constructed with inserted "gold standard" flaws, where we currently find that only GPT-4 has any success in detecting errors. Similarly, LLMs may be evaluated on the 119 {checklist question, paper} pairs we have labeled. On the 10 pairs of abstracts we have constructed, their accuracy on direct comparative evaluations and their resistance to distractions can be examined. Of course, this assumes that the LLM is not itself trained on this very paper. In addition, our aforementioned constructions can also be used for few-shot learning or prompting of these models.</p>
<p>We now review other useful datasets pertaining to (peer) reviewing. Given the fact that the assignment of reviewers to papers is the most automated part of peer review, the paper [Ste+23] releases a "gold standard" dataset of expertise of reviewers for papers to train and/or evaluate such algorithms. Motivated by the absence of any datasets for the problem of mitigating collusion rings, the paper [Jec+23] releases a dataset on collusion strategies constructed from a mock peer-review exercise. Another issue is strategic behavior of reviewers where they manipulate reviews they are providing (e.g., lower scores of papers they are reviewing) to improve the chances of their own paper, and the paper [SSS21a] releases a dataset of such strategies also obtained from a mock peer-review exercise. The paper $[\mathrm{Ker}+20]$ releases anonymized review ratings and author-provided feedback.</p>
<p>In the past few years, platforms such as OpenReview.net and conferences such as ICLR and NeurIPS release the peer reviews along with the papers. Such reviews have been compiled and released as useful datasets [Kan+18]. The release of reviews is not limited to the field of computer science alone: scipost.org and f1000research.com outside computer science also release the peer reviews publicly. Subsequently, a number of studies have released datasets that annotate the reviews (and discussion posts), including annotations based on the arguments [Hua+19; Fro+20; Ken+21; Kuz+22; Gho+22] or sentiments [CGM20; Bul+20].</p>
<h1>3 Can LLMs detect errors in computer science papers?</h1>
<p>In this section, we discuss our experiment to investigate whether LLMs can identify flaws in short computer science papers. Based on our pilot (Appendix A.1) in which GPT-4 is the only model that detected the error, we use GPT-4 as the LLM for these experiments.</p>
<h3>3.1 Methods</h3>
<p>We constructed 13 short papers (detailed in Section 3.3). In each of these papers, we deliberately inserted an error, encompassing mathematical mistakes to conceptual fallacies. We then asked the LLM to identify if there were any errors in the short paper. We used the following three prompts:</p>
<ul>
<li>Prompt-Direct: You are an expert reviewer for a scientific conference. You will be provided with a short version of a paper that contains the setting of the paper and the main claims. Please check for the validity and correctness of these claims, and in particular, report if you can figure out if any of these claims is false based on the information provided in this short paper. Think step by step when checking each claim. Here is the short paper: "..."</li>
<li>Prompt-OneShot: You are an expert reviewer for a scientific conference. You will be provided with a short version of a paper that contains the setting of the paper and the main claims. Please check for the validity and correctness of these claims, and in particular, report if you can figure out if any of these claims is false based on the information provided in this short paper. Think step by step when checking each claim.
You will first be provided with an example. Consider this example short paper:
We consider the problem of searching in a list of numbers, where the list is already sorted in a non-decreasing order. If the number exists in the list, then the search should return the position of that number in the list (if there are multiple copies of the number in the list, then it can return any one position). If the number does not exist in the list, then it should output 'does not exist'. The number to be searched is specified by the user. For this problem, we develop a new artificial intelligence based algorithm and also prove theoretical guarantees for it. Specifically, we show that our algorithm requires only $\operatorname{sqrt}(\mathrm{m})$ comparisons between pairs of numbers, where m is the size of the entire list. We also provide an associated informationtheoretic lower bound showing that our bound is tight, i.e., any algorithm will need at least these many comparisons in the worst case. We conduct a large number of simulations that confirm this theoretical result. Our simulations vary the size of the list, the values in the list (drawing them from various distributions), as well as the value to be searched. Our result is especially important as it is rare for such practical artificial intelligence algorithms to also have such strong theoretical guarantees. We hope practitioners will take note of this new result.</li>
</ul>
<p>The review of this short paper should point out the error that searching in a sorted list of numbers needs only $\log (\mathrm{m})$ comparisons (via binary search), and hence the paper's claim that they provide a $\operatorname{sqrt}(\mathrm{m})$ worst case lower bound is false.</p>
<p>Now, please check for the validity and correctness of the claims in the following short paper, and in particular, report if you can figure out if any of these claims is false based on the information provided in this short paper. Think step by step when checking each claim. Here is the short paper: "..."</p>
<ul>
<li>Prompt-Parts: You are an expert reviewer for a scientific conference. You will be provided with a short version of a paper that contains the setting of the paper and the main claims. Please check for the validity and correctness of these claims, and in particular, report if you can figure out if any of these claims is false based on the information provided in this short paper. You will be provided the paper one set of sentences at a time.
Here is the first set of sentences of the paper: "..." Does this contain any incorrect claim? Think step by step to reason out your answer.
Here is the next set of sentences of the paper: "..." Based on the context of the previous sentences, does this contain any incorrect claim or does it invalidate any claim made in the previous sentences of this paper? Think step by step to reason out your answer.</li>
</ul>
<p>Note that Prompt-Parts does not provide the entire short paper at once, but instead provides parts of the paper a few sentences at a time. In particular, for the sentences where the main claims are made, each provided part comprises a single sentence. In the prompts for both this and the next experiment, the phrase "think step by step" is inspired by $[\mathrm{Koj}+23]$. For this section's experiments, we access the GPT-4 model through ChatGPT (May 3 and May 12 builds) [Ope23a].</p>
<h1>3.2 Summary of results</h1>
<p>In Table 1, we provide a summary of our results. We queried the LLM for three responses per prompt and we provide an evaluation of each response as well as an overall quantification of the performance. In the 'overall' row and column in the table, we consider it as a $\checkmark$ if any of the responses to any of the prompts was a $\checkmark$. This is because in practice, one can obtain multiple responses to multiple prompts and flag the paper if any of them detect an error.</p>
<p>We find that the LLM (GPT-4) can detect the error in 7 of the 13 short papers. Furthermore, in each of the 6 papers where it failed to detect the error, the paper did not contain the complete proof for the claim, thereby disallowing the LLM to detect a deductive error, and requiring it to figure out the flaw based on extraneous knowledge. We also find that the LLM occasionally outputs false positives, that is, falsely claims a correct part of the paper to be incorrect.</p>
<p>In passing, it is worth noting that we subsequently also tried other models-Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM. For prompting, we used Prompt-Direct. None of these models were able to identify the error in any of the papers. Additionally, some of these other models raised concerns reminiscent of the proverbial "Reviewer #2", making unwarranted critiques such as "the flaw in this paper is that they do not conduct experiments in addition to theoretical results" or even more nonsensical comments.</p>
<h3>3.3 Paper and response details</h3>
<p>In this section, we describe the 13 papers we constructed. Along with each paper, we provide one example response from the LLM. Additional responses from the LLM are available at https://github.com/ niharshah/ReviewerGPT2023</p>
<h3>3.3.1 Bias/fairness</h3>
<p>The following constructed paper rebuts another fictitious paper (Reference 1) that analyzed data involving Simpson's paradox. The paper has a logical fallacy, and we investigate whether the LLM can identify it.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Paper topic</th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt-Direct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt-OneShot</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt-Parts</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1.</td>
<td style="text-align: center;">Bias/fairness</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2.</td>
<td style="text-align: center;">Non-parametric regression</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3.</td>
<td style="text-align: center;">Sorting (harder)</td>
<td style="text-align: center;">$\times!$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">4.</td>
<td style="text-align: center;">Sorting (easier)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">5.</td>
<td style="text-align: center;">Noisy pairwise comparisons (harder)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">6.</td>
<td style="text-align: center;">Noisy pairwise comparisons (easier)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">7.</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">8.</td>
<td style="text-align: center;">Game theory</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$ !</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times!$</td>
<td style="text-align: center;">$\checkmark!$</td>
<td style="text-align: center;">$\checkmark$ !</td>
</tr>
<tr>
<td style="text-align: center;">9.</td>
<td style="text-align: center;">Error correcting codes</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">10.</td>
<td style="text-align: center;">Optimization</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">11.</td>
<td style="text-align: center;">Clustering</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">12.</td>
<td style="text-align: center;">Distinguishing styles: $80 \%$ accuracy</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">13.</td>
<td style="text-align: center;">Distinguishing styles: $50 \%$ accuracy</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$5 \checkmark$</td>
<td style="text-align: center;">$8 \times$</td>
<td style="text-align: center;">1 !</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$6 \checkmark$</td>
<td style="text-align: center;">$7 \times$</td>
<td style="text-align: center;">1 !</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$7 \checkmark$</td>
<td style="text-align: center;">$6 \times$</td>
<td style="text-align: center;">1 !</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of the LLM in detecting incorrect claims in short papers we constructed. We show the performance for the 3 generated responses for each of the three prompts. The notation $\checkmark$ indicates that the LLM correctly detected the error, $\times$ indicates that it did not detect the error, and ! indicates a false alarm.</p>
<p>The purpose of this paper is to rebut the claims of the paper [Reference 1] and show that their claims are false. For the reader not aware of [Reference 1], we first provide a brief background.</p>
<p>A new company, called FairAIrecruiter, is helping four companies - let's call these companies as Company I, Company II, Company III, and Company IV - to automate their hiring process as follows:</p>
<ul>
<li>Any applicant who wishes to apply to either of these four companies must submit their resume to FairAIrecruiter.</li>
<li>FairAIrecruiter has put a constraint that any applicant can apply to only one of the four companies.</li>
<li>FairAIrecruiter collects all resumes until a certain deadline and then processes them. The goal of this processing is to choose applicants whom each company will then interview.</li>
<li>Company I and Company II have told FairAIrecruiter that they will interview the top $10 \%$ of applicants who have applied to Company I and Company II respectively. Company III and Company IV have told FairAIrecruiter that they will interview the top $25 \%$ of applicants who have applied to Company III and Company IV respectively.</li>
</ul>
<p>Now, FairAIrecruiter uses machine learning to decide the set of applicants to be interviewed. Specifically, FairAIrecruiter has designed a function h:All possible resumes $-&gt;[0,1]$, where the label space is the predicted 'quality' of the applicant (higher is better). For each of the four companies, FairAIrecruiter follows the following procedure. FairAIrecruiter takes the set of all applicants for that company, computes the predicted quality for each applicant using the designed function h, and selects the top applicants in terms of the predicted quality. The number of applicants selected for each tech company is determined by the aforementioned percentages that the company has supplied.</p>
<p>After completion of this process, the authors of [Reference 1] started investigating if FairAIrecruiter is really fair. They considered two groups of people (where the groups were determined according to a specific demographic attribute). Call them group A and group B. The authors of [Reference</p>
<p>1] found out that there were a total of 1000 applicants belonging to group A and 1000 applicants belonging to group B. They also found out that the distribution of qualities of the 1000 group A applicants is identical to the distribution of the qualities of the 1000 group B applicants. They then found that, however, 220 applicants from group A received interview calls whereas only 130 applicants from group B received interview calls. They thus concluded that an applicant from group A was nearly twice as likely to receive an interview call under FairAIrecruiter's system even though the applicants in the two groups had identical distributions of qualities. The paper [Reference 1] then accused FairAIrecruiter's algorithm of a clear bias against group B.</p>
<p>In the present paper, we delve deeper into this issue. We take a deeper look at the data. In what follows, for each of the four companies, we list out the total number of applicants from group A, the number of applicants from group A invited for interview, the total number of applicants from group B, and the number of applicants from group B invited for interview respectively:</p>
<ul>
<li>Company I: 100, 10, 400, 40</li>
<li>Company II: 100, 10, 400, 40</li>
<li>Company III: 400, 100, 100, 25</li>
<li>Company IV: 400, 100, 100, 25</li>
</ul>
<p>Crucially, observe that for each company, the rate at which applicants from group A were invited for interview is identical to the rate at which applicants from group B were invited for interview. There is thus no bias, thereby falsifying the claim made in [Reference 1].</p>
<p>While the mathematical statements made in [Reference 1] - that of being 1000 applicants in total from either group, and 220 group A and 130 group B applicants receiving interview calls - are correct, and seem to indicate evidence of bias, we have actually found there is no bias. As a consequence, this result therefore implies that the remaining part of the argument used by [Reference 1] to arrive at their false conclusion - their claim that the distribution of qualities of the 1000 group A applicants is identical to the distribution of the qualities of the 1000 group B applicants - must be false. We thus encourage more future studies on robust and unbiased assessments.</p>
<p>The paper has a logical fallacy in the claimed implication that the remaining part of the argument of Reference 1 must be false. The LLM successfully identifies this fallacy. Here is an example of its response to Prompt-OneShot.</p>
<p>The claims of the short paper can be analyzed as follows:</p>
<ol>
<li>
<p>${ }^{<em> </em>}$ Claim: For each company, the rate at which applicants from group A were invited for interview is identical to the rate at which applicants from group B were invited for interview, thereby showing no bias.** This claim appears to be correct, based on the data presented in the paper. Indeed, for each company, the ratio of applicants from group A and B that were invited for an interview is the same:</p>
</li>
<li>
<p>Company I: $10 / 100=40 / 400$</p>
</li>
<li>Company II: $10 / 100=40 / 400$</li>
<li>Company III: $100 / 400=25 / 100$</li>
<li>Company IV: $100 / 400=25 / 100$</li>
</ol>
<p>These ratios align with the promised interview rates by each company ( $10 \%$ for Companies I and II, $25 \%$ for Companies III and IV), meaning the process is fair when viewed on a per-company basis.</p>
<ol>
<li>${ }^{<em> </em>}$ Claim: The conclusion of [Reference 1] that the system is biased against group B is false.** Based on the information presented in the paper, this claim seems to be correct. It is true that, in aggregate, fewer candidates from group B received interview calls ( 130 vs 220 for group A). However, when the numbers are broken down by company, it is clear that the same proportion of candidates from each group received calls for each company. The difference in aggregate numbers is due to the distribution of applicants from each group among the companies, not due to any discernible bias in the process.</li>
<li>${ }^{<em> </em>}$ Claim: The assertion by [Reference 1] that the distribution of qualities of the 1000 group A applicants is identical to the distribution of the qualities of the 1000 group B applicants must be false.**</li>
</ol>
<p>This claim is more complex and doesn't necessarily hold up under scrutiny. The claim that the distribution of qualities is the same among group A and B applicants does not necessarily contradict the observed outcomes. The result could indeed be due to a fair selection process, but with differing applicant distribution among the companies. The more substantial number of group B applicants applied to Companies I and II, which have a lower interview rate ( $10 \%$ ), leading to a lower aggregate call rate for group B. This is not indicative of a difference in the distribution of qualities within each group.</p>
<p>In summary, the paper's claims regarding the lack of bias in FairAIrecruiter's system seem valid, based on the provided data. However, the claim that the distribution of applicant qualities must differ between groups A and B due to the observed outcomes is not necessarily correct. The outcomes could be explained by the differing distribution of applicants between the companies, rather than differing applicant qualities.</p>
<h1>3.3.2 Non-parametric regression</h1>
<p>The content of the following paper is fairly mathematically involved, describing a statistical model followed by some properties of an estimator.</p>
<p>We consider a statistical model of the form ' $\mathrm{y} . \mathrm{i}=\mathrm{f}(\mathrm{x} . \mathrm{i})+\mathrm{epsilon} . \mathrm{i}$ ' for i ranging from 1 through n , where n is the sample size. Here, for each i in 1 through $\mathrm{n}, \mathrm{x} . \mathrm{i}$ is a known and given vector in d-dimensional Euclidean space, and y_i is a real value. Each vector x_i is chosen independently and uniformly at random from $[-1,1]^{\wedge} \mathrm{d}$. The notation f denotes an unknown function from $[-1,1]^{\wedge} \mathrm{d}$ to R . It is known that the function f is coordinate-wise non-increasing, but no other information about it is known, and it can be arbitrary. The terms epsilon_1,...,epsilon_n denote n i.i.d. random variables which follow the standard Gaussian distribution; the values of these are not known. The goal is to estimate the function f from the observations y_1,...,y_n (and using the knowledge of x_1,...,x_n).</p>
<p>Although this problem seems hard at first, we show that a simple estimator - one that chooses the function f from the coordinate-wise monotonic class that best fits the data under the squared loss asymptotically recovers the true f. Specifically, letting $\backslash$ hat ${\mathrm{f}} _$n denote the estimate of the function when there are n samples, we show that integral_ ${\mathrm{x}$ in $[-1,1]^{\wedge} \mathrm{d}}(\mathrm{f}-\backslash$ hat ${\mathrm{f}}<em>\mathrm{n})^{\wedge} 2 \mathrm{dx}$ approaches 0 as n goes to infinity. As an immediate implication of this result, we have that the sequence Probability $(\backslash$ hat ${\mathrm{f}}</em>\mathrm{n}=\mathrm{f})$ converges to 1 as n goes to infinity. This completes the theoretical guarantees. Furthermore, our algorithm is computationally efficient (e.g., executes in a matter of minutes on a standard laptop when $\mathrm{n}=10,000$ ), thereby making it incredibly useful in practice.</p>
<p>The paper incorrectly claims that the mean squared convergence of the estimator $\hat{f}<em _infty="\infty" _rightarrow="\rightarrow" n="n">{n}$ to $f$ directly implies $\lim </em>=f\right)=1$. Under Prompt-Direct the LLM does not recognize this issue. In two}$ Probability $\left(\hat{f}_{n</p>
<p>of the three attempts with Prompt-OneShot, the LLM hints at this issue. Using Prompt-Parts, the LLM correctly figures out that this is an incorrect claim. Here is a response to the problematic sentence "As an immediate implication of this result, we have that the sequence Probability $(\backslash$ hat ${\mathrm{f}} . \mathrm{n}=\mathrm{f})$ converges to 1 as n goes to infinity." under Prompt-Parts.</p>
<p>This claim seems to be a misunderstanding or misstatement of the previous results. While the authors previously stated that the integral over the d-dimensional cube of the squared difference between the true function f and the estimate $\backslash$ hat ${\mathrm{f}}$.n approaches zero as n goes to infinity, it doesn't imply that the probability of $\backslash$ hat ${\mathrm{f}}$.n being exactly equal to f converges to 1 .</p>
<p>Here is a step-by-step breakdown:</p>
<ol>
<li>The authors claim that the sequence Probability $(\backslash$ hat ${\mathrm{f}} . \mathrm{n}=\mathrm{f})$ converges to 1 as n goes to infinity. This would imply that the estimator $\backslash$ hat ${\mathrm{f}}$.n is exactly equal to the true function f with probability approaching 1 as the sample size n goes to infinity.</li>
<li>However, the previous result, integral. ${\mathrm{x}$ in $[-1,1]^{\wedge} \mathrm{d}}(\mathrm{f}-\backslash$ hat ${\mathrm{f}} . \mathrm{n})^{\wedge} 2 \mathrm{dx}$ approaches 0 as n goes to infinity, doesn't guarantee that $\backslash$ hat ${\mathrm{f}}$.n will be exactly equal to f . This integral represents the expected squared error over the entire domain of the function. While this expected error approaches zero as n goes to infinity, it does not guarantee that the error is exactly zero, which would be required for $\backslash$ hat ${\mathrm{f}}$.n to be exactly equal to f .</li>
<li>In statistical estimation theory, even as sample size goes to infinity, there is generally still some degree of uncertainty about the true parameter value. In this case, even though $\backslash$ hat ${\mathrm{f}}$.n may be a good estimate of f , the probability that they are exactly equal is not necessarily 1 , especially since the function f can be arbitrary and we are dealing with continuous space where exact equality is a strong condition.</li>
<li>The authors appear to be conflating the idea of an estimator converging in mean square (which is what the integral suggests) with the idea of an estimator being equal to the true value with probability 1 . These are not the same thing.</li>
</ol>
<p>Therefore, based on the context and the principles of statistical theory, this claim appears to be incorrect or, at the very least, a misinterpretation of the previous results.</p>
<h1>3.3.3 Sorting (harder)</h1>
<p>This paper considers the standard problem of sorting an array, but assumes some additional conditions on the array which will make no difference to the orderwise complexity of the sorting problem.</p>
<p>Suppose you have an array of R distinct real-valued numbers. Suppose you wish to sort the array in increasing order. The sorting algorithm can only query for comparisons of the form 'is [value of number at a certain position] greater than [value of number at a different position]?'. Suppose that such queries are guaranteed to get accurate Yes/No answers. In this setting, it is well known that the worst-case number of comparisons required to perform this sorting task is of order $\mathrm{R}^{*} \log (\mathrm{R})$.</p>
<p>We begin with a brief summary of our result and then delve into more detail. We consider a variant of this problem that puts certain restrictions on the possible values that the list can take. We then design an algorithm that, under these assumed conditions, requires only order sqrt(R)<em> $\log (\mathrm{R})$ comparisons in the worst case rather than order $\mathrm{R}^{</em>} \log (\mathrm{R})$. It is important to note that even though</p>
<p>the worst case complexity of sorting is said to be order $\mathrm{R}^{<em>} \log (\mathrm{R})$, our result does not violate this lower bound since our result applies only in a restricted setting whereas the $\mathrm{R}^{</em>} \log (\mathrm{R})$ result is when the values can be arbitrary.</p>
<p>In what follows, we provide more details about the setting we assume in this work. We suppose that $R$ is an even number. We also suppose that the $R$ values in the array may not be arbitrary real-valued numbers. Instead, these values must be greater than 9, each value must be a part of the Fibonacci sequence (and hence must be integers), and must not be a prime number. Furthermore, we assume that the items in positions $1, \ldots, \mathrm{R} / 2$ all have values smaller than the items in positions $\mathrm{R} / 2+1, \ldots, \mathrm{R}$. For settings where these conditions hold, we use properties of the Fibonacci sequence and additional carefully crafted arguments to design a deterministic algorithm which we prove requires at most $\operatorname{sqrt}(\mathrm{R})^{*} \log (\mathrm{R})$ comparisons in the worst case. Of course, one may wonder in what applications these specific conditions actually arise, and at this point we do not have an answer to this. Admittedly, our result presently is of purely theoretical interest. In future work, we hope to relax the conditions we assumed for this result and move towards making our result more practically applicable.</p>
<p>The LLM fails to recognize that the inclusion of additional restrictions has no impact on the necessity of conducting at least order $\mathrm{R}^{*} \log (\mathrm{R})$ comparisons in the worst-case. It is important to note that the paper did not provide a comprehensive proof of this assertion. Our objective here is to evaluate whether the LLM can directly discern the futility of the additional restrictions. In addition, the LLM once raised a false alarm under Prompt-Direct regarding the Fibonacci condition:</p>
<p>The paper sets forth an intriguing claim about a sorting algorithm that works under very specific conditions, stating that it can perform the sort operation with complexity of $\operatorname{sqrt}(\mathrm{R})^{<em>} \log (\mathrm{R})$ as opposed to the typical $R^{</em>} \log (R)$. It's important to verify these claims within the constraints of the conditions presented in the paper.</p>
<p>Let's proceed by examining the claims step by step:</p>
<ol>
<li>${ }^{<em> </em>}$ Setting*<em>: The paper starts by stating a classic fact about comparison-based sorting algorithms. Given an array of $R$ distinct real-valued numbers, the worst-case complexity for sorting the array is indeed $\mathrm{O}\left(\mathrm{R}^{</em>} \log (\mathrm{R})\right)$. This statement is a well-established fact in computer science.</li>
<li>
<p>${ }^{<em> </em>}$ Claim*<em>: The authors suggest that they have found an algorithm that can sort in $\mathrm{O}(\operatorname{sqrt}(\mathrm{R})^{</em>} \log (\mathrm{R}))$ comparisons under specific conditions. The conditions are:</p>
</li>
<li>
<p>R is an even number</p>
</li>
<li>The values are not arbitrary real-valued numbers, but they are greater than 9, each value is a part of the Fibonacci sequence, and they are not prime numbers</li>
<li>The items in positions $1, \ldots, \mathrm{R} / 2$ all have values smaller than the items in positions $\mathrm{R} / 2+1, \ldots, \mathrm{R}$</li>
</ol>
<p>It is crucial to note here that the claim doesn't seem to violate any known theoretical lower bound because it applies to a restricted subset of the sorting problem.</p>
<p>Let's take a deeper look at the constraints for the claim to hold:</p>
<ul>
<li>${ }^{<em> </em>} \mathrm{R}$ is an even number**: This condition doesn't seem problematic. We can certainly have lists of even length.</li>
<li>${ }^{<em> </em>}$ Values are part of the Fibonacci sequence, are greater than 9, and are not prime**: This</li>
</ul>
<p>condition is problematic. The first few Fibonacci numbers are $0,1,1,2,3,5,8,13,21,34,55$, 89 , and so on. Among these, 13 is a prime number, which would not be allowed by the conditions. However, the next Fibonacci number, 21, is greater than 9 and is not a prime number. But, there's an issue: the next Fibonacci number, 34, is also not a prime number, but if we wanted a list with more than two numbers, we wouldn't have any additional Fibonacci numbers that aren't prime and are greater than 9. The next Fibonacci number, 55, is a prime number and thus would be excluded by the conditions. Therefore, for any R greater than 2 , there are not enough numbers that satisfy the conditions set by the paper.</p>
<ul>
<li><em><em>Items in positions $1, \ldots, \mathrm{R} / 2$ all have values smaller than the items in positions $\mathrm{R} / 2+1, \ldots, \mathrm{R}^{</em> </em>}$ : Given the condition above, this is only possible for $\mathrm{R}=2$, where one number is smaller than the other. For any $\mathrm{R}&gt;2$, this condition can't be satisfied because there aren't enough numbers that meet the conditions.</li>
</ul>
<p>In conclusion, the claims made in the paper are not valid for any R greater than 2 , due to the constraints set on the numbers that can be included in the list. The conditions defined in the paper are too restrictive to allow for a list of size greater than 2 .</p>
<h1>3.3.4 Sorting (easier)</h1>
<p>This short paper is identical to the "Sorting - harder" paper in the previous section, but with fewer restrictions on the array.</p>
<p>Suppose you have an array of R distinct real-valued numbers. Suppose you wish to sort the array in increasing order. The sorting algorithm can only query for comparisons of the form 'is [value of number at a certain position] greater than [value of number at a different position]?'. Suppose that such queries are guaranteed to get accurate Yes/No answers. In this setting, it is well known that the worst-case number of comparisons required to perform this sorting task is of order $\mathrm{R}^{*} \log (\mathrm{R})$.</p>
<p>We begin with a brief summary of our result and then delve into more detail. We consider a variant of this problem that puts certain restrictions on the possible values that the list can take. We then design an algorithm that, under these assumed conditions, requires only order $\operatorname{sqrt}(\mathrm{R})^{<em>} \log (\mathrm{R})$ comparisons in the worst case rather than order $\mathrm{R}^{</em>} \log (\mathrm{R})$. It is important to note that even though the worst case complexity of sorting is said to be order $\mathrm{R}^{<em>} \log (\mathrm{R})$, our result does not violate this lower bound since our result applies only in a restricted setting whereas the $\mathrm{R}^{</em>} \log (\mathrm{R})$ result is when the values can be arbitrary.</p>
<p>In what follows, we provide more details about the setting we assume in this work. We assume each of the R values is greater than 9 . We use carefully crafted arguments to design a deterministic algorithm for which we prove that when the aforementioned assumption holds, requires at most order $\operatorname{sqrt}(\mathrm{R})^{*} \log (\mathrm{R})$ comparisons in the worst case. Of course, one may wonder in what applications these specific conditions actually arise, and at this point we do not have an answer to this. Admittedly, our result presently is of purely theoretical interest. In future work, we hope to relax the conditions we assumed for this result and move towards making our result more practically applicable.</p>
<p>Observe that the only restriction on the array is that all the $R$ values are greater than 9. The LLM still does not recognize that this simple restriction will make no difference to requirement of at least order $\mathrm{R}^{<em>} \log (\mathrm{R})$ comparisons in the worst case, and will disallow an algorithm requiring only $\operatorname{sqrt}(\mathrm{R})^{</em>} \log (\mathrm{R})$ comparisons. To its credit, the LLM does say that it is impossible to verify this claim without additional details. Here is an example response to Prompt-OneShot.</p>
<p>The short paper details a theoretical approach to a sorting problem, where the authors claim they've designed an algorithm that, under certain conditions, reduces the worst-case number of comparisons from order $\mathrm{R}^{<em>} \log (\mathrm{R})$ to order $\operatorname{sqrt}(\mathrm{R})^{</em>} \log (\mathrm{R})$. The condition specified is that each of the R real-valued numbers in the array is greater than 9 .</p>
<p>From the information provided, it seems that the claim is theoretically possible under the specific condition stated. The claim doesn't violate the established lower bound for the general sorting problem (order $\mathrm{R}^{*} \log (\mathrm{R})$ ) because the authors explicitly state their result applies only in a restricted setting. Therefore, the claim appears to be valid within the context they have provided.</p>
<p>However, there are several important considerations to note:</p>
<ol>
<li>The lack of practical application: The authors themselves admit that their result is purely theoretical at this point, and that they are unaware of any practical applications where the conditions for their result arise. This is important to consider when evaluating the impact of their work.</li>
<li>Need for a detailed proof: Although the authors state they have a deterministic algorithm that meets the described complexity, the brief summary doesn't provide the proof. It's crucial to ensure the proof is rigorous and valid.</li>
<li>No empirical validation: The authors haven't mentioned any empirical validation, such as simulations or testing, of their algorithm. This is an important step in verifying the effectiveness of an algorithm in practice.</li>
</ol>
<p>While the authors' claim seems to be theoretically possible and doesn't violate any known theoretical bounds, the points above should be addressed in a full paper review. A detailed examination of their proposed algorithm, the associated proof, and any empirical validation would be necessary to fully evaluate the validity and correctness of the claims.</p>
<h1>3.3.5 Noisy pairwise comparison model (harder)</h1>
<p>This short paper describes a statistical model for noisy pairwise comparisons that is a generalization of the popular Bradley-Terry-Luce (BTL) model [BT52; Luc59]. It makes certain claims about estimation under this model.</p>
<p>We consider a statistical model where there are d items. Each item i (in $1, \ldots, \mathrm{~d}$ ) is associated with an unknown real-valued parameter w_i. The parameters w_1,...,w_d can take any arbitrary distinct values. We obtain n samples, where each sample is obtained in the following manner. First pick two distinct items uniformly at random (without replacement) from the set of d items. Then return a comparison between the two items - the comparison must say whether item i is better than j (i.e., item i has a higher parameter value than item j) or whether item j is better than item i (i.e., item j has a higher parameter value than item i). These comparisons are noisy. The noise is governed by a known, increasing function $\mathrm{f}: \mathrm{R}-&gt;\mathrm{R}$ as follows: if items i and j are compared, then the probability with which item i is said to be better than item j is given by $\mathrm{f}(\mathrm{w} . \mathrm{i}) /(\mathrm{f}(\mathrm{w} . \mathrm{i})+\mathrm{f}(\mathrm{w} . \mathrm{j}))$, and otherwise item j is said to be better than item i. All samples (i.e., comparisons) are statistically mutually independent.</p>
<p>For this model, we design an algorithm to estimate the values of the parameters w_1,...,w_d from the n samples. We consider the squared loss function, that is, for any estimate $\backslash$ hat ${\mathrm{w}}$ and true value w of the parameters, we define the loss as sum $_{i}=1}^{d}{\mathrm{~d}}(\backslash$ hat ${\mathrm{w}} . \mathrm{i}-\mathrm{w} . \mathrm{i})^{\wedge} 2$. We consider</p>
<p>any arbitrary (but known) function f that satisfies the following three requirements: (a) it should be twice differentiable, (b) it should be 1-Lipschitz, and (c) it should be bijective.</p>
<p>We prove that if the function f satisfies the three aforementioned requirements, the risk (i.e., expected value of the loss) incurred by our algorithm is guaranteed to be at most $2 \mathrm{~d} / \operatorname{sqrt}{\mathrm{n}}$. We do not have matching lower bounds yet, but we conjecture that our upper bound is tight up to a positive universal constant. We have also released a python package implementing our algorithm, and are currently also implementing it in other programming languages.</p>
<p>It is well known in the literature that in the absence of additional assumptions, accurate estimation of the parameters is impossible (e.g., "the minimax error for estimating [the parameters] will diverge to infinity if we are allowed to consider models [without additional restrictions]" [Sha+16]). Here is an example to illustrate this issue. Suppose the function $f$ is the identity function, which meets all the three requirements imposed on it by the paper. Then the probability of item $i$ beating item $j$ reduces to $\frac{w_{i}}{w_{i}+w_{j}}$. Now, for any value $z \neq 0$, if the non-zero parameters $\left(w_{1}, \ldots, w_{d}\right)$ are replaced by $\left(z w_{1}, \ldots, z w_{d}\right)$, the probability distributions of the pairwise comparison outcomes do not change. Thus it is impossible to distinguish any non-zero parameters $\left(w_{1}, \ldots, w_{d}\right)$ from $\left(z w_{1}, \ldots, z w_{d}\right)$ for any $z \neq 0$ from this data given any number of samples, and therefore an error upper bound of $\frac{2 d}{\sqrt{n}}$ is not possible. The LLM does not recognize this error nor does it recover this issue from the literature. However, it does clarify that it cannot verify the claim as the full proof is not provided. Here is a response to Prompt-Direct.</p>
<p>Given that this is a statistical paper, it's difficult to definitively say whether any of these claims are false without a detailed examination of the underlying proofs and empirical evidence. However, based on the information provided, there are a few key areas that can be evaluated for plausibility and consistency with known principles.</p>
<ol>
<li>The statistical model: The paper describes a statistical model for comparing d items based on an unknown parameter w_i associated with each item. The comparison is noisy, but the noise follows a known, increasing function f. There is nothing inherently implausible about this model setup based on the provided information. As long as the conditions stated for function f are satisfied, the model appears reasonable.</li>
<li>The algorithm: The authors propose an algorithm to estimate the parameter values from n samples. Given the model setup, this is plausible. However, the paper does not provide any details on the workings of the algorithm, so it's impossible to check the correctness of the algorithm or its suitability for the problem at hand.</li>
<li>The loss function: The paper uses a squared loss function, which is a common choice in many statistical problems, as it emphasises large errors over small ones. This is plausible.</li>
<li>Risk bound: The authors claim that if function f satisfies certain conditions, the risk (expected value of the loss) incurred by their algorithm is at most $2 \mathrm{~d} / \operatorname{sqrt}{\mathrm{n}}$. Without seeing the proof, it's impossible to say whether this claim is correct. However, the fact that the risk decreases with increasing n (the number of samples) is consistent with the intuition that more data usually leads to better estimates.</li>
<li>No lower bounds yet: The authors mention that they don't have lower bounds yet for the risk. This is an important part of the analysis, as lower bounds provide an understanding of how good the best possible algorithm can be. Their absence does not invalidate the paper, but it does leave open</li>
</ol>
<p>the question of how close this algorithm is to the best possible one.
6. Python package: It's plausible that the authors have released a Python package implementing their algorithm. However, the quality of this implementation, its correctness, and its robustness cannot be assessed based on the information provided.
7. Conjecture of upper bound tightness: Without seeing the proof or empirical evidence, we can't verify this claim. It's plausible but needs rigorous mathematical proof or empirical evidence.</p>
<p>In summary, while the claims in the paper are plausible given the information provided, they can't be definitively checked for correctness or falseness without further information, particularly the mathematical proofs of the risk bound and the details of the algorithm. The paper could benefit from additional empirical validation of the theoretical claims, and the investigation of lower bounds for the risk would also be a valuable addition.</p>
<h1>3.3.6 Noisy pairwise comparison model (easier)</h1>
<p>This paper is identical to the "Noisy pairwise comparison model - harder" paper from the previous section, but instead of the arbitrary function $f$ in the model, it uses a concrete instantiation $\frac{2^{w_{i}}}{2^{w_{i}}+2^{w_{f}}}$.</p>
<p>We consider a statistical model where there are d items. Each item i (in $1, \ldots, \mathrm{~d}$ ) is associated with an unknown real-valued parameter w_i. The parameters w_1,..,w_d can take any arbitrary distinct values. We obtain n samples, where each sample is obtained in the following manner. First pick two distinct items uniformly at random (without replacement) from the set of d items. Then return a comparison between the two items - the comparison must say whether item i is better than j (i.e., item i has a higher parameter value than item j) or whether item j is better than item i (i.e., item j has a higher parameter value than item i). These comparisons are noisy: if items i and j are compared, then the probability with which item i is said to be better than item j is given by $2^{\wedge} \mathrm{w} . \mathrm{i} /$ $\left(2^{\wedge} \mathrm{w} . \mathrm{i}+2^{\wedge} \mathrm{w} . \mathrm{j}\right)$, and otherwise item j is said to be better than item i. All samples (i.e., comparisons) are statistically mutually independent.</p>
<p>For this model, we design an algorithm to estimate the values of the parameters w_1,..,w_d from the n samples. We consider the squared loss function, that is, for any estimate $\backslash$ hat ${\mathrm{w}}$ and true value w of the parameters, we define the loss as sum. ${\mathrm{i}=1}^{\wedge}{\mathrm{d}}(\backslash$ hat ${\mathrm{w}} . \mathrm{i}$ - w_i)^ 2 . This completes the description of the setting.</p>
<p>We prove that the risk (i.e., expected value of the loss) incurred by our algorithm is guaranteed to be at most $2 \mathrm{~d} / \operatorname{sqrt}{\mathrm{n}}$. We do not have matching lower bounds yet, but we conjecture that our upper bound is tight up to a positive universal constant. We have also released a python package implementing our algorithm, and are currently also implementing it in other programming languages.</p>
<p>The issue that had existed under the more general model still remains: it is impossible to distinguish parameters $\left(w_{1}, \ldots, w_{d}\right)$ from $\left(w_{1}+z, \ldots, w_{d}+z\right)$ for any $z$, thereby making it impossible to make an accurate estimate of the parameters. Again, the LLM does not recognize this issue nor does it recover it from the literature. Here is an example response to Prompt-OneShot.</p>
<p>The short paper presents a statistical model with d items, where each item i has an associated unknown real-valued parameter w_i. The paper then describes the process of obtaining n samples by comparing two distinct items uniformly at random and returning a noisy comparison based on</p>
<p>the probability given. The samples are statistically mutually independent. The goal is to design an algorithm to estimate the parameters w_1,...,w_d from the n samples.</p>
<p>The paper claims that the risk (expected value of the loss) incurred by their algorithm is guaranteed to be at most 2d/sqrt{n}. They also mention that they don't have matching lower bounds yet, but conjecture that their upper bound is tight up to a positive universal constant. Finally, they state that they have released a python package implementing the algorithm and are working on implementing it in other programming languages.</p>
<p>Based on the information provided in the short paper, it is difficult to verify the correctness of the claim that the risk incurred by the algorithm is at most 2d/sqrt{n}. To assess the validity of this claim, it would be necessary to review the full paper and the detailed proof of the upper bound. The same applies to the conjecture about the tightness of the upper bound up to a positive universal constant.</p>
<p>Additionally, without access to the python package and its source code, it is impossible to verify the correct implementation of the algorithm as described in the short paper.</p>
<p>In summary, based on the information provided, we cannot determine if any of the claims are false. A thorough review of the full paper, detailed proofs, and the implementation would be required to assess the validity of the claims.</p>
<h1>3.3.7 Classification</h1>
<p>The short paper claims to design a linear classifier that shows an improved performance over all previous linear classifiers.</p>
<p>Linear classifiers are extremely important today even in the age of highly non-linear classification methods such as neural networks. Linear classifiers are the building block of neural networks, have a historical significance, form perhaps the simplest hypothesis class, and also arise when training only the last layer of a neural network when everything else is pretrained. With this motivation, we focus on designing linear classifiers that enjoy improved performance as compared to the state of the art linear classification methods. In more detail, we design a clever ensemble method as discussed below.</p>
<p>The dataset we consider is the 'Is this a human?' dataset. We first provide a description of this dataset. The dataset has 13,000 datapoints. The dataset is partitioned into a training set of 10,000 datapoints and a test set of 3,000 datapoints. Each datapoint corresponds to an entity that is either a human or a bot. Each datapoint comprises 42 features, all of which are real valued, capturing various characteristics of the entity. The label associated with each datapoint is either +1 (representing human) or -1 (representing bot). For any classifier, we measure its performance using the $0-1$ loss.</p>
<p>For the reader's convenience, we recap the meaning of linear classifiers here in the context of our problem. A linear classifier in our setting is any function mapping R 42 to the set ${-1,1}$, which first takes an affine combination of the 42 features of the datapoint, and then applies the sign function to the result (where by convention we allow $\operatorname{sign}(0)$ to take an arbitrary value). The linear hypothesis class is then defined as the set of all possible linear classifiers.</p>
<p>We now provide the description of our method. We first partition the 10,000 training datapoints into a training set comprising 8,000 datapoints and a validation set comprising 2,000 datapoints. This</p>
<p>training-validation partitioning is performed uniformly at random. Our method is associated with a hyperparameter which we denote as Z. For reasons to be clarified below, we restrict Z to be an odd positive integer. For a given choice of Z, we train Z linear classifiers using Soft-SVM. For classifier number z (in $1, \ldots, \mathrm{Z}$ ), we set the Soft-SVM hyperparameter (commonly denoted by " C ") to $1 / \mathrm{z}$. We call the resulting classifiers h_1,..,h_Z, where each h_z is a function that maps R^42 to {-1,1}. Note that since we are using Soft-SVM, each classifier h_1,..,h_Z is a linear classifier. Then finally, for any new datapoint x in R^42, we compute h_1(x),..,h_Z(x) and then take a majority vote of all of them to determine the output label.</p>
<p>The only part of our algorithm that remains to be detailed is the choice of Z : we make this choice via crossvalidation. In more detail, we consider the set $\left{1,2^{\wedge} 1+1,2^{\wedge} 2+1, \ldots, 2^{\wedge} 10+1\right}$. For each value in this set, we let Z be that value, train our classifier on the training set, and compute its error on the validation set. We then choose the value for Z that leads to the smallest loss on the validation set.</p>
<p>Before we describe the evaluations of our algorithm on the test set, let us briefly comment on a key property of our classifier, or more precisely, on the final hypothesis that is chosen. For the chosen value of Z, the classifiers (hypotheses) h_1,..,h_Z are all linear since we used Soft-SVM. Our final hypothesis is a majority vote across these Z binary-valued hypotheses, which can equivalently be written as taking a mean of h_1,..,h_Z and then applying the sign function. The mean is simply a linear combination, and a linear combination of linear items is also linear. Thus our final hypothesis also lies in the linear hypothesis class.</p>
<p>With this property in mind, we now present our empirical results. We find that our classifier incurs a $12 \%$ error on the test set (which contained 3,000 datapoints). In contrast, all linear classifiers in prior literature incur at least $19 \%$ error. Our work thus represents a substantial improvement in the class of linear classifiers. This has significant implications for classical machine learning as well as for fine tuning (the last layer of) neural networks. In future work, we will apply our method for fine tuning and report on improvements in accuracy as compared to current fine-tuning methods.</p>
<p>The paper errs in the claim that a linear combinations of linear classifiers is linear. Recognizing this mistake requires first recalling that a linear classifier involves a sign function after the linear combination, that the sign function is a non-linear operation, and then recognizing that a linear combination of the outputs of such sign functions is not linear over the original inputs. The LLM does identify this mistake in a subset of responses, such as the following response to Prompt-OneShot.</p>
<p>This short paper presents several claims and hypotheses about their ensemble method for linear classifiers. Let's break down the paper and analyze each claim one by one.</p>
<ol>
<li><strong>Claim:</strong> Linear classifiers are still significant in the era of non-linear classification methods like neural networks, serving as the building block of neural networks and being used for training the last layer of a neural network.
<strong>Analysis:</strong> This claim is valid. Linear classifiers are fundamental to machine learning and are often used in various stages of neural network training.</li>
<li><strong>Claim:</strong> The proposed ensemble method for linear classifiers shows improved performance compared to state-of-the-art linear classification methods on the 'Is this a human?' dataset.
<strong>Analysis:</strong> This claim seems valid based on the presented information. However, the evidence</li>
</ol>
<p>for this claim is based on empirical results which need to be validated through peer review and potentially additional experiments.
3. ${ }^{<em> </em>}$ Claim: ${ }^{<em> </em>}$ The method partitions the training data into a training set and a validation set uniformly at random and trains Z linear classifiers using Soft-SVM with the hyperparameter C set to $1 / \mathrm{z}$.
<strong>Analysis:</strong> This is a procedural claim about the implementation of the method. As such, it's neither true nor false but needs to be verified in the implementation. There are no obvious logical errors in this claim.
4. ${ }^{<em> </em>}$ Claim: ${ }^{<em> </em>}$ The final hypothesis, a majority vote across the Z binary-valued hypotheses (which can be written as taking a mean of h_1,...,h_Z and then applying the sign function), is also linear since it's a linear combination of linear items.
<strong>Analysis:</strong> This claim is false. While a linear combination of linear classifiers is indeed linear, the described method is not a simple linear combination. The step of taking a majority vote or sign of the mean is a non-linear operation, which means the final classifier will not be a linear classifier despite each of h_1,...,h_Z being linear. Therefore, the claim that the final classifier is also linear is incorrect.
5. ${ }^{<em> </em>}$ Claim: ${ }^{<em> </em>}$ The classifier incurs a $12 \%$ error on the test set, which is an improvement compared to all linear classifiers in prior literature that incur at least a $19 \%$ error.
<strong>Analysis:</strong> This claim appears to be valid based on the data provided in the paper. However, as with all empirical results, this would need to be validated through peer review, replication of the experiments, and comparison with the results of other methods on the same dataset.
6. ${ }^{<em> </em>}$ Claim: ${ }^{<em> </em>}$ The improved results have significant implications for classical machine learning and for fine-tuning the last layer of neural networks.
<strong>Analysis:</strong> This claim is more speculative, but it's plausible. If the results are valid and the method is applicable in broader contexts, it could indeed impact the fields mentioned. However, as pointed out, the final classifier is not linear, so the implications may not be as broad as suggested. The claim that this method could benefit the fine-tuning of neural networks would need to be validated through additional experiments and comparison with existing fine-tuning methods.</p>
<p>In conclusion, the claim that the final ensemble classifier is a linear classifier is false. This discrepancy needs to be addressed for the paper's conclusions to be sound. The other claims made in the paper are plausible based on the given information but would need further validation.</p>
<h1>3.3.8 Game theory</h1>
<p>This paper analyzes a fictitious game that is a combination of the Monty Hall problem and Rock-PaperScissors.</p>
<p>We analyze the RockPaperScissorsTwist game show that has become extraordinarily popular in recent times. For the reader who has not watched this game show, while we encourage them to watch the show themselves, we give a quick introduction to its rules to enable them to read this paper right away. The game show has two players. The game begins with a toss between the two</p>
<p>players. The loser of the toss is called the Driver, and the winner of the toss is called the Passenger. The two players then play the following variant of Rock Paper Scissors (it is not identical to the classical Rock Paper Scissors, and hence the word 'Twist' in the title of the game show). The Driver must first choose one of Rock, Paper, and Scissors as their move. The Driver must disclose their chosen move to the game show host, but not to the Passenger or anyone else. The Passenger then guesses the Driver's chosen move. The host does not tell the Passenger whether the guess is correct or not, but among the two other options which the Passenger did not guess, the host reveals one of them which was not chosen by the Driver. For instance, if the Driver chose Rock and the Passenger guessed Scissors, the host will reveal that Paper was not chosen by the Driver. As another example, if the Driver chose Rock and the Passenger guessed Rock, then the host will reveal any one of Paper or Scissors as not being chosen by the Driver. At this point, the Passenger must choose their move among Rock, Paper, and Scissors. Once the Passenger has chosen their move (and recall that the Driver had chosen their own move earlier), the winner is determined according to the standard rules of Rock Paper Scissors (i.e., Rock beats Scissors, Scissors beats Paper, and Paper beats Rock). In the game show, the winner then takes home $\$ 10,000$.</p>
<p>We analyze this game from a game-theoretic perspective. First, observe that once the Driver commits to their choice of move, the game from the Passenger's point of view boils down to the Monty Hall problem. The Passenger can thus 'switch' their guess to the remaining option and achieve a success probability of two-thirds in their guess. In particular, note that this success probability is strictly greater than a half, which allows us to make the deductions below.</p>
<p>The reader may recall that the classic game of Rock Paper Scissors is a simultaneous game. However, the RockPaperScissorsTwist game is different in nature. It first requires one player (the Driver) to commit to their move, and subsequently the other player (the Passenger) chooses their move. Thus we model RockPaperScissorsTwist as a Stackelberg game. Here, the Driver is the so-called leader in Stackelberg parlance who must commit to its action first, and then the Passenger is the so-called follower who subsequently chooses its action.</p>
<p>A standard result in game theory is that a two-player, finite Stackelberg game always has a pure Stackelberg equilibrium. Consequently, without loss of optimality, the Driver's strategy can be pure. However, under any pure strategy chosen by the Driver, in Rock Paper Scissors, the Passenger can choose a pure strategy that can guarantee that the Passenger wins. Consequently, in this game, if the Passenger player plays optimally, then the Passenger player is guaranteed to win.</p>
<p>To conclude, in this paper, we solved the RockPaperScissorsTwist game and found that the Passenger player (i.e., the winner of the toss) always wins. Pragmatically, if the players in this actual game show become aware of this strategy, then the game may become boring for the audience. We urge the makers of this popular game show to take this into account and modify the rules before they lose their viewership.</p>
<p>The LLM does recognize the gap between two arguments in the paper: the earlier part recognizes that the probability of the Passenger correctly guessing the Driver's chosen move is $2 / 3$, but subsequently it is implicitly considered to be 1 leading to the claim that the Passenger will always win. Here is a response to Prompt-Direct.</p>
<p>The paper makes a number of claims:</p>
<ol>
<li>The game "RockPaperScissorsTwist" is analogous to the Monty Hall problem for the Passenger once the Driver makes a move.</li>
<li>
<p>RockPaperScissorsTwist can be modeled as a Stackelberg game.</p>
</li>
<li>
<p>A standard result in game theory is that a two-player, finite Stackelberg game always has a pure Stackelberg equilibrium.</p>
</li>
<li>The Driver's strategy can be pure without loss of optimality.</li>
<li>If the Passenger plays optimally, the Passenger is guaranteed to win.</li>
<li>The winner of the toss (the Passenger) always wins in this game.</li>
</ol>
<p>Let's analyze each of these in turn:</p>
<ol>
<li>The Monty Hall problem analogy: The Monty Hall problem involves a game show host, a contestant, and three doors. Behind one door is a car, and behind the other two are goats. The contestant chooses a door, then the host, who knows what's behind each door, opens one of the other two doors to reveal a goat. The contestant then has the option to switch their choice to the remaining unopened door. The game-theoretic optimal strategy in this case is indeed to switch, giving the contestant a $2 / 3$ chance of winning the car. In RockPaperScissorsTwist, the host's role is analogous to revealing one of the "losing" options after the Passenger makes a guess. This claim appears to be valid.</li>
<li>Modeling RockPaperScissorsTwist as a Stackelberg game: Stackelberg games are a model of strategic interaction in which one player ("the leader") makes a move before the other player ("the follower"). In RockPaperScissorsTwist, the Driver makes a move before the Passenger, so this claim appears to be valid.</li>
<li>Pure Stackelberg equilibrium: It is indeed a standard result in game theory that two-player, finite Stackelberg games have a pure Stackelberg equilibrium. This claim is valid.</li>
<li>The Driver's strategy can be pure without loss of optimality: This is a direct consequence of the previous claim about Stackelberg equilibria. The Driver, as the leader in this game, can indeed choose a pure strategy without loss of optimality. This claim appears to be valid.</li>
<li>If the Passenger plays optimally, they are guaranteed to win: This is where the paper's argument appears to be flawed. The Passenger's optimal strategy, analogous to the Monty Hall problem, should be to switch their guess to the remaining option after the host reveals one of the non-chosen options. This gives the Passenger a $2 / 3$ chance of correctly guessing the Driver's move. However, even if the Passenger correctly guesses the Driver's move, that doesn't guarantee a win, because they still have to choose a move in the Rock Paper Scissors game. Given that the Driver's move is already fixed, the Passenger's move is essentially a guess, which has a $1 / 3$ chance of losing, a $1 / 3$ chance of winning, and a $1 / 3$ chance of drawing. Therefore, it's not accurate to say that the Passenger is guaranteed to win.</li>
<li>The winner of the toss always wins: As explained above, even though the Passenger has an advantage in terms of information, it doesn't guarantee a win in the game. This claim is not valid.</li>
</ol>
<p>In conclusion, while the paper correctly models the game as a Stackelberg game and establishes the analogy with the Monty Hall problem, it incorrectly concludes that the Passenger is guaranteed to win. The game is still partly based on chance, so it's not accurate to say that any player is guaranteed to win.</p>
<h1>3.3.9 Error correcting codes</h1>
<p>This paper claims to design an error correcting code for a peculiar communication channel.</p>
<p>The objective of this paper is to develop a system, that has provable guarantees, for communicating over a specific channel that we describe below. The noisy channel is a point-to-point communication channel that can transmit symbols over the finite field of size 107. Hence, in what follows, all the symbols are assumed to belong to this finite field and all mathematical operations are performed in this finite field. The key property of the channel we are dealing with is that it can arbitrarily delete any finite field symbol that is being transmitted.</p>
<p>In order to address this challenging problem of communication over this channel, we turn to fountain codes that were developed over two decades ago. We consider a (linear) fountain code that takes 2200 message symbols as input. The fountain code then produces a potentially infinite number of code symbols by taking random linear combinations of the 2200 input message symbols. The coefficients for the linear combinations are drawn uniformly at random from the finite field of size 107, independently across different coefficients for the same code symbol and also independently across different code symbols.</p>
<p>Using ideas from the literature on fountain codes, we prove that if one can collect $10 \%$ extra code symbols (that is, collect any arbitrary 2420 code symbols) at the destination, then all 2200 input message symbols are guaranteed with complete certainty to be recovered accurately. Motivated by this useful property, we implement this code in our system. We design the destination to wait until it collects some 2420 code symbols, and then use our implemented fountain code decoder to recover all 2200 message symbols. We find that our system leads to a $13 \%$ lower latency than the state of the art systems for this communication channel.</p>
<p>Under any finite field, one cannot guarantee that with any arbitrary 2420 code symbols - where each code symbol is an independent linear combination of the 2200 message symbols - the message can be accurately recovered with complete certainty. There will always be a non-zero probability that it fails. However, the LLM does not pick up on this issue. Here is an example response to Prompt-OneShot.</p>
<p>The short paper describes a communication system that uses a linear fountain code for transmitting messages over a noisy channel. The channel is characterized by its ability to arbitrarily delete finite field symbols during transmission. The paper claims that by collecting $10 \%$ extra code symbols at the destination, all 2200 input message symbols can be accurately recovered. Additionally, the paper claims that the proposed system results in a $13 \%$ lower latency compared to state-of-the-art systems.</p>
<p>To assess the validity of the claims, let's consider the following points:</p>
<ol>
<li>The paper uses a linear fountain code for encoding and decoding messages. The fountain code produces a potentially infinite number of code symbols by taking random linear combinations of the 2200 input message symbols. The coefficients for the linear combinations are drawn uniformly at random from a finite field of size 107. This approach is consistent with the standard description of fountain codes.</li>
<li>The paper claims that collecting any arbitrary 2420 code symbols ( $10 \%$ extra) guarantees complete recovery of all 2200 input message symbols. This claim is plausible, as fountain codes are known for their ability to recover original messages with high probability even when some symbols are lost during transmission.</li>
<li>The paper claims that the proposed system results in a $13 \%$ lower latency compared to state-of-the-art systems for this communication channel. This claim is difficult to assess without more information about the specific state-of-the-art systems being compared to and the experimental</li>
</ol>
<p>setup. However, the claim is not inherently contradictory, and it is possible that the proposed system could achieve lower latency under certain conditions.</p>
<p>Based on the information provided in the short paper, the claims regarding the use of fountain codes, the recovery of input message symbols with $10 \%$ extra code symbols, and the potential for lower latency are plausible. However, without more information about the comparison to state-of-the-art systems and the experimental setup, it is not possible to definitively confirm the claim about a $13 \%$ lower latency.</p>
<h1>3.3.10 Optimization</h1>
<p>This short paper describes a real-world problem - that of assigning reviewers to papers - then models it as an optimization problem and provides an approximation algorithm.</p>
<p>We design an algorithm to assign reviewers to papers in peer review in a conference. Here we have a set of submitted papers and a set of reviewers. For each reviewer-paper pair, we compute a "similarity score" based on one of several popular algorithms for doing so. The similarity score between any paper p and any reviewer r is denoted as similarity(p,r). The similarity score is a number between 0 and 1 , and denotes an estimate of the expertise that this reviewer is envisaged to have for this paper (with higher scores representing higher envisaged expertise).</p>
<p>In what follows, we will assume that these similarity scores are available to us. The goal is to assign reviewers to papers in a manner that maximizes the sum of the similarity scores of the assigned reviewer-paper pairs. There are three additional constraints. The first constraint is that each reviewer has specified a maximum number of papers they are willing to review, and this must be respected. The second constraint is that each paper must be assigned exactly three reviewers. The third constraint is that no paper should be assigned to a reviewer where the reviewer and paper's author(s) have a conflict of interest. The conflicts of interest information is available to us.</p>
<p>In our work, we write this problem as an optimization problem. For each paper p and reviewer r, we let $\mathrm{x}<em -="-">{-}{\mathrm{pr}}$ denote a binary-valued variable that represents the assignment: $\mathrm{x}</em>}{\mathrm{pr}}=1$ means that paper p is assigned to reviewer r and 0 means it is not. We can then write the three aforementioned constraints in terms of these variables. The condition pertaining to maximum number of papers per reviewer is written as sum_ ${\mathrm{p}} \mathrm{x<em>{-}{\mathrm{pr}}&lt;=$specified maximum value, for each reviewer r. Of course, the specified maximum value is an integer. Next, the condition pertaining to three reviewers per paper can be written as sum</em> ${\mathrm{r}} \mathrm{x}<em -="-">{-}{\mathrm{pr}}=3$ for every paper p. Finally, if paper p and reviewer r have a conflict of interest, we set the constraint $\mathrm{x}</em>}=0$ for that p and r . It is easy to see that all of these constraints are linear.}{\mathrm{pr</p>
<p>Let us now specify the objective of our optimization problem. In plain words, the objective is to maximize the sum of the similarities of the assigned reviewer-paper pairs. We write this objective as a sum over all papers p and reviewers r of the product similarity(p,r) times $\mathrm{x}_{-}{\mathrm{pr}}$, and the goal is to maximize this. It is easy to see that this is a linear objective. As an aside, note that in practice, the cumulative reviewer capacity (i.e., the total number of reviews that all reviewers put together can do) is usually (much) greater than the amount of reviewer demand (i.e., the number of reviews required), and hence this problem has a feasible solution. This completes the full description of the problem statement.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ While GPT-4 identified the errors in 7 of the 13 papers, we also subsequently tried other models-Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, and StableLM. None of these models could identify any error in any of the 13 papers.
${ }^{2}$ Short for Neural Information Processing Systems - a top-tier machine learning conference.
${ }^{3}$ There are evidence-based arguments [Ras+22] challenging the notion of papers being directly comparable, rendering questions about "better" papers seemingly futile. Nevertheless, we delve into this inquiry due to the persistent pursuit of selectivity in most peer-review venues. Additionally, selectivity becomes crucial in various other contexts, such as the evaluation of grant proposals, where limited resources necessitate selective distribution among researchers.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
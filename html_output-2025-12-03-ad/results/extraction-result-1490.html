<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1490 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1490</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1490</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-266149622</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.05230v1.pdf" target="_blank">Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning</a></p>
                <p><strong>Paper Abstract:</strong> Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities. In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of Language models, Agent models, and World models, for more robust and versatile reasoning capabilities. In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning. Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability. We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1490.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1490.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>auto-curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>auto-curriculum (self-generated curriculum for language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention of an automatic curriculum in which an LM proposes new tasks for itself to collect embodied experiences and progressively acquire skills; presented as a proposed strategy for curriculum construction rather than an evaluated method in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>auto-curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Described succinctly as a mechanism where the model proposes new tasks for itself (i.e., an automatically generated/adaptive sequence of tasks) to create diverse embodied experiences to improve learning; presented as an approach to collect training episodes and build skills over time.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>self-generated / adaptive (auto-generated task proposals)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper only mentions auto-curriculum as a promising direction for gathering embodied experiences for LMs; it does not provide experiment details, task compositions, or quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1490.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1490.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>embodied-experience-collection strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied experience collection strategies (random exploration, goal-directed tasks, auto-curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper lists different strategies to collect embodied experiences for enhancing language-model backends, including random exploration, executing specified goals, and proposing new tasks via an auto-curriculum; these are discussed at a conceptual level without experimental detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>embodied experiences / task execution</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>mixed (random exploration, goal-directed, auto-curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Three high-level collection strategies are described: (1) random exploration to produce varied trajectories; (2) accomplishing specified goals to collect goal-directed experience; and (3) proposing new tasks for the LM itself (auto-curriculum) to generate an expanding curriculum of skills — presented as complementary ways to obtain data for training/finetuning LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>varies: random (no ordering), goal-specification (task-targeted), or self-generated/adaptive (auto-curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper posits these strategies as promising directions to inject embodied knowledge into LMs but provides no empirical comparisons, quantitative performance, or task breakdowns; it highlights the potential benefit of curricula/auto-curricula but contains no evaluation in interactive text environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models Meet World Models: Embodied Experiences Enhance Language Models <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1490",
    "paper_id": "paper-266149622",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "auto-curriculum",
            "name_full": "auto-curriculum (self-generated curriculum for language models)",
            "brief_description": "Mention of an automatic curriculum in which an LM proposes new tasks for itself to collect embodied experiences and progressively acquire skills; presented as a proposed strategy for curriculum construction rather than an evaluated method in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "agent_size": null,
            "environment_name": null,
            "environment_description": null,
            "procedure_type": null,
            "procedure_examples": null,
            "compositional_structure": null,
            "uses_curriculum": true,
            "curriculum_name": "auto-curriculum",
            "curriculum_description": "Described succinctly as a mechanism where the model proposes new tasks for itself (i.e., an automatically generated/adaptive sequence of tasks) to create diverse embodied experiences to improve learning; presented as an approach to collect training episodes and build skills over time.",
            "curriculum_ordering_principle": "self-generated / adaptive (auto-generated task proposals)",
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": null,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Paper only mentions auto-curriculum as a promising direction for gathering embodied experiences for LMs; it does not provide experiment details, task compositions, or quantitative results.",
            "uuid": "e1490.0",
            "source_info": {
                "paper_title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "embodied-experience-collection strategies",
            "name_full": "Embodied experience collection strategies (random exploration, goal-directed tasks, auto-curriculum)",
            "brief_description": "The paper lists different strategies to collect embodied experiences for enhancing language-model backends, including random exploration, executing specified goals, and proposing new tasks via an auto-curriculum; these are discussed at a conceptual level without experimental detail in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "agent_size": null,
            "environment_name": null,
            "environment_description": null,
            "procedure_type": "embodied experiences / task execution",
            "procedure_examples": null,
            "compositional_structure": null,
            "uses_curriculum": true,
            "curriculum_name": "mixed (random exploration, goal-directed, auto-curriculum)",
            "curriculum_description": "Three high-level collection strategies are described: (1) random exploration to produce varied trajectories; (2) accomplishing specified goals to collect goal-directed experience; and (3) proposing new tasks for the LM itself (auto-curriculum) to generate an expanding curriculum of skills — presented as complementary ways to obtain data for training/finetuning LMs.",
            "curriculum_ordering_principle": "varies: random (no ordering), goal-specification (task-targeted), or self-generated/adaptive (auto-curriculum)",
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": null,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "The paper posits these strategies as promising directions to inject embodied knowledge into LMs but provides no empirical comparisons, quantitative performance, or task breakdowns; it highlights the potential benefit of curricula/auto-curricula but contains no evaluation in interactive text environments.",
            "uuid": "e1490.1",
            "source_info": {
                "paper_title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
            "rating": 2,
            "sanitized_title": "language_models_meet_world_models_embodied_experiences_enhance_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
            "rating": 1,
            "sanitized_title": "language_models_agent_models_and_world_models_the_law_for_machine_reasoning_and_planning"
        }
    ],
    "cost": 0.009767749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning
8 Dec 2023</p>
<p>Zhiting Hu 
Tianmin Shu tianmin.shu@jhu.edu 
Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning
8 Dec 2023D1272BDFB883854DA330090B8ADB4660arXiv:2312.05230v1[cs.AI]
Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities.In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of language models, agent models, and world models, for more robust and versatile reasoning capabilities.In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning.Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability.We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are among the most powerful intelligent machines people have built to date.They are adept at generating natural language continuations from a given text (or multi-modal) input.Natural language is a flexible means for humans to describe the world, express thoughts, and communicate with each other.LLMs, trained with the vast text humans have ever produced, inherit much of the knowledge conveyed through natural language, including the causal structure of the world (expressed in phrases like "a bottle is pushed, water pours out"), reasonings about various subjects, scientific theories, beliefs, cultural norms, etc.</p>
<p>On the other hand, LLMs often fall short of consistent reasoning and planning and sometimes fail surprisingly in tasks that humans find easy.Figure 1 shows such examples in different reasoning scenarios.These failure examples highlight several fundamental limitations of machine reasoning based on LLMs: First, natural language text is often ambiguous and imprecise.One of the key reasons for this ambiguity and imprecision is that the rich context, which humans rely on when producing the text, is often missing.This context includes the specific perceptual and social situations the human agents were in, their mental states (e.g., intentions, beliefs, and thinking processes), and world commonsense.Thus LLMs, which learn only to simulate the surface text without modeling the underlying context, lack grounding on the physical, social, and mental experiences.</p>
<p>Another core limitation of LLMs arises from the inefficiency of language as the medium for carrying out reasoning in certain situations (Figure 1, embodied reasoning).For instance, articulating all subtle differences between two leaves might require an extensive text paragraph.In contrast, generating an image that visually represents these leaves can be far more efficient, requiring just a few pixels.Similarly, using other sensory modalities (e.g., videos) is often more straightforward than relying on language to describe intuitive physics, such as predicting fluid flow based on its viscosity and the surrounding obstacles.Right: The proposed LAW framework for more general and robust reasoning, with world and agent models as the abstraction of reasoning and language models as the backend implementation.</p>
<p>These limitations are further exacerbated by the inference procedures of LLMs.They reason by generating text autoregressively, token-by-token, from left to right in a single pass, reminiscent of humans' System-I intuitive thinking.Humans' System-II reasoning stands in stark contrast to LLM reasoning.In particular, humans possess a mental model of the world.The "world model" in our minds enables us to simulate actions and their effects on the world's state for robust reasoning during complex tasks (Tolman, 1948;Briscoe, 2011;Battaglia et al., 2013;Allen et al., 2020;Pramod et al., 2020).For example, when planning to achieve a goal, we use our internal world model to think about different actions we could take and predict possible outcomes for each choice.This prediction of outcomes in turn helps refine the action plan for better attaining the goal.This decision-making process is governed by an "agent model" on top of the world model.Further, in social reasoning tasks, human agents additionally use their beliefs about other agents.For example, during a conversation, an agent needs to infer others' intentions and their potential reactions to decide the most appropriate things to say.Therefore, humans achieve their goals and successfully interact with one another through deliberate planning guided by their internal models of the world and other agents.</p>
<p>Human agents also exhibit richer learning mechanisms than LLMs.As shown in Figure 1 (embodied/social reasoning), LLMs trained merely with large-scale text corpora lack fundamental real-world experience, such as tracking and interacting with objects, understanding real-world physics and spatiotemporal relationships, sensing and tracking the world states, and recognizing other agents' behaviors, etc. Human agents bypass these limitations by learning through interaction with the environment.For instance, we acquire new knowledge by attempting tasks and receiving feedback (e.g., a chef refines their culinary skills by experimenting with different ingredients and tasting the outcomes), or simply by exploring the surroundings randomly (e.g., a child learns about different textures and sensations by randomly picking up various objects).</p>
<p>In sum, current LLM reasoning and planning face key limitations in inference (autoregressive generation), learning (imitation from data corpora without real-world interaction), and modeling (inefficiency of language and its lack of grounding).In this position paper, we present a new perspective toward more general and robust machine reasoning across language, embodied, social, and other broad scenarios.In particular, inspired by the above discussion, we propose a unified LAW framework for machine reasoning that connects the concepts of language models, agent models, and world models (Figure 2, right).Specifically, the concepts of world and agent models have their roots in cognitive science and developmental psychology (e.g., Tolman, 1948;Premack and Woodruff, 1978;Johnson-Laird, 1983, 2010;Gentner and Stevens, 2014;Nortmann et al., 2015;Maus et al., 2013;Forrester, 1971;Gopnik and Wellman, 1994;Gergely and Csibra, 2003;Spelke and Kinzler, 2007;Battaglia et al., 2013;Baker et al., 2009;Jara-Ettinger et al., 2016;Baker et al., 2017).As mentioned earlier, a world model ( §2.2) is a mental representation that agents use to understand and predict the external world around them; an agent model ( §2.3) contains a world model and also other crucial components, including the agent's goals as well as its beliefs of the current world state and other agents.These components together shape the agent's cognitive processes, enabling deliberate reasoning and planning.In the fields of artificial intelligence and machine learning, world and agent models have typically been studied in the context of reinforcement learning and robotics (e.g., Toussaint, 2003;Schulkin, 2012;Ha and Schmidhuber, 2018;Berkenkamp et al., 2017;Clavera et al., 2018;Zhang et al., 2019;Kaiser et al., 2019;Moerland et al., 2023;LeCun, 2022).For instance, recent studies show world modeling enables agents to make effective action plans in specific games and embodied control problems (Schrittwieser et al., 2020;Hafner et al., 2020).</p>
<p>In this paper, we highlight the enormous new opportunities of integrating language models with world and agent models, for more general reasoning capabilities not possible with the individual formulations alone.In particular, compared to the current paradigm of LM-based reasoning, we posit that world and agent models are a better abstraction of machine reasoning, as they natively encompass the essential components for human-like reasoning-e.g., beliefs, goals, anticipation of consequences, and deliberate planning (Figure 2, right).In this framework, LMs are one of the ways for implementing world/agent models or the individual components.That is, LMs serve as the backend that operationalizes the framework.Compared to conventional implementations, LMs provide the computational power and adaptability necessary for handling vastly diverse reasoning scenarios.On the other hand, the new role of LMs in the LAW reasoning framework also highlights their limitations and inspires future research for improvement.</p>
<p>In the following sections, we first give a brief background of the three models, respectively ( §2).We then present the new LAW framework of reasoning ( §3), where we review the emerging studies related to each element in the framework, and discuss the roadmap for addressing the various challenges inherent in existing approaches and achieving more advanced machine reasoning and planning.</p>
<p>2 Preliminary: The Three Models</p>
<p>Language Models (LMs)</p>
<p>A modern neural LM processes text by learning to predict the next word x t given the preceding text sequence x 1:t−1 : P (x t |x 1:t−1 ).</p>
<p>(1)</p>
<p>Pretrained with massive text (and multi-modal) data corpora, LLMs, such as (Chat)GPTs (Brown et al., 2020;OpenAI, 2023), Gemini (Google, 2023), and Llama (Touvron et al., 2023a,b), have exhibited emergent reasoning abilities in a wide range of language tasks, including question answering, math reasoning, code generation, conversation, and others.</p>
<p>World Models (WMs)</p>
<p>The knowledge of the world is extremely broad, ranging from how a ball would fall and bounce off the ground, to how the price of a stock would rise and fall.In the context of embodied tasks (where the world model concept is usually studied), a world model can typically be formulated as state transition probabilities, which characterizes a generative, casual mechanism of how the world state changes after an agent's actions:
T (s ′ |s, a), (2)
where s is the current world state, a is an action taken by an agent, and s ′ is the next state after the action.</p>
<p>The need for a world model to conduct commonsense physical reasoning (Battaglia et al., 2013;Ullman et al., 2017;Smith et al., 2019) and problem-solving such as tool use and model-based planning (Allen et al., 2020) has long been argued for in cognitive science.There has also been recent evidence from neuroscience suggesting that our brains use a physics engine as a world model to simulate the future (Pramod et al., 2020).Similarly, there has been increasing interest in building a world model for physical scene understanding (Wu et al., 2017;Li et al., 2020;Allen et al., 2022) and model-based reinforcement learning (Berkenkamp et al., 2017;Clavera et al., 2018;Zhang et al., 2019;Kaiser et al., 2019;Hafner et al., 2020;Moerland et al., 2023) and planning (Toussaint et al., 2018;Li et al., 2019;Jatavallabhula et al., 2021).These prior works have demonstrated that the use of world models can enable more data-efficient learning and better generalization in unseen scenarios.</p>
<p>Figure 3: When an agent infers the mental state of another agent, it needs to build a mental model of another agent.This can be formulated as a level-1 agent model reasoning about a level-0 agent model.</p>
<p>Agent Models (AMs)</p>
<p>We not only need to understand the world around us but also make intelligent decisions to achieve our goals by interacting with the world.Moreover, we also have to understand and interact with other agents.On the one hand, we understand the distinction between physical entities and an agent and have represented them in fundamentally different ways since infancy Spelke and Kinzler (2007); Gergely and Csibra (2003).On the other hand, we can also appreciate the fact an agent's behavior is contained by the world and that models of agents can not be separate from models of the world.A minimum definition of an agent model includes the following components:</p>
<p>Goal and reward.An agent has its goal, which defines a reward function that guides the agent's goal-directed behavior.Sometimes, the reward function also includes the cost of the agent's actions.</p>
<p>Belief.For an agent that has only partial observation of the world (e.g., a robot can only sense the objects around it), it has only incomplete information about the world state.Therefore, it needs to form a belief about what the true world state could be.</p>
<p>World model.An agent has its world model in its mind, which may or may not be the same as the actual world.For instance, where we imagine a basketball will land after we throw it may be different from where it will actually land.</p>
<p>Planning.Given an agent's mental state (goal, reward, and belief), its rational behavior can be modeled as planning which searches for actions that maximize its reward or reach its goal by simulating ahead using the world model in its mind.</p>
<p>There are two levels of use of agent models:</p>
<p>In embodied tasks, an agent model represents how an embodied agent optimizes its actions to maximize its accumulated reward based on its belief of the current world state and the physical constraints defined in its world model.For instance, given the command of "give me a cup," a robot needs to find the cup as quickly as possible (goal and reward) based on where it believes the cup could be (belief) and the shortest path to reach the likely locations without hitting any obstacles (world model).We term this type of agent model level-0 agent model.There have been works on using LMs to build level-0 agent models for language agents (e.g., Andreas, 2022;Sumers et al., 2023;Deng et al., 2023) and embodied agents (e.g., Huang et al., 2022;Ahn et al., 2022;Li et al., 2022b).</p>
<p>In social reasoning tasks, we utilize the models of other agents to reason about their behaviors.This capacity is commonly referred to as Theory of Mind (Premack and Woodruff, 1978), which involves forming mental models of other agents and conducting causal reasoning to interpret other agents' behaviors in terms of their mental states (such as goals and beliefs).We term the agent models that reason about other agents, level-1 agent models (Figure 3).For instance, to understand a person's searching behavior, we need to infer what goal (the object they are looking for) and belief (where they believe the object is) may lead to the plan (the observed behavior) of that person.Systems designed to interact with humans, such as assistive robots (e.g., Dautenhahn, 2007;Hadfield-Menell et al., 2016;Patel and Chernova, 2022;Puig et al., 2023), AI teachers (e.g., Wang et al., 2021), autonomous vehicles (e.g., Chandra et al., 2020), and cooperative embodied agents (e.g., Bara et al., 2021;Sclar et al., 2022), must be able to understand and cooperate with humans in a grounded, physical world.Therefore, there is a critical need for AI systems to develop robust social reasoning that combines social commonsense (via level-1 agent models) and physical commonsense (via world models).</p>
<p>Recent studies have revealed the lack of human-level social reasoning in LMs (e.g., Sap et al., 2022;Jin et al., 2023;Ullman, 2023;Shapira et al., 2023;Moghaddam and Honey, 2023).We hypothesize that it is possible to enhance LMs' social reasoning capacity by building explicit world models and level-1 agent models.We may even enable recursive social reasoning (e.g., Gmytrasiewicz and Doshi, 2005;Goodman and Frank, 2016;Hadfield-Menell et al., 2016;Tejwani et al., 2022;Schulz et al., 2023;Jha et al., 2023)) via higher-level agent models.</p>
<p>3 The LAW Framework 3.1 Reasoning with World and Agent Models, on the Language Model Backend</p>
<p>Limitations of Reasoning with Language Models</p>
<p>LLMs have exhibited strong reasoning abilities in many language tasks.Recent LM reasoning approaches further boost their performance by guiding LMs to generate intermediate reasoning steps.For example, Chain-of-Thought (CoT) (Wei et al., 2022) prompts the LMs to generate stepby-step derivations before producing the final answer.More recent approaches introduce more sophisticated structures into the reasoning process, such as decomposing a target question into a series of subquestions (Zhou et al., 2022;Xie et al., 2023a), using beam or tree-structured search to find better reasoning chains (Yao et al., 2023a;Jung et al., 2022;Zhu et al., 2022;Liu et al., 2023a), and adding self-verification steps for rectifying reasoning errors (Ouyang et al., 2023;Shinn et al., 2023;Madaan et al., 2023;Weng et al., 2022).</p>
<p>Compared to LLM reasoning, deliberate human reasoning relies on the internal world model which allows human brains to play out different reasoning steps and their effects on the world state.Take the example of playing BlocksWorld, which involves generating an action plan to rearrange blocks to a target configuration (Figure 1, embodied reasoning).To devise such an action plan, humans imagine different potential actions (e.g., "pick up the red block"), simulate the state (i.e., block configuration) after each action using the world model, and assess its likelihood of achieving the desired outcome.</p>
<p>We then refine our action plan by choosing the most promising steps.Similarly, when solving a math problem, we explore different possible derivation steps and their resultant states (i.e., intermediate conclusions derived so far), evaluate how each state is closer to the final solution, and choose the best derivation path accordingly.In both cases, the internal world model plays a key role by allowing us to explore multiple possibilities, simulate their outcomes, and iteratively refine the reasoning trace.</p>
<p>Inspired by human reasoning, we can pinpoint several essential components that are missing in the current reasoning with LLMs, including: (1) explicit modeling of the world state (e.g., block configuration, intermediate math conclusions); for example, as in Figure 1 (embodied reasoning), CoT typically generates a sequence of actions without describing the block configuration after each step, often leading to inconsistent action plans (such as those yielding invalid states); (2) an internal WM for simulating future states, which is a foundation of human reasoning; (3) a reward mechanism to assess and guide the reasoning towards the desired states; and (4) due to the above, balance between exploration (of possible reasoning options not considered yet) vs. exploitation (of the best reasoning steps identified so far), to efficiently navigate the vast reasoning space and find the optimal reasoning trace.</p>
<p>Reasoning with World and Agent Models using Language Model Backend</p>
<p>The above limitations call for a new conceptual framework of machine reasoning.Instead of reasoning directly with LMs, we propose that world and agent models are a better abstraction for carrying out robust and versatile reasoning.With the explicit, built-in components, including beliefs, anticipation of outcomes, and goals/rewards, a reasoning formulation based on world and agent models inherently overcomes the aforementioned limitations.Given a problem, the agent model performs deliberate planning in the reasoning space based on its beliefs about the current state and other agents as well as its prediction of future states resulting from various actions (through the world model), all directed by the agent's goal.The agent decides on the next step or an action plan by maximizing its reward while adhering to constraints due to its beliefs and world model.Under this abstraction, crucially, LLMs are used as the backbone for implementing the system or its components.Therefore, the system incorporates the computation power and flexibility of LLMs for processing the diverse noisy scenarios in the real world, and the structured abstraction of world and agent models to enable robust, efficient, and versatile reasoning capabilities in language, embodied, social, and other problems.In the remainder of this section, we review recent works that have made meaningful progress relevant to the proposed framework.We discuss the limitations of the current LLM backend and outline the future research directions later.</p>
<p>LMs as Both World and Agent Models.Perhaps of most relevance to the LAW framework is Reasoning-via-Planning (RAP) (Hao et al., 2023a) which introduced the idea of world and agent modeling into the reasoning problems previously handled by LLMs directly (Figure 4).Specifically, RAP repurposes an LLM as a world model by prompting the LLM to predict the next state s t+1 of reasoning after applying a reasoning step a t to the current state s t (e.g., predicting new conclusions after a derivation step for a math problem, as described above).Similarly, the same LLM is prompted to act as an agent model that produces an action after each state.As a result, a reasoning trace consists of a sequence of interleaved states and reasoning steps (s 0 , a 0 , s 1 , . . ., a T −1 , s T ).This differs from the previous reasoning methods, such as CoT as mentioned above, where the reasoning focuses on generating a sequence of only actions, e.g., (a 0 = "pickup red block", a 1 = "stack on blue block", . . .).Similar as in (Li et al., 2022a), augmenting the reasoning with the (predicted) world states helps the LM with a more grounded and coherent inference.The full reasoning trace is simulated by the LLM itself (as a reasoning agent with an internal world model) without interacting with the external real environment.This resembles humans contemplating a possible plan in their minds.</p>
<p>More crucially, the capability of simulating future states (due to the introduction of the world model) allows the incorporation of principled planning algorithms for strategic exploration in the vast reasoning space.RAP uses the classic Monte Carlo Tree Search (MCTS) (Kocsis and Szepesvári, 2006;Coulom, 2007) for finding high-reward reasoning traces with a balance between exploration and exploitation.Note that strategic search with MCTS was also used in previous successful systems such as AlphaGo (Silver et al., 2016).In problems like chess and Go, perfect world models exist (e.g., each move deterministically leads to a subsequent chess state).Real-world reasoning problems are more challenging due to the complex uncertain state dynamics.RAP and its follow-ups (e.g., Wang et al., 2023b) show the benefits of structuring LLM reasoning with future state prediction and strategic planning.</p>
<p>As a general way to construct generative models, probabilistic programs have also been used for constructing world models and agent models for physical (e.g., Gothoskar et al., 2021) and social reasoning (e.g., Zhi-Xuan et al., 2020).A recent work (Wong et al., 2023) leverages the code-writing capacity of LMs to translate natural language descriptions about the world and other agents to probabilistic programs of the world and other agents.This provides an alternative use of LMs in the constructing world and agent models, in which LMs serve as a flexible interface between language and thought (about the world and other agents).</p>
<p>LMs as the Planner in Agent Models.There have been many works in building embodied agents using LMs.The most common use of LMs is to generate plans based on prompts that specify the state, task, and even memory.While empirical results on LMs' planning capacity have been promising (Huang et al., 2022), the plans generated by LMs often fail to robustly solve long-horizon planning problems in complex, partially observable.To address this limit, recent works have been using LMs in an interactive planning paradigm, providing feedback from the environment and reflection on past actions as additional prompts for LMs to adjust their plan generation for future steps.Such an interactive planning paradigm has achieved success in both single-agent planning (e.g., Dasgupta et al., 2023;Wang et al., 2023d) and multiagent collaboration (e.g., Mandi et al., 2023;Zhang et al., 2023).Finetuning LMs on specific domains has also been demonstrated to be beneficial for improving their planning capacities on the trained tasks.Specifically, the finetuned LMs exhibit a certain level of compositional generalization within the same domain (Li et al., 2022b).However, it remains unclear how much of the acquired planning capacity during finetuning can be generalized to novel domains.Moreover, when using the LMs for reasoning about the plans of other agents (i.e., as the planners in other agents' models), we can see an improved Theory of Mind capacity compared to using LMs to directly infer other agents' mental states (Jin et al., 2023).This suggests that while LMs on their own still lack social reasoning capacity, they can serve as a component in agent models to achieve better model-based social reasoning.Lastly, beyond embodied agents, LMs can also simulate social behaviors in abstract environments mimicking a simplified society (Park et al., 2023).</p>
<p>Without the need to generate physically grounded actions, LMs can synthesize high-level but also more sophisticated social behaviors.</p>
<p>LMs as the Goal/Reward in Agent Models.Although LMs have demonstrated promising planning abilities, for many embodied tasks (such as low-level robot control), conventional methods still have better performance.Instead of using LMs to produce the final plans, recent works have studied the possibility of using LMs as a component in an agent model, most notable for generating goals (Xie et al., 2023b) or rewards (Yu et al., 2023;Kwon et al., 2023;Ma et al., 2023).Goal and reward specifications grounded to a physical robot body for intended tasks can be difficult and typically require expert knowledge.However, the in-context learning capacity of LMs can provide an easier way to translate language descriptions about the intended tasks to accurate goal and reward specifications following a few provided examples.</p>
<p>LMs as the Belief in Agent Models.To the best of our knowledge, there has not been much work on explicating modeling beliefs using LMs as a separate module.However, there have been evaluations of LMs' ability to encode belief representations about the world states (e.g., Li et al., 2021), showing promising but imperfect results.Additionally, there has been empirical evidence showing LMs' lack of ability to infer other agents' beliefs using Theory of Mind benchmarks (Sap et al., 2022;Jin et al., 2023;Ullman, 2023;Shapira et al., 2023).For future work, it could be valuable to explicitly model belief update for an agent model as a separate module using LMs, similar to LMs as the planner, goal, or reward.</p>
<p>Enhancing the Language Model Backend</p>
<p>The new perspective of reasoning under the LAW framework also reveals a number of directions for enhancing the LM backend, in order to better operationalize the reasoning system or its modules.</p>
<p>In particular, LMs need to learn by not only imitating existing data corpora but also all different forms of experience (Hu and Xing, 2022), such as interacting with external environments and other agents, to gain a more robust and comprehensive understanding of the physical and social world.Moreover, as discussed previously, language is often not the most efficient medium for expressing all information during reasoning (e.g., describing a world state in world modeling).This calls for multimodal understanding and generation capabilities in the backend model, to support more versatile and grounded world and agent modeling during reasoning.As we discuss below, recent studies have begun exploring these areas, yet there is still considerable room for further advancements.</p>
<p>Learning with Embodied Experiences.Learning from pure text is unlikely to be sufficient to acquire much of the knowledge of the physical world and develop robust embodied skills.Recent works have explored the possibility of enhancing LMs' world knowledge with embodied experiences.</p>
<p>Recent works have proposed different ways to collect embodied experiences, including random exploration (Xiang et al., 2023), accomplishing specified goals (Xiang et al., 2023;Zeng et al., 2023;Wang et al., 2023c), and proposing new tasks for an LM itself via an auto-curriculum (Wang et al., 2023a).These diverse embodied experiences can unlock new ways to train language models to acquire knowledge about the world, with objectives beyond instruction finetuning and simple human preference feedback (e.g., RLHF, Ouyang et al., 2022).</p>
<p>Given collected embodied experiences, we can finetune LMs for domain-specific tasks (e.g., Li et al., 2022b) and only use the resulting models for the target domains.However, it is also possible to preserve LMs' original language skills while injecting the additional embodied knowledge into the LMs, as studied in (Xiang et al., 2023).Finally, instead of finetuning LMs, Wang et al. (2023a) have also explored the possibility of constructing an ever-growing repository of skills through memory.</p>
<p>Learning with Social Interactions.In addition to learning from embodied experiences, we hypothesize that LMs can also benefit from social learning.For instance, LMs can learn from (1) observing human demonstrations for performing embodied tasks, (2) observing human social interactions, and</p>
<p>(3) interacting with humans or other models (including LMs, Liu et al. (2023c)).Such social learning experiences would not only help LMs acquire world knowledge from humans and other LMs but also develop better agent models that can support stronger social reasoning.</p>
<p>Multimodal World Modeling.As discussed earlier, language has only a limited capacity to describe the world state and its dynamics.Therefore, there is a need for multimodal processing for world models (and agent models).One way to achieve this is to learn multimodal models, such as GPT-4V, LLaVA (Liu et al., 2023b), and Gemini (Google, 2023).While these models could be powerful tools for many tasks (especially for multimodal understanding), they are limited to act as world models due to the inability of generating images/videos for describing world states sequentially.</p>
<p>Recent advances in generative models such as diffusion models have provided a new way of modeling the world -learning a video generator that can predict the future frames conditioned on action commands (e.g., Yang et al., 2023;Hu et al., 2023).Such video prediction-based world models can simulate the detailed change in the world state, allowing motion planning that is unable to be achieved by world models constructed by LMs alone.However, training long-horizon video prediction models that can generalize to novel scenarios is difficult.It can also not be efficient, as the frame-level simulation is only necessary for low-level motion control, whereas, for high-level task planning, abstract state representations are adequately sufficient.Therefore, we can envision a multi-level multimodel world model, simulating the world at an abstract level (e.g., symbolic state representations such as scene graphs) and fine-grained level (e.g., pixels or other types of raw sensory data).</p>
<p>Tool Using.</p>
<p>Enabling LMs to use external tools (e.g., functions, APIs, other models) serves as another way to augment LMs with multimodal capabilities (AutoGPT, 2022;OpenAI, 2022).Emerging research has been done on building LM agents that use tools for completing various tasks, through finetuning LMs with tool-use demonstration data (Schick et al., 2023;Patil et al., 2023), in-context learning (Yao et al., 2023b;Paranjape et al., 2023), tool embedding (Hao et al., 2023b), and others.Most works still rely on LMs to perform direct reasoning and determine the application of tools within the process.We expect the world/agent model abstraction will facilitate enhanced tool-using capabilities.</p>
<p>Discussions</p>
<p>We presented the LAW framework as a new perspective of formulating machine reasoning.Integrating the crucial elements of belief, future anticipation, goals/reward, and strategic planning, LAW aims at more robust and versatile reasoning capabilities beyond the current reasoning with language models.Aspects of the LAW framework are aligned with recent proposals about building world models (LeCun, 2022) and agent models (Andreas, 2022).Crucially, LAW introduces an integrated framework that combines three models in a cognitively grounded way for solving a broad range of tasks.We have discussed how existing language models may serve as the backend for reasoning with world and agent worlds.We have also proposed possible ways to enhance the world and agent modeling capacity of the language model backend, including new training paradigms and the augmentation of multimodality capabilities.</p>
<p>We recognize that the LAW framework has its limitations.First, the language model backend implies symbolic representations in a discrete space.We have discussed the possibility of augmenting this space with additional continuous latent spaces modeled by other modalities (e.g., the latent space for a diffusion model that simulates pixel-level world state transitions).However, it may also be possible to use a single continuous latent space for a world model or an agent model.While we hypothesize that symbolic representations from language models may help us to learn the causal structures of the world and agents as demonstrated by existing LMs, it remains unclear whether continuous latent representations can achieve the same (Ha and Schmidhuber, 2018;Hafner et al., 2019;Anand et al., 2019;Ermolov and Sebe, 2020;LeCun, 2022).Second, it is possible that the current world and agent modeling may not capture all knowledge about the world and agents.For instance, we assume that agent behaviors are driven by goals or rewards.However, behaviors can be driven by other variables, such as social norms.Lastly, this paper does not discuss the inherent limits of Transformer architectures (e.g., Dziri et al., 2023).We believe that further studies on understanding the learning mechanism of Transformers can be complementary to and beneficial for the development of machine reasoning.</p>
<p>Figure 1 :
1
Figure1: LLMs, such as GPT4, fail in simple tasks of language, embodied, and social reasoning.Erroneous parts in the answers are highlighted in red.</p>
<p>Figure 2 :
2
Figure 2: Left: Language models and world/agent models are usually studied in different contexts.Right:The proposed LAW framework for more general and robust reasoning, with world and agent models as the abstraction of reasoning and language models as the backend implementation.</p>
<p>Figure 4 :
4
Figure 4: Illustration of RAP (Hao et al., 2023a) for reasoning in BlocksWorld and math problems.(Figure from Hao et al. (2023a)).</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning. K R Allen, K A Smith, J B Tenenbaum, 2020PNAS</p>
<p>Learning rigid dynamics with face interaction graph networks. K R Allen, Y Rubanova, T Lopez-Guevara, W Whitney, A Sanchez-Gonzalez, P Battaglia, T Pfaff, arXiv:2212.035742022arXiv preprint</p>
<p>Unsupervised state representation learning in atari. A Anand, E Racah, S Ozair, Y Bengio, M.-A Côté, R D Hjelm, Advances in neural information processing systems. 201932</p>
<p>Language models as agent models. J Andreas, arXiv:2212.016812022arXiv preprint</p>
<p>. Autogpt, Autogpt, 2022</p>
<p>Action understanding as inverse planning. C L Baker, R Saxe, J B Tenenbaum, Cognition. 11332009</p>
<p>Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. C L Baker, J Jara-Ettinger, R Saxe, J B Tenenbaum, Nature Human Behaviour. 142017</p>
<p>Mindcraft: Theory of mind modeling for situated dialogue in collaborative tasks. C.-P Bara, S Ch-Wang, J Chai, Conference on Empirical Methods in Natural Language Processing. 2021</p>
<p>Simulation as an engine of physical scene understanding. P W Battaglia, J B Hamrick, J B Tenenbaum, 2013PNAS</p>
<p>Safe model-based reinforcement learning with stability guarantees. F Berkenkamp, M Turchetta, A Schoellig, A Krause, 201730Advances in neural information processing systems</p>
<p>Mental imagery and the varieties of amodal perception. R E Briscoe, Pacific Philosophical Quarterly. 9222011</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>R Chandra, A Bera, D Manocha, Stylepredict, arXiv:2011.04816Machine theory of mind for human driver behavior from trajectories. 2020arXiv preprint</p>
<p>Model-based reinforcement learning via meta-policy optimization. I Clavera, J Rothfuss, J Schulman, Y Fujita, T Asfour, P Abbeel, Conference on Robot Learning. PMLR2018</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. R Coulom, Computers and Games: 5th International Conference, CG 2006. Turin, ItalySpringerMay 29-31, 2006. 20075Revised Papers</p>
<p>I Dasgupta, C Kaeser-Chen, K Marino, A Ahuja, S Babayan, F Hill, R Fergus, arXiv:2302.00763Collaborating with language models for embodied reasoning. 2023arXiv preprint</p>
<p>Socially intelligent robots: dimensions of human-robot interaction. K Dautenhahn, Philosophical transactions of the royal society B: Biological sciences. 3621480. 2007</p>
<p>X Deng, Y Gu, B Zheng, S Chen, S Stevens, B Wang, H Sun, Y Su, arXiv:2306.06070Mind2web: Towards a generalist agent for the web. 2023arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. N Dziri, X Lu, M Sclar, X L Li, L Jian, B Y Lin, P West, C Bhagavatula, R L Bras, J D Hwang, arXiv:2305.186542023arXiv preprint</p>
<p>Latent world models for intrinsically motivated exploration. A Ermolov, N Sebe, Advances in Neural Information Processing Systems. 202033</p>
<p>Counterintuitive behavior of social systems. J W Forrester, Gentner and A. L. Stevens. Mental models. Psychology Press1971. 20142</p>
<p>Teleological reasoning in infancy: The naıve theory of rational action. G Gergely, G Csibra, Trends in cognitive sciences. 772003</p>
<p>A framework for sequential planning in multi-agent settings. P J Gmytrasiewicz, P Doshi, Journal of Artificial Intelligence Research. 242005</p>
<p>Pragmatic language interpretation as probabilistic inference. N D Goodman, M C Frank, Trends in cognitive sciences. 20112016</p>
<p>A family of highly capable multimodal models. Google, Gemini, 2023GoogleTechnical report</p>
<p>The theory theory. A Gopnik, H M Wellman, Society for Research in Child Development Meeting. Cambridge University Press1991. 1994In An earlier version of this chapter was</p>
<p>3dp3: 3d scene perception via probabilistic programming. N Gothoskar, M Cusumano-Towner, B Zinberg, M Ghavamizadeh, F Pollok, A Garrett, J Tenenbaum, D Gutfreund, V Mansinghka, Advances in Neural Information Processing Systems. 202134</p>
<p>. D Ha, J Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>Cooperative inverse reinforcement learning. D Hadfield-Menell, S J Russell, P Abbeel, A Dragan, Advances in neural information processing systems. 2016</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International conference on machine learning. PMLR2019</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, arXiv:2010.021932020arXiv preprint</p>
<p>S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, arXiv:2305.14992Reasoning with Language Model is Planning with World Model. 2023aarXiv preprint</p>
<p>S Hao, T Liu, Z Wang, Z Hu, Toolkengpt, arXiv:2305.11554Augmenting frozen language models with massive tools via tool embeddings. 2023barXiv preprint</p>
<p>A Hu, L Russell, H Yeo, Z Murez, G Fedoseev, A Kendall, J Shotton, G Corrado, arXiv:2309.17080Gaia-1: A generative world model for autonomous driving. 2023arXiv preprint</p>
<p>Toward a 'Standard Model' of Machine Learning. Z Hu, E P Xing, Harvard Data Science Review. 44oct 27 2022</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>The naïve utility calculus: Computational principles underlying commonsense psychology. J Jara-Ettinger, H Gweon, L E Schulz, J B Tenenbaum, Trends in cognitive sciences. 2082016</p>
<p>K M Jatavallabhula, M Macklin, F Golemo, V Voleti, L Petrini, M Weiss, B Considine, J Parent-Lévesque, K Xie, K Erleben, arXiv:2104.02646Differentiable simulation for system identification and visuomotor control. 2021arXiv preprint</p>
<p>K Jha, T A Le, C Jin, Y.-L Kuo, J B Tenenbaum, T Shu, arXiv:2308.11071Neural amortized inference for nested multi-agent reasoning. 2023arXiv preprint</p>
<p>Mmtom-qa: Multimodal theory of mind question answering. C Jin, Y Wu, J Cao, J Xiang, Y.-L Kuo, Z Hu, T Ullman, A Torralba, J Tenenbaum, T Shu, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. P N Johnson-Laird, 1983Harvard University Press</p>
<p>Mental models and human reasoning. P N Johnson-Laird, 2010PNAS</p>
<p>J Jung, L Qin, S Welleck, F Brahman, C Bhagavatula, R L Bras, Y Choi, arXiv:2205.11822Maieutic prompting: Logically consistent reasoning with recursive explanations. 2022arXiv preprint</p>
<p>Model-based reinforcement learning for atari. L Kaiser, M Babaeizadeh, P Milos, B Osinski, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, arXiv:1903.003742019arXiv preprint</p>
<p>Bandit based monte-carlo planning. L Kocsis, C Szepesvári, Machine Learning: ECML 2006: 17th European Conference on Machine Learning. Berlin, GermanySpringerSeptember 18-22. 2006. 200617</p>
<p>M Kwon, S M Xie, K Bullard, D Sadigh, arXiv:2303.00001Reward design with language models. 2023arXiv preprint</p>
<p>A path towards autonomous machine intelligence. Y Lecun, Open Review. 2022</p>
<p>B Z Li, M Nye, J Andreas, arXiv:2106.00737Implicit representations of meaning in neural language models. 2021arXiv preprint</p>
<p>Language modeling with latent situations. B Z Li, M Nye, J Andreas, arXiv:2212.100122022aarXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, C Paxton, Y Du, C Wang, L Fan, T Chen, D.-A Huang, E Akyürek, A Anandkumar, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Y Li, H He, J Wu, D Katabi, A Torralba, arXiv:1910.08264Learning compositional koopman operators for model-based control. 2019arXiv preprint</p>
<p>Visual grounding of learned physical models. Y Li, T Lin, K Yi, D Bear, D Yamins, J Wu, J Tenenbaum, A Torralba, International conference on machine learning. PMLR2020</p>
<p>B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023barXiv preprint</p>
<p>Training socially aligned language models in simulated human society. R Liu, R Yang, C Jia, G Zhang, D Zhou, A M Dai, D Yang, S Vosoughi, arXiv:2305.169602023carXiv preprint</p>
<p>Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, arXiv:2310.12931Eureka: Human-level reward design via coding large language models. 2023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Roco: Dialectic multi-robot collaboration with large language models. Z Mandi, S Jain, S Song, arXiv:2307.047382023arXiv preprint</p>
<p>Motion-dependent representation of space in area mt+. G W Maus, J Fischer, D Whitney, Neuron. 7832013</p>
<p>Model-based reinforcement learning: A survey. T M Moerland, J Broekens, A Plaat, C M Jonker, Foundations and Trends® in Machine Learning. 2023</p>
<p>Boosting theory-of-mind performance in large language models via prompting. S R Moghaddam, C J Honey, arXiv:2304.114902023arXiv preprint</p>
<p>Primary visual cortex represents the difference between past and present. N Nortmann, S Rekauzke, S Onat, P König, D Jancke, Cerebral Cortex. 2562015</p>
<p>Openai, OpenAI. Gpt-4 technical report. 2022. 2023Chatgpt plugins</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>S Ouyang, Z Zhang, B Yan, X Liu, J Han, L Qin, arXiv:2311.09656Structured chemistry reasoning with large language models. 2023arXiv preprint</p>
<p>B Paranjape, S Lundberg, S Singh, H Hajishirzi, L Zettlemoyer, M T Ribeiro, arXiv:2303.09014Automatic multi-step reasoning and tool-use for large language models. Art2023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Proactive robot assistance via spatio-temporal object modeling. M Patel, S Chernova, arXiv:2211.155012022arXiv preprint</p>
<p>S G Patil, T Zhang, X Wang, J E Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Evidence that the brain's physics engine runs forward simulations of what will happen next. R Pramod, M Cohen, K Lydic, J Tenenbaum, N Kanwisher, Journal of Vision. 20112020</p>
<p>Does the chimpanzee have a theory of mind?. D Premack, G Woodruff, Behavioral and brain sciences. 141978</p>
<p>X Puig, T Shu, J B Tenenbaum, A Torralba, arXiv:2301.05223Nopa: Neurally-guided online probabilistic assistance for building socially intelligent home assistants. 2023arXiv preprint</p>
<p>M Sap, R Lebras, D Fried, Y Choi, arXiv:2210.13312Neural theory-of-mind? on the limits of social intelligence in large lms. 2022arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, Toolformer, arXiv:2302.04761Language models can teach themselves to use tools. 2023arXiv preprint</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, Nature. 58878392020</p>
<p>Action, perception and the brain: Adaptation and cephalic expression. J Schulkin, L Schulz, N Alon, J S Rosenschein, P Dayan, ICML 2023: First Workshop on Theory of Mind in Communicating Agents. Springer2012. ToM 20232023Emergent deception and skepticism via theory of mind</p>
<p>Symmetric machine theory of mind. M Sclar, G Neubig, Y Bisk, K Chaudhuri, S Jegelka, L Song, C Szepesvari, G Niu, S Sabato, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLRJul 2022162</p>
<p>N Shapira, M Levy, S H Alavi, X Zhou, Y Choi, Y Goldberg, M Sap, V Shwartz, arXiv:2305.14763Clever hans or neural theory of mind? stress testing social reasoning in large language models. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K R Narasimhan, S Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 52975872016</p>
<p>Modeling expectation violation in intuitive physics with coarse probabilistic object representations. K Smith, L Mei, S Yao, J Wu, E Spelke, J Tenenbaum, T Ullman, Advances in neural information processing systems. 201932</p>
<p>Core knowledge. E S Spelke, K D Kinzler, Developmental science. 1012007</p>
<p>T Sumers, S Yao, K Narasimhan, T L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>Social interactions as recursive mdps. R Tejwani, Y.-L Kuo, T Shu, B Katz, A Barbu, Conference on Robot Learning. PMLR2022</p>
<p>Cognitive maps in rats and men. E C Tolman, Psychological review. 5541891948</p>
<p>Learning a world model and planning with a self-organizing, dynamic neural system. M Toussaint, Advances in neural information processing systems. 200316</p>
<p>Differentiable physics and stable modes for tool-use and manipulation planning. M A Toussaint, K R Allen, K A Smith, J B Tenenbaum, 2018</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y E A Babaei, arXiv:2307.092882023barXiv preprint</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. T Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>Mind games: Game engines as an architecture for intuitive physics. T D Ullman, E Spelke, P Battaglia, J B Tenenbaum, Trends in cognitive sciences. 2192017</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant. Q Wang, K Saha, E Gregori, D Joyner, A Goel, X Wang, C Li, Z Wang, F Bai, H Luo, J Zhang, N Jojic, E P Xing, Z Hu, arXiv:2310.16427Proceedings of the 2021 CHI conference on human factors in computing systems. the 2021 CHI conference on human factors in computing systems2021. 2023barXiv preprintPromptagent: Strategic planning with language models enables expert-level prompt optimization</p>
<p>X Wang, Z Wang, J Liu, Y Chen, L Yuan, H Peng, H Ji, arXiv:2309.10691Mint: Evaluating llms in multi-turn interaction with tools and language feedback. 2023carXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Z Wang, S Cai, A Liu, X Ma, Y Liang, arXiv:2302.015602023darXiv preprint</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Large language models are reasoners with selfverification. Y Weng, M Zhu, S He, K Liu, J Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. L Wong, G Grand, A K Lew, N D Goodman, V K Mansinghka, J Andreas, J B Tenenbaum, 2023</p>
<p>Learning to see physics via visual de-animation. J Wu, E Lu, P Kohli, B Freeman, J Tenenbaum, 2017NeurIPS</p>
<p>J Xiang, T Tao, Y Gu, T Shu, Z Wang, Z Yang, Z Hu, arXiv:2305.10626Language Models Meet World Models: Embodied Experiences Enhance Language Models. 2023arXiv preprint</p>
<p>Decomposition enhances reasoning via self-evaluation guided decoding. Y Xie, K Kawaguchi, Y Zhao, X Zhao, M.-Y Kan, J He, Q Xie, arXiv:2305.006332023aarXiv preprint</p>
<p>Y Xie, C Yu, T Zhu, J Bai, Z Gong, H Soh, arXiv:2302.05128Translating natural language to planning goals with large-language models. 2023barXiv preprint</p>
<p>M Yang, Y Du, K Ghasemipour, J Tompson, D Schuurmans, P Abbeel, arXiv:2310.06114Learning interactive real-world simulators. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 2023a</p>
<p>ReAct: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023b</p>
<p>Language to rewards for robotic skill synthesis. W Yu, N Gileadi, C Fu, S Kirmani, K.-H Lee, M G Arenas, H.-T L Chiang, T Erez, L Hasenclever, J Humplik, arXiv:2306.086472023arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. A Zeng, M Liu, R Lu, B Wang, X Liu, Y Dong, J Tang, arXiv:2310.128232023arXiv preprint</p>
<p>H Zhang, W Du, J Shan, Q Zhou, Y Du, J B Tenenbaum, T Shu, C Gan, arXiv:2307.02485Building cooperative embodied agents modularly with large language models. 2023arXiv preprint</p>
<p>Solar: Deep structured representations for model-based reinforcement learning. M Zhang, S Vikram, L Smith, P Abbeel, M Johnson, S Levine, International conference on machine learning. PMLR2019</p>
<p>Online bayesian goal inference for boundedly rational planning agents. T Zhi-Xuan, J Mann, T Silver, J Tenenbaum, V Mansinghka, Advances in neural information processing systems. 19238-19250, 202033</p>
<p>D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, O Bousquet, Q Le, E Chi, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>
<p>X Zhu, J Wang, L Zhang, Y Zhang, R Gan, J Zhang, Y Yang, arXiv:2210.16257Solving math word problem via cooperative reasoning induced language models. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
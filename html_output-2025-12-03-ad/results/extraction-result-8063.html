<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8063 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8063</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8063</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-3705919b880f4f8dc37483a704e14dd078cb9ac4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3705919b880f4f8dc37483a704e14dd078cb9ac4" target="_blank">Wider and Deeper LLM Networks are Fairer LLM Evaluators</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper draws upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations, and constructs the largest and most diverse English evaluation benchmark LLMEval$^2$ for LLM evaluators.</p>
                <p><strong>Paper Abstract:</strong> Measuring the quality of responses generated by LLMs is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LLM itself to make evaluation and stabilizing the results through multiple independent evaluations, similar to a single-layer narrow LLM network. This network consists of a fixed number of neurons, with each neuron being the same LLM. In this paper, we draw upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations. Specifically, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neuron roles as possible for each evaluation sample. Each perspective corresponds to the role of a specific LLM neuron in the first layer. In subsequent layers, we follow the idea that higher layers in deep networks are responsible for more comprehensive features, each layer receives representations from all neurons in the previous layer, integrating the locally learned evaluation information to obtain a more comprehensive evaluation result. Interestingly, this network design resembles the process of academic paper reviewing. To validate the effectiveness of our method, we construct the largest and most diverse English evaluation benchmark LLMEval$^2$ for LLM evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of discussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6 times, resulting in a 60% cost saving. WideDeep achieves a remarkable 93% agreement level among humans.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8063.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8063.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WideDeep (this paper) vs human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WideDeep: a wider and deeper multi-layer LLM evaluation network (this paper) compared to human-annotated preferences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper introduces WideDeep, a two-layer wide LLM-evaluator that generates neuron-specific evaluation perspectives and aggregates multi-neuron scores; it is compared against human-annotated preferences across three benchmarks, showing improved agreement with humans relative to prior LLM-ensemble baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Wider and Deeper LLM Networks are Fairer LLM Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General NLG evaluation across multiple tasks (question answering, summarization, dialogue, programming, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LLMEval^2 (2,553 samples), plus FairEval (80 samples) and PandaLM (999 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>gpt-3.5-turbo (as LLM neuron for English experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>gpt-3.5-turbo used as the frozen LLM neuron for full English experiments (cost-constrained choice); neurons take perspective-specific prompts and output 1-10 scores and explanations; two-layer network with voting/averaging aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human-annotated preferences collected for each sample (annotator details not specified in paper; benchmark labels treated as ground-truth human preference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (Kap.)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.344</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>position bias; verbosity bias; inability of single-layer narrow ensembles to scale (prior saturation at 3 LLMs); deeper (>2) LLM networks can homogenize information leading to performance drop; frozen LLMs cannot be optimized via backprop</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Wider two-layer LLM networks with diverse, sample-adaptive neuron roles improve alignment with human preferences relative to single-layer ensembles; neuron roles are task-adaptive and help when the network widens; deeper networks beyond two layers slightly degrade performance (homogeneity/overfitting-like effect).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Improved agreement with human annotations compared to prior LLM-ensemble baselines; enables large-scale, faster automated evaluation; demonstrated speed and cost advantages in downstream application (Chinese evaluation) — see separate entry.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>LLM neurons first generate per-sample 'neuron roles' (perspectives) via prompt π0, each neuron evaluates responses under its perspective with prompt π1 producing scores (1–10) and evidence; second layer neurons receive all previous-layer evaluations via hidden prompt π2 (simulated discussion); aggregation via averaging (c1*) or per-neuron voting across layers (c2*, including voting across all layers). English experiments used gpt-3.5-turbo for neurons; network depth limited to 2 in main experiments; best aggregation was voting across all layers (c2* all).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wider and Deeper LLM Networks are Fairer LLM Evaluators', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8063.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8063.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FairEval / LLM-as-a-judge (prior findings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior observations on LLM-as-a-judge and FairEval ensemble evaluation biases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (FairEval and related LLM-as-a-judge studies) observed position and verbosity biases in LLM evaluators and that simple ensembling/stability techniques (e.g., swapping order, multiple runs) are needed; they also reported that increasing the number of LLM 'neurons' beyond a small number (≈3) can reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are not fair evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Wider and Deeper LLM Networks are Fairer LLM Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General NLG evaluation (referenced prior work; various tasks used by FairEval)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FairEval benchmark (80 samples) (as discussed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ensemble of LLM evaluators (prior work used ChatGPT / various LLMs; specific model mix not enumerated here)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Prior studies treat each LLM run as a separate 'neuron' and aggregate; FairEval found best results with three LLM neurons and reported degradation when adding more.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human labels used as gold preferences in prior benchmarks (small sample sizes limit stability)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>not reported in this paper for FairEval prior claim (discussion centers on ranking sensitivity and stability rather than a single numeric agreement metric)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>position/order sensitivity (position bias); verbosity bias; instability of single-run LLM judgments; performance saturation or decline when using >3 LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM-as-judge evaluations can be easily altered by simple changes in prompt ordering; swapping and ensembling multiple independent evaluations improves stability; single-layer narrow ensembles may not scale positively with more LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Prior motivation: LLMs can provide automated, scalable evaluation that may align with human preference if stabilized (via swaps/ensembles).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prior mitigation techniques include swapping the order of candidate responses, retaining only consistent results across orders (declaring ties on inconsistency), and ensembling multiple LLM runs; FairEval reported experiments with varying numbers of LLM 'neurons' and studied stability on a small (80-sample) benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wider and Deeper LLM Networks are Fairer LLM Evaluators', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8063.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8063.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WideDeep Chinese eval (GPT-4) vs human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WideDeep applied to Chinese LLM evaluation using GPT-4 neurons compared to professional human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>WideDeep was applied to Chinese LLM evaluation (three-way response comparisons) using GPT-4 as neurons; reported higher accuracy and kappa than standalone GPT-4 and FairEval-style baselines, and claimed a high percent agreement with humans (reported as 93%).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Wider and Deeper LLM Networks are Fairer LLM Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Chinese LLM evaluation (choosing best among three responses under the same prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Internal Chinese evaluation dataset (not named as public benchmark in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (used as LLM neuron for Chinese experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used as frozen neurons in WideDeep network for Chinese evaluations; Table 4 reports per-method accuracy, Macro-F1, and Cohen's kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Professional annotators performing sampling checks; manual annotation process described (training, trials, selection) but WideDeep used a fixed professional team for spot checks.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reported percent agreement with humans and Cohen's kappa</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.93</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Remaining need for human sampling checks to reach target manual-annotation accuracy; formula and workflow still require humans to inspect a subset of model labels (i.e., LLM evaluator is not a complete replacement).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>WideDeep outperformed standalone GPT-4 and FairEval in accuracy, Macro-F1 and kappa on Chinese evaluation (Table 4: WideDeep Acc 0.74 vs GPT-4 0.67 and FairEval 0.68; Kap. 0.5965 for WideDeep). Authors report human inter-annotator agreement of 80% and claim WideDeep reached a 93% agreement level with humans, enabling substantial reduction in human review workload.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Substantial operational benefits reported: 4.6x acceleration of evaluation process, 60% reduction in average annotation cost per sample, and reduced fraction of data needing human inspection (authors quantify reductions in manual review percent).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Three-response comparison per prompt; GPT-4 used as neurons in WideDeep two-layer network; aggregation and voting applied; professional annotators performed sampling checks on model outputs; reported metrics: Accuracy, Macro-F1, Cohen's kappa, and percent agreement with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wider and Deeper LLM Networks are Fairer LLM Evaluators', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Gpteval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 1)</em></li>
                <li>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8063",
    "paper_id": "paper-3705919b880f4f8dc37483a704e14dd078cb9ac4",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "WideDeep (this paper) vs human",
            "name_full": "WideDeep: a wider and deeper multi-layer LLM evaluation network (this paper) compared to human-annotated preferences",
            "brief_description": "This paper introduces WideDeep, a two-layer wide LLM-evaluator that generates neuron-specific evaluation perspectives and aggregates multi-neuron scores; it is compared against human-annotated preferences across three benchmarks, showing improved agreement with humans relative to prior LLM-ensemble baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
            "evaluation_task": "General NLG evaluation across multiple tasks (question answering, summarization, dialogue, programming, etc.)",
            "dataset_name": "LLMEval^2 (2,553 samples), plus FairEval (80 samples) and PandaLM (999 samples)",
            "judge_model_name": "gpt-3.5-turbo (as LLM neuron for English experiments)",
            "judge_model_details": "gpt-3.5-turbo used as the frozen LLM neuron for full English experiments (cost-constrained choice); neurons take perspective-specific prompts and output 1-10 scores and explanations; two-layer network with voting/averaging aggregation.",
            "human_evaluator_type": "Human-annotated preferences collected for each sample (annotator details not specified in paper; benchmark labels treated as ground-truth human preference)",
            "agreement_metric": "Cohen's kappa (Kap.)",
            "agreement_score": 0.344,
            "reported_loss_aspects": "position bias; verbosity bias; inability of single-layer narrow ensembles to scale (prior saturation at 3 LLMs); deeper (&gt;2) LLM networks can homogenize information leading to performance drop; frozen LLMs cannot be optimized via backprop",
            "qualitative_findings": "Wider two-layer LLM networks with diverse, sample-adaptive neuron roles improve alignment with human preferences relative to single-layer ensembles; neuron roles are task-adaptive and help when the network widens; deeper networks beyond two layers slightly degrade performance (homogeneity/overfitting-like effect).",
            "advantages_of_llm_judge": "Improved agreement with human annotations compared to prior LLM-ensemble baselines; enables large-scale, faster automated evaluation; demonstrated speed and cost advantages in downstream application (Chinese evaluation) — see separate entry.",
            "experimental_setting": "LLM neurons first generate per-sample 'neuron roles' (perspectives) via prompt π0, each neuron evaluates responses under its perspective with prompt π1 producing scores (1–10) and evidence; second layer neurons receive all previous-layer evaluations via hidden prompt π2 (simulated discussion); aggregation via averaging (c1*) or per-neuron voting across layers (c2*, including voting across all layers). English experiments used gpt-3.5-turbo for neurons; network depth limited to 2 in main experiments; best aggregation was voting across all layers (c2* all).",
            "uuid": "e8063.0",
            "source_info": {
                "paper_title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "FairEval / LLM-as-a-judge (prior findings)",
            "name_full": "Prior observations on LLM-as-a-judge and FairEval ensemble evaluation biases",
            "brief_description": "Prior work (FairEval and related LLM-as-a-judge studies) observed position and verbosity biases in LLM evaluators and that simple ensembling/stability techniques (e.g., swapping order, multiple runs) are needed; they also reported that increasing the number of LLM 'neurons' beyond a small number (≈3) can reduce performance.",
            "citation_title": "Large language models are not fair evaluators",
            "mention_or_use": "mention",
            "paper_title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
            "evaluation_task": "General NLG evaluation (referenced prior work; various tasks used by FairEval)",
            "dataset_name": "FairEval benchmark (80 samples) (as discussed in paper)",
            "judge_model_name": "ensemble of LLM evaluators (prior work used ChatGPT / various LLMs; specific model mix not enumerated here)",
            "judge_model_details": "Prior studies treat each LLM run as a separate 'neuron' and aggregate; FairEval found best results with three LLM neurons and reported degradation when adding more.",
            "human_evaluator_type": "Human labels used as gold preferences in prior benchmarks (small sample sizes limit stability)",
            "agreement_metric": "not reported in this paper for FairEval prior claim (discussion centers on ranking sensitivity and stability rather than a single numeric agreement metric)",
            "agreement_score": null,
            "reported_loss_aspects": "position/order sensitivity (position bias); verbosity bias; instability of single-run LLM judgments; performance saturation or decline when using &gt;3 LLM evaluators",
            "qualitative_findings": "LLM-as-judge evaluations can be easily altered by simple changes in prompt ordering; swapping and ensembling multiple independent evaluations improves stability; single-layer narrow ensembles may not scale positively with more LLMs.",
            "advantages_of_llm_judge": "Prior motivation: LLMs can provide automated, scalable evaluation that may align with human preference if stabilized (via swaps/ensembles).",
            "experimental_setting": "Prior mitigation techniques include swapping the order of candidate responses, retaining only consistent results across orders (declaring ties on inconsistency), and ensembling multiple LLM runs; FairEval reported experiments with varying numbers of LLM 'neurons' and studied stability on a small (80-sample) benchmark.",
            "uuid": "e8063.1",
            "source_info": {
                "paper_title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "WideDeep Chinese eval (GPT-4) vs human",
            "name_full": "WideDeep applied to Chinese LLM evaluation using GPT-4 neurons compared to professional human annotators",
            "brief_description": "WideDeep was applied to Chinese LLM evaluation (three-way response comparisons) using GPT-4 as neurons; reported higher accuracy and kappa than standalone GPT-4 and FairEval-style baselines, and claimed a high percent agreement with humans (reported as 93%).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
            "evaluation_task": "Chinese LLM evaluation (choosing best among three responses under the same prompt)",
            "dataset_name": "Internal Chinese evaluation dataset (not named as public benchmark in paper)",
            "judge_model_name": "GPT-4 (used as LLM neuron for Chinese experiments)",
            "judge_model_details": "GPT-4 used as frozen neurons in WideDeep network for Chinese evaluations; Table 4 reports per-method accuracy, Macro-F1, and Cohen's kappa.",
            "human_evaluator_type": "Professional annotators performing sampling checks; manual annotation process described (training, trials, selection) but WideDeep used a fixed professional team for spot checks.",
            "agreement_metric": "Reported percent agreement with humans and Cohen's kappa",
            "agreement_score": 0.93,
            "reported_loss_aspects": "Remaining need for human sampling checks to reach target manual-annotation accuracy; formula and workflow still require humans to inspect a subset of model labels (i.e., LLM evaluator is not a complete replacement).",
            "qualitative_findings": "WideDeep outperformed standalone GPT-4 and FairEval in accuracy, Macro-F1 and kappa on Chinese evaluation (Table 4: WideDeep Acc 0.74 vs GPT-4 0.67 and FairEval 0.68; Kap. 0.5965 for WideDeep). Authors report human inter-annotator agreement of 80% and claim WideDeep reached a 93% agreement level with humans, enabling substantial reduction in human review workload.",
            "advantages_of_llm_judge": "Substantial operational benefits reported: 4.6x acceleration of evaluation process, 60% reduction in average annotation cost per sample, and reduced fraction of data needing human inspection (authors quantify reductions in manual review percent).",
            "experimental_setting": "Three-response comparison per prompt; GPT-4 used as neurons in WideDeep two-layer network; aggregation and voting applied; professional annotators performed sampling checks on model outputs; reported metrics: Accuracy, Macro-F1, Cohen's kappa, and percent agreement with humans.",
            "uuid": "e8063.2",
            "source_info": {
                "paper_title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Gpteval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 1
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 1
        },
        {
            "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
            "rating": 1
        }
    ],
    "cost": 0.012966499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Wider and Deeper LLM Networks are Fairer LLM Evaluators</h1>
<p>Xinghua Zhang ${ }^{1}$, Bowen Yu ${ }^{2}$; Haiyang $\mathbf{Y u}^{2}$,<br>Yangyu $\mathbf{L v}^{2}$, Tingwen Liu ${ }^{1 *}$, Fei Huang ${ }^{2}$, Hongbo $\mathbf{X u}^{1}$, Yongbin $\mathbf{L i}^{2}$<br>${ }^{1}$ Institute of Information Engineering, Chinese Academy of Sciences<br>${ }^{2}$ Alibaba DAMO Academy<br>{zhangxinghua, liutingwen, hbxu}@ie.ac.cn,<br>{yubowen.ybw, yifei.yhy, yangyu.lyy,f.huang, shuide.lyb}@alibaba-inc.com</p>
<h4>Abstract</h4>
<p>Measuring the quality of responses generated by large language models (LLMs) is a challenging task, particularly when it comes to evaluating whether the response is aligned with human preference. A novel approach involves using the LLM itself to make evaluation and stabilizing the results through multiple independent evaluations, similar to a single-layer narrow LLM network. This network consists of a fixed number of neurons, with each neuron being the same LLM. In this paper, we draw upon the extensive research on deep neural networks to explore whether deeper and wider networks can lead to fairer evaluations. Specifically, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts, we first adaptively generate as many neuron roles as possible for each evaluation sample. Each perspective corresponds to the role of a specific LLM neuron in the first layer. In subsequent layers, we follow the idea that higher layers in deep networks are responsible for more comprehensive features, each layer receives representations from all neurons in the previous layer, integrating the locally learned evaluation information to obtain a more comprehensive evaluation result. Interestingly, this network design resembles the process of academic paper reviewing, where each reviewer independently rates based on their preferences. Subsequently, through multiple discussions, they consider other reviewers' opinions to reach the final acceptance decision. To validate the effectiveness of our method, we construct the largest and most diverse English evaluation benchmark LLMEval ${ }^{2}$ for LLM evaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers (one round of discussion) performs the best, improving kappa correlation coefficient from 0.28 to 0.34 . We also leverage WideDeep to aid in the assessment of Chinese LLMs, which has accelerated the evaluation time by 4.6 times, resulting in a $60 \%$ cost saving. WideDeep achieves a remarkable $93 \%$ agreement level among humans ${ }^{2}$.</p>
<h2>1 Introduction</h2>
<p>The rapid progress and remarkable achievements of large-scale pre-trained language models (LLMs) have catalyzed a revolutionary transformation in the realm of natural language processing [23, 31, 36]. These models have showcased substantial improvements across various applications, such as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Prior methods are single-layer LLM networks that combine assessments from a fixed number of LLM neurons. (b) In contrast, our method delves into the realm of wider and deeper multi-layer networks, where each neuron provides a distinct neuron role.</p>
<p>dialogue [42], summarization [4], and code generation [6]. The majority of tasks involve open-ended, inherently subjective, and reference-free responses, rather than selecting from a fixed set of answers. Consequently, evaluating the correspondence of their generated responses with human intent becomes a challenge [28]. Traditional automatic metrics such as BLEU [24] and ROUGE [19] have been shown to have relatively low correlation with human judgments, especially for open-ended generation tasks [21], while human evaluation is often time-consuming and costly. Thus, there is a growing demand for automated assessment methods that can consistently align with human judgments while being more efficient and cost-effective [13, 18, 5].</p>
<p>Recent research has introduced the LLMs-as-evaluator paradigm, utilizing LLMs to compare candidate responses with the assumption that LLMs have learned to assign higher probabilities to high-quality and fluent texts [7, 8, 14, 16, 32]. FairEval [33] finds that the ranking result of candidate responses can be easily altered by exchanging their order of appearance in prompt context. They swap the position of two responses for two rounds of scores and ensemble the results of multiple LLM runs in pursuit of result stability. Similarly, LLM-as-a-judge [43] also observes the position bias. It swappes the order of two answers and retaines the evaluation score only if the results remain consistent in both orders. In cases of inconsistency after swapping, it declares a tie. Essentially, they regard each LLM as an individual neuron and construct single-layer narrow networks, aggregating evaluation scores from a limited quantity of LLMs. FairEval [33] identifies that the optimal performance is achieved when three LLM neurons are employed; an increase in the number of neurons leads to a decline in effectiveness. Moreover, existing benchmarks for assessing LLMs' performance in evaluating text quality lack diverse evaluation capabilities. For instance, the benchmark utilized by FairEval comprises only 80 samples. Thus, there is an urgent requirement for more comprehensive datasets that can holistically evaluate LLMs' ability to assess the quality of generated text.</p>
<p>In this paper, we first delve into the realm of deeper and wider LLM networks for LLM evaluation. Systematic design has led to the development of deeper and wider neural networks, such as ResNets [11] for depth and ResNeXT [37] for width. These advancements have resulted in enhanced learning and ultimately improved performance compared to relatively shallow and narrow networks [17]. Therefore, we aim to increase the number of LLM neurons and layers that collaborate in the evaluation network, with the goal of creating a fairer LLM evaluator. It has been observed that different neurons in each layer of state-of-the-art deep networks match human-interpretable but distinct concepts [44, 39, 22, 2, 15, 27, 3]. Moreover, the features in different layers focus on different views for samples [10, 26, 20]. For example, the features in lower layers tend to encode more local contents with basic syntactic representations in NLP. Higher layers capture more complex semantics and usually produce higher-level semantic representations [9, 25]. However, in the evaluation network composed of different LLM neurons, we can only achieve forward computation and cannot update parameters as in deep neural networks where the different neurons are responsible for detecting different concepts and different layers abstract different granularity features through backpropagation. Therefore, in the network design, we artificially implement these two important</p>
<p>characteristics. Specifically, for each evaluation sample, we first ask the LLM about the candidate perspectives that could be used to assess the sample quality. Each perspective is explicitly injected into the evaluation process of each LLM neuron in the first layer as the concept that this neuron is responsible for detecting, outputting evaluation scores and reasons as the neuron's representation. For subsequent layers in the multi-layer LLM network, each layer receives representations from all neurons in the previous layer, integrating and abstracting the previously learned local evaluation information to obtain a more comprehensive evaluation result.</p>
<p>Interestingly, our wider and deeper LLM network can be likened to the process of paper review. First, each reviewer independently assigns a score based on their own research background and understanding of the paper (the evaluation sample), representing the first layer. Then, a discussion phase follows, during which all reviewers take into account each other's evaluations to update their scores. This iterative process can continue through multiple rounds, analogous to subsequent layers in our network. Finally, the Chair or Editor consolidates all the reviewers' opinions to make the decision on whether the paper will be accepted. The final experiments reveal that a LLM network with a wider scope yet limited to only two layers performs the best. This coincidence aligns with the current mainstream conference paper review process, where many reviewers are brought in for blind reviews and a single round of discussion, after which the chair makes the final decision.</p>
<p>To facilitate the research on LLM evaluator, we also build a comprehensive benchmark that encompasses 15 tasks, such as question answering, text summarization, and programming. Additionally, the benchmark assesses 8 different abilities, such as logical reasoning, semantic understanding and text composition. To ensure thorough evaluation, we have compiled 2,553 samples, each of which comes with human-annotated preferences, 31 times larger than the dataset used in FairEval [33].</p>
<p>The major contributions of this paper are summarized as follows:</p>
<ul>
<li>We explore the multi-layer wide network where each neuron possesses distinct neuron role and cooperative evaluations are performed among different layers of neurons. We observe that a wider two-layer LLM network, namely WideDeep, can achieve the best evaluation results, which is essentially a paper review process.</li>
<li>We introduce the largest and most diverse benchmark LLMEval ${ }^{2}$ for LLM evaluator. LLMEval ${ }^{2}$ involves diverse ability evaluation, and contributes to more sufficient assessment.</li>
<li>Our WideDeep network's effectiveness has been extensively validated through thorough experimentation on existing two benchmarks and LLMEval ${ }^{2}$. This validation reveals a notable 3.5-point increase in accuracy, coupled with a noteworthy enhancement of 0.06 in the kappa correlation coefficient. Notably, we've successfully addressed a limitation previously identified in FairEval, where employing more than three LLMs failed to yield performance enhancements. This accomplishment underscores that augmenting the number of LLM neurons contributes to a more equitable evaluation process.</li>
<li>We also leverage WideDeep to assess the performance of the Chinese LLMs. WideDeep's advantages have further expanded compared to English benchmarks, with improvements of 6pts, 5.5pts, and 0.09 in accuracy, F1 score, and kappa correlation coefficient, respectively, achieving a labeling accuracy of $74 \%$ and reaching a $93 \%$ agreement level among humans. We demonstrate WideDeep has accelerated the LLM evaluation process by 4.6 times and decreased the average annotation cost per sample by $60 \%$.</li>
</ul>
<h1>2 Related Work</h1>
<p>There has been a proliferation of LLM-based chatbots that harness instruction fine-tuning and learn from human feedback to unlock the ability of responding to questions following human preferences [1, 38, 29]. However, assessing whether LLM is well aligned with human preference is not a straightforward task. Traditional LLM benchmarks like MMLU [12] fall short in effectively distinguishing between these aligned models and the base models, as they only require LLM to answer multiple-choice questions. Even if we have evaluation benchmarks available, such as several questions and manually annotated responses, commonly used ngram-based metrics like BLEU [24] and ROUGE [19], as well as embedding-based metrics like BERTScore [40] and MoverScore [41], can only measure lexical and semantic similarity between a generated response and the reference response. These metrics have been shown to have relatively low correlation with human judgments [21].</p>
<p>In recent research, it has been noticed that extensive generative pre-training has enabled LLMs to excel in assigning higher probabilities to high-quality responses based on given instructions and context [8]. Building on this insight, researchers have leveraged ChatGPT and GPT-4 to evaluate numerous natural language generation tasks, including text summarization, story generation, data-totext generation, and machine translation, showcasing remarkable performance [21, 32, 16]. However, subsequent investigations have unveiled certain issues with LLM evaluators, particularly concerning biases related to position and verbosity [33, 43]. To address these biases, researchers have adopted techniques such as swapping the order of candidate responses and conducting multiple independent evaluations, which effectively mitigates biases and yields more reliable results. In this paper, we propose a unified approach, considering previous LLM evaluators as one-layer narrow LLM networks with varying numbers of neurons. Each neuron independently scores candidate samples from the same evaluation perspective. Drawing inspiration from deep neural networks, we delve into wider and deeper LLM networks, assigning distinct functionalities and roles to different LLM neurons. Each layer takes evaluation outputs from all neurons in the previous layer, resulting in a fairer LLM evaluator. Furthermore, we contribute to the field by creating an extensive benchmark for evaluation across various tasks, aiming to drive progress and innovation in this research domain.</p>
<h1>3 Methodology</h1>
<p>In this section, we begin by introducing the multi-layer wide LLM network in Sec.3.1. Next, we provide a more intuitive explanation from the perspective of academic paper review in Sec.3.2.</p>
<h3>3.1 Deeper and Wider LLM Network</h3>
<p>State-of-the-art deep neural networks are composed of interconnected layers of neurons, where each neuron performs a specific function by processing input from other neurons and producing output for the next layer. At the bottom layer of the network, a considerable number of neurons are responsible for processing the input data and extracting diverse features that are relevant to the task at hand. As we move up the layers of the network, the neurons capture higher-level features and relationships by combining the lower-level features learned in preceding layers, which can be critical for solving more complex tasks. However, it remains unexplored whether widening and deepening the single-layer LLM network with a fixed number of neurons in Figure 1 (a) can improve the evaluation performance. Inspired by this, we enhance the network by augmenting the number of neurons in each layer and increasing the depth of the network in Figure 1 (b), making the LLM network deeper and wider. Building such a network involves three key points: The role of each neuron, The connection of different layers and The aggregation of final results.</p>
<p>The role of each neuron. In deep neural networks, different neurons perform distinct functions where they may learn to respond to different linguistic features such as word order, grammar or semantics by back-propagation optimization. The role of each neuron is learned by gradient back-propagation to adjuest the neuron parameters. However, within our LLM network, each neuron represents a frozen LLM, and we are unable to adjust the parameters of the network. To keep different functions for LLM neurons, we first query LLMs to generate diverse neuron roles for each sample according to its content. Concretely, given a testing question $q$, two candidate responses $A=\left{a_{1}, a_{2}\right}$, a prompt $\pi_{0}$, and a template $\mathbb{F}()$, the generation of neuron roles describes a probability distribution $p_{\text {LLM }}\left(\mathrm{P} \mid \mathbb{F}\left(q, A, \pi_{0}\right)\right)$ over output perspectives $\mathrm{P}=\left{\mathrm{p}<em 2="2">{1}, \mathrm{p}</em>}, \ldots, \mathrm{p<em 0="0">{n}\right}$ as computed by the LLM. $\mathbb{F}()$ aims to fill the question $q$ and responses $A$ into the slots of prompt $\pi</em>$ is summarized as follows:}$. The neuron role prompt $\pi_{0</p>
<h2>Neuron Role Prompt $\pi_{0}$</h2>
<p>Please help me summarize that for a user question " ${{$ question $} } "$ ", if I want to determine which of two answers is better, from what angles do we need to evaluate? The two answers are respectively " ${{$ answer_1 $} } "$ and " ${{$ answer_2 $} } "$.
Output the name and evaluation content of each angle. Each line is an evaluation angle. Use a newline to separate different evaluation angles. Each evaluation angle Name starts with \$ and ends with \&amp;.</p>
<p>For the generated neuron roles $\mathrm{P}=\left{\mathrm{p}<em 2="2">{1}, \mathrm{p}</em>}, \ldots, \mathrm{p<em i="i">{n}\right}$, we respectively assign $\mathrm{p}</em>}$ to each neuron $\mathrm{n<em 1="1">{i}$ in each layer, simulating the different roles of neurons in deep neural networks. For example, as shown in Figure 1 (b), the LLM, such as gpt-3.5-turbo, generates four perspectives including coherence, relevance, harmlessness and accuracy, and then the LLM network would possess four neurons in each layer where each neuron played by the LLM is respectively responsible for evaluating the candidate responses from one of the four perspectives. For the input layer of LLM network, given a prompt $\pi</em>}$, and a template $\mathrm{F}()$, each neuron $\mathrm{n<em _mathrm_LLM="\mathrm{LLM">{i}$ defines a probability distribution $p</em>}}^{i}\left(\mathrm{e<em 1="1">{1}^{i} \mid q, A\right)$ over output evaluation result $\mathrm{e}</em>$ as computed by the LLM:}^{i</p>
<p>$$
p_{\mathrm{LLM}}^{i}\left(\mathrm{e}<em _mathrm_LLM="\mathrm{LLM">{1}^{i} \mid q, A\right)=p</em>}}^{i}\left(\mathrm{e<em i="i">{1}^{i} \mid \mathrm{F}\left(q, A, \mathrm{p}</em>}, \pi_{1}\right)\right) p_{\mathrm{LLM}}\left(\mathrm{p<em 0="0">{i} \mid \mathrm{F}\left(q, A, \pi</em>\right)\right)
$$</p>
<p>where the input layer evaluation prompt $\pi_{1}$ for LLMs is described as follows:</p>
<h1>Input Layer Evaluation Prompt $\pi_{1}$</h1>
<p>You are a member of the expert group for checking the quality of answer. You are given a question and two answers. Your job is to decide which answer is better for replying question. [Question]
${{$ question $}}$
[The Start of Assistant 1's Answer]
{{answer_1}}
[The End of Assistant 1's Answer]
[The Start of Assistant 2's Answer]
${{$ answer_2}}
[The End of Assistant 2's Answer]
[System]
Take ${{$ perspective $}}$ as the Angle of View, we would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Each assistant receives an overall score on a scale of 1 to $10, \ldots$
...
PLEASE OUTPUT WITH THE FOLLOWING FORMAT:
<start output>
Evaluation evidence: <your evaluation explanation here>
Score of Assistant 1: <score>
Score of Assistant 2: <score>
<end output>
Now, start your evaluation:</p>
<p>The connection of different layers. In naive deep neural networks, the neurons in each layer are interconnected through weighted connections. These connections are responsible for transmitting information from one layer to the next during the forward pass of the network. Concretely, within each hidden layer, each neuron is connected to all the neurons in the previous layer. The connections between neurons in the hidden layers are weighted, and the weights are learned through the training process to allow the network to capture and represent complex patterns and features from the input data. In our LLM network, there is neither numerical weights nor training optimization. Therefore, inspired by Stacked LLMs [30], we write the prompt $\pi_{2}$ which serves as the weights to connect each neuron with all neurons in the previous layer. Similarly, each neuron $\tilde{\mathrm{n}}<em h="h" t="t">{i}$ in the $l</em>}$ layer defines a probability distribution $p_{\mathrm{LLM}}^{i}\left(\mathrm{e<em l="l">{l}^{i} \mid q, A\right)$ over output evaluation result $\mathrm{e}</em>$ as computed by the LLM:}^{i</p>
<p>$$
p_{\mathrm{LLM}}^{i}\left(\mathrm{e}<em j="1">{l}^{i} \mid q, A\right)=\sum</em>}^{n} p_{\mathrm{LLM}}^{i}\left(\mathrm{e<em l-1="l-1">{l}^{i} \mid \mathrm{F}\left(q, A, \mathrm{e}</em>}^{j}, \mathrm{p<em 2="2">{l-1}^{j}, \pi</em>}\right)\right) p_{\mathrm{LLM}}^{j}\left(\mathrm{e<em j="j">{l-1}^{j} \mid \mathrm{F}\left(q, A, \mathrm{p}</em>\right)\right)
$$}, \pi_{1</p>
<p>where $n$ is the number of neurons in the previous layer, $\mathrm{p}<em h="h" t="t">{l-1}^{j}$ is the role of $j</em>$ neuron in the $(l-1)<em 2="2">{t h}$ layer. $\pi</em>$ is the hidden layer evaluation prompt for LLMs which is described as follows:</p>
<h1>Hidden Layer Evaluation Prompt $\varepsilon_{2}$</h1>
<p>You are a member of the expert group for checking the quality of answer. You are given a question and two answers. Your job is to decide which answer is better for replying question. [Question]
${{$ question $}}$
[The Start of Assistant 1's Answer]
${{$ answer_1 $}}$
[The End of Assistant 1's Answer]
[The Start of Assistant 2's Answer]
${{$ answer_2 $}}$
[The End of Assistant 2's Answer]
[System]
You and your colleagues in the expert group have conducted several rounds of evaluations.
[The Start of Your Historical Evaluations]
${{$ Your own evaluation from last layer $}}$
[The End of Your Historical Evaluations]
[The Start of Other Colleagues' Evaluations]
${{$ Other evaluations from last layer $}}$
[The End of Other Colleagues' Evaluations]
Again, take ${{$ inherited perspectives $}}$ as the Angle of View, we would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. Each assistant receives an overall score on a scale of 1 to $10, \ldots$
...
PLEASE OUTPUT WITH THE FOLLOWING FORMAT:
<start output>
Evaluation evidence: <your evaluation explanation here>
Score of Assistant 1: <score>
Score of Assistant 2: <score>
<end output>
Now, start your evaluation:
Note: The slot " ${{$ Your own evaluation from last layer $}}$ " should be filled in the output evaluation evidece and score of the neuron in the prior layer that corresponds to the same position as the current neuron, while other neurons' output values are filled in " ${{$ Other evaluations from last layer $}}$ ". The slot " ${{$ inherited perspectives $}}$ " represents the union of all neurons' roles in the previous layer.
The aggregation of final results. The output layer in deep neural network generates the final prediction for the task. Similarly, we aggregate the evaluation results from neurons in the LLM network, and there actually exists a variety of aggregation strategies to derive the ultimate evaluation conclusion. The strategies employed in this study include: (1) Averaging the scores from all neurons in the network for each response and subsequently comparing the average scores of the responses to determine which is better $\left(\mathrm{c}_{1}^{<em>}\right)$. (2) Comparing the evaluation scores of the responses from each neuron to choose the better one, and then voting over neurons in all layers or each layer $\left(\mathrm{c}_{2}^{</em>}\right)$.</p>
<p>$$
\begin{gathered}
c_{1}^{<em>}=\operatorname{argmax} \sum_{l} \sum_{i=1}^{n} p_{\mathrm{LLM}}^{i}\left(\mathrm{e}<em 2="2">{l}^{i} \mid q, A\right) \
c</em>^{</em>}=\max <em i_="i," l="l">{\text {count }} \bigcup</em> \mid q, A\right)\right}
\end{gathered}
$$}\left{\operatorname{argmax} p_{\mathrm{LLM}}^{i}\left(\mathrm{e}_{l}^{i</p>
<h3>3.2 Explain LLM Network as a Academic Paper Review Process</h3>
<p>To offer more coherent insights into our deeper and wider LLM network, we present an analogy using the perspective of academic paper review, as depicted in Figure 2. The review process usually consists of three key stages: blind review, reviewer discussion, and chair summary. In the blind review stage, each reviewer diligently examines the candidate paper based on their research background. Subsequently, they provide feedback in the form of a written report, akin to the input layer of our LLM network. Following the individual blind reviews, reviewers may engage in discussions to further</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Academic paper review process for evaluating the quality of candidate responses, comprising of blind review, reviewer discussion and chair summary.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left is the distribution of all datasets in LLMEval ${ }^{2}$. The outer and middle circles display the names of datasets and their associated tasks, respectively. The inner circle represents the proportions of three categories of data in the benchmark concerning the preference between two responses: the first one being better, the second one being better, or the two responses having similar quality. Right illustrates covered 8 evaluation abilities of LLMEval ${ }^{2}$.
evaluate the candidate responses. These discussions resemble the subsequent layers of our network, where reviewers compare and contrast their assessments, explore areas of agreement or disagreement, and identify potential biases or blind spots. This iterative process of discussion can span multiple rounds, analogous to the deep layers in our network. Finally, the chair makes a decision, akin to the result aggregation step in our network, by considering the collective feedback from the reviewers. By illustrating the functioning of our LLM network through the academic paper review analogy, we aim to provide a more intuitive understanding of its operations and effectiveness.</p>
<h1>4 LLMEval ${ }^{2}$ Benchmark</h1>
<p>In addition to exploring the wider and deeper LLM network to obtain fairer evaluation results, we also seek to propose improvements to the current LLM evaluator benchmark. The widely used benchmarks, such as FairEval [33] and MT-bench [43], only consist of 80 testing samples, leading to unstable evaluation results and making it challenging to comprehensively assess the LLM evaluator's capabilities. While PandaLM constructs a test set comprising 999 samples, it still lacks statistics for different abilities and suffers from a limitation in data diversity, as it solely relies on a single self-instruct source [35]. To address these shortcomings, we present LLMEval ${ }^{2}$, the largest and most diverse evaluation benchmark for the LLM Evaluator to date.
Benchmark Construction. Assessing the capabilities of the LLM evaluator requires data that includes a question, a pair of candidate responses, and a human label indicating the preferred response. We notice that the format of the evaluation data resembles that of the samples used to train a reward model. The reward trainer aims to grasp human preferences by ranking the candidate responses based on human labels. Thus, we compile datasets used for training a reward model, totaling 15 datasets (shown as the outer circle in Figure 3 left). Next, we employ data sampling techniques to balance data diversity and evaluation costs, resulting in a collection of 2,553 evaluation samples, each annotated with human preferences, across all 15 datasets.
Statistics. In this benchmark, 1,050 samples of response 1 are considered to align with human preferences, while another 1,021 samples of response 2 are deemed superior. Additionally, two responses from the 482 samples are considered difficult to differentiate in terms of quality. As illustrated in Figure 3 (left), the benchmark encompasses eight tasks: Story Generation, Text Summarization, Data-to-Text Generation, Retrieval QA, Dialogue, Commonsense NLI, Open-domain QA, and Programming. These tasks evaluate eight abilities of the benchmark: Induction and Summarization, Semantic Understanding, Knowledge QA, Logical Reasoning, Text Composition, Dialogue, Harmlessness and Multilingual.</p>
<h2>5 Experiments</h2>
<p>In this section, our primary focus is to address the following research questions: (RQ1) Does a LLM network with a wider and deeper structure yield improved evaluation performance? (RQ2) Which neuron roles does LLM prioritize, and how do they impact the results? (RQ3) To what extent can our LLM evaluator accelerate manual annotation speed in real LLM business?</p>
<h3>5.1 Experimental Settings</h3>
<p>Datasets. We conduct evaluations on three benchmarks, consisting of two existing datasets, FairEval [33] and PandaLM [34], along with our newly constructed dataset, LLMEval ${ }^{2}$. FairEval comprises a total of 80 samples, and the candidate responses are generated by Vicuna-13b and ChatGPT. Meanwhile, PandaLM consists of 999 samples, which were drawn from the diverse human evaluation dataset of self-instruct [35]. The paired responses in PandaLM are generated by LLaMA-7B, Bloom-7B, Cerebras-GPT-6.7B, OPT-7B, and Pythia-6.9B.</p>
<p>Implementation Details. We use accuracy (Acc), Macro-F1, and the kappa correlation coefficient (Kap.) as our evaluation metrics. For reporting the main results, we utilize gpt-3.5-turbo as the LLM neuron on the full dataset due to cost constraints. Additionally, we construct a smaller version called LLMEval ${ }^{2}$ mini, which consists of 20 samples drawn from each of the 15 datasets, resulting in a total of 300 samples. These samples are used for analytical experiments.</p>
<h3>5.2 Experimental Results</h3>
<p>Table 1 shows the main results of our multi-layer wide LLM network WideDeep compared with prior single-layer network with fixed number of neurons FairEval [33]. We implement four variants WideDeep $\mathrm{c}<em 1="1">{1}^{<em>}$, WideDeep $\mathrm{c}_{2}^{</em>}\left(l</em>}\right)$, WideDeep $\mathrm{c<em 1="1">{2}^{<em>}\left(l_{2}\right)$ and WideDeep $\mathrm{c}_{2}^{</em>}($ all $)$. WideDeep $\mathrm{c}</em>^{<em>}$ indicates averaging the scores from all neurons in LLM network and choosing the response with higher score ( $c_{1}^{</em>}$ in Equation 3). For the latter three, we aggregate the results based on $c_{2}^{*}$ in Equation 3. WideDeepc $\left(l_{1}\right)$ represents voting the evaluation results only in the $1_{s t}$ layer and WideDeep $\mathrm{W}<em 2="2">{2}\left(l</em>$ layer of LLM network. Voting all evaluation results in all layers is}\right)$ means only voting in the $2_{n d</p>
<p>Table 1: Main Results on FairEval, PandaLM and LLMEval^{2} benchmarks.</p>
<table>
<thead>
<tr>
<th></th>
<th>FairEval Benchmark</th>
<th></th>
<th></th>
<th>PandaLM Benchmark</th>
<th></th>
<th></th>
<th>LLMEval^{2} Benchmark</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc</td>
<td>Macro-F1</td>
<td>Kap.</td>
<td>Acc</td>
<td>Macro-F1</td>
<td>Kap.</td>
<td>Acc</td>
<td>Macro-F1</td>
<td>Kap.</td>
</tr>
<tr>
<td>FairEval [33]</td>
<td>0.587</td>
<td>–</td>
<td>0.31</td>
<td>0.7147</td>
<td>0.5531</td>
<td>0.4891</td>
<td>0.5735</td>
<td>0.4663</td>
<td>0.2807</td>
</tr>
<tr>
<td>WideDeep $\mathrm{c}_{1}^{*}$</td>
<td>0.6063</td>
<td>0.4457</td>
<td>0.3336</td>
<td>0.7447</td>
<td>0.5834</td>
<td>0.5371</td>
<td>0.5946</td>
<td>0.4446</td>
<td>0.3197</td>
</tr>
<tr>
<td>WideDeep $\mathrm{c}<em 1="1">{2}^{*}\left(l</em>\right)$</td>
<td>0.6125</td>
<td>0.4394</td>
<td>0.3215</td>
<td>0.7467</td>
<td>0.6481</td>
<td>0.5524</td>
<td>0.5895</td>
<td>0.4622</td>
<td>0.3155</td>
</tr>
<tr>
<td>WideDeep $\mathrm{c}<em 2="2">{2}^{*}\left(l</em>\right)$</td>
<td>0.6188</td>
<td>0.4479</td>
<td>0.3472</td>
<td>0.7447</td>
<td>0.6295</td>
<td>0.5504</td>
<td>0.5962</td>
<td>0.5028</td>
<td>0.3345</td>
</tr>
<tr>
<td>WideDeep $\mathrm{c}_{2}^{*}($ all $)$</td>
<td>0.6188</td>
<td>0.4465</td>
<td>0.3462</td>
<td>0.7568</td>
<td>0.6545</td>
<td>0.5726</td>
<td>0.6036</td>
<td>0.5041</td>
<td>0.3440</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of accuracy between WideDeep and FairEval under eight abilities.</p>
<p>denoted as WideDeep $\mathrm{c}_{2}^{*}($ all $)$. The best results over evaluation metrics are in bold. Note that we have attempted to use deeper LLM networks (more than 2 layers), but it resulted in a decrease in performance. Therefore, in our main experiment, we do not restrict the number of neurons in each layer, but we limit the network depth to 2 layers. We will discuss the impact of network depth on the results in the analysis experiment.</p>
<p>We can observe that our multi-layer wide LLM network outperforms FairEval significantly, with an increase in accuracy by 3.2pts, 4.4pts, and 3pts, and an improvement in kappa correlation by 3.7pts, 8.4pts, and 6.3pts on the three respective benchmarks. Compared with voting in each layer of the LLM network WideDeep $\mathrm{c}<em 2="2">{2}^{<em>}\left(l_{1}\right)$ and WideDeep $\mathrm{c}_{2}^{</em>}\left(l</em>}\right)$, WideDeep $\mathrm{c<em 1="1">{2}^{<em>}($ all $)$ which votes evaluation results from all layers achieves the better overall performance. Meanwhile, in comparison withWideDeep $\mathrm{c}_{2}^{</em>}\left(l</em>}\right)$, WideDeep $\mathrm{c<em 2="2">{2}^{*}\left(l</em>\right)$ reaches the higher performance which demonstrates that the effectiveness of deepening the LLM network.</p>
<h3>5.3 Experimental Analyses</h3>
<p>Due to cost constraints, we extract 20 samples from each of the 15 datasets included in LLMEval^{2}, resulting in a total of 300 testing samples, namely LLMEval^{2} mini. This mini dataset allows us to easily assess the impact of network width, depth and neuron roles.</p>
<p>Wider LLM network is a Fairer Evaluator. Table 2 illustrates the performance improvement as the number of neurons in each layer of the LLM network ( $n$ ) increases. When the number of layers $l$ is limited to one or two, we observe a consistent upward trend in performance. This demonstrates the effectiveness of widening the LLM network, fully unleashing the potential of a group of neurons.</p>
<p>Slightly deeper LLM network is a Fairer Evaluator. From Table 2, we can also observe that increasing the number of layers ( $l$ ) in the network from 1 to 2 while keeping the number of neurons</p>
<p>Table 2: Performance on wider and deeper network. NL indicates no limit on the number of neurons.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$n=1$</th>
<th style="text-align: center;">$n=2$</th>
<th style="text-align: center;">$n=3$</th>
<th style="text-align: center;">$n=4$</th>
<th style="text-align: center;">$n=\mathrm{NL}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$l=1$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">0.6033</td>
<td style="text-align: center;">0.6333</td>
<td style="text-align: center;">0.6300</td>
<td style="text-align: center;">0.6267</td>
<td style="text-align: center;">0.6300</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">0.4709</td>
<td style="text-align: center;">0.4704</td>
<td style="text-align: center;">0.4793</td>
<td style="text-align: center;">0.4885</td>
<td style="text-align: center;">0.5116</td>
</tr>
<tr>
<td style="text-align: center;">$l=2$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">0.6333</td>
<td style="text-align: center;">0.6400</td>
<td style="text-align: center;">0.6433</td>
<td style="text-align: center;">0.6500</td>
<td style="text-align: center;">0.6567</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">0.4819</td>
<td style="text-align: center;">0.5187</td>
<td style="text-align: center;">0.4772</td>
<td style="text-align: center;">0.5159</td>
<td style="text-align: center;">0.5666</td>
</tr>
<tr>
<td style="text-align: center;">$l=3$</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">0.6533</td>
<td style="text-align: center;">0.6400</td>
<td style="text-align: center;">0.6433</td>
<td style="text-align: center;">0.6300</td>
<td style="text-align: center;">0.6500</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">0.5076</td>
<td style="text-align: center;">0.5084</td>
<td style="text-align: center;">0.4764</td>
<td style="text-align: center;">0.4798</td>
<td style="text-align: center;">0.5053</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Word clouds of neuron roles on (a) Dialogue (b) Harmlessness QA (c) Story Generation (d) Programming task.
per layer fixed resulted in significant performance improvements. However, further deepening the network led to a slight decline in performance. The reason for this could be that deeper LLM networks tend to hold more homogeneous information, similar to overfitting in deep neural networks.</p>
<p>Neuron roles are diverse and effective. To mimic the characteristic of different neurons in a neural network being responsible for detecting different concepts, we require the LLM to generate potential evaluation dimensions before assessing the samples. In the network, each LLM in every layer is responsible for evaluating one specific dimension. To elucidate the roles that LLM assigns to neurons for each task, we present word clouds for four tasks in Figure 5: dialogue, harmlessness QA, story generation, and programming. Note that we did not explicitly provide task names or definitions to LLM when generating the roles. Remarkably, these assigned roles appear to be logical and adaptable, dynamically changing based on the specific task characteristics. For harmlessness QA,</p>
<p>Table 3: Effectiveness of neuron roles. NL indicates no limit on the number of neurons in each layer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Macro-F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">WideDeep $(l=2, n=2)$</td>
<td style="text-align: center;">0.6400</td>
<td style="text-align: center;">0.5187</td>
</tr>
<tr>
<td style="text-align: center;">WideDeep $(l=2, n=2)$ W/0 Neuron Roles</td>
<td style="text-align: center;">0.6267</td>
<td style="text-align: center;">0.4992</td>
</tr>
<tr>
<td style="text-align: center;">WideDeep $(l=2, n=\mathrm{NL})$</td>
<td style="text-align: center;">0.6567</td>
<td style="text-align: center;">0.5666</td>
</tr>
<tr>
<td style="text-align: center;">WideDeep $(l=2, n=\mathrm{NL})$ W/0 Neuron Roles</td>
<td style="text-align: center;">0.6400</td>
<td style="text-align: center;">0.5086</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Performance under different neuron quantity constraints.</p>
<p>Table 4: Performance on chinese LLM evaluation with gpt-4 as the neurons.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Macro-F1</th>
<th style="text-align: center;">Kap.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.6700</td>
<td style="text-align: center;">0.6261</td>
<td style="text-align: center;">0.4587</td>
</tr>
<tr>
<td style="text-align: center;">FairEval</td>
<td style="text-align: center;">0.6800</td>
<td style="text-align: center;">0.6692</td>
<td style="text-align: center;">0.5074</td>
</tr>
<tr>
<td style="text-align: center;">WideDeep (Ours)</td>
<td style="text-align: center;">0.7400</td>
<td style="text-align: center;">0.7245</td>
<td style="text-align: center;">0.5965</td>
</tr>
</tbody>
</table>
<p>LLM generates roles related to security, including Safety, Legal, and Ethical. In story generation, LLM assigns roles like Coherence, Relevance, and Character. Meanwhile, the programming task involves algorithm-related roles, such as Correctness and Efficiency. Having reliable and diverse neuron roles allows the LLM network to effectively utilize multiple neurons' value when the network becomes wider. As illustrated in Table 3, we conduct two groups of experiments where the number of layers $l$ is set to 2 and neurons $n$ to no limit, respectively. The results show that the accuracy and Macro-F1 metrics decrease by $1.33 \%, 1.67 \%$ and $1.95 \%, 5.80 \%$ without neuron roles.
Widedeep can consume more neurons than baselines. With a wider and deeper architecture and diverse neuron roles, our WideDeep network can utilize an unlimited number of LLM neurons. Previous methods, such as FairEval [33], can also harness a large number of LLM neurons by integrating multiple independent LLM evaluations. In Figure 6, we demonstrate that Deepwide can more efficiently leverage LLM neurons to achieve significantly improved accuracy across almost all neuron quantity constraints than FairEval. Moreover, as the number of neurons increases, the performance continues to improve. For our experiments, we opted for a two-layered Deepwide network, where, with an odd-numbered neuron constraint, the second layer's neurons are reduced by one. On the other hand, FairEval's performance saturates when the number of neurons reaches five, and any further increase leads to a decline in performance. This observation aligns with the conclusions of the original research, further confirming the positive impact of our deeper network and diversified neuron roles.</p>
<h1>5.4 Application in Chinese LLM Evaluation</h1>
<p>We also utilize WideDeep to assess the performance of the Chinese LLMs by determining which of the three responses under the same prompt is better. Due to variations in evaluation data and tasks, the traditional manual annotation process involves multiple steps such as annotator training, smallscale trial annotation, selection of official annotators, and cross-annotation by multiple individuals. However, with the assistance of WideDeep, this process has been simplified to involve only a fixed team of professional annotators who perform sampling checks on the results generated by WideDeep.</p>
<p>In Table 4, we present a comparison of the effectiveness of WideDeep, FairEval, and standalone gpt-4 Evaluator in Chinese LLM evaluation. WideDeep's advantages have further expanded compared to English benchmarks, with improvements of 6pts, 5.5pts, and 8.9pts in accuracy, F1 score, and kappa correlation coefficient, respectively, achieving a labeling accuracy of $74 \%$. The agreement among humans during the Chinese LLM evaluation stands at $80 \%$, which indicates that WideDeep has reached a $93 \%$ agreement level among humans. In fact, with each point increase in accuracy, a significant amount of manual annotation time can be reduced. Assuming the LLM evaluator's accuracy is x , the annotators only need to review $\frac{0.8-2}{1-2} \%$ of the data annotated by the LLM Evaluator to correct the labeling errors and achieve an $80 \%$ accuracy level, aligning with manual annotation. Therefore, the annotators only need to inspect $23 \%$ of the predicted results from WideDeep, while they would have to inspect $37.5 \%$ from FairEval and $39.3 \%$ from GPT-4. Overall, WideDeep has accelerated the LLM evaluation process by 4.6 times, saving a significant amount of time for human annotators. Furthermore, the average annotation cost per sample has decreased by $60 \%$.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we explore whether evaluation performance can be improved in deeper and wider LLM networks. Specifically, each neuron within the LLM network assumes a distinct evaluation role, and multiple neurons interact and collaborate, much like the interaction observed in deep neural networks. The evaluation process follows a feedforward approach, with each layer of neurons receiving inputs from the previous layer, facilitating a thorough and comprehensive assessment. An intuitive analogy for our designed LLM network can be drawn to the process of academic paper reviewing. Additionally, we present LLMEval ${ }^{2}$, the largest and most diverse evaluation benchmark developed to date for the LLM Evaluator. Through extensive experiments, we demonstrate that a two-layer wider LLM network yields the best results, significantly enhancing the ability of LLMs to evaluate the quality of generated text. Furthermore, we apply our evaluator to assess the performance of Chinese LLMs, where it proves to speed up LLM evaluation process by 4.6 times and decrease the average annotation cost per sample by $60 \%$.</p>
<h2>References</h2>
<p>[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
[2] Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Identifying and controlling important neurons in neural machine translation. arXiv preprint arXiv:1811.01157, 2018.
[3] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences, 117(48):30071-30078, 2020.
[4] Adithya Bhaskar, Alex Fabbri, and Greg Durrett. Prompted opinion summarization with gpt-3.5. In Findings of the Association for Computational Linguistics: ACL 2023, pages 9282-9300, 2023.
[5] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.
[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[7] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study, 2023.
[8] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>[9] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV 16, pages 214-229. Springer, 2020.
[10] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Visualizing and understanding the effectiveness of bert. arXiv preprint arXiv:1908.05620, 2019.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $770-778,2016$.
[12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
[13] Siddhartha Jain, Xiaofei Ma, Anoop Deoras, and Bing Xiang. Self-consistency for open-ended generations. arXiv preprint arXiv:2307.06857, 2023.
[14] Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang Li. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences, 2023.
[15] Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.
[16] Tom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520, 2023.
[17] Hyungtae Lee and Heesung Kwon. Going deeper with contextual cnn for hyperspectral image classification. IEEE Transactions on Image Processing, 26(10):4843-4855, 2017.
[18] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023.
[19] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
[20] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11915-11925, 2021.
[21] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.
[22] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5188-5196, 2015.
[23] OpenAI. Introducing chatgpt. 2022.
[24] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.
[25] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Joao Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020.
[26] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. Understanding the behaviors of bert in ranking. arXiv preprint arXiv:1904.07531, 2019.</p>
<p>[27] Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.
[28] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.
[29] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.
[30] Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux. Deep language networks: Joint prompt training of stacked llms using variational inference. arXiv preprint arXiv:2306.12509, 2023.
[31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
[32] Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048, 2023.
[33] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023.
[34] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization, 2023.
[35] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508. Association for Computational Linguistics, 2023.
[36] Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. Polylm: An open source polyglot large language model. arXiv preprint arXiv:2307.06018, 2023.
[37] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017.
[38] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.
[39] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, pages 818-833. Springer, 2014.
[40] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.
[41] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622, 2019.
[42] Yingxiu Zhao, Bowen Yu, Haiyang Yu, Bowen Li, Chao Wang, Fei Huang, Yongbin Li, and Nevin L Zhang. Causal document-grounded dialogue pre-training. arXiv preprint arXiv:2305.10927, 2023.</p>
<p>[43] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
[44] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. arXiv preprint arXiv:1412.6856, 2014.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Correspondence to: Bowen Yu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#121;&#117;&#98;&#111;&#119;&#101;&#110;&#46;&#121;&#98;&#119;&#64;&#97;&#108;&#105;&#98;&#97;&#98;&#97;&#45;&#105;&#110;&#99;&#46;&#99;&#111;&#109;">&#121;&#117;&#98;&#111;&#119;&#101;&#110;&#46;&#121;&#98;&#119;&#64;&#97;&#108;&#105;&#98;&#97;&#98;&#97;&#45;&#105;&#110;&#99;&#46;&#99;&#111;&#109;</a>, Tingwen Liu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#105;&#117;&#116;&#105;&#110;&#103;&#119;&#101;&#110;&#64;&#105;&#101;&#46;&#97;&#99;&#46;&#99;&#110;">&#108;&#105;&#117;&#116;&#105;&#110;&#103;&#119;&#101;&#110;&#64;&#105;&#101;&#46;&#97;&#99;&#46;&#99;&#110;</a>. ${ }^{2}$ The data and code of this work is available at https://github.com/AlibabaResearch/DAMO-ConvAI/ tree/main/WideDeep</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
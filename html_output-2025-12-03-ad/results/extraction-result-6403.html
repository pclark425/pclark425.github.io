<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6403 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6403</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6403</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-272707652</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.12122v1.pdf" target="_blank">Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement</a></p>
                <p><strong>Paper Abstract:</strong> In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance. Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6403.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6403.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large (72B) math-specialized instruct-tuned decoder-only LLM trained on a mathematics-heavy corpus (Qwen Math Corpus v2) with synthetic CoT and Tool-Integrated Reasoning data, supervised fine-tuning and RL with a math-specific reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained and post-trained on Qwen Math Corpus v2 (>1T tokens) containing math web text, code snippets, exam problems, and large amounts of synthetic mathematical Q-A and CoT/TIR data generated by prior Qwen2/ Qwen2-Math models; supervised fine-tuning uses Rejection Sampling and RL with Qwen2.5-Math-RM.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, AMC2023, AIME2024 (and other multi-difficulty math benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step mathematical problem solving (algebra, arithmetic, contest math, word problems, symbolic computations)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math problems; for TIR tasks, natural-language prompts augmented with code snippets/invocations to a Python interpreter; multiple-choice for some benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>range: grade-school arithmetic (GSM8K) to high-school contest and olympiad level (MATH, AMC, AIME); includes very hard competition problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot Chain-of-Thought (CoT) for base evaluation; zero-shot/few-shot CoT for instruct; Tool-Integrated Reasoning (TIR) using a Python interpreter; best-of-N sampling with reward-model (RM@N) and majority voting (Maj@N) used at inference; RL (GRPO) used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@1 / accuracy on benchmarks; majority-vote (Maj@N) and reward-model selection (RM@N) for best-of-N; counts solved on competition sets (e.g., # solved of 40).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Base MATH (pre-instruct) 66.8 (state-of-the-art among base models). As an instruct model: substantially improved across benchmarks; in TIR mode the 72B model achieves 'close to 90' points on MATH (paper text), and nearly perfect on AMC2023 when using RM-guided sampling; on AIME2024 it solved 9 problems in greedy CoT and 12 in TIR (numbers reported in Table 5). RM@N and Maj@N evaluation modes substantially improve pass@1 relative to greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No neuron-level mechanistic probes reported; qualitative analysis: authors note that the chain-of-thought reasoning traces are often logically sound but suffer from computational inaccuracies; introduction of TIR (Python) fixes many computation errors; reward model provides fine-grained ranking and is used for data selection and RL; training includes decontamination checks (13-gram and LCS ratio) to prevent leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Primary recurring failure is computation error in pure CoT (incorrect arithmetic even when reasoning chain is correct), occasional contamination risks from training data overlaps (mitigated with decontamination), and remaining label-noise from synthesized data. The paper also notes that labeling for RM is based on final-answer correctness (so intermediate-step correctness can be imperfectly supervised).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Improvements in training data synthesis, bilingual CoT/TIR data, and RM-guided fine-tuning yield large gains such that a 7B instruct model can approach or match 72B instruct performance in some settings; TIR (tool use) greatly reduces the need for parameter scaling for computational accuracy (tool-use gives outsized gains vs. pure parameter scale).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6403.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6403.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Medium-size (7B) instruct-tuned math model in the Qwen2.5-Math series, trained with synthesized CoT and TIR data plus RM-guided SFT and RL; demonstrates strong performance and favorable compute–performance tradeoffs when using tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Initialized from Qwen2.5 base weights and further pre-trained/fine-tuned on Qwen Math Corpus v2 with synthetic CoT and TIR samples, supervised fine-tuning with RM-guided rejection sampling, and GRPO reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, AMC2023, AIME2024, GaoKao, CMATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and math problems; contest-style problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language questions (free-response and multiple-choice); for TIR, natural-language plus program calls to Python</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>middle-to-high (GSM8K up to contest-level MATH and AMC/AIME problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot CoT; Tool-Integrated Reasoning (TIR) with Python interpreter; RM best-of-N (RM@N) and majority voting (Maj@N) used at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@1 / accuracy; MATH scores reported (paper uses pass@1 / accuracy metrics across benchmarks); solved counts for contest sets (out of 40).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported MATH scores for this model: CoT ≈ 83.6 and TIR ≈ 85.3 (paper text states these values for the 7B instruct model). With TIR the 7B model can match the performance of much larger (72B) models on several benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper emphasizes how TIR (calling Python) removes arithmetic/computational mistakes; reward-model selection further improves wins. No fine-grained mechanistic probes provided for numeric token representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without TIR, arithmetic mistakes remain a dominant error; with TIR, residual failures include tool-integration orchestration errors (e.g., incorrect parsing of tool outputs) and occasional reasoning-chain mistakes unrelated to raw computation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Significant upshift in effective performance from adding TIR and improved data/training; authors highlight that improved data and techniques can compensate for parameter scale, enabling 7B to rival 72B in many math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6403.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6403.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-Math-1.5B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-Math-1.5B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small (1.5B) instruct-tuned math model trained in the Qwen2.5-Math pipeline; demonstrates that tool-use and RM-guided selection yield surprisingly high performance even at small scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-1.5B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained and fine-tuned on Qwen Math Corpus variants with synthetic CoT and TIR examples; SFT and RL stages use rejection sampling, RM-guided selection and GRPO.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, AMC2023, AIME2024, GaoKao</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and contest math problems; typical grade-school to contest difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language Q-A; TIR uses Python interpreter calls in the generated reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>from elementary arithmetic to hard contest problems (when combined with tools)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot CoT; Tool-Integrated Reasoning (TIR) with Python interpreter; RM@N selection (RM@256 used in some reported evaluations); majority voting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / pass@1; solved counts on contest sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports that the 1.5B instruct model achieves around ~80 on MATH when using the Python interpreter (TIR). On AMC2023, using RM@256 in CoT mode it solved 29/40 problems; with TIR and RM selection it achieves higher solved counts (Table 5 reports up to 36/40 for some RM settings).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors note small models benefit disproportionately from tool integration: reasoning chains are often correct conceptually and require external computation for numeric accuracy; the RM and best-of-N sampling substantially boost small-model results.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Small-model failure modes include hallucinated arithmetic, occasional incorrect use/formatting of tool calls, and lower robustness to complex multi-step logic, but many errors are mitigated by TIR and RM-driven sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Tool integration + RM-guided decoding yields large gains for small models; 1.5B TIR performance approaches or exceeds many much larger models that do not use tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6403.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6403.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-Math-RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-Math Reward Model (Qwen2.5-Math-RM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A math-specific scalar reward model trained to rank candidate reasoning responses, used to filter SFT data, guide rejection sampling, shape RL rewards, and perform RM@N inference selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-RM</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based scalar reward head (initialized from SFT model with scalar head replacing LM head)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (initialized from SFT checkpoints; trained as a reward head on top of transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on 361K English + 257K Chinese math problems; for each problem 6 candidate responses sampled from Qwen2.5-Math (mixture of sizes/versions); positive labels when final answer matches gold, negatives otherwise; listwise ranking loss used.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used internally for selection on GSM8K, MATH, AIME, AMC and other math datasets (and to report RM@N in evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>ranking/scoring of multi-step reasoning outputs (to prefer correct chains and final answers)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>candidate natural-language reasoning chains paired with source problem</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>applies across all difficulties; trained to discriminate correct final answers and (implicitly) quality of derivations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>not a generation model; used to score generated candidate outputs during rejection sampling, SFT construction, RL shaping, and RM@N inference selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improvement in pass@1 when using RM-based selection (RM@N) versus majority voting (Maj@N) or greedy decoding; used in RL objective (GRPO) and to compute shaped reward r = sigma(alpha * r_m) + (r_v - 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports RM@N substantially outperforms majority voting Maj@N across nearly all benchmarks and models (RM@N acts as a reliable performance oracle); used RM@256 and RM@64 in reported contest evaluations producing major gains (e.g., raising solved counts on AMC/AIME). Exact numeric deltas vary by benchmark and model size (tables show RM@8/64/256 improvements over Maj@N).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Training labels are derived from final-answer correctness; listwise ranking loss implemented for efficiency; reward shaping combines RM score and a rule-based verifier (r = sigma(0.5 * r_m) + (r_v - 1)). The RM is central for constructing higher-quality SFT data (via rejection sampling) and for improving RL via GRPO.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>RM relies on final-answer correctness for positives, so it can reward superficial chains that reach correct answer via spurious means; RM could underweight stepwise logical correctness when final answer matches incorrectly derived reasoning; potential bias if candidate pools are not diverse. The paper mitigates some of these by mixing responses from different intermediate models/sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>RM-guided selection and best-of-N sampling yield larger marginal gains for smaller models (making RM an efficient lever to improve pass@1 without scaling parameters dramatically); RM quality improves iteratively as better SFT models are produced and used to generate new RM training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6403.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6403.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-Integrated Reasoning (TIR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-Integrated Reasoning (TIR) with Python Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting & training paradigm where the model is trained and prompted to call an external Python interpreter during its chain-of-thought to perform precise arithmetic, symbolic manipulation, or algorithmic steps, improving numeric accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TIR (method used with Qwen2.5-Math models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>method / prompting + executor integration (model + external tool)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>method not tied to a single size; evaluated with 1.5B, 7B, 72B Qwen2.5-Math-Instruct models</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>TIR training dataset: 190K annotated and 205K synthesized problems; includes annotated TIR chains created via online Rejection Fine-Tuning (RFT) and translated Chinese samples; synthesized queries generated via MuggleMath/DotaMath methods.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, AIME2024, AMC2023 and other benchmarks used for evaluating tool-assisted reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>precise arithmetic computation, symbolic manipulation, algorithmic subroutines embedded inside multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language question plus interleaved 'call tool' code blocks (Python) and parsed interpreter output (the model consumes executor output and continues reasoning); final answers boxed as usual.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>especially helpful on tasks requiring exact numeric computation (from multi-digit arithmetic to eigenvalues / symbolic algebra and competition-level numeric tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot / few-shot TIR prompts (paper provides specific TIR prompt templates in Figure 12); training includes TIR demonstrations; during RL the outputs produced by the Python executor are masked (not used in policy gradients) to stabilize training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@1 / accuracy; improvement in solved counts on contest benchmarks; comparison CoT vs TIR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>TIR consistently improves performance over pure CoT: e.g., 72B TIR 'close to 90' on MATH; 7B TIR improves MATH score (reported 85.3 for 7B TIR vs 83.6 CoT); 1.5B TIR yields MATH ~80; on AMC/AIME contest sets TIR increases solved counts substantially (Table 5 reports many TIR improvements, including near-complete solving for 72B on AMC2023).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors report that TIR addresses the dominant failure mode of CoT — arithmetic/computation errors — by delegating numerically exact steps to a Python executor; they train the model to format calls and parse outputs; during RL they mask executor tokens to avoid attributing reward to the tool's tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Remaining failure types include incorrect code generation or parsing mistakes, incorrect decomposition of a problem into tool calls (i.e., wrong algorithmic approach), and occasional miscoordination between natural-language reasoning and tool outputs; in Chinese the TIR advantage is reported to be smaller, suggesting language-specific gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>TIR provides strong returns across scales: small models gain large absolute improvements, enabling 7B and even 1.5B models to approach large-model performance on computationally demanding benchmarks; authors emphasize tool-use as a scaling-efficient substitute for parameter increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Mugglemath: Assessing the impact of query and response augmentation on math reasoning <em>(Rating: 2)</em></li>
                <li>Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Beyond human data: Scaling self-training for problem-solving with language models <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models (Minerva/Minerva Math references) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6403",
    "paper_id": "paper-272707652",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Qwen2.5-Math-72B-Instruct",
            "name_full": "Qwen2.5-Math-72B-Instruct",
            "brief_description": "Large (72B) math-specialized instruct-tuned decoder-only LLM trained on a mathematics-heavy corpus (Qwen Math Corpus v2) with synthetic CoT and Tool-Integrated Reasoning data, supervised fine-tuning and RL with a math-specific reward model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-72B-Instruct",
            "model_family": "decoder-only transformer",
            "model_size": "72B",
            "training_data_description": "Pre-trained and post-trained on Qwen Math Corpus v2 (&gt;1T tokens) containing math web text, code snippets, exam problems, and large amounts of synthetic mathematical Q-A and CoT/TIR data generated by prior Qwen2/ Qwen2-Math models; supervised fine-tuning uses Rejection Sampling and RL with Qwen2.5-Math-RM.",
            "benchmark_name": "MATH, GSM8K, AMC2023, AIME2024 (and other multi-difficulty math benchmarks)",
            "task_type": "multi-step mathematical problem solving (algebra, arithmetic, contest math, word problems, symbolic computations)",
            "problem_format": "natural-language math problems; for TIR tasks, natural-language prompts augmented with code snippets/invocations to a Python interpreter; multiple-choice for some benchmarks",
            "difficulty_level": "range: grade-school arithmetic (GSM8K) to high-school contest and olympiad level (MATH, AMC, AIME); includes very hard competition problems",
            "prompting_method": "few-shot Chain-of-Thought (CoT) for base evaluation; zero-shot/few-shot CoT for instruct; Tool-Integrated Reasoning (TIR) using a Python interpreter; best-of-N sampling with reward-model (RM@N) and majority voting (Maj@N) used at inference; RL (GRPO) used during training.",
            "performance_metric": "pass@1 / accuracy on benchmarks; majority-vote (Maj@N) and reward-model selection (RM@N) for best-of-N; counts solved on competition sets (e.g., # solved of 40).",
            "performance_value": "Base MATH (pre-instruct) 66.8 (state-of-the-art among base models). As an instruct model: substantially improved across benchmarks; in TIR mode the 72B model achieves 'close to 90' points on MATH (paper text), and nearly perfect on AMC2023 when using RM-guided sampling; on AIME2024 it solved 9 problems in greedy CoT and 12 in TIR (numbers reported in Table 5). RM@N and Maj@N evaluation modes substantially improve pass@1 relative to greedy decoding.",
            "internal_analysis": "No neuron-level mechanistic probes reported; qualitative analysis: authors note that the chain-of-thought reasoning traces are often logically sound but suffer from computational inaccuracies; introduction of TIR (Python) fixes many computation errors; reward model provides fine-grained ranking and is used for data selection and RL; training includes decontamination checks (13-gram and LCS ratio) to prevent leakage.",
            "failure_modes": "Primary recurring failure is computation error in pure CoT (incorrect arithmetic even when reasoning chain is correct), occasional contamination risks from training data overlaps (mitigated with decontamination), and remaining label-noise from synthesized data. The paper also notes that labeling for RM is based on final-answer correctness (so intermediate-step correctness can be imperfectly supervised).",
            "scaling_trend": "Improvements in training data synthesis, bilingual CoT/TIR data, and RM-guided fine-tuning yield large gains such that a 7B instruct model can approach or match 72B instruct performance in some settings; TIR (tool use) greatly reduces the need for parameter scaling for computational accuracy (tool-use gives outsized gains vs. pure parameter scale).",
            "uuid": "e6403.0",
            "source_info": {
                "paper_title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Qwen2.5-Math-7B-Instruct",
            "name_full": "Qwen2.5-Math-7B-Instruct",
            "brief_description": "Medium-size (7B) instruct-tuned math model in the Qwen2.5-Math series, trained with synthesized CoT and TIR data plus RM-guided SFT and RL; demonstrates strong performance and favorable compute–performance tradeoffs when using tool integration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-7B-Instruct",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Initialized from Qwen2.5 base weights and further pre-trained/fine-tuned on Qwen Math Corpus v2 with synthetic CoT and TIR samples, supervised fine-tuning with RM-guided rejection sampling, and GRPO reinforcement learning.",
            "benchmark_name": "MATH, GSM8K, AMC2023, AIME2024, GaoKao, CMATH",
            "task_type": "multi-step arithmetic and math problems; contest-style problems",
            "problem_format": "natural-language questions (free-response and multiple-choice); for TIR, natural-language plus program calls to Python",
            "difficulty_level": "middle-to-high (GSM8K up to contest-level MATH and AMC/AIME problems)",
            "prompting_method": "few-shot CoT; Tool-Integrated Reasoning (TIR) with Python interpreter; RM best-of-N (RM@N) and majority voting (Maj@N) used at inference.",
            "performance_metric": "pass@1 / accuracy; MATH scores reported (paper uses pass@1 / accuracy metrics across benchmarks); solved counts for contest sets (out of 40).",
            "performance_value": "Reported MATH scores for this model: CoT ≈ 83.6 and TIR ≈ 85.3 (paper text states these values for the 7B instruct model). With TIR the 7B model can match the performance of much larger (72B) models on several benchmarks.",
            "internal_analysis": "Paper emphasizes how TIR (calling Python) removes arithmetic/computational mistakes; reward-model selection further improves wins. No fine-grained mechanistic probes provided for numeric token representations.",
            "failure_modes": "Without TIR, arithmetic mistakes remain a dominant error; with TIR, residual failures include tool-integration orchestration errors (e.g., incorrect parsing of tool outputs) and occasional reasoning-chain mistakes unrelated to raw computation.",
            "scaling_trend": "Significant upshift in effective performance from adding TIR and improved data/training; authors highlight that improved data and techniques can compensate for parameter scale, enabling 7B to rival 72B in many math tasks.",
            "uuid": "e6403.1",
            "source_info": {
                "paper_title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Qwen2.5-Math-1.5B-Instruct",
            "name_full": "Qwen2.5-Math-1.5B-Instruct",
            "brief_description": "Small (1.5B) instruct-tuned math model trained in the Qwen2.5-Math pipeline; demonstrates that tool-use and RM-guided selection yield surprisingly high performance even at small scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-1.5B-Instruct",
            "model_family": "decoder-only transformer",
            "model_size": "1.5B",
            "training_data_description": "Pre-trained and fine-tuned on Qwen Math Corpus variants with synthetic CoT and TIR examples; SFT and RL stages use rejection sampling, RM-guided selection and GRPO.",
            "benchmark_name": "MATH, GSM8K, AMC2023, AIME2024, GaoKao",
            "task_type": "multi-step arithmetic and contest math problems; typical grade-school to contest difficulty",
            "problem_format": "natural-language Q-A; TIR uses Python interpreter calls in the generated reasoning",
            "difficulty_level": "from elementary arithmetic to hard contest problems (when combined with tools)",
            "prompting_method": "few-shot CoT; Tool-Integrated Reasoning (TIR) with Python interpreter; RM@N selection (RM@256 used in some reported evaluations); majority voting",
            "performance_metric": "accuracy / pass@1; solved counts on contest sets",
            "performance_value": "Paper reports that the 1.5B instruct model achieves around ~80 on MATH when using the Python interpreter (TIR). On AMC2023, using RM@256 in CoT mode it solved 29/40 problems; with TIR and RM selection it achieves higher solved counts (Table 5 reports up to 36/40 for some RM settings).",
            "internal_analysis": "Authors note small models benefit disproportionately from tool integration: reasoning chains are often correct conceptually and require external computation for numeric accuracy; the RM and best-of-N sampling substantially boost small-model results.",
            "failure_modes": "Small-model failure modes include hallucinated arithmetic, occasional incorrect use/formatting of tool calls, and lower robustness to complex multi-step logic, but many errors are mitigated by TIR and RM-driven sampling.",
            "scaling_trend": "Tool integration + RM-guided decoding yields large gains for small models; 1.5B TIR performance approaches or exceeds many much larger models that do not use tools.",
            "uuid": "e6403.2",
            "source_info": {
                "paper_title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Qwen2.5-Math-RM",
            "name_full": "Qwen2.5-Math Reward Model (Qwen2.5-Math-RM)",
            "brief_description": "A math-specific scalar reward model trained to rank candidate reasoning responses, used to filter SFT data, guide rejection sampling, shape RL rewards, and perform RM@N inference selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-RM",
            "model_family": "transformer-based scalar reward head (initialized from SFT model with scalar head replacing LM head)",
            "model_size": "not specified (initialized from SFT checkpoints; trained as a reward head on top of transformer)",
            "training_data_description": "Trained on 361K English + 257K Chinese math problems; for each problem 6 candidate responses sampled from Qwen2.5-Math (mixture of sizes/versions); positive labels when final answer matches gold, negatives otherwise; listwise ranking loss used.",
            "benchmark_name": "Used internally for selection on GSM8K, MATH, AIME, AMC and other math datasets (and to report RM@N in evaluations).",
            "task_type": "ranking/scoring of multi-step reasoning outputs (to prefer correct chains and final answers)",
            "problem_format": "candidate natural-language reasoning chains paired with source problem",
            "difficulty_level": "applies across all difficulties; trained to discriminate correct final answers and (implicitly) quality of derivations",
            "prompting_method": "not a generation model; used to score generated candidate outputs during rejection sampling, SFT construction, RL shaping, and RM@N inference selection.",
            "performance_metric": "improvement in pass@1 when using RM-based selection (RM@N) versus majority voting (Maj@N) or greedy decoding; used in RL objective (GRPO) and to compute shaped reward r = sigma(alpha * r_m) + (r_v - 1).",
            "performance_value": "Paper reports RM@N substantially outperforms majority voting Maj@N across nearly all benchmarks and models (RM@N acts as a reliable performance oracle); used RM@256 and RM@64 in reported contest evaluations producing major gains (e.g., raising solved counts on AMC/AIME). Exact numeric deltas vary by benchmark and model size (tables show RM@8/64/256 improvements over Maj@N).",
            "internal_analysis": "Training labels are derived from final-answer correctness; listwise ranking loss implemented for efficiency; reward shaping combines RM score and a rule-based verifier (r = sigma(0.5 * r_m) + (r_v - 1)). The RM is central for constructing higher-quality SFT data (via rejection sampling) and for improving RL via GRPO.",
            "failure_modes": "RM relies on final-answer correctness for positives, so it can reward superficial chains that reach correct answer via spurious means; RM could underweight stepwise logical correctness when final answer matches incorrectly derived reasoning; potential bias if candidate pools are not diverse. The paper mitigates some of these by mixing responses from different intermediate models/sizes.",
            "scaling_trend": "RM-guided selection and best-of-N sampling yield larger marginal gains for smaller models (making RM an efficient lever to improve pass@1 without scaling parameters dramatically); RM quality improves iteratively as better SFT models are produced and used to generate new RM training data.",
            "uuid": "e6403.3",
            "source_info": {
                "paper_title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Tool-Integrated Reasoning (TIR)",
            "name_full": "Tool-Integrated Reasoning (TIR) with Python Interpreter",
            "brief_description": "A prompting & training paradigm where the model is trained and prompted to call an external Python interpreter during its chain-of-thought to perform precise arithmetic, symbolic manipulation, or algorithmic steps, improving numeric accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TIR (method used with Qwen2.5-Math models)",
            "model_family": "method / prompting + executor integration (model + external tool)",
            "model_size": "method not tied to a single size; evaluated with 1.5B, 7B, 72B Qwen2.5-Math-Instruct models",
            "training_data_description": "TIR training dataset: 190K annotated and 205K synthesized problems; includes annotated TIR chains created via online Rejection Fine-Tuning (RFT) and translated Chinese samples; synthesized queries generated via MuggleMath/DotaMath methods.",
            "benchmark_name": "MATH, GSM8K, AIME2024, AMC2023 and other benchmarks used for evaluating tool-assisted reasoning",
            "task_type": "precise arithmetic computation, symbolic manipulation, algorithmic subroutines embedded inside multi-step reasoning",
            "problem_format": "natural-language question plus interleaved 'call tool' code blocks (Python) and parsed interpreter output (the model consumes executor output and continues reasoning); final answers boxed as usual.",
            "difficulty_level": "especially helpful on tasks requiring exact numeric computation (from multi-digit arithmetic to eigenvalues / symbolic algebra and competition-level numeric tasks)",
            "prompting_method": "zero-shot / few-shot TIR prompts (paper provides specific TIR prompt templates in Figure 12); training includes TIR demonstrations; during RL the outputs produced by the Python executor are masked (not used in policy gradients) to stabilize training.",
            "performance_metric": "pass@1 / accuracy; improvement in solved counts on contest benchmarks; comparison CoT vs TIR",
            "performance_value": "TIR consistently improves performance over pure CoT: e.g., 72B TIR 'close to 90' on MATH; 7B TIR improves MATH score (reported 85.3 for 7B TIR vs 83.6 CoT); 1.5B TIR yields MATH ~80; on AMC/AIME contest sets TIR increases solved counts substantially (Table 5 reports many TIR improvements, including near-complete solving for 72B on AMC2023).",
            "internal_analysis": "Authors report that TIR addresses the dominant failure mode of CoT — arithmetic/computation errors — by delegating numerically exact steps to a Python executor; they train the model to format calls and parse outputs; during RL they mask executor tokens to avoid attributing reward to the tool's tokens.",
            "failure_modes": "Remaining failure types include incorrect code generation or parsing mistakes, incorrect decomposition of a problem into tool calls (i.e., wrong algorithmic approach), and occasional miscoordination between natural-language reasoning and tool outputs; in Chinese the TIR advantage is reported to be smaller, suggesting language-specific gaps.",
            "scaling_trend": "TIR provides strong returns across scales: small models gain large absolute improvements, enabling 7B and even 1.5B models to approach large-model performance on computationally demanding benchmarks; authors emphasize tool-use as a scaling-efficient substitute for parameter increases.",
            "uuid": "e6403.4",
            "source_info": {
                "paper_title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Mugglemath: Assessing the impact of query and response augmentation on math reasoning",
            "rating": 2,
            "sanitized_title": "mugglemath_assessing_the_impact_of_query_and_response_augmentation_on_math_reasoning"
        },
        {
            "paper_title": "Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning",
            "rating": 2,
            "sanitized_title": "dotamath_decomposition_of_thought_with_code_assistance_and_selfcorrection_for_mathematical_reasoning"
        },
        {
            "paper_title": "Beyond human data: Scaling self-training for problem-solving with language models",
            "rating": 1,
            "sanitized_title": "beyond_human_data_scaling_selftraining_for_problemsolving_with_language_models"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models (Minerva/Minerva Math references)",
            "rating": 1,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models_minervaminerva_math_references"
        }
    ],
    "cost": 0.01917975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>QWEN2.5-MATH TECHNICAL REPORT: TOWARD MATHEMATICAL EXPERT MODEL VIA SELF-IMPROVEMENT
18 Sep 2024</p>
<p>An Yang 
Beichen Zhang 
Binyuan Hui 
Bofei Gao 
Bowen Yu 
Chengpeng Li 
Dayiheng Liu 
Jianhong Tu 
Jingren Zhou 
Junyang Lin 
Keming Lu 
Mingfeng Xue 
Runji Lin 
Tianyu Liu 
Xingzhang Ren 
Zhenru Zhang 
QWEN2.5-MATH TECHNICAL REPORT: TOWARD MATHEMATICAL EXPERT MODEL VIA SELF-IMPROVEMENT
18 Sep 2024DA0F8A612AC246759CD65F944BB8089EarXiv:2409.12122v1[cs.CL]
In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B.The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data.(2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct.This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT).With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration.On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.(3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance.Qwen2.5-Math-Instructsupports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR).We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems.The flagship model, Qwen2.5-Math-72B-Instruct,significantly outperforms both open-source models and leading closed-source models (e.g., GPT-4o, Gemini Math-Specialized 1.5 Pro).Particularly in the challenging AMC 2023, with the assistance of RM, Qwen2.5-Math-72B-Instructsuccessfully solves almost all the problems.Qwen2.5-Math-7B-Instructsurpasses Qwen2-Math-Instruct 72B in performance.Under CoT and TIR settings, it achieves MATH scores of 83.6 and 85.3, respectively.Even our smallest 1.5B model, achieving a MATH score of around 80 when utilizing the Python Interpreter, outperforms the majority of current models in this domain.We hope that Qwen2.5-Math can contribute to the community for solving complex mathematical problems.The base models, instruct models, and reward model of the Qwen2.5-Mathseries are available on Hugging Face 1 and ModelScope 2 , and the evaluation scripts on GitHub 3 .We have also developed a demo that supports the TIR mode in Qwen-Agent 4 , which allows running code locally to experience Tool-Integrated Reasoning capabilities of Qwen2.5-Math.</p>
<p>INTRODUCTION</p>
<p>Figure 1: The pass@1 performance of Qwen2.5-Math-72B-Instruct on MATH by the Chain-of-Thought reasoning.</p>
<p>Over the past year, we have devoted considerable effort to researching and enhancing the reasoning capabilities of large language models, with a particular emphasis on their ability to solve arithmetic and mathematical problems.In this report, we introduce a series of math-specific large language models, Qwen2.5-Math,Qwen2.5-Math-RM, and Qwen2.5-Math-Instruct-1.5B/7B/72B.To provide a comprehensive understanding of the technical developments behind Qwen2.5-Math,we also offer a detailed overview of its predecessor, Qwen2-Math (Qwen, 2024).</p>
<p>We introduce a series of self-improvement techniques to develop Qwen2.5-Mathmodels on top of the Qwen2-Math.Self-improvement techniques take advantage of supervision from large language models themselves (Cao et al., 2024).Specifically, we apply self-improvement from three aspects during the training of Qwen2.5-Math.In pre-training, we employ Qwen2-Math-Instruct to synthesize math queries and corresponding responses on a large scale to enrich the pre-training corpus of Qwen2.5-Math.In post-training, we train a reward model on massive sampling from previous models and apply it to the iterative evolution of data in supervised fine-tuning.The better mathematical models trained from this enhancement lead to a more robust reward model, Qwen2.5-Math-RM.Then, we use this reward model in reinforcement learning and best-of-N sampling during inference.Synthetic data and judgment play a significant role in the enhancement of Qwen2.5-Mathcompared with its predecessor.Specifically, the overall pipelines for developing Qwen2-Math and Qwen2.5-Math are illustrated in Figure 2. First, the Qwen2-Math base models are trained on a high-quality mathematical pre-training dataset called the Qwen Math Corpus v1, which contains approximately 700 billion tokens.Second, we train a math-specific reward model Qwen2-Math-RM, derived from Qwen2-Math-72B, to create the Qwen2-Math-Instruct models.This reward model is used to construct Supervised Fine-Tuning (SFT) data through Rejection Sampling (Yuan et al., 2023).Moreover, the reward model plays a key role in the reinforcement learning stage, where we employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) following SFT.Third, leveraging the Qwen2-Math-72B-Instruct model, we synthesize additional high-quality mathematical pre-training data, which serves as the foundation for Qwen Math Corpus v2.This updated corpus contains over 1 trillion tokens and is used to pre-train the Qwen2.5-Mathmodels.Lastly, similar to the process used for the Qwen2-Math-Instruct models, we construct the Qwen2.5-Math-RMand Qwen2.5-Math-Instructmodels.An important distinction in this stage is the inclusion of both English and Chinese Chain-of-Thought (CoT) reasoning data, as well as Tool-Integrated Reasoning (TIR) data, for training the Qwen2.5-Math-Instructmodels, as opposed to using only English CoT data as was done for Qwen2-Math-Instruct.</p>
<p>We evaluate our math-specific models on eight English and Chinese math benchmarks.Notably, the Qwen2.5-Math-7Bbase model achieves scores of 91.6, 55.4, and 57.6 on GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), and GaoKao Math Cloze (Zhang et al., 2023), respectively, outperforming the Qwen2-72B (Yang et al., 2024) general model, which achieves scores of 89.5, 51.1, and 55.9 on the same datasets.Additionally, the Qwen2.5-Math-72Bbase model sets a new state-of-the-art on the MATH benchmark, achieving a score of 66.8-an improvement of 5.3 points over Qwen2-Math-72B and 15.7 points over Qwen2-72B.</p>
<p>For the Instruct models, in CoT mode, the Qwen2.5-Math-1.5B-Instructmodel surpasses the performance of all currently available open-source models on most metrics, including models as large as 70B parameters.Furthermore, the Qwen2.5-Math-7B-Instructmodel nearly matches the performance of the Qwen2-Math-72B-Instruct model, indicating that improvements to the training data and strategy can, to a certain extent, compensate for the scaling up of parameters.The Qwen2.5-Math-72B-Instruct model outperforms the Qwen2-Math-72B-Instruct model by an average margin of 4.4 and 6.1 points in English and Chinese, respectively, establishing itself as the best open-source mathematical model currently available.Moreover, all model sizes demonstrate significant improvements in their Chinese math problem-solving capabilities.In our newly introduced TIR mode, performance sees further enhancement compared to CoT.For instance, the 72B model achieves close to 90 points on the MATH benchmark, and even the 1.5B model scores around 80, demonstrating that Qwen2.5 is now highly proficient at leveraging the Python Interpreter for accurate mathematical computation.</p>
<p>QWEN2.5-MATH PRE-TRAINING</p>
<p>In mathematical pre-training, our primary focus is on constructing a high-quality dataset rich in mathematical content.This dataset encompasses a wide variety of sources, including math-related web texts, code snippets, encyclopedias, exam questions, and synthetic mathematical data generated by Qwen2 (Yang et al., 2024).The process of assembling this pre-training dataset involves several key steps: data recall, deduplication, filtering, data synthesis, and optimization of the data mixture.The final curated dataset, which forms the foundation of our pre-training, is termed the Qwen Math Corpus v1.The Qwen2-Math base models, initialized with Qwen2-1.5B/7B/72B,undergo continuous pre-training using the Qwen Math Corpus v1.</p>
<p>Prior to the construction of Qwen Math Corpus v1, we observe that the suboptimal performance of general language models in mathematical reasoning stems from an insufficiency of mathematical data during pre-training.The existing endeavors pre-training to large-scale, specialized LLMs focused on mathematics (Shao et al., 2024;Ying et al., 2024;Lewkowycz et al., 2022a;Azerbayev et al., 2024) have unequivocally demonstrated the value of extracting a considerable corpus of mathematical texts from digital databases.Our initial strategy involves the recall of mathematical data from web sources, such as Common Crawl, to escalate the quantity of data.Concretely, we train a FastText (Joulin et al., 2016) classifier utilizing high-quality mathematical seed data and general text data.We leverage iterative training with more math data each epoch to continuously enhance the performance of the classifier.To recognize the missing mathematical-related data in the corpus pool, we leverage meta-information, such as URLs, from the recalled data to expand the data pool for mathematical data retrieval.Subsequently, deduplication techniques, including MinHash (Broder, 2000), are employed to filter out similar mathematical documents.</p>
<p>Upon collecting a substantial volume of mathematical data, our focus shifts toward enhancing its quality.For this, we implement a language-model-based filtering technique to further curate the dataset.Specifically, we utilize the Qwen2-0.5B-Instructmodel (Yang et al., 2024), augmented with prompt engineering, to evaluate the quality of potential data entries.Data that receive higher scores, indicating higher quality according to the language model, are prioritized for inclusion in the final dataset.Beyond recalling a diverse set of mathematical documents and filtering out low-quality data, we draw inspiration from previous efforts in generating synthetic mathematical data (Yue et al., 2024;Zhou et al., 2024).We employ the Qwen2-72B-Instruct model to synthesize a large amount of mathematical pre-training corpus.At this stage, the high-quality mathematical data already collected are used as reference materials.Using the Qwen2-72B-Instruct model, we: (1) extract and refine existing mathematical question-answer data from these references, and (2) directly generate new mathematical question-answer pairs.</p>
<p>In the final phase, we conduct ablation studies on data mixture using a small math-specific language model, Qwen2-Math-1.5B.Based on the findings, we construct the Qwen Math Corpus v1, which comprises 700 billion tokens in total.We initialize the Qwen2-Math-1.5B/7B/72Bpre-training with intermediate checkpoints from the corresponding Qwen2-1.5B/7B/72Bbase models.These models are then continuously pre-trained on Qwen Math Corpus v1 with a context length of 4K.</p>
<p>Following the training of the Qwen2-Math base models, we further upgrade them to Qwen2.5-Math models through three primary avenues: (1) We utilize the Qwen2-Math-72B-Instruct model, further post-trained with the steps described in Section 3, to synthesize additional high-quality mathematical pre-training data.2) We aggregate more high-quality mathematical data, especially in Chinese, sourced from web documents, books, and code repositories across multiple recall cycles.As a result of these efforts, we compile the Qwen Math Corpus v2 for Qwen2.5-Math-1.5B/7B/72Bpre-training, while maintaining a context length of 4K.Compared to Qwen Math Corpus v1, the total token count of Qwen Math Corpus v2 escalates from 700B to over 1T.(3) Instead of initializing from the Qwen2 series, we leverage the Qwen2.5 series base models for parameter initialization, as they exhibit enhanced capabilities in language understanding, code generation, and text reasoning.Qwen2.5-Mathmodels are continuously pre-trained on Qwen Math Corpus v2 under a math pre-training setup similar to Qwen2-Math.Benefiting from the improvements in both the dataset and the base model, Qwen2.5-Mathmodels demonstrate further advancements in mathematical reasoning abilities beyond Qwen2-Math.</p>
<p>QWEN2.5-MATH POST-TRAINING</p>
<p>After completing extensive mathematical pre-training, we proceed with post-training to further augment the mathematical logical reasoning capabilities of Qwen-Math, specifically focusing on Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR).Our investigation is particularly focused on two key challenges: (1) How to automatically generate a substantial volume of highquality and reliable CoT and TIR annotations, and (2) How to effectively leverage these annotations for both Supervised Fine-Tuning and Reinforcement Learning.</p>
<p>SUPERVISED FINE-TUNING</p>
<p>We aim for Qwen-Math to excel in two core capabilities: solving math problems through step-by-step natural language reasoning (Wei et al., 2022), and leveraging external tools (e.g., a Python interpreter) to address complex mathematical or algorithmic reasoning tasks (Yue et al., 2023).We have constructed dedicated datasets for both Chain-of-Though (CoT) and Tool-integrated Reasoning (TIR) and combined these datasets to train the model jointly.All models are trained for 3 epochs with a sequence length of 4,096 tokens.For the 72B model, we use a batch size of 256 and a learning rate of 5 × 10 −6 .For the 1.5B and 7B models, we set the batch size to 128 and the learning rate to 2 × 10 −5 .</p>
<p>During training, the learning rate gradually decays to a final value of 7 × 10 −7 .</p>
<p>CHAIN-OF-THOUGHT DATA SYNTHESIS</p>
<p>Query Construction.The chain-of-thought dataset comprises a wide-ranging collection of 580K English and 500K Chinese mathematical problems, including both annotated and synthesized items.The annotated problems are derived from well-established sources such as the training set of GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), andNuminaMath (LI et al., 2024).In an effort to bolster the Chinese reasoning capabilities of Qwen2.5-Math,we have further enriched the dataset with additional Chinese mathematical problems from exclusive K-12 problem collections.The synthesized problems are evolved from the annotated ones using the MuggleMath approach (Li et al., 2024b).To maintain a balanced distribution across varying levels of problem complexity, we utilize a difficulty-scoring model to categorize our problem set effectively.</p>
<p>Response Construction.We adopt an iterative approach that leverages rejection sampling, guided by reward modeling and annotated answers, to incrementally enhance the quality of responses (Yuan et al., 2023).At each iteration, the current best model is deployed to generate multiple reasoning pathways for the given problems, expanding the pool of candidate solutions.For problems with annotated answers, we select the top-k reasoning paths with correct final answers from the pool.For synthesized problems lacking definitive answers, we implement a weighted majority voting mechanism to deduce the most plausible correct reasoning paths.From these, we choose the top-k pathways that receive the highest reward scores.In the development of Qwen2.5-Math, an additional iteration is conducted using the Qwen2-Math-Instruct models to polish the quality of responses further.The final CoT training set encompasses 2000K English samples and 500K Chinese samples.</p>
<p>TOOL-INTEGRATED REASONING DATA SYNTHESIS</p>
<p>It is important to recognize that while CoT prompting plays a crucial role in enhancing the reasoning skills of large language models, it faces challenges in achieving computational accuracy and in handling complex mathematical or algorithmic problems, such as finding the roots of quadratic equations or computing the eigenvalues of matrices (Yue et al., 2023).To overcome these limitations and improve the model's proficiency in precise calculations, symbolic manipulation, and algorithmic reasoning, we have developed a dataset that incorporates a tool-integrated reasoning format.This innovative format enables the model to leverage a Python interpreter as an auxiliary resource in reasoning tasks.</p>
<p>Query Construction.The tool-integrated reasoning dataset consists of 190K annotated problems and 205K synthesized problems.The annotated problems are sourced from the training sets of established benchmarks, including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), CollegeMath (Tang et al., 2024a), andNuminaMath (LI et al., 2024).The synthesized problems are generated by employing techniques from MuggleMath (Li et al., 2024b) and DotaMath (Li et al., 2024a) designed to facilitate query evolution within the GSM8K and MATH training sets.Additionally, we have selected 75K annotated problems for translation into Chinese using the Qwen2-72B model (Yang et al., 2024), aimed at enhancing the model's reasoning capabilities in Chinese.</p>
<p>Response Construction.For the annotated problems, we utilize an online Rejection Fine-Tuning (RFT) (Yuan et al., 2023;Singh et al., 2024) approach to iteratively generate tool-integrated reasoning paths whose final answers align with the reference answers.In each RFT iteration, we carry out multiple nucleus samplings with the currently best model at various temperatures, increasing the sample size for particularly challenging problems.After each iteration, to enhance data diversity, we apply a deduplication process to the responses, and the resulting cleaned dataset is then used to fine-tune the model for the next iteration.For the synthesized problems, we employ the optimal model derived from the online RFT process to generate reasoning samples.Majority voting is employed to select the most probable correct reasoning paths, which are subsequently incorporated into the overall dataset.</p>
<p>REWARD MODEL TRAINING</p>
<p>To provide supervisory signals beyond merely the final answer during both the selection of supervised fine-tuning data and the subsequent stages of reinforcement learning training, we have developed a mathematical reward model for Qwen2-Math and Qwen2.5-Math,referred to as Qwen2-Math-RM and Qwen2.5-Math-RM,respectively.These reward models are specifically designed to guide the model throughout the training process by offering more granular feedback on the quality of reasoning and intermediate steps, ultimately facilitating more robust model improvements.</p>
<p>DATA SYNTHESIS</p>
<p>In the development of Qwen2-Math-RM, we utilize 206K English mathematical problems, each paired with 6 candidate responses sampled from an intermediate version of Qwen2-Math.For Qwen2.5-Math-RM, we further enhance its support for both the Chinese language and TIR mode, training it with a more diverse set of 361K English and 257K Chinese mathematical problems, with each problem accompanied by 6 responses sampled from Qwen2.5-Math.This expansion ensures that Qwen2.5-Math-RM is well-equipped to provide supervisory feedback across a broader range of problem types and languages.</p>
<p>To establish the preference signals among the responses, we check the final answers of the responses to determine their correctness.Responses with the correct answers are labeled as positive, while those with incorrect answers are labeled as negative, thereby naturally creating a ranking relationship among the responses.We then filter out any cases where all responses are either entirely correct or entirely incorrect.However, to avoid the potential drawback of retaining only overly simplistic data, we enrich the dataset with responses from various intermediate versions and models of different sizes.This strategy ensures a more balanced distribution of query difficulty and maintains an even ratio of positive to negative responses.</p>
<p>TRAINING STRATEGY</p>
<p>We initialize the reward model from the supervised fine-tuning model.In terms of architecture, we replace the language modeling head originally used for next-token prediction with a scalar-value head, consisting of two linear layers.As previously mentioned, each query in the reward model's training dataset is paired with 6 responses, comprising both positive and negative candidates.If there are k positive responses, then the remaining 6 − k are negative.Following Ouyang et al. (2022), the loss function for the reward model can therefore be formulated as follows:
L rm (θ) = − 1 k × (6 − k) E (x,ypos,yneg)∼D [log (σ (r θ (x, y pos ) − r θ (x, y neg )))] .(1)
Here, r θ (x, y) denotes the output of the reward model, where x represents the problem and y is the corresponding response.Rather than breaking these into multiple individual pairs and computing the loss in a pairwise fashion, we adopt a listwise approach to compute the ranking loss directly over valid pairs.This method enhances both training efficiency and effectiveness.</p>
<p>REINFORCEMENT LEARNING</p>
<p>Query Selection.The queries for reinforcement learning training are selected from the reward model's training set.We leverage supervised fine-tuning models with varying sizes to resample 8 responses for each query, with each response classified as either correct or incorrect by comparing it to the gold-standard answer.In the reinforcement learning stage, our primary goal is to ensure that the model consistently produces correct answers for queries where a correct response is possible.Therefore, we only retain queries for which 2 to 5 out of the 8 responses are correct.Queries with fewer than 2 correct answers are excluded as they indicate that the current Math model lacks the fundamental capability to learn from them.Likewise, queries with more than 5 correct responses are omitted since the model already demonstrates competence in these cases and no further training is necessary.In the end, we retain 66K queries for training.</p>
<p>Group Relative Policy Optimization (GRPO).As introduced by Shao et al. (2024), GRPO is a reinforcement learning method specifically designed for large language models, obviating the need for additional value function approximation as in PPO.GRPO uses the average rewards of a group of sampled outputs as a baseline to calculate the advantages of each output.The objective of GRPO is defined as Eq.2:
JGRP O (θ) = E [q∼P (Q),{o i } G i=1 ∼π θ old (O|q)] 1 G G i=1 1 |oi| |o i | t=1 {min[ π i,t θ π i,t θ old Âi,t, clip( π i,t θ π i,t θ old , 1 − ϵ, 1 + ϵ) Âi,t] − βDKL[π θ ||πref]},(2)
where .
π i,t = π(o i,t |q, o i,&lt;t ), G
Then this sequence-level advantage is applied to each token in the response as Âi,t .</p>
<p>Reward Shaping.We combine the rewards from both a rule-based verifier and the reward model to shape the overall reward signal.The rule-based verifier extracts potential answers from each response and compares them against the gold-standard answer.</p>
<p>Given that the output of the reward model is denoted as r m ∈ R, and the sparse reward from the rule-based verifier as r v ∈ {0, 1}, the overall reward is calculated as follows:
r = σ(α • r m ) + (r v − 1),(3)
where α is set as 0.5 in all of our experiments.</p>
<p>This shaping mechanism ensures that correct responses consistently receive higher overall rewards compared to incorrect ones.Within each of the correct and incorrect groups, the responses are ranked based on the scores from the reward models.ecially in hard samples.</p>
<p>Implementations.Our experiments are implemented based on the open-source RLHF framework ChatLearn5 .The core implementation of our rule-based verifier is similar to the one used in our evaluation6 .All policy models in different parameter sizes are trained with the same reward model.We sample 32 responses for each query.Considering a pair of queries and responses as a sample, the number of samples in one episode is 4,096 and 2,048 for training 7B and 72B, respectively.All models are trained with a 512 global batch size.The learning rates are 1 × 10 −5 and 5 × 10 −6 for 7B and 72B, respectively.And the KL coefficient for all training is 1 × 10 −3 .We mask all output tokens the Python executor provides in reinforcement learning of tool-integrated reasoning.</p>
<p>DECONTAMINATION</p>
<p>Decontamination is critical to ensuring unbiased model performance evaluation.Following prior work (Yang et al., 2024), we exclude potentially contaminated training samples using 13-gram matching.To improve the accuracy of this matching process, we perform text normalization, removing irrelevant punctuation and symbols.To further reduce false negatives, particularly for common mathematical expressions, we introduce an additional criterion: the ratio of the longest common subsequence must exceed 0.6 for a sample to be considered contaminated.For pre-training data, we filter potentially contaminated samples against datasets such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b).When dealing with post-training data, including SFT data, RM training data, and the RL query set, we exclude any potentially contaminated problems or solutions across all reported evaluation datasets.These evaluation datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022b), Gaokao 2023 En (Liao et al., 2024), Olympiad Bench (He et al., 2024), College Math (Tang et al., 2024b), MMLU STEM (Hendrycks et al., 2021a), GaoKao (Zhong et al., 2024), CMATH (Wei et al., 2023), CN Middle School 24, AIME 24, and AMC 23.During the analysis of contaminated samples, we identify that some existing training datasets (e.g., the MATH training dataset) contain a significant proportion of problems that share highly similar concepts or structures with those found in test datasets.Although these variations are not exact duplicates, they could potentially compromise the integrity of our evaluation.Therefore, we continue to exclude such samples from the training corpora.</p>
<p>EVALUATION</p>
<p>BASE MODELS</p>
<p>We evaluate our Qwen2-Math and Qwen2.5-Mathbase models on three widely used English math benchmarks GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), and MMLU-STEM (Hendrycks et al., 2021a).In addition, we also evaluate three Chinese math benchmarks CMATH (Wei et al., 2023), GaoKao Math Cloze (Zhong et al., 2024), and GaoKao Math QA (Zhong et al., 2024).All evaluations are tested with few-shot chain-of-thought prompting.The prompts of these benchmarks are shown in Appendix B. For general models, we report the results on LLama-3.1-8B/70B/405B(AI@Meta, 2024) and Qwen2-1.5B/7B/72B(Yang et al., 2024).For specific models, DeepSeekMath-Base-7B (Shao et al., 2024), DeepSeek-Coder-V2-Lite-Base (Zhu et al., 2024), and Intermln2-Math-Base-20B (Ying et al., 2024) are used as baselines.</p>
<p>The results are shown in Table 2.We can see that the smallest model of the Qwen2.5-Mathseries, Qwen2.5-Math-1.5B,outperforms all specific baselines on GSM8K, MATH, CMATH, GaoKao Math Cloze, and Gaokao Math QA.Furthermore, the medium-size model, Qwen2.5-Math-7B,obtains 91.6 and 55.4 scores on GSM8K and MATH, which outperforms Qwen2-72B with 89.5 and 51.1, and Llama-3.1-405B with 89.0 and 53.8.Our flagship Qwen2.5-Math-72Bachieves new SOTA on MATH, CMATH, Gaokao Math Cloze, and Gaokao Math QA, which obtains 66.8 on MATH.</p>
<p>Compared to Qwen2-Math-1.5B/7B/72B,Qwen2.5-Math-1.5B/7B/72Bhave achieved significant improvements on all benchmarks.For example, Qwen2.5-Math-1.5B/7B/72Bobtains 5.4, 5.0, 6.3 scores improvement on MATH, and 3.4, 12.2, 19.8 scores improvement on Gaokao Math QA, which demonstrates the effectiveness of our Qwen Math corpus v2.</p>
<p>INSTRUCTION MODELS</p>
<p>We evaluate Qwen2-Math-Instruct on mathematical benchmarks in both English and Chinese.In addition to the widely-used benchmarks, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), we also involve more exams that are more challenging to fully inspect the capabilities of Qwen2-Math-Instruct and Qwen2.5-Math-Instruct,such as OlympiadBench (He et al., 2024) Table 2: The results of Qwen2.5-Math and other base models on English and Chinese mathematical benchmarks.Models are evaluated with few-shot chain-of-thought prompting.CollegeMath (Tang et al., 2024a), GaoKao 2023 En (Liao et al., 2024), AIME20247 , and AMC20238 .For Chinese mathematical benchmarks, we use CMATH (Wei et al., 2023), GaoKao (including GaoKao I/II 20249 , GaoKao-Math-QA (Zhong et al., 2024), GaoKao-Math-Cloze (Zhong et al., 2024) and 91 collected GaoKao problems in 2024), and CN Middle School 24 (101 collected problems from China High School Entrance Examination in 2024).We report greedy, Maj@8, and RM@8 performance on all benchmarks in the zero-shot setting, except for the multi-choice benchmarks (including MMLU STEM and multiple-choice problems in GaoKao and CN Middle School 24) with a 5-shot setting.</p>
<p>We take Qwen2-1.5/7/72B-Instruct(Yang et al., 2024), Llama-3.91.8maj@8 48.2maj@8 83.1maj@8 64.5maj@8 58.3maj@8 85.0maj@8 75.4maj@8 96.4rm@8 92.9rm@8 49.3rm@8 83.4rm@8 65.9rm@8 59.7rm@8 90.0rm@8 76.8rm@8</p>
<p>Table 3: The results of Qwen2.5-Math-Instruct and other instruct models on English benchmarks.For CoT, we report few-shot pass@1 performance on MMLU(STEM) and zero-shot pass@1 performance on other benchmarks.For PoT, all benchmarks are evaluated in the zero-shot setting.Except for the pass@1 scores, we also provide the Qwen2-Math and Qwen2.5-Mathperformance with majority voting and reward model best-of-N among 8 sampled responses.Best pass@1 performance in CoT and TIR are marked in bold.</p>
<p>Let us first analyze the performance on English benchmarks.As shown in Qwen2.5-Math-72B-Instruct68.5 93.0 78.2 79.9 72.0maj@8 93.5maj@8 78.2maj@8 81.2 maj@8 75.4rm@8 93.8rm@8 80.2rm@8 83.1rm@8Table 4: The results of Qwen2.5-Math-Instruct and other instruct models on Chinese benchmarks.</p>
<p>model achieves an average score higher than any sub-70B model currently available.The 7B model performs on par with Qwen2-72B-Instruct, and Qwen2-Math-72B-Instruct surpasses the latest version of GPT-4o by 3.7 points.</p>
<p>(2) The performance of Qwen2.5-Math-Instructrepresents a further upgrade over Qwen2-Math-Instruct.In the traditional CoT mode, the 1.5B and 7B Qwen2.5-Math-Instructmodels achieve results comparable to the 7B and 72B Qwen2-Math-Instruct models, respectively, demonstrating a cross-scale improvement.Qwen2.5-Math-72B-Instructachieves an average score of 2.5 points ahead of the current best model and is 6.2 points higher than GPT-4o.This shows that our improvements in training data and strategy can provide an alternative pathway for performance enhancements beyond simply increasing model size.</p>
<p>(3) The TIR mode introduced in Qwen2.5-Math-Instruct is highly effective.With the assistance of a Python Interpreter, the 7B model already matches the performance of Qwen2.5-Math-72B-Instruct.This indicates that precise mathematical calculations via external tools can significantly aid LLM reasoning.In many cases, the reasoning process of LLMs is sound, but computational errors can arise.( 4) Our RM performs exceptionally well.Across almost all benchmarks and models, RM@N scores are substantially better than Maj@N scores.This provides a reliable performance oracle for improving reinforcement learning strategies in the future.It is likely that we may soon see models with greedy decoding exceeding 90 points on MATH, even for the 7B scale.The Performance of Qwen2.5-Math-1.5/7/72B-Instructby using TIR compared to using CoT.We use blue color to represent the performance of TIR, and orange to represent the performance of CoT.It can be seen that TIR can achieve further performance improvement compared to CoT.</p>
<p>Let's now shift our attention to Table 4 to analyze the performance on the Chinese benchmarks.For Qwen2-Math-Instruct, no specifically Chinese mathematics-related training data was incorporated.However, thanks to Qwen2's strong language transfer capabilities, the Qwen2-Math-1.5B-Instructmodel has already surpassed GPT-4o in terms of the average Chinese score.During the development of Qwen2.5-Math-Instruct,we intentionally integrated Chinese-specific math post-training data, resulting in substantial improvements in Chinese performance.The Qwen2.5-Math-1.5B-Instructmodel achieves results similar to Qwen2-Math-72B-Instruct, while Qwen2.5-Math-72B-Instructoutperforms GPT-4o by an impressive 17.5 points.Our RM also exhibits strong performance in Chinese benchmarks.Similar to our results in English, RM@N scores consistently surpass Maj@N scores, highlighting its effectiveness.However, one key difference from the English results is that the TIR mode in Chinese does not show a significant performance advantage over the CoT mode.We will continue to investigate this aspect in future research.</p>
<p>Lastly, we intend to evaluate the model's ability to solve complex mathematical problems on highly challenging competition benchmarks such as AIME 2024 and AMC 2023.As shown in Table 5, we observe a significant improvement in performance on difficult problems with Qwen2.5-Math-Instructcompared to Qwen2-Math-Instruct.With the support of the RM, Qwen2.5-Math-1.5B-Instruct,using the RM@256 in CoT mode, successfully solves 29 out of 40 problems on AMC 2023, significantly outperforming NuminaMath-72B CoT.Moreover, Qwen2.5-Math-72B-Instructnearly achieves a perfect score in TIR mode, solving almost all the problems.We attribute this impressive performance to the extensive amounts of challenging mathematical data collected and synthesized during pretraining.On the extremely difficult AIME 2024 benchmark, Claude3 Opus, GPT-4 Turbo, and Gemini 1.5 Pro manage to solve only 1 or 2 questions out of 30.In contrast, Qwen2.5-Math-72B-Instructsolves 9 problems in Greedy decoding CoT mode and 12 problems in TIR mode.With the help of the RM, Qwen2.5-Math-7B-Instruct could even solve up to 21 problems, further demonstrating the outstanding mathematical problem-solving ability of Qwen2.5-Math-Instruct.</p>
<p>MODEL AIME24 AMC23</p>
<p>CHAIN-OF-THOUGHT 29/40maj@64 9/30rm@64 29/40rm@64 11/30rm@256 28/40rm@256 Qwen2.5-Math-1.5B-Instruct3/30 24/40 10/30rm@256 29/40rm@256 Qwen2.5-Math-7B-Instruct5/30 25/40 10/30rm@256 30/40rm@256 Qwen2.5-Math-72B-Instruct9/30 28/40 9/30maj@64 30/40maj@64 13/30rm@64 29/40rm@64 13/30rm@256 30/40rm@256</p>
<p>TOOL-INTEGRATED REASONING</p>
<p>Qwen2.5-Math-1.5B-Instruct7/30 20/40 9/30maj@64 31/40maj@64 18/30rm@64 36/40rm@64 9/30maj@256 32/40maj@256 19/30rm@256 36/40rm@256 Qwen2.5-Math-7B-Instruct6/30 27/40 13/30maj@64 31/40maj@64 21/30rm@64 33/40rm@64 14/30maj@256 31/40maj@256 21/30rm@256 35/40rm@256 Qwen2.5-Math-72B-Instruct12/30 28/40 14/30maj@64 36/40maj@64 18/30rm@64 37/40rm@64 16/30maj@256 36/40maj@256 19/30rm@256 39/40rm@256</p>
<p>Table 5: The results on the mathematics competition problems.</p>
<p>CONCLUSION</p>
<p>In this report, we introduce Qwen2.5-Math, which features several key technical highlights: ( 1 What is the smallest positive integer t such that there exist integers x 1 , x 2 , . . ., x t with
x 3 1 + x 3 2 + . . . + x 3 t = 2002 2002 ? Qwen2-Math-72B-Instruct Solution
To determine the smallest positive integer t such that there exist integers x 1 , x 2 , . . ., x t satisfying the equation
x 3 1 + x 3 2 + • • • + x 3 t = 2002 2002 ,
we start by analyzing the properties of cubes and their sums.We need to find the smallest t for which the sum of t cubes equals 2002 2002 .</p>
<p>First, we note that 2002 can be factored into prime factors:
2002 = 2 × 7 × 11 × 13.
Thus,
2002 2002 = (2 × 7 × 11 × 13) 2002 = 2 2002 × 7 2002 × 11 2002 × 13 2002 .
Next, we consider the properties of cubes modulo 9. Cubes modulo 9 can only be congruent to 0, 1, or −1.This is because: Next, we use Euler's theorem, which states that a ϕ(n) ≡ 1 (mod n) for gcd(a, n) = 1.Here, ϕ(9) = 6, so:
0 3 ≡ 0 (mod 9),4 6 ≡ 1 (mod 9).
First, recall that the sum of the digits of a number k modulo 2 is equivalent to k mod 2. This is because the sum of the digits of k in base 10 is congruent to k mod 9, and since 9 ≡ 1 mod 2, the sum of the digits modulo 2 is the same as k mod 2.</p>
<p>Given the polynomial P (x) = x n + a n−1 x n−1 + • • • + a 1 x + a 0 , we need to determine the parity of P (k) for any positive integer k.Specifically, we need to check if P (k) mod 2 is the same as k mod 2.</p>
<p>Let's consider the polynomial modulo 2:
P (x) ≡ x n + a n−1 x n−1 + • • • + a 1 x + a 0 mod 2.
Since a i are positive integers, a i mod 2 is either 0 or 1.Let b i = a i mod 2. Then we can rewrite the polynomial modulo 2 as:
P (x) ≡ x n + b n−1 x n−1 + • • • + b 1 x + b 0 mod 2.
Now, we need to check the parity of P (k) for k ≡ 0 mod 2 and k ≡ 1 mod 2.</p>
<ol>
<li><strong>Case 1: k ≡ 0 mod 2</strong> P (0) ≡ b 0 mod 2.</li>
</ol>
<p>For s(P (0)) to have the same parity as s( 0), b 0 must be 0. Therefore, a 0 must be even.</p>
<ol>
<li><strong>Case 2: k ≡ 1 mod 2</strong>
P (1) ≡ 1 n + b n−1 • 1 n−1 + • • • + b 1 • 1 + b 0 ≡ 1 + b n−1 + • • • + b 1 + b 0 mod 2.
For s(P (1)) to have the same parity as s( 1),
1 + b n−1 + • • • + b 1 + b 0 must be odd. Since 1 is odd, the sum b n−1 + • • • + b 1 + b 0 must be even.
From the above cases, we have: -a 0 must be even.Let R + denote the set of positive real numbers.Find all functions f : R + → R + such that for each x ∈ R + , there is exactly one y ∈ R + satisfying
xf (y) + yf (x) ≤ 2 Qwen2-Math-72B-Instruct Solution
To find all functions f : R + → R + such that for each x ∈ R + , there is exactly one y ∈ R + satisfying
xf (y) + yf (x) ≤ 2,
we will proceed step by step.</li>
</ol>
<p>Step 1: Analyze the given inequality
Given x ∈ R + , there exists a unique y ∈ R + such that xf (y) + yf (x) ≤ 2.
We need to explore the implications of this condition.</p>
<p>Step 2: Consider the case x = y Substitute y = x into the inequality:
xf (x) + xf (x) = 2xf (x) ≤ 2.
This simplifies to:
xf (x) ≤ 1.
Thus, for all x ∈ R + ,
f (x) ≤ 1 x .
Step 3: Consider the uniqueness condition</p>
<p>For each x ∈ R + , there is exactly one y ∈ R + such that xf (y) + yf (x) ≤ 2. We need to ensure that this y is unique.</p>
<p>Step 4: Test the function f
(x) = 1 x Consider the function f (x) = 1 x . Substitute f (x) = 1
x and f (y) = 1 y into the inequality:
x • 1 y + y • 1 x = x y + y x .
We need:
x y + y x ≤ 2.
Using the AM-GM inequality, we know:
x y + y x ≥ 2.
Equality holds if and only if x y = 1, which implies x = y.</p>
<p>Thus, for f (x) = 1</p>
<p>x , the inequality xf (y) + yf (x) ≤ 2 holds if and only if x = y.This satisfies the condition that there is exactly one y for each x.</p>
<p>Step 5: Verify if other functions can satisfy the condition Suppose there exists another function g : R + → R + such that g(x) ̸ = 1</p>
<p>x for some x.Then, for some x, g(x) &lt; 1</p>
<p>x or g(x) &gt; 1</p>
<p>x .-If g(x) &lt; 1</p>
<p>x , then xg(y) + yg(x) might not satisfy the inequality for all y.-If g(x) &gt; 1 x , then xg(y) + yg(x) might exceed 2 for some y.Thus, any function other than f (x) = 1</p>
<p>x would not satisfy the uniqueness condition.</p>
<p>Conclusion</p>
<p>The only function f : R + → R + that satisfies the given condition is:
f (x) = 1 x .</p>
<p>Problem from International Zhautykov Olympiad 2020</p>
<p>Let Z be the set of all integers.Find all the function f : Z → Z such that f (4x + 3y) = f (3x + y) + f (x + 2y) For all integers x, y</p>
<p>Qwen2-Math-72B-Instruct Solution</p>
<p>To find all functions f : Z → Z that satisfy the functional equation f (4x + 3y) = f (3x + y) + f (x + 2y) for all integers x and y, we will proceed step by step.</p>
<p>Step 1: Initial Substitution First, let's substitute specific values for x and y to gain insight into the function.
SUBSTITUTION 1: x = 0 f (4 • 0 + 3y) = f (3 • 0 + y) + f (0 + 2y) f (3y) = f (y) + f (2y) SUBSTITUTION 2: y = 0 f (4x + 3 • 0) = f (3x + 0) + f (x + 2 • 0) f (4x) = f (3x) + f (x)
Step 2: Exploring Further Substitutions We now have two equations:
1. f (3y) = f (y) + f (2y) 2. f (4x) = f (3x) + f (x) SUBSTITUTION 3: y = 1 IN THE FIRST EQUATION f (3 • 1) = f (1) + f (2 • 1) f (3) = f (1) + f (2) SUBSTITUTION 4: x = 1 IN THE SECOND EQUATION f (4 • 1) = f (3 • 1) + f (1) f (4) = f (3) + f (1)
Step 3: Using the Equations to Find a Pattern We have:
f (3) = f (1) + f (2) f (4) = f (3) + f (1)
Substitute f (3) from the first equation into the second equation:
f (4) = (f (1) + f (2)) + f (1) f (4) = 2f (1) + f (2)
Step 4: Generalizing the Function We hypothesize that f might be a linear function.Let's assume f (x) = kx for some integer k.</p>
<p>VERIFICATION Substitute f (x) = kx into the original functional equation:
f (4x + 3y) = k(4x + 3y) f (3x + y) = k(3x + y) f (x + 2y) = k(x + 2y)
The original equation becomes:
k(4x + 3y) = k(3x + y) + k(x + 2y) 4kx + 3ky = 3kx + ky + kx + 2ky 4kx + 3ky = 4kx + 3ky
This equation holds true for all x and y, confirming that f (x) = kx is a solution.</p>
<p>Conclusion</p>
<p>The only functions f : Z → Z that satisfy the given functional equation are of the form f (x) = kx for some integer k.</p>
<p>Thus, the final answer is: f (x) = kx for some integer k</p>
<p>Problem From Baltic Way 2023</p>
<p>Find all functions f : R → R such that
f (f (x) + y) + xf (y) = f (xy + y) + f (x)
for reals x, y.</p>
<p>Qwen2-Math-72B-Instruct Solution:</p>
<p>To find all functions f : R → R that satisfy the functional equation
f (f (x) + y) + xf (y) = f (xy + y) + f (x),
we will proceed step by step.</p>
<p>Step 1: Initial Substitution First, let's substitute y = 0 into the equation:
f (f (x) + 0) + xf (0) = f (x • 0 + 0) + f (x).
This simplifies to:
f (f (x)) + xf (0) = f (0) + f (x).
Rearranging terms, we get:
f (f (x)) = f (x) + f (0) − xf (0).
Step 2: Analyzing the Simplified Equation Let c = f (0).The equation becomes:
f (f (x)) = f (x) + c − xc.
Step 3: Substituting x = 0 Next, substitute x = 0 into the original equation:
f (f (0) + y) + 0 • f (y) = f (0 • y + y) + f (0).
This simplifies to:
f (c + y) = f (y) + c.
Step 4: Substituting
y = −c Substitute y = −c into the equation f (c + y) = f (y) + c: f (c − c) = f (−c) + c.
This simplifies to:
f (0) = f (−c) + c. Since f (0) = c, we have: c = f (−c) + c.
Subtracting c from both sides, we get:
f (−c) = 0.
Step 5: Substituting y = −f (x) Substitute y = −f (x) into the original equation:
f (f (x) − f (x)) + xf (−f (x)) = f (x(−f (x)) − f (x)) + f (x).
This simplifies to:
f (0) + xf (−f (x)) = f (−xf (x) − f (x)) + f (x).
Since f (0) = c, we have:
c + xf (−f (x)) = f (−xf (x) − f (x)) + f (x).
Step 6: Considering f (x) = 0 Assume f (x) = 0 for all x.Substitute f (x) = 0 into the original equation:
f (0 + y) + xf (y) = f (xy + y) + f (x).
This simplifies to:
f (y) + 0 = f (y) + 0,
which is always true.</p>
<p>Step 7: Verifying f (x) = x Assume f (x) = x.Substitute f (x) = x into the original equation:
f (x + y) + xf (y) = f (xy + y) + f (x).
This simplifies to:
x + y + xy = xy + y + x,
which is always true.</p>
<p>Conclusion</p>
<p>The only functions that satisfy the given functional equation are f (x) = 0 and f (x) = x.Therefore, the solutions are:
f (x) = 0 or f (x) = x .</p>
<p>A.3 COUNTING &amp; PROBABILITY</p>
<p>Problem From Lusophon Mathematical Olympiad 2023</p>
<p>A long time ago, there existed Martians with 3 different colours: red, green and blue.As Mars was devastated by an intergalactic war, only 2 Martians of each colours survived.In order to reconstruct the Martian population, they decided to use a machine that transforms two Martians of distinct colours into four Martians of colour different to the two initial ones.For example, if a red Martian and a blue Martian use the machine, they'll be transformed into four green Martians.</p>
<p>a) Is it possible that, after using that machine finitely many times, we have 2022 red Martians, 2022 green Martians and 2022 blue Martians?b) Is it possible that, after using that machine finitely many times, we have 2021 red Martians, 2022 green Martians and 2023 blue Martians?</p>
<p>Qwen2-Math-72B-Instruct Solution</p>
<p>Let's analyze the problem step by step.</p>
<p>Part (a)</p>
<p>We start with 2 red, 2 green, and 2 blue Martians.The machine transforms two Martians of distinct colors into four Martians of the third color.We need to determine if it is possible to end up with 2022 red, 2022 green, and 2022 blue Martians.</p>
<p>First, let's consider the parity (odd or even nature) of the number of Martians of each color.Initially, we have: -2 red (even) -2 green (even) -2 blue (even)</p>
<p>Each transformation changes the parity of the number of Martians of the two initial colors and the parity of the number of Martians of the third color.Specifically: -If we transform 1 red and 1 green Martian, we get 4 blue Martians.This changes the parity of red and green Martians from even to odd, and the parity of blue Martians from even to odd.-If we transform 1 red and 1 blue Martian, we get 4 green Martians.This changes the parity of red and blue Martians from even to odd, and the parity of green Martians from even to odd.-If we transform 1 green and 1 blue Martian, we get 4 red Martians.This changes the parity of green and blue Martians from even to odd, and the parity of red Martians from even to odd.</p>
<p>In all cases, the parity of the number of Martians of each color changes from even to odd or from odd to even.Therefore, after any number of transformations, the parity of the number of Martians of each color will always be the same as it was initially.</p>
<p>Since initially all three colors have an even number of Martians, after any number of transformations, the number of Martians of each color will remain even.</p>
<p>Given that 2022 is an even number, it is possible to have 2022 red, 2022 green, and 2022 blue Martians after a finite number of transformations.</p>
<p>Thus, the answer to part (a) is:</p>
<p>Yes
Part (b)
We start again with 2 red, 2 green, and 2 blue Martians.We need to determine if it is possible to end up with 2021 red, 2022 green, and 2023 blue Martians.</p>
<p>As in part (a), we consider the parity of the number of Martians of each color.Initially, we have: -2 red (even) -2 green (even) -2 blue (even)</p>
<p>Each transformation changes the parity of the number of Martians of the two initial colors and the parity of the number of Martians of the third color.Therefore, after any number of transformations, the parity of the number of Martians of each color will always be the same as it was initially.</p>
<p>Since initially all three colors have an even number of Martians, after any number of transformations, the number of Martians of each color will remain even.</p>
<p>Given that 2021 is an odd number, it is impossible to have 2021 red Martians after any number of transformations.Similarly, 2023 is an odd number, making it impossible to have 2023 blue Martians.</p>
<p>Thus, the answer to part (b) is:</p>
<p>No Problem From Balkan MO 2023</p>
<p>Joe and Penny play a game.Initially there are 5000 stones in a pile, and the two players remove stones from the pile by making a sequence of moves.On the k-th move, any number of stones between 1 and k inclusive may be removed.Joe makes the odd-numbered moves and Penny makes the even-numbered moves.The player who removes the very last stone is the winner.Who wins if both players play perfectly?</p>
<p>Qwen2-Math-72B-Instruct Solution</p>
<p>To determine the winner of the game, we need to analyze the strategy and the sequence of moves.</p>
<p>Let's break down the problem step by step.</p>
<ol>
<li>
<p><strong>Understanding the Game Rules:</strong> -Initially, there are 5000 stones.-Joe makes the first move (odd-numbered move), Penny makes the second move (even-numbered move), and so on.-On the k-th move, a player can remove between 1 and k stones.-The player who removes the last stone wins.</p>
</li>
<li>
<p><strong>Key Insight:</strong> -The game is symmetric in the sense that the number of stones a player can remove on their turn increases linearly with the move number.-The total number of moves in the game is equal to the number of stones initially in the pile, which is 5000.</p>
</li>
<li>
<p><strong>Total Number of Moves:</strong> -Since there are 5000 stones, there will be 5000 moves in total.-Joe makes the odd-numbered moves: 1st, 3rd, 5th, ..., 4999th.-Penny makes the even-numbered moves: 2nd, 4th, 6th, ..., 5000th.</p>
</li>
<li>
<p><strong>Determining the Winner:</strong> -The last move (5000th move) is an even-numbered move.-Therefore, Penny will make the last move.</p>
</li>
<li>
<p><strong>Conclusion:</strong> -Since Penny makes the last move, she will be the one to remove the last stone.-Hence, Penny wins the game if both players play perfectly.</p>
</li>
</ol>
<p>Thus, the final answer is:</p>
<p>Penny</p>
<p>Problem From Math Odyssey</p>
<p>The school offers 10 elective courses, and each student can enroll in any number of courses.The director selects k students, where although each student's combination of courses is different, any two students have at least one course in common.At this point, it is found that any student outside these k students cannot be classmates with these k students regardless of how they enroll (having one course in common is enough to be classmates).Find the maximum number of k.</p>
<p>Qwen2-Math-72B-Instruct Solution</p>
<p>To solve this problem, we need to determine the maximum number of students k such that each student has a unique combination of elective courses, any two students share at least one course, and no student outside these k students can be classmates with all of them.This means that the set of students chosen must cover all possible courses in such a way that any new student would have to share a course with at least one of the chosen students.</p>
<p>First, let's consider the total number of possible combinations of courses.Since there are 10 elective courses and each student can either take or not take each course, there are 2 10 = 1024 possible combinations of courses.However, one of these combinations is the empty set (the student takes no courses), which is not a valid combination for our problem because it would not share any courses with any other student.Therefore, there are 1024 − 1 = 1023 non-empty combinations of courses.</p>
<p>We need to find the maximum number of students k such that any two students share at least one course, and no student outside these k students can be classmates with all of them.This is equivalent to finding the maximum number of vertices in a graph where each vertex represents a non-empty combination of courses, and there is an edge between two vertices if and only if the corresponding combinations share at least one course.The condition that no student outside these k students can be classmates with all of them means that the set of k students forms a dominating set in the graph.</p>
<p>The maximum size of such a set is the complement of the minimum size of an independent set in the graph.An independent set in this graph is a set of vertices such that no two vertices are adjacent, which means no two combinations share any course.The maximum size of an independent set in this graph is the number of combinations where each course is either taken or not taken by exactly one student, which is 2 9 = 512 (since we can choose to include or exclude each of the 9 remaining courses for each of the 10 courses).</p>
<p>Therefore, the minimum size of an independent set is 512, and the maximum size of a dominating set (which is the complement of the independent set) is 1023 − 512 + 1 = 511.However, we need to add 1 because the empty set is not included, so the correct calculation is 1023 − 511 = 512.</p>
<p>Thus, the maximum number of students k is 512 .</p>
<p>A.4 GEOMETRY</p>
<p>Problem from USAMO 2010</p>
<p>Let ABC be a triangle with ∠A = 90 • .Points D and E lie on sides AC and AB, respectively, such that ∠ABD = ∠DBC and ∠ACE = ∠ECB.Segments BD and CE meet at I. Determine whether or not it is possible for segments AB, AC, BI, ID, CI, IE to all have integer lengths.</p>
<p>Qwen2-Math-72B-Instruct Solution</p>
<p>To determine whether it is possible for segments AB, AC, BI, ID, CI, and IE to all have integer lengths in triangle ABC with ∠A = 90 • , we start by analyzing the given conditions and properties of the triangle.</p>
<p>Given: -∠A = 90 • -∠ABD = ∠DBC -∠ACE = ∠ECB -Segments BD and CE intersect at I First, note that since ∠A = 90 • , triangle ABC is a right triangle with A as the right angle.Points D and E are such that BD and CE are angle bisectors of ∠ABC and ∠ACB, respectively.Therefore, I is the incenter of triangle ABC.</p>
<p>Step 1: Coordinates and Lengths Assume A is at the origin (0, 0), B is at (a, 0), and C is at (0, b).The lengths of the sides are:
-AB = a -AC = b -BC = √ a 2 + b 2
Step 2: Incenter Coordinates The incenter I of a right triangle with legs a and b and hypotenuse √ a 2 + b 2 is located at:
I = a 2 , b2
Step 3: Lengths of Segments BI and CI Using the distance formula, we calculate BI and CI:
BI = a − a 2 2 + 0 − b 2 2 = a 2 2 + b 2 2 = √ a 2 + b 2 2 CI = 0 − a 2 2 + b − b 2 2 = a 2 2 + b 2 2 = √ a 2 + b 22
Step 4: Lengths of Segments ID and IE Since D and E are points on AC and AB respectively, and I is the incenter, ID and IE are the distances from the incenter to the sides of the triangle.For a right triangle, these distances are given by:
ID = IE = r = a + b − √ a 2 + b 2 2
Step 5: Integer Lengths For AB, AC, BI, ID, CI, and IE to all be integers, the following must hold: -a and b are integers -
√ a 2 +b 2 2 is an integer -a+b− √ a 2 +b 2 2 is an integer Step 6: Analysis For √ a 2 +b 2 2
to be an integer, √ a 2 + b 2 must be an even integer.Let √ a 2 + b 2 = 2k for some integer k.Then:
a 2 + b 2 = 4k 2 For a+b− √ a 2 +b 2 2
to be an integer, a + b − 2k must be even.Since a and b are integers, a + b must be even.</p>
<p>Conclusion</p>
<p>Given the constraints, it is possible to find integer values for a and b such that all the conditions are satisfied.For example, if a = 3 and b = 4, then: -AB = 3 -AC = 4 -BC = 5 -BI = CI = 5 2 = 2.5 (not an integer) Thus, it is not possible for all segments AB, AC, BI, ID, CI, and IE to have integer lengths simultaneously.Question: Zilla spent 7% of her monthly earnings on rent, half of it on her other monthly expenses, and put the rest in her savings.If she spent $133 on her rent, how much does she deposit into her savings account in a month?\newline Let's think step by step Since $133 is equal to 7% of her earnings, then 1% is equal to $133/7 = $19. \newline The total monthly earning of Zilla is represented by 100%, so $19 x 100 = $1900 is her monthly earnings.\newline So, $1900/2 = $950 is spent on her other monthly expenses.\newline The total amount spent on the rent and other monthly expenses is $133 + $950 = $1083.\newline Hence, she saves $1900 -$1083 = $817 per month.\newline The answer is 817 Question: If Buzz bought a pizza with 78 slices at a restaurant and then decided to share it with the waiter in the ratio of 5:8, with Buzz's ratio being 5, what's twenty less the number of slices of pizza that the waiter ate? \newline Let's think step by step The total ratio representing the slices of pizza that Buzz bought is 5+8=13 \newline If he shared the slices of pizza with the waiter, the waiter received a fraction of 8/13 of the total number of slices, which totals 8/13 * 78 = 48 slices \newline Twenty less the number of slices of pizza that the waiter ate is 48-20 = 28 \newline The answer is 28 Question: Jame gets a raise to $20 per hour and works 40 hours a week.His old job was $16 an hour for 25 hours per week.How much more money does he make per year in his new job than the old job if he works 52 weeks a year?\newline Let's think step by step He makes 20<em>40=$800 per week \newline He used to make 16</em>25=$400 per week \newline So his raise was 800-400=$400 per week \newline So he makes 400*52=$20,800 per year more \newline The answer is 20800 Question: Adam has an orchard.Every day for 30 days he picks 4 apples from his orchard.After a month, Adam has collected all the remaining apples, which were 230.How many apples in total has Adam collected from his orchard?\newline Let's think step by step During 30 days Adam picked 4 * 30 = 120 apples.\newline So in total with all the remaining apples, he picked 120 + 230 = 350 apples from his orchard.\newline The answer is 350 Question: {question} \newline Let's think step by step       In animal cells, which of the following represents the most likely pathway that a secretory protein takes as it is synthesized in a cell?Answer Choices: (A) Plasma membrane-Golgi apparatus-ribosome-secretory vesicle-rough ER (B) Ribosome-Golgi apparatus-rough ER-secretory vesicle-plasma membrane (C) Plasma membrane-Golgi apparatusribosome-secretory vesicle-rough ER (D) Ribosome-rough ER-Golgi apparatus-secretory vesicle-plasma membrane Protein synthesis starts at the ribosome, so we can eliminate (A) and (C).The ribosome is often in the endoplasmic reticulum and moves from there to the Golgi apparatus, where it is modified and packaged into a vesicle.The vesicle then floats to the plasma membrane and is secreted.The answer is (D).</p>
<p>No</p>
<p>A microwave oven is connected to an outlet, 120 V, and draws a current of 2 amps.At what rate is energy being used by the microwave oven?Answer Choices: (A) 10 W (B) 30 W (C) 60 W (D) 240 W Rate of energy usage is known as power; in an dissipative electrical circuit, power is given by voltage times current.So in our case, the power is 120 V times 2 amps, or 240 W. The answer is (D).</p>
<p>Which of the following is considered an acid anhydride?Answer Choices: (A) HCl (B) H2SO3 (C) SO2 (D) Al(NO3)3 An acid anhydride is a compound that is derived by removing water from an acid.The chemical formula for water is H2O, which means that we need to determine which of these options, when combined with H2O, forms an acid.SO2, or Sulfur dioxide, when combined with H2O, makes H2SO4, or sulfuric acid.The answer is (C).</p>
<p>What is the output of "abc"[::-1] in Python 3? Answer Choices: (A) Error (B) abc (C) cba (D) c We know that the slicing operator [::-1] takes all of the elements in the string in reverse order, so we reverse the order of the string "abc", resulting in "cba".The answer is (C).</p>
<p>Figure 2 :
2
Figure 2: The development pipelines of Qwen2-Math and Qwen2.5-Math.</p>
<p>is the number of responses in a group.π ref , π θ , and π old are reference, training, and sampling models, respectively.q and {o i } G i=1 are questions and generated responses set in training.The advantage of each responses Âi is calculated by Âi = ri−mean(ri) std(ri)</p>
<p>Figure 3 :
3
Figure 3: The Performance of Qwen2.5-Math-1.5/7/72B-Instructon MATH by CoT compared to models of the same size.</p>
<p>Figure 4:  The Performance of Qwen2.5-Math-1.5/7/72B-Instructby using TIR compared to using CoT.We use blue color to represent the performance of TIR, and orange to represent the performance of CoT.It can be seen that TIR can achieve further performance improvement compared to CoT.</p>
<p>) extensive use of synthesized mathematical data from Qwen2-Math during the pre-training phase, (2) iterative generation of fine-tuning data and reinforcement training guided by the reward model during the post-training and inference phase and (3) support for bilingual (English and Chinese) queries, along with chain-of-thought and tool-integrated reasoning capabilities.As a result, Qwen2.5-Mathrepresents the most advanced open-source math model series to date.The Qwen2.5-Math-1.5B-Instructmodel already surpasses most previous 70B math models, while the Qwen2.5-Math-7B-Instructmatches the performance of Qwen2-Math-72B-Instruct. Our flagship model, Qwen2.5-Math-7B-Instruct,outperforms Qwen2-Math-72B-Instruct with an average score increase of 4.4 points across 7 datasets.We hope that the advances we've made with specialized models like Qwen2.5-Math will continue to strengthen the overall capabilities of the Qwen model and bring us closer to achieving artificial general intelligence.A CASE STUDY OF QWEN2-MATH ON OLYMPIAD-LEVEL PROBLEMSA.1 NUMBER THEORY   Problem From IMO Shortlist 2002:</p>
<p>need to determine 2002 2002 mod 9. First, we find 2002 mod 9: 2002 ÷ 9 = 222 remainder 4 ⇒ 2002 ≡ 4 (mod 9).Thus, 2002 2002 ≡ 4 2002 (mod 9).</p>
<p>-The sum b n−1 + • • • + b 1 + b 0 must be even.However, if a 0 is even, then b 0 = 0.This means the sum b n−1 + • • • + b 1 must be even.But since b i are either 0 or 1, the sum of b n−1 + • • • + b 1 being even implies that there must be an even number of 1's among b n−1 , . . ., b 1 .Now, consider the polynomial P (x) = x n + 1.For this polynomial: -a 0 = 1 (odd), which contradicts our requirement that a 0 must be even.-The sum b n−1 + • • • + b 1 + b 0 = 1 (odd), which also contradicts our requirement that the sum must be even.Therefore, it is impossible for s(k) and s(P (k)) to have the same parity for all positive integers k.</p>
<p>Question: In 2004, there were 60 kids at a cookout.In 2005, half the number of kids came to the cookout as compared to 2004.In 2006, 2/3 as many kids came to the cookout as in 2005.How many kids came to the cookout in 2006?\newline Let's think step by step In 2005, 60/2=30 kids came to the cookout.\newline In 2006, 30/3*2=20 kids came to the cookout.\newline The answer is 20</p>
<p>Question: Mr. Gardner bakes 20 cookies, 25 cupcakes, and 35 brownies for his second-grade class of 20 students.If he wants to give each student an equal amount of sweet treats, how many sweet treats will each student receive?\newline Let's think step by step Mr. Gardner bakes a total of 20 + 25 + 35 = 80 sweet treats \newline Each student will receive 80 / 20 = 4 sweet treats \newline The answer is 4 Question: A used car lot has 24 cars and motorcycles (in total) for sale.A third of the vehicles are motorcycles, and a quarter of the cars have a spare tire included.How many tires are on the used car lot's vehicles in all?\newline Let's think step by step The used car lot has 24 / 3 = 8 motorcycles with 2 tires each.\newline The lot has 24 -8 = 16 cars for sale \newline There are 16 / 4 = 4 cars with a spare tire with 5 tires each.\newline The lot has 16 -4 = 12 cars with 4 tires each.\newline Thus, the used car lot's vehicles have 8 * 2 + 4 * 5 + 12 * 4 = 16 + 20 + 48 = 84 tires in all.\newline The answer is 84 Question: Norma takes her clothes to the laundry.She leaves 9 T-shirts and twice as many sweaters as T-shirts in the washer.When she returns she finds 3 sweaters and triple the number of T-shirts.How many items are missing?\newline Let's think step by step Norma left 9 T-shirts And twice as many sweaters, she took 9 * 2= 18 sweaters \newline Adding the T-shirts and sweaters, Norma left 9 + 18 = 27 clothes \newline When she came back, she found 3 sweaters And triple the number of T-shirts, she found 3 * 3 = 9 T-shirts \newline Adding the T-shirts and sweaters, Norma found 3 + 9 = 12 clothes \newline Subtracting the clothes she left from the clothes she found, 27 -12 = 15 clothes are missing \newline The answer is 15</p>
<p>Figure 5 :
5
Figure5: The prompt used in evaluating GSM8K on the base models.For friendly presentation, we denote a line break as a "\newline".</p>
<p>Figure 9: The prompt used in evaluating GaoKao Math Cloze on the base models.</p>
<p>Figure 10 :
10
Figure 10: The prompt used in evaluating GaoKao Math QA on the base models.</p>
<p>{question}</p>
<p>Please reason step by step, and put your final answer within \boxed{}</p>
<p>Figure 11 :
11
Figure 11: The prompt used in evaluating the zero-shot Chain of Thought Reasoning on the instruct models.</p>
<p>{question}</p>
<p>Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \boxed{}.</p>
<p>Figure 12 :
12
Figure 12: The prompt used in evaluating the zero-shot Tool-Integrated Reasoning on the instruct models.</p>
<p>{question}</p>
<p>Figure 13 :
13
Figure13: The prompt used in evaluating the MMLU-STEM on the instruct models.</p>
<p>Table 1
1
provides examples of similar problems identified across the training and test sets.
Problems from MATH train (filtered):Problems from MATH test:What is the remainder when 1 + 2 + 3 + 4 +What is the remainder when 1 + 2 + 3 + 4 +• • • + 9 + 10 is divided by 8?• • • + 9 + 10 is divided by 9?For how many integer values of n between 1For how many integer values of n between 1and 1000 inclusive does the decimal represen-and 1000 inclusive does the decimal represen-tation of n 1400 terminate?tation of n 1375 terminate?Krista put 1 cent into her new bank on a Sun-Krista put 1 cent into her new bank on a Sun-day morning. On Monday she put 2 cents intoday morning. On Monday she put 2 cents intoher bank. On Tuesday she put 4 cents into herher bank. On Tuesday she put 4 cents into herbank, and she continued to double the amountbank, and she continued to double the amountof money she put into her bank each day forof money she put into her bank each day fortwo weeks. On what day of the week did thetwo weeks. On what day of the week did thetotal amount of money in her bank first ex-total amount of money in her bank first ex-ceed $2?ceed $5?</p>
<p>Table 1 :
1
Examples of filtered samples in the MATH training set with similar samples in the test set.</p>
<p>,
BENCHMARKENZHMODELGSM8K MATHMMLU STEMCMATHGaoKao Math ClozeGaoKao Math QA8-shot4-shot4-shot6-shot5-shot4-shotGeneral ModelLlama-3.1-8B56.720.353.151.58.528.5Llama-3.1-70B85.541.478.175.511.943.3Llama-3.1-405B89.053.8----Qwen2-1.5B58.521.744.855.612.735.6Qwen2-7B79.944.267.676.737.351.6Qwen2-72B89.551.179.985.455.972.6Specific ModelDeepSeekMath-Base-7B64.236.256.571.720.340.7DeepSeek-Coder-V2-Lite-Base68.338.159.577.825.451.3Internlm2-Math-Base-20B68.230.463.065.916.940.2Qwen2-Math-1.5B71.344.450.479.637.350.7Qwen2-Math-7B80.450.465.783.248.357.3Qwen2-Math-72B89.160.579.186.472.969.5Qwen2.5-Math-1.5B76.849.851.383.047.554.1Qwen2.5-Math-7B91.655.467.885.057.669.5Qwen2.5-Math-72B90.866.882.889.772.986.3</p>
<p>Table 3
3BenchmarkZHModelGaoKaoCMATHCN Middle School 24Avg.CHAIN-OF-THOUGHTGPT-4o-2024-08-0642.692.560.465.2DeepSeekMath-7B-RL33.686.767.362.5DeepSeek-Coder-V2-Lite-Instruct51.189.866.369.1Internlm2-math-plus-7B34.582.732.750.0Internlm2-math-plus-20B36.181.333.750.4Internlm2-math-plus-mixtral8x7B37.385.739.654.2Mathstral-7B-v0.131.676.742.650.3NuminaMath-7B-CoT36.478.260.458.3NuminaMath-72B-CoT47.987.375.270.1Llama-3.1-8B-Instruct30.464.843.646.3Llama-3.1-70B-Instruct41.786.759.462.6Qwen2-1.5B-Instruct17.065.531.738.1Qwen2-7B-Instruct35.183.554.557.7Qwen2-72B-Instruct54.692.274.373.746.584.266.365.7Qwen2-Math-1.5B-Instruct50.1maj@888.0maj@870.3maj@869.5maj@858.2rm@892.2rm@875.2rm@875.2rm@849.090.069.369.4Qwen2-Math-7B-Instruct59.5maj@891.7maj@872.3maj@874.5 maj@862.7rm@894.0rm@878.2rm@878.3rm@859.892.877.276.6Qwen2-Math-72B-Instruct61.7maj@893.2maj@879.2maj@878.0 maj@867.7rm@894.2rm@878.2rm@880.0 rm@8Qwen2.5-Math-1.5B-Instruct62.4 66.4maj@889.7 91.7maj@876.2 77.2maj@876.1 78.4 maj@867.5rm@894.0rm@880.2rm@880.6 rm@8Qwen2.5-Math-7B-Instruct66.3 68.1maj@891.8 92.7maj@873.3 78.2maj@877.1 79.7 maj@872.2rm@894.5rm@881.2rm@882.6 rm@8Qwen2.5-Math-72B-Instruct68.6 75.0maj@894.3 95.3maj@879.2 79.2maj@882.7 83.2 maj@876.5rm@895.7rm@880.2rm@884.1 rm@8TOOL-INTEGRATED REASONINGQwen2.5-Math-1.5B-Instruct59.6 68.3maj@889.3 90.8maj@871.3 78.2maj@873.4 79.1 maj@864.1rm@893.2rm@878.2rm@878.5 rm@8Qwen2.5-Math-7B-Instruct62.9 70.8maj@890.5 92.0maj@875.2 78.2maj@876.2 80.3 maj@872.9rm@894.2rm@880.2rm@882.4 rm@8
, we can draw the following conclusions: (1) Qwen2-Math-Instruct has demonstrated exceptional capabilities.The 1.5B</p>
<p>https://github.com/alibaba/ChatLearn
https://github.com/QwenLM/Qwen2-Math/tree/main/evaluation
https://huggingface.co/datasets/AI-MO/aimo-validation-aime
https://huggingface.co/datasets/AI-MO/aimo-validation-amc
https://github.com/llmeval/Llmeval-Gaokao2024-Math
ACKNOWLEDGEMENTSWe sincerely appreciate the support from other members of the Qwen team.We would also like to thank the ChatLearn team from PAI, Alibaba, for their infrastructure support of large-scale reinforcement learning.We need to find the smallest t such that the sum of t cubes is congruent to 4 modulo 9.The possible sums of cubes modulo 9 are: 0 ≡ 0 (mod 9), 1 ≡ 1 (mod 9), −1 ≡ 8 (mod 9), 1 + 1 ≡ 2 (mod 9), 1 + (−1) ≡ 0 (mod 9), −1 + (−1) ≡ −2 ≡ 7 (mod 9), 1 + 1 + 1 ≡ 3 (mod 9), 1 + 1 + (−1) ≡ 1 (mod 9), 1 + (−1) + (−1) ≡ −1 ≡ 8 (mod 9), −1 + (−1) + (−1) ≡ −3 ≡ 6 (mod 9), 1 + 1 + 1 + 1 ≡ 4 (mod 9).We see that the sum of four cubes can be congruent to 4 modulo 9. Therefore, the smallest t is 4.Thus, the smallest positive integer t such that there exist integers x 1 , x 2 , . . ., x t withProblem From IMO Shortlist 2022For a positive integer n we denote by s(n) the sum of the digits of n.x + a 0 be a polynomial, where n ≥ 2 and a i is a positive integer for all 0 ≤ i ≤ n − 1.Could it be the case that, for all positive integers k, s(k) and s(P (k)) have the same parity?Qwen2-Math-72B-Instruct SolutionTo determine whether it is possible for s(k) and s(P (k)) to have the same parity for all positive integers k, we need to analyze the behavior of the sum of the digits function s and the polynomial P (x).Problem from JBMO Shortlist 2011Let AD, BF and CE be the altitudes of △ ABC.A line passing through D and parallel to ABintersects the line EF at the point G.If H is the orthocenter of △ ABC, find the angle ∠CGH.Qwen2-Math-72B-Instruct SolutionTo solve for the angle ∠CGH in the given triangle △ABC with altitudes AD, BF , and CE intersecting at the orthocenter H, and a line through D parallel to AB intersecting EF at G, we will follow these steps: Problem: Find the domain of the expression $\frac{\sqrt{x-2}}{\sqrt{5-x}}$.Solution:The expressions inside each square root must be non-negative.Therefore, $x-2 \ge 0$, so $x\ge2$, and $5 -x \ge 0$, so $x \le 5$.Also, the denominator cannot be equal to zero, so $5-x&gt;0$, which gives $x&lt;5$.Therefore, the domain of the expression is $\boxed{[2,5)}$.The answer is: $[2,5)$.Problem: If $\det \mathbf{A} = 2$ and $\det \mathbf{B} = 12,$ then find $\det (\mathbf{A} \mathbf{B}).$Solution:We have that $\det (\mathbf{A} \mathbf{B}) = (\det \mathbf{A})(\det \mathbf{B}) = (2)(12) = \boxed{24}.$The answer is: $24$.Problem:Terrell usually lifts two 20-pound weights 12 times.If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?Solution:If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\cdot 12\cdot20=480$ pounds of weight.If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\cdot15\cdot n=30n$ pounds of weight.Equating this to 480 pounds, we can solve for $n$: \begin{align<em>} 30n&amp;=480\ \Rightarrow\qquad n&amp;=480/30=\boxed{16} \end{align</em>} The answer is: $16$.Problem:If the system of equations \begin{align<em>} 6x-4y&amp;=a,\ 6y-9x &amp;=b.\end{align</em>} has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\frac{a}{b},$ assuming $b$ is nonzero.Solution:If we multiply the first equation by $-\frac{3}{2}$, we obtain $$6y-9x=-\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have $$-\frac{3}{2}a=b\Rightarrow\frac{a}{b}=\boxed{-\frac{2}{3}}.$$The answer is: $-\frac{2}{3}$.Problem: {instruction}Solution: The expressions inside each square root must be non-negative.Therefore, $x-2 \ge 0$, so $x\ge2$, and $5 -x \ge 0$, so $x \le 5$.Also, the denominator cannot be equal to zero, so $5-x&gt;0$, which gives $x&lt;5$.\newline Therefore, the domain of the expression is $\boxed{[2,5)}$.\newline Final Answer: The final answer is (B).I hope it is correct.
A I , Meta , Llama 3 model card. 2024</p>
<p>Llemma: An open language model for mathematics. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, ICLR. OpenReview.net2024</p>
<p>Identifying and filtering near-duplicate documents. Andrei Z Broder, CPM. Lecture Notes in Computer Science. Springer20001848</p>
<p>Towards scalable automated alignment of LLMs: A survey. Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, Bowen Yu, CoRR, abs/2406.012522024</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Olympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, ACL (1). Association for Computational Linguistics2024</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, ICLR. OpenReview.net. 2021a</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, NeurIPS Datasets and Benchmarks. 2021b</p>
<p>Fasttext.zip: Compressing text classification models. Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, Tomás Mikolov, CoRR, abs/1612.036512016</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Slone, NeurIPS. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, 2022a</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Cem Slone, Imanol Anil, Theo Schlag, Yuhuai Gutman-Solo, Behnam Wu, Guy Neyshabur, Vedant Gur-Ari, Misra, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022, 2022b</p>
<p>Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning. Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, Dayiheng Liu, 2024a</p>
<p>Mugglemath: Assessing the impact of query and response augmentation on math reasoning. Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024b1ACL 2024</p>
<p>. L I Jia, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Longhui Rasul, Albert Yu, Ziju Jiang, Zihan Shen, Bin Qin, Li Dong, Yann Zhou, Guillaume Fleureau, Stanislas Lample, Polu, Numinamath, 2024</p>
<p>MARIO: math reasoning with code interpreter output -A reproducible pipeline. Minpeng Liao, Chengxi Li, Wei Luo, Jing Wu, Kai Fan, Findings of the Association for Computational Linguistics, ACL 2024. Team Mistral-AI. Mathstral. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024. 2024and virtual meeting</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Jan Paul F Christiano, Ryan Leike, Lowe, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Introducing qwen2-math. Team Qwen, 2024</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, CoRR, abs/2402.033002024</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, ICML. OpenReview.net. 2024a</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, AustriaJuly 21-27, 2024. 2024bOpenReview.net</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>CMATH: can your language model pass chinese elementary school math test?. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, Bin Wang, CoRR, abs/2306.166362023</p>
<p>. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, 2024Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, CoRR, abs/2402.06332Internlm-math: Open math large language models toward verifiable reasoning. Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin, 2024</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, 2023</p>
<p>MAmmoTH: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.056532023arXiv preprint</p>
<p>Mammoth2: Scaling instructions from the web. Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, CoRR, abs/2405.035482024</p>
<p>Evaluating the performance of large language models on GAOKAO benchmark. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng Qiu, 10.48550/arXiv.2305.124742023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, NAACL-HLT (Findings). Association for Computational Linguistics2024</p>
<p>Jiuzhang3.0: Efficiently improving mathematical reasoning by training small data synthesis models. Kun Zhou, Beichen Zhang, Jiapeng Wang, Zhipeng Chen, Wayne Xin Zhao, Jing Sha, Zhichao Sheng, Shijin Wang, Ji-Rong Wen, CoRR, abs/2405.143652024</p>
<p>Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Yukun Wu, Huazuo Li, Shirong Gao, Ma, arXiv:2406.11931问题：芳芳买了一本书有99页，看了90页，她还剩多少页没有看？ 答案：还剩的没有看的页数=书的总页数-芳芳看了的页数，99-90=9。所以答案是：9。 问题：张师傅上午修了18把椅子，下午修了29把椅子，一天共修了多少把椅子？ 答案：一天共修的椅子数量=上午修的椅子数量+下午修的椅子数量，18+29=47。所以答案是：47。 问题：小猴摘了84个桃子，平均分给6只猴子，每只猴子能吃到几个桃子？ 答案：每只猴子能吃到的桃子数=总桃子数/猴子的数量，84/6=14。所以答案是：14。 问题：用面包机烤面包时，第一面烤2分钟，第二面只要烤1分钟，即烤一片面包需要3分钟，小勤的面包 机一次只能放2片，他每天早上吃3片面包，至少需要烤多少分钟？ 答案：可以现将两片面包放入面包机烤2分钟，再将其中一片拿出来，将第三片面包放进去，烤1分钟， 这样第一片面包就烤好了，将第一片面包拿出来将第二片面包放进去，继续烤1分钟，于是第二片面包也 烤好了将其拿出来，第三片面包再烤1分钟也就烤好了，一共是2+1+1=5。所以答案是：5。 问题：一组学生植树，每人栽6棵还剩4棵；如果其中3人各栽5棵，其余每人各栽7棵，正好栽完。这一组 学生有多少人？ 答案：假设学生的数量是x，每人栽6棵还剩4棵，也就是说树苗的数量=6x+4，又知道如果其中3人各栽5 棵，其余每人各栽7棵，正好栽完，即6x+4=3<em>5+. 2024arXiv preprintx-3)</em>7，化简方程得到：x=10。所以答案是：10。 问题：某小学在"献爱心--为汶川地震区捐款"活动中，六年级五个班共捐款8000元，其中一班捐款1500</p>
<p>元，二班比一班多捐款200元，三班捐款1600元，四班与五班捐款数之比是3：5．四班捐款多少元？ 答案：一班捐款1500元，而二班比一班多捐200元，所以二班捐款1500+200=1700元，又知道六年级五个 班一共捐款8000元，所以四班和五班捐款之和 = 一共捐款 -一班和二班和三班捐款之和，即8000-1500-1700-1600=3200元，而题目说四班与五班捐款数之比是3：5，则四班捐款了3200/(3+5)*3=1200元。所以 答案是：1200。 问题：{question} 答案： Figure 8: The prompt used in evaluating CMATH on the base models. 选择题:下列有关命题的说法正确的是( ) \newline A. 命题"若$ x^{2}=1 $，则$ x=1 $"的否命题为："若$ x^{2}=1 $，则$ x\neq 1 $" \newline B. 命题"若$ x=y $，则$ \sin x=\sin y $"的逆否命题为真命题 \newline C. 命题"存在$ x∈R $，使得$ x^{2}+x+1 &lt; 0 $"的否定是："对任意$ x∈R $，均有$ x^{2}+x+1 &lt; 0 $" \newline D. "$ x=-1 $"是"$ x^{2}-5x-6=0 $"的必要不充分条件 解:命题"若$ x^{2}=1 $,则$ x=1 $"的否命题为"若$. x^{2}\neq 1 $,则$ x\neq 1 $",故排除$ A $</p>
<p>. $ , $ 命题"若$ X=y, $ , 则 $ \sin X=\sin Y $"为真命题, 故其逆否命题为真命题 , </p>
<p>存在$ x∈R $,使得$ x^{2}+x+1 &lt; 0 $"的否定是:"对任意$ x∈R $,均有$ x^{2}+x+1 &gt; 0 $. 故排除$ C $</p>
<p>$ , $ , $ x^{2}-5x-6=0 $"$ ⇔ $"$ x=-1 $或$ x=6 $",$ ∴ $"$ x=-1 $"是"$ x^{2}-5x-6=0 $"的充分不必要条件, 排除 $ D $. </p>
<p>B 故选, 推理结束, 选择题, 已知函数$ f $($ x $$ )= $$ x $$ {\,!}^{2}+ $$ mx $$ -1 $,若对于任意$ x $$ ∈[ $$ m $,$ m $$ +1] $,都 有$ f $($ x $$ ). </p>
<p>若复数$ a+ \dfrac{5i}{1-2i}(a∈R) $是纯虚数,则$ a $等于( ) \newline A. $ -1 $ \newline B. $ 1 $ \newline C. $ 2 $ \newline D. $ -2 $ 解:$ ∵a+ \dfrac {5i}{1-2i}=a+ \dfrac {5i(1+2i)}{(1-2i)(1+2i)}=a+ \dfrac {-10+5i}{5}=a-2+i $是纯虚数. $ 成立, 则实数$ M $的取值范围是(, $ ∴a=2 $. 故选:C. 推理结束。 选择题:已知集合$ A={2\leqslant x &lt; 7} $,$ B={x|3 &lt; x &lt; 10} $,$ C={x|a-5 &lt; x &lt; a} $ $ . $若非空集合$ C⊑(A∪B) $,则$ a $的取值范围是( ) \newline A. $ 7\leqslant a\leqslant 10 $ \newline B. $ 7\leqslant x &lt; 10 $ \newline C. $ 8 &lt; x &lt; 10 $ \newline D. $ 8\leqslant x\leqslant 10 $ 解:$ ∵ $集合$ A={x|2\leqslant x &lt; 7} $,$ B={x|3 &lt; x\leqslant 10} $,$ ∴A∩B={x|3 &lt; x &lt; 7} $, $ A∪B={x|2\leqslant x\leqslant 10} $. A \left(-\sqrt{2},0\right) $ \newline, B \left(-2,0\right) $ \newline, C \left, \dfrac{ \sqrt{2}}{2},0\right) $ 解:由题意可得$ \begin{cases}f(m)=2{m}^{2}-1 &lt; 0 \ f(m+1)=2{m}^{2}+3m &lt; 0\end{cases} $, 求得$ -\dfrac{ \sqrt{2}}{2} &lt; m &lt; 0 $, 即实数$ m $的取值范围为$ (-\dfrac{ \sqrt{2}}{2} ,0) $. 故选:D. 推理结束。 选择题:设$ i $是虚数单位. A∪B={x|2\leqslant x\leqslant 10} $, 当$ C\neq \varnothing $时,要使$ C⊆(A∪B) $,$ \begin{cases}a-5\geqslant 2 \ a\leqslant 10\end{cases} $,解得$ 7\leqslant a\leqslant 10 $</p>
<p>$ ∴a $的取值范围是$ 7\leqslant a\leqslant 10 $. 故选:A. 推理结束。 选择题:{question} \newline A. {option_a} \newline B. {option_b} \newline C. {option_c} \newline D. {option_d} 解: 已知 $\alpha, \beta, \gamma$ 是互不相同的锐角, 则在 $\sin \alpha \cos \beta, \sin \beta \cos \gamma, \sin \gamma \cos \alpha$ 三个值中, 大于 $\frac{1}{2}$ 的个数的最大值是 (. 3从以下选项中选择:\newline(A) 0 :\newline(B) 1 :\newline(C) 2 :\newline(D)</p>
<p>如果 $\alpha, \beta, \gamma$ 均小于 $60^\circ$，那么他们的正弦值都小于 $\frac{1}{2}$，因此三个值中 不可能有大于 $\frac{1}{2}$ 的值。 \newline2. 如果有一个角大于 $60^\circ$，假设为 $\alpha$，那么对应 的正弦值大于 $\frac{1}{2}$。此时，由于三角形内角和为 $180^\circ$，所以 $\beta + \gamma &lt; 120^\circ$。 这意味着 $\beta, \gamma$ 的余弦值均大于 $\frac{1}{2}$，所以此时 $\sin \alpha \cos \beta &gt; \frac{1}{2}, \sin \beta \cos \gamma &gt; \frac{1}{2}$。 \newline3. 如果有两个角大于 $60^\circ$，例如 $\alpha$ 和 $\beta$，那么 由于三角形内角和为 $180^\circ$，我们可以得到 $\gamma &lt; 60^\circ$，此时 $\sin \gamma &lt; \frac{1}{2}$。 由于 $\alpha$ 和 $\beta$ 的余弦值都小于 $\frac{1}{2}$，因此三个值中不可能有大于 $\frac{1}{2}$ 的值。. </p>
<p>从以下选项中选择: :\newline(A) $\frac{\sqrt{2}}{3}$ :\newline(B) $\frac{\sqrt{3}}{3}$ :\newline(C) $\frac{2}{3}$ :\newline(D) $\frac{\sqrt{6}}{3}$ 设上下底面的中心分别为 $\mathrm{O}<em 1="1">{1}, \mathrm{O}$, 设正方体的棱长等于 1. ， 如果三个角都大于 $60^\circ$，显然不符合题意。 \newline综上所述, $ 当有一个角大于 $60^\circ$ 时， 大于 $\frac{1}{2}, 。 答案是 C 正方体 $a B C D-A</em>} B_{1} C_{1} D_{1}$ 中 的个数的最大值是, $b B_{1}$ 与平面 $a C D_{1}$ 所成角的余弦值为 ; 则 $o_{1} O$ 与平面 $a C D_{1}$ 所成角就是 $b B_{1}$ 与平面 $a C D_{1}$ 所成角, 即 $\angle O_{1} O D_{1}$, 直角三角形 $\mathrm{OO<em 1="1">{1} \mathrm{D}</em>}$ 中, $\cos \angle \mathrm{O<em 1="1">{1} \mathrm{OD}</em>}=\frac{\mathrm{O<em 1="1">{1} \mathrm{O}}{\mathrm{OD}</em>1+\log }}=\frac{\frac{1}{\sqrt{6}}}{2}=\frac{\sqrt{6}}{3}$. 答案是 C 设函数 $f(x)=\left{\begin{array}{ll<em 2="2">{2}(2-x), &amp; x&lt;1 \ 2^{x-1}, &amp; x \geqslant 1,\end{array}\right.$ 则 $f. 12+f\left(\log </em> 12\right)=$ ( ) 从以下选项中选择: :\newline(A) 3 :\newline(B) 6 :\newline(C) 9 :\newline(D)</p>
<p>从以下选项中选择: :\newline(A) $(1,+\infty)$ :\newline(B) $(2,+\infty)$ :\newline(C) $(-\infty,-1)$ :\newline(D) $(-\infty,-2)$ 首先，我们可以通过求出函数的导函数 $f'(x)$ 来判断函数在 $x&gt;0$ 区间内的单调性。在这里，我们求出 导函数 $f'(x)$ 为 $f'(x)=3ax^2-6x$。\newline然后，我们需要求出导函数 $f'(x)$ 的零点，以确定函数 $f(x)$ 在 $x&gt;0$ 区间内的单调性。导函数 $f'(x)$. 首先，我们可以根据定义计算 $f ; $ 和 $f, 的零点为 $x=0$ 和 $x=\frac{2}{\sqrt{a}}$。注意到 $x&gt;0$，所以 我们得到 $a&lt;0$。此外，由于函数 $f(x)$ 在 $x=0$ 处的函数值为 $1$，因此不能有 $a=\frac{4}{3}$。 \newline综上所述，当 $a$ 的取值范围为 $a&lt;-\frac{4}{3}$ 时，函数 $f(x)$ 在 $x&gt;0$ 区间内是单调递减的， 此时存在唯一的零点 $x_0$。因此，答案为 $(-\infty,-2)$。 答案是 D 设 $\left{a_{n}\right}$ 是公差不为 0 的无穷等差数列, 则" $\left{a_{n}\right}$ 为递增数列"是"存在正 整数 $N_{0}$, 当 $n&gt;N_{0}$ 时, $a_{n}&gt;0$ "的 ( ) 从以下选项中选择: :\newline(A) 充分而不必要条件:\newline(B) 必要而不充分条件:\newline(C) 充分必要条 件:\newline(D) 既不充分也不必要条件 首先，我们可以通过举例来判断该条件是充分还是必要条件。如果一个数列递增，那么它的公差一定大 于 0，也就是存在正整数 $N_{0}$，当 $n&gt;N_{0}$ 时，$a_{n}&gt;0$。因此，" $\left{a_{n}\right}$ 为递增 数列"是"存在正整数 $N_{0}$, 当 $n&gt;N_{0}$ 时, $a_{n}&gt;0$ "的必要条件。\newline接下来，我们需要判 断是否充分。也就是说，如果存在正整数 $N_{0}$，当 $n&gt;N_{0}$ 时，$a_{n}&gt;0$，那么能否得出 " $\left{a_{n}\right}$ 为递增数列"这一结论。答案是肯定的。因为如果 $a_{n}&gt;0$，那么 $a_{n+1}-a_{n}&gt;0$，即公差大于 0，因此该数列是递增的。因此，该条件是充分条件. =1+\log_2 4=3$ $f(\log_2 12)=2^{\log_2 12-1}=6$\newline因此，$f(-2)+f(\log_2 12)=3+6=9$。 答案是 C 已知函数 $f(x)=a x^{3}-3 x^{2}+1$, 若 $f(x)$ 存在唯一的零点 $x_{0}$, 且 $x_{0}&gt;$ 0 , 则实数 $\mathrm{a}$ 的取值范围是 (. 。\newline综上所述，选项为 (C) 充分必要条件。 答案是 C {question} Figure 14: The prompt used in evaluating the multiple-choice problems in GaoKao on the instruct models</p>            </div>
        </div>

    </div>
</body>
</html>
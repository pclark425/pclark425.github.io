<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7861 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7861</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7861</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-276249976</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.06193v3.pdf" target="_blank">Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</a></p>
                <p><strong>Paper Abstract:</strong> Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored. In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide...</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7861.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7861.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BatchEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BatchEval (output-based LLM-as-a-judge inference strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An output-based evaluation strategy that scores batches of responses across multiple rounds to encourage a more balanced score distribution; in this study it was used with large LLMs (GPT-4o) to judge SE outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Code Translation (also evaluated on Code Generation and Code Summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CodeTransOcean (MultilingualTrans2 subset) for translation; ComplexCodeEval for generation; CodeXGLUE for summarization</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>BatchEval (prompting/inference strategy applied to GPT-4o for main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>BatchEval applied with GPT-4o, temperature 0.2, batch size 10, number of rounds 5; GPT-4o is a large proprietary LLM used via API in the study</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two domain-expert human evaluators (expertise in Java/Python/C/C++); final human score is the average of two evaluators' overall scores (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson R (primary reported high-value metric in paper for best methods)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.8132</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>poor generalizability across tasks; low performance on code summarization; inconsistent pairwise comparison results (low agreement when swapping order); misses fine-grained dependency usage in ComplexCodeEval</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>BatchEval with a large LLM closely matches human score distributions in code translation (near-human alignment) and outperforms conventional metrics there; performance drops on code generation and is poor on summarization; pairwise comparison accuracy higher than random but suffers very low consistency when positions are swapped.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High human-alignment in certain tasks (code translation); does not require reference answers; can produce explanations; more human-like, balanced score distribution when using batching strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>150 instructions (50 per task), 3 responses per instruction (randomly sampled LLMs) → 450 responses; BatchEval used GPT-4o with 5 rounds, batch size 10, temperature 0.2; human evaluation used two expert annotators scoring aspects (1-5) and averaging; correlations computed (Spearman, Pearson, Kendall) and scaled in paper (values presented ×100).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7861.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7861.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V2.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-Coder-V2.5 (output-based LLM judge / large code LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large code-specialized LLM used in output-based evaluation; in the study it provided some of the best human alignment on code generation and strong performance on translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Code Generation (primary strong result) and Code Translation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ComplexCodeEval (code generation); CodeTransOcean (translation)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>DeepSeek-V2.5 (output-based evaluation using DeepSeek V2.5)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>DeepSeek-V2.5 is a large mixture-of-experts code LLM (model family referenced in paper) used with greedy decoding for evaluations; specifics: used locally or via API per study.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two domain-expert human evaluators; averaged overall score (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson R</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.6851</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>limited generalizability across tasks; poorer performance on summarization; struggles with interpreting complex dependency usage beyond lexical matching in ComplexCodeEval</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DeepSeek-V2.5 is among the top output-based judge LLMs and outperforms conventional metrics on some tasks (notably translation) and is the only LLM to surpass conventional metrics on generation in this study; still fails on summarization and pairwise comparison consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Strong human alignment for generation when large, code-specialized models are used; can operate without reference answers and provide explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Vanilla output-based prompting with greedy decoding for scoring; responses generated from sampled LLMs; 450 responses evaluated, correlations computed between LLM scores and average human scores (Spearman, Pearson, Kendall).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7861.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7861.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (output-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used as an output-based judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art general-purpose LLM used in output-based evaluation modes (Vanilla, G-Eval, BatchEval) that shows high alignment with human judgments in code translation and moderate alignment in generation but poor alignment in summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Code Translation, Code Generation, Code Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CodeTransOcean (translation), ComplexCodeEval (generation), CodeXGLUE (summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (output-based prompting: Vanilla, G-Eval, BatchEval configurations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4o used via API for output-based scoring; used with greedy decoding (Vanilla) and with G-Eval/BatchEval strategies; large proprietary multimodal LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two domain-expert human evaluators; averaged final overall score (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson R</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.7911</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>inconsistency in pairwise comparisons (very low agreement when swapping pair order); verbosity bias (gives high scores to overly detailed summaries); sometimes misses subtle functional-preservation errors in translations</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4o output-based judgments closely mirror human score distributions for code translation, produce balanced distributions and high Pearson correlation; however, it can overlook subtle semantic/functionality differences and exhibits systematic biases (verbosity), and pairwise comparisons are often inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High human-alignment in translation and reasonable performance in generation; does not require references; generates explanations for judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Vanilla: single inference pass with greedy decoding (temperature 0); G-Eval: CoT + 20 samples averaged; BatchEval: multi-round batching; experiments used 450 responses and 900 pairs; correlations (Spearman, Pearson, Kendall) computed versus averaged human scores; pairwise comparisons evaluated for accuracy and agreement when swapping order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7861.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7861.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChrF++ (conventional)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChrF++ (character and token n-gram based conventional metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional, reference-based metric using character and token n-grams that showed relatively high alignment with human judgments for code generation in this study, outperforming many embedding- and probability-based LLM-judge methods on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Code Generation (also reported for translation and summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ComplexCodeEval (generation); CodeTransOcean; CodeXGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChrF++ (conventional automatic metric)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Reference-based character n-gram and token n-gram F2 scoring (ChrF++); applied with tokenization aligned to code via tiktoken</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two domain-expert human evaluators; averaged overall score (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson R</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.6492</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>focuses on lexical similarity so struggles with paraphrase and semantic equivalence in summarization; cannot provide explanations;</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChrF++ (and some conventional metrics) achieve mid-to-high correlations with human judgments on code generation—sometimes matching or exceeding smaller LLM-based judges—because lexical overlap reveals correct usage of dependencies; however, conventional metrics perform poorly on translation when reference implementations differ (they are disadvantaged by reference mismatch) and on summarization due to paraphrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>N/A (this entry documents a conventional metric used as a comparison baseline; advantages vs humans: fast and deterministic but limited in semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Reference-based metric computed between generated response and reference answer for each of the 450 responses; correlations (Spearman, Pearson, Kendall) computed with averaged human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7861.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7861.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PairwiseComparisonResults</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise comparison performance of output-based LLM-as-a-judge methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLM judges' ability to select the better of two responses (ternary: first, second, tie); study reports Accuracy vs human pairwise labels and Agreement (consistency when swapping the order of presented responses).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise comparison across Code Translation, Code Generation, Code Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Paired responses constructed from CodeTransOcean, ComplexCodeEval, CodeXGLUE (900 pairs total across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Output-based judges (Vanilla, G-Eval, BatchEval, GPT-4o, DeepSeek-V2.5, SFT models)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Various: Vanilla (greedy decoding), G-Eval (CoT + sampling), BatchEval (batched rounds), SFT models (Auto-J, Prometheus-v2 etc.); tests use GPT-4o and DeepSeek-V2.5 for best-performing setups</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human pairwise ground truth derived from absolute difference of averaged human scores (tie if diff < 0.5); two domain-expert annotators used to create ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (match to human pairwise label) and Agreement (consistency when swapping pair order)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.6533</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>low accuracy on generation and summarization; very low consistency when swapping response order (Agreement often <0.25); many methods near-random on summarization</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Although output-based methods with large LLMs achieve the highest pairwise accuracy (e.g., BatchEval/G-Eval ~0.64–0.65 on translation), their Agreement when responses are swapped is very low (often <0.25), indicating inconsistent and brittle comparison decisions; pairwise methods overall perform worse than individual scoring correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>For translation, pairwise output-based strategies with large LLMs yield higher accuracy than random and can sometimes outperform simple baselines; still, inconsistency undermines reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Three response pairs per instruction created; 900 total pairs; LLMs prompted with both responses and asked to pick better or tie; accuracy computed vs human-derived pairwise labels; Agreement computed as fraction of trials with same judgment after swapping order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7861.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7861.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-Interrater</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human inter-annotator agreement (reference for human-human reliability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measured agreement between the two human evaluators used for meta-evaluation; provides a ceiling/reference for LLM-human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Code Translation, Code Generation, Code Summarization (human-human agreement measured on the human scores used as ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Subset of CodeTransOcean, ComplexCodeEval, CodeXGLUE (the 450 responses annotated by humans)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>N/A (this row reports human-human agreement rather than an LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Two human experts in the chosen programming languages; each response scored by both and averaged</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two domain-expert human evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho, Pearson R, Kendall's tau (reported by task)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.8586</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (this documents human-human agreement as reference); however, human agreement is not perfect and sets an approximate ceiling for automated judges.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human evaluators show high but not perfect agreement: Pearson R for translation/generation/summarization = 0.8586 / 0.7970 / 0.7374 respectively (paper reports percentages scaled ×100); these numbers are used as an upper-bound baseline when assessing LLM alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>N/A for humans; used as a benchmark to compare LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Two expert annotators scored each response on two aspects then an overall score; final human score is average of the two overall scores; reported inter-annotator correlations: Spearman ρ = (0.8307, 0.7542, 0.7420), Pearson R = (0.8586, 0.7970, 0.7374), Kendall τ = (0.7226, 0.6340, 0.6257) for translation, generation, summarization respectively (paper displays these values ×100).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7861.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7861.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-Dependence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-dependent performance of LLM-as-a-judge methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate finding that LLM-as-a-judge alignment with human judgments depends strongly on the SE task: near-human for code translation (output-based large LLMs), on-par or mixed for code generation, and poor for code summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Cross-task conclusion (Code Translation, Code Generation, Code Summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CodeTransOcean, ComplexCodeEval, CodeXGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Various LLM-as-a-judge methods (output-based, embedding-based, probability-based, SFT LLMs, conventional metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Study compares seven LLM-as-a-judge methods (embedding/probability/output categories), two SFT judges, and five conventional metrics across three SE datasets; some methods use large LLMs (GPT-4o, DeepSeek-V2.5)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two domain-expert human evaluators per response (averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman rho / Pearson R / Kendall tau (used to quantify alignment between LLM judges and humans)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>lack of generalizability across tasks; embedding/probability-based methods underperform; supervised-fine-tuned judges inconsistent when not trained on SE-specific preference data; LLMs show verbosity bias and miss concise summarization criteria</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Output-based methods with large LLMs achieve the best human alignment, but performance varies by task: high in translation (near-human), similar to conventional metrics in generation, and poor in summarization; inference strategies give marginal improvements for individual scoring but larger (yet insufficient) gains in pairwise comparisons; embedding- and probability-based methods both have limited applicability and lower human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When effective (task-dependent), LLM judges remove the need for reference answers, can provide explanations, and can scale evaluation faster than human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Meta-evaluation across 450 responses (50 instructions × 3 tasks × 3 LLMs per instruction), correlations computed per method per task; pairwise comparisons on 900 pairs; methods included embedding-based (BERTScore, MoverScore), probability-based (GPTScore, FFLM), output-based (Vanilla, G-Eval, BatchEval), and SFT models (Auto-J, Prometheus).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7861.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7861.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FailureCases-Examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative failure modes and case-study examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete qualitative failure examples observed: (1) translation that fails to preserve internal seeding semantics but is partially recognized by LLM judges; (2) summarization where GPT-4o rewards verbosity with a high score despite violating concision requirement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Code Translation (example) and Code Summarization (example)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Selected sample cases from CodeTransOcean (translation) and CodeXGLUE (summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (used in case-study explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4o provided textual explanations and final scores in output-based (Vanilla) mode; examples highlighted subtle misses and verbosity bias</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Two domain-expert human evaluators (ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>verbosity bias (prefers detailed step-by-step explanations, rewarding non-concise summaries); misses subtle functional differences (e.g., seeding/range differences in translated random-number code); inconsistent extraction of intended semantics</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Case: translation where Python replacement used stdlib random without seeding — GPT-4o identified discrepancy and gave a moderate score but overlooked some subtle numeric-range implications; Case: summarization where GPT-4o awarded 5/5 to an overly detailed summary, revealing a verbosity bias and misinterpretation of summarization criteria vs human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Explanations produced by LLMs can surface their judgment rationale, aiding debugging of judge behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Qualitative case-study: authors inspected explanations generated by GPT-4o under Vanilla prompting for specific failing and passing examples and compared them to human scores and criteria (illustrative examples included in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BatchEval: Towards Human-like Text Evaluation <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment <em>(Rating: 2)</em></li>
                <li>GPTScore: Evaluate as You Desire <em>(Rating: 1)</em></li>
                <li>Generative Judge for Evaluating Alignment <em>(Rating: 1)</em></li>
                <li>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization <em>(Rating: 1)</em></li>
                <li>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7861",
    "paper_id": "paper-276249976",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "BatchEval",
            "name_full": "BatchEval (output-based LLM-as-a-judge inference strategy)",
            "brief_description": "An output-based evaluation strategy that scores batches of responses across multiple rounds to encourage a more balanced score distribution; in this study it was used with large LLMs (GPT-4o) to judge SE outputs.",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Code Translation (also evaluated on Code Generation and Code Summarization)",
            "dataset_name": "CodeTransOcean (MultilingualTrans2 subset) for translation; ComplexCodeEval for generation; CodeXGLUE for summarization",
            "judge_model_name": "BatchEval (prompting/inference strategy applied to GPT-4o for main experiments)",
            "judge_model_details": "BatchEval applied with GPT-4o, temperature 0.2, batch size 10, number of rounds 5; GPT-4o is a large proprietary LLM used via API in the study",
            "human_evaluator_type": "Two domain-expert human evaluators (expertise in Java/Python/C/C++); final human score is the average of two evaluators' overall scores (1-5)",
            "agreement_metric": "Pearson R (primary reported high-value metric in paper for best methods)",
            "agreement_score": 0.8132,
            "reported_loss_aspects": "poor generalizability across tasks; low performance on code summarization; inconsistent pairwise comparison results (low agreement when swapping order); misses fine-grained dependency usage in ComplexCodeEval",
            "qualitative_findings": "BatchEval with a large LLM closely matches human score distributions in code translation (near-human alignment) and outperforms conventional metrics there; performance drops on code generation and is poor on summarization; pairwise comparison accuracy higher than random but suffers very low consistency when positions are swapped.",
            "advantages_of_llm_judge": "High human-alignment in certain tasks (code translation); does not require reference answers; can produce explanations; more human-like, balanced score distribution when using batching strategy.",
            "experimental_setting": "150 instructions (50 per task), 3 responses per instruction (randomly sampled LLMs) → 450 responses; BatchEval used GPT-4o with 5 rounds, batch size 10, temperature 0.2; human evaluation used two expert annotators scoring aspects (1-5) and averaging; correlations computed (Spearman, Pearson, Kendall) and scaled in paper (values presented ×100).",
            "uuid": "e7861.0",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DeepSeek-V2.5",
            "name_full": "DeepSeek-Coder-V2.5 (output-based LLM judge / large code LLM)",
            "brief_description": "A large code-specialized LLM used in output-based evaluation; in the study it provided some of the best human alignment on code generation and strong performance on translation.",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Code Generation (primary strong result) and Code Translation",
            "dataset_name": "ComplexCodeEval (code generation); CodeTransOcean (translation)",
            "judge_model_name": "DeepSeek-V2.5 (output-based evaluation using DeepSeek V2.5)",
            "judge_model_details": "DeepSeek-V2.5 is a large mixture-of-experts code LLM (model family referenced in paper) used with greedy decoding for evaluations; specifics: used locally or via API per study.",
            "human_evaluator_type": "Two domain-expert human evaluators; averaged overall score (1-5)",
            "agreement_metric": "Pearson R",
            "agreement_score": 0.6851,
            "reported_loss_aspects": "limited generalizability across tasks; poorer performance on summarization; struggles with interpreting complex dependency usage beyond lexical matching in ComplexCodeEval",
            "qualitative_findings": "DeepSeek-V2.5 is among the top output-based judge LLMs and outperforms conventional metrics on some tasks (notably translation) and is the only LLM to surpass conventional metrics on generation in this study; still fails on summarization and pairwise comparison consistency.",
            "advantages_of_llm_judge": "Strong human alignment for generation when large, code-specialized models are used; can operate without reference answers and provide explanations.",
            "experimental_setting": "Vanilla output-based prompting with greedy decoding for scoring; responses generated from sampled LLMs; 450 responses evaluated, correlations computed between LLM scores and average human scores (Spearman, Pearson, Kendall).",
            "uuid": "e7861.1",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o (output-based)",
            "name_full": "GPT-4o used as an output-based judge",
            "brief_description": "A state-of-the-art general-purpose LLM used in output-based evaluation modes (Vanilla, G-Eval, BatchEval) that shows high alignment with human judgments in code translation and moderate alignment in generation but poor alignment in summarization.",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Code Translation, Code Generation, Code Summarization",
            "dataset_name": "CodeTransOcean (translation), ComplexCodeEval (generation), CodeXGLUE (summarization)",
            "judge_model_name": "GPT-4o (output-based prompting: Vanilla, G-Eval, BatchEval configurations)",
            "judge_model_details": "GPT-4o used via API for output-based scoring; used with greedy decoding (Vanilla) and with G-Eval/BatchEval strategies; large proprietary multimodal LLM.",
            "human_evaluator_type": "Two domain-expert human evaluators; averaged final overall score (1-5)",
            "agreement_metric": "Pearson R",
            "agreement_score": 0.7911,
            "reported_loss_aspects": "inconsistency in pairwise comparisons (very low agreement when swapping pair order); verbosity bias (gives high scores to overly detailed summaries); sometimes misses subtle functional-preservation errors in translations",
            "qualitative_findings": "GPT-4o output-based judgments closely mirror human score distributions for code translation, produce balanced distributions and high Pearson correlation; however, it can overlook subtle semantic/functionality differences and exhibits systematic biases (verbosity), and pairwise comparisons are often inconsistent.",
            "advantages_of_llm_judge": "High human-alignment in translation and reasonable performance in generation; does not require references; generates explanations for judgments.",
            "experimental_setting": "Vanilla: single inference pass with greedy decoding (temperature 0); G-Eval: CoT + 20 samples averaged; BatchEval: multi-round batching; experiments used 450 responses and 900 pairs; correlations (Spearman, Pearson, Kendall) computed versus averaged human scores; pairwise comparisons evaluated for accuracy and agreement when swapping order.",
            "uuid": "e7861.2",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ChrF++ (conventional)",
            "name_full": "ChrF++ (character and token n-gram based conventional metric)",
            "brief_description": "A conventional, reference-based metric using character and token n-grams that showed relatively high alignment with human judgments for code generation in this study, outperforming many embedding- and probability-based LLM-judge methods on some tasks.",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Code Generation (also reported for translation and summarization)",
            "dataset_name": "ComplexCodeEval (generation); CodeTransOcean; CodeXGLUE",
            "judge_model_name": "ChrF++ (conventional automatic metric)",
            "judge_model_details": "Reference-based character n-gram and token n-gram F2 scoring (ChrF++); applied with tokenization aligned to code via tiktoken",
            "human_evaluator_type": "Two domain-expert human evaluators; averaged overall score (1-5)",
            "agreement_metric": "Pearson R",
            "agreement_score": 0.6492,
            "reported_loss_aspects": "focuses on lexical similarity so struggles with paraphrase and semantic equivalence in summarization; cannot provide explanations;",
            "qualitative_findings": "ChrF++ (and some conventional metrics) achieve mid-to-high correlations with human judgments on code generation—sometimes matching or exceeding smaller LLM-based judges—because lexical overlap reveals correct usage of dependencies; however, conventional metrics perform poorly on translation when reference implementations differ (they are disadvantaged by reference mismatch) and on summarization due to paraphrasing.",
            "advantages_of_llm_judge": "N/A (this entry documents a conventional metric used as a comparison baseline; advantages vs humans: fast and deterministic but limited in semantics).",
            "experimental_setting": "Reference-based metric computed between generated response and reference answer for each of the 450 responses; correlations (Spearman, Pearson, Kendall) computed with averaged human scores.",
            "uuid": "e7861.3",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "PairwiseComparisonResults",
            "name_full": "Pairwise comparison performance of output-based LLM-as-a-judge methods",
            "brief_description": "Evaluation of LLM judges' ability to select the better of two responses (ternary: first, second, tie); study reports Accuracy vs human pairwise labels and Agreement (consistency when swapping the order of presented responses).",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Pairwise comparison across Code Translation, Code Generation, Code Summarization",
            "dataset_name": "Paired responses constructed from CodeTransOcean, ComplexCodeEval, CodeXGLUE (900 pairs total across tasks)",
            "judge_model_name": "Output-based judges (Vanilla, G-Eval, BatchEval, GPT-4o, DeepSeek-V2.5, SFT models)",
            "judge_model_details": "Various: Vanilla (greedy decoding), G-Eval (CoT + sampling), BatchEval (batched rounds), SFT models (Auto-J, Prometheus-v2 etc.); tests use GPT-4o and DeepSeek-V2.5 for best-performing setups",
            "human_evaluator_type": "Human pairwise ground truth derived from absolute difference of averaged human scores (tie if diff &lt; 0.5); two domain-expert annotators used to create ground truth",
            "agreement_metric": "Accuracy (match to human pairwise label) and Agreement (consistency when swapping pair order)",
            "agreement_score": 0.6533,
            "reported_loss_aspects": "low accuracy on generation and summarization; very low consistency when swapping response order (Agreement often &lt;0.25); many methods near-random on summarization",
            "qualitative_findings": "Although output-based methods with large LLMs achieve the highest pairwise accuracy (e.g., BatchEval/G-Eval ~0.64–0.65 on translation), their Agreement when responses are swapped is very low (often &lt;0.25), indicating inconsistent and brittle comparison decisions; pairwise methods overall perform worse than individual scoring correlations.",
            "advantages_of_llm_judge": "For translation, pairwise output-based strategies with large LLMs yield higher accuracy than random and can sometimes outperform simple baselines; still, inconsistency undermines reliability.",
            "experimental_setting": "Three response pairs per instruction created; 900 total pairs; LLMs prompted with both responses and asked to pick better or tie; accuracy computed vs human-derived pairwise labels; Agreement computed as fraction of trials with same judgment after swapping order.",
            "uuid": "e7861.4",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Human-Interrater",
            "name_full": "Human inter-annotator agreement (reference for human-human reliability)",
            "brief_description": "Measured agreement between the two human evaluators used for meta-evaluation; provides a ceiling/reference for LLM-human alignment.",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Code Translation, Code Generation, Code Summarization (human-human agreement measured on the human scores used as ground truth)",
            "dataset_name": "Subset of CodeTransOcean, ComplexCodeEval, CodeXGLUE (the 450 responses annotated by humans)",
            "judge_model_name": "N/A (this row reports human-human agreement rather than an LLM judge)",
            "judge_model_details": "Two human experts in the chosen programming languages; each response scored by both and averaged",
            "human_evaluator_type": "Two domain-expert human evaluators",
            "agreement_metric": "Spearman's rho, Pearson R, Kendall's tau (reported by task)",
            "agreement_score": 0.8586,
            "reported_loss_aspects": "N/A (this documents human-human agreement as reference); however, human agreement is not perfect and sets an approximate ceiling for automated judges.",
            "qualitative_findings": "Human evaluators show high but not perfect agreement: Pearson R for translation/generation/summarization = 0.8586 / 0.7970 / 0.7374 respectively (paper reports percentages scaled ×100); these numbers are used as an upper-bound baseline when assessing LLM alignment.",
            "advantages_of_llm_judge": "N/A for humans; used as a benchmark to compare LLM judges.",
            "experimental_setting": "Two expert annotators scored each response on two aspects then an overall score; final human score is average of the two overall scores; reported inter-annotator correlations: Spearman ρ = (0.8307, 0.7542, 0.7420), Pearson R = (0.8586, 0.7970, 0.7374), Kendall τ = (0.7226, 0.6340, 0.6257) for translation, generation, summarization respectively (paper displays these values ×100).",
            "uuid": "e7861.5",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Task-Dependence",
            "name_full": "Task-dependent performance of LLM-as-a-judge methods",
            "brief_description": "Aggregate finding that LLM-as-a-judge alignment with human judgments depends strongly on the SE task: near-human for code translation (output-based large LLMs), on-par or mixed for code generation, and poor for code summarization.",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Cross-task conclusion (Code Translation, Code Generation, Code Summarization)",
            "dataset_name": "CodeTransOcean, ComplexCodeEval, CodeXGLUE",
            "judge_model_name": "Various LLM-as-a-judge methods (output-based, embedding-based, probability-based, SFT LLMs, conventional metrics)",
            "judge_model_details": "Study compares seven LLM-as-a-judge methods (embedding/probability/output categories), two SFT judges, and five conventional metrics across three SE datasets; some methods use large LLMs (GPT-4o, DeepSeek-V2.5)",
            "human_evaluator_type": "Two domain-expert human evaluators per response (averaged)",
            "agreement_metric": "Spearman rho / Pearson R / Kendall tau (used to quantify alignment between LLM judges and humans)",
            "agreement_score": null,
            "reported_loss_aspects": "lack of generalizability across tasks; embedding/probability-based methods underperform; supervised-fine-tuned judges inconsistent when not trained on SE-specific preference data; LLMs show verbosity bias and miss concise summarization criteria",
            "qualitative_findings": "Output-based methods with large LLMs achieve the best human alignment, but performance varies by task: high in translation (near-human), similar to conventional metrics in generation, and poor in summarization; inference strategies give marginal improvements for individual scoring but larger (yet insufficient) gains in pairwise comparisons; embedding- and probability-based methods both have limited applicability and lower human alignment.",
            "advantages_of_llm_judge": "When effective (task-dependent), LLM judges remove the need for reference answers, can provide explanations, and can scale evaluation faster than human raters.",
            "experimental_setting": "Meta-evaluation across 450 responses (50 instructions × 3 tasks × 3 LLMs per instruction), correlations computed per method per task; pairwise comparisons on 900 pairs; methods included embedding-based (BERTScore, MoverScore), probability-based (GPTScore, FFLM), output-based (Vanilla, G-Eval, BatchEval), and SFT models (Auto-J, Prometheus).",
            "uuid": "e7861.6",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "FailureCases-Examples",
            "name_full": "Representative failure modes and case-study examples",
            "brief_description": "Concrete qualitative failure examples observed: (1) translation that fails to preserve internal seeding semantics but is partially recognized by LLM judges; (2) summarization where GPT-4o rewards verbosity with a high score despite violating concision requirement.",
            "citation_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "mention_or_use": "use",
            "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
            "evaluation_task": "Code Translation (example) and Code Summarization (example)",
            "dataset_name": "Selected sample cases from CodeTransOcean (translation) and CodeXGLUE (summarization)",
            "judge_model_name": "GPT-4o (used in case-study explanations)",
            "judge_model_details": "GPT-4o provided textual explanations and final scores in output-based (Vanilla) mode; examples highlighted subtle misses and verbosity bias",
            "human_evaluator_type": "Two domain-expert human evaluators (ground truth)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "verbosity bias (prefers detailed step-by-step explanations, rewarding non-concise summaries); misses subtle functional differences (e.g., seeding/range differences in translated random-number code); inconsistent extraction of intended semantics",
            "qualitative_findings": "Case: translation where Python replacement used stdlib random without seeding — GPT-4o identified discrepancy and gave a moderate score but overlooked some subtle numeric-range implications; Case: summarization where GPT-4o awarded 5/5 to an overly detailed summary, revealing a verbosity bias and misinterpretation of summarization criteria vs human preference.",
            "advantages_of_llm_judge": "Explanations produced by LLMs can surface their judgment rationale, aiding debugging of judge behavior.",
            "experimental_setting": "Qualitative case-study: authors inspected explanations generated by GPT-4o under Vanilla prompting for specific failing and passing examples and compared them to human scores and criteria (illustrative examples included in paper).",
            "uuid": "e7861.7",
            "source_info": {
                "paper_title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BatchEval: Towards Human-like Text Evaluation",
            "rating": 2,
            "sanitized_title": "batcheval_towards_humanlike_text_evaluation"
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "GPTScore: Evaluate as You Desire",
            "rating": 1,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "Generative Judge for Evaluating Alignment",
            "rating": 1,
            "sanitized_title": "generative_judge_for_evaluating_alignment"
        },
        {
            "paper_title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
            "rating": 1,
            "sanitized_title": "pandalm_an_automatic_evaluation_benchmark_for_llm_instruction_tuning_optimization"
        },
        {
            "paper_title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
            "rating": 1,
            "sanitized_title": "prometheus_2_an_open_source_language_model_specialized_in_evaluating_other_language_models"
        }
    ],
    "cost": 0.0192015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering
21 Apr 2025</p>
<p>Ruiqi Wang 0009-0000-9336-0598
Jiyu Guo 
Guodong Fan guodong.fan@126.com 
Cuiyun Gao gaocuiyun@hit.edu.cn 0000-0003-4774-2434
Chun Yong Chong chunyong@ieee.org 
Xin Xia xin.xia@acm.org </p>
<p>Harbin Institute of Technology
ShenzhenChina</p>
<p>Harbin Institute of Technology
ShenzhenChina</p>
<p>CUIYUN GAO *
Harbin Institute of Technology
ShenzhenChina</p>
<p>Harbin Institute of Technology
ShenzhenChina</p>
<p>CHUN YONG CHONG
Monash University Malaysia
Malaysia</p>
<p>Harbin Institute of Technology
Shenzhen, ShenzhenChina</p>
<p>hit.edu.cn; Jiyu Guo
China</p>
<p>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering
21 Apr 2025854D7608A7645440DA91DA792459637210.1145/3728963arXiv:2502.06193v3[cs.SE]large language modelsmodel evaluationhuman preference
XIN XIA, Zhejiang University, ChinaRecently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks.However, assessing the quality of these LLM-generated code and text remains challenging.The commonly used Pass@ metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLMgenerated text.Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny.In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge.These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers.Nevertheless, their exact human alignment in SE tasks remains unexplored.In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments.We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation.After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response.Finally, we compare the scores generated by these methods with human evaluation.The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92.Such outputbased methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns.Finally, we provide insights and implications, concluding that current state-of-the-art LLM-as-a-judge methods can potentially replace human evaluations in certain SE tasks.CCS Concepts: • Software and its engineering; • Computing methodologies → Artificial intelligence;</p>
<p>Introduction</p>
<p>Since BERT [6] and GPT [36], pre-trained language models (PLMs) have been widely used in various natural language processing (NLP) tasks, such as machine translation and text summarization.With the scaling of PLM parameters, the concept of large language models (LLMs) has been proposed.Featuring up to hundreds of billions of parameters, LLMs emerge new capabilities absent on smaller models [49], beyond solving simple linguistic tasks.These capabilities include, but are not limited to, instruction following and multi-step reasoning, enabling LLMs to simulate human experts and achieve state-of-the-art performance in certain domains.Software engineering (SE) is one of the specialized domains that benefits from this trend.Many researchers and companies either emphasize their LLMs' strong coding performance [33,37], or develop specialized code LLMs.For instance, DeepSeek-Coder-V2 [5] correctly generates code for 75.3% instructions in HumanEval [4] and MBPP [1] with 236B parameters, second only to GPT-4o.Qwen2.5-Coder[18] achieves 88.4% Pass@1 on HumanEval with merely 7B parameters.</p>
<p>However, there has been limited progress in evaluating LLM-generated content for SE.The commonly used Pass@ metric executes the first  generated code snippets on human-curated unit tests.While Pass@ evaluates the code's functional correctness accurately, it has several limitations, such as requiring comprehensive unit tests and manual configuration of test environments.What is more, Pass@ is unable to evaluate code from non-functional aspects, such as readability and adherence to good practice, nor can it be used to judge text-generating SE tasks like code summarization and code review [17].Therefore, some SE datasets [9,55] resort to use conventional metrics such as BLEU [34] and CodeBLEU [38], which also have downsides like inability to perform multi-aspect evaluation and requiring human-annotated reference answers.These metrics also focus on lexical rather than semantic similarity, making the evaluation results questionable.</p>
<p>Meanwhile, NLP researchers attempt to apply LLMs to evaluate the quality of LLM-generated content, known as LLM-as-a-judge [60].While human effort remains reliable for evaluation and for curating the reference answers in datasets, it is both slow and expensive, defeating the purpose of automatic evaluation.Therefore, researchers prompt or train LLMs to align with human preference, as an attempt to replace human evaluators.Since both code and text can be viewed as sequences of tokens, LLM-as-a-judge methods can be potentially adopted on SE tasks.Unfortunately, current meta-evaluation benchmarks feature a limited number of simple coding tasks as they mostly target NLP tasks.The lack of test samples and the insufficient task difficulty create a gap between benchmarking on existing datasets and real-world SE scenarios, where the instructions, code, and responses are usually more complex and varied.</p>
<p>To bridge the gap, we conduct an empirical study to apply a range of LLM-as-a-judge methods on realistic SE datasets.Specifically, we select a task for each of the three input-output type combinations, and a recent representative dataset for each task: CodeTransOcean [55] for Code Translation (Code-Code), ComplexCodeEval [9] for Code Generation (Text-Code), and CodeXGLUE [32] for Code Summarization (Code-Text).Their corresponding papers only adopt conventional metrics like Exact Match (EM), BLEU, and CodeBLEU.We randomly sample 50 instructions from each dataset, and three out of 12 code LLMs to generate responses for each instruction.For each response, we manually assign a score indicating its quality, resulting in a dataset of 450 samples of (instruction, response, score) triplets in total.Then we perform meta-evaluation of different types of LLM-as-a-judge methods by calculating their score alignment with human scores, to validate whether their judgments match human preference in real-world scenarios.</p>
<p>We design the following three research questions (RQs):</p>
<p>• RQ1: Which LLM-as-a-judge method aligns with human preference better, and do they outperform conventional metrics?</p>
<p>We aim to assess whether various LLM-as-a-judge methods can replace human evaluators due to high human alignment and superior performance to conventional metrics.We select seven methods across embedding-based, probability-based, and output-based categories, along with two LLMs fine-tuned specifically for NLP evaluation along with their base model, and conventional metrics such as BLEU.We compute the correlations between human scores and scores from these methods to indicate how well they align with human preference on the selected SE tasks.• RQ2: What are the characteristics of LLM scores, more specifically their alignments with one another and score distributions?</p>
<p>We aim to characterize the score distributions from LLM-as-a-judge methods with their distributions and correlations.Specifically, we measure the correlations among all methods, to determine if similar methods yield similar results, and to assess whether they actually mimic human evaluators beyond merely measuring lexical similarity.We also analyze the score distribution of each method to investigate their ability to generate varied scores.• RQ3: How do LLMs perform when prompted to make pairwise comparisons instead of individual scoring?Comparing two responses is also a common choice for LLM-as-a-judge methods, with some studies claiming its superiority over scoring individual responses [28].We conduct similar experiments to evaluate the performance of these methods when LLMs are instead prompted to select a better response from two, or declare a tie.Since embedding-based and probabilitybased methods cannot perform this ternary classification without scoring each response first, we focus solely on output-based methods in this RQ.</p>
<p>Through answering the RQs, we conclude that:</p>
<p>• The human alignments of studied methods heavily depend on the SE tasks.Among them, output-based methods with large LLMs perform best, achieving near-human performance in code translation and generation.• Similar methods yield similar score distributions, most of which differ from those of conventional metrics.The best human-aligning methods demonstrate more balanced and human-like distributions.• Studied methods fail to deliver accurate and consistent comparison results.Output-based methods with large LLMs still provide the highest accuracy, but often yield inconsistent results after swapping the positions of two responses.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• Our work serves as the first empirical study to investigate applying LLM-as-a-judge methods specifically to SE tasks, with much more difficult code-specific instructions and responses compared to previous studies.• We manually curate a meta-evaluation dataset based on three existing SE datasets for different tasks, to evaluate human alignment of LLM-as-a-judge methods.• We explore how different LLM-as-a-judge methods prefer to score responses, and discuss the findings and possible implications for their future studies and applications in SE.</p>
<p>The rest of the paper is organized as follows: Section 2 introduces research relevant to code LLMs, SE task evaluation, and notable LLM-as-a-judge methods.Section 3 offers more details in different categories of LLM-as-a-judge methods.Section 4 presents the overall study design.Section 5 records the experimental results and analyzes our findings.Section 6 analyzes score explanations as a case study and possible future directions based on our findings.Section 7 concludes the paper.</p>
<p>Related Work</p>
<p>Code LLMs for SE</p>
<p>LLMs are large-scale PLMs.Some of them are instruction-tuned to follow instructions in human language.In this paper, we do not distinguish between PLMs and LLMs, and use LLMs to refer to pre-trained Transformers [44] in general.</p>
<p>While many general-purpose LLMs demonstrate satisfying performance on SE tasks, especially code generation, there are many LLMs pre-trained specifically for code-related tasks.CodeBERT [10] is one of the earliest attempts to pre-train a Transformer on both code and text data.It is an encoder-only model with 125M parameters, pre-trained on over 8M datapoints from CodeSearchNet [19].CodeT5 [47] is an encoder-decoder Transformer with up to 770M parameters, pre-trained with a denoising sequence-to-sequence objective on the same dataset.UniXcoder [14] supports encoder-only, decoder-only, and encoder-decoder modes, allowing abstract syntax trees (ASTs) as input after transforming ASTs into sequences.</p>
<p>Recently, larger decoder-only LLMs have been increasingly popular in generation tasks.Codex [4] is a series of GPT-based LLMs with up to 12B parameters, achieving a Pass@1 score of 28.81% on HumanEval.CodeLlama [39] is another LLM family with up to 70B parameters from Meta AI, trained from Llama 2 [43] to follow human instructions.DeepSeek-Coder [15] is a family of LLMs with up to 33B parameters, supporting both normal generation and fill-in-the-middle (FIM).Its successor, DeepSeek-Coder-V2 [5], is a mixture-of-experts (MoE) LLM with 16B or 236B parameters, claiming to have GPT-4 [33] level performance at a Pass@1 score of 90.2% on HumanEval.</p>
<p>SE Benchmarks and Metrics</p>
<p>Many SE benchmarks focus solely on code generation, where LLMs generate code for the given requirements and function signatures.HumanEval [4] is one of the most adopted code generation benchmarks, featuring 164 human-curated Python problems.It uses Pass@ as the evaluation metric.MBPP [1] is another popular benchmark with 974 Python problems, aiming at entrylevel development.APPS [16] is a much larger Python benchmark with 10000 problems, ranging from being solvable in one-line to presenting substantial challenges in algorithms.ClassEval [7] challenges LLMs with 100 class-level code generation problems in Python, and measure class-level and method-level Pass@.Some benchmarks target other SE tasks.CodeReviewer [27] aims at three tasks in the code review process: commit quality estimation, reviewer comment generation, and code editing.CodeXGLUE [32] supports 10 SE tasks such as code summarization and code search.ComplexCodeEval [9] collects code from influential GitHub repositories for 4 tasks such as code generation and unit test generation.These benchmarks all evaluate responses with conventional metrics including Exact Match, Edit Similarity, BLEU, and CodeBLEU instead of Pass@, even for code-generating tasks.CRUXEval [13] evaluates LLMs from other aspects such as code understanding and execution with 800 short Python functions for input or output predictions.It requires LLMs to output assert statements to obtain Pass@ scores.</p>
<p>However, limited efforts are made to curate meta-evaluation benchmarks to test evaluation metrics, as most datasets only contain instructions and reference answers, without responses of different quality or human-annotated scores.NoFunEval [41] designs six evaluation aspects, including functional correctness and non-functional aspects like latency and maintainability.It tests whether LLMs can improve code based on a specific aspect or select the better of two code snippets from that perspective.CodeUltraFeedback [52] evaluates LLMs' alignment with human evaluation from five non-functional code aspects like instruction following and coding style.2.3 LLM-as-a-Judge in NLP 2.3.1 Embedding-Based Methods.Some researchers obtain contextual token representations of the response and reference answer using encoder-only LLMs, and compute pairwise similarity to obtain the score.BERTScore [58] calculates Recall, Precision and F 1 score based on token representations obtained from BERT [6].It also applies inverse document frequencies (IDFs) to reduce the weight of overly common and thus less essential tokens.MoverScore [59] constructs a transportation cost matrix based on token representations and computes Word Mover's Distance [23].CodeBERTScore [62] is a code-specific adaptation of BERTScore with CodeBERT, approximating functional correctness and human scores with F 3 and F 1 scores respectively.While these methods match contextual embeddings instead of n-grams, unlike many conventional metrics, they still measure how a response resembles the reference answer.</p>
<p>2.3.2Probability-Based Methods.Since more LLMs come with decoders, it becomes possible to use generating probabilities for evaluation.BARTScore [57] assumes that BART [25] is more likely to generate a higher-quality response.It uses the probability of BART generating a given response as the score.GPTScore [11] applies a similar approach with 19 LLMs of sizes from 80M to 175B, supporting both reference-free and reference-based evaluation from multiple aspects.FFLM [20] is a reference-free method designed to evaluate the faithfulness of summaries.It calculates the probabilities of generating the summary with and without the original text as posterior and prior probabilities respectively.FFLM assumes that a faithful summary has higher posterior than prior probability, and calculates their difference as the score.</p>
<p>2.3.3</p>
<p>Output-Based Methods.While the above methods usually align with human evaluation better than conventional metrics, they do not explain their scores or support certain closed-source LLMs that do not provide probabilities or representations.Output-based methods prompt LLMs to output the judgments, and do not require access to their internal implementations.G-Eval [31] utilizes Chain-of-Thought (CoT) [50] to request evaluation steps, samples multiple scores and then averages them as the final score.ChatEval [3] assigns different personas to several LLM agents, asking them to discuss and select a better response from two.</p>
<p>Some researchers construct training sets to fine-tune LLMs instead of designing prompting or inference strategies.InstructScore [54] is fine-tuned on GPT-4-synthesized data to generate error reports of text from various domains.PandaLM [48] is fine-tuned on pairwise comparison results and reference answers generated by GPT-3.5, aiming at addressing subjective aspects including conciseness and clarity.X-Eval [30] has an extra training stage to learn the connections between fine-grained evaluation aspects, allowing evaluating from aspects not seen during training.</p>
<p>However, these methods have not been tested on a sufficient number of challenging SE samples, leaving it unclear whether they achieve reliable human alignment for SE applications.</p>
<p>LLM-as-a-Judge Framework Overview</p>
<p>In this section, we offer an overview of existing LLM-as-a-judge methods.As seen in Fig. 1, we categorize these methods based on the types of LLM features used1 , including embedding-based, probability-based, and output-based methods.We denote the instruction (source) as  =   • Probability-based: The LLM receives an input-output pair (, ), and returns the conditional log-probability of generating , i.e. log 
(𝑜𝑢𝑡 |𝑖𝑛) = 1 |𝑜𝑢𝑡 | |𝑜𝑢𝑡 |
=1 log  (  |,  &lt; ).Typical (, ) combinations include (, ), (  , ), (,   ), and (, ), where  means no input is provided.We then score  with these log-probabilities.Additional content may be present in the prompt, such as evaluation aspects like clarity.</p>
<p>• Output-based: These methods first craft a prompt  with  and .Depending on the design,  may also feature   , evaluation aspects and criteria, and evaluation steps.After obtaining the judgment  = LLM(), we extract the final score from .Many prompting and inference strategies can also be applied, such as multi-agent and repeated sampling, where multiple scores are often combined using methods like a majority vote or averaging.LLMs can also be fine-tuned as specialized judges, usually applied with a single inference pass and no additional strategies.State-of-the-art LLMs like GPT-4 are often used to generate reference judgments for training.In this paper, we discuss the performance of these LLMs, instead of focusing on the detailed training process.</p>
<p>Unlike embedding-based and probability-based methods, which usually have scoring ranges of [0, 1] and (−∞, 0) (or (−∞, ∞)) respectively without rescaling, most output-based methods require LLMs to score on a scale of 1 to 5 or 1 to 10.They can also compare two responses and decide the better one or declare a tie.In our study, we investigate individual scoring in RQ1 and RQ2, and pairwise comparison in RQ3.</p>
<p>Study Design</p>
<p>In this section, we elaborate on the details of our study design.In our study, we focus on leveraging different types of LLM-as-a-judge methods to evaluate the responses of three SE tasks.We collect instructions and generate responses from representative datasets, then perform human and LLM evaluation on these responses, and analyze their correlations.</p>
<p>Proc.ACM Softw.Eng., Vol. 2, No. ISSTA, Article ISSTA086.Publication date: July 2025.</p>
<p>Can LLMs Replace Human Evaluators?An Empirical Study of LLM-as-a-Judge in Software Engineering ISSTA086:7</p>
<p>Datasets and Preprocessing</p>
<p>Instruction Collection.</p>
<p>To ensure the difficulty of instructions and to approximate real-world development scenarios, we collect instructions from a recent dataset for each of the three widelystudied SE tasks for our empirical evaluation:</p>
<p>• Code Translation is a code-to-code task demanding translating code between two languages while preserving the functionality.It challenges LLMs' skills to understand syntax and library usages in both languages, and to choose replacements when certain functionalities are unavailable in the target language.• Code Summarization is a code-to-text task involving generating a concise and fluent description of a given code snippet.It challenges LLMs' abilities to abstract the code, leaving only critical information about core functionality rather than explaining step-by-step.• Code Generation is a text-to-code task requiring generating a function based on a natural language description and the signature.It tests LLMs' capabilities to breakdown the functional requirement into steps, and to utilize provided dependencies.</p>
<p>We select the MultilingualTrans2 subset of CodeTransOcean [55] for code translation.Code-TransOcean contains three translation subsets for different purposes, with the MultilingualTrans subset covering eight popular languages with 7545 samples.Compared to previous benchmarks, CodeTransOcean offers more pairs of programming languages of longer code, with the average length of test sets reaching 491 tokens in MultilingualTrans, as opposed to 58 tokens in CodeTrans featured in CodeXGLUE, which only supports Java and C#.CodeTransOcean evaluates translations with conventional metrics such as Exact Match, BLEU, and CodeBLEU, rather than execution-based metrics like Pass@, since they require constructing unit tests and testing environments.</p>
<p>We select the code-text subset of CodeXGLUE [32] for code summarization, which is a filtered version of CodeSearchNet3 [19].Initially developed for code search, i.e. retrieving relevant code based on natural language queries, CodeSearchNet contains two million code snippets in six programming languages accompanied by docstrings.These docstrings come from the associated function documentation and serve as summaries.CodeXGLUE removes samples with syntactically incorrect code, or docstrings that are either non-English, overly lengthy, or too short.After filtering, 14918 Python samples and 10955 Java samples remain, along with samples in four other programming languages.CodeXGLUE uses BLEU to evaluate generated summaries.</p>
<p>We select ComplexCodeEval4 [9] for code generation.ComplexCodeEval is a benchmark with 3897 and 7184 Java and Python samples respectively, supporting four tasks: code generation, code completion, unit test generation, and API recommendation.Compared to previous benchmarks, ComplexCodeEval provides comprehensive supplemental material for each code snippet, including functional dependencies, timestamps, and unit tests.It expects LLMs to learn project-specific dependencies beyond standard library or popular third-party APIs.ComplexCodeEval evaluates code-generating tasks with conventional metrics such as Edit Similarity, BLEU, and CodeBLEU.</p>
<p>When training, validation, and test sets are available, we only adopt the test set for our evaluation.To ensure the accuracy of manual evaluation, we limit the programming languages to Java, Python, C, and C++ according to the human evaluators' expertise.Following previous work's [22,45] context length of 4096 tokens, we also apply length limits of 1536, 1536, and 1024 tokens 5 for instructions, responses 6 , and output-based judgments respectively, removing samples with lengthy  [51] UIUC &amp; THU 6.7B 2023.12Codestral-v0.1 7Mistral AI 22B 2024.5 DeepSeek-Coder-V2-Lite [5] DeepSeek AI 16B 2024.6 CodeGeeX4-ALL [61] Zhipu AI &amp; THU 9.4B 2024.7 Qwen2.5-Coder[18] Alibaba 1.5/7B 2024.9  1 from Hugging Face [53].We generate responses using these LLMs with vLLM [24] on an Ubuntu 20.04 server with two Intel Xeon Platinum 8276L CPUs, four NVIDIA A100-40GB GPUs, and 256 GB RAM.For each instruction, we randomly select three LLMs to respond, yielding three responses , , .For pairwise comparisons, we create three response pairs (, ), (, ), (, ), and another three pairs (, ), (, ), (, ) in order to check if studied methods yield consistent judgment after reversing the order within a response pair.Thus, we obtain 150 responses and 300 response pairs per task, resulting in 450 responses and 900 pairs in total.</p>
<p>As part of the prompt, contextual information in Table 2 is provided for LLMs.The full prompts are available in our repository [46].LLMs are permitted to generate at most 3072 tokens, two times the maximum reference answer length, to minimize the need for truncation.</p>
<p>After preliminary experiments, we discover that many reference summaries in CodeXGLUE are in fact incorrect.Therefore, we require the reference summary to at least have 15 tokens, reselect the instructions, and manually examine each instruction.We also find that in code generation, selected LLMs struggle to generate interpretable code because they cannot use dependencies effectively, as the only available dependency information in ComplexCodeEval is their names, which makes human evaluation almost impossible to yield meaningful scores.Consequently, we reselect the instructions with at most five dependencies to reduce difficulty, and augment the dependency information with GPT-4o 8 , prompting it to extract the signature from the reference answer 9 and generate a short description for each dependency.We manually examine the descriptions to ensure that no other information about the reference answers is included.Responses are generated with the updated dependency information as well as other contextual information.</p>
<p>Manual Evaluation.</p>
<p>For manual evaluation, we design two evaluation aspects per task to guide human evaluators, enabling more fine-grained assessment without overwhelming evaluators with too many aspects and complicated criteria.The first aspect assesses the response's alignment with the instruction, e.g.Consistency with Code for summarization, requiring the summary to capture the code's core functionality.The second aspect judges the response's intrinsic quality, e.g.Readability &amp; Idiomatic Usage for translation, demanding the responded code to be both readable and follow common coding styles in the target language.We also curate the criteria for each integer score ranging from 1 to 5 for both aspects.In general, a 5-point response is near perfect, a 4-or 3-point response contains minor or major issues but still makes sense, and a 2-or 1-point response is practically useless.Below, we show an example of Readability aspect and its corresponding criteria for code summarization as an example, while the remaining aspects and criteria can be found in our repository:</p>
<p>Readability: How clear, concise, and fluent is the summary in describing the code's function?-5/5: Extremely clear, concise, and well-structured; very easy to understand.-4/5: Mostly clear and concise, with minor readability issues.</p>
<p>-3/5: Understandable but may contain some unclear or awkward phrasing.</p>
<p>-2/5: Hard to follow due to unclear language or poor structure.</p>
<p>-1/5: Very confusing, with significant language or structural issues.</p>
<p>Two human evaluators with expertise in the chosen programming languages are involved in judging each of the 450 responses.During manual evaluation, we provide the corresponding instruction and the reference answer along with the response to be evaluated.Each evaluator is required to score both aspects before assigning an overall score, which is not necessarily the average of the former.The final human score for each response is the average of overall scores from two evaluators 10 .For pairwise comparison, we calculate the absolute difference between the final human scores of two responses in a pair, declaring a tie when the difference is smaller than 0.5 11 , or deciding the higher-scored response is better otherwise.</p>
<p>Selected Methods</p>
<p>Conventional Metrics.</p>
<p>We choose five popular conventional metrics, each requiring the response  and the reference answer   but not the instruction.We verify if these metrics align better or worse with human evaluation compared to LLM-as-a-judge methods.For details about Recall, Precision, and F  scores, please refer to their original papers.</p>
<p>BLEU [34] calculates modified n-gram precision ( = 1, 2, 3, 4) for  and   , and applies a brevity penalty to penalize overly short responses.</p>
<p>ROUGE-L [29] measures the length of the longest common subsequence LCS between  and   .It computes the F 1 score based on LCS.</p>
<p>METEOR [2] matches tokens in  and   , and computes the F 3 score based on the number of matched tokens.It also penalizes fragmented alignment by counting the number of contiguous match chunks in .</p>
<p>ChrF++ [35] computes F 2 scores using character n-grams (up to 6-grams) and token n-grams (up to 2-grams).The average character F 2 score and the average token F 2 score are then averaged to produce the final score.</p>
<p>CrystalBLEU [8] is specifically designed to measure code similarity.It removes the most common n-grams in a corpus from  and   , as these trivial n-grams can obscure meaningful differences between them, before calculating BLEU score.For each task, we use the test set from its corresponding dataset as the corpus, including all instructions and reference answers.</p>
<p>We implement the first four methods with Hugging Face Evaluate, and the last with the Crystal-BLEU package.For methods with replaceable tokenizers, we substitute them with OpenAI Tiktoken with o200k_base vocabulary because the built-in tokenizers are usually not designed for code.</p>
<p>4.2.2</p>
<p>Embedding-Based Methods.We choose two methods based on embedding, i.e. token representations of the response  and the reference answer   .We use UniXcoder [14] in place of BERT or other non-code LLMs as our encoder, due to its ability to process both code and text.</p>
<p>BERTScore [58] calculates pairwise token similarity between  and   with token representations, and obtains the average Recall and Precision, which are combined into the F 1 score as the final score.BERTScore also applies inverse document frequencies (IDFs) as token weights.</p>
<p>MoverScore [59] proposes to use Word Mover's Distance [23], measuring semantic dissimilarity as the minimum cost flow between n-gram representations, which is the IDF-weighted average of token representations.For each , it constructs a cost matrix for each n-gram in , and flow requirements based on IDF.The final score is the minimum cost to establish such a flow.</p>
<p>4.2.3</p>
<p>Probability-Based Methods.We select two probability-based methods.These methods may take at least two of the following as input: instruction , response , and reference   , plus supplementary information like evaluation aspects 12 .We use davinci-002 here, since later OpenAI models only return probabilities of newly generated tokens instead of prompt tokens.</p>
<p>GPTScore [11] simply uses the sequence log probability log  ( |, ) as the score according to their paper, which is the average of all token log probabilities.However, their code instead uses the harmonic mean of log  ( |  , ) and log  (  |, ).To mitigate this difference, we additionally include  in both conditions, i.e. using log  ( |  , , ) and log  (  |, , ).</p>
<p>FFLM [20] is a reference-free metric that obtains both the prior probability  () and the posterior probability  ( |).It claims that high-loss (low-probability) tokens contribute more to low-quality content, thus assigning a higher weight to them.FFLM also introduces the prefix probability  ( | : ) by prepending  to , assuming that the prefix increases the generating probability if  is inconsistent with .These three probabilities are fused into the final score.</p>
<p>4.2.4</p>
<p>Output-Based Methods.We select two methods: G-Eval and BatchEval, which apply different inference strategies, in addition to a control group (Vanilla) with no strategies applied, to assess if these strategies improve alignment with human evaluation for general-purpose LLMs.Unless otherwise stated, we use GPT-4o for these methods.</p>
<p>We also include a supervised fine-tuning (SFT) group, with two LLMs fine-tuned for NLP evaluation, along with their base LLMs without fine-tuning, to determine if fine-tuning for NLP evaluation also enhances human alignment in SE evaluation.</p>
<p>We provide only the instruction , response , and evaluation aspects 13 in the prompt.For the detailed prompts, please refer to our repository.Note that these methods can also perform pairwise comparison, where we include both responses in the prompt.</p>
<p>Vanilla performs inference once with greedy decoding (temperature set to 0), where LLMs score each aspect first before assigning the final score.For pairwise comparison, LLMs compare on each aspect and then make the final decision.We use DeepSeek-Coder-V2-Lite locally, and DeepSeek-V2.5 and GPT-4o via API.[31] requires the LLM to generate the evaluation steps first and embeds it into the prompt, followed by 20 inference passes with a high temperature of 1.0 and averaging the scores.Following their practice, we prompt the LLM to return the score first with a limit of 20 generated tokens.Judgments without a score are discarded.For pairwise comparison, we consider comparison results as scores of 1 or -1 when one response is better, or 0 for a tie.If the absolute value of the average score is less than 0.7 14 , we declare a draw, otherwise considering one response better.</p>
<p>G-Eval</p>
<p>BatchEval [56] performs multi-round scoring.In each round, it first batches all responses and then scores each batch in one inference pass.During batching, it diversifies the scores of responses in each batch, so that the LLM can learn an unbiased score distribution for more accurate scoring.We follow their practice by setting temperature to 0.2, batch size to 10, and number of rounds to 5.</p>
<p>SFT involves two LLMs fine-tuned for NLP evaluation, Auto-J [26] and Prometheus-v2-BGB-8x7B [22], as well as their base LLMs Llama2-13B-Chat [43] and Mixtral-8x7B-Instruct [21].We apply the default prompt template of each judge LLM to itself and its base LLM, and exclude the evaluation aspects for Auto-J and Llama-2-13B-Chat since Auto-J's template does not provide a place for aspects.We perform greedy decoding locally with temperature 0.</p>
<p>We only consider the final verdict (score or comparison result) in our meta-evaluation and discard the explanations.For verdict extraction, we use G-Eval's code for itself, and, for all other methods in this category, we set several rules to match with regular expressions, such as "Overall: X" and "[[X]]" where X is the non-negative final score, or comparison result "First", "Second", or "Draw".If no valid verdict is found or the extracted score exceeds 10, which we consider invalid, we assign a score of -1 or a comparison result as draw as a penalty.</p>
<p>Meta-Evaluation</p>
<p>Meta-evaluation refers to the process of evaluating different evaluation metrics.For the default method of individual scoring, we meta-evaluate the metrics via their correlation with human scores, including Spearman's , Pearson correlation coefficient , and Kendall's .For pairwise comparison in RQ3, we compute the Accuracy of LLM-generated labels, in addition to the Agreement which checks if an LLM makes the same judgment when two responses in the prompt swap their positions.</p>
<p>For the ease of reading, all correlation coefficients, Accuracies, and Agreements in this paper are multiplied by 100.We also check if the -value of each correlation coefficient in RQ1 is smaller than 0.05 to ensure a 95% confidence interval.</p>
<p>Study Results</p>
<p>In this section, we present experimental results and our analysis to answer the research questions.</p>
<p>RQ1: Alignment with Human Scores</p>
<p>We use LLM-as-a-judge methods to score individual responses and evaluate their correlation with human scores.Table 3 presents the alignment between human scores and scores generated by various methods, including both LLM-as-a-judge methods and conventional metrics.We notice that the three types of correlation coefficients display similar trends, and make the following discoveries:</p>
<p>Current LLM-as-a-judge methods lack generalizability, as they demonstrate drastically different performance in different tasks and scenarios.In Code Translation, BatchEval reaches the highest human alignment, offering near-human performance at  = 73.67, = 81.32,and  = 59.80, while G-Eval, DeepSeek-V2.5, and GPT-4o also reach a high correlation of  &gt; 70 or  &gt; 60, greatly outperforming conventional metrics capped at  = 34.23, = 31.30.We attribute this to the characteristic of responses and reference answers: LLMs often copy statements from the original code with subtle language-specific modifications as the response.Meanwhile, although the reference answer maintain unchanged core functionality, its exact implementation and behavior might noticeably differ.This presents a disadvantage for reference-based methods including most nonoutput-based methods and conventional metrics.Output-based methods, however, are designed to work without reference and can utilize LLMs' knowledge of programming languages in evaluation.</p>
<p>On the contrary, LLM-as-a-judge methods struggle to outperform conventional metrics in evaluating code generation outputs and are completely surpassed in evaluating code summarization.For code generation, conventional metrics can reach a mid-high correlation of  = 67.11, = 65.55, and  = 49.66,while DeepSeek-V2.5 is the only LLM outperforming them at  = 66.39, = 68.51,and  = 54.74without any additional inference strategies.This can be attributed to the characteristics of the ComplexCodeEval dataset, which emphasizes the usage of complicated dependencies by filling out the correct arguments and calling them at the right time instead of designing sophisticated algorithms.Therefore, a response-reference comparison at the lexical level can offer an insight of Proc.ACM Softw.Eng., Vol. 2, No. ISSTA, Article ISSTA086.Publication date: July 2025.</p>
<p>Can LLMs Replace Human Evaluators?An Empirical Study of LLM-as-a-Judge in Software Engineering ISSTA086:13 the response's quality, while the LLMs' limited understanding of the dependencies fail to provide benefits in evaluation.With that said, for code generation, LLM-as-a-judge methods with large LLMs like GPT-4o are still applicable, since they display similar performance as conventional metrics but provide the benefits of not requiring reference answers.For code summarization, LLM-as-a-judge techniques are completely defeated by conventional metrics, hardly reaching a score of 30 in any correlation coefficient or even demonstrating a negative correlation with human evaluation.Nonetheless, conventional metrics also fail to deliver satisfying alignment with human evaluation, with ,  &lt; 50 and  &lt; 40.This is potentially due to the fact that many LLMs try to explain the code step-by-step instead of summarizing the core functionality, which is difficult for these LLM-as-a-judge methods to detect.While conventional metrics can assign low scores to these responses, they have trouble handling paraphrasing, which is common in summaries.It is an interesting future direction to explore new metrics that align with humans for code summarization.</p>
<p>Finding 1: Current LLM-as-a-judge methods demonstrate low generalizability in aligning with human evaluation, outperforming conventional metrics in code translation, performing on par with them in code generation, while being outperformed in code summarization.</p>
<p>Inference using large LLMs yields the best human alignment across all tasks, while inference strategies only provide marginal improvement.Embedding-based and probability-based methods underperform output-based methods in most scenarios, capped at  = 34.77,47.35, 29.44 versus the top performance of the latter at  = 81.32,68.51, 26.19, and the top performance of conventional metrics at  = 34.23,65.55, 47.01 in code translation, code generation, and code summarization respectively.Furthermore, embedding-based and probability-based methods require access to internal states, while the API services of many state-of-the-art LLMs only allow access to the final output.Therefore, these methods cannot be applied with such LLMs, limiting their applicability.Based on the low human alignment and limited applicable LLMs, we conclude that embedding-based and probability-based methods are impractical for evaluating SE tasks.</p>
<p>Among the output-based methods, we find that DeepSeek-V2.5 and GPT-4o outperform other LLMs without further training.Although Auto-J and Prometheus 2, trained to match human preference, provide better performance than their base model, with a 5.18% to 16.03% increase in Pearson's , achieving  = 38.92 and  = 40.33 in evaluating code generation respectively, the overall performance is still inferior.This is likely due to the limited number of parameters, as Auto-J and Prometheus 2 only have 13B and 47B parameters.Another possible reason is the misalignment between evaluating NLP tasks during training, and evaluating SE tasks during inference.Though many NLP training datasets contain programming tasks, they may only present common tasks like code generation and fail to present sufficiently challenging instructions.Unfortunately, to the best of our knowledge, no multi-task human preference training sets for SE task evaluation have been curated so far.Hence, we are unable to investigate LLMs fine-tuned on such SE-specific datasets.</p>
<p>Similarly, current inference strategies, when employed to GPT-4o, produce an inadequate performance boost of Δ = 2.21, 6.03, 3.04 at maximum.Despite recent work claiming the effectiveness of scaling inference [42], we found that existing inference strategies for SE evaluation only bring marginal improvement in human alignment.Moreover, they have different downsides: G-Eval forces LLMs to generate the overall score first, restricting the efficacy of the Chain-of-Thought procedure, while greatly increasing inference cost if the full explanations are needed; BatchEval increases the token count, leading to more expensive inference due to multi-round evaluation.Therefore, greedy decoding remains a viable LLM-as-a-judge solution with satisfactory performance and lower requirements of token count, when equipped with colossal state-of-the-art LLMs.  5. Correlation between output-based LLM-as-a-judge methods grouped by the sizes of the LLMs they use.Methods are categorized based on the underlying model size: "Small" (using &lt;50B LLMs), which includes all methods from the SFT group and DeepSeek-Coder-V2-Lite from the Vanilla group, and "Large" (using &gt;100B LLMs), including DeepSeek-V2.5 and GPT-4o, the latter used by G-Eval and BatchEval.Finding 2: Among the LLM-as-a-judge methods studied, output-based methods with large state-of-the-art LLMs perform best, regardless of inference strategies.</p>
<p>LLM Sizes</p>
<p>RQ2: Score Characteristics</p>
<p>We investigate the score characteristics of various LLM-as-a-judge methods.Table 4 shows the maximum correlation between metrics from the same or different categories of LLM-as-a-judge methods, while Fig. 2 displays the score distributions 15 of different methods: (1) for manual evaluation, (2) for conventional metrics, (3) for embedding-based methods, (4) for probability-based methods, (5)(6)(7) for output-based methods without SFT, and (8)(9) for output-based methods with SFT.For each distribution, rather than focusing on the specific shape of the curve, we examine whether it is unimodal and note the peak frequency and the corresponding score.We make the following discoveries: Most non-SFT LLM-as-a-judge methods have low correlations with those from other categories and high correlations with those from the same category.In Table 4, we observe that  other &lt; 50 for all categories, meaning that each category demonstrates a unique distribution of scores instead of resembling others.Conversely,  inner &gt; 60 under most non-SFT circumstances, exhibiting a medium to high level of agreement among similar methods.This phenomenon suggests that the mechanics governing each category may significantly influence their score distributions.In contrast, scores from SFT methods correlate poorly even within the same category, likely due to variations in their base LLMs and fine-tuning datasets.Given the high level of disagreement among current fine-tuned LLMs, we argue that selecting an appropriate fine-tuned LLM is crucial for evaluating under specific SE contexts.Otherwise, it may produce entirely unexpected scores.</p>
<p>Output-based methods using large LLMs tend to align well with each other, whereas those using smaller LLMs exhibit low correlations with other methods.Since output-based methods offer the best human alignment, we further investigate whether LLM size influences  correlations between methods by grouping these LLM-as-a-judge methods into those using large LLMs (&gt;100B) and those using small LLMs (&lt;50B).In Table 5, we observe that methods employing large LLMs achieve high correlations of  &gt; 80 for code translation and  &gt; 65 for code generation with each other.These methods use DeepSeek-V2.5 and GPT-4o, and maintain strong alignment despite the difference in LLMs and inference strategies.In contrast, methods using small LLMs yield  &lt; 50 when compared to methods in the "large" group, and  &lt; 30 within the "small" group.This pattern reflects a performance gap, as the "large" group align substantially better with human evaluations than the "small" group.</p>
<p>Finding 3: As anticipated, methods within the same category generally exhibit high correlations with each other and low correlations with those in different categories.Among output-based methods, those using large LLMs not only align well with human scores but also show strong correlations with each other.</p>
<p>Only embedding-based methods resemble conventional metrics.We discover in Table 4 that  conv = 81.45,79.78, 57.07 for the 3 tasks with embedding-based methods, indicating a high correlation with conventional metrics.As shown in Fig. 2, MoverScore from this category exhibits a distribution similar to that of ChrF++, one of the most human-aligning conventional metrics, as both tend to assign low to medium scores to responses.This similarity is anticipated, given that both metrics are designed to assess the similarity between the response and the reference.While MoverScore leverages contextual token representations beyond simple lexical matching, the underlying principles remain fundamentally aligned.On the other hand, distributions from other categories differ markedly from those of ChrF++ as shown by their low  conv values, further underscoring their limited resemblance to conventional metrics.</p>
<p>The best human-aligning methods closely replicate the distribution of human scores.As shown in Fig. GPT-4o, G-Eval, and DeepSeek-V2.5 in plots (5)(6)( 7) demonstrate the highest alignment with human judgment in plot (1), with similar peak frequencies and corresponding scores between 0.6 and 0.8.GPTScore in plot (4) also shows similar peaks for code translation and code generation, though its scores are less evenly distributed compared to these top methods, as seen in the high peak frequency and reduced variance values  2 .In contrast, the underperforming methods, such as ChrF++ in plot (2) and MoverScore in plot (3), peak at much lower scores, resulting in average scores notably below those of human evaluators.Interestingly, Prometheus 2 in plot (9) shows a comparable peak location and a relatively balanced distribution after fine-tuning on Mixtral outside code summarization, yet this does not correspond to a high human alignment.6 presents the results.</p>
<p>In general, current LLM-as-a-judge methods fail to deliver satisfactory and consistent comparison performance on SE tasks.For code translation, G-Eval and BatchEval reach the highest Accuracy of 64.67 and 65.33, followed by GPT-4o and DeepSeek-V2.5, and all other methods fall below 50 Accuracy.On code generation, even the best-performing methods struggle to achieve 50 Accuracy, while all methods become completely unusable on code summarization.</p>
<p>We also evaluate their consistency by reversing the order of the two responses in the prompt to check if methods yield the same comparison results, measured as Agreement.Table 6 shows that methods with the highest accuracy on code translation and generation yield extremely low Agreement below 25, indicating poor consistency.Meanwhile, the most consistent methods from the SFT category barely outperform random guessing, where each outcome (selecting a better response or declaring a tie) has an equal 1  3 chance.Although unreliable and inconsistent, their comparison Accuracy displays a similar trend as in individual scoring.For the first two tasks, methods applying inference strategies on large LLMs, such as G-Eval and BatchEval with GPT-4o, exhibit the highest Accuracy, followed by DeepSeek-V2.5 and GPT-4o with greedy decoding and no further strategies, though the performance impact of inference strategies is noticeably larger than in individual scoring.For example, BatchEval provides up to +8 Accuracy boost here for GPT-4o compared to +3 in Spearman's  in RQ1 on code translation.Besides, DeepSeek-Coder-V2-Lite, a code LLM with merely 16B parameters, also defeats LLMs fine-tuned to evaluate NLP tasks, but again lags behind large LLMs.</p>
<p>Finding 5: Current LLM-as-a-judge methods exhibit disappointing Accuracy in pairwise comparisons and often yield inconsistent results when the order of two responses is reversed.As with individual scoring, output-based methods using large LLMs achieve the highest Accuracy, yet inference strategies provide a larger performance boost than in individual scoring.However, these strategies do not fully resolve the inconsistency issue.</p>
<p>6 Discussion</p>
<p>Case Study</p>
<p>In Section 5, we only study the human alignment of scores from various LLM-as-a-judge methods.However, output-based methods usually generate explanations for scores as well.Thus, we study two cases of scoring explanations here.Both judgments are generated by GPT-4o from group Vanilla in the output-based category.We present a passing case of code translation on the left side of Fig. 3.In this example, the original C code implements a seeded custom pseudo-random function to generate random integers from 0 to 999999.However, the translated Python code simply invokes a similar function from the standard library without setting the seed, which clearly disregards the original purpose, and produces an incorrect range of [0, 1000000].GPT-4o accurately identifies this discrepancy and highlights the failure to preserve the original intention, aligning with its moderate score of 3/5, although it overlooks the subtle difference in the range of possible random integers.</p>
<p>We also present a failing case of code summarization on the right side of Fig. 3. Here, the responding LLM produces an overly detailed method summary, including excessive specifics such as identifier names and internal procedures, contrary to the instruction to summarize functionality without extensive details.Despite this, GPT-4o assigns a perfect score of 5/5, viewing the entire step-by-step explanation as essential information, while an ideal summary can be as concise as "The method iterates over a directory stream and filters out non-directory files." This case aligns with LLMs' verbosity bias introduced in related work [40].</p>
<h1>include<stdio.h>long long seed; long long random() {return seed = seed * seed / 1000 % 1000000;} int main(){ seed = 675248; for(int i=1;i&lt;=5;i++) printf("%lld\n",random()); return 0; } The given code is a method named <code>iterateOverMFileCollection</code> that iterates over a directory stream of files in a specified directory.It uses a custom filter <code>MyStreamFilter</code> to filter out files that meet certain criteria.For each file in the directory stream, it reads the file's basic attributes and checks if it's not a directory.If it's not a directory, it creates an instance of <code>MFileOS7</code> with the file's path and passes it to a <code>Visitor</code> object for further processing.If an error occurs while reading the file's attributes, it logs the error and skips the file.The method also keeps a count of the number of files processed and prints this count to the console if debug mode is enabled.</h1>
<p>Readability: 5/5 -The summary is extremely clear, concise, and wellstructured.It effectively describes the function of the code without being verbose.</p>
<p>-Language is fluent, and the explanation logically follows the structure of the code, making it very easy to understand.-Each step of the process in the code is outlined in a straightforward manner, contributing to excellent readability.Consistency: 5/5 -The summary perfectly aligns with the code, capturing all key functionalities such as iterating over files, using a custom filter, checking file attributes, and handling errors.-It does not omit important details, such as the creation of <code>MFileOS7</code> objects and passing them to a <code>Visitor</code>, nor does it introduce less critical information.</p>
<p>-The inclusion of the debug mode functionality and error logging ensures a comprehensive representation of the code's purpose.</p>
<p>Overall: 5/5</p>
<p>The summary provides an accurate and clear account of the code, both in terms of readability and consistency, resulting in an excellent overall score.GPT-4o Score: 3</p>
<p>Fig. 3. Case study.The successful case from code translation is on the left while the failing case from code summarization is on the right.</p>
<p>Implications of Findings</p>
<p>For developers: Our findings indicate the potential of LLM-as-a-judge methods to replace human evaluators, to effectively evaluate the quality of LLM-generated content in certain SE tasks and save developers' time on selecting the best LLMs.We further conclude the following insights:</p>
<p>(1) Developers should carefully select LLM-as-a-judge methods, as their performance varies significantly across different categories of methods.</p>
<p>• Output-based methods with large LLMs like GPT-4o or DeepSeek-V2.5 offer the most human-aligning evaluation with proper prompt and inference strategies.• Individual scoring should be preferred over pairwise comparison with current methods.</p>
<p>(2) For different tasks, developers should leverage LLM-as-a-judge methods in diverse ways to exploit their strengths, as their performance is highly task-dependent:</p>
<p>• For code translation and generation, state-of-the-art methods demonstrate mid-to-strong performance and can be used standalone, particularly when reference answers are unavailable or scoring explanations are required.• For code summarization, LLM-as-a-judge methods should not be used directly or alone due to their insufficient alignment with human judgments.However, with carefully designed prompts to mitigate common LLM biases, they can still serve as a valuable complement to conventional metrics.</p>
<p>For researchers: Our study reveals the effectiveness and limitations of LLM-as-a-judge methods in SE tasks and shows some potential future directions, specifically:</p>
<p>(1) Current methods lack generalizability across SE tasks, as evidenced by their task-dependent performance:</p>
<p>• While SFT methods for evaluation exist, their performance on SE tasks is likely limited by the absence of challenging SE-specific data in training sets.Future research could benefit from curating difficult, SE-specific human preference datasets for fine-tuning smaller LLMs.Instructions in these datasets can originate from challenging benchmarks or even complicated real-world scenarios.It is also important to design strategies for state-of-the-art LLMs to generate responses and human-like judgments.• Non-SFT methods use uniform prompt formats and inference strategies across tasks, without task-specific adaptations or utilizing code-specific features or structures.We therefore propose that designing SE-or even task-specific evaluation methods may yield more accurate and robust results than general-purpose evaluation frameworks.(2) There are still gaps to be bridged between LLM and human evaluators:</p>
<p>• During evaluation, LLMs typically rely solely on the predefined evaluation criteria.In contrast, human evaluators can compare multiple responses, implicitly identifying common strengths and weaknesses to streamline the evaluation process.To bridge this gap, researchers could enhance LLM-as-a-judge methods by asking LLMs to summarize insights from previous evaluation sessions.These insights could be included in the prompt to provide additional evaluation context.Multiple responses in the prompt are also valuable for LLMs to make comparisons.• Human evaluators often discuss and reach a consensus, whereas LLM-as-a-judge frameworks typically contain a single LLM instance.To bridge this gap, researchers could develop multi-agent evaluation systems, where multiple LLM instances evaluate responses from different perspectives.This approach would enable more comprehensive and nuanced evaluations, akin to collaborative human judgment.• The underperformance of LLM-as-a-judge in evaluating code summaries reveals a critical misalignment between benchmark task definitions and LLM interpretations.In our experiments, CodeXGLUE expects concise, docstring-style summaries while LLMs default to detailed explanations due to their verbosity bias.To improve evaluation reliability, researchers should mitigate LLMs' implicit, bias-influenced assumptions about the task to ensure they correctly understand task objectives.</p>
<p>Conclusion</p>
<p>In this paper, we empirically investigate the effectiveness of different types of LLM-as-a-judge methods on three SE datasets.We generate and manually score LLM responses, and assess these methods' alignment with human scores.Our results indicate that these methods demonstrate taskdependent performance, ranging from near-human to unusable when scoring individual responses, and generally perform worse in pairwise comparisons.We further analyze score characteristics, discovering that the most human-aligning methods display a balanced human-like distribution.Finally, we discuss key findings and implications for future development and application of LLMas-a-judge in SE evaluation, hoping that these insights can assist future research in this area.</p>
<p>1 ... | | , the response (target) as  =  1 ... | | , and the reference answer as   =  1 ... |  | .• Embedding-based: These methods first obtain token representations of the response and reference answer  () =  ( 1 )... ( | | ) and  (  ) =  ( 1 )... ( |  | ) from the LLM encoder  .We then evaluate  via fusing token-wise cosine similarities    =  (  ) • (  ) ∥  (  ) ∥ ∥  (  ) ∥ .</p>
<p>Functionality:FunctionalityFig. 1 .
1
Fig. 1.Overview of different LLM-as-a-judge methods.</p>
<p>Fig. 2 .
2
Fig. 2. Score distributions of selected metrics.,  2 refer to the means and variances of scores for code translation, code generation, and code summarization.All scores are rescaled into range [0, 1].</p>
<p>Finding 4 :
4
Only embedding-based methods align closely with conventional metrics, while the most human-aligned output-based methods display more balanced score distributions that mirror human scoring patterns.5.3 RQ3: Pairwise Comparison versus Individual ScoringSince embedding-based and probability-based methods can only score individual responses, we only analyze output-based methods for pairwise comparison.Table</p>
<p>the use of the <code>random.randint</code>function is unnecessary and misleading given the context, as it does not replicate the functionality of generating pseudorandom numbers based on a simple seed-manipulation algorithm like the original C code.This deviation affects comprehension…Consistency with Source：2/5The translated code diverges significantly from the original C code's functionality and purpose… However, the translated code directly uses Python's <code>random.randint</code>function… Overall：3/5 …but it fails to preserve the core logic and functionality of the original C code.The use of <code>random.randint</code>fundamentally alters the intended pseudorandom number generation process…</p>
<p>Proc.ACM Softw.Eng., Vol. 2, No. ISSTA, Article ISSTA086.Publication date: July 2025.
Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software EngineeringISSTA086:5</p>
<p>Table 1 .
1
Selected LLMs for response generation, sorted by their release date.
LLM FamilyDeveloperSizeDateCodeLlama-Instruct [39]Meta AI7/13/34B2023.8DeepSeek-Coder [15]DeepSeek AI1.3/6.7/33B 2023.11MagiCoder-S-DS</p>
<p>Table 2 .
2
Contextual information provided for each task in response generation.
TaskContextual Informationinstructions or reference answers. We sample 50 instructions from each filtered dataset, resultingin 150 instructions in total.
4.1.2Response Generation.We deploy 12 recent code LLMs with different sizes from seven families shown in Table</p>
<p>Table 3 .
3
Experimental results for individual scoring.DS2.5 means DeepSeek-V2.5 while DSC2-Lite means DeepSeek-Coder-V2-Lite.The best alignment in each column is marked bold.The best conventional metric alignment and better results in other categories are underlined.Coefficients with  &gt; 0.05 are marked red.BERTScore 27.72 32.49 19.54 41.39 44.74 30.36 21.71 21.89 15.57MoverScore 28.29 26.22 19.99 46.64 47.35 33.66 31.86 29.44 22.82
Method𝜌Translation 𝑅𝜏𝜌Generation 𝑅𝜏𝜌Summarization 𝑅𝜏Conventional MetricsBLEU31.12 28.08 22.43 58.08 55.83 41.90 19.80 24.77 16.78ROUGE-L28.55 28.57 20.29 55.72 57.62 40.81 48.45 47.01 35.47METEOR22.48 31.79 15.98 67.11 65.55 49.66 38.83 40.01 28.27ChrF++31.30 34.23 22.65 64.02 64.92 46.60 47.26 44.65 33.86CrystalBLEU 23.63 25.26 17.43 59.02 56.65 42.88 23.19 24.96 17.24Embedding-basedProbability-basedGPTScore33.53 34.77 25.12 46.65 45.42 35.00 -13.34 -15.04 -9.28FFLM34.03 29.37 25.50 29.31 29.62 21.65 -2.29 -8.71 -1.94Output-based: VanillaDSC2-Lite33.10 46.26 26.56 15.71 28.28 12.50 -17.76 -17.47 -15.25DS2.562.43 70.27 49.48 66.39 68.51 54.74 17.73 18.10 14.14GPT-4o70.67 79.11 57.85 54.70 57.02 43.56 24.52 23.15 19.27Output-based: Inference strategiesG-Eval68.96 77.14 52.90 60.71 63.05 46.36 23.34 26.19 17.18BatchEval73.67 81.32 59.80 59.54 63.04 48.62 22.56 22.46 18.39Output-based: SFTLlama22.611.031.92 23.91 22.89 18.94 -15.61 -15.81 -12.82Auto-J20.99 14.43 17.45 36.53 38.92 29.79 -5.13 -4.92 -4.36Mixtral24.67 34.07 19.52 14.41 25.32 11.18 -3.97 -8.98 -3.39Prometheus 32.42 39.25 26.60 29.03 40.33 23.09 -17.12 -17.14 -14.24</p>
<p>Table 4 .
4
Category-wise correlation. conv ,  other , and  inner are the maximum Spearman's  between the specified category with either conventional metrics, metrics from the other three categories, and other metric(s) from the same category.Coefficients above 50 are underlined, while those above 75 are marked bold. other  inner  conv  other  inner  conv  other  inner Embedding-based 81.45 37.81 74.83 79.78 46.74 84.20 57.07 23.32 66.61 Probability-based 32.18 28.60 31.3063.69 46.74 60.92 32.78 23.32 63.13 Output-based w/o SFT 30.60 35.64 90.64 47.25 39.56 88.25 20.04 48.21 79.43 Output-based w/ SFT 37.14 37.81 24.25 30.96 37.52 27.44 11.21 48.21 23.44
Translation 𝜌 conv Table CategoryGenerationSummarization</p>
<p>Compared TranslationGeneration Summarization  min  max  min  max
𝜌 min𝜌 maxSmall-Small-4.10 24.25 -2.74 27.44 -5.5523.44Large-Large83.04 90.64 68.63 88.25 31.5079.43Small-Large4.16 48.92 11.70 37.84 -16.15 48.21</p>
<p>Table 6 .
6
Experimental results for pairwise comparison, where Acc.means Accuracy and Agr.means agreement.The best result in each column is marked bold.Results higher than 50 are underlined.
MethodTranslation Acc. Agr.Generation Acc. Agr.Summarization Acc. Agr.Random guess 33.33 33.33 33.33 33.33 33.3333.33VanillaDSC2-Lite44.67 36.00 38.67 36.67 30.3332.00DS2.551.00 10.67 48.33 16.67 26.6716.67GPT-4o57.33 13.33 49.33 13.33 25.0016.00Inference strategiesG-Eval64.67 17.33 54.67 15.33 34.3332.67BatchEval65.33 21.33 52.67 24.00 36.3338.00SFTLlama236.00 78.67 34.67 72.67 31.67 56.00Auto-J33.33 52.00 38.33 28.67 23.3316.00Mixtral29.00 48.67 32.00 40.00 32.6748.67Prometheus33.67 35.33 37.67 31.33 42.00 26.00
Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA086. Publication date: July 2025.
Our categorization is inspired by[12]. We merge similar categories based on LLM feature types.Proc. ACM Softw. Eng., Vol.
, No. ISSTA, Article ISSTA086. Publication date: July 2025.
Collected from Rosetta Code, https://rosettacode.org/wiki/Rosetta_Code.
Collected from public GitHub repositories.
Collected from GitHub repositories.
Measured with OpenAI's Tiktoken, https://github.com/openai/tiktoken, with GPT-4o's vocabulary o200k_base.
Here we limit the length of reference answers instead of actual responses generated in the next step. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA086. Publication date: July 2025.
Announced at https://mistral.ai/news/codestral/.
We use the 2024-08-06 version for all experiments. The prompt for augmentation is available in our repository.
The authors of ComplexCodeEval extract dependency names from reference answers as well, and we follow their practice to utilize reference answers.Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA086. Publication date: July 2025.Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering ISSTA086:9
The two human evaluators reach a high level of agreement, achieving Spearman's 𝜌 of (83.07, 75.42, 74.20), Pearson's 𝑅 of (85.86, 79.70, 73.74), and Kendall's 𝜏 of (72.26, 63.40, 62.57) on code translation, generation, and summarization respectively.
The value is chosen so that ties occur for about a third of the response pairs for each task.Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA086. Publication date: July 2025.
Aspects are identical to those in human evaluation.
Aspects are identical to those in human evaluation. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA086. Publication date: July 2025.
The value is chosen so that ties occur for about a third of the response pairs. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA086. Publication date: July 2025.
Frequency estimated using Kernel Density Estimation (KDE). All scores rescaled into range [0, 1]. Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA086. Publication date: July 2025.
Code Translation Original Code Code SummarizationOriginal Code Code Generation Signature, Description, DependenciesData AvailabilityOur source code and data is publicly available at[46].
Program Synthesis with Large Language Models. Jacob Austin, Augustus Odena, Maxwell I Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, Proc. ACM Softw. Eng. 22021. July 2025ISSTA, Article ISSTA086. Publication date</p>
<p>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005. the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005Ann Arbor, Michigan, USAAssociation for Computational Linguistics2005. June 29, 2005</p>
<p>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024. May 7-11, 2024</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Felipe Petroski Such. Joshua Achiam, Vedant Misra, Evan Morikawa, Alec RadfordJan Leike. 2021Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374</p>
<p>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. Deepseek-Ai , Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, Wenfeng Liang, 2024. 2024</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191</p>
<p>Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, CoRR abs/2308.01861ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. 2023. 2023</p>
<p>CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code. Aryaz Eghbali, Michael Pradel, 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022. Rochester, MI, USAACM2022. October 10-14, 20222812</p>
<p>ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code. Jia Feng, Jiachen Liu, Cuiyun Gao, Chun Yong Chong, Chaozheng Wang, Shan Gao, Xin Xia, CoRR abs/2409.102802024. 2024</p>
<p>CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou, Findings of the Association for Computational Linguistics: EMNLP 2020. Findings of ACL. Association for Computational Linguistics2020. 16-20 November 2020. 2020</p>
<p>GPTScore: Evaluate as You Desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 20241NAACL 2024</p>
<p>LLM-based NLG Evaluation: Current Status and Challenges. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan, 10.48550/arXiv.2402.01383CoRR abs/2402.013832024. 2024</p>
<p>CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. Alex Gu, Baptiste Rozière, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, Sida Wang, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024</p>
<p>UniXcoder: Unified Cross-Modal Pre-training for Code Representation. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>DeepSeek-Coder: When the Large Language Model Meets Programming -The Rise of Code Intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, CoRR abs/2401.141962024. 2024</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt, Measuring Coding Challenge Competence With APPS. 2021</p>
<p>ISSTA, Article ISSTA086. Publication date. July 20252</p>
<p>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering. 08621</p>
<p>Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021December 2021</p>
<p>Large Language Models for Software Engineering: A Systematic Literature Review. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C Grundy, Haoyu Wang, CoRR abs/2308.106202023. 2023</p>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, abs/2409.12186Qwen2.5-coder technical report. 2024. 2024</p>
<p>CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, CoRR abs/1909.094362019. 2019</p>
<p>Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model. Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Emma Bou Casas, Florian Hanna, Gianna Bressand, Guillaume Lengyel, Guillaume Bour, Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, CoRR abs/2401.04088Mixtral of Experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024. 2024</p>
<p>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, CoRR abs/2405.015352024. 2024</p>
<p>From Word Embeddings To Document Distances. Matt J Kusner, Yu Sun, Nicholas I Kolkin, Kilian Q Weinberger, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015. the 32nd International Conference on Machine Learning, ICML 2015Lille, France2015. 6-11 July 201537Workshop and Conference Proceedings</p>
<p>Efficient Memory Management for Large Language Model Serving with PagedAttention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023. the 29th Symposium on Operating Systems Principles, SOSP 2023Koblenz, GermanyACM2023. October 23-26, 2023</p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>Generative Judge for Evaluating Alignment. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024. May 7-11, 2024</p>
<p>Automating code review activities by large-scale pre-training. Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022Singapore, SingaporeACM2022. November 14-18, 2022</p>
<p>Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, CoRR abs/2310.014322023. 2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects. Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 20241NAACL 2024</p>
<p>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shengyu Shao Kun Deng, Shujie Fu, Liu, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212021. December 2021</p>
<p>abs/2303.08774ISSTA, Article ISSTA086. 2023. July 20252Technical Report. CoRRPublication date</p>
<p>Bleu: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USA. ACL2002. July 6-12, 2002</p>
<p>chrF++: words helping character n-grams. Maja Popovic, Proceedings of the Second Conference on Machine Translation. the Second Conference on Machine TranslationCopenhagen, DenmarkAssociation for Computational Linguistics2017. 2017. September 7-8, 2017</p>
<p>Improving language understanding by generative pre-training. Alec Radford, 2018. 2018</p>
<p>. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross Mcilroy, Melvin Johnson, Johan Schalkwyk, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, 2024. 2024Eli Collins, Eliza Rutherford, Erica Moreira,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR abs/2403.05530</p>
<p>CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, 2020. 2009. 202010297</p>
<p>Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Bhatt, Aaron Canton-Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, abs/2308.12950Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. 2023</p>
<p>Verbosity Bias in Preference Labeling by Large Language Models. Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto, CoRR abs/2310.100762023. 2023</p>
<p>NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness. Manav Singhal, Tushar Aggarwal, Abhijeet Awasthi, Nagarajan Natarajan, Aditya Kanade, CoRR abs/2401.159632024. 2024</p>
<p>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, CoRR abs/2408.033142024. 2024</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurélien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>Learning Personalized Story Evaluation. Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian, CoRR abs/2310.033042023. 2023</p>
<p>Replication package for paper "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering. Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, Xin Xia, 2024</p>
<p>CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Yue Wang, Weishi Wang, Shafiq R Joty, Steven C H Hoi, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican RepublicAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024. May 7-11, 2024</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 20222022. 2022</p>
<p>ISSTA, Article ISSTA086. Publication date. July 20252</p>
<p>Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering. 08623</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Magicoder: Empowering Code Generation with OSS-Instruct. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024</p>
<p>CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences. Martin Weyssow, Aton Kamanda, Houari A Sahraoui, CoRR abs/2403.090322024. 2024</p>
<p>HuggingFace's Transformers: State-of-the-art Natural Language Processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew, CoRR abs/1910.037712019. 2019</p>
<p>IN-STRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, Lei Li, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation. Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>BatchEval: Towards Human-like Text Evaluation. Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, Yao Hu, Kan Li, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>BARTScore: Evaluating Generated Text as Text Generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. NeurIPS2021. 2021. December 6-14, 2021</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020</p>
<p>MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data MiningLong Beach, CA, USAACM2023. August 6-10, 20232023</p>
<p>CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig, 2024-10-31; accepted 2025-03-31Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
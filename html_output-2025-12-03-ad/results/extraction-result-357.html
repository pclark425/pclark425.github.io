<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-357 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-357</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-357</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-219260893</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2006.01962v2.pdf" target="_blank">Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures</a></p>
                <p><strong>Paper Abstract:</strong> We propose a new long-term declarative memory for Soar that leverages the computational models of analogical reasoning and generalization. We situate our research in interactive task learning (ITL) and embodied language processing (ELP). We demonstrate that the learning methods implemented in the proposed memory can quickly learn a diverse types of novel concepts that are useful in task execution. Our approach has been instantiated in an implemented hybrid AI system AILEEN and evaluated on a simulated robotic domain.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e357.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e357.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AILEEN Concept Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical Concept Memory for AILEEN (SME + SAGE integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic, analogical long-term concept memory that stores exemplars as predicate-calculus facts and incrementally forms probabilistic relational generalizations (contexts) using SME for matching and SAGE for generalization; used to recognize concepts, infer unseen properties, and project next states for action planning in an embodied agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AILEEN Concept Memory (SME+SAGE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A module combining the Structure-Mapping Engine (SME) for analogical matching and SAGE for sequential analogical generalization; it represents scenes and episodic traces as predicate-calculus facts, maintains generalization contexts with per-fact probabilities, and exposes create/store/query/project commands to the Soar agent for grounding language and projecting action outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive Task Learning (ITL) in a simulated tabletop world</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>AILEEN is taught visual, spatial, and action concepts via interactive lessons (inform, verify, react) in a Webots simulated tabletop domain with objects of known shapes/colors; the agent must ground referring expressions, recognize relations (e.g., left-of), and plan/manipulate objects (point, pick-up, place) to achieve relational goals (e.g., move red cylinder right of blue box).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / object manipulation / multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + procedural + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>exemplar demonstrations and grounded lessons (episodic traces), plus perceptual outputs from modules (YOLO visual percepts and QSRLIB qualitative spatial relations)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>interactive indexing (Indexical Model) with explicit commands to concept memory (create/store/query/project), analogical matching via SME, generalization via SAGE; used during verify/react lessons and planning search</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic predicate-calculus facts (scene graphs), qualitative spatial calculi (CDC, RCC8), generalization contexts with per-fact probabilities (SAGE), candidate inferences with Skolem terms (SME) for projected next states</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>generality exam score (5 verify lessons, scored 0/1 each), specificity exam score (negative tests), number of store requests (examples stored), and successful task completion in react lessons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Visual concepts: generality reached perfect (5/5) by lesson ~15 averaged over 10 trials; specificity maintained (no bleed-over) (5/5). Number of store requests decreased to 0 by end of trials. Spatial concepts: rapid learning but generality scores sometimes <5 due to physics/noise in multi-distractor scenes. Action concepts: learned quickly from episodic traces; task demonstration succeeded (agent achieved move goals after 2 successive projections and search steps).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Rapid generalization of visual attributes (colors/shapes) from very few exemplars; learning of qualitative spatial relations (mapped prepositions to CDC/RCC8 relations) using relational representations; projection of procedural steps for actions (candidate inferences indicate holding, relative spatial constraints) enabling multi-step planning; robust incremental accumulation of examples into probabilistic generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance degraded in noisy/distractor scenes where physics caused objects to move from intended configurations (leading to apparent verification failures); the system created separate action concepts for similar spatial variants (e.g., move-left vs move-above) indicating limited composability of learned action concepts; the concept memory remained a separate module (representation mismatch with Soar), limiting real-time integration and requiring engineering work.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No explicit baselines reported (no random or ML baselines provided); comparisons are qualitative against prior Soar/ROSIE EBL approaches in discussion only.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No formal ablation studies reported; paper discusses future integration/engineering improvements but no quantitative component-removal experiments were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Analogical generalization (SME+SAGE) can encode object-relational and spatial knowledge as symbolic predicate facts and probabilistic generalizations from few exemplars and can project procedural next states (with Skolem terms) to guide embodied planning; qualitative spatial calculi (CDC, RCC8) provide a language-level mapping for prepositions, and indexical grounding links language to these symbolic representations enabling planning and execution even under partial observability, though robustness degrades with noisy perceptual dynamics and current implementations treat spatial variants as separate action concepts rather than composable components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e357.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e357.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-Mapping Engine (SME)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analogical matching engine that computes one-to-one correspondences and similarity scores between relational symbolic representations, producing candidate inferences by aligning facts between an example (or generalization) and a scene.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extending sme to handle largescale cognitive modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Structure-Mapping Engine (SME)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Symbolic analogical matcher that takes predicate-calculus style representations and returns correspondences, similarity match scores, and candidate inferences (including Skolem-generated entities) used to infer unseen facts and to align episodic traces with generalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Analogy-driven grounding and projection for ITL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used during query and project operations to match current scene facts or episodic traces against stored generalizations, yielding candidate inferences for concept recognition and next-state predictions to support planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic reasoning / projection for planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial + procedural (via aligned episodic traces)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>symbolic episodic traces and stored generalizations (explicit predicate facts)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>invoked by concept memory via query/project commands during indexical comprehension and react lessons</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>operates over explicit symbolic predicate-calculus representations (scene facts, episodic traces, generalization contexts); outputs candidate inferences with Skolem terms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>internal similarity score (used versus thresholds: assimilation threshold 0.01, match threshold 0.75) and the binary success/failure of indexical grounding or projection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Used as core matching mechanism; matches whose similarity exceeded thresholds produced valid inferences used by AILEEN; quantitative SME accuracy metrics not separately reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Produced correspondences that enabled robust alignment of perceptual predicates (e.g., CVRed) across examples and scenes, enabling inference of concept membership and next-state properties (e.g., held(O5), spatial relations).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Performance contingent on representational alignment and the richness of exemplars; mismatches in noisy scenes or insufficient shared structure reduce similarity scores and block inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared against other matchers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablations isolating SME reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SME provides the mechanism to extract candidate inferences (including temporal/skolemized entities) from matches between scenes/episodic traces and generalizations, enabling symbolic projection of procedural steps and object-relational inferences even when some sensory detail is unavailable (i.e., SME returns inferred facts to help elaborate unseen states).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e357.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e357.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Analogical Generalization Engine (SAGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analogical generalizer that incrementally assimilates matched exemplars into relational generalizations, maintaining per-fact probabilities to capture typicality across examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extending analogical generalization with near-misses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAGE</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Incremental generalization engine that takes SME correspondences and either assimilates new examples into existing generalizations or stores them as separate exemplars, producing generalized relational schemas with probability weights for facts (used as concept definitions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Incremental concept induction for ITL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produces generalization contexts for visual, spatial, and action concepts by assimilating episodic examples (predicate facts) and updating probabilities that specific facts belong to the concept.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>concept learning / generalization</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial + procedural (action schemas)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>episodic exemplars stored from interactive lessons and demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>called when store commands add examples; uses SME correspondence and assimilation threshold to decide to generalize</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>generalization contexts with abstract entities (GenEntFn skolem-like identifiers) and per-fact probabilities reflecting frequency across assimilated examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>assimilation threshold (0.01) and probability threshold for considering a fact part of a concept (0.6); qualitative exam scores (generality/specificity) reported for concepts built by SAGE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Enabled rapid learning from few exemplars; visual concepts reached perfect generality by lesson ~15; SAGE-created generalizations supported successful recognition and projection in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Efficient incremental generalization from small numbers of exemplars; maintained probabilistic notion of typicality that supported tolerant matching and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When exemplars include irrelevant or noisy facts, generalizations may reflect spurious facts unless sufficient exemplars correct them; composability of action/generalization schemas not fully exploited (separate generalized actions for similar spatial variants).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared quantitatively to other generalizers in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation experiments reported isolating SAGE's contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SAGE's probabilistic relational generalizations allow AILEEN to represent object-relational and procedural knowledge compactly and to update these representations incrementally from few grounded exemplars, facilitating both recognition and projection for embodied actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e357.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e357.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial Processing Module (QSRLIB + CDC/RCC8)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial Processing Module using QSRLIB with CDC and RCC8 calculi</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Converts detected bounding boxes and centroids into qualitative spatial relations using QSRLIB, extracting CDC (cardinal directions) and RCC8 topological relations and enabling sampling of regions/points that satisfy qualitative constraints for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qsrlib: A software for online acquisition of qsrs from video.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QSRLIB-based Spatial Processing Module</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline that takes centroids and bounding boxes from the visual module and computes pairwise qualitative spatial relations (CDC and RCC8), can convert sets of constraints into geometric regions and sample points to support placement planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Qualitative spatial relation extraction for grounding prepositions and planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides relational spatial descriptors for every object pair (e.g., east-of, disconnected, within) that serve as the semantic targets for grounding language and as constraints for planner-generated continuous placements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial reasoning / grounding / placement planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>computed from visual detections (bounding boxes/centroids) via QSRLIB</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>deterministic computation of qualitative relations from visual module outputs; these relations are written into Soar working memory and used during indexical comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>qualitative spatial relations (CDC, e.g., e(a1,a2); RCC8 topological relations, e.g., dc(a1,a2)), convertible to sampled continuous regions for planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>implicit in downstream task success (generality/specificity exams and action execution); no separate metric for relation extraction reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Module reliably produced CDC/RCC8 relations used to ground prepositions; errors in downstream tasks were mainly due to simulator physics (object movement) rather than relation extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Supported mapping of prepositional language to conjunctions of qualitative relations (e.g., left of -> east âˆ§ disconnected) and enabled sampling of feasible locations satisfying qualitative constraints for placement actions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When scene objects moved due to physics/distractors, relational descriptions could change causing verification or execution failures; mapping between high-level spatial concepts and calculi can be imperfect in ambiguous contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No baselines provided; module choice motivated by prior QSRLIB capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation experiments removing spatial module reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Qualitative spatial calculi (CDC, RCC8) provide an effective symbolic interface between perception and language grounding, enabling relational descriptions and sampling of continuous regions that drive embodied planning; however, robustness depends on stable percepts and careful mapping from linguistic prepositions to conjunctions of calculi.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e357.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e357.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Indexical Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Indexical Model of Situated Language Comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search-based model of language grounding that interprets linguistic symbols as cues to retrieve perceptual and conceptual referents from current environment and semantic memory, integrating with interaction and learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Indexical Model (implemented in Soar for AILEEN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Formulates comprehension as a goal-directed search: parse input, create grounding goals for object/relation/action references, query semantic/concept memory for percepts and spatial calculi, compose references into a task goal, and invoke planning or learning when failures occur.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language grounding for ITL and embodied action</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grounds nouns/adjectives to visual percepts, prepositions to qualitative spatial calculi, and verbs to action concepts; drives creation of task goals for planning or creates learning goals upon grounding failures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / language grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial + procedural</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>semantic memory (pre-encoded mappings), concept memory queries (SME/SAGE), and perceptual modules (visual + spatial)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>used during lesson interactions (inform/verify/react); failures produce store requests to concept memory; react lessons trigger projection and planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>compositions of percept labels, spatial calculi, and action concept identifiers in Soar working memory graphs that are enhanced with concept memory inferences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success/failure flags on verify/react lessons and number of store requests generated (active learning metric)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Indexical comprehension successfully grounded language to perceptions and concept memory in the experiments; active evaluation limited number of stored exemplars by only storing when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively triggered learning when grounding failed, used semantic and perceptual cues to resolve object references and relations, and integrated projection outputs from concept memory to form task goals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Dependent on completeness of semantic mappings and stability of percepts; failures produce learning goals but may require instructor guidance in curriculum design.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually to prior ROSIE/EBL integration (qualitative), but no numeric baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit ablations of the indexical model reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Formulating comprehension as a search over perceptual, spatial, and conceptual cues tightly couples language with symbolic spatial and procedural representations and provides a mechanism by which missing knowledge triggers exemplar collection and analogical generalization; combined with concept memory projection, this supports generating plans from language even when some sensory detail is missing or when concepts were learned from very few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e357.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e357.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Module (YOLO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep visual perception module built with YOLO (You Only Look Once)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CNN-based perception component that detects object bounding boxes and produces coarse percept labels for shape and color which are converted into symbolic percepts for the agent (e.g., CVCone, CVRed).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>You only look once: Unified, real-time object detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>YOLO-based visual module</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>YOLO-based detector pre-trained on simulator images to output bounding boxes and two percept symbols per object (shape, color); colors are determined with k-means clustering in cropped regions and heuristics to select dominant cluster.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Perceptual grounding for ITL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detects objects on a top-down camera frame in the Webots simulator and provides centroids and percept labels that are translated into predicate-calculus facts for concept memory and spatial processing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception / grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (percepts: shapes/colors) + spatial (centroids for relation computation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised pre-training on simulator-generated images (12,000 images) and heuristics for color assignment</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>continuous camera frames fed into YOLO; outputs written to Soar working memory for downstream processing</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>bounding boxes, centroids, and percept symbols (CVBlue, CVRed, CVCone, etc.) converted to predicate facts (isa o1 CVBlue, isa o1 CVCone)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>detection error rates reported for shapes and colors (<0.1% in simulator), reliability of percept label extraction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>High detection accuracy in the simulator for shapes and colors (reported error <0.1% for both shape and color recognition in the controlled simulated dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provided reliable object hypotheses that enabled concept learning and spatial relation computation in the controlled simulated environment.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Real-world robustness not evaluated; percept-to-concept pipeline sensitive to cropping and clustering heuristics; when objects physically move due to simulation physics, downstream grounding and verification can fail.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No alternative perception modules compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation of visual module reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A high-accuracy perceptual front end supplying symbolic percepts (shape/color plus centroids) is effective for grounding language to symbolic relational representations; this perceptual-symbolic interface is essential for the concept memory to form and apply relational generalizations for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extending sme to handle largescale cognitive modeling. <em>(Rating: 2)</em></li>
                <li>Extending analogical generalization with near-misses. <em>(Rating: 2)</em></li>
                <li>Qsrlib: A software for online acquisition of qsrs from video. <em>(Rating: 2)</em></li>
                <li>You only look once: Unified, real-time object detection. <em>(Rating: 2)</em></li>
                <li>Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds. <em>(Rating: 2)</em></li>
                <li>Using analogy to model spatial language use and multimodal knowledge capture. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-357",
    "paper_id": "paper-219260893",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "AILEEN Concept Memory",
            "name_full": "Analogical Concept Memory for AILEEN (SME + SAGE integration)",
            "brief_description": "A symbolic, analogical long-term concept memory that stores exemplars as predicate-calculus facts and incrementally forms probabilistic relational generalizations (contexts) using SME for matching and SAGE for generalization; used to recognize concepts, infer unseen properties, and project next states for action planning in an embodied agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AILEEN Concept Memory (SME+SAGE)",
            "model_size": null,
            "model_description": "A module combining the Structure-Mapping Engine (SME) for analogical matching and SAGE for sequential analogical generalization; it represents scenes and episodic traces as predicate-calculus facts, maintains generalization contexts with per-fact probabilities, and exposes create/store/query/project commands to the Soar agent for grounding language and projecting action outcomes.",
            "task_name": "Interactive Task Learning (ITL) in a simulated tabletop world",
            "task_description": "AILEEN is taught visual, spatial, and action concepts via interactive lessons (inform, verify, react) in a Webots simulated tabletop domain with objects of known shapes/colors; the agent must ground referring expressions, recognize relations (e.g., left-of), and plan/manipulate objects (point, pick-up, place) to achieve relational goals (e.g., move red cylinder right of blue box).",
            "task_type": "instruction following / object manipulation / multi-step planning",
            "knowledge_type": "spatial + procedural + object-relational",
            "knowledge_source": "exemplar demonstrations and grounded lessons (episodic traces), plus perceptual outputs from modules (YOLO visual percepts and QSRLIB qualitative spatial relations)",
            "has_direct_sensory_input": true,
            "elicitation_method": "interactive indexing (Indexical Model) with explicit commands to concept memory (create/store/query/project), analogical matching via SME, generalization via SAGE; used during verify/react lessons and planning search",
            "knowledge_representation": "explicit symbolic predicate-calculus facts (scene graphs), qualitative spatial calculi (CDC, RCC8), generalization contexts with per-fact probabilities (SAGE), candidate inferences with Skolem terms (SME) for projected next states",
            "performance_metric": "generality exam score (5 verify lessons, scored 0/1 each), specificity exam score (negative tests), number of store requests (examples stored), and successful task completion in react lessons",
            "performance_result": "Visual concepts: generality reached perfect (5/5) by lesson ~15 averaged over 10 trials; specificity maintained (no bleed-over) (5/5). Number of store requests decreased to 0 by end of trials. Spatial concepts: rapid learning but generality scores sometimes &lt;5 due to physics/noise in multi-distractor scenes. Action concepts: learned quickly from episodic traces; task demonstration succeeded (agent achieved move goals after 2 successive projections and search steps).",
            "success_patterns": "Rapid generalization of visual attributes (colors/shapes) from very few exemplars; learning of qualitative spatial relations (mapped prepositions to CDC/RCC8 relations) using relational representations; projection of procedural steps for actions (candidate inferences indicate holding, relative spatial constraints) enabling multi-step planning; robust incremental accumulation of examples into probabilistic generalizations.",
            "failure_patterns": "Performance degraded in noisy/distractor scenes where physics caused objects to move from intended configurations (leading to apparent verification failures); the system created separate action concepts for similar spatial variants (e.g., move-left vs move-above) indicating limited composability of learned action concepts; the concept memory remained a separate module (representation mismatch with Soar), limiting real-time integration and requiring engineering work.",
            "baseline_comparison": "No explicit baselines reported (no random or ML baselines provided); comparisons are qualitative against prior Soar/ROSIE EBL approaches in discussion only.",
            "ablation_results": "No formal ablation studies reported; paper discusses future integration/engineering improvements but no quantitative component-removal experiments were performed.",
            "key_findings": "Analogical generalization (SME+SAGE) can encode object-relational and spatial knowledge as symbolic predicate facts and probabilistic generalizations from few exemplars and can project procedural next states (with Skolem terms) to guide embodied planning; qualitative spatial calculi (CDC, RCC8) provide a language-level mapping for prepositions, and indexical grounding links language to these symbolic representations enabling planning and execution even under partial observability, though robustness degrades with noisy perceptual dynamics and current implementations treat spatial variants as separate action concepts rather than composable components.",
            "uuid": "e357.0",
            "source_info": {
                "paper_title": "Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "SME",
            "name_full": "Structure-Mapping Engine (SME)",
            "brief_description": "An analogical matching engine that computes one-to-one correspondences and similarity scores between relational symbolic representations, producing candidate inferences by aligning facts between an example (or generalization) and a scene.",
            "citation_title": "Extending sme to handle largescale cognitive modeling.",
            "mention_or_use": "use",
            "model_name": "Structure-Mapping Engine (SME)",
            "model_size": null,
            "model_description": "Symbolic analogical matcher that takes predicate-calculus style representations and returns correspondences, similarity match scores, and candidate inferences (including Skolem-generated entities) used to infer unseen facts and to align episodic traces with generalizations.",
            "task_name": "Analogy-driven grounding and projection for ITL",
            "task_description": "Used during query and project operations to match current scene facts or episodic traces against stored generalizations, yielding candidate inferences for concept recognition and next-state predictions to support planning.",
            "task_type": "symbolic reasoning / projection for planning",
            "knowledge_type": "object-relational + spatial + procedural (via aligned episodic traces)",
            "knowledge_source": "symbolic episodic traces and stored generalizations (explicit predicate facts)",
            "has_direct_sensory_input": false,
            "elicitation_method": "invoked by concept memory via query/project commands during indexical comprehension and react lessons",
            "knowledge_representation": "operates over explicit symbolic predicate-calculus representations (scene facts, episodic traces, generalization contexts); outputs candidate inferences with Skolem terms",
            "performance_metric": "internal similarity score (used versus thresholds: assimilation threshold 0.01, match threshold 0.75) and the binary success/failure of indexical grounding or projection",
            "performance_result": "Used as core matching mechanism; matches whose similarity exceeded thresholds produced valid inferences used by AILEEN; quantitative SME accuracy metrics not separately reported in the paper.",
            "success_patterns": "Produced correspondences that enabled robust alignment of perceptual predicates (e.g., CVRed) across examples and scenes, enabling inference of concept membership and next-state properties (e.g., held(O5), spatial relations).",
            "failure_patterns": "Performance contingent on representational alignment and the richness of exemplars; mismatches in noisy scenes or insufficient shared structure reduce similarity scores and block inferences.",
            "baseline_comparison": "Not compared against other matchers in this paper.",
            "ablation_results": "No ablations isolating SME reported.",
            "key_findings": "SME provides the mechanism to extract candidate inferences (including temporal/skolemized entities) from matches between scenes/episodic traces and generalizations, enabling symbolic projection of procedural steps and object-relational inferences even when some sensory detail is unavailable (i.e., SME returns inferred facts to help elaborate unseen states).",
            "uuid": "e357.1",
            "source_info": {
                "paper_title": "Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "SAGE",
            "name_full": "Sequential Analogical Generalization Engine (SAGE)",
            "brief_description": "An analogical generalizer that incrementally assimilates matched exemplars into relational generalizations, maintaining per-fact probabilities to capture typicality across examples.",
            "citation_title": "Extending analogical generalization with near-misses.",
            "mention_or_use": "use",
            "model_name": "SAGE",
            "model_size": null,
            "model_description": "Incremental generalization engine that takes SME correspondences and either assimilates new examples into existing generalizations or stores them as separate exemplars, producing generalized relational schemas with probability weights for facts (used as concept definitions).",
            "task_name": "Incremental concept induction for ITL",
            "task_description": "Produces generalization contexts for visual, spatial, and action concepts by assimilating episodic examples (predicate facts) and updating probabilities that specific facts belong to the concept.",
            "task_type": "concept learning / generalization",
            "knowledge_type": "object-relational + spatial + procedural (action schemas)",
            "knowledge_source": "episodic exemplars stored from interactive lessons and demonstrations",
            "has_direct_sensory_input": false,
            "elicitation_method": "called when store commands add examples; uses SME correspondence and assimilation threshold to decide to generalize",
            "knowledge_representation": "generalization contexts with abstract entities (GenEntFn skolem-like identifiers) and per-fact probabilities reflecting frequency across assimilated examples",
            "performance_metric": "assimilation threshold (0.01) and probability threshold for considering a fact part of a concept (0.6); qualitative exam scores (generality/specificity) reported for concepts built by SAGE",
            "performance_result": "Enabled rapid learning from few exemplars; visual concepts reached perfect generality by lesson ~15; SAGE-created generalizations supported successful recognition and projection in experiments.",
            "success_patterns": "Efficient incremental generalization from small numbers of exemplars; maintained probabilistic notion of typicality that supported tolerant matching and inference.",
            "failure_patterns": "When exemplars include irrelevant or noisy facts, generalizations may reflect spurious facts unless sufficient exemplars correct them; composability of action/generalization schemas not fully exploited (separate generalized actions for similar spatial variants).",
            "baseline_comparison": "Not compared quantitatively to other generalizers in the experiments.",
            "ablation_results": "No ablation experiments reported isolating SAGE's contribution.",
            "key_findings": "SAGE's probabilistic relational generalizations allow AILEEN to represent object-relational and procedural knowledge compactly and to update these representations incrementally from few grounded exemplars, facilitating both recognition and projection for embodied actions.",
            "uuid": "e357.2",
            "source_info": {
                "paper_title": "Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Spatial Processing Module (QSRLIB + CDC/RCC8)",
            "name_full": "Spatial Processing Module using QSRLIB with CDC and RCC8 calculi",
            "brief_description": "Converts detected bounding boxes and centroids into qualitative spatial relations using QSRLIB, extracting CDC (cardinal directions) and RCC8 topological relations and enabling sampling of regions/points that satisfy qualitative constraints for planning.",
            "citation_title": "Qsrlib: A software for online acquisition of qsrs from video.",
            "mention_or_use": "use",
            "model_name": "QSRLIB-based Spatial Processing Module",
            "model_size": null,
            "model_description": "A pipeline that takes centroids and bounding boxes from the visual module and computes pairwise qualitative spatial relations (CDC and RCC8), can convert sets of constraints into geometric regions and sample points to support placement planning.",
            "task_name": "Qualitative spatial relation extraction for grounding prepositions and planning",
            "task_description": "Provides relational spatial descriptors for every object pair (e.g., east-of, disconnected, within) that serve as the semantic targets for grounding language and as constraints for planner-generated continuous placements.",
            "task_type": "spatial reasoning / grounding / placement planning",
            "knowledge_type": "spatial + object-relational",
            "knowledge_source": "computed from visual detections (bounding boxes/centroids) via QSRLIB",
            "has_direct_sensory_input": true,
            "elicitation_method": "deterministic computation of qualitative relations from visual module outputs; these relations are written into Soar working memory and used during indexical comprehension",
            "knowledge_representation": "qualitative spatial relations (CDC, e.g., e(a1,a2); RCC8 topological relations, e.g., dc(a1,a2)), convertible to sampled continuous regions for planning",
            "performance_metric": "implicit in downstream task success (generality/specificity exams and action execution); no separate metric for relation extraction reported",
            "performance_result": "Module reliably produced CDC/RCC8 relations used to ground prepositions; errors in downstream tasks were mainly due to simulator physics (object movement) rather than relation extraction accuracy.",
            "success_patterns": "Supported mapping of prepositional language to conjunctions of qualitative relations (e.g., left of -&gt; east âˆ§ disconnected) and enabled sampling of feasible locations satisfying qualitative constraints for placement actions.",
            "failure_patterns": "When scene objects moved due to physics/distractors, relational descriptions could change causing verification or execution failures; mapping between high-level spatial concepts and calculi can be imperfect in ambiguous contexts.",
            "baseline_comparison": "No baselines provided; module choice motivated by prior QSRLIB capabilities.",
            "ablation_results": "No ablation experiments removing spatial module reported.",
            "key_findings": "Qualitative spatial calculi (CDC, RCC8) provide an effective symbolic interface between perception and language grounding, enabling relational descriptions and sampling of continuous regions that drive embodied planning; however, robustness depends on stable percepts and careful mapping from linguistic prepositions to conjunctions of calculi.",
            "uuid": "e357.3",
            "source_info": {
                "paper_title": "Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Indexical Model",
            "name_full": "Indexical Model of Situated Language Comprehension",
            "brief_description": "A search-based model of language grounding that interprets linguistic symbols as cues to retrieve perceptual and conceptual referents from current environment and semantic memory, integrating with interaction and learning.",
            "citation_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds.",
            "mention_or_use": "use",
            "model_name": "Indexical Model (implemented in Soar for AILEEN)",
            "model_size": null,
            "model_description": "Formulates comprehension as a goal-directed search: parse input, create grounding goals for object/relation/action references, query semantic/concept memory for percepts and spatial calculi, compose references into a task goal, and invoke planning or learning when failures occur.",
            "task_name": "Language grounding for ITL and embodied action",
            "task_description": "Grounds nouns/adjectives to visual percepts, prepositions to qualitative spatial calculi, and verbs to action concepts; drives creation of task goals for planning or creates learning goals upon grounding failures.",
            "task_type": "instruction following / language grounding",
            "knowledge_type": "object-relational + spatial + procedural",
            "knowledge_source": "semantic memory (pre-encoded mappings), concept memory queries (SME/SAGE), and perceptual modules (visual + spatial)",
            "has_direct_sensory_input": true,
            "elicitation_method": "used during lesson interactions (inform/verify/react); failures produce store requests to concept memory; react lessons trigger projection and planning",
            "knowledge_representation": "compositions of percept labels, spatial calculi, and action concept identifiers in Soar working memory graphs that are enhanced with concept memory inferences",
            "performance_metric": "success/failure flags on verify/react lessons and number of store requests generated (active learning metric)",
            "performance_result": "Indexical comprehension successfully grounded language to perceptions and concept memory in the experiments; active evaluation limited number of stored exemplars by only storing when needed.",
            "success_patterns": "Effectively triggered learning when grounding failed, used semantic and perceptual cues to resolve object references and relations, and integrated projection outputs from concept memory to form task goals.",
            "failure_patterns": "Dependent on completeness of semantic mappings and stability of percepts; failures produce learning goals but may require instructor guidance in curriculum design.",
            "baseline_comparison": "Compared conceptually to prior ROSIE/EBL integration (qualitative), but no numeric baseline reported.",
            "ablation_results": "No explicit ablations of the indexical model reported in this paper.",
            "key_findings": "Formulating comprehension as a search over perceptual, spatial, and conceptual cues tightly couples language with symbolic spatial and procedural representations and provides a mechanism by which missing knowledge triggers exemplar collection and analogical generalization; combined with concept memory projection, this supports generating plans from language even when some sensory detail is missing or when concepts were learned from very few examples.",
            "uuid": "e357.4",
            "source_info": {
                "paper_title": "Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Visual Module (YOLO)",
            "name_full": "Deep visual perception module built with YOLO (You Only Look Once)",
            "brief_description": "A CNN-based perception component that detects object bounding boxes and produces coarse percept labels for shape and color which are converted into symbolic percepts for the agent (e.g., CVCone, CVRed).",
            "citation_title": "You only look once: Unified, real-time object detection.",
            "mention_or_use": "use",
            "model_name": "YOLO-based visual module",
            "model_size": null,
            "model_description": "YOLO-based detector pre-trained on simulator images to output bounding boxes and two percept symbols per object (shape, color); colors are determined with k-means clustering in cropped regions and heuristics to select dominant cluster.",
            "task_name": "Perceptual grounding for ITL",
            "task_description": "Detects objects on a top-down camera frame in the Webots simulator and provides centroids and percept labels that are translated into predicate-calculus facts for concept memory and spatial processing.",
            "task_type": "perception / grounding",
            "knowledge_type": "object-relational (percepts: shapes/colors) + spatial (centroids for relation computation)",
            "knowledge_source": "supervised pre-training on simulator-generated images (12,000 images) and heuristics for color assignment",
            "has_direct_sensory_input": true,
            "elicitation_method": "continuous camera frames fed into YOLO; outputs written to Soar working memory for downstream processing",
            "knowledge_representation": "bounding boxes, centroids, and percept symbols (CVBlue, CVRed, CVCone, etc.) converted to predicate facts (isa o1 CVBlue, isa o1 CVCone)",
            "performance_metric": "detection error rates reported for shapes and colors (&lt;0.1% in simulator), reliability of percept label extraction",
            "performance_result": "High detection accuracy in the simulator for shapes and colors (reported error &lt;0.1% for both shape and color recognition in the controlled simulated dataset).",
            "success_patterns": "Provided reliable object hypotheses that enabled concept learning and spatial relation computation in the controlled simulated environment.",
            "failure_patterns": "Real-world robustness not evaluated; percept-to-concept pipeline sensitive to cropping and clustering heuristics; when objects physically move due to simulation physics, downstream grounding and verification can fail.",
            "baseline_comparison": "No alternative perception modules compared in this paper.",
            "ablation_results": "No ablation of visual module reported.",
            "key_findings": "A high-accuracy perceptual front end supplying symbolic percepts (shape/color plus centroids) is effective for grounding language to symbolic relational representations; this perceptual-symbolic interface is essential for the concept memory to form and apply relational generalizations for planning.",
            "uuid": "e357.5",
            "source_info": {
                "paper_title": "Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extending sme to handle largescale cognitive modeling.",
            "rating": 2,
            "sanitized_title": "extending_sme_to_handle_largescale_cognitive_modeling"
        },
        {
            "paper_title": "Extending analogical generalization with near-misses.",
            "rating": 2,
            "sanitized_title": "extending_analogical_generalization_with_nearmisses"
        },
        {
            "paper_title": "Qsrlib: A software for online acquisition of qsrs from video.",
            "rating": 2,
            "sanitized_title": "qsrlib_a_software_for_online_acquisition_of_qsrs_from_video"
        },
        {
            "paper_title": "You only look once: Unified, real-time object detection.",
            "rating": 2,
            "sanitized_title": "you_only_look_once_unified_realtime_object_detection"
        },
        {
            "paper_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds.",
            "rating": 2,
            "sanitized_title": "towards_an_indexical_model_of_situated_language_comprehension_for_cognitive_agents_in_physical_worlds"
        },
        {
            "paper_title": "Using analogy to model spatial language use and multimodal knowledge capture.",
            "rating": 1,
            "sanitized_title": "using_analogy_to_model_spatial_language_use_and_multimodal_knowledge_capture"
        }
    ],
    "cost": 0.0164635,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures
19 Jun 2020</p>
<p>Shiwali Mohan shiwali.mohan@parc.com 
Palo Alto Research Center
94306Palo AltoCAUSA</p>
<p>Matthew Shreve matthew.shreve@parc.com 
Palo Alto Research Center
94306Palo AltoCAUSA</p>
<p>Kent Evans kent.evans@parc.com 
Palo Alto Research Center
94306Palo AltoCAUSA</p>
<p>Aaron Ang aaron.ang@parc.com 
Palo Alto Research Center
94306Palo AltoCAUSA</p>
<p>John Maxwell john.maxwell@parc.com 
Palo Alto Research Center
94306Palo AltoCAUSA</p>
<p>Characterizing an Analogical Concept Memory for Newellian Cognitive Architectures
19 Jun 2020100A00483F3E18C21511AFEC8F8B6058arXiv:2006.01962v2[cs.AI]Submitted X/20XX; published X/20XX
We propose a new long-term declarative memory for Soar that leverages the computational models of analogical reasoning and generalization.We situate our research in interactive task learning (ITL) and embodied language processing (ELP).We demonstrate that the learning methods implemented in the proposed memory can quickly learn a diverse types of novel concepts that are useful in task execution.Our approach has been instantiated in an implemented cognitive system AILEEN and evaluated on a simulated robotic domain.</p>
<p>Introduction</p>
<p>Newellian cognitive architectures -Soar (Laird, 2012), ACT-R (Anderson, 2009), Sigma (Rosenbloom et al., 2016) -have been prominent not only in cognitive modeling but also in design of complex intelligent agents.The past 30 years of research in applying these architectures to a variety of tasks has culminated in a proposal for the common model of cognition (Laird et al., 2017).The architectures aim to implement a set of domain-general computational processes which operate over domain-specific knowledge to result in effective task behavior.Early research in these architectures studied procedural knowledge -the knowledge of how to perform tasks, often expressed as if-else rules.It explored the computational underpinnings of a general purpose decision making process that can apply hand-engineered procedural knowledge to perform a wide-range of tasks.Later research studied various ways in which procedural knowledge can be learned and optimized.</p>
<p>While Newellian architectures have been applied widely, Hinrichs &amp; Forbus (2017) note that reasoning in them focuses exclusively on problem solving, decision making, and behavior.Further, they argue that a distinctive and arguably signature feature of human intelligence is being able to build complex conceptual structures of the world.In the Newellian architecture terminology, the knowledge of concepts is declarative knowledge -the knowledge of what.While Newellian architectures agree that conceptual structures are useful for intelligent behavior, there is limited understanding of how declarative knowledge about the world is acquired.</p>
<p>In this paper, we study the questions of declarative concept representation, acquisition, and usage in task performance in a Newellian architecture.We conduct our research in the Soar cognitive architecture.However, as it bears significant similarities with ACT-R and Sigma in the organization of computation and knowledge, our findings can be generalized to those architectures as well.To design the concept memory, we leverage the computational processes that underlie analogical reasoning and generalization in the Companions cognitive architecture -the Structure Mapping Engine (SME; Forbus et al. 2017) and the Sequential Analogical Generalization Engine (SAGE; McLure et al. 2015).Our design exploration is motivated by the interactive task learning problem (ITL; Gluck &amp; Laird 2019) in embodied agents.ITL agents rely on natural interaction modalities such as linguistic dialog to learn new tasks.Conceptual knowledge and language are inextricably tied -language is a medium through which conceptual knowledge about the world is communicated and learned.Consequently, language processing for ITL provides a set of functional requirements that an architectural concept memory must address.Towards an architectural concept memory to support ITL, this paper makes the following contributions: 1. Define the concept learning problem embedded within the larger context of ITL 2. Identify a desiderata for an architectural concept memory 3. Implement a concept memory for Soar agents using the models of analogical processing 4. Characterize the learning performance of the implemented concept memory for diverse concepts</p>
<p>Preliminaries</p>
<p>Declarative Long-Term Memories in Soar In the past two decades, algorithmic research in Soar has augmented the architecture with decalartive long-term memories (dLTMs).Soar has two -semantic (Derbinsky et al., 2010) and episodic (Derbinsky &amp; Laird, 2009) -that serve distinct cognitive functions following the hypotheses about organization of memory in humans (Tulving &amp; Craik, 2005).Semantic memory enables enriching what is currently observed in the world with what is known generally about it.For example, if a dog is observed in the environment, for certain types of tasks it may be useful to elaborate that it is a type of a mammal.Episodic memory gives an agent a personal history which can later be recalled to establish reference to shared experience with a collaborator, to aid in decision-making by predicting the outcome of possible courses of action, to aid in reasoning by creating an internal model of the environment, and by keeping track of progress on long-term goals.The history is also useful in deliberate reflection about past events to improve behavior through other types of learning such as reinforcement learning or explanation-based learning.Using dLTMs in Soar agents has enabled reasoning complexity that wasn't possible earlier (Xu &amp; Laird, 2010;Mohan &amp; Laird, 2014;Kirk &amp; Laird, 2014;Mininger &amp; Laird, 2018).However, a crucial question remains unanswered -how is knowledge in semantic memory acquired?(Kirk &amp; Laird, 2014) provide an answer to this question -semantic knowledge is acquired through interactive instruction which builds declarative structures that are later operationalized.We explore a different view -semantic knowledge is acquired through generalization of concrete experiences grouped together through interactions with an intelligent trainer.</p>
<p>Analogical Processing Analogical matching, retrieval, and generalization form the foundation of the Companions Cognitive architecture.In Why we are so smart?, Gentner claims that what makes human cognition superior to other animals is "First, relational concepts are critical to higher-order cognition, but relational concepts are both non-obvious in initial learning and elusive in memory retrieval.Second, analogy is the mechanism by which relational knowledge is revealed.Third, language serves both to invite learning relational concepts and to provide cognitive stability once they are learned."(Gentner, 2003) These ideas permeate Companions which has demonstrated how analogical processing enables reasoning over multi-modal input representations to learn spatial relationships (Lockwood, 2009), to learn object concepts (Chen et al., 2019) and to solve multimodal physical reasoning problems (Klenk et al., 2011).The diversity of reasoning tasks captured in these approaches motivates using analogical processes to develop an architectural concept memory.</p>
<p>Embodied Language Processing and ITL Language is an immensely powerful communication medium for humans enabling exchange of information pertaining to not only current events but to events that have happened in the past, events that may happen in the future, and events that are unlikely to have ever occurred.To make progress towards generally intelligent agents that can assist and collaborate with humans, a capability to effectively use language is crucial.We focus on language capabilities necessary to achieve collaboration in physical worlds and term this capability embodied language processing (ELP).ELP can enable novel learning capabilities such as ITL in artificial agents.The main goal of ELP is to establish reference to entities and actions in the shared space such that joint goals can be achieved.Consider the scene in Figure 1 in which a human collaborator asks the robot to move the blue cone to the left of red cylinder.For successful collaboration, the robot must localize blue cone as a specific object on the scene and execute an action on that object.The robot must solve the inverse problem to generate language.It must be able to describe objects and actions that are relevant to its own task performance such that the human collaborator is able to correctly identify them.This paper builds upon a method for ELP -the Indexical Model of situated comprehension (Mohan et al., 2014) which is a computational, architectural instantiation of the Indexical Hypothesis (Glenberg &amp; Robertson, 1999).This model has been shown to be useful in ITL (Mohan &amp; Laird, 2014;Kirk &amp; Laird, 2014).</p>
<p>AILEEN</p>
<p>AILEEN is a cognitive system that learns new concepts through interactive experiences with a trainer in a simulated world.A system diagram is shown in Figure 1.AILEEN lives in a simulated robotic world built in Webots * .The world contains a table-top on which various simple objects can be placed.A simulated camera above the table captures top-down visual information.AILEEN is engaged in a continuous perceive-decide-act loop with the world.A trainer can set up a scene in the simulated world by placing simple objects on the scene and providing instructions to the agent.AILEEN is designed in Soar which has been integrated with a deep learning-based vision module and an analogical concept memory.It is related to ROSIE, a cognitive system that has demonstrated interactive, flexible learning on a variety of tasks (Mohan et al., 2012(Mohan et al., , 2014;;Mohan &amp; Laird, 2014;Kirk &amp; Laird, 2014;Mininger &amp; Laird, 2018), and implements a similar organization of knowledge.Visual Module The visual module processes the image taken from the simulated camera.It produces output in two channels: object detections as bounding boxes whose centroids are localized on the table-top and two perceptual symbols or percepts corresponding to the object's shape and color each.The module is built using a deep learning framework -You Only Look Once (YOLO: Redmon et al. ( 2016)).YOLO is pre-trained with supervision from the ground truth in the simulator (12, 000 images).It is detects four shapes (error rate &lt; 0.1%) -box (percept -CVBox), cone (CVCone), ball (CVSphere), and cylinder (CVCylinder).</p>
<p>For colors, each detected region containing an object is cropped from the image, and a K-means clustering is applied all color pixel values within the crop.Next, two weighted heuristics are applied that select the cluster that likely comprises the detected shape among any background pixels and/or neighboring objects.The first heuristic select the cluster with the maximum number of pixels.The second heuristic selects the cluster with the centroid that is closest to the image center of the cropped region.The relative weighted importance of each of these heuristics is then trained using a simple grid search over w 1 and w 2 : Score = w 1 R s + w 2 (1 âˆ’ C s ), s âˆˆ D, where w 1 + w 2 = 1, D is the set clusters, R s denotes the ratio between the number of pixels in each cluster and the the number of pixels in the image crop, and C s is the Euclidean distance between the centroid of the cluster and the image center normalized by the cropped image width.The average RGB value is calculated for all pixels included in the cluster with the highest score and compared with the preset list of color values.The color label associated with the color value that has the smallest Euclidean distance to the average RGB value is selected.The module can recognize 5 colors (error rate &lt; 0.1%): CVGreen, CVBlue, CVRed, CVYellow, and CVPurple.Note that the percepts are named so to be readable for system designers -the agent does not rely on the percept symbol strings for any reasoning.</p>
<p>Spatial Processing Module</p>
<p>The spatial processing module uses QSRLIB (Gatsoulis et al., 2016) to process the bounding boxes and centroids generated by the visual module to generate a qualitative description of the spatial configuration of objects.For every pair of objects, the module extracts qualitative descriptions using two spatial calculi (qsrs): cardinal direction (CDC) and region connection (RCC8).Additionally, the spatial processing module can also convert a set of calculi into regions and sample points from them.This enables AILEEN to identify locations in continuous space that satisfy qualitative spatial constraints when planning actions.</p>
<p>Behavior The outputs of the visual module and the spatial module are compiled together to form an object-oriented relational representation of the current state of the world.This representation is written to Soar's working memory graph.Procedural knowledge is encoded as rules in Soar and, similarly to ROSIE (Mohan et al., 2012), consists of knowledge for: 1. Interaction: As in ROSIE (Mohan et al., 2012) AILEEN implements collaborative discourse theory (Rich et al., 2001) to manage its interactive behavior.It captures the state of task-oriented interaction and is integrated with comprehension, task execution, and learning.2. Comprehension: AILEEN implements the Indexical Model of comprehension (Mohan et al., 2014) to process language by grounding it in the world and domain knowledge.This model formulates language understanding as a search process.It interprets linguistic symbols and their associated semantics as cues to search the current environment as well as domain knowledge.</p>
<p>Formulating language comprehension in this fashion integrates naturally with interaction and learning where ambiguities and failures in the search process drive interaction and learning.3. External task execution: AILEEN has been programmed with primitive actions that enable it to manipulate its environment: point(o), pick-up(o), and place ([x, y, z]).Following Mohan &amp; Laird (2014), each primitive action has a proposal rule that encodes its pre-conditions, a model that captures state changes expected to occur when the action is applied, and an application rule.Additionally, given a task goal, AILEEN can use iterative-deepening search to plan a sequence of primitive actions to achieve the goal and execute the task in the world.4. Learning: Learning in AILEEN is the focus of this paper and is significantly different from ROSIE.ROSIE uses an interactive variation of explanation-based learning (Mohan &amp; Laird, 2014) to learn representation and execution of tasks.AILEEN uses analogical reasoning and generalization to learn diverse concepts including those relevant to task performance (Sections 4 and 5).A crucial distinction is that EBL requires a complete domain theory to correctly generalize observed examples while analogical reasoning and generalization can operate with partial domain theory by leveraging statistical information in observed examples.The ongoing ITL research in Soar demonstrates the strength of this organization of knowledge in hybrid cognitive systems.Our conjecture is that an ideal concept memory in an architecture must support complex, integrated, intelligent behavior such as ITL.</p>
<p>Interactive Concept Learning</p>
<p>Concepts in AILEEN support two behaviors: (1) indexical comprehension, which brings to attention various parts of the environment and domain knowledge and (2) task execution, which applies a sequence of actions to achieve a goal in the world.Consider the world in Figure 1 and the corresponding working memory graph in Figure 2. Let there be some associative conceptual knowledge maps -in semantic memory (shown in Figure 2).Phrases (1) blue cone left of red cylinder and (2) move blue cone right of red cylinder can be understood via indexical comprehension as follows: 1. Parse the linguistic input into semantic components.Both (1) and ( 2 (2) has a reference to an action: {act1: {act-name: move, argument1: or1, argument2: or2, relation: left of} }.For this paper, we assume that the knowledge for this step is pre-encoded.2. Create a goal for grounding each reference.The goal of processing an object reference is to find a set of objects that satisfy the properties specified.It starts with first resolving properties.The process queries semantic memory for a percept that corresponds to various properties in the parse.If the knowledge in Figure 2 is assumed, property blue resolves to percept CVBlue, cone to CVCone, red to CVRed, and cylinder to CVCylinder.Using these percepts, AILEEN queries its scene to resolve object references.For or1, it finds an object that has both CVBlue and CVCone in its description.Let or1 resolve to o1 and or2 to o2 where o1 and o1 are identifiers of objects visible on the scene.The goal of processing a relation reference is to find a set of spatial calculi that correspond to the name specified.If knowledge in Figure 2 is assumed, rel1 in (1) is resolved to a conjunction of qsrs e(a1,a2)âˆ§dc(a1,a2) i.e, object mapping to a1 should be east (in CDC) of a2 and they should be disconnected.Similarly, act1 in (2) resolves to a task goal which is a conjunction of qsrs w(a1,a2)âˆ§dc(a1,a2) 3. Compose all references: Use semantic constraints to resolve the full input.For (1) and (2) a1 is matched to to ar1 and consequently to o1.Similarly, a2 is resolved to o2 via ar2.Under certain circumstances (further explained in section 4.2), AILEEN attempts to act.AILEEN creates a task goal g1 in the working memory and searches for and executes a plan in the world.</p>
<p>Concept Learning</p>
<p>With an understanding of how indexical comprehension connects language with perceptions and actions, we can begin to define the concept learning problem.Our main question is this -where does the conceptual knowledge in semantic memory (in Figure 2) come from?We study how conceptual knowledge can be acquired through situated interactions about the world with an intelligent trainer.Information in interactions helps collect similar experiences about the world and a generalization process distills the common elements in the collection.This process can be seen as mediating knowledge in Soar's episodic memory and semantic memory.To develop our ideas further, we focus on three kinds of concepts.These concepts are crucial for ELP and ITL.Visual concepts correspond to perceptual attributes of objects and include colors and shapes.They provide meaning to nouns and adjectives in the linguistic input.Spatial concepts correspond to configuration of objects and provide grounding to prepositional phrases in the linguistic input.Action concepts correspond to temporal changes in object configurations and provide grounding to verb phrases.</p>
<p>Curriculum of Guided Participation</p>
<p>We introduce a novel interactive process for training AILEEN to recognize and use novel conceptsguided participation.Guided participation sequences and presents lessons -conjoint stimuli (world and language) -to AILEEN.A lesson consists of a scenario setup in AILEEN's world and an interaction with AILEEN.A scenario can be a static scene when training visual and spatial concepts or a sequence of scenes when training an action concept.An interaction has a linguistic component (content) and a non-linguistic component (signal).The signal component of instruction guides reasoning in AILEEN and determines how it processes and responds to the content.Currently, AILEEN can interpret and process the following types of signals: 1. inform: AILEEN performs active learning.It uses all its available knowledge to process the content through indexical comprehension (Section 4).If failures occur, AILEEN creates a learning goal for itself.In this goal, it uses the current scenario to generate a concrete example of the concept described in the content.This example is sent to its concept memory.If no failure occurs, AILEEN does not learn from the example.AILEEN learning is deliberate; it evaluates the applicability of its current knowledge in processing the linguistic content.It learns only when the current knowledge isn't applicable, and consequently, AILEEN accumulates the minimum number of examples necessary to correctly comprehend the content in its lessons.2. verify: AILEEN analyzes the content through indexical comprehension and determines if the content refers to specific objects, spatial relationships, or actions in the accompanying scenario.</p>
<p>If AILEEN lacks knowledge to complete verification, AILEEN indicates a failure to the instructor.3. react: This signal is defined only when the linguistic content contains a reference to an action.</p>
<p>AILEEN uses its knowledge to produce an action instantiation.Upon instantiation, AILEEN determines a goal state in the environment and then plans, a sequence of actions to achieve the goal state.This sequence of actions is executed in the environment.Incorporating these variations in how AILEEN responds to the linguistic content in a lesson enables flexible interactive learning.A trainer can evaluate the current state of knowledge in AILEEN by assigning it verify and react lessons.While the verify lesson tests if AILEEN can recognize a concept in the world, the REACT lesson tests if AILEEN can use a known concept to guide its own behavior in the environment.Observing failures helps the trainer in structuring inform lessons that guide AILEEN's learning.In an inform lesson, AILEEN evaluates its own learning and only adds examples when necessary.Such learning strategy distributes the onus of learning between both participants.Lessons can be structured in a flexible, reactive way in real human-robot training scenarios.</p>
<p>Desiderata for a Concept Memory</p>
<p>We propose the following desiderata for a concept memory, which differs from previous approaches (Langley, 1987) due to our emphasis on embedding it within a larger task, in this case ELP and ITL: D0 Is (a) architecturally integrated and (b) uses relational representations.D1 Can represent and learn a diverse types of concepts.In particular, for AILEEN, the concept memory must be able to learn visual concepts, spatial concepts, and action concepts.D2 Learn from exemplars acquired through experience in the environment.AILEEN is taught through lessons that have two stimuli -a scenario and linguistic content that describes it.D3 Enable incremental accumulation of knowledge.Interactive learning is a distinctive learning approach in which behavior is intertwined with learning.It has been previously argued that interleaving behavior and learning splits the onus of learning between the instructor and the learner such that the instructor can observe the learner's behavior and provide more examples/instruction if necessary D4 Facilitate diverse reasoning over definitions of concepts.</p>
<p>(a) Evaluate existence of a concept in the current environment, including its typicality.This enables recognizing a concept in the environment.(b) Envision a concept by instantiating it in the current environment.This enables action in the environment.(c) Evaluate the quality of concept definitions.This enables active learning -if the quality of a concept is poor, more examples can be added to improve it.D5 Learn from little supervision as realistically humans cannot provide a lot of examples.</p>
<p>Concept Memory</p>
<p>Concept learning in AILEEN begins with a failure during Indexical comprehension in an inform lesson.Assume that AILEEN does not know the meaning of red, i.e, it does not know that red implies the percept CVRed in the object description.When attempting to ground the phrase red cylinder in our example, Indexical comprehension will fail when it tries to look-up the meaning of the word red in its semantic memory.As in ROSIE, a failure (or an impasse) in AILEEN is an opportunity to learn.Learning occurs through interactions with a novel concept memory in addition to Soar's semantic memory.Similarly to Soar's dLTM, the concept memory is accessed by placing commands in a working memory buffer (a specific sub-graph).The concept memory interface has 4 commands: create, store, query, and project.Of these, store and query are common with other Soar dLTMs.create and project are novel and explained in the following sections.</p>
<p>AILEEN's concept memory is built on two models of cognitive processes: SME (Forbus et al., 2017) and SAGE (McLure et al., 2015) and can learn visual, spatial, and action concepts (desiderata D0).Below we describe how each function of concept memory is built with these models.The current implementation of the memory represents knowledge as predicate calculus statements or facts.We have implemented methods that automatically convert Soar's object-oriented graph description to a list of facts when needed.Example translations from Soar's working memory graph to predicate calculus statements are shown in Table 1.Visual and spatial learning requires generating facts from the current scene.Examples for action learning are provided through a demonstration which is automatically encoded in Soar's episodic memory.An episodic trace of facts is extracted from the episodic memory (shown in Table 1).We will rely on examples in Table 1 for illustrating the operation of the concept memory in the remainder of this section.We have summarized various terms and parameters used in analogical processing in Table 2.</p>
<p>Creation and Storage</p>
<p>When AILEEN identifies a new concept in linguistic content (word red), it creates a new symbol (RRed).This new symbol in incorporated in a map in Soar's semantic memory and is passed on to the concept memory for creation of a new concept via the create command.The concept memory creates a new reasoning symbol as well as a new generalization context (shown in Figure 3).A generalization context is an accumulation of concrete experiences with a concept.Each generalization context is a set of individual examples and generalizations.</p>
<p>After creating a new concept, Soar stores an example in the concept memory.The command {store: [(isa o2 CVRed) (isa o2 CVCylinder) (isa o2 RRed)], concept: RRed} stores that the object o2 in the world is an example of the concept RRed.This example A is stored in the RRed generalization context as is -as a set of facts.Assume that at a later time, Soar sends another example B of RRed concept through the command {store: [(isa o3 CVRed) (isa o3 CVCube) (isa o3 RRed)], concept: RRed}.The concept memory adds the new example to the RRed generalization context by these two computational steps: 1. SME performs an analogical match between the two examples.The result of analogical matching has two components: a correspondence set and a similarity score.A correspondence set contains Facts P (isa (GenEntFn 0 RRedMt) RRed) 1.0 (isa (GenEntFn 0 RRedMt) CVRed)</p>
<p>1.0 (isa (GenEntFn 0 RRedMt) CVCube) 0.5 (isa (GenEntFn 0 RRedMt) CVCylinder) 0.5</p>
<p>Figure 3: (left) SAGE maintains a generalization context for each concept.For each example (circle) of a concept, it is either added to a generalization (rounded rectangle) or maintained as an independent example for the concept.(right) Facts and their probabilities in generalization context for RRed alignment of each fact in one example with at most one fact from other.The similarity score indicates the degree of overlap between the two representations.In the two examples A and B, there are two corresponding facts: (isa o2 CVRed) aligns with (isa o3 CVRed) and (isa o2 RRed) aligns with (isa o3 RRed).If the similarity score exceeds an assimilation threshold (Table 2), SAGE continues to the next step to create a generalization.2. SAGE assimilates the two examples A and B into a generalization (e.g. Figure 3, right).It :</p>
<p>(a) Uses the correspondence to create abstract entities.In the two examples provided, (isa o2 RRed) aligns with (isa o3 RRed) and (isa o2 CVRed) with (isa o3 CVRed).Therefore, identifiers o2 and o3 can be replaced with an abstract entity (GenEntFn 0 RRedMt).(b) Maintains a probability that a fact belongs in the generalization.Because (isa (GenEntFN 0 RRedMT) RRed) and (isa (GenEntFn 0 RRedMT) CVRed) are common in both examples, they are assigned a probability of 1.Other facts are not in the correspondences and appear in 1 of the 2 examples in the generalization resulting in a probability of 0.5.Each time a new example is added to this generalization, the probabilities will be updated the reflect the number of examples for which the facts were aligned with each other.Upon storage in a generalization context, a generalization becomes available for matching and possible assimilation with future examples enabling incremental (D3), example-driven (D2) learning.</p>
<p>Query</p>
<p>During indexical comprehension, AILEEN evaluates if a known concept exists in the current world through the query command.Assume that in an example scene with two objects, indexical comprehension attempts to find the one that is referred to by red through {query: {scene: [(isa o4 CVRed) (isa o4 CVBox) (isa o5 CVGreen) (isa o2 CVCylinder)], pattern: (isa ?oRRed)}}.In response to this command, the concept memory evaluates if it has enough evidence in the generalization context for RRed to infer (isa o2 RRed).The concept memory performs this inference through the following computations.1. SME generates a set of candidate inferences.It matches the scene with the generalization in Figure 3 (right).This match results in a correspondence between the facts (isa o4 CVRed) in scene) and (isa (GenEntFn 0 RRedMt) CVRed), which aligns o4 with (GenEntFn 0 RRedMt).</p>
<p>Other facts that have arguments that align, but are not in the correspondences, are added to the set of candidate inferences.In our example, a candidate inference would be (isa o4 RRed).2. AILEEN filters the candidate inferences based on the pattern in the query command.It removes all inferences that do not fit the pattern.If the list has an element, further support is calculated.3. AILEEN evaluates the support for inference by comparing the similarity score of the match to the match threshold.That is, the more facts in the generalization that participate in the analogical match then it is more likely that the inference is valid.Through queries to the concept memory and resultant analogical inferences, the working memory graph (of the world in Figure 4) is enhanced.This enhanced working memory graph supports indexical comprehension in Section 4. Note that the internal concept symbols in blue (such as RBlue) are generalization contexts in the concept memory that accumulate examples from training.Consequently, the 'meaning' of the word blue will evolve as more examples are accumulated.</p>
<p>Projection</p>
<p>In ITL, simply recognizing that an action has been demonstrated is insufficient, the agent must also be able to perform the action if directed (desiderata D4).One of the advantages of analogical generalization is that the same mechanism is used for recognition and projection.AILEEN uses SAGE to project the concept of an action onto future states.This is done by using SME to match the current scene against the generalization context of the action.Then, the concept memory provides all the candidate inferences that symbolically describe the next state of the action concept.</p>
<p>Consider the case of AILEEN receiving a command to move a red object to the left of a green object.During indexical comprehension, AILEEN performs queries to identify the red object, O5, and green object, O6.Given the react signal from the instructor, AILEEN invokes projection using the RMove concept and the current scene.The resulting candidate inferences (shown in Figure 5) indicate that in the next state, O5 should be held.</p>
<p>(H (:skolem (GenEntFn 0 0 rMoveMt)) (held O5) (after (:skolem (GenEntFn 0 0 rMoveMt)) T0)</p>
<p>Figure 5: Candidate inferences indicate that the next state of the move action is to hold object O5.Skolem terms are generated by SME to indicate that the candidate inference refers to an entity from the concept for which there is no correspondence in the scene.In this case, the skolem represents the next temporal state of the action as denoted by the after relation.</p>
<p>After executing a pick-up action, AILEEN invokes projection again to determine if RMove requires more steps.In this case, it does, and the candidate inferences specify that O5 should be located to the w of O6 and they should be topologically disjoint.Further, these candidate inferences indicate that this is the last step in the action, and therefore AILEEN marks the action as completed.</p>
<p>Evaluation</p>
<p>As per desiderata D1, a concept memory must be able to learn diverse types of concepts.We demonstrate this capability by demonstrating learning of visual, spatial, and action concepts.The concepts are taught through lessons in a curriculum of guided participation (Section 4.2) demonstrating that concepts can be learned from grounded exemplars (D2).The experiments emulate aspects of ITL with humans where concepts are introduced incrementally during behavior (D3).</p>
<p>Method We performed separate learning experiments for visual, spatial, and action concepts.We leverage the lessons of guided participation in the design of our experimental trials.Each trial is a sequence of inform lessons.In an inform lesson, a concept is randomly selected from a predetermined set and shown to AILEEN accompanied with linguistic content describing the concept.The lesson is simplified, i.e, there are no distractor objects (examples are shown in Figures 6, 7, &amp; 8).This lesson is presented to AILEEN and we record the number of store requests it makes to the concept memory.Recall that AILEEN learns actively; i.e, it deliberately evaluates if it can understand the linguistic content with its current knowledge and stores examples only when necessary.The number of store requests made highlight the impact of such active learning.</p>
<p>Additionally, to measure generality and correctness, we test AILEEN knowledge after every inform lesson through two exams: generality and specificity (examples are shown in Figures 6, 7,  &amp; 8).Both exams are made up of 5 verify lessons that are randomly selected at the beginning of the trial.As AILEEN learns, the scores on these test demonstrate how well AILEEN can apply what it has learned until now.In the generality lessons, AILEEN is asked to verify if the concept in the linguistic input exists on the scene.If AILEEN returns with a success status, it is given a score of 1 and 0 otherwise.In the specificity lessons, AILEEN is asked to verify the existence of a concept, however, the concept in the scenario is different from the concept is the linguistic content.If AILEEN returns with a failed status, it is given a score of 1 and 0 otherwise.Both types of exam lessons have 0âˆ’3 distractor objects introduced on the scene to evaluate if existence of noise impacts the application of conceptual knowledge.</p>
<p>Results Figure 6 illustrates visual concept learning.AILEEN begins without any knowledge of any concept.As two concepts (green and cone) are introduced in the first lesson, it provides several store commands to its concept memory (shown in blue bars).The number of commands reduce as the training progresses.As is expected, the score on the generality exam is very low because AILEEN doesn't know any concepts.However, this score grows very quickly with training eventually reaching perfect performance at lesson 15.The score on the specificity exam starts at 5, this is to be expected as well.This is because if a concept is unknown, AILEEN cannot recognize it on the scene.However, as the trial progress we see that this score doesn't drop.This indicates that conceptual knowledge of one concept doesn't bleed into others.Note that the exams have distractor Figure 7 illustrates spatial concept learning (commenced after all visual concepts are already known).Spatial relationships are defined between two objects each of which be 1/20 possible in the domain.Concrete examples include irrelevant information (e.g., left of " does not depend on visual properties of the objects).Despite this large complex learning space, learning is quick and spatial concepts can be learned with few examples.These results demonstrate the strength of analogical generalization over relational representations.An interesting observation is that generality scores do not converge to 5 as in visual concept learning.A further analysis revealed that in noisy scenes when the trainer places several distractors on the scene, sometimes the objects move because they are placed too close and the environment has physics built into it.The movement causes objects to move from the intended configuration leading to apparent error in AILEEN's performance.This is a problem with our experimental framework.The learning itself is robust as demonstrated by number of store commands in the trial which reduce to 0 at the end.</p>
<p>Figure 8 illustrates action learning (commenced after all visual and spatial concepts have been learned).Actions are generated through the template move <object reference 1> <relation> <object reference 2>.Similarly to spatial concepts, the learning space is very large and complex.When AILEEN asks, it is provided a demonstration of action performance as shown in Figure 8 (T0, T1, T2).AILEEN stores the demonstration trace in its episodic memory.For storing an example in the concept memory, information in Soar's episodic memory is translated into an episodic trace as shown Table 1.Similarly to visual and spatial learning, inform lessons with simplified scene are used to teach a concept.Exams made up of positive and negative verify lessons are used to evaluate learning.As we see in Figure 8, AILEEN can quickly learn action concepts.Errors towards the later part of the experimental trial occur for the same reason we identified in spatial learning.</p>
<p>Task Demonstration After visual, spatial, and action concepts were taught, we used a react lesson to see if AILEEN could perform the actions such as move the red cylinder right of the blue box.AILEEN successfully use analogy-driven planning.After every action, it queried the concept memory to find the next desired state that it should try to achieve in the environment using the project command.After retrieving the next state, it applied the models of actions to determine which action will achieve the desired state.Upon finding an action, it applied it to the environment.After 2 successive projections and search steps, if achieved the goal.This is extremely encouraging -AILEEN could learn to perform an action by learning through demonstrations.Further, this evaluation establishes that the concept represented explored in this paper not only support interactive learning of concepts, they can be applied for recognition as well as action.</p>
<p>Related Work</p>
<p>Diverse disciplines in AI have proposed approaches for concept learning from examples, however, very few are integrated with reasoning and behavior (D0a).ML-based classification approaches are designed for specific problems (D1, D4), typically require a large number of examples (D5) which are added in batch-mode (D3).EBL and inductive logic programming (Muggleton &amp; De Raedt, 1994) can learn from a few datapoints (D5) but require fully-specified domain theory (D2).Bayesian concept learning Tenenbaum (1999) uses propositional representations (D0b) and has been demonstrated for a specific type of concepts (D1).</p>
<p>There are a few cognitive systems' approaches to the concept learning problem that achieve the desiderata that we delineated in Section 4. In the late 1980s -early 1990s, there was a concerted effort to align machine learning and cognitive science around concept formation (Fisher, 1987).For example, LABYRINTH (Thompson &amp; Langley, 1991) creates clusters of examples, summary descriptions, and a hierarchical organization of concepts using a sequence of structure examples.COBWEB3 (Fisher, 1987) incorporates numeric attributes and provides a probabilistic definition differences between concepts.Building off these ideas, TRESTLE (MacLellan et al., 2015) learns concepts that include structural, relational, and numerical information.Our work can be seen as a significant step along this direction.The concept memory proposed here leverages the computational models of analogical processing that have been shown to emulate analogical reasoning in humans.Further, we view concept learning as an incremental active process that is greatly facilitated by trainer-learner interactions.By integrating concept learning with ELP and ITL, we contribute substantially to prior work which has typically focussed on acquisition independent of how this knowledge might be applied.Finally, integration of concept learning with recent, state-of-the-art vision methods provides evidence that these approaches can be deployed on to real-world systems.</p>
<p>Discussion, Conclusions, and Future Work</p>
<p>In this paper, we explored the design and evaluation of a novel concept memory for Soar (and other Newellian cognitive architectures).The computations in the memory use models of analogical processing -SAGE and SME.This memory can be used to acquire new situated, concepts in interactive settings.The concepts learned are not only useful in ELP and recognition but also in task execution.While the results presented here are encouraging, the work described in this paper is only a small first step towards an architectural concept memory.We have only explored a functional integration of analogical processing in Soar.The memory has not be integrated into the architecture but is a separate module that Soar interacts with.There are significant differences between representations that Soar employs and those in the memory.For an efficient integration and a reactive performance that Soar has historically committed to, several engineering enhancements have to be made.</p>
<p>There are several avenues for extending this work.We are looking at three broad classes of research: disjunctive concepts, composable concepts, and expanded mixed-initiative learning.Disjunctive concepts arise from homographs (e.g., bow in a musical instrument versus bow -a part of a ship) as well as when the spatial calculi do not align with the concept or the functional aspects of the objects must be taken into account (e.g., a cup is under a teapot when it is under the spigot, while a saucer is under a cup when it is directly underneath).One of the promises of relational declarative representations of the form learned here is that they are composable.This isn't fully exploited for learning actions with spatial relations in them.Our approach ends up with different concepts for move-left and move-above.A better solution would be to have these in the same generalization such that AILEEN would be able to respond to the command to move cube below cylinder assuming it been taught a move action previously along with the concepts for below, cube, and cylinder.Another avenue is contextual application of concepts.For example, bigger box requires comparison between existing objects.Finally a cognitive system should learn not only from a structured curriculum designed by an instructor but also in a semi-supervised fashion while performing tasks.In our context this means adding additional examples to concepts when they were used as part of a successful execution.This also means, when there are false positives that lead to incorrect execution, revising the learned concepts based on this knowledge.One approach from analogical generalization focuses on exploiting these near-misses with SAGE (McLure et al., 2015).</p>
<p>Inducing general conceptual knowledge from observations is a crucial capability of generally intelligent agents.The capability supports a variety of intelligent behaviors such as operation in partially observable scenarios (where conceptual knowledge elaborates what is not seen), in language understanding (including ELP), in commonsense reasoning, as well in task execution.Analogical processing enables robust incremental induction from few examples and has been demonstrated as a key cognitive capability in humans.This paper explores how analogical processing can be integrated into the Soar cognitive architecture which is capable of flexible and contextual decision making and has been widely used to design complex intelligent agents.This paper paves the way for an exciting exploration of new kinds of intelligent behavior enabled by analogical processing.</p>
<p>Figure 1 :
1
Figure 1: System diagram for Advanced cognItive LEarning for Embodied compreheNsion (AILEEN)</p>
<p>Figure 2 :
2
Figure 2: (left) Simplified, partial working memory graph for the scene in Figure 1. Green colored symbols are generated in the visual module and yellow colored symbols are generated in the spatial module.Black colored symbols are internal to Soar and are used for driving behavior.(right) Concepts in semantic memory property: cylinder}}.Additionally, (1) has a reference to a spatial relationship: {rel1: {rel-name: left of, argument1: or1, argument2: or2}}.(2)has a reference to an action: {act1: {act-name: move, argument1: or1, argument2: or2, relation: left of} }.For this paper, we assume that the knowledge for this step is pre-encoded.2. Create a goal for grounding each reference.The goal of processing an object reference is to find a set of objects that satisfy the properties specified.It starts with first resolving properties.The process queries semantic memory for a percept that corresponds to various properties in the parse.If the knowledge in Figure2is assumed, property blue resolves to percept CVBlue, cone to CVCone, red to CVRed, and cylinder to CVCylinder.Using these percepts, AILEEN queries its scene to resolve object references.For or1, it finds an object that has both CVBlue and CVCone in its description.Let or1 resolve to o1 and or2 to o2 where o1 and o1 are identifiers of objects visible on the scene.The goal of processing a relation reference is to find a set of spatial calculi that correspond to the name specified.If knowledge in Figure2is assumed, rel1 in (1) is resolved to a conjunction of qsrs e(a1,a2)âˆ§dc(a1,a2) i.e, object mapping to a1 should be east (in CDC) of a2 and they should be disconnected.Similarly, act1 in (2) resolves to a task goal which is a conjunction of qsrs w(a1,a2)âˆ§dc(a1,a2) 3. Compose all references: Use semantic constraints to resolve the full input.For (1) and (2) a1 is matched to to ar1 and consequently to o1.Similarly, a2 is resolved to o2 via ar2.Under certain circumstances (further explained in section 4.2), AILEEN attempts to act.AILEEN creates a task goal g1 in the working memory and searches for and executes a plan in the world.</p>
<p>Figure 4 :
4
Figure 4: Working memory graph corresponding to scene in Figure 1 now enhanced with concept symbols (blue).Each concept symbol refers to a generalization context in the concept memory.The graph is enhanced based on inferences supported by analogical processing.</p>
<p>Figure 6 :
6
Figure 6: (left) Learning curve for visual concepts averaged from 10 trials.A trial includes lessons from 5 colors and 4 and shapes = 20 unique objects.Lessons include reference only to shape and color and shape.(right) Examples of an inform lesson (I) and generality (G) and specificity (S) exam lessons.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: (left) Learning curve for spatial concepts averaged from 10 trials.A trial includes lessons about 4 types of binary relations defined over 20 unique objects.(right) Examples of an inform lesson (I) and generality (G) and specificity (S) exam lessons.</p>
<p>Table 1 :
1
Predicate calculus representation for the world scene in Figure1corresponding to Soar's working memory graph in Figure2.CVCyl is short for the CVCylinder symbol and H for holdsIn predicate
Current world sceneEpisodic traceobjectsrelationsT0T1T2(isa o1 CVBlue) (e o1 o2)(H T0 (dc o1 o2)) (H T1 (held O1)) (H T2 (w o1 o2))(isa o1 CVCone) (dc o1 o2) (H T0 (e o1 o2))......(isa o2 CVRed)(w o2 o1)......(final T2 T1)(isa o2 CVCyl)(dc o2 o1) (isa T0 start)(after T1 T0)(after T2 T1)</p>
<p>Table 2 :
2
Terms used in analogical processing, their definitions, and values in AILEEN's concept memory
TermDefinitionSimilarityThe score representing the quality of an analogical match, degree of overlapCorrespondenceA one-to-one alignment between the compared representationsCandidate Inference Inferences resulting from the correspondences of the analogyThresholdDefinitionValueAssimilationScore required to include a new example into a generalization instead of stor-0.01ing it as an exampleProbabilityOnly facts exceeding this value are considered part of the concept.0.6MatchScore required to consider that an inference is applicable in a given scene0.75
AcknowledgementsThe authors thank Ken Forbus and Irina Rabkina for their support in developing the SME and SAGE models described in this paper.The work presented in this paper was supported by the DARPA GAILA program under award number HR00111990056 and an AFOSR grant on Levels of Learning a sub-contract from University of Michigan (PI: John Laird, FA9550-18-1-0180).Any opinions, findings, conclusions, or recommendations expressed are of the authors' and do not necessarily reflect the views of the DARPA, AFOSR, Army Research Office, or US government.
How can the human mind occur in the physical universe?. J R Anderson, 2009Oxford University Press</p>
<p>Human-like sketch object recognition via analogical learning. K Chen, I Rabkina, M D Mclure, K D Forbus, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2019</p>
<p>N Derbinsky, J E Laird, Efficiently implementing episodic memory. International Conference on Case-Based Reasoning. Springer2009</p>
<p>Towards efficiently supporting large symbolic declarative memories. N Derbinsky, J E Laird, B Smith, Proceedings of the 10th international conference on cognitive modeling. the 10th international conference on cognitive modelingCiteseer2010</p>
<p>Knowledge acquisition via incremental conceptual clustering. Machine learning. D H Fisher, 1987</p>
<p>Extending sme to handle largescale cognitive modeling. K D Forbus, R W Ferguson, A Lovett, D Gentner, Cognitive Science. 412017</p>
<p>Qsrlib: A software for online acquisition of qsrs from video. Y Gatsoulis, 2016</p>
<p>Why weÃ¢ È‚ Å¹re so smart. D Gentner, Language in mind: Advances in the study of language and thought. 2003</p>
<p>Indexical understanding of instructions. A M Glenberg, D A Robertson, 1999Discourse processes</p>
<p>Interactive task learning: Humans, robots, and agents acquiring new tasks through natural interactions. K A Gluck, J E Laird, 2019MIT Press26</p>
<p>Towards a comprehensive standard model of human-like minds. T R Hinrichs, K D Forbus, AAAI Fall Symposium Series. 2017. 2017</p>
<p>Interactive task learning for simple games. J R Kirk, J E Laird, Advances in Cognitive Systems. 2014</p>
<p>Using analogical model formulation with sketches to solve bennett mechanical comprehension test problems. M Klenk, K Forbus, E Tomai, H Kim, Journal of Experimental &amp; Theoretical Artificial Intelligence. 232011</p>
<p>The Soar Cognitive Architecture. J E Laird, 2012</p>
<p>Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. J E Laird, C Lebiere, P S Rosenbloom, 201738AI Magazine</p>
<p>Machine learning and concept formation. P Langley, Machine Learning. 19872</p>
<p>Using analogy to model spatial language use and multimodal knowledge capture. K Lockwood, 2009Doctor of Philosophy</p>
<p>Trestle: Incremental learning in structured domains using partial matching and categorization. C J Maclellan, E Harpstead, V Aleven, K R Koedinger, Proceedings of the 3rd Annual Conference on Advances in Cognitive Systems-ACS. the 3rd Annual Conference on Advances in Cognitive Systems-ACS2015</p>
<p>Extending analogical generalization with near-misses. M D Mclure, S E Friedman, K D Forbus, Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015</p>
<p>Interactively learning a blend of goal-based and procedural tasks. A Mininger, J E Laird, Thirty-Second AAAI Conference on Artificial Intelligence. 2018</p>
<p>Learning goal-oriented hierarchical tasks from situated interactive instruction. S Mohan, J Laird, Twenty-Eighth AAAI Conference on Artificial Intelligence. 2014</p>
<p>Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds. S Mohan, A Mininger, J Laird, Advances in Cognitive Systems. 2014</p>
<p>Acquiring grounded representations of words with situated interactive instruction. S Mohan, A H Mininger, J R Kirk, J E Laird, Advances in Cognitive Systems. 2012</p>
<p>Inductive logic programming: Theory and methods. S Muggleton, L De Raedt, The Journal of Logic Programming. 191994</p>
<p>You only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Collagen: Applying collaborative discourse theory to human-computer interaction. C Rich, C L Sidner, N Lesh, AI magazine. 222001</p>
<p>The sigma cognitive architecture and system: Towards functionally elegant grand unification. P S Rosenbloom, A Demski, V Ustun, Journal of Artificial General Intelligence. 72016</p>
<p>Bayesian modeling of human concept learning. J B Tenenbaum, Advances in neural information processing systems. 1999</p>
<p>Concept formation in structured domains. K Thompson, P Langley, Concept formation. 1991</p>
<p>The oxford handbook of memory. E Tulving, F I Craik, 2005Oxford University Press</p>
<p>Instance-based online learning of deterministic relational action models. J Z Xu, J E Laird, Twenty-Fourth AAAI Conference on Artificial Intelligence. 2010</p>            </div>
        </div>

    </div>
</body>
</html>
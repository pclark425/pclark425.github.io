<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7414 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7414</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7414</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-247778598</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.15754v1.pdf" target="_blank">Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have shown that impressive zero-shot performance can be achieved through natural language prompts (Radford et al., 2019; Brown et al., 2020; Sanh et al., 2021). Creating an effective prompt, however, requires significant trial and error. That \textit{prompts} the question: how do the qualities of a prompt effects its performance? To this end, we collect and standardize prompts from a diverse range of tasks for use with tasks they were not designed for. We then evaluate these prompts across fixed multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance. We find that including the choices and using prompts not used during pre-training provide significant improvements. All experiments and code can be found https://github.com/gabeorlanski/zero-shot-cross-task.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7414.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7414.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChoicesPresent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Presence of answer choices in the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including the task's answer choices (the choice string) inside the natural-language prompt substantially improves zero-shot multiple-choice performance of T0 compared to prompts that omit the choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multitask prompted encoder-decoder model fine-tuned on many datasets with natural-language prompts (T0 family described in Sanh et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-task zero-shot evaluation on fixed multiple-choice datasets (8 datasets e.g., ANLI, AQuA, CB, CraigslistBargains, RTE, WiC, ...)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot prediction on multiple-choice problems where the set of choices is constant across examples; evaluated across eight different datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt that explicitly contains the answer choice string (e.g., 'Choices are: A) yes B) no').</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; prompts standardized via PromptSource; choices provided in a dedicated input field to keep how choices are presented constant across prompts; rank scoring with average log-likelihood decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Median Accuracy Rank (MAR) and unweighted multiclass F1 (MFR); also reported absolute accuracy improvements in paper text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>With choices: MAR = 33.12 (median rank across 95 prompts); paper reports 'prompts with the choices in them are 66.82% better' than those that leave choices out (accuracy improvement reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No choices: MAR = 52.25 (median rank when the choice string is left out)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>MAR improvement of 19.13 ranks (33.12 vs 52.25); paper reports ~66.82% better accuracy when choices included (as stated in the manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot evaluation with T0-3B; prompts collected and standardized across tasks (PromptSource); rank scoring using Average Log-Likelihood to select choices.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7414.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7414.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistinctChoiceFormatting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Presenting answer options as clearly delimited multiple distinct choices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When answer options are presented as distinct, clearly delimited choices (e.g., 'A) ... B) ... C) ...'), performance improves further relative to prompts where choices are present but not clearly separated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multitask prompt-finetuned encoder-decoder model (Sanh et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-task zero-shot evaluation on fixed multiple-choice datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multiple-choice classification across eight datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt with choices presented as multiple distinct, clearly delimited options (e.g., 'A) yes B) no C) maybe').</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / answer formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; delimitation/clear separators between choices; compared to non-delimited choice presentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Median Accuracy Rank (MAR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Distinct choices: MAR = 22.25 (median accuracy rank for prompts with clearly delimited choices).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Choices present but not clearly delimited: MAR = 36.00</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>MAR improvement of 13.75 ranks (22.25 vs 36.00) when choices are clearly delimited.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot T0-3B; prompts compared across the same set of evaluation tasks; choices normalized into a dedicated choice field.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7414.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7414.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExtraTaskNL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adding small amount of task-specific natural language to generalized prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending a small amount of task-specific natural-language text to a generalized prompt (to align inputs) increases average zero-shot accuracy and F1 across the evaluation tasks, though it can increase variance and sometimes decrease minimum performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-finetuned multitask encoder-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-task zero-shot evaluation on fixed multiple-choice datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multiple-choice classification across eight datasets; generalized prompts are adapted by adding small task-specific NL to align inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generalized natural-language prompt plus added task-specific NL (small text to better align fields), zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Generalized prompts standardized into CLASSIFICATION/ENTAILMENT/QA forms; extra text is a small amount of task-specific NL added to map dataset fields to prompt template inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (absolute) and unweighted multiclass F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across eight tasks: mean increase of +4.65 percentage points accuracy (median reported) and +2.34 percentage points unweighted multiclass F1 when adding extra task-specific NL.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No extra text (generalized prompt only): baseline used to compute above increases; Table 5 shows No Extra Text median MAR = 48.00 vs Extra Text median MAR = 46.75 (rank metrics) and absolute accuracy changes reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+4.65% absolute accuracy (median across evaluation tasks); +2.34% absolute unweighted multiclass F1; but minimum accuracy decreased by 4.21% and minimum F1 decreased by 13.90% in worst cases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot with T0-3B; prompts standardized and then augmented with small mapping text; rank scoring with average log-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7414.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7414.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptLength</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt length (number of tokens) effect on performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt length correlates with performance: mid-length prompts (approx. 14–24 tokens) performed best, while both longer (>=25 tokens) and very short (<14 tokens) prompts performed worse on median rank metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multitask prompt-finetuned encoder-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-task zero-shot evaluation on fixed multiple-choice datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multiple-choice classification across eight datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt length measured in tokens (various lengths compared).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt length</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts binned by token-length ranges; median accuracy rank (MAR) and median F1 rank (MFR) measured per length bucket; correlation analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Median Accuracy Rank (MAR) and Median F1 Rank (MFR); correlation with rank</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Best bucket [14,21): MAR = 28.50, MFR = 36.50; prompts with length >=25: median MAR = 50.25, MFR = 72.00; prompts length <14: median MAR = 47.75, MFR = 49.00. Correlation of length with rank: positive (length correlate 0.14 accuracy, 0.18 F1 in Table 7), indicating longer prompts tend to have worse rank.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Best bucket ([14,21) tokens) used as positive baseline: MAR = 28.50</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Prompts >=25 tokens worse by +21.75 MAR relative to best bucket (50.25 vs 28.50); very short prompts (<14) also worse than best bucket by +19.25 MAR (47.75 vs 28.50).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot T0-3B; tokenization counts per prompt; median ranks computed across 95 prompts and 8 evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7414.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7414.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeenVsUnseenPrompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of whether prompts were seen during T0 pre-training (training prompts) vs unseen prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts that were not used during T0's pre-training ('unseen prompts') tended to transfer better in cross-task zero-shot evaluation than prompts that were used during training; unseen prompts achieved better median ranks and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multitask prompt-finetuned encoder-decoder model trained on collections of prompts/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-task zero-shot evaluation on fixed multiple-choice datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multiple-choice classification across eight datasets; prompts are labeled based on whether they were included among T0's pretraining prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts categorized as 'training prompts' (used during T0 training) or 'unseen prompts' (not used during training).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt provenance / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts were partitioned by whether they match prompts used in T0 pretraining; median accuracy rank (MAR) and median F1 rank (MFR) compared between groups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Median Accuracy Rank (MAR) and Median F1 Rank (MFR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Training prompts: median MAR = 50.25, MFR = 55.75; Unseen prompts: median MAR = 42.00, MFR = 36.50. Paper also states 'prompts not used in pre-training are 19.64% better than those that were.'</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Training prompts baseline: MAR = 50.25, MFR = 55.75</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Unseen prompts improved MAR by 8.25 ranks (50.25 -> 42.00) and improved MFR by 19.25 ranks (55.75 -> 36.50); stated ~19.64% better (paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot T0-3B; prompts labeled by provenance per PromptSource and dataset lists; median ranks reported across 8 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7414.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7414.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UsePromptVsNoPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of using any natural-language prompt versus no prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a natural-language prompt (even generalized or cross-task prompts) yields better zero-shot performance on all eight evaluation tasks compared to providing no prompt at all.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multitask prompt-finetuned encoder-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-task zero-shot evaluation on fixed multiple-choice datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multiple-choice classification across eight datasets; comparisons include 'No Prompt' baseline vs prompts borrowed from other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>No-prompt baseline (raw input without a natural-language prompt) vs natural-language prompt (generalized or original prompt from some task).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / presence of prompt</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>No-prompt baseline vs applying varied prompts; rank scoring used; the prompts include both those original to the task and those adapted from other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (absolute) and rank statistics across prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper states: 'there is no task out of the eight used for evaluation where not using a prompt has the best performance' (i.e., prompts always improved performance relative to 'No Prompt'). Example: ANLI R1 No Prompt accuracy = 34.15 vs an unseen prompt (ANLI) accuracy = 37.60.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No Prompt baseline: per-task example ANLI R1 accuracy = 34.15 (Table entries available per dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Improvement varies by task and prompt; example ANLI R1 improvement +3.45 absolute accuracy (37.60 vs 34.15) for an unseen prompt; overall prompts consistently outperformed no-prompt across all eight tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot T0-3B; evaluation on validation splits of datasets; rank scoring with average log-likelihood used for choice selection.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7414.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7414.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AvgLogLikelihoodRankScoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average Log-Likelihood rank scoring for multiple-choice outputs (length-normalized scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To avoid penalizing longer choice strings in multiple-choice tasks, the paper uses Average Log-Likelihood over tokens for ranking candidate choices rather than raw sequence probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (T0-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multitask prompt-finetuned encoder-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fixed multiple-choice tasks where choices differ in length</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot multiple-choice classification with candidate choices of varying token lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice selection via decoding; candidates are scored by average per-token log-probability (Average Log-Likelihood) rather than raw joint probability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>decoding / scoring method</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Rank scoring using Average Log-Likelihood to reduce length bias when comparing choices of unequal lengths (cites Holtzman et al., 2021); employed for all multiple-choice evaluations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline raw rank-scoring without length normalization would be argmax P(choice|context) across token sequence (paper avoids this due to length bias).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; Average Log-Likelihood computed per candidate choice token sequence; used across all experiments to determine predicted choice.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Finetuned language models are zero-shot learners <em>(Rating: 2)</em></li>
                <li>How many data points is a prompt worth? <em>(Rating: 2)</em></li>
                <li>Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections <em>(Rating: 2)</em></li>
                <li>Surface form competition: Why the highest probability answer isn't always right <em>(Rating: 2)</em></li>
                <li>Learning from task descriptions <em>(Rating: 1)</em></li>
                <li>Cross-task generalization via natural language crowdsourcing instructions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7414",
    "paper_id": "paper-247778598",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "ChoicesPresent",
            "name_full": "Presence of answer choices in the prompt",
            "brief_description": "Including the task's answer choices (the choice string) inside the natural-language prompt substantially improves zero-shot multiple-choice performance of T0 compared to prompts that omit the choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (T0-3B)",
            "model_description": "A multitask prompted encoder-decoder model fine-tuned on many datasets with natural-language prompts (T0 family described in Sanh et al., 2021).",
            "model_size": "3B",
            "task_name": "Cross-task zero-shot evaluation on fixed multiple-choice datasets (8 datasets e.g., ANLI, AQuA, CB, CraigslistBargains, RTE, WiC, ...)",
            "task_description": "Zero-shot prediction on multiple-choice problems where the set of choices is constant across examples; evaluated across eight different datasets.",
            "problem_format": "Natural-language prompt that explicitly contains the answer choice string (e.g., 'Choices are: A) yes B) no').",
            "format_category": "prompt style",
            "format_details": "Zero-shot; prompts standardized via PromptSource; choices provided in a dedicated input field to keep how choices are presented constant across prompts; rank scoring with average log-likelihood decoding.",
            "performance_metric": "Median Accuracy Rank (MAR) and unweighted multiclass F1 (MFR); also reported absolute accuracy improvements in paper text",
            "performance_value": "With choices: MAR = 33.12 (median rank across 95 prompts); paper reports 'prompts with the choices in them are 66.82% better' than those that leave choices out (accuracy improvement reported in text).",
            "baseline_performance": "No choices: MAR = 52.25 (median rank when the choice string is left out)",
            "performance_change": "MAR improvement of 19.13 ranks (33.12 vs 52.25); paper reports ~66.82% better accuracy when choices included (as stated in the manuscript).",
            "experimental_setting": "Zero-shot evaluation with T0-3B; prompts collected and standardized across tasks (PromptSource); rank scoring using Average Log-Likelihood to select choices.",
            "statistical_significance": null,
            "uuid": "e7414.0"
        },
        {
            "name_short": "DistinctChoiceFormatting",
            "name_full": "Presenting answer options as clearly delimited multiple distinct choices",
            "brief_description": "When answer options are presented as distinct, clearly delimited choices (e.g., 'A) ... B) ... C) ...'), performance improves further relative to prompts where choices are present but not clearly separated.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (T0-3B)",
            "model_description": "Multitask prompt-finetuned encoder-decoder model (Sanh et al., 2021).",
            "model_size": "3B",
            "task_name": "Cross-task zero-shot evaluation on fixed multiple-choice datasets",
            "task_description": "Zero-shot multiple-choice classification across eight datasets.",
            "problem_format": "Natural-language prompt with choices presented as multiple distinct, clearly delimited options (e.g., 'A) yes B) no C) maybe').",
            "format_category": "prompt style / answer formatting",
            "format_details": "Zero-shot; delimitation/clear separators between choices; compared to non-delimited choice presentation.",
            "performance_metric": "Median Accuracy Rank (MAR)",
            "performance_value": "Distinct choices: MAR = 22.25 (median accuracy rank for prompts with clearly delimited choices).",
            "baseline_performance": "Choices present but not clearly delimited: MAR = 36.00",
            "performance_change": "MAR improvement of 13.75 ranks (22.25 vs 36.00) when choices are clearly delimited.",
            "experimental_setting": "Zero-shot T0-3B; prompts compared across the same set of evaluation tasks; choices normalized into a dedicated choice field.",
            "statistical_significance": null,
            "uuid": "e7414.1"
        },
        {
            "name_short": "ExtraTaskNL",
            "name_full": "Adding small amount of task-specific natural language to generalized prompts",
            "brief_description": "Appending a small amount of task-specific natural-language text to a generalized prompt (to align inputs) increases average zero-shot accuracy and F1 across the evaluation tasks, though it can increase variance and sometimes decrease minimum performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (T0-3B)",
            "model_description": "Prompt-finetuned multitask encoder-decoder model.",
            "model_size": "3B",
            "task_name": "Cross-task zero-shot evaluation on fixed multiple-choice datasets",
            "task_description": "Zero-shot multiple-choice classification across eight datasets; generalized prompts are adapted by adding small task-specific NL to align inputs.",
            "problem_format": "Generalized natural-language prompt plus added task-specific NL (small text to better align fields), zero-shot.",
            "format_category": "prompt style",
            "format_details": "Generalized prompts standardized into CLASSIFICATION/ENTAILMENT/QA forms; extra text is a small amount of task-specific NL added to map dataset fields to prompt template inputs.",
            "performance_metric": "Accuracy (absolute) and unweighted multiclass F1",
            "performance_value": "Across eight tasks: mean increase of +4.65 percentage points accuracy (median reported) and +2.34 percentage points unweighted multiclass F1 when adding extra task-specific NL.",
            "baseline_performance": "No extra text (generalized prompt only): baseline used to compute above increases; Table 5 shows No Extra Text median MAR = 48.00 vs Extra Text median MAR = 46.75 (rank metrics) and absolute accuracy changes reported in text.",
            "performance_change": "+4.65% absolute accuracy (median across evaluation tasks); +2.34% absolute unweighted multiclass F1; but minimum accuracy decreased by 4.21% and minimum F1 decreased by 13.90% in worst cases.",
            "experimental_setting": "Zero-shot with T0-3B; prompts standardized and then augmented with small mapping text; rank scoring with average log-likelihood.",
            "statistical_significance": null,
            "uuid": "e7414.2"
        },
        {
            "name_short": "PromptLength",
            "name_full": "Prompt length (number of tokens) effect on performance",
            "brief_description": "Prompt length correlates with performance: mid-length prompts (approx. 14–24 tokens) performed best, while both longer (&gt;=25 tokens) and very short (&lt;14 tokens) prompts performed worse on median rank metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (T0-3B)",
            "model_description": "Multitask prompt-finetuned encoder-decoder model.",
            "model_size": "3B",
            "task_name": "Cross-task zero-shot evaluation on fixed multiple-choice datasets",
            "task_description": "Zero-shot multiple-choice classification across eight datasets.",
            "problem_format": "Natural-language prompt length measured in tokens (various lengths compared).",
            "format_category": "prompt style / prompt length",
            "format_details": "Prompts binned by token-length ranges; median accuracy rank (MAR) and median F1 rank (MFR) measured per length bucket; correlation analysis reported.",
            "performance_metric": "Median Accuracy Rank (MAR) and Median F1 Rank (MFR); correlation with rank",
            "performance_value": "Best bucket [14,21): MAR = 28.50, MFR = 36.50; prompts with length &gt;=25: median MAR = 50.25, MFR = 72.00; prompts length &lt;14: median MAR = 47.75, MFR = 49.00. Correlation of length with rank: positive (length correlate 0.14 accuracy, 0.18 F1 in Table 7), indicating longer prompts tend to have worse rank.",
            "baseline_performance": "Best bucket ([14,21) tokens) used as positive baseline: MAR = 28.50",
            "performance_change": "Prompts &gt;=25 tokens worse by +21.75 MAR relative to best bucket (50.25 vs 28.50); very short prompts (&lt;14) also worse than best bucket by +19.25 MAR (47.75 vs 28.50).",
            "experimental_setting": "Zero-shot T0-3B; tokenization counts per prompt; median ranks computed across 95 prompts and 8 evaluation tasks.",
            "statistical_significance": null,
            "uuid": "e7414.3"
        },
        {
            "name_short": "SeenVsUnseenPrompts",
            "name_full": "Effect of whether prompts were seen during T0 pre-training (training prompts) vs unseen prompts",
            "brief_description": "Prompts that were not used during T0's pre-training ('unseen prompts') tended to transfer better in cross-task zero-shot evaluation than prompts that were used during training; unseen prompts achieved better median ranks and F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (T0-3B)",
            "model_description": "Multitask prompt-finetuned encoder-decoder model trained on collections of prompts/datasets.",
            "model_size": "3B",
            "task_name": "Cross-task zero-shot evaluation on fixed multiple-choice datasets",
            "task_description": "Zero-shot multiple-choice classification across eight datasets; prompts are labeled based on whether they were included among T0's pretraining prompts.",
            "problem_format": "Natural-language prompts categorized as 'training prompts' (used during T0 training) or 'unseen prompts' (not used during training).",
            "format_category": "prompt provenance / prompt style",
            "format_details": "Prompts were partitioned by whether they match prompts used in T0 pretraining; median accuracy rank (MAR) and median F1 rank (MFR) compared between groups.",
            "performance_metric": "Median Accuracy Rank (MAR) and Median F1 Rank (MFR)",
            "performance_value": "Training prompts: median MAR = 50.25, MFR = 55.75; Unseen prompts: median MAR = 42.00, MFR = 36.50. Paper also states 'prompts not used in pre-training are 19.64% better than those that were.'",
            "baseline_performance": "Training prompts baseline: MAR = 50.25, MFR = 55.75",
            "performance_change": "Unseen prompts improved MAR by 8.25 ranks (50.25 -&gt; 42.00) and improved MFR by 19.25 ranks (55.75 -&gt; 36.50); stated ~19.64% better (paper text).",
            "experimental_setting": "Zero-shot T0-3B; prompts labeled by provenance per PromptSource and dataset lists; median ranks reported across 8 tasks.",
            "statistical_significance": null,
            "uuid": "e7414.4"
        },
        {
            "name_short": "UsePromptVsNoPrompt",
            "name_full": "Effect of using any natural-language prompt versus no prompt",
            "brief_description": "Using a natural-language prompt (even generalized or cross-task prompts) yields better zero-shot performance on all eight evaluation tasks compared to providing no prompt at all.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0 (T0-3B)",
            "model_description": "Multitask prompt-finetuned encoder-decoder model.",
            "model_size": "3B",
            "task_name": "Cross-task zero-shot evaluation on fixed multiple-choice datasets",
            "task_description": "Zero-shot multiple-choice classification across eight datasets; comparisons include 'No Prompt' baseline vs prompts borrowed from other tasks.",
            "problem_format": "No-prompt baseline (raw input without a natural-language prompt) vs natural-language prompt (generalized or original prompt from some task).",
            "format_category": "prompt style / presence of prompt",
            "format_details": "No-prompt baseline vs applying varied prompts; rank scoring used; the prompts include both those original to the task and those adapted from other tasks.",
            "performance_metric": "Accuracy (absolute) and rank statistics across prompts",
            "performance_value": "Paper states: 'there is no task out of the eight used for evaluation where not using a prompt has the best performance' (i.e., prompts always improved performance relative to 'No Prompt'). Example: ANLI R1 No Prompt accuracy = 34.15 vs an unseen prompt (ANLI) accuracy = 37.60.",
            "baseline_performance": "No Prompt baseline: per-task example ANLI R1 accuracy = 34.15 (Table entries available per dataset)",
            "performance_change": "Improvement varies by task and prompt; example ANLI R1 improvement +3.45 absolute accuracy (37.60 vs 34.15) for an unseen prompt; overall prompts consistently outperformed no-prompt across all eight tasks.",
            "experimental_setting": "Zero-shot T0-3B; evaluation on validation splits of datasets; rank scoring with average log-likelihood used for choice selection.",
            "statistical_significance": null,
            "uuid": "e7414.5"
        },
        {
            "name_short": "AvgLogLikelihoodRankScoring",
            "name_full": "Average Log-Likelihood rank scoring for multiple-choice outputs (length-normalized scoring)",
            "brief_description": "To avoid penalizing longer choice strings in multiple-choice tasks, the paper uses Average Log-Likelihood over tokens for ranking candidate choices rather than raw sequence probability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T0 (T0-3B)",
            "model_description": "Multitask prompt-finetuned encoder-decoder model.",
            "model_size": "3B",
            "task_name": "Fixed multiple-choice tasks where choices differ in length",
            "task_description": "Zero-shot multiple-choice classification with candidate choices of varying token lengths.",
            "problem_format": "Multiple-choice selection via decoding; candidates are scored by average per-token log-probability (Average Log-Likelihood) rather than raw joint probability.",
            "format_category": "decoding / scoring method",
            "format_details": "Rank scoring using Average Log-Likelihood to reduce length bias when comparing choices of unequal lengths (cites Holtzman et al., 2021); employed for all multiple-choice evaluations in the paper.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": "Baseline raw rank-scoring without length normalization would be argmax P(choice|context) across token sequence (paper avoids this due to length bias).",
            "performance_change": null,
            "experimental_setting": "Zero-shot; Average Log-Likelihood computed per candidate choice token sequence; used across all experiments to determine predicted choice.",
            "statistical_significance": null,
            "uuid": "e7414.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Finetuned language models are zero-shot learners",
            "rating": 2,
            "sanitized_title": "finetuned_language_models_are_zeroshot_learners"
        },
        {
            "paper_title": "How many data points is a prompt worth?",
            "rating": 2,
            "sanitized_title": "how_many_data_points_is_a_prompt_worth"
        },
        {
            "paper_title": "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
            "rating": 2,
            "sanitized_title": "adapting_language_models_for_zeroshot_learning_by_metatuning_on_dataset_and_prompt_collections"
        },
        {
            "paper_title": "Surface form competition: Why the highest probability answer isn't always right",
            "rating": 2,
            "sanitized_title": "surface_form_competition_why_the_highest_probability_answer_isnt_always_right"
        },
        {
            "paper_title": "Learning from task descriptions",
            "rating": 1,
            "sanitized_title": "learning_from_task_descriptions"
        },
        {
            "paper_title": "Cross-task generalization via natural language crowdsourcing instructions",
            "rating": 1,
            "sanitized_title": "crosstask_generalization_via_natural_language_crowdsourcing_instructions"
        }
    ],
    "cost": 0.014700249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting</p>
<p>Gabriel Orlanski go533@nyu.edu 
Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting</p>
<p>Large language models have shown that impressive zero-shot performance can be achieved through natural language prompts (Radford  et al., 2019; Brown et al., 2020; Sanh et al.,  2021). Creating an effective prompt, however, requires significant trial and error. That prompts the question: how do the qualities of a prompt effects its performance? To this end, we collect and standardize prompts from a diverse range of tasks for use with tasks they were not designed for. We then evaluate these prompts across fixed multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance. We find that including the choices and using prompts not used during pre-training provide significant improvements. All experiments and code can be found https://github.com/gabeorlanski/zeroshot-cross-task. . 2021. Evaluating large language models trained on code. ArXiv, abs/2107.03374.</p>
<p>Introduction</p>
<p>Recent work has shown that using a natural language (NL) prompt with pre-trained language models (LM) significantly improves performance in few-shot and zero-shot settings (Brown et al., 2020;Schick and Schütze, 2021b), to the point where they can be worth 100s of data points (Scao and Rush, 2021). Further, T5 (Raffel et al., 2020) showed that simple prompts and reformulating NLP tasks as text-to-text performs well on a wide range of tasks. Recent models such as FLAN (Wei et al., 2021b) and T0 (Sanh et al., 2021) demonstrate that multi-task training a large LM with prompts results in improved zero-shot performance on a wide range of tasks. However, manually designing a prompt is a non-trivial task due to the trial-and-error nature of the task (Jiang et al., 2020;Shin et al., 2021). Some works focus on "prompt programming"-best practices for designing prompts (Reynolds and McDonell, 2021;Liu et al., 2021a). While others have looked towards continuous "soft-prompts"-random vectors added to the input sequence, which are then fine-tuned (Zhong et al., 2021b;Qin and Eisner, 2021). However, recent work has revealed that these prompts are susceptible to Figure 1: Overview of the approach. For a given example from a dataset (leftmost box) we use a prompt from a different task to perform zero shot predictions with T0-3B (Sanh et al., 2021). The bolded text in the example represents its choices. minor perturbations (Mishra et al., 2021).</p>
<p>Motivated by this, we aim to conduct a quantitative analysis of what affects a prompt's performance. We evaluate T0-3B (Sanh et al., 2021) on generalized prompts from a wide range of tasks with eight datasets to provide a quantitative analysis of how the qualitative aspects of a prompt effect its performance.</p>
<p>We collected 95 prompts across 20 tasks and evaluated each on eight datasets. We find that using a prompt performs better for every evaluation task than not using a prompt. Further, we find that the set of prompts with the highest performance is not the ones designed for the specific task for seven of the eight datasets. In our ablations, we find that adding a small amount of task-specific NL to the generalized prompt increases performance by a median of 4.65%. Finally, we find that prompts with the choices present outperform those that do not and that presenting the options as multiple distinct choices further improves the results. Additionally, the longer a prompt is, the worse it performs.</p>
<p>Methodology</p>
<p>Our overall approach is detailed in Figure 1. Given a downstream multiple-choice task T o = {(x 1 , y 1 ), . . . , (x n , y n )}, evaluate how well T0 (Sanh et al., 2021) performs when using a prompt P d designed for a different task T d .</p>
<p>Fixed Choice Tasks</p>
<p>For the purposes of this paper, we limit the scope of the tasks we look at to be only multiple choice tasks whose choices are constant across all examples. Thus, for
T o = {(x 1 , y 1 ), . . . , (x n , y n )}, every y i ∈ C o where C o = {C 1 , . . . , C c } with lengths = { 1 , . . . , c }.
To make a prediction for the example point x i , we follow Holtzman et al. (2021) and use rank scoring: taking the choice with the highest probability as defined by
argmax Cj ∈Co j k=1 P (C k j |x i , C 1 j . . . C k−1 j )(1)
However, the lengths in are not guaranteed to be the same and thus Equation 1 will unintentionally penalize longer choices (Brown et al., 2020;Holtzman et al., 2021). We thus follow prior work and take the choice with the highest Average Log-Likelihood
argmax Cj ∈Co j k=1 log[P (C k j |x i , C 1 j . . . C k−1 j )] j(2)</p>
<p>Generalized Prompts</p>
<p>We use the PromptSource 1 framework proposed by Sanh et al. (2021) for templates as it provides a standardized format for managing prompts. We standardize the input fields and answer formats across all tasks such that they fell into three general categories: CLASSIFICATION, ENTAILMENT, and QUESTION ANSWERING (QA). Table 1 displays an example of what the generalization would look like for an ENTAILMENT prompt. To use a prompt with a task that does not have the same number of inputs, we add additional task specific NL to better align the inputs. To use the example prompt from Table 1 with a sentiment classification task, we would map the input text from the task to the premise field and pass "what is the sentiment" as the hypothesis. In prompts where answers choices are present, we replace them with an additional input field for the choice string(i.e. "yes, no, or maybe") to hold how the choices are presented constant across all prompts. A detailed breakdown of the tasks used for the generalized prompts can be found in Table 3.</p>
<p>Experimental Setup</p>
<p>Datasets</p>
<p>For all datasets, we use the most recent version on HuggingFace (Wolf et al., 2020). Every evaluation is done using the validation split as per Sanh et al.   (He et al., 2018). Descriptions for these two tasks can be found in Appendix B.</p>
<p>For the generalized prompts, we collect 86 prompts across 19 distinct tasks. In addition to these, we also include 12 prompts with no additional NL for each of the three categories for 4 different ablations. More details can be found in Appendix A.</p>
<p>Model and Metrics</p>
<p>We evaluate the performance of the 3B parameter T0, and T5 (Raffel et al., 2020) models with the Hugging-Face implementation (Wolf et al., 2020) as we were limited to a single RTX Titan 24GB card. Following prior works (Sanh et al., 2021;Brown et al., 2020;Holtzman et al., 2021), we report the accuracy and F1 scores on each of the eight datasets.</p>
<p>As each task will have different mean metrics, we cannot compare the raw accuracy and F1 scores across tasks. For a given prompt, we calculate the median accuracy and F1 ranks compared to the 95 other prompts for all evaluation tasks. As the rank is ascending, lower values for median accuracy rank (MAR) and median F1 rank (MFR) indicate a better performing prompt.</p>
<p>Results</p>
<p>Baselines On New Tasks</p>
<p>As we evaluate T0-3B on two new tasks, we first want to gauge how the model performs as shown in both Figure 2 and Table 4. We follow Sanh et al. (2021) in using rank scoring without length normalization as defined by Equation 1 and find that for both AQuA and CraigslistBargains, the base T5 model performs better than T0. However, T0's unweighted multi-class F1 is better than that of T5 on both tasks, indicating that T5 achieves a higher score due to only predicting a subset of the choices. Figure 3 provides further evidence for this hypothesis. In both AQuA and CraigslistBargains, T0 more evenly distributes its predictions across the possible choices where as T5 heavily favors a subset of the choices. Thus, the disparity in the accuracy is likely a result of class imbalance in the evaluation datasets in which T5 is 'lucky' in heavily predicting the class that was more populous. We consider this to be 'luck' as T5 outperforming T0 only occurs in only two of the datasets we examined.</p>
<p>Generalized Prompts</p>
<p>We report the results of the cross-task evaluation in Table 2. We find that the only task in which the original 2 prompt performs best is ANLI R2 with an accuracy of 34.70. Conversely, the worst performing prompts for the AQuA task were its original prompts with an accuracy of 17.32. Furthermore, we find that there is no task out of the eight used for evaluation where not using a prompt has the best performance. 2 "math problem" has a similar meaning in sentences A and B. "A", "B", "C", "D" or "E"?  Table 2: Median Accuracy when using modified prompts for cross task zero-shot evaluation. Bolded entries are prompts for the original task. Green Cells and Red Cells are the best and worst performing tasks for a column respectively. Rank is the median rank of prompts from this task out of 95 total prompts. ANLI and CB both use the same prompts for their original task prompts per PromptSource. Some tasks are left out for clarity. The full table can be found in Table 6.</p>
<p>We report how the added NL discussed in subsection 2.2 effects the zero-shot performance in Table 5. Across the eight evaluation tasks, adding some task specific text leads to an average increase of 4.65% in the accuracy and a 2.34% increase to the unweighted multiclass F1. However, there was also a decrease of 4.21% and 13.90% to minimum accuracy and unweighted multiclass F1 respectively. This implies that the added extra text helps to better amplify the negative and positive elements of a prompt. Table 5 displays the rank statistics across multiple ablations and Table 7 displays the correlations of a prompt's qualities with their rank. In prompts which have choices, the MAR is 33.12 compared to 52.25 when the choices are left out. However, the range as indicated by the Q1 and Q3 MARs is significantly larger when the choices are included, implying that adding the choice string causes high variance.</p>
<p>Qualitative Analysis of Prompts</p>
<p>We also find that when the choices are presented as multiple distinct choices 3 the MAR is further improves 3 Presented with a clear delimiters/separation. For example A) yes B) no C) maybe to 22.25. In comparison, the prompts with choices that are not in this format have a MAR of 36.00. These results provide further evidence to the findings from Wei et al. (2021a) that clearly distinguishing the options in a prompt improves performance.</p>
<p>Next, we find that the median rank of prompts used in training is 50.25 compared to 42.00 of the unseen prompts. The F1 scores display a similar pattern with training prompts having a MFR of 55.75 while unseen prompts have a median of 36.50. Although this implies that prompts that share tokens with those used for training will perform worse, we find that there is no significant correlation between the number of tokens a prompt P shares with those used in training and its rank.</p>
<p>Finally, we find that longer prompts have a slightly negative impact on the performance of a prompt. Figure 4 shows that, with the 95 prompts used across eight tasks, the best performing prompts are those whose length is in the range [14, 21) as their MAR is 28.50 and MFR is 36.50. The Q1 values are 18.00 and 15.38, respectively. In comparison, we find that prompts with lengths with lengths ≥ 25 have a median MAR of 50.25 and MFR of 72.00, indicating a negative impact on per-formance. Surprisingly, we find that prompts whose lengths are &lt; 14 have a median MAR of 47.75 and MFR of 49.00. While this is a negative impact on performance, it is not as large as that in longer prompts.</p>
<p>Related Works</p>
<p>Pre-trained Language Models In the past few years, large pre-trained models have rose to prominence due to their strong performance on a wide range of NLP tasks (Radford et al., 2019;Brown et al., 2020;Lewis et al., 2020). In particular, T5 (Raffel et al., 2020) explored transferred learning for large LMs by transforming all NLP problems to a text-to-text format. Beyond natural language, these models have also excelled at tasks in other modalities such as code generation (Austin et al., 2021;Chen et al., 2021;Orlanski and Gittens, 2021), incontext learning (Min et al., 2021;Zhong et al., 2021a), and semantic parsing (Rongali et al., 2020). One aspect of large LMs is that they perform well in zero-shot settings (Radford et al., 2019;Brown et al., 2020;Vu et al., 2020).</p>
<p>NL Prompting A drawback of these large LMs is that their size makes it costly to fine-tune them. This lead to the rise of the "pre-train, prompt, and predict" paradigm in which a downstream tasks are modified to resemble those used in training through the use of NL prompts (Liu et al., 2021b). These prompts have improved fewshot and zero-shot performance across a vast number of models and tasks (Brown et al., 2020;Schick and Schütze, 2021b,a;Mishra et al., 2021;Scao and Rush, 2021;Shin et al., 2020). Recent models such as FLAN (Wei et al., 2021b) and T0 (Sanh et al., 2021) have shown that even better zero-shot performance can be achieved through using a multi-task pre-training objectives with a diverse set of prompts.</p>
<p>Conclusion</p>
<p>In this paper, we examined T0's performance on a range of fixed multiple-choice tasks. We find that T0 does worse than T5 on two unseen complex tasks. Next, we evaluated how the performance of a prompt transfers between tasks. Our results show that using a prompt performs consistently better than not using any prompt. We conclude with a quantitative analysis of what aspects of a prompt affect its performance. We find that prompts with the choices in them are 66.82% better than those that leave the choices out. Next, we find that prompts not used in pre-training are 19.64% better than those that were. Finally, we find that prompts whose length are between 14 and 24 perform better than both longer and shorter prompts. Further work should examine prompt transfer with larger models while also expanding the number of prompts and tasks used.   Table 7: Correlations with metric rank for a given prompt quality. Per the definition of rank, a lower score is better and therefore a negative correlation indicates a quality improves performance. Length is measured as the raw number of tokens in a prompt.   Table 6: Median Accuracy when using modified prompts for cross task zero-shot evaluation. Bolded entries are prompts for the original task. Green Cells and Red Cells are the best and worst performing tasks for a column respectively. Rank is the median rank of prompts from this task out of 95 total prompts. ANLI and CB both use the same prompts for their original task prompts per PromptSource.</p>
<p>Figure 4: Accuracy and F1 rank compared with the number of tokens in the prompt. The tick value is the lower bound of the range. p=The number of prompts that fall into that respective range.</p>
<p>CraigslistBargains</p>
<p>Task Prompt Original
PromptSentence A: {{sentence1}} Sentence B: {{sentence2}} "{{word}}" has a similar meaning in sentences A and B. True or False? Generalized Sentence A: {{premise}} Sentence B: {{hypothesis}} "{{domain}}" has a similar meaning in sentences A and B. {{ choice string }}? Sentence A: What is 2+2? Sentence B: Choices are: ∞, -10, fish, 4,Example </p>
<p>√ </p>
<p>Table 1 :
1Sample prompt from WordsInContext Task (Pilehvar and osé Camacho-Collados, 2018) and its generalized form. Each {{ }} represents an input from the dataset. The colors are the alignment of inputs.ANLI R1 ANLI R2 ANLI R3 AQuA 
CB 
Craigslist RTE 
WiC 
Rank </p>
<p>No Prompt 
34.15 
33.35 
33.42 
26.77 24.11 
16.83 
59.57 50.24 46.25 </p>
<p>Unseen 
Prompts </p>
<p>ANLI 
37.60 
34.70 
34.08 
25.95 32.14 
21.44 
64.62 50.16 24.50 
AQuA 
36.10 
33.40 
35.42 
17.32 33.93 
23.45 
71.12 51.57 18.25 
COPA 
39.30 
34.40 
34.00 
20.47 26.79 
16.58 
69.31 50.63 21.25 
Craigslist 
31.40 
31.30 
32.83 
25.79 
8.04 
26.72 
49.82 50.16 71.25 
MathQA 
37.30 
33.50 
34.25 
19.29 26.79 
16.25 
73.29 51.10 24.50 
RTE 
36.10 
33.20 
33.58 
22.05 23.21 
20.27 
61.37 50.47 43.25 
SemEval2010 
33.10 
32.00 
32.58 
27.56 14.29 
25.63 
55.23 50.47 66.50 
WiC 
31.75 
33.45 
32.33 
26.57 13.39 
18.01 
55.05 50.47 64.25 </p>
<p>Training 
Prompts </p>
<p>AppReviews 
34.20 
33.10 
33.62 
27.17 19.64 
33.17 
61.55 50.31 33.50 
IMDB 
33.00 
32.20 
33.08 
26.38 12.50 
14.57 
55.23 50.16 71.25 
Yelp 
33.25 
32.35 
33.04 
26.77 12.50 
24.29 
62.27 51.57 41.75 </p>
<p>Small language models are also fewshot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352, Online. Association for Computational Linguistics.Figure 2: Median Accuracy with interquartile range for three models: GPT-3 , T5 , and T0 . Darker indicates larger model. Results for GPT-3 Model are from Brown et al. (2020). Results for the 11B T0 and T5 models are taken from Sanh et al. (2021)Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2005. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges Workshop, 
pages 177-190. Springer. </p>
<p>Marie-Catherine De Marneff, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse. 
proceedings of Sinn und Bedeutung 23. </p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, 
and Bill Dolan. 2007. The third pascal recognizing 
textual entailment challenge. In Proceedings of the 
ACL-PASCAL workshop on textual entailment and 
paraphrasing, pages 1-9. Association for Computa-
tional Linguistics. </p>
<p>Giovanni Grano, Andrea Di Sorbo, Francesco Mercaldo, 
Corrado A. Visaggio, Gerardo Canfora, and Sebas-
tiano Panichella. 2017. Android apps and user feed-
back: A dataset for software evolution and quality 
improvement. In Proceedings of the 2nd ACM SIG-
SOFT International Workshop on App Market Analyt-
ics, WAMA 2017, page 8-11, New York, NY, USA. 
Association for Computing Machinery. </p>
<p>He He, Derek Chen, Anusha Balakrishnan, and Percy 
Liang. 2018. Decoupling strategy and generation in 
negotiation dialogues. In Proceedings of the 2018 
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2333-2343, Brussels, Bel-
gium. Association for Computational Linguistics. </p>
<p>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, 
Preslav Nakov, Diarmuid 'O S'eaghdha, Sebastian 
Pad'o, Marco Pennacchiotti, Lorenza Romano, and 
Stan Szpakowicz. 2010. SemEval-2010 task 8: Multi-
way classification of semantic relations between pairs 
of nominals. In Proceedings of the 5th International 
Workshop on Semantic Evaluation, pages 33-38, Up-
psala, Sweden. Association for Computational Lin-
guistics. </p>
<p>Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi, 
and Luke Zettlemoyer. 2021. Surface form competi-
tion: Why the highest probability answer isn't always 
right. In EMNLP. </p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham 
Neubig. 2020. How can we know what language 
models know? </p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 
Ghazvininejad, Abdelrahman Mohamed, Omer Levy, 
Veselin Stoyanov, and Luke Zettlemoyer. 2020. 
BART: Denoising sequence-to-sequence pre-training 
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics, 
pages 7871-7880, Online. Association for Computa-
tional Linguistics. </p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang 
Ren. 2020a. Birds have four legs?! numersense: 
Probing numerical commonsense knowledge of pre-
trained language models. In Proceedings of EMNLP. 
To appear. </p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei 
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang 
Ren. 2020b. CommonGen: A constrained text gener-
ation challenge for generative commonsense reason-
ing. In Findings of the Association for Computational 
Linguistics: EMNLP 2020, pages 1823-1840, Online. 
Association for Computational Linguistics. </p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word 
problems. ACL. </p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, 
Lawrence Carin, and Weizhu Chen. 2021a. What 
makes good in-context examples for gpt-3? </p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, 
Hiroaki Hayashi, and Graham Neubig. 2021b. Pre-
train, prompt, and predict: A systematic survey of 
prompting methods in natural language processing. 
ArXiv, abs/2107.13586. </p>
<p>Yao Lu, Yue Dong, and Laurent Charlin. 2020. Multi-
xscience: A large-scale dataset for extreme multi-
document summarization of scientific articles. arXiv 
preprint arXiv:2010.14235. </p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, 
Dan Huang, Andrew Y. Ng, and Christopher Potts. 
2011. Learning word vectors for sentiment analysis. 
In Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics: Human 
Language Technologies, pages 142-150, Portland, 
Oregon, USA. Association for Computational Lin-
guistics. </p>
<p>P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and 
P. Takala. 2014. Good debt or bad debt: Detecting se-
mantic orientations in economic texts. Journal of the 
Association for Information Science and Technology, 
65. </p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2021. Metaicl: Learning to learn in 
context. ArXiv, abs/2110.15943. </p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and 
Hanna Hajishirzi. 2021. Cross-task generalization 
via natural language crowdsourcing instructions. </p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 
2018. Don't give me the details, just the summary! 
topic-aware convolutional neural networks for ex-
treme summarization. ArXiv, abs/1808.08745. </p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, 
Jason Weston, and Douwe Kiela. 2020. Adversarial 
nli: A new benchmark for natural language under-
standing. In Proceedings of the 58th Annual Meeting 
of the Association for Computational Linguistics. As-
sociation for Computational Linguistics. </p>
<p>Gabriel Orlanski and Alex Gittens. 2021. Read-
ing StackOverflow encourages cheating: Adding 
question text improves extractive code generation. 
In Proceedings of the 1st Workshop on Natural 
Teven Le Scao and Alexander M. Rush. 2021. How 
many data points is a prompt worth? In NAACL. </p>
<p>Timo Schick and Hinrich Schütze. 2021a. Exploiting 
cloze-questions for few-shot text classification and 
natural language inference. In Proceedings of the 
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume, 
pages 255-269, Online. Association for Computa-
tional Linguistics. </p>
<p>Timo Schick and Hinrich Schütze. 2021b. It's not just 
size that matters: Task 
Model Accuracy 
F1 </p>
<p>ANLI R1 
T0 
34.30 
26.17 
T5 
33.40 
20.28 </p>
<p>ANLI R2 
T0 
33.40 
23.70 
T5 
33.50 
21.25 </p>
<p>ANLI R3 
T0 
33.42 
21.82 
T5 
33.33 
24.84 </p>
<p>AQuA 
T0 
18.90 
15.04 
T5 
24.41 
12.74 </p>
<p>CB 
T0 
55.36 
38.62 
T5 
21.43 
19.41 </p>
<p>CraigslistBargains 
T0 
33.42 
18.45 
T5 
44.89 
16.47 </p>
<p>RTE 
T0 
59.21 
69.56 
T5 
52.89 
36.08 </p>
<p>WiC 
T0 
50.86 
8.81 
T5 
50.16 
5.44 </p>
<p>Table 4 :
4Median accuracy and F1 for the correspondingFigure 2.Accuracy 
F1 </p>
<p>Has Choices 
-0.14 -0.27 
Is MCQ 
-0.16 -0.25 
Training Prompt 
0.07 
0.13 
Length 
0.14 
0.18 </p>
<p>Figure 3 :
3Distribution of choices for T0 and T5 on the AQuA and CraigslistBargains.Accuracy </p>
<p>F1 
Ablation 
Mean Median 
Q1 
Q3 Mean Median 
Q1 
Q3 </p>
<p>Training Prompts 51.46 
50.25 45.25 63.00 54.18 
55.75 38.00 66.00 
Unseen Prompts 
42.72 
42.00 23.50 60.75 42.46 
36.50 22.00 62.50 </p>
<p>With Choices 
39.44 
33.12 20.19 58.62 39.37 
31.00 19.12 61.75 
No Choices 
51.73 
52.25 44.75 60.50 55.93 
53.50 43.00 66.00 </p>
<p>Is MCQ 
25.80 
22.25 16.50 26.00 23.14 
16.50 13.75 25.25 
Not MCQ 
43.28 
36.00 26.38 62.62 43.95 
36.50 22.00 65.50 </p>
<p>Extra Text 
44.99 
46.75 28.81 60.31 46.44 
46.50 27.75 66.00 
No Extra Text 
44.41 
48.00 32.75 57.62 45.43 
44.50 26.62 62.25 </p>
<p>Table 5 :
5Accuracy and F1 ranks for different ablations. It is calculated by taking the median rank of a given prompt across all 8 tasks then taking the Mean, Median, Q1, and Q3 of that. Lower is better. Q1 and Q3 are the first and third quartile.ANLI R1 ANLI R2 ANLI R3 AQuA 
CB 
Craigslist RTE 
WiC 
Rank </p>
<p>No Prompt 
34.15 
33.35 
33.42 
26.77 24.11 
16.83 
59.57 50.24 46.25 </p>
<p>Unseen 
Prompts </p>
<p>ANLI 
37.60 
34.70 
34.08 
25.95 32.14 
21.44 
64.62 50.16 24.50 
AQuA 
36.10 
33.40 
35.42 
17.32 33.93 
23.45 
71.12 51.57 18.25 
COPA 
39.30 
34.40 
34.00 
20.47 26.79 
16.58 
69.31 50.63 21.25 
Craigslist 
31.40 
31.30 
32.83 
25.79 
8.04 
26.72 
49.82 50.16 71.25 
FinNews 
33.05 
31.65 
32.83 
25.95 18.75 
19.68 
55.78 50.31 64.00 
LAMBADA 
34.00 
32.40 
32.50 
26.77 19.64 
16.08 
57.76 50.78 58.50 
MathQA 
37.30 
33.50 
34.25 
19.29 26.79 
16.25 
73.29 51.10 24.50 
Multi-XSci 
34.20 
32.70 
32.75 
27.17 19.64 
19.43 
58.84 50.31 54.75 
NumerSense 
37.70 
33.30 
33.17 
25.20 25.00 
15.75 
65.70 50.63 40.50 
RTE 
36.10 
33.20 
33.58 
22.05 23.21 
20.27 
61.37 50.47 43.25 
SemEval2010 
33.10 
32.00 
32.58 
27.56 14.29 
25.63 
55.23 50.47 66.50 
WiC 
31.75 
33.45 
32.33 
26.57 13.39 
18.01 
55.05 50.47 64.25 
ZEST 
35.20 
32.65 
33.38 
26.77 23.21 
17.76 
66.79 50.71 38.25 </p>
<p>Training 
Prompts </p>
<p>AppReviews 
34.20 
33.10 
33.62 
27.17 19.64 
33.17 
61.55 50.31 33.50 
CommonGen 
33.75 
33.35 
32.50 
25.39 13.39 
23.62 
51.81 51.18 58.75 
IMDB 
33.00 
32.20 
33.08 
26.38 12.50 
14.57 
55.23 50.16 71.25 
XSum 
33.50 
32.00 
33.00 
26.97 10.71 
19.26 
57.22 50.86 58.50 
Yelp 
33.25 
32.35 
33.04 
26.77 12.50 
24.29 
62.27 51.57 41.75 </p>
<p>By original we are referring to the prompts specifically designed for a task.
A Generalized PromptsThe tasks we took prompts from that were not used to train T0 are:• COPA(Roemmele et al., 2011)• FinancialNews(Malo et al., 2014)• LAMBADA(Paperno et al., 2016)• MathQA(Amini et al., 2019)• MultiXSci(Lu et al., 2020)• NumerSense(Lin et al., 2020a)• SemEval2010(Hendrickx et al., 2010)• ZEST(Weller et al., 2020)The tasks we took prompts from that were used to train T0 are:B Complex Task DatasetsThe two complex task used to evaluate T0's performance are:Algebra Question Answering (AQuA) Dataset of multiple choice algebraic word problems. The choices for this task are {A,B,C,D,E} and each letter maps to a potential mathematical answer(Ling et al., 2017). CraigslistBargains A collection of dialogues involving two-parties negotiating the price of an item for sale on Craigslist. For the scope of this paper, we use the task of classifying who won the negotiation (He et al., 2018).C Additional ResultsThis section is for results that could not be included in the main body of the paper due to the page limits.
MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha- jishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based for- malisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, Program synthesis with large language models. ArXiv, abs/2108.07732Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. ArXiv, abs/2108.07732.</p>
<p>The second pascal recognising textual entailment challenge. Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, Idan Szpektor, Proceedings of the second PASCAL challenges workshop on recognising textual entailment. the second PASCAL challenges workshop on recognising textual entailmentVenice6Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising tex- tual entailment challenge. In Proceedings of the sec- ond PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice.</p>
<p>Beat the ai: Investigating adversarial human annotation for reading comprehension. Max Bartolo, Alastair Roberts, Johannes Welbl, 10.1162/tacl_a_00338Sebastian Riedel, and Pontus Stenetorp. 2020. 8Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas- tian Riedel, and Pontus Stenetorp. 2020. Beat the ai: Investigating adversarial human annotation for read- ing comprehension. Transactions of the Association for Computational Linguistics, 8:662-678.</p>
<p>The fifth pascal recognizing textual entailment challenge. Luisa Bentivogli, TACPeter Clark, TACIdo Dagan, TACDanilo Giampiccolo, TACLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. In TAC.</p>
<p>Constrained language models yield few-shot semantic parsers. Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios. Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers.</p>
<p>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, 10.18653/v1/2020.emnlp-main.346Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Au- tomatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Exploring and predicting transferability across NLP tasks. Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, Mohit Iyyer, 10.18653/v1/2020.emnlp-main.635Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan- dro Sordoni, Adam Trischler, Andrew Mattarella- Micke, Subhransu Maji, and Mohit Iyyer. 2020. Ex- ploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7882-7926, Online. Association for Computational Linguistics.</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, abs/2109.01652ArXiv. M. Dai, and Quoc V. Le. 2021aJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V. Le. 2021a. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652.</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M Dai, and Quoc V Le. 2021b. Finetuned lan- guage models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Learning from task descriptions. Orion Weller, Nicholas Lourie, Matt Gardner, Matthew E Peters, 10.18653/v1/2020.emnlp-main.105Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsOrion Weller, Nicholas Lourie, Matt Gardner, and Matthew E. Peters. 2020. Learning from task de- scriptions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1361-1375, Online. Association for Computational Linguistics.</p>
<p>. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Drame, 10.18653/v1/2020.emnlp-demos.6Quentin Lhoest, and Alexander Rush. 2020. TransformersState-of-the-art natural language processingThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language processing.</p>
<p>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational LinguisticsIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein, EMNLP. Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021a. Adapting language models for zero-shot learn- ing by meta-tuning on dataset and prompt collections. In EMNLP.</p>
<p>. Zexuan Zhong, Dan Friedman, Danqi Chen, 2021b. Factual probing is [mask]: Learning vs. learning to recallZexuan Zhong, Dan Friedman, and Danqi Chen. 2021b. Factual probing is [mask]: Learning vs. learning to recall.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8511 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8511</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8511</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-767a7e949ba4520888e7442ee01e6a37c254fc53</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/767a7e949ba4520888e7442ee01e6a37c254fc53" target="_blank">CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> CLIN is presented, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates.</p>
                <p><strong>Paper Abstract:</strong> Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory centered on causal abstractions (rather than general"helpful hints") that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8511.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8511.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual Learning from INteractions (CLIN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented generative language agent that continually learns across repeated trials and episodes by maintaining a persistent, dynamic natural-language memory of causal abstractions; memory is generated and updated by a frozen LLM and retrieved to guide goal selection and action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based agent composed of a controller (LLM) that generates goals, an executor (LLM) that maps goals to valid actions, a persistent natural-language memory of causal abstractions, and a memory-generator (LLM) that reflects on trials to create/update memories; designed for continual test-time adaptation and generalization without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>An instruction-tuned, frozen large language model used as the controller, executor, and memory-generator in CLIN; the paper does not specify model size or further architecture details beyond 'gpt-4'.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A high-fidelity, text-based simulated environment of science-themed tasks (short and long), expressed in natural language with partial observability and large action spaces; requires multi-step interactive reasoning and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>persistent dynamic natural-language causal-abstraction memory (episodic per-trial memory plus aggregated 'meta-memory')</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory is stored as a list of English sentences expressing causal abstractions in constrained templates (e.g., 'X SHOULD BE NECESSARY to Y', 'X MAY BE NECESSARY to Y', 'X DOES NOT CONTRIBUTE to Y'). A memory-generator (frozen LLM) is invoked at the end of each trial, provided with the recent trial trace, final reward (converted to NL feedback), and the three most recent memories; it outputs an updated memory list. The generator performs saliency-based pruning using trial success (reward) and includes uncertainty markers in surface form. A meta-memory is created by selecting the best trial per episode (archive size 10) and combining those memories via a specialized prompt to make generalized abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory items are retrieved using the task instruction and trial history; the controller selects one or more relevant memory items and appends them to its prompt (context) to generate the next goal; the executor also scans the selected learnings to form rationale for selecting the next action. Memory retrieval is explicit selection and concatenation into the prompt (retrieval-augmented prompt engineering).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ADAPT (per-trial memory, continual updates): average reward = 62.2 (All tasks, out of 100); GEN-ENV zero-shot with meta-memory start = 52.7 (vs. BASE 48.6); GEN+ADAPT (meta-memory + continued updates) = 69.5 (All tasks). Per-task numbers are reported in Tables 1 and 2 (e.g., GrowFruit G+A = 94.5 in Table 1 / 71.6 in Table 2 depending on split).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>BASE (zero-shot / no prior memory at start) average reward = 48.6 (All tasks, out of 100).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations/comparisons reported: (1) Unstructured (free-form) memory (no constrained causal templates) causes average reward drop of ~6 points in ~10% of cases compared to CLIN's structured memory. (2) Ablating the controller (goal generator) reduces BASE performance by ~18 points, making it equivalent to ReAct. (3) CLIN outperforms Reflexion (a reflective baseline) by large margins (paper reports CLIN beating SOTA reflective agent by ~23 points in some setups). (4) Meta-memory (aggregated from best trials) gives a stronger zero-shot start (52.7 vs BASE 48.6) and accelerates gains through trials (reported numerics: +4 points zero-shot improvement (GEN-ENV) and +13 for new tasks, plus additional gains of +17 (GEN-ENV) and +7 (GEN-TASK) when allowed to adapt further).</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Structured, persistent causal-abstraction memory with constrained templates ('X SHOULD/MAY BE NECESSARY to Y' and 'X DOES NOT CONTRIBUTE to Y'), saliency-based pruning based on trial reward, and a meta-memory constructed from the most successful trials per episode; retrieving and concatenating selected memory items into the controller/executor prompts yields the best results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) lack of exploration — CLIN cannot learn or generate insights about unobserved locations/actions, so memory is limited by the agent's own exploration history; (2) poor memory retrieval — suboptimal retrieval can cause applying inapplicable memories (example: repeatedly retrieving 'activate stove' insight when stove is broken, causing failures); (3) memory representation/retrieval scheme can cause misapplication of insights in varied initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use a persistent, dynamically updated memory of structured causal abstractions (with explicit uncertainty markers) rather than free-form reflections; generate meta-memory from successful trials to aid zero-shot generalization; retrieve and append selected memory items to the controller and executor prompts for decision-making; apply saliency-based pruning to keep high-value items and track uncertainty to refine usefulness over time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8511.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8511.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reflective language-agent approach that improves by generating language-based insights (reflections) from its latest failed attempt and using that reflection to replan on the next trial; reflections are task- and environment-specific and not maintained as a long-term persistent memory in the original Reflexion design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A frozen-LLM-based agent that, after a failed attempt, reflects on its actions to produce a textual insight or plan which is then included in the prompt for a retry; used as a SOTA reflective baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-4 (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Instruction-tuned frozen LLM (same underlying model used for baselines in the experiments); the paper uses gpt-4 as the underlying LLM for Reflexion in the comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based science tasks with partial observability and sparse rewards; Reflexion is evaluated here as a baseline on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trial-local reflections (non-persistent reflective insights tied to the last trial)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Reflections are generated by a frozen LLM based on the most recent trial's trace (failures), producing a language insight/plan; Reflexion as described in the referenced work does not maintain a persistent long-term memory archive across trials in the same way CLIN does.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Reflection text is appended to the prompt used to generate the next plan/behavior for the subsequent trial (retrieval is immediate from the last-trial reflection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reflexion (with per-trial reflection) average reward = 39.4 (All tasks, Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct (no reflection) average reward = 29.6 (All tasks, Table 2) — used as the baseline for comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper compares Reflexion to ReAct and to CLIN. Reflexion improves over ReAct (numbers in Table 2), but CLIN's persistent causal memory and meta-memory yield substantially higher improvements than Reflexion's per-trial reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Within the paper's analysis, Reflexion's per-trial reflection is less effective than CLIN's persistent, structured causal-memory and meta-memory approach; the paper recommends persistent causal abstractions over per-trial, environment-specific reflection for better generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reflexion's reflections are often task- and environment-specific (e.g., 'go to desk 1 and find the lamp') and thus have limited transfer across environments or tasks; lacks a long-term persistent memory, limiting continual improvement over multiple trials and episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper concludes that while per-trial reflections can help, persistent and structured memories of causal abstractions that are maintained and pruned across trials/episodes provide stronger continual adaptation and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8511.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8511.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that interleaves chain-of-thought reasoning traces with action-selection for LLM agents; used here as a base generative-agent baseline that does not maintain long-term memory across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based method that integrates reasoning (thoughts) with actions; in this paper ReAct is used as a base agent baseline (i.e., no long-term memory or reflection archive).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>gpt-4 (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Instruction-tuned frozen LLM used as the underlying model for ReAct baseline evaluations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based science tasks where ReAct serves as the base agent (no persistent memory) for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>N/A (ReAct baseline does not use a persistent memory mechanism in the experiments reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct average reward = 29.6 (All tasks, Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Used as the base agent to compare the effect of adding reflection (Reflexion) and structured persistent memory (CLIN). CLIN's ablated controller made its BASE similar to ReAct, demonstrating the role of goal-generation.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not learn or adapt from experience across trials in the manner of persistent memory systems; lower average reward compared to Reflexion and CLIN in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Paper uses ReAct as a representative non-memory baseline; findings indicate that augmenting ReAct-style agents with structured persistent memory or reflection improves performance, with structured persistent memory (CLIN) giving the best results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8511.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8511.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based embodied agent (in Minecraft) that grows a code-based skill library from experience (a form of persistent skill memory) enabling open-ended improvement; mentioned as related prior work that uses a persistent skill/library memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-driven agent that builds and uses a persistent, evolving skill library (code-based) derived from exploration and feedback in Minecraft; cited as prior work on persistent memory/skill accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Minecraft (as used by Voyager; mentioned in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>An open-ended 3D environment; Voyager collects skills/code snippets to build an expanding library of capabilities from experience.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>persistent skill library (code-based skills)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Skill library of code snippets/skills grown from agent's interactions (high-level description from the referenced Voyager work; details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Skill library is used to augment agent capability over time; exact integration details are in the Voyager paper (cited), not described in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as prior related work demonstrating persistent skill/memory accumulation; not evaluated in ScienceWorld experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Cited as inspiration that persistent non-parametric knowledge (skill library) can enable continual improvement, but CLIN differs by using textual causal abstractions rather than code-based skills.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8511.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8511.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenerativeAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of LLM-driven simulated agents that maintain event/memory traces to model human-like behavior and social simulations; cited as work that leverages memory to improve LLM-driven behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that store and use narrated memories and summaries to imitate complex human social behavior; mentioned as evidence that memory of experiences can improve frozen LLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Social simulation / interactive agent environments (as in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Interactive social simulations where maintaining and retrieving memory traces improves behavior; details of tasks are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>narrative/event memory (experience traces used in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory stores events and summaries used to influence future behavior; exact architecture is in the cited paper rather than this one.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memories are retrieved and used to inform subsequent utterances/actions in the simulation (high-level description only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Cited to support the claim that memory of useful learnings can improve frozen LLM behavior; the present paper extends this idea specifically to structured causal abstractions for task adaptation/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8511",
    "paper_id": "paper-767a7e949ba4520888e7442ee01e6a37c254fc53",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "CLIN",
            "name_full": "Continual Learning from INteractions (CLIN)",
            "brief_description": "A memory-augmented generative language agent that continually learns across repeated trials and episodes by maintaining a persistent, dynamic natural-language memory of causal abstractions; memory is generated and updated by a frozen LLM and retrieved to guide goal selection and action generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CLIN",
            "agent_description": "An LLM-based agent composed of a controller (LLM) that generates goals, an executor (LLM) that maps goals to valid actions, a persistent natural-language memory of causal abstractions, and a memory-generator (LLM) that reflects on trials to create/update memories; designed for continual test-time adaptation and generalization without parameter updates.",
            "llm_model_name": "gpt-4",
            "llm_model_description": "An instruction-tuned, frozen large language model used as the controller, executor, and memory-generator in CLIN; the paper does not specify model size or further architecture details beyond 'gpt-4'.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "A high-fidelity, text-based simulated environment of science-themed tasks (short and long), expressed in natural language with partial observability and large action spaces; requires multi-step interactive reasoning and planning.",
            "memory_used": true,
            "memory_type": "persistent dynamic natural-language causal-abstraction memory (episodic per-trial memory plus aggregated 'meta-memory')",
            "memory_architecture": "Memory is stored as a list of English sentences expressing causal abstractions in constrained templates (e.g., 'X SHOULD BE NECESSARY to Y', 'X MAY BE NECESSARY to Y', 'X DOES NOT CONTRIBUTE to Y'). A memory-generator (frozen LLM) is invoked at the end of each trial, provided with the recent trial trace, final reward (converted to NL feedback), and the three most recent memories; it outputs an updated memory list. The generator performs saliency-based pruning using trial success (reward) and includes uncertainty markers in surface form. A meta-memory is created by selecting the best trial per episode (archive size 10) and combining those memories via a specialized prompt to make generalized abstractions.",
            "memory_integration_strategy": "Memory items are retrieved using the task instruction and trial history; the controller selects one or more relevant memory items and appends them to its prompt (context) to generate the next goal; the executor also scans the selected learnings to form rationale for selecting the next action. Memory retrieval is explicit selection and concatenation into the prompt (retrieval-augmented prompt engineering).",
            "performance_with_memory": "ADAPT (per-trial memory, continual updates): average reward = 62.2 (All tasks, out of 100); GEN-ENV zero-shot with meta-memory start = 52.7 (vs. BASE 48.6); GEN+ADAPT (meta-memory + continued updates) = 69.5 (All tasks). Per-task numbers are reported in Tables 1 and 2 (e.g., GrowFruit G+A = 94.5 in Table 1 / 71.6 in Table 2 depending on split).",
            "performance_without_memory": "BASE (zero-shot / no prior memory at start) average reward = 48.6 (All tasks, out of 100).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations/comparisons reported: (1) Unstructured (free-form) memory (no constrained causal templates) causes average reward drop of ~6 points in ~10% of cases compared to CLIN's structured memory. (2) Ablating the controller (goal generator) reduces BASE performance by ~18 points, making it equivalent to ReAct. (3) CLIN outperforms Reflexion (a reflective baseline) by large margins (paper reports CLIN beating SOTA reflective agent by ~23 points in some setups). (4) Meta-memory (aggregated from best trials) gives a stronger zero-shot start (52.7 vs BASE 48.6) and accelerates gains through trials (reported numerics: +4 points zero-shot improvement (GEN-ENV) and +13 for new tasks, plus additional gains of +17 (GEN-ENV) and +7 (GEN-TASK) when allowed to adapt further).",
            "best_memory_strategy": "Structured, persistent causal-abstraction memory with constrained templates ('X SHOULD/MAY BE NECESSARY to Y' and 'X DOES NOT CONTRIBUTE to Y'), saliency-based pruning based on trial reward, and a meta-memory constructed from the most successful trials per episode; retrieving and concatenating selected memory items into the controller/executor prompts yields the best results.",
            "limitations_or_failure_cases": "Reported limitations include: (1) lack of exploration — CLIN cannot learn or generate insights about unobserved locations/actions, so memory is limited by the agent's own exploration history; (2) poor memory retrieval — suboptimal retrieval can cause applying inapplicable memories (example: repeatedly retrieving 'activate stove' insight when stove is broken, causing failures); (3) memory representation/retrieval scheme can cause misapplication of insights in varied initial conditions.",
            "recommendations_or_conclusions": "Use a persistent, dynamically updated memory of structured causal abstractions (with explicit uncertainty markers) rather than free-form reflections; generate meta-memory from successful trials to aid zero-shot generalization; retrieve and append selected memory items to the controller and executor prompts for decision-making; apply saliency-based pruning to keep high-value items and track uncertainty to refine usefulness over time.",
            "uuid": "e8511.0",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A reflective language-agent approach that improves by generating language-based insights (reflections) from its latest failed attempt and using that reflection to replan on the next trial; reflections are task- and environment-specific and not maintained as a long-term persistent memory in the original Reflexion design.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "A frozen-LLM-based agent that, after a failed attempt, reflects on its actions to produce a textual insight or plan which is then included in the prompt for a retry; used as a SOTA reflective baseline in the paper's experiments.",
            "llm_model_name": "gpt-4 (as used in experiments)",
            "llm_model_description": "Instruction-tuned frozen LLM (same underlying model used for baselines in the experiments); the paper uses gpt-4 as the underlying LLM for Reflexion in the comparisons.",
            "benchmark_name": "ScienceWorld (as evaluated in this paper)",
            "benchmark_description": "Text-based science tasks with partial observability and sparse rewards; Reflexion is evaluated here as a baseline on these tasks.",
            "memory_used": true,
            "memory_type": "trial-local reflections (non-persistent reflective insights tied to the last trial)",
            "memory_architecture": "Reflections are generated by a frozen LLM based on the most recent trial's trace (failures), producing a language insight/plan; Reflexion as described in the referenced work does not maintain a persistent long-term memory archive across trials in the same way CLIN does.",
            "memory_integration_strategy": "Reflection text is appended to the prompt used to generate the next plan/behavior for the subsequent trial (retrieval is immediate from the last-trial reflection).",
            "performance_with_memory": "Reflexion (with per-trial reflection) average reward = 39.4 (All tasks, Table 2).",
            "performance_without_memory": "ReAct (no reflection) average reward = 29.6 (All tasks, Table 2) — used as the baseline for comparison in the paper.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Paper compares Reflexion to ReAct and to CLIN. Reflexion improves over ReAct (numbers in Table 2), but CLIN's persistent causal memory and meta-memory yield substantially higher improvements than Reflexion's per-trial reflections.",
            "best_memory_strategy": "Within the paper's analysis, Reflexion's per-trial reflection is less effective than CLIN's persistent, structured causal-memory and meta-memory approach; the paper recommends persistent causal abstractions over per-trial, environment-specific reflection for better generalization.",
            "limitations_or_failure_cases": "Reflexion's reflections are often task- and environment-specific (e.g., 'go to desk 1 and find the lamp') and thus have limited transfer across environments or tasks; lacks a long-term persistent memory, limiting continual improvement over multiple trials and episodes.",
            "recommendations_or_conclusions": "The paper concludes that while per-trial reflections can help, persistent and structured memories of causal abstractions that are maintained and pruned across trials/episodes provide stronger continual adaptation and generalization.",
            "uuid": "e8511.1",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "brief_description": "An approach that interleaves chain-of-thought reasoning traces with action-selection for LLM agents; used here as a base generative-agent baseline that does not maintain long-term memory across trials.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "An LLM-based method that integrates reasoning (thoughts) with actions; in this paper ReAct is used as a base agent baseline (i.e., no long-term memory or reflection archive).",
            "llm_model_name": "gpt-4 (as used in experiments)",
            "llm_model_description": "Instruction-tuned frozen LLM used as the underlying model for ReAct baseline evaluations in the paper.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "Text-based science tasks where ReAct serves as the base agent (no persistent memory) for comparison.",
            "memory_used": false,
            "memory_type": null,
            "memory_architecture": "",
            "memory_integration_strategy": "N/A (ReAct baseline does not use a persistent memory mechanism in the experiments reported).",
            "performance_with_memory": null,
            "performance_without_memory": "ReAct average reward = 29.6 (All tasks, Table 2).",
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Used as the base agent to compare the effect of adding reflection (Reflexion) and structured persistent memory (CLIN). CLIN's ablated controller made its BASE similar to ReAct, demonstrating the role of goal-generation.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Does not learn or adapt from experience across trials in the manner of persistent memory systems; lower average reward compared to Reflexion and CLIN in these experiments.",
            "recommendations_or_conclusions": "Paper uses ReAct as a representative non-memory baseline; findings indicate that augmenting ReAct-style agents with structured persistent memory or reflection improves performance, with structured persistent memory (CLIN) giving the best results.",
            "uuid": "e8511.2",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An LLM-based embodied agent (in Minecraft) that grows a code-based skill library from experience (a form of persistent skill memory) enabling open-ended improvement; mentioned as related prior work that uses a persistent skill/library memory.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "An LLM-driven agent that builds and uses a persistent, evolving skill library (code-based) derived from exploration and feedback in Minecraft; cited as prior work on persistent memory/skill accumulation.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Minecraft (as used by Voyager; mentioned in the paper)",
            "benchmark_description": "An open-ended 3D environment; Voyager collects skills/code snippets to build an expanding library of capabilities from experience.",
            "memory_used": true,
            "memory_type": "persistent skill library (code-based skills)",
            "memory_architecture": "Skill library of code snippets/skills grown from agent's interactions (high-level description from the referenced Voyager work; details not provided in this paper).",
            "memory_integration_strategy": "Skill library is used to augment agent capability over time; exact integration details are in the Voyager paper (cited), not described in detail here.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as prior related work demonstrating persistent skill/memory accumulation; not evaluated in ScienceWorld experiments in this paper.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": null,
            "recommendations_or_conclusions": "Cited as inspiration that persistent non-parametric knowledge (skill library) can enable continual improvement, but CLIN differs by using textual causal abstractions rather than code-based skills.",
            "uuid": "e8511.3",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GenerativeAgents",
            "name_full": "Generative agents (Park et al., 2023)",
            "brief_description": "A class of LLM-driven simulated agents that maintain event/memory traces to model human-like behavior and social simulations; cited as work that leverages memory to improve LLM-driven behavior.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative agents (Park et al.)",
            "agent_description": "Agents that store and use narrated memories and summaries to imitate complex human social behavior; mentioned as evidence that memory of experiences can improve frozen LLM behavior.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Social simulation / interactive agent environments (as in the cited work)",
            "benchmark_description": "Interactive social simulations where maintaining and retrieving memory traces improves behavior; details of tasks are in the cited paper.",
            "memory_used": true,
            "memory_type": "narrative/event memory (experience traces used in simulation)",
            "memory_architecture": "Memory stores events and summaries used to influence future behavior; exact architecture is in the cited paper rather than this one.",
            "memory_integration_strategy": "Memories are retrieved and used to inform subsequent utterances/actions in the simulation (high-level description only).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": null,
            "best_memory_strategy": null,
            "limitations_or_failure_cases": null,
            "recommendations_or_conclusions": "Cited to support the claim that memory of useful learnings can improve frozen LLM behavior; the present paper extends this idea specifically to structured causal abstractions for task adaptation/generalization.",
            "uuid": "e8511.4",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 1
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        }
    ],
    "cost": 0.017877999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CLIN: A CONTINUALLY LEARNING LANGUAGE AGENT FOR RAPID TASK ADAPTATION AND GENERALIZATION</h1>
<p>Bodhisattwa Prasad Majumder ${ }^{1}$, Bhavana Dalvi Mishra ${ }^{1}$, Peter Jansen ${ }^{1,2}$, Oyvind Tafjord ${ }^{1}$, Niket Tandon ${ }^{1}$, Li Zhang ${ }^{3}$, Chris-Callison Burch ${ }^{3}$, Peter Clark ${ }^{1}$<br>${ }^{1}$ Allen Institute of AI<br>${ }^{2}$ University of Arizona<br>${ }^{3}$ University of Pennsylvania<br>Contact: {bodhisattwam, bhavanad}@allenai.org<br>Project page: https://allenai.github.io/clin/</p>
<h4>Abstract</h4>
<p>Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time, beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory, centered on causal abstractions (rather than general "helpful hints"), that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points ( 13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points ( 7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have been increasingly used to interact with external environments (e.g., simulated worlds) as goal-driven agents (Reed et al., 2022). However, it has been challenging for these language agents to efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning (Chen et al., 2021; Ammanabrolu et al., 2020). More recently, new techniques have appeared in which an agent reflects on its own past experience solving a task in a particular environment, and generates languagebased insights to help it retry the task, e.g., Reflexion (Shinn et al., 2023). Such methods have the advantage of not requiring parameter updates (particularly useful given the growing popularity of frozen language models). However, the style of such insights plays a crucial role in performance, and not all insights improve generalization performance. For example, a specific insight such as "In the next trial, I will go to desk 1 and find the lamp" (Shinn et al., 2023) may have limited value (or even hurt) for a different environment or task.</p>
<p>Our goal is a system that will continually improve over time, both while attempting the same task in the same environment, and across different tasks and environments. Our approach builds on prior work on reflection in two ways: First, we conjecture that a specific style of insight will be useful, namely one that captures causal abstractions about agent's actions, e.g., "opening doors</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: CLIN creates (Trial1) or adapts (Trial2+) a memory of causal abstractions to help in future trials by reflecting on the last trial and current memory. It does this using a suitably prompted LLM to generate the updated memory (Section 3.2). Here, reflecting on Trial1, CLIN notes in memory that going to the kitchen helped with finding seeds, enabling it to find the seeds faster in Trial2. From there, it also learns that moving the seeds to the pot helped plant the seeds. To further generalize across episodes (sequences of trials, right figure) for use in new environments, CLIN generates a summary ("meta-memory") of the best (starred) memories from each prior episode, here generating the generalization that moving to different rooms helps finding objects (Section 3.3)
may be necessary for movement between rooms". Causal abstractions can potentially help the agent decide which action to take in the future, and can be viewed as a kind of action model learning (Arora et al., 2018), but placed in the modern context of language models. Second, we maintain these abstractions in a continually evolving, dynamic memory, which is regularly updated as the agent gains experience, allowing useful causal knowledge to persist (and unhelpful knowledge to be dropped) over time and between tasks and environments, as illustrated in Figure 1.</p>
<p>We operationalize and evaluate this approach in a memory-augmented language agent called CLIN (continual learning from interactions). CLIN is an agent that operates in ScienceWorld (Wang et al., 2022), a virtual, text-based environment in which an agent is tasked with science-based goals, e.g., boiling a liquid, growing a plant. We find that CLIN is able to rapidly learn about the environment and its action vocabulary and continually improve on repeated trials on the same task and environment, outperforming state-of-the-art (SOTA) reflective language agents like Reflexion by 23 points. CLIN can also transfer its learning to new environments (or tasks), improving its zero-shot performance by 4 (13 for new tasks) points and can further improve performance through continual memory updates, enhancing performance by an additional 17 ( 7 for new tasks) points. Our contributions are as follows:</p>
<ul>
<li>For memory-based language agents, we show that memory of causal abstractions is effective at helping the agents learn over an extended period and in varying conditions.</li>
<li>We describe and evaluate CLIN, an architecture for a novel nonparametric learning paradigm. We show that CLIN learns faster than prior systems and generalizes better to new tasks and new environments, achieving state-of-the-art.</li>
</ul>
<p>Overall, this work suggests that a dynamically maintained memory, centered around causal knowledge, is a promising way forward for agents built on frozen models to continually improve over time.</p>
<h1>2 Related Work</h1>
<p>There is a long literature of work on agents that can navigate complex environments. A common approach is to use reinforcement learning (RL), e.g., DRRN (He et al., 2015), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), CALM (Yao et al., 2020), where agents learn a task over repeated trials. However, while effective, such agents typically require a large number of trials to learn and have trouble adapting to unexpected changes in the test environment. More recently, (Adaptive-AgentTeam et al., 2023) demonstrated AdA, an agent that could rapidly adapt to open-ended novel 3D problems, using meta-reinforcement learning, essentially being able to change its policy on the fly.</p>
<p>However, AdA required vast amounts of pretraining, and this skill was still limited to the style of environments and problems seen in pretraining.</p>
<p>Recently, LLMs have provided a new tool for building goal-directed agents (Huang et al., 2022). Given a linguistic description of the world state, a task, and a history, the LLM can be prompted to suggest next actions to take to achieve a goal, exploiting their wealth of semantic knowledge about the world and requiring little training, e.g., SayCan (Ahn et al., 2022), ReAct (Yao et al., 2022), and more recently SwiftSage (Lin et al., 2023), which combines a supervised agent and a deliberative agent together. However, while performing reasonably with little training data, such agents are unable to learn and adapt from experience.</p>
<p>Two recent systems have demonstrated how a frozen-model-based agent could improve at a task. Voyager (Wang et al., 2023) operates in the world of Minecraft, growing a (code-based) skill library from rich feedback of its failures. Reflexion (Shinn et al., 2023) improves at a task by reflecting on a failed attempt at that task and devise a new plan that accounted for that mistake, used in the subsequent prompt to retry the task. While Reflexion did not have a long-term memory, and its reflections were task- and environment-specific, e.g., "In the next trial, I will go to desk 1 and find the lamp.", we take inspiration from it to build an agent, CLIN, which continually maintains and adapts a long-term, persistent memory of reflections, useful across different trials, tasks, and environments.</p>
<p>More generally, others have found that a memory of useful learnings can be used to improve frozen LLM behavior, e.g., in QA (Dalvi et al., 2022; Tandon et al., 2022; Madaan et al., 2023), or for modeling social behavior (Park et al., 2023). We apply this finding to goal-directed agents.</p>
<p>Finally, we note that the content of experiential memory is also important. Specifically, CLIN learns a memory of causal abstractions, which can be seen as learning a linguistic form of action model, describing the causal effects of actions. While there has been substantial work in the planning community of learning action models in a fully formalized context (Arora et al., 2018; Aineto et al., 2018), CLIN loosely applies this idea in the linguistic world of LLM agents.</p>
<h1>3 APPROACH</h1>
<p>Problem Formulation. Sequential decision-making tasks require agents to repeatedly observe and then act in an environment until they accomplish a specific goal. At a high level, this can be accomplished by developing beliefs about the world, acting on the environment based on those beliefs, and then updating one's beliefs based on the observed outcome. Here, we investigate constructing an agent that can continually update its beliefs through interaction and observation while exploiting its past experience toward solving unseen parametric variations of tasks.</p>
<p>Setup. We investigate our continual learning agents in simulated environments. Our environments are modeled in a high-fidelity text-based simulator (Wang et al., 2022), where both actions and observations are expressed in natural language. Let's define the task space to be $\mathcal{M}$, a collection of partially observable Markov Decision Processes (POMDPs) that can be executed in a set of environment configurations $\mathcal{E}$. Each task $m \in \mathcal{M}$ has an initial state and a desired winning state, which vary depending on the environment $e \in \mathcal{E}$.</p>
<p>Our setup allows an agent to attempt a task several times; each time is denoted as a trial, $\mathcal{T}$, which consists of a total of $\tau$ steps. Each step comprises an action by the agent (a), and in response, the simulator returns the result of that action in the form of an observation (o) and a reward ( $r$ ). A collection of $K$ trials is called an episode. The environment gets reset when the task reaches an end state (such as completing, failing, or timing out). In our continual learning setup, the agent retains its memory across trials/episodes, reaping the benefits of continued interaction with the environment.</p>
<h3>3.1 CLIN: A GENERATIVE AGENT CONTINUALLY LEARNING FROM INTERACTIONS</h3>
<p>To act in the world, CLIN uses three modules: a memory, a controller, and an executor, illustrated in Figure 2 and which we now describe. Learning then occurs using a fourth module, a memory generator, to generate an updated memory after each trial.</p>
<p>Memory. CLIN's memory $(\mathcal{S})$ is a persistent, dynamic collection of NL sentences that express causal abstractions, generated by reflecting on past experiences in order to help the agent perform</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The architecture of CLIN. A controller takes the current task, retrievals from memory, and the trial so far, to generate the next goal to achieve. The executor then converts this to a valid action to perform towards that goal. The simulator then performs the action and returns an observation of that action's effect. Memory is updated at the end of each trial by the memory generator (Section 3.2).
better in the future. This generation process is described shortly in Section 3.2. For example, having an explicit causal insight, "opening the fridge is necessary to access apple juice", learned from past experiences, can reduce the action search space for CLIN while looking for "apple juice" in the same environment in future trials. To aid in continual learning, the memory also captures mistakes made by the agent in previous trials, similar to reflective agents explored in recent work (Shinn et al., 2023; Park et al., 2023), noting actions that failed to contribute to a task.</p>
<p>Controller. The role of the controller is to generate the next goal to pursue in service of the task. In CLIN it is a frozen LLM, whose prompt includes the current task $m$, e.g., "convert water into steam", retrievals from the current memory $\mathcal{S}$, and the trial so far (the sequence of goal-action-observation triples $\left{g_{1}, a_{1}, o_{1}, \ldots, g_{t}, a_{t}, o_{t}\right}$ ), and is prompted to output the next goal $g_{t+1}$ to pursue, e.g., "find water". Memory items are retrieved using both the task instruction and the trial history. The controller first selects one or more memory items given the current state and if they are useful for the next action to progress in the task. After that, it appends the learning, if selected, in context to generate a goal, otherwise the goal is generated based on the trial history (see full prompt in Figure 6).
Executor. The role of the executor is to convert the generated goal $g_{t+1}$ into a valid action $a_{t+1}$ that can be executed in the environment in pursuit of that goal. Again a (frozen) LLM is used, whose prompt includes the goal $g_{t+1}$, the trial so far, and all the possible actions that can be performed in the current state (provided by the simulator, as is standard practice in current generative agent research (Ahn et al., 2022; Yao et al., 2022; Lin et al., 2023; Park et al., 2023)). The list of possible actions is expressed as possible action templates and available objects that can instantiate them, rather than a combinatorially large enumeration of possible actions. The model is then prompted to generate a candidate action to perform (see prompt in Figure 6). Finally, CLIN checks this candidate action is one of the valid actions. If it is not, it finds the most similar valid action using the pre-trained embeddings from the sentence-transformer model (Reimers \&amp; Gurevych, 2019). If the top-ranked valid action has a similarity score greater than a threshold (here, 0.9 , chosen as a hyperparameter), the action is selected. Otherwise, we perform iterative refinement (Madaan et al., 2023) by suffixing the context with feedback that the generated candidate action is not executable. This allows the executor to retry the generation for up to a maximum number of tries (here, 5).
Finally, upon executing the action $a_{t+1}$, CLIN receives a partial next state, as an observation, from the simulator and the reward $(r) \in[0,1]$. Rewards are nominally given by the simulator for achieving either major task subgoals (e.g., finding water, for the boiling task), or minor and optional subgoals (e.g., being in the kitchen, for the boiling task). Rewards are sparse and generally only supplied after the completion of a task subgoal. A snapshot of a full trial is given in lines 4-10 in Algorithm 1.
Note that CLIN does not make use of any gold data to identify goals and memories. Rather, we expect CLIN to perform a balanced act of exploration-exploitation by interacting, learning, and adapting to unseen tasks or environment configurations-a key difference from few-shot generative agents by previous work (Ahn et al., 2022; Yao et al., 2022; Lin et al., 2023; Park et al., 2023).</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Continual Learning with CLIN
    procedure ADAPTATION(Task: \(m\), Env: \(e\), Mem-
        ory: \(\mathcal{S}\) ):
        Initialize Memory: \(\mathcal{S}_{0}\)
        for \(k \in 1, \cdots, K\) do:
            Intialize Trial \(\mathcal{T}, t\)
            while \(t&lt;\) max. steps or task not complete do:
                \(g_{t}=\operatorname{Controller}\left(m, e, \mathcal{T}_{&lt;t}, \mathcal{S}_{k-1}\right)\)
                \(a_{t}=\) Executor ( \(g_{t}\), admissible actions)
                \(r_{t}, o_{t}=\operatorname{Simulator}\left(\mathcal{T}_{&lt;t}, a_{t}\right)\)
                \(\mathcal{T}_{&lt;t+1}=\mathcal{T}_{&lt;t}+\left(g_{t}, a_{t}, o_{t}, r_{t}\right)\)
            Final reward \(r_{k}=r_{t}\)
            \(\mathcal{S}_{k}=\) memory-generator \(\left(\left\{\mathcal{S}_{&lt;k}\right\}, \mathcal{T}_{k}, r_{k}\right)\)
        procedure Generalization(Task: \(m\), Env: \(e\),
            past \(\left.m^{\prime} / e^{\prime}\right)\)
            \(\left\{\mathcal{S}_{\text {crucial }}, r_{k}\right\}=\) crucial-memories (past \(\left.m^{\prime} / e^{\prime}\right)\)
            \(\mathcal{S}_{\text {meta }}=\operatorname{meta-memory}\left(\left\{\mathcal{S}_{\text {crucial }}, r_{k}\right\}, m\right)\)
            ADAPTATION \(\left(m, e, \mathcal{S}_{\text {meta }}\right)\)
</code></pre></div>

<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (LEFT) CLIN's continual learning algorithm. (RIGHT) Example causal abstractions.</p>
<h1>3.2 MEMORY: A COLLECTION OF ADAPTIVE CAUSAL ABSTRACTIONS</h1>
<p>At the end of each trial (completion or failure), CLIN uses a memory generator to create or update its memory. The memory generator is a (frozen) LLM prompted to reflect on the current trial and memory, and generate a new memory of insights in the form of (English sentences expressing) useful causal abstractions, as we now describe.
Learning about state transitions is essential for sequential decision-making tasks (Mnih et al., 2013), which can be manifested by knowing 1) actions enabling desired state transitions, 2) actions producing undesired or no change in states, and 3) state transitions that contribute to the task progress. To generate these kinds of knowledge, the generator is prompted to generate insights in a particular syntax (see prompt in Figure 7). To capture good actions enabling desired changes and helpful state transitions, we use the template " X is NECESSARY to Y ", and to capture contrastive examples of unsuitable actions and state transitions, we employ "X DOES NOT CONTRIBUTE to Y", as depicted in Section 4.3, where X, Y are related to actions. These abstractions are functionally analogous to hindsight experience replay (Andrychowicz et al., 2017), obtained from CLIN's past self-explorations.
While useful insights can be abstracted from the trials, CLIN's exploration can be limited, especially in the early trials, given an incredibly large action space. Hence, incomplete exploration can pose varying degrees of uncertainty on extracted insights. To capture this, we also include a measure of uncertainty in each abstraction by either of the two linguistic variations in their surface forms: " X may . . " to denote moderate to high uncertainty, and " $X$ should . . ." to indicate low uncertainty (See Section 4.3). In the course of continual learning, as CLIN gathers experience, we expect the level of uncertainty to change depending on the frequency of their use and their fitness to the task.</p>
<p>Updating Memory Across Trials. CLIN continually attempts to solve a task in an environment for multiple trials (in sum, an episode). To update the memory after each trial within an episode, the memory generator is prompted with the most recent trial (a sequence of $\left(g_{t}, a_{t}, o_{t}\right)$ tuples and the final reward $\left.r_{k}{ }^{1}\right)$, and the memories from the three most recent trials $\left{\mathcal{S}<em k-1="k-1">{k-2}, \mathcal{S}</em>}, \mathcal{S<em k_1="k+1">{k}\right}$. It is then prompted to generate an updated memory $\mathcal{S}</em>$, namely a new list of semi-structured causal abstractions in the forms described above, for use in the next trial. Although we do not specify a maximum size for the memory, we observe that size of the generated memory (i.e., the number of causal abstractions generated) is far less than the number of actions executed in the trial, indicating</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the memory-generator additionally performs a saliency-based pruning to keep only important insights based on the success of the trial, as indicated by the final reward $r_{k}$ at the end of the trial $\mathcal{T}_{k}$.</p>
<h1>3.3 Meta-Memory for Better Generalization</h1>
<p>Updating memory based on past insights and the current trial to influence future trials for the same task in the current environment configuration during test-time adaptation. However, to generalize across tasks or environment configurations, the memory needs to contain more generalized causal abstractions than memories used across trials in an episode. We call this as meta-memory, abstracted across multiple episodes of solving different tasks in different environment configurations to be applicable in future episodes.</p>
<p>Auto-curriculum Selection. Before we generate the meta-memory, it is important to choose memories extracted from the best trials from previous episodes because random sampling may not benefit CLIN in zero-shot generalization (Adaptive-Agent-Team et al., 2023). Following the prioritized level replay scheme (Jiang et al., 2021), we choose the most successful trial per episode and retrieve memories abstracted from those trials with a fixed archive of size 10, a hyperparameter.</p>
<p>Generating Meta-Memory. The goal of the meta-memory is to help CLIN generalize to unseen tasks and/or environments. While we keep the format of the causal abstractions the same as memories generated across trials, the prompt to generate the meta-memory is different than those used for generating per-trial memory. When the new memory is to be used for the same task but in a different environment, the prompt instructs for a meta-memory helpful "to solve the same task in a new environment configuration" given the target task description with an expectation that metamemory abstractions will entail generic causal insights about the task irrespective of environment configurations (see Figure 8). Similarly, when the new memory is to be used for a different task, the prompt is modified accordingly to reflect this (Figure 9). Along with the target task description for better memory generation, each past memory selected to generate the meta-memory is attached to the final rewards for the associated trials, allowing the generator to combine insights across episodes and assign the levels of uncertainty using the evidence of success.</p>
<h2>4 Results and Analysis</h2>
<p>Experimental Setup. Test-time adaptation and generalization via continual learning require a variety of complex tasks and environment configurations to allow an agent to explore, learn latent causal insights from interactions, and exploit them in the future. We choose ScienceWorld (Wang et al., 2022), a text-based interactive environment requiring complex interactive reasoning processes to solve a plethora of science-theory-based tasks spanning several diverse classes (e.g., thermodynamics, genetics, friction, etc.). The virtual space presents 10 sub-places: foundry, greenhouse, outside area, an art studio, workshop, kitchen, living room, bedroom, bathroom, and a hallway connecting inside rooms. The presence of several objects, their individual states, and action templates renders the search space intractable for any agent. ScienceWorld presents strikingly different environment configurations across task types, making it a rich testbed for evaluating adaptation and generalization. ScienceWorld tasks are partitioned into Short (S), e.g., pick \&amp; place and Long (L), e.g., grow plant, tasks based on the number of required actions to succeed.</p>
<p>Here, we define our setups for zero-shot adaptation (ADAPT) and generalization (GEN-ENV and GENTASK). For all setups, we test CLIN and competing baselines on 18 tasks (two task instances from 9 classes) in several environment configurations from the test split of the ScienceWorld benchmark resulting in a total of 164 task-environment combinations unless stated otherwise. We evaluate based on the final reward score provided by the ScienceWorld simulator.</p>
<p>ADAPT: This setup focuses on CLIN's ability to adapt to a task by attempting it for several trials in the same environment configuration. Most importantly, CLIN initializes with an empty memory at the beginning of the first trial and generates memory at the end of each trial. While the environment gets reset at the trial boundary, CLIN's memory continues to be updated, capturing informative causal abstractions pertaining to both successful and failed actions. Here, we compare with Reflexion (Shinn et al., 2023), a SOTA, however, CLIN differs from Reflexion by how the memory is abstracted.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Rapid task adaptation with CLIN. (a) Example tasks where CLIN improves scores across trials. For CLIN, Trial-0 is the BASE, Trial-4 is the ADAPT. (b) Comparison of CLIN with Reflexion (Shinn et al., 2023). (c) CLIN improves from BASE to ADAPT (full results in Appendix C).</p>
<p>GEN-ENV: In this setup, we focus on CLIN's ability to transfer its learning from past experiences to solve tasks in an unseen environment. For a task $m$, we run CLIN for 10 different (train) environment settings (with varying objects and starting locations) and then create meta-memories from its exploration to solve the same task in an unseen (test) environment. Here, we compare CLIN with RL methods DRRN (He et al., 2015), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), and CALM (Yao et al., 2020) trained on all (large) training variations with simulator reward and Generative Language agents, SayCan (Ahn et al., 2022), ReAct (Yao et al., 2022), and Reflexion (Shinn et al., 2023), prompted with few-shot demonstrations.</p>
<p>GEN-TASK: In this setup, we focus on CLIN's ability to transfer its learning from past experiences to solve a new task in the same environment. For an environment $e$, we run CLIN for to solve a task $m$ and then condense its learning to solve a novel task $m^{\prime}$ in the environment $e$. We took all test examples where we have a different task defined in the same environment configuration. (Adaptive-Agent-Team et al., 2023) suggests that transferring learning from a random task can be very hard; hence we couple tasks that are related (revolve around overlapping task-critical objects/locations such water, kitchen), such as boil and freeze to measure transfer learning from one to the other. This is a novel setup where we do not have any off-the-shelf baselines. However, here, we compare against CLIN-BASE, a strong baseline agent.</p>
<p>GEN-ADAPT (G+A): If CLIN, in GEN-ENV or GEN-TASK setting, does not successfully complete the new task, it can continue learning and retrying that task. We refer to this setup as GEN-ADAPT. CLIN can use any instruction-tuned LLM (Chung et al., 2022) as part of the controller, executor, and memory generator. In this paper, we use gpt-4, the same as our generative agent baselines.</p>
<h1>4.1 CLIN EXHIBITS RAPID TASK ADAPTATION</h1>
<p>Figure 4a demonstrates two example trends where CLIN learns from its own prior attempts (ADAPT) and gets better at solving a given task. Apart from length, the difficulty level of a task also depends on the environment configuration (hence, variance across environment configurations for each task). CLIN quickly adapts to a short task, Pick \&amp; Place, solving it in its 4th attempt, whereas for a longer task, Grow Fruit, it is not solved after 5th (max) attempts. Furthermore, Figure 4a depicts, CLIN becomes more efficient in later trials by solving the tasks with a lower number of (average) steps. Figure 4c shows an average number of attempts ${ }^{2}$ for CLIN to solve a task and $\%$ episodes per task where scores improved compared to its own first trial.</p>
<p>Next, we compare CLIN with Reflexion, the reflective SOTA agent, in Figure 4b. CLIN already starts off with a stronger base performance (see discussion in 4.3), however, CLIN's relative improvement in ADAPT is significantly stronger than Reflexion's gain from its base agent ReAct. Furthermore, CLIN's relative improvement is higher for longer tasks. This can be attributed to CLIN's persistent memory, which gets refined over past trials, whereas Reflexion may fall short of collecting useful learnings from earlier trials as it only focuses on the current trial for its reflections (hence not long-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RL Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generative Language Agents</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CLIN (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">KGA2C</td>
<td style="text-align: center;">CALM</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">BASE</td>
<td style="text-align: center;">GEN-ENV</td>
<td style="text-align: center;">G+A</td>
</tr>
<tr>
<td style="text-align: center;">Temp $_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">Temp $_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">58.2</td>
</tr>
<tr>
<td style="text-align: center;">Pick\&amp;Place ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Pick\&amp;Place ${ }_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">93.3</td>
</tr>
<tr>
<td style="text-align: center;">Lifespan ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Lifespan ${ }_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">Biology ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: center;">Boil</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: center;">Freeze</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">GrowPlant</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;">GrowFruit</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">94.5</td>
</tr>
<tr>
<td style="text-align: center;">Biology ${ }_{2}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: center;">Force</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Friction</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">94.0</td>
</tr>
<tr>
<td style="text-align: center;">Genetics ${ }_{1}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Genetics ${ }_{2}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">71.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">69.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparing CLIN with baselines for generalization across unseen environments
term). Furthermore, CLIN accumulates both useful (for the task) and harmful (for the task) causal learnings, whereas Reflexion only learns from its mistakes, lacking comprehensive learning.</p>
<h1>4.2 CLIN OUTPERFORMS SOTA, GENERALIZING TO NOVEL ENVIRONMENTS AND TASKS</h1>
<p>Generalizing to new environments. Table 1 compares CLIN with baselines that learn from training environmental variants for a task to improve its performance in a novel environment ${ }^{3}$. Language agents (including CLIN) that use NL feedback from the ScienceWorld (e.g., "Door to the kitchen is closed") perform significantly better compared to RL methods that purely rely on (sparse) numeric rewards from the environment to learn a policy. We observe a positive generalization effect in GEN-ENV (average 4 point gain) compared to BASE where CLIN tries to solve the tasks zero-shot. With a strong BASE performance, CLIN beats all baselines in generalization performance. Furthermore, in G+A, CLIN shows a substantial 16 additional improvement, beating the SOTA reflective agent by 23 points. Figure 5a additionally shows trend of improvement compared to when CLIN does not start with a meta-memory. Meta-memory helps CLIN with a stronger start than BASE ( 52.7 vs. 48.6), with a continued gain in scores till the end of Trial-4 (G+A: 69.5 vs. ADAPT: 62.2). The stronger start for CLIN with meta-memory also results in fewer steps to solve a task. Unlike imitation learning-based agents, TDT (Wang et al., 2022) and SwiftSage (Lin et al., 2023), CLIN (and most baselines) does not use any gold trajectories. Refining its memory only from self-generated trajectories, CLIN outperforms TDT on all 18 tasks and SwiftSage on 8/18 (mostly long) tasks.</p>
<p>Generalizing to new tasks. Mirroring trends from GEN-ENV, CLIN demonstrates strong transfer learning to new tasks with 13-point improvement over its BASE performance, being better at $38.8 \%$ of times (Figure 5c). The improvement attributes to critical learning about the environment ("apple juice is in the fridge", required for both boiling and freezing it), leading to improvement in previously low-performing tasks in both ADAPT and GEN-ENV setups. This transfer learning in GEN-TASK and $\mathrm{G}+\mathrm{A}$ helps CLIN to solve the tasks with a lesser number of steps ${ }^{4}$ and achieve higher rewards.</p>
<h3>4.3 DISCUSSION</h3>
<p>Importance of memory structure. CLIN extracts causal abstractions structured around 'necessary' and 'does not contribute' relations. As an ablation study, we modified our memory generator to generate free-form advice for future trials (without any constraint on their formats). We find that the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reward and #steps trends for CLIN in (a) GEN-ENV and (b) GEN-TASK. (c) \% episode improvements and score change than CLIN without meta-memory (GEN-TASK). (d) CLIN ablations.
average reward drops by 6 points (in $10 \%$ cases compared to CLIN) when using the unstructured memory, indicating the usefulness of causal abstractions, as shown in Figure 5d.</p>
<p>Superior BASE performance. Figure 4 depicts a superior BASE performance for CLIN than the final performance of both ReAct and Reflexion despite using the same underlying LLM (here, gpt-4). We find if we ablate for the controller module in CLIN, responsible for generating a goal before outputting the next action, CLIN's BASE performance drops in $44 \%$ cases. With an 18 point drop in average reward (see Figure 5d), Abl-Contoller-BASE version of CLIN becomes equivalent to ReAct, the base agent for Reflexion, demonstrating the importance of controller even in BASE setup.</p>
<p>A qualitative example. Figure 3 depicts how memory items get refined during task adaptation and for generalization for a task boil. Env2 has a working stove, whereas in Env1, the stove is broken, but a lighter is available as an alternative. With a number of trials in these environments, CLIN learns how to use these two devices to generate heat. In an unseen environment with a broken stove, CLIN quickly receives a positive reward by using a lighter to heat a substance. While insights within an episode are often specific, e.g., "Using the lighter on the metal pot should be necessary to heat the water in the pot", CLIN compiles these insights for a new target environment (as meta-memory), e.g., "Using a heat source (stove, lighter) on the container should be necessary to heat a substance." Appendix B contains examples of memories generated during adaptation and generalization.</p>
<p>Limitation: Lack of exploration. CLIN's learnings are dependent on its own past experience. If CLIN never explores a location in the environment or does not perform an action, an insight related to the unobserved activity can never be generated. Hence, exploration becomes important when task-critical location or action in unknown to CLIN from past trials. For example, in task of creating an orange paint, the agent is supposed to find red and yellow paints from the art studio. However, art studio is not visible when CLIN starts from location 'outside'. Unless the CLIN knows that there exists an art studio, it tries alternative method to create orange paints from other irrelevant objects (e.g., an orange) and remains unsuccessful. When a memory related art-studio appears from past exploration, CLIN is able to successfully complete the task. Similarly, in boil or freeze tasks, CLIN is unable to perform well which requires it to consistently measure the temperature of the substance to know its boiling/freezing point-an act it could never perform successfully in past trials resulting into less useful memory insights and subsequent lower performance in future trials.</p>
<p>Limitation: Poor memory retrieval. For a task of boiling gallium, CLIN is supposed to use oven/blast furnace and not a stove. In the meta-memory for boiling tasks, there are two insights regarding the act of boiling: "Activating stove should be necessary to boil a substance" and "Using an alternative heat source (e.g., oven or fire pit) may be necessary if the initial heat source is insufficient." However, CLIN repeatedly retrieves the former and hence failing at the task despite performing other actions (e.g., finding gallium) correctly. This problem intensifies at the initial trial during</p>
<p>generalization due to the presence of insights with varied initial conditions for them to be applied. This can be circumvented by improved memory representation, which we leave as a future work.</p>
<h1>5 CONCLUSION</h1>
<p>Our goal is a system that can continually improve over time, both while rapidly adapting to a task by multiple retries and efficiently generalizing to novel tasks and environments. We propose CLIN, an architecture for language agents that constructs a persistent, dynamic memory of causal abstractions, refines it over time and uses it effectively to improve its performance on future tasks, achieving state-of-the-art performance. Our work systematically evaluates a novel nonparametric learning paradigm, promising never-ending learning abilities to frozen language agents.</p>
<p>Acknowledgement We sincerely thank Aristo team members Tushar Khot, Ashish Sabharwal, Shashank Gupta, Nathaniel Weir, Kyle Richardson, Jiangjie Chen, Archiki Prasad, and other members such as Faeze Brahman, Alexander Koller at the Allen Institute of AI for their generous feedback.</p>
<h2>REFERENCES</h2>
<p>Adaptive-Agent-Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreyaan Pathak, Nicolas Perez Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, and Lei M. Zhang. Human-timescale adaptation in an open-ended task space. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:255998274.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, KuangHuei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022. URL https://api.semanticscholar.org/CorpusID:247939706.</p>
<p>Diego Aineto, Sergio Jiménez, and Eva Onaindía. Learning strips action models with classical planning. In International Conference on Automated Planning and Scheduling, 2018. URL https://api.semanticscholar.org/CorpusID:49405691.</p>
<p>Prithviraj Ammanabrolu and Matthew J. Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In ICLR, 2020.</p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktaschel, and Jason Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In North American Chapter of the Association for Computational Linguistics, 2020. URL https://api.semanticscholar.org/CorpusID:222125301.</p>
<p>Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. ArXiv, abs/1707.01495, 2017. URL https://api.semanticscholar.org/CorpusID:3532908.</p>
<p>Ankuj Arora, Humbert Fiorino, Damien Pellier, Marc Métivier, and Sylvie Pesty. A review of learning planning action models. The Knowledge Engineering Review, 33, 2018. URL https: //api.semanticscholar.org/CorpusID:56483203.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar. org/CorpusID:235294299.</p>
<p>Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkongu Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022. URL https://api.semanticscholar.org/CorpusID:253018554.</p>
<p>Bhavana Dalvi, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. In EMNLP, 2022.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. arXiv: Artificial Intelligence, 2015. URL https://api.semanticscholar.org/CorpusID:15986631.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022.</p>
<p>Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Nicolaus Foerster, Edward Grefenstette, and Tim Rocktaschel. Replay-guided adversarial environment design. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar.org/CorpusID:238408352.</p>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. ArXiv, abs/2305.17390, 2023. URL https: //api.semanticscholar.org/CorpusID:258960143.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with selffeedback. ArXiv, abs/2303.17651, 2023. URL https://api.semanticscholar.org/CorpusID: 257900871 .</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv, abs/1312.5602, 2013. URL https://api.semanticscholar.org/CorpusID:15238391.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Trans. Mach. Learn. Res., 2022, 2022. URL https://api.semanticscholar.org/CorpusID:248722148.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908. 10084 .</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve GPT-3 after deployment. In ACL Workshop on Commonsense Representation and Reasoning (CSRR'22), 2022. (also arxiv:2201.06009).</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv, abs/2305.16291, 2023. URL https://api.semanticscholar.org/CorpusID: 258887849 .</p>
<p>Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID:247451124.</p>
<p>Shunyu Yao, Rohan Rao, Matthew J. Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language models for action generation in text-based games. ArXiv, abs/2010.02903, 2020. URL https://api.semanticscholar.org/CorpusID:222142129.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395.</p>
<h1>A CLIN PROMPTS</h1>
<p>Figures 6 to 9 are the complete prompts for next-action generation (controller + executor), memorygenerator during ADAPT, GEN-ENV, and GEN-TASK.</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">System</span><span class="o">]</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="n">helping</span><span class="w"> </span><span class="k">execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">science</span><span class="w"> </span><span class="n">experiment</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">simulated</span>
<span class="n">environment</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">limited</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">step</span><span class="p">.</span>
<span class="o">[</span><span class="n">User</span><span class="o">]</span><span class="err">:</span>
<span class="n">Possible</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">OBJ</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="p">)</span><span class="err">:</span>
<span class="err">{</span><span class="n">objects_str</span><span class="err">}</span>
<span class="n">Your</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="nl">formats</span><span class="p">:</span>
<span class="n">Possible</span><span class="w"> </span><span class="nl">actions</span><span class="p">:</span>
<span class="err">{</span><span class="n">actions_str</span><span class="err">}</span>
<span class="k">If</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">say</span><span class="w"> </span><span class="err">\</span><span class="ss">&quot;Ambiguous request\&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">things</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">case</span><span class="p">,</span>
<span class="n">respond</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">corresponding</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="p">.</span>
<span class="n">What</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="k">next</span><span class="vm">?</span>
<span class="k">First</span><span class="p">,</span><span class="w"> </span><span class="n">scan</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="p">(</span><span class="n">unordered</span><span class="p">)</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">learnings</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">provided</span><span class="p">.</span><span class="w"> </span><span class="n">Decide</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span>
<span class="n">learnings</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">applicable</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">last</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">make</span><span class="w"> </span><span class="n">progress</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">task</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span>
<span class="k">only</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">selected</span><span class="w"> </span><span class="n">learnings</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">,</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">rationale</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">picking</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span>
<span class="k">action</span><span class="p">.</span><span class="w"> </span><span class="k">If</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="n">Learning</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">selected</span><span class="p">,</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rationale</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">last</span>
<span class="n">observation</span><span class="p">.</span><span class="w"> </span><span class="nf">Format</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">follows</span><span class="p">:</span>
<span class="k">Write</span><span class="w"> </span><span class="s1">&#39;I used learning id(s):&#39;</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="p">;</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="k">no</span>
<span class="n">learnings</span><span class="w"> </span><span class="n">selected</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span><span class="p">,</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="err">$$$</span><span class="w"> </span><span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rationale</span><span class="p">.</span><span class="w"> </span><span class="n">Finally</span><span class="p">,</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="err">###</span>
<span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">think</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">completed</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">TASK_COMPLETE</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">requires</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="s1">&#39;focus&#39;</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">something</span><span class="w"> </span><span class="p">(</span><span class="n">OBJ</span><span class="p">),</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">FOCUS</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="o">&lt;</span><span class="n">OBJ</span><span class="o">&gt;</span><span class="w"> </span><span class="k">as</span>
<span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span><span class="w"> </span><span class="n">FOCUS</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">extremely</span><span class="w"> </span><span class="n">critical</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span>
<span class="k">of</span><span class="w"> </span><span class="n">times</span><span class="w"> </span><span class="s1">&#39;focus&#39;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">mentioned</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">description</span><span class="p">.</span><span class="w"> </span><span class="k">Using</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="ow">or</span>
<span class="n">inappropriately</span><span class="w"> </span><span class="p">(</span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="k">terminate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">session</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">will</span>
<span class="n">be</span><span class="w"> </span><span class="n">rendered</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">incomplete</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">performed</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">requires</span><span class="w"> </span><span class="n">waiting</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">effect</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="s1">&#39;wait&#39;</span>
<span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span>
</code></pre></div>

<p>Figure 6: Prompt for the Controller and the Executor</p>
<h2>B EXAMPLE MEMORIES</h2>
<p>Example generated memory for ADAPT, GEN-ENV, and GEN-TASKsetups in Figures 10 to 12.</p>
<p>[System]: You are an expert assistant.
[User]:
You are given CURRENT TRACE, a sequence of actions that an agent made in a world to accomplish a task.</p>
<p>Task is detailed at the beginning.
For each action, there is a rationale why the agent made that action.
There is an observation that provide details about the new state of the world after each action was executed.
The CURRENT TRACE is accompanied by an EVALUATION REPORT indicating the success of the attempt to the task.</p>
<p>You can also be provided with PREVIOUS LEARNINGS which are learnings from the previous attempts by the agent for the same task in the same environment/world. TASK indicates the task description. EPISODE indicates the number of previous attempts of the task.</p>
<p>Generate a summary of learning, as a numbered list, that will help the agent to successfully accomplish the SAME task AGAIN, in the SAME world.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY BE CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
{CURRENT TRACE}
Action: ...
Observation: ...
...
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>Summary of learning as a numbered list:</p>
<p>Figure 7: Prompt for CLIN's memory generator during ADAPT</p>
<p>[System]: You are an expert assistant.
[User]: You are given a collection of learning lists, that are derived from actions made by an agent and subsequent observations from a world to accomplish a TYPE of TASKs. All of these TASKs belong to a same TYPE (such as 'boiling') but they are executed in different ENVIRONMENT configurations. A different ENVIRONMENT configuration means there are presence of a different set of objects (lighter instead of a stove) that are critical for solving the TASK, presence of a different set of distractor objects that are not useful for the TASK, a different floor plan, etc.</p>
<p>For each learning list, the TASK description is provided at the beginning as TASK:
Each learning list indicates a list of learnings from the agent's best attempt to solve the TASK.</p>
<p>Each learning list is associated with an EVALUATION REPORT indicated how sucessful the respective attempt was for solving the task.</p>
<p>Consider all learning lists and combine them in to a summary of learnings, as a numbered list, that will help the agent to successfully accomplish a NEW TASK related to the previous TASKs (such as 'boling') in an ENVIRONMENT configuration that it has not seen before. The NEW TASK description will be provided.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY NOT CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
{PREVIOUS LEARNINGS}
TASK: ...
LEARNINGS:...
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>NEW TASK: ...
Summary of learning as a numbered list:</p>
<p>Figure 8: Prompt for CLIN's memory generator during GEN-ENV</p>
<p>[System]: You are an expert assistant.
[User]: You may be given a list of learnings, that are derived from actions made by an agent and subsequent observations from a world to accomplish a TASK in an ENVIRONMENT CONFIGURATION.</p>
<p>For the learning list, the TASK description is provided at the beginning as TASK:
The learnings are from the agent's best attempt to solve the TASK.
The learning list is associated with an EVALUATION REPORT indicated how sucessful the attempt was for solving the task.</p>
<p>Now, generate a summary of learnings from the existing ones if provided, such that they will be useful to the NEW TASK in the SAME ENVIRONMENT CONFIGURATION. The NEW TASK may require different actions which are not captured in the given learnings but given learnings can be used to infer about the ENVIRONMENT CONFIGURATION. The NEW TASK description will be given. If PREVIOUS LEARNINGS says 'No learnings available', improvise learnings for the NEW TASK.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY NOT CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
(PREVIOUS LEARNINGS)
TASK: . . .
LEARNINGS: . . .
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>NEW TASK: ...
Summary of learning as a numbered list:</p>
<p>Figure 9: Prompt for CLIN's memory generator during GEN-TASK</p>
<p>Task: Your task is to determine whether round seed shape is a dominant or recessive trait in the pea plant. If the trait is dominant, focus on the blue box. If the trait is recessive, focus on the orange box.</p>
<p>Memory (ADAPT), after Trial5:</p>
<ol>
<li>Looking around MAY BE NECESSARY to gather information about the environment.</li>
<li>Moving to the greenhouse SHOULD BE NECESSARY to find the pea seeds.</li>
<li>Picking the jar SHOULD BE NECESSARY to access the pea seeds.</li>
<li>Moving the round green pea seed to flower pot 2 SHOULD BE NECESSARY to grow the pea plants.</li>
<li>Moving the wrinkly green pea seed to flower pot 3 SHOULD BE NECESSARY to grow the pea plants.</li>
<li>Waiting SHOULD BE NECESSARY for the pea plants to grow and show their traits.</li>
<li>Looking at the adult pea plant in flower pots 2 and 3 MAY BE NECESSARY to observe the traits of the offspring.</li>
<li>Focusing on the blue box SHOULD BE NECESSARY to indicate the trait of the seed shape.</li>
<li>Continuously looking at the same environment DOES NOT CONTRIBUTE to the task of determining the dominant or recessive trait in the pea plant.</li>
<li>Moving to the workshop DOES NOT CONTRIBUTE to determining the dominant or recessive trait in the pea plant.</li>
<li>Picking up the battery or the wires in the workshop DOES NOT CONTRIBUTE to finding the pea seeds.</li>
<li>Waiting for longer periods MAY BE NECESSARY to allow the pea plants to fully grow and show their traits.</li>
</ol>
<p>Figure 10: Memory generated after Trial5 in ADAPT for a Genetics task.</p>
<p>Task: Your task is to determine which of the two inclined planes (aluminum, platinum) has the most friction. After completing your experiment, focus on the inclined plane with the most friction.</p>
<p>Meta-memory (GEN-ENV):</p>
<ol>
<li>Moving to the hallway SHOULD BE NECESSARY to reach the workshop.</li>
<li>Moving to the workshop SHOULD BE NECESSARY to find the block.</li>
<li>Picking up the block SHOULD BE NECESSARY to move it to the inclined planes.</li>
<li>Placing the block on the first inclined plane (either aluminum or platinum) SHOULD BE NECESSARY to measure the friction.</li>
<li>Activating the stopwatch SHOULD BE NECESSARY to time the experiment.</li>
<li>Waiting for a certain period MAY CONTRIBUTE to observing the friction effect.</li>
<li>Deactivating the stopwatch SHOULD BE NECESSARY to stop timing the experiment.</li>
<li>Moving the block to the second inclined plane (either aluminum or platinum) SHOULD BE NECESSARY to compare the friction.</li>
<li>Activating the stopwatch again SHOULD BE NECESSARY to time the second part of the experiment.</li>
<li>Waiting for a certain period again MAY BE NECESSARY to observe the friction effect.</li>
<li>Deactivating the stopwatch again SHOULD BE NECESSARY to stop timing the experiment.</li>
<li>Focusing on the inclined plane with the most friction SHOULD BE NECESSARY to conclude the experiment.</li>
<li>Repeating the experiment multiple times MAY BE NECESSARY for more accurate results.</li>
<li>Looking around in the initial room multiple times DOES NOT CONTRIBUTE to the task.</li>
<li>Moving the block back and forth between the two inclined planes DOES NOT CONTRIBUTE to the task.</li>
</ol>
<p>Figure 11: Meta-memory used in GEN-ENV for a Friction task.</p>
<p>Task: Your task is to determine whether round seed shape is a dominant or recessive trait in the pea plant. If the trait is dominant, focus on the blue box. If the trait is recessive, focus on the orange box.</p>
<p>Meta-memory (GEN-TASK):
Task: Your task is to freeze mercury. First, focus on the substance. Then, take actions that will cause it to change its state of matter.</p>
<p>Meta-memory (GEN-TASK):</p>
<ol>
<li>Looking around MAY BE NECESSARY to identify the available resources and the layout of the environment.</li>
<li>Moving to different rooms SHOULD BE NECESSARY to find the tools and materials needed to change the state of the substance.</li>
<li>Picking up items like glass cups or metal pots SHOULD BE NECESSARY to contain the substance for changing its state.</li>
<li>Focusing on the substance SHOULD BE NECESSARY to understand its properties and how to interact with it.</li>
<li>Picking up the thermometer SHOULD BE NECESSARY to monitor the temperature of the substance.</li>
<li>Using the thermometer on the substance SHOULD BE NECESSARY to monitor the progress of the task.</li>
<li>Puring the substance into the container SHOULD BE NECESSARY to prepare it for cooling.</li>
<li>Moving the container to a cooling device SHOULD BE NECESSARY to cool the substance.</li>
<li>Waiting for a period of time after cooling the substance SHOULD BE NECESSARY to allow the substance to change state.</li>
<li>Repeatedly checking the temperature of the substance SHOULD BE NECESSARY to monitor the progress of the task.</li>
<li>Activating the stove DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Picking up unrelated items like a lighter DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Moving to unrelated rooms like the workshop DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Teleporting to the kitchen MAY BE NECESSARY for the task as it speeds up the process of moving between rooms.</li>
<li>Using the thermometer multiple times on the substance after it reaches freezing point DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
</ol>
<p>Figure 12: Meta-memory used in GEN-TASK for a Freeze task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generative L. Agents</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CLIN (ours)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">BASE</td>
<td style="text-align: center;">ADAPT</td>
</tr>
<tr>
<td style="text-align: left;">Temp</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">14.3</td>
</tr>
<tr>
<td style="text-align: left;">Temp</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: left;">Pick\&amp;Place</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Pick\&amp;Place</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: left;">Lifespan</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Lifespan</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: left;">Biology</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: left;">Boil</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">15.2</td>
</tr>
<tr>
<td style="text-align: left;">Freeze</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: left;">GrowPlant</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">GrowFruit</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">71.6</td>
</tr>
<tr>
<td style="text-align: left;">Biology</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">81.0</td>
</tr>
<tr>
<td style="text-align: left;">Force</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Friction</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">72.5</td>
</tr>
<tr>
<td style="text-align: left;">Genetics</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Genetics</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">92.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">$\mathbf{6 2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">$\mathbf{6 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">$\mathbf{6 2 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparing CLIN with baselines for adaptation</p>
<h1>C MORE RESULTS</h1>
<p>Full results for CLIN outperforming Reflexion is in Table 2. For ScienceWorld benchmark, we exclude electricity tasks since they deviate from standard electrical conventions, prohibiting us from fairly using LLM agents. We choose the first 10 test variants for each 18 tasks selected. The full list of 18 tasks from the benchmark, with the number of test variants used in parentheses:
grow-plant (10), identify-life-stages-1 (5), grow-fruit (10), measure-melting-point-known-substance (10), mendelian-genetics-unknown-plant (10), chemistry-mix-paint-secondary-color (9), freeze (9), lifespan-longest-lived (10), inclined-plane-determine-angle (10), boil (9), use-thermometer (10), chemistry-mix (8), lifespan-shortest-lived (10), find-plant (10), find-living-thing (10), identify-life-stages-2 (4), mendelian-genetics-known-plant (10), inclined-plane-friction-named-surfaces (10).
Short tasks have oracle lengths less than 37 steps (median), and Long tasks have oracle lengths more than equal to 37 steps.</p>
<p>The map to the short names used for tasks in the paper:
Temp: use-thermometer, measure-melting-point-known-substance; Pick\&amp;Place: find-plant, find-living-thing; Chemistry: chemistry-mix, chemistry-mix-paint-secondary-color; Lifespan: lifespan-longest-lived, lifespan-shortest-lived; Biology: identify-life-stages-1, identify-life-stages-2, Boil; Freeze; Grow Plant, Grow Fruit; Force: inclined-plane-determine-angle; Friction: inclined-plane-friction-named-surfaces; Genetics: mendelian-genetics-known-plant, mendelian-genetics-unknownplant.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Baseline numbers are derived from Table 1 in (Lin et al., 2023)
${ }^{4} #$ steps in Figure 5a,b are normalized between $0-1,1$ being maximum # steps allowed for a task.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
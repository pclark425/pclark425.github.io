<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4376 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4376</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4376</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-281682866</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.25499v1.pdf" target="_blank">Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking</a></p>
                <p><strong>Paper Abstract:</strong> Human-AI interaction researchers face an overwhelming challenge: synthesizing insights from thousands of empirical studies to understand how AI impacts people and inform effective design. Existing approach for literature reviews cluster papers by similarities, keywords or citations, missing the crucial cause-and-effect relationships that reveal how design decisions impact user outcomes. We introduce the Atlas of Human-AI Interaction, an interactive web interface that provides the first systematic mapping of empirical findings across 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach identifies causal relationships, and visualizes them through an AI-enabled interactive web interface as a navigable knowledge graph. We extracted 2,037 empirical findings, revealing research topic clusters, common themes, and disconnected areas. Expert evaluation with 20 researchers revealed the system's effectiveness for discovering research gaps. This work demonstrates how AI can transform literature synthesis itself, offering a scalable framework for evidence-based design, opening new possibilities for computational meta-science across HCI and beyond.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4376.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4376.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atlas of Human-AI Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, LLM-powered research synthesis platform that extracts empirical findings from paper abstracts, converts them into structured [cause, relationship, effect] triplets, clusters and normalizes entities via embeddings, and constructs a navigable knowledge graph visualized through 3D graph, Cause-Effect (Sankey), and Paper views.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Atlas of Human-AI Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A five-stage pipeline: (1) targeted paper/abstract collection, (2) LLM-powered empirical findings extraction and conversion into SPO triplets, (3) triplet key embedding and synonym-merge via density clustering, (4) semantic entity clustering (k-means) and cluster naming via LLM, and (5) structured graph construction with community detection and interactive visualizations (3D force graph, Sankey Cause-Effect, Paper View). The runtime stack includes Claude Opus 4.1 for natural-language extraction and triplet conversion, Qwen3-Embedding-8B for key embeddings, DBSCAN and k-means for clustering/normalization, Louvain and structural-hole analyses for network-level synthesis, and a Svelte/Three.js/D3.js web front-end for interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Claude Opus 4.1 (claude-opus-4-1-20250805) for text extraction and cluster naming; Qwen3-Embedding-8B (4096-d) for semantic embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting with task-specific schema (SPO Subject-Predicate-Object), multi-stage prompts: (a) extract concrete empirical findings from abstracts, (b) transform findings into structured triplets with canonicalized subject:type/subtype/feature fields.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Embedding-based synonym merge (DBSCAN on embeddings), semantic entity clustering (k-means), knowledge-graph construction from normalized triplets, community detection (Louvain), structural-hole analysis (Burt's measures) to surface cross-community synthesis opportunities.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1,888 collected; 1,124 (59.53%) with extractable empirical findings resulting in 2,037 extracted findings</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Human-AI interaction / HCI literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured triplets ([cause, relationship, effect]), knowledge graph (nodes = entity-feature pairs, edges = empirical relations), interactive visualizations (3D graph, Cause-Effect Sankey, Paper View)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Graph statistics (nodes, edges), clustering counts (clusters per type), community modularity (Louvain modularity), Burt's constraint/structural-hole scores, user study subjective Likert ratings and qualitative feedback</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Extracted 2,037 empirical findings; produced a knowledge graph of 1,310 nodes and 3,908 edges; synonym-merge produced 62 DBSCAN clusters (eps=0.2); semantic clustering yielded 7 human, 8 AI, 8 concept clusters; Louvain detected 126 communities (modularity=0.669). User study with 20 expert participants: 3D Graph View usefulness for gap identification M=4.95 (SD=1.24); Cause-Effect View M=5.45 (SD=1.12).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively to traditional literature review/search workflows (keyword/citation-based searches, manual reviews); users reported ad-hoc use of GPT for summarization as an informal baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative improvement: participants reported faster discovery of cross-disciplinary connections and easier identification of research gaps compared to traditional keyword/citation searches; no formal quantitative improvement over human baselines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM prompting + structured schemas can scale extraction of empirical findings into machine-readable triplets; embedding-based normalization reduces lexical variability; knowledge-graph synthesis surfaces structural holes and cross-community bridges (e.g., LLMs and generative AI span many communities), and interactive visualizations support exploratory sensemaking for expert users.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Extraction limits: abstracts-only extraction misses depth present in full papers; semantic direction/valence interpretation can be context-dependent (e.g., 'DECREASES' may be beneficial in some contexts); domain limits (performs less well for mathematical/formula-heavy literature); potential LLM errors/hallucinations, coverage gaps, and need for improved verification and contradiction resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Implemented on ~1.9k collected abstracts and produced ~2k findings; authors discuss continuous updates and automated ingestion for scalability but provide no controlled scaling experiments across orders of magnitude or model-size ablations; embedding dimensionality (4096) and clustering hyperparameters (DBSCAN eps=0.2) tuned for this scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4376.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4376.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Chains</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Chains (chaining large language model prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that composes sequences of LLM prompts (chains) to create transparent and controllable human-AI interactions; cited here as producing improvements in perceived transparency and controllability via prompt-chaining techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Chains (prompt chaining)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A method that decomposes complex tasks into sequences of LLM prompt steps (chains), enabling intermediate outputs, user control points, and improved interpretability/controllability of LLM-driven workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-chaining (multi-step LLM prompting) rather than single-shot extraction; used to assemble transparency/controllability affordances</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-step LLM composition to produce richer, controllable outputs and improve user model of the system; cited as increasing perceived transparency and collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1 (single cited study)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Human-AI interaction / HCI</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Interaction-level behavior improvements and enhanced transparency (not explicitly a literature-synthesis product)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>User perception measures (subjective measures of transparency, controllability, sense of collaboration) in the cited study</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited finding: 'llm chains significantly enhanced users' perceived system transparency, controllability, and sense of collaboration' (no numerical effect sizes provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against non-chained or single-shot LLM interfaces (baseline without chains)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported qualitative/subjective improvements in user perceptions; no numeric comparisons provided here</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chaining LLM prompts can increase perceived transparency and user control, making LLM-driven processes more interpretable and actionable for end-users.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper only cites the approach; broader limitations not detailed here (possible increased latency, design complexity, and error propagation across chained steps).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4376.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4376.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dunn et al. 2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that applies fine-tuned large language models to extract structured information from complex scientific text, demonstrating LLM fine-tuning for information extraction tasks in scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fine-tuned LLM-based scientific IE (Dunn et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in citation only: a system that fine-tunes large language models to perform structured information extraction from scientific text; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>fine-tuned large language models (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Fine-tuning LLMs for structured information extraction from scientific documents</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific text / scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured information extraction (e.g., entities, relations) from scientific text</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of LLM application for scientific text processing beyond metadata extraction; supports the premise that fine-tuned LLMs can produce structured outputs from complex scientific prose.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4376.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4376.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative IE Survey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for generative information extraction: A survey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited survey that reviews approaches using large language models for generative information extraction from text, summarizing techniques, capabilities, and challenges of using LLMs for IE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for generative information extraction: A survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative Information Extraction (surveyed methods)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Survey-level treatment of generative IE methods that leverage LLMs to produce structured outputs (e.g., triples, tables) from unstructured text, covering prompting strategies, few-shot/zero-shot and fine-tuning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Generative information extraction using LLMs (prompting, few-shot, fine-tuning) as summarized by the survey</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Survey synthesizes multiple approaches; does not itself implement synthesis across papers</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General NLP / information extraction literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey (meta-analysis) of methods and tasks for LLM-driven IE</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to situate the Atlas among recent advances in LLM-based scientific text processing and generative IE methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4376.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4376.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maciej et al. 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study demonstrating extraction of materials-science data from research papers using conversational language models plus prompt-engineering techniques to improve accuracy of extracted structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conversational LLM + prompt-engineering extractor (materials domain)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in citation only: uses conversational LLMs and engineered prompts to extract materials data from papers; domain-focused extraction pipeline leveraging dialogue-style interactions for improved accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>conversational language models (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Conversational LLM interactions with engineered prompts to extract domain-specific data</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Materials science literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured materials data (e.g., measured properties) extracted from papers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy of extracted materials data (implied by title)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that conversational LLMs with careful prompt engineering can achieve accurate extraction of domain-specific scientific data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4376.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4376.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Song et al. 2025</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently cited system that uses multiple fine-tuned open-source LLM actors to extract key insights from scientific articles, presumably via an ensemble or multi-actor workflow for robust key-insight extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-actor fine-tuned open-source LLM key-insight extractor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited system (details not provided in this paper) that appears to use an ensemble or 'multi-actor' architecture composed of multiple fine-tuned open-source LLMs to extract salient insights from scientific articles.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>fine-tuned open-source large language models (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-actor ensemble of fine-tuned LLMs for key-insight extraction</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Ensembling/multi-actor aggregation of outputs to produce robust key insights (implied by title)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific article key-insight extraction (general scientific literature)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Key-insight extractions from scientific articles</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a contemporary example of open-source fine-tuned LLM systems designed to extract structured insights from scientific articles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from complex scientific text with fine-tuned large language models <em>(Rating: 2)</em></li>
                <li>Large language models for generative information extraction: A survey <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models <em>(Rating: 2)</em></li>
                <li>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4376",
    "paper_id": "paper-281682866",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Atlas",
            "name_full": "Atlas of Human-AI Interaction",
            "brief_description": "An open-source, LLM-powered research synthesis platform that extracts empirical findings from paper abstracts, converts them into structured [cause, relationship, effect] triplets, clusters and normalizes entities via embeddings, and constructs a navigable knowledge graph visualized through 3D graph, Cause-Effect (Sankey), and Paper views.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Atlas of Human-AI Interaction",
            "system_description": "A five-stage pipeline: (1) targeted paper/abstract collection, (2) LLM-powered empirical findings extraction and conversion into SPO triplets, (3) triplet key embedding and synonym-merge via density clustering, (4) semantic entity clustering (k-means) and cluster naming via LLM, and (5) structured graph construction with community detection and interactive visualizations (3D force graph, Sankey Cause-Effect, Paper View). The runtime stack includes Claude Opus 4.1 for natural-language extraction and triplet conversion, Qwen3-Embedding-8B for key embeddings, DBSCAN and k-means for clustering/normalization, Louvain and structural-hole analyses for network-level synthesis, and a Svelte/Three.js/D3.js web front-end for interaction.",
            "llm_model_used": "Claude Opus 4.1 (claude-opus-4-1-20250805) for text extraction and cluster naming; Qwen3-Embedding-8B (4096-d) for semantic embeddings",
            "extraction_technique": "Structured prompting with task-specific schema (SPO Subject-Predicate-Object), multi-stage prompts: (a) extract concrete empirical findings from abstracts, (b) transform findings into structured triplets with canonicalized subject:type/subtype/feature fields.",
            "synthesis_technique": "Embedding-based synonym merge (DBSCAN on embeddings), semantic entity clustering (k-means), knowledge-graph construction from normalized triplets, community detection (Louvain), structural-hole analysis (Burt's measures) to surface cross-community synthesis opportunities.",
            "number_of_papers": "1,888 collected; 1,124 (59.53%) with extractable empirical findings resulting in 2,037 extracted findings",
            "domain_or_topic": "Human-AI interaction / HCI literature",
            "output_type": "Structured triplets ([cause, relationship, effect]), knowledge graph (nodes = entity-feature pairs, edges = empirical relations), interactive visualizations (3D graph, Cause-Effect Sankey, Paper View)",
            "evaluation_metrics": "Graph statistics (nodes, edges), clustering counts (clusters per type), community modularity (Louvain modularity), Burt's constraint/structural-hole scores, user study subjective Likert ratings and qualitative feedback",
            "performance_results": "Extracted 2,037 empirical findings; produced a knowledge graph of 1,310 nodes and 3,908 edges; synonym-merge produced 62 DBSCAN clusters (eps=0.2); semantic clustering yielded 7 human, 8 AI, 8 concept clusters; Louvain detected 126 communities (modularity=0.669). User study with 20 expert participants: 3D Graph View usefulness for gap identification M=4.95 (SD=1.24); Cause-Effect View M=5.45 (SD=1.12).",
            "comparison_baseline": "Compared qualitatively to traditional literature review/search workflows (keyword/citation-based searches, manual reviews); users reported ad-hoc use of GPT for summarization as an informal baseline.",
            "performance_vs_baseline": "Qualitative improvement: participants reported faster discovery of cross-disciplinary connections and easier identification of research gaps compared to traditional keyword/citation searches; no formal quantitative improvement over human baselines provided.",
            "key_findings": "LLM prompting + structured schemas can scale extraction of empirical findings into machine-readable triplets; embedding-based normalization reduces lexical variability; knowledge-graph synthesis surfaces structural holes and cross-community bridges (e.g., LLMs and generative AI span many communities), and interactive visualizations support exploratory sensemaking for expert users.",
            "limitations_challenges": "Extraction limits: abstracts-only extraction misses depth present in full papers; semantic direction/valence interpretation can be context-dependent (e.g., 'DECREASES' may be beneficial in some contexts); domain limits (performs less well for mathematical/formula-heavy literature); potential LLM errors/hallucinations, coverage gaps, and need for improved verification and contradiction resolution.",
            "scaling_behavior": "Implemented on ~1.9k collected abstracts and produced ~2k findings; authors discuss continuous updates and automated ingestion for scalability but provide no controlled scaling experiments across orders of magnitude or model-size ablations; embedding dimensionality (4096) and clustering hyperparameters (DBSCAN eps=0.2) tuned for this scale.",
            "uuid": "e4376.0",
            "source_info": {
                "paper_title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "AI Chains",
            "name_full": "AI Chains (chaining large language model prompts)",
            "brief_description": "An approach that composes sequences of LLM prompts (chains) to create transparent and controllable human-AI interactions; cited here as producing improvements in perceived transparency and controllability via prompt-chaining techniques.",
            "citation_title": "AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts",
            "mention_or_use": "mention",
            "system_name": "AI Chains (prompt chaining)",
            "system_description": "A method that decomposes complex tasks into sequences of LLM prompt steps (chains), enabling intermediate outputs, user control points, and improved interpretability/controllability of LLM-driven workflows.",
            "llm_model_used": null,
            "extraction_technique": "Prompt-chaining (multi-step LLM prompting) rather than single-shot extraction; used to assemble transparency/controllability affordances",
            "synthesis_technique": "Multi-step LLM composition to produce richer, controllable outputs and improve user model of the system; cited as increasing perceived transparency and collaboration.",
            "number_of_papers": "1 (single cited study)",
            "domain_or_topic": "Human-AI interaction / HCI",
            "output_type": "Interaction-level behavior improvements and enhanced transparency (not explicitly a literature-synthesis product)",
            "evaluation_metrics": "User perception measures (subjective measures of transparency, controllability, sense of collaboration) in the cited study",
            "performance_results": "Cited finding: 'llm chains significantly enhanced users' perceived system transparency, controllability, and sense of collaboration' (no numerical effect sizes provided in this paper)",
            "comparison_baseline": "Compared against non-chained or single-shot LLM interfaces (baseline without chains)",
            "performance_vs_baseline": "Reported qualitative/subjective improvements in user perceptions; no numeric comparisons provided here",
            "key_findings": "Chaining LLM prompts can increase perceived transparency and user control, making LLM-driven processes more interpretable and actionable for end-users.",
            "limitations_challenges": "Paper only cites the approach; broader limitations not detailed here (possible increased latency, design complexity, and error propagation across chained steps).",
            "scaling_behavior": null,
            "uuid": "e4376.1",
            "source_info": {
                "paper_title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Dunn et al. 2022",
            "name_full": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "brief_description": "A cited work that applies fine-tuned large language models to extract structured information from complex scientific text, demonstrating LLM fine-tuning for information extraction tasks in scientific domains.",
            "citation_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "mention_or_use": "mention",
            "system_name": "Fine-tuned LLM-based scientific IE (Dunn et al. 2022)",
            "system_description": "Described in citation only: a system that fine-tunes large language models to perform structured information extraction from scientific text; details not provided in this paper.",
            "llm_model_used": "fine-tuned large language models (unspecified in this paper)",
            "extraction_technique": "Fine-tuning LLMs for structured information extraction from scientific documents",
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "General scientific text / scientific literature",
            "output_type": "Structured information extraction (e.g., entities, relations) from scientific text",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as an example of LLM application for scientific text processing beyond metadata extraction; supports the premise that fine-tuned LLMs can produce structured outputs from complex scientific prose.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4376.2",
            "source_info": {
                "paper_title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Generative IE Survey",
            "name_full": "Large language models for generative information extraction: A survey",
            "brief_description": "A cited survey that reviews approaches using large language models for generative information extraction from text, summarizing techniques, capabilities, and challenges of using LLMs for IE tasks.",
            "citation_title": "Large language models for generative information extraction: A survey",
            "mention_or_use": "mention",
            "system_name": "Generative Information Extraction (surveyed methods)",
            "system_description": "Survey-level treatment of generative IE methods that leverage LLMs to produce structured outputs (e.g., triples, tables) from unstructured text, covering prompting strategies, few-shot/zero-shot and fine-tuning approaches.",
            "llm_model_used": null,
            "extraction_technique": "Generative information extraction using LLMs (prompting, few-shot, fine-tuning) as summarized by the survey",
            "synthesis_technique": "Survey synthesizes multiple approaches; does not itself implement synthesis across papers",
            "number_of_papers": null,
            "domain_or_topic": "General NLP / information extraction literature",
            "output_type": "Survey (meta-analysis) of methods and tasks for LLM-driven IE",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited to situate the Atlas among recent advances in LLM-based scientific text processing and generative IE methods.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4376.3",
            "source_info": {
                "paper_title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Maciej et al. 2024",
            "name_full": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "brief_description": "A cited study demonstrating extraction of materials-science data from research papers using conversational language models plus prompt-engineering techniques to improve accuracy of extracted structured data.",
            "citation_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "mention_or_use": "mention",
            "system_name": "Conversational LLM + prompt-engineering extractor (materials domain)",
            "system_description": "Described in citation only: uses conversational LLMs and engineered prompts to extract materials data from papers; domain-focused extraction pipeline leveraging dialogue-style interactions for improved accuracy.",
            "llm_model_used": "conversational language models (unspecified in this paper)",
            "extraction_technique": "Conversational LLM interactions with engineered prompts to extract domain-specific data",
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Materials science literature",
            "output_type": "Structured materials data (e.g., measured properties) extracted from papers",
            "evaluation_metrics": "Accuracy of extracted materials data (implied by title)",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as evidence that conversational LLMs with careful prompt engineering can achieve accurate extraction of domain-specific scientific data.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4376.4",
            "source_info": {
                "paper_title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Song et al. 2025",
            "name_full": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
            "brief_description": "A recently cited system that uses multiple fine-tuned open-source LLM actors to extract key insights from scientific articles, presumably via an ensemble or multi-actor workflow for robust key-insight extraction.",
            "citation_title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
            "mention_or_use": "mention",
            "system_name": "Multi-actor fine-tuned open-source LLM key-insight extractor",
            "system_description": "Cited system (details not provided in this paper) that appears to use an ensemble or 'multi-actor' architecture composed of multiple fine-tuned open-source LLMs to extract salient insights from scientific articles.",
            "llm_model_used": "fine-tuned open-source large language models (unspecified)",
            "extraction_technique": "Multi-actor ensemble of fine-tuned LLMs for key-insight extraction",
            "synthesis_technique": "Ensembling/multi-actor aggregation of outputs to produce robust key insights (implied by title)",
            "number_of_papers": null,
            "domain_or_topic": "Scientific article key-insight extraction (general scientific literature)",
            "output_type": "Key-insight extractions from scientific articles",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as a contemporary example of open-source fine-tuned LLM systems designed to extract structured insights from scientific articles.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4376.5",
            "source_info": {
                "paper_title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_complex_scientific_text_with_finetuned_large_language_models"
        },
        {
            "paper_title": "Large language models for generative information extraction: A survey",
            "rating": 2,
            "sanitized_title": "large_language_models_for_generative_information_extraction_a_survey"
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
            "rating": 2,
            "sanitized_title": "a_scientificarticle_keyinsight_extraction_system_based_on_multiactor_of_finetuned_opensource_large_language_models"
        },
        {
            "paper_title": "AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts",
            "rating": 2,
            "sanitized_title": "ai_chains_transparent_and_controllable_humanai_interaction_by_chaining_large_language_model_prompts"
        }
    ],
    "cost": 0.016853,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>29 Sep 2025</p>
<p>Chayapatr Archiwaranguprok 
Awu Chen awuchen@media.mit.edu 
Sheer Karny skarny@media.mit.edu 
Hiroshi Ishii ishii@media.mit.edu 
Pattie Maes pattie@media.mit.edu 
Pat Pataranutaporn </p>
<p>Meta-Science Platform for Large-Scale Research Literature Sensemaking CHAYAPATR ARCHIWARANGUPROK
AWU CHEN
SHEER KARNY
HIROSHI ISHII</p>
<p>PAT PATARANUTAPORN
PATTIE MAES
MIT Media Lab
USA</p>
<p>MIT Media Lab
CambridgeMassachusettsUSA</p>
<p>Human-AI Interaction, Research Findings
Research Network Analysis</p>
<p>29 Sep 20256148CB99929F27B7482B0F146BD3E725arXiv:2509.25499v1[cs.HC]CCS Concepts:Human-centered computing  Human computer interaction (HCI) Computing methodologies  Artificial intelligence Information systems  Information retrieval
Fig. 1.The Atlas of Human-AI Interaction methodology and interface design: (Top) The complete pipeline from research paper collection through findings extraction to knowledge graph construction, showing how empirical findings are transformed into structured triplets and clustered graph data; (Bottom) Three complementary visualization modes of the resulting Atlas: 3D Graph View for exploring research landscapes and discovering connections, Cause-Effect View using Sankey diagrams for tracing causal relationships, and Paper View for detailed investigation of individual studies and their extracted findings.Human-AI interaction researchers face an overwhelming challenge: synthesizing insights from thousands of empirical studies to understand how AI impacts people and inform effective design.Existing approach for literature reviews cluster papers by similarities, keywords or citations, missing the crucial cause-and-effect relationships that reveal how design decisions impact user outcomes.We introduce the Atlas of Human-AI Interaction, an interactive web interface that provides the first systematic mapping of empirical findings across 1,000+ HCI papers using LLM-powered knowledge extraction.Our approach identifies causal relationships, and visualizes them through an AI-enabled interactive web interface as a navigable knowledge graph.We extracted 2,037 empirical findings, revealing research topic clusters, common themes, and disconnected areas.Expert evaluation with 20 researchers revealed the system's effectiveness for discovering research gaps.</p>
<p>Introduction</p>
<p>"You can't use an old map to explore a new world." -Albert Einstein, theoretical physicist "The complexity of challenges we face may soon outpace our ability to solve them ... So a core challenge for organizations tackling important challenges: Getting smarter at getting smarter... " -Douglas C. Engelbart A foundational vision of human-computer interaction (HCI) research has long been to empower people to transform complex information into actionable insights.As Douglas C. Engelbart articulated in his seminal work, "by augmenting human intellect we mean increasing the capability of a human to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems" [16].With the rapid advancement of generative AI and other increasingly sophisticated computational tools, the stakes of this question have never been higher.As HCI researchers, we should not merely embrace the excitement surrounding these technologies, but rather critically examine whether we are making meaningful progress toward Engelbart's foundational vision, or whether the proliferation of AI is paradoxically diminishing our capacity for comprehension by drowning us in ever-expanding seas of information.</p>
<p>Contemporary human-AI interaction research encompasses extraordinary diverse areas, spanning foundational theoretical questions [36,37,41,51,62,63] to domain-specific implementation challenges to revisiting foundational debates in HCI-such as the influential 1990s exchange on direct manipulation versus interface agents [31,53], which has gained renewed relevance in light of generative AI.Emerging capabilities are reshaping established interaction paradigms through LLM-powered chatbots [43,69], autonomous agents [3,47], and virtual characters [25,40,43,46].These systems not only reconfigure the boundaries of human-computer interaction but also foster intimate relationships that introduce novel social dynamics.Such dynamics carry both risks, including emotional dependence, and potential benefits, such as supporting mental health and alleviating loneliness [18,29,44].</p>
<p>The field simultaneously explores individual empowerment through AI-augmented reasoning and decision making empowering user to make their own choices with agency [13,14,20,24], to unexpected psychological phenomena such as placebo effects in human-AI interaction [1,41,60].Research trajectories span the entire human experience from individual to societal scale [23,37], from childhood learning to afterlife planning [27,33,42], while grappling with critical ethical dimensions ranging from responsible AI development [5,55] while mitigating AI manipulation [38,39].</p>
<p>This expansive research landscape has generated numerous guidelines for human-AI interaction [1,28,61], yet what practitioners need to know continues to evolve rapidly [50].</p>
<p>The pace and complexity of human-AI interaction research present formidable challenges for those entering the field.New work is published at a rate that makes it difficult even for established scholars and mentors to remain fully up to date.While digital tools provide the advantage of comprehensive visibility, yet most research databases remain constrained by ranking mechanisms-such as citation counts, publication date, or narrowly defined keywords that obscure connections across fields and hinder the identification of emerging trends.As a result, researchers often struggle to see how adjacent work, employing different terminology or situated in parallel domains, might inform their own inquiries.</p>
<p>Although junior researchers are more likely to suffer from the consequences of a fragmented literature understanding, experienced researchers may also struggle to situate their work within the broader landscape.This can lead to repeated efforts and studies that unintentionally duplicate existing findings rather than build upon them.Such redundancy not only slows cumulative progress but also narrows the diversity of ideas circulating in the field.Traditional literature reviews and meta-analyses, while valuable, typically organize knowledge by topical similarity or methodological approaches rather than tracing the intricate web of cause-and-effect relationships that characterize how AI systems actually influence human experience [22,49].</p>
<p>The implications extend well beyond academia.Practitioners designing and deploying AI systems often lack clear, evidence-based guidance about which design choices reliably produce specific human outcomes across different contexts.Without synthesized knowledge, they rely instead on intuition, anecdotal evidence, or short-term business imperatives, leading to uneven quality and missed opportunities for user benefit.</p>
<p>At the societal level, this research-practice gap creates vulnerabilities.AI systems are rapidly scaling into domains that shape everyday life, yet our collective ability to anticipate and guide their psychological, social, and ethical consequences lags dangerously behind deployment.The result is a striking paradox: while there is more research on human-AI interaction than ever before, its capacity to shape real-world design and policy remains partial, fragmented, and inconsistently applied.</p>
<p>This new era of human-AI interaction demands new maps, yet our current approaches to understanding this landscape remain limited.Returning to Engelbart's vision of augmenting human intellect, we propose that AI itself can help us understand the ever-expanding field of human-AI interactions.This paper presents the Atlas of Human-AI Interaction, a novel framework for mapping the complex landscape of empirical findings in human-AI interaction research.</p>
<p>While existing literature reviews typically organize papers by themes or topics, our approach leverages large language models (LLMs) as instruments for systematic knowledge synthesis at a scale previously impossible with manual methods.By analyzing over 1,000 papers from major HCI venues and extracting empirical findings as structured triplets in the form [cause, relationship, effect], we construct a knowledge graph that reveals the distributed evidence about AI's influence on human experience.We visualize this knowledge through three complementary view modes: 3D Graph View that reveal related areas of human-AI findings for comprehensive exploration; Cause-Effect View that present findings as cause-and-effect relationships, illuminating pathways to desired outcomes; and Paper View that present individual papers with extracted findings in standardized, digestible formats.</p>
<p>This methodology enables us to move beyond the limitations of traditional literature reviews, which are necessarily selective and often biased toward recent or highly, cited work, toward a more comprehensive and empirically-grounded understanding of the field's collective findings.The resulting Atlas serves multiple critical functions: revealing structural patterns in the research landscape for researchers, providing evidence-based foundations for design decisions for practitioners, and establishing a framework for continuous knowledge integration that can evolve as new findings emerge.The contributions of this work are fourfold:</p>
<p>( By applying systematic meta-analytic techniques to extract and connect empirical findings rather than merely cataloging research topics, this study empower researchers and practitioners with a navigational aid for future research in human-AI interaction.</p>
<p>As Einstein observed, exploring new worlds requires new maps.This work aims to contribute to that effort by offering an initial framework grounded in empirical evidence and designed to adapt as our understanding of this rapidly evolving frontier grows.While this represents only a first iteration of our approach, and we do not claim to resolve all challenges or fully address every problem, we hope it serves as a starting point to encourage the community to engage with the broader challenge of synthesizing HCI knowledge in the age of AI.</p>
<p>2 Background and Related Works</p>
<p>Research Synthesis Methods in HCI</p>
<p>Traditional systematic reviews and meta-analyses have served as primary tools for consolidating HCI research knowledge [56], offering structured approaches to summarize existing evidence.In parallel, scientometric and bibliometric approaches have emerged to map broader research landscapes through citation network analysis [4,12] and keyword examination [30], processing substantial volumes of academic literature to identify macro-level patterns and trends.Despite these advances, current synthesis methods predominantly cluster papers by topical similarity [21,32,57,67], methodological approaches [19,52,56], or citation relationships [7,59] rather than analyzing the complex interrelationships between empirical findings themselves.This fundamental limitation creates a significant gap in our ability to understand how research outcomes inform, contradict, or complement each other across the field-particularly critical in a rapidly evolving field where empirical results may vary widely depending on context, implementation details, and methodological choices.</p>
<p>Knowledge Graph, and Scientific Paper Extraction, and LLM</p>
<p>The use of Large Language Models (LLMs) for scientific literature analysis represents a more recent but quickly growing body of related work [34,58].This emerging approach builds upon traditional knowledge graph methods while addressing several of their limitations.Researchers have developed sophisticated methods for extracting structured information from academic papers [15,54], exploring applications of LLMs in scientific text processing that go beyond simple metadata extraction.Recent advances in this area include work on specialized prompt engineering for scientific tasks [45], approaches to ensuring the reliability of LLM-based analysis [11], and methods for converting unstructured text into structured knowledge representations [68].These techniques show particular promise for scaling literature analysis beyond what was previously possible with manual or rule-based approaches [66].While these efforts have shown encouraging results in tasks like paper summarization and information extraction, they have primarily focused on individual paper analysis [15] rather than creating comprehensive maps of research findings and their relationships across large bodies of literature.</p>
<p>Our work bridges this gap by applying LLM-based extraction techniques specifically to the challenge of synthesizing research findings across multiple papers.Rather than merely creating document-level connections, we focus on extracting and relating the actual empirical findings themselves, creating a more nuanced and actionable map of the human-AI interaction research landscape.</p>
<p>Design and Implementation</p>
<p>The research landscape of human-AI interaction contains complex, interconnected information where findings from different papers overlap and relate to one another in non-obvious ways.The Atlas is designed to untangle these relationships by visualizing how research entities and findings connect across the field of human-AI interaction.This design philosophy translates into an AI-enabled knowledge graph implementation that systematically maps empirical relationships between research concepts.The methodology follows a five-stage process for extracting, processing, and visualizing research findings from academic literature:</p>
<p>(1) Research Abstracts Collection: Paper abstracts within the field of HAI are collected from four academic databases including ACM Digital Library, IEEE Xplore, Springer Nature Link, and ArXiv (2) Empirical Findings Extraction and Triplet Formation: Empirical findings are extracted from paper abstracts using LLM prompting, then transformed into structured relationship triplets (3) Finding Triplets Processing: To improve interoperability of the entities, we embed each key (cause/effect in the triplets) and apply DBSCAN clustering to merge synonymous keys (4) Semantic Entity Clustering: Keys are clustered by type (human, AI, concept) using k-means clustering to create thematic groupings (5) Structured Graph Construction: Data is transformed into a navigable graph structure with nodes representing research entities and edges representing empirical relationships Each stage was designed to progressively refine and structure the research findings while maintaining their semantic relationships.The overall pipeline for the graph generation process is illustrated in Figure 2.</p>
<p>Research Abstracts Collection</p>
<p>To focus specifically on human-AI interaction research, we collected abstracts using the targeted query "human-ai interaction" from four academic databases: (1) ACM Digital Library, (2) IEEE Xplore, (3) Springer Nature Link, and (4) ArXiv.This query was chosen to maintain focus on direct human-AI interaction findings rather than broader human-computer interaction or artificial intelligence topics.While we used a single targeted query for this initial implementation, the methodology is designed to accommodate expanded keyword sets in future iterations, allowing for broader coverage of related research domains.</p>
<p>We applied several filtering criteria to ensure relevance and quality.First, we restricted our collection to specific publication types: ACM: Extended Abstracts, Research Articles, Works in Progress, Posters, and Short Papers; IEEE: Journals and Conferences; Springer: Research Articles.Additionally, we only included papers with abstracts available in Semantic Scholar to ensure consistent metadata access.Data collection was performed in September 2025, resulting in 1,888 papers meeting our criteria.</p>
<p>Empirical Findings Extraction and Triplet Formation</p>
<p>The extraction process leverages an LLM Claude Opus 4.1 (claude-opus-4-1-20250805) to process the abstract through a two-stage pipeline.First, we extract the findings of each paper from its abstract.Each paper may have several findings.If no empirical findings are extracted, we instruct the LLM model to explain the reason.Then, for each extracted finding, we instruct the model to transform the natural language findings into a triplet.</p>
<p>Empirical Findings Identification.</p>
<p>The prompt instructs the model to filter abstracts for concrete findings with the involvement of the concerned parties, that is, the interaction between human and AI system and their results.Each finding extracted must be in the form of a clear subject-predicate-object structure.For instance, "Interactive Machine Learning interfaces enhance artists' creativity" would be a key finding.</p>
<p>As a result, 59.53% (1,124 papers) contained extractable empirical findings, while 40.47% (764 papers) did not present direct findings due to their nature.The papers without explicit findings primarily consisted of conceptual frameworks (303 papers), systematic review (114 papers), workshop announcement (88 papers), system and methodology improvements (80 papers), and other types, such as design methodology papers (76 papers), technical specification (24 papers), research proposal (4 papers).With one paper may have multiple findings, the extraction results in 2,037 findings.</p>
<p>Triplets Extraction.</p>
<p>To construct more structural information, Claude Opus 4.1 is then utilized to transform each of the 2,037 findings into a structured triplet in the form [cause, relationship, effect].</p>
<ol>
<li>Subject (Cause/Effect) Classification: Each causal element is structured hierarchically to capture both the actor and the specific characteristics involved in the relationship:</li>
</ol>
<p>(1) Type: For the Human-AI Atlas, we imposed broad categorization with three types: human for individual actors, ai for artificial intelligence systems or components, or co for abstract concepts that do not fit the two categories.</p>
<p>(2) Subtype: The main subject being focused on, serving as the primary classification mechanism for systematic analysis across studies.This taxonomical category enables consistent grouping of similar entities: human subtypes reflect roles or expertise levels (e.g., student, clinician), AI subtypes indicate technological approaches or specific system types (e.g., llm, generative, chatgpt), and concept subtypes organize abstract ideas by functional domain (e.g., collaboration, interaction, trust).To ensure the simplicity, when additional specificity is needed, parentheses denote subtype refinements (e.g., student(medical)).</p>
<p>The subtype level provides the key organizational structure for comparing findings across different studies and contexts.</p>
<p>(3) Feature: The specific attribute or property being affected, representing expressed characteristics rather than defining categories.</p>
<p>The complete coding scheme in the system and this paper follows the format type:subtype(specificity)&gt; feature(specificity), where parentheses denote optional refinements and the "&gt;" symbol separates the subject from its expressed characteristic.For example, medical students' trust perceptions of AI systems are represented as human:student(medical)&gt;trust(ai), where student(medical) serves as the refined subtype classification and trust(ai) represents the specific perceptual characteristic being measured.</p>
<p>To improve extraction consistency, we employ additional standardization rules as following:</p>
<p> Feature Normalization The complexity of distinguishing between actual features and subjective perceptions often leads to inconsistent extractions.To improve extraction consistency, we use "#" prefix to standardize features representing perceptions rather than objective attributes (e.g., human:perception_of_trust  human:#trust).This standardized notation addresses this common issue and significantly improves processing consistency. Multi-word Features Use underscores instead of spaces, dashes, or other separators in multi-word features to ensure consistent formatting (e.g., user_experience instead of user experience or user-experience). Descriptive Terminology Avoid redundant or non-descriptive terms that don't aid in clustering, such as ai:system (since AI already implies a system) or human:user/participant, which adds no meaningful distinction.</p>
<p>Relationship Classification:</p>
<p>Each relationship between concepts uses directional verbs to capture the nature of influence:</p>
<p>(1) INCREASES relationships indicate direct positive impact on measurable attributes (e.g., AI explanations increase user trust).(2) DECREASES relationships represent direct negative impact on measurable attributes (e.g., automation decreases human skill development).(3) INFLUENCES relationships capture complex or indirect effects on behavior and perception where the direction may be context-dependent (e.g., AI recommendations influence decisionmaking processes).</p>
<p>Each relationship is further classified by net outcome: [positive] for beneficial human impacts, [negative] for detrimental effects, [neutral] for balanced or negligible outcomes, and [undetermined] for unclear or mixed results requiring further investigation.</p>
<p>Finding Triplets Processing</p>
<p>3.3.1 Keys Embedding.For each triplet being generated through separate prompts, there are several differences in the wording.We employ several methods to merge and cluster the subjects to improve interoperability.We first construct a set of unique keys from the triplets.Each key represents a subject-feature pair that appears in either a cause or effect position.For instance, from the triplet [human:expert&gt;knowledge, INFLUENCES, ai&gt;performance], we extract human:expert&gt;knowledge and ai&gt;performance as distinct keys.This approach accounts for the bidirectional nature of relationships where elements can serve as causes and effects across different findings.</p>
<p>We then generate semantic embeddings for each key using Qwen3-Embedding-8B, which produces 4096-dimensional vector representations that capture the semantic meaning and contextual relationships of each term.This embedding process enables quantitative comparison between semantically similar but lexically different keys (e.g., "ai:elder" and "ai:elderly"), providing the numerical foundation for synonym merge and clustering algorithms that are deployed in the latter steps.</p>
<p>Synonyms Merge.</p>
<p>The synonym merging phase addresses semantic redundancy in the extracted keys through density-based clustering.We employ DBSCAN [17] on the embeddings, configured with a high epsilon value to capture only a cluster with very high similarity.Using cosine similarity as the distance metric enables the identification of semantically equivalent terms while accounting for variations in terminology.For each identified cluster, we select a canonical representative by first computing the cluster's centroid through mean embedding aggregation.The representative term is then selected as the one whose embedding has the highest cosine similarity to this centroid.In the actual process, we use epsilion=0.2,resulting in the detection of 62 clusters.Three samples of the merged clusters are given below.</p>
<p> ai:gpt_4 (members: ai:gpt4, ai:gpt_4o, ai:gpt_4)  ai:interpretability (members: ai&gt;interpretability, ai:interpretability, ai:interpretable)  human:non_expert (members: human:non_professional, human:non_expert)</p>
<p>Semantic Entity Clustering</p>
<p>After synonym merging, we perform semantic clustering to organize keys into thematically coherent groups based on their embedding representations.Keys are first segregated by their subject type (human, ai, or co) to ensure that subsequent clustering respects the fundamental categorical differences in these domains.For each type, we apply k-means clustering with parameters optimized through silhouette analysis to determine the optimal number of clusters.The process results in 7 clusters for human-related terms, 8 for AI-related terms, and 8 for concept/object terms.</p>
<p>We then use Claude Opus 4.1 to generate a name and a description for each cluster.First, we identify the 20 most representative terms from each cluster based on their proximity to the cluster centroid.These terms serve as exemplars that capture the cluster's semantic range.Then, the representative terms, along with their subject type context, are provided to the LLM.</p>
<p>Structured Graph Construction</p>
<p>After keyword processing, we merged the processed keywords back into the original triplets and converted them into a graph-based representation.In our graph structure, each node represents a unique research entity (a subject-feature combination), while edges represent the empirical relationships between them.For example, the triplet [ai:chatbot&gt;explanation, INCREASES, human:student&gt;trust] creates two nodes (ai:chatbot&gt;explanation and human:student&gt;trust) connected by an edge labeled INCREASES.</p>
<p>To manage graph complexity, we applied a connectivity threshold: entities that appeared in fewer than a specified number of relationships remained combined with their features, while highly connected features (such as trust and explainability) were separated into standalone nodes to reduce visual clutter and improve interpretability.Each edge contains metadata including the original finding statement and source paper information.The final graph contains 1,310 nodes and 3,908 edges, exported to JSON format for web visualization using the NetworkX Python library.</p>
<p>Interactive Web Visualizer Development</p>
<p>The Atlas was designed with three distinct, complimentary views to support a researcher in different stages of research.The 3D Graph View is the primary interface for exploring the research landscape.It enables researchers to comprehend academic field by visualizing major research themes and their relationships.By spatially representing these relationships, researchers can understand the overall research landscape and uncover underexplored areas.</p>
<p>While the 3D Graph View excels at comprehending the research landscape and uncovering research gap, it can be visually dense when tracing a specific cause-and-effect relationship.The Cause-Effect View was built for this purpose.Structured as a sankey diagram, it isolates and highlights the direct relationship between research findings.</p>
<p>Both the 3D Graph View and the Cause-Effect View are designed for high-level analysis, the Paper View fills the gap between high-level visualization and detailed investigation.Researchers are able to find and compare research findings through the Paper View which resembles a traditional research interface like Google Scholar.From the 3D Graph View or Cause-Effect View, users are able to transition seamlessly into the Paper View whenever they want to explore a paper in detail.This design ensured that a researcher can always have access to the paper's findings.</p>
<p>The interactive web-based visualizer was implemented using Svelte.jsas the core framework.The 3D graph visualization utilizes Three.js for WebGL rendering while D3.js and d3-force-3d were used for force graph calculation and generation, while the cause-effect view employs D3.js and d3-sankey to generate Sankey diagrams.The system handles real-time filtering and search across the graph data, with synchronized state management enabling seamless transitions between the three visualization modes.Due to the complexity of the graph, several performance optimization were implemented, including the rendering process that switch from line to tube objects in Three.jsonly after the force graph stabilizes to lower computational load during layout calculations, and the use of graph caching to reduce re-rendering operations.The interface of the platform is illustrated in Figure 3.</p>
<p>Result and Analysis</p>
<p>Our analysis of the human-AI interaction knowledge graph reveals several patterns in how researchers have explored relationships between AI systems and human experience.The analysis presented here represents a subset of potential approaches enabled by the Atlas framework; the structured knowledge graph could support additional analytical methods including citation network integration, and domain-specific clustering techniques.This section presents our findings organized into two main areas: (1) topic clusters and common themes across the research landscape, (2) research integration opportunities identified through structural network analysis.</p>
<p>Topic Clusters and Common Themes</p>
<p>K-means clustering analysis identified 24 distinct clusters from the three types (human, ai, concept/object), revealing the diversity of stakeholders engaged in human-AI interaction research.Full descriptions of all 24 clusters with complete member lists are available in the supplemental section A. In addition, the analysis of the most frequently occurring relationships in the knowledge graph reveals the dominant patterns in current human-AI interaction research.These represent the connections that appear most often across the 2,037 extracted findings, indicating where research attention has been concentrated.</p>
<p>(1) AI Systems and Educational Contexts: Our analysis identified 91 empirical relationships involving students as the primary effect, demonstrating substantial research activity examining how various AI technologies impact educational settings.Representative findings include:</p>
<p> Does human-AI trust affect human-AI interaction in the metaverse?Insight from a pilot study [64] found that students with 2d ai teachers engaged in more multi-dimensional interactions compared to those with 3d ai teachers, as revealed by dialogue data text mining.(ai:teacher(2d To systematically identify these integration opportunities, we employed network analysis techniques to map the community structure of human-AI interaction research.Using the Louvain algorithm [9] for community detection followed by structural hole analysis [10], we identified research concepts that serve as bridges between otherwise disconnected communities.A structural hole exists when a research concept (like "Large Language Models") appears in multiple research communities that don't communicate with each other-for instance, LLMs are studied separately in healthcare, education, and accessibility research, but these communities rarely share insights or methods despite facing similar human-AI interaction challenges.</p>
<p>Structural holes indicate opportunities for synthesis across parallel but isolated research streams.Our analysis identified 126 distinct research communities with a modularity score of 0.669, indicating strong community structure within the research network.We assessed each concept using Burt's constraint measure, which quantifies how redundant a node's connections are; lower constraint values indicate greater potential for cross-community knowledge brokerage.The constraint measure (M=0.356,SD=0.197) reveals that while most research concepts operate within their established communities, a small number span multiple disconnected areas, representing prime opportunities for theoretical and methodological integration.</p>
<p>The analysis reveals five concepts positioned at the largest structural holes, representing the greatest opportunities for cross-community research synthesis:</p>
<p>(  4) Conversational Interfaces (ai:chatbot): Technical advances in natural language processing proceed independently from human factors research on conversation design.This separation between technical capability and human-centered interaction principles represents a significant integration opportunity for improving conversational AI effectiveness.(5) Autonomous Agents (ai:agent): Human-agent interaction challenges (delegation, monitoring, and intervention) appear consistently across domains from robotic control to digital workflow management.However, systematic investigation of these shared interaction patterns across application contexts remains limited.</p>
<p>Complementing these structural holes, we identified active bridges: the concepts that appear across multiple research communities but where each community has developed isolated approaches without systematic cross-referencing.Active bridges are identified by measuring each concept's external connectivity: the number of distinct research communities (beyond its home community) that contain empirical findings related to that concept.Generative AI serves as the strongest bridge, appearing in 18 different communities, yet each community has developed domain-specific interaction paradigms without leveraging insights from parallel research.Educational applications (human:student) and explainability research (ai:explanation) each span 15 communities, positioning them as natural candidates for developing unified theoretical frameworks.</p>
<p>The distinction between structural holes and active bridges suggests different integration strategies: structural holes require new theoretical frameworks to connect currently isolated communities, while active bridges need systematic synthesis of existing but fragmented knowledge across their established presence.Both represent pathways for accelerating field-wide progress through crosscommunity collaboration.These findings point to five concrete research directions with high potential for field-wide impact:</p>
<p>These findings point to five concrete research directions with high potential for field-wide impact:</p>
<p>(1) Universal LLM Interaction Patterns: Identify common human-LLM interaction challenges across healthcare, education, and accessibility to develop generalizable design principles while preserving domain-specific adaptations.</p>
<p>(2) Generative AI Collaboration Models: Synthesize interaction paradigms from creative tools, decision support systems, and content generation to establish comprehensive frameworks for human-generative AI partnership.(3) Transferable Trust Mechanisms: Extract trust-building strategies that work across AI applications, creating adaptive models that can be customized for specific domains while leveraging universal psychological principles.(4) Unified Agent Interaction Theory: Bridge research on robotic assistants, software agents, and autonomous systems to develop comprehensive models of human-agent collaboration, delegation, and control.(5) Cross-Domain Testing Frameworks: Establish educational contexts as proving grounds for HAI theories, leveraging their connections across communities to validate approaches that could transfer to other domains.</p>
<p>User Study</p>
<p>We conducted a user study with expert researchers to evaluate the Atlas research exploration tool in different research scenarios.In our four-phase study, experts in the field of human-AI interaction first filled out a pre-study questionnaire regarding their prior research experience.They were then introduced to the tool, and asked to complete a series of tasks by interacting with the Atlas's three primary visualization modes-the 3D Graph View, Cause-Effect View, and Paper View-and then evaluated the Atlas along subjective metrics in the post-study survey.</p>
<p>Participants</p>
<p>We recruited twenty experts in the field of human-AI interaction (HAI), using a convenience sample, to participate in a 30-minute study.The ages of the participants ranged from 18 to 34 years, with 12 participants identifying as male and 8 identifying as female.The participants were predominantly experienced researchers, with 65% of participants having an intermediate to expert research level within the field of HAI or in an adjacent field.The present study follows the principles of the Declaration of Helsinki.The protocol was reviewed and granted an exemption by the Institutional Review Board.</p>
<p>Design</p>
<p>We used a within-subjects comparative analysis focusing on qualitative feedback backed up by quantitative metrics to assess the usability of our research tool, the Atlas.Our goal is to evaluate the tool's impact and gather qualitative insights rather than prove a quantitative hypothesis with statistical significance.Our experiment followed a multi-phase protocol to gather rich qualitative data on the participant's experience.The study did not involve any manipulation of the order of tools or tasks, as all participants experienced the same flow, serving as their own control group for a within-subjects comparative analysis.</p>
<p>Survey Section</p>
<p>We developed a post-study survey to understand the user experience associated with the Atlas, capturing the participant's subjective preferences for traditional research tools as compared to our tool.To accomplish this, we developed a seven Likert-rated questionnaire items and qualitative measures, which can be viewed in 6.</p>
<p>Procedure</p>
<p>Participants were notified about the study on an email list-serve for the academic department in which we reside.Participants scheduled an in-person or online evaluation of Atlas using a Google Calendar sign-up link attached to the email.Of the 20 participants, 12 completed the study in-person at our lab and 8 joined online via Zoom.For both in-person and online sessions, participants' screens and voices were recorded with their consent, which was obtained in the pre-study questionnaire.</p>
<p>Each session began with a short questionnaire to understand the participant's existing research interests and methods, which established a baseline for comparison.Participants were then introduced to the Atlas and its various representations orally using a pre-rehearsed script and a live demo.Following the introduction, participants were asked to perform a series of structured tasks on the Atlas, starting with identifying a major research theme, research gap, tracing cause-andeffect relations, explored paper findings, and compared the findings across multiple papers.After completing the tasks, we directed participants to a post-study survey containing both open-ended questions and Likert-rated questions to reflect on their overall experience with the Atlas.</p>
<p>Data Analysis</p>
<p>We employed a mixed-methods approach to analyze the data from the pre-study questionnaires, post-task surveys, and session recordings.</p>
<p>For the quantitative data, we summarized the participants' Likert scale ratings using descriptive statistics.For each question, we reported key metrics, including the mean, median, mode, and standard deviation, to provide a clear measure of central tendency and data spread.We also calculated the percentage distribution across the 7-point Likert scale to understand the spread of agreement and disagreement.</p>
<p>For the qualitative data, we conducted a thematic analysis to manually identify recurring themes and insights from both the open-ended survey responses and the transcribed audio recordings of the sessios.This allowed us to systematically identify recurring themes, insights, and direct quotes from participants interactions.Furthermore, we leveraged screen recordings of the participants to understand how they navigated the interface.</p>
<p>By using this approach, we were able to support our quantatative findings with qualitative and behavioral data, providing a more comprehensive analysis.</p>
<p>Results</p>
<p>Our study with 20 expert users revealed that Atlas addressed key limitations in traditional literature review methods through three primary capabilities: comprehensive landscape visualization, relationship discovery, and systematic gap identification.Participants reported that Atlas externalized the mental mapping process typically required in literature reviews, enabling them to discover crossdisciplinary connections that would not emerge through conventional keyword-based searches.The 3D graph interface allowed users to identify research gaps through visual analysis of connection density, while the cause-effect view enabled rapid validation of hypothesized relationships between concepts without manually synthesizing multiple papers.</p>
<p>Participant feedback also clarified Atlas's scope and positioning within research workflows.Users noted that the tool excelled at high-level exploration and pattern recognition but required deeper information for detailed investigation.The system's effectiveness varied by research domain, performing well for conceptual areas while showing limitations in highly technical contexts.These findings highlight Atlas position as an exploratory overview tool that serves specific functions in the research process, particularly for initial field mapping and connection discovery, particularly for initial field mapping and connection discovery, to complement current literature review workflows rather than replace them.Fig. 6.In this figure we present pariticapnts' respective ratings of our 7 Likert-rated items from the survey section.Participants rated their agreement with these items from 1 (Not at all) to 7 (Extremely).</p>
<p>3D Graph Visualization Effectiveness</p>
<p>The 3D Graph View received a high rating on the Likert scale for helping participants identify underexplored areas or research gaps (M=4.95,SD=1.24).The participants' open-ended responses reinforced this rating, suggesting that this spatial representation allowed them to discover relationships and thematic structures that would be difficult to identify through traditional sequential search methods.One participant noted, "Operational Roles (Cluster) -I found this by rotating the 3D graph and clicking nodes which seemed to have the fewest connections.I could tell this more easily due to the 3D nature of the graph -it was very easy to see which areas had the fewest lines connected to them."</p>
<p>Another participant was able to uncover the research gap "by narrowing down to the node that has the least links." These quotations reveal that participants intuitively understood that research gaps could be identified by examining connection patterns rather than just content similarity.The ability to "rotate the 3D graph" represents a novel approach to identify research gap compared to traditional methods that rely primarily on keyword frequency or citation counts.The threedimensional nature was crucial here, as it allowed participants to perceive connection patterns that might be obscured in two-dimensional representations.</p>
<p>The interactive nature of the 3D Graph View provided researchers with an engaging, spatial approach to research exploration.As one participant noted:</p>
<p>"I tend to prefer visual and spatial mediums when interacting with information and learning.This felt like a very intuitive way to scout out information I am looking to utilize and connect.To me, it felt as easy as looking to find something in a room." This sentiment was reflected across our participants, where camera manipulation and node size comparison provided researchers with visual cues for understanding the relative importance and relationships between different research findings.The ability to spatially navigate the data allowed participants to assess both the volume of research in different areas and the patterns of connection between them.</p>
<p>The clustering capabilities of the 3D Graph View facilitated serendipitous discovery of related work, enabling researchers to identify thematic groupings that transcended traditional categorical boundaries.One participant noted, "I started by searching for 'music'...I went to that area, clicked on one, then zoomed out to see the general theme they were attached to." This demonstrates how the 3D Graph View supported both targeted searching and exploratory browsing within a single interface, allowing researchers to move fluidly between specific papers and thematic relationships.</p>
<p>Visualizing the Cause and Effect</p>
<p>Participants' self-reports indicated that they were able to successfully utilize the Cause-Effect view for identifying research relationships between findings (M=5.45,SD=1.12).This is likely due to simplified nature of this view, indicating opportunities for literature visualization and review tools that include multiple, distinct, and complementary interfaces.</p>
<p>"For example, if I want to see how AI creativity could be used in educational tool and see the related papers, going from flow view to paper view is an intuitive experience.That would be really helpful.It's much better than just searching around like ACM library." This participant's experience highlights how the flow view facilitated interdisciplinary connection-making by allowing researchers to trace relationships between AI creativity and educational applications.The seamless transition "from flow view to paper view" demonstrates the value of integrating multiple visualization modes within a single platform.The comparison to "searching around like ACM library" underscores how traditional database searching often fails to surface these kinds of cross-domain relationships, which require researchers to manually synthesize information across disparate search results.</p>
<p>The flow view proved particularly valuable for conducting rapid relationship validation, allowing researchers to quickly assess whether their intuitions about connections between research areas were supported by empirical evidence.</p>
<p>"I found 'ai&gt;output -[INCREASES] co:collaboration' and 'ai:onboarding -[INCREASES] human:practitioner'. The relations gave me a quick sanity check between two subjects. "</p>
<p>The "sanity check" functionality described here represents a novel form of literature validation that would be time-consuming using traditional methods.Rather than having to read multiple papers to verify whether a hypothesized relationship was supported by evidence, researchers could quickly scan relationship patterns in the flow view.This capability was particularly valuable for researchers working across disciplinary boundaries who needed to rapidly assess the validity of connections between their primary field and related areas.</p>
<p>However, participants also identified important limitations in the flow view's representation of causal relationships, particularly around the semantic interpretation of relationship directions and valences.</p>
<p>"I was easily able to trace the general relationship between AI and Visual Arts and humans, and this made it easy to scroll through quickly to identify what each paper was saying; but it was sometimes unclear if the findings were beneficial or not, as even papers labeled 'DECREASES' were sometimes net positives for people ('decreases human effort')."</p>
<p>This observation reveals a critical challenge in automated relationship extraction and visualization: the context-dependent nature of causal relationships.What appears as a "decrease" relationship may actually represent a positive outcome depending on the specific context and domain.This participant's experience suggests that while the flow view was effective at surfacing relationships, it required additional contextual information to support accurate interpretation.</p>
<p>Paper View and Detailed Exploration</p>
<p>The paper view functionality generated more mixed responses from participants, with strong appreciation for its search and filtering capabilities balanced against concerns about information depth and comprehensiveness.This is likely due to participants' familiarity with traditional research tools such as Google Scholar, Semantic Scholar, and conference proceedings.Participants noted that while the Paper View offers a nice way to compare the paper findings, it lacked the structured context found in other Atlas visualizations.</p>
<p>Despite this, some participants noted that the strength of the Paper View was its ability to facilitate serendipitous discovery within focused domains:</p>
<p>"I was able to quite easily narrow in on my areas of interest by using keywords ('music') -this was a great view, because it actually led me to consider papers I would never have previously found with my typical methods of search." This participant's discovery of papers about "AI music co-designed with therapists" illustrates how the system surfaced interdisciplinary connections that might not emerge through traditional database searching, which typically requires researchers to already know relevant keywords or author names.The "inspirational" quality of this discovery process suggests that the paper view supported not just information retrieval but also creative research ideation.</p>
<p>The efficiency gains compared to traditional conference proceeding searches were consistently highlighted by participants, one participant noted "Right now I have to go to the proceeding page.Search using keywords, scroll through 50 Papers in each page.I use GPT to summarize all the abstracts."</p>
<p>As this participant noted, there exists substantial overhead in traditional literature searching, including the need to navigate multiple pages, manually review large numbers of abstracts, and even employ additional AI tools for summarization.The participant's mention of using "GPT to summarize all the abstracts" highlights how researchers have been developing ad-hoc solutions to manage information overload in traditional systems.This user's sentiment suggests that an integrated approach such as Atlas could reduces the cognitive load of literature review while potentially increasing the comprehensiveness of the search process.</p>
<p>However, participants consistently expressed concerns about the depth and adequacy of information provided in the paper summaries, indicating a tension between overview-level browsing and detailed investigation.</p>
<p>"It was helpful in being able to quickly view a large spectrum of related topics; but I felt like the couple of sentences weren't enough to get substantial information or context from.I wish I could click in to read more of the paper itself, or, failing that, at least a longer summary of their findings."</p>
<p>These concerns highlight a fundamental challenge in literature review tool design: balancing the need for high-level overview with detailed investigation capabilities.The participants' desire for more detail and better integration with other views suggests that while the paper view successfully supported broad exploration, it fell short of supporting the deep investigation phase that typically follows initial discovery.The suggestion to connect paper summaries to the cluster view indicates that participants were thinking about how to integrate insights across multiple visualization modes for more comprehensive understanding.</p>
<p>Overall System Evaluation</p>
<p>Atlas demonstrated strong performance across multiple dimensions of research capability enhancement and received positive ratings for expanding research capabilities, identifying dominant research themes, and improving understanding of research landscapes through multiple visualization modes.The consistently high ratings across these different aspects suggest that Atlas may successfully address several fundamental challenges in academic research while introducing novel capabilities that extends beyond traditional literature review tools.</p>
<p>The preference ratings for Atlas over traditional research tools showed moderate but positive results, indicating potential for integration into existing research workflows while acknowledging that significant improvements would be needed for widespread adoption.The moderate rating likely reflects both the innovative potential of the system and participants' recognition of areas requiring further development.</p>
<p>Participants consistently emphasized that Atlas provided capabilities that were qualitatively different from existing tools, particularly in terms of revealing broader research contexts and facilitating unexpected discoveries."This tool can help me not just finish the related works part faster than before, but also give me a better vision of a larger research field that I may overlooked.I would really love to use this tool one day."</p>
<p>The distinction between finishing "the related works part faster" and gaining "a better vision of a larger research field" captures two different types of value that Atlas provided.The first represents an efficiency gain over existing methods, while the second represents a qualitatively new capability for research exploration.The participant's expression of strong interest ("I would really love to use this tool one day") suggests that the broader vision capability was particularly compelling, even if the current implementation required further refinement.</p>
<p>The transformation of research workflow from linear information seeking to spatial relationship exploration was a recurring theme in participant responses."My original workflow was more like building a map on papers or in my mind, visualizations and relations could help me quickly identify the connections and dominant domains" This participant's description of their original workflow reveals the cognitive burden typically involved in literature review, where researchers must manually construct mental models of relationships between papers.Atlas's visualization capabilities externalized this mental mapping process, potentially reducing cognitive load while increasing the sophistication and accuracy of the conceptual maps that researchers could construct.</p>
<p>The multi-modal approach to research exploration was consistently praised as a key strength of the system, with participants appreciating how different views provided complementary perspectives on the same research landscape."I really really like the tool -the three different views provide a comprehensive framework for identifying new papers across my areas of interest in different ways." "Great for early stage research exploration.You can freely navigate around all the research opportunities.No time sink on going through different publications."</p>
<p>The emphasis on "early stage research exploration" suggests that Atlas was particularly valuable during the initial phases of research when broad understanding and opportunity identification are crucial.The ability to "freely navigate around all the research opportunities" without the traditional "time sink" of sequential publication review represents a significant potential improvement in research efficiency.However, the specific mention of early-stage exploration also implies that additional tools and capabilities might be needed to support later stages of the research process.</p>
<p>Participants provided nuanced feedback about the system's strengths and limitations, often identifying both domain-specific advantages and areas where traditional methods might remain superior.</p>
<p>"Works well for social science literatures, but does not work well for mathematical, or formula based research.Worry that people might miss out on important information."</p>
<p>This domain-specific assessment highlights important boundaries around Atlas's applicability.The effectiveness for "social science literatures" compared to "mathematical, or formula based research" suggests that the system's relationship extraction and visualization methods may be better suited to research areas where connections are primarily conceptual rather than formal or quantitative.The concern about missing "important information" reflects the ongoing tension between comprehensive coverage and selective presentation that affects all literature review tools."However, I would be hesitant to use it unless I was sure it contained many more papers, as well as a range of papers over the previous decades.It would be helpful to see trends over time, or narrow in by year."</p>
<p>The concerns about paper coverage and temporal analysis capabilities point to important requirements for research tool adoption in academic contexts.The desire for "trends over time" and the ability to "narrow in by year" suggests that temporal analysis represents a crucial missing dimension in the current Atlas implementation.These temporal capabilities would be necessary for comprehensive literature reviews that need to trace the historical development of research areas or identify emerging trends.</p>
<p>The overall evaluation revealed that while Atlas successfully introduced novel capabilities for research exploration and relationship discovery, its integration into established research workflows would require addressing concerns about comprehensiveness, temporal analysis, and domainspecific applicability.Despite these limitations, participants expressed strong interest in the tool's potential, suggesting that further development addressing these concerns could lead to significant improvements in academic research practices.</p>
<p>Discussion</p>
<p>The rapid expansion of human-AI interaction research presents a fundamental challenge: as the field grows in both volume and complexity, our ability to synthesize and comprehend the collective knowledge diminishes.This creates a paradox where the very technologies we study may also provide solutions to the information overload they help generate.Looking back at Engelbart's vision, the Atlas represents an attempt to apply AI techniques to understand AI's own effects on human experience, using technology to augment human intellect in service of tackling complex problems.</p>
<p>Our systematic analysis of over 1,000 human-AI interaction papers through LLM-powered knowledge extraction reveals both the current structure of empirical understanding and significant gaps in our collective knowledge.This implementation of a computational research tool and accompanying meta-study demonstrates how AI-assisted approaches can transform literature synthesis itself, moving beyond traditional categorical organizations toward relationship-based knowledge mapping that captures the complex interdependencies between research findings.This section examines the implications of these findings for researchers and practitioners, while acknowledging the limitations inherent in our approach and identifying directions for future investigation.</p>
<p>Research Implications</p>
<p>For researchers, the Atlas reveals several structural patterns that have important implications for future human-AI interaction research.The dominance of certain research subjects, particularly students, generative AI systems, and trust relationships, indicates both areas of established knowledge and potential blind spots in our collective understanding.The network analysis reveals significant opportunities for cross-community integration.Large language models, generative AI, and trust dynamics emerged as concepts positioned at major structural holes, connecting otherwise disconnected research communities.This suggests that these areas, while heavily studied, lack unified theoretical frameworks that could bridge insights across domains.The clustering analysis identified specific research directions with high potential for field-wide impact:</p>
<p> Developing universal interaction patterns for LLMs across healthcare, education, and accessibility contexts rather than domain-specific approaches  Creating comprehensive frameworks for human-generative AI collaboration that synthesize insights from creative tools, decision support systems, and content generation  Establishing transferable trust mechanisms that work across AI applications while preserving domain-specific adaptations  Building unified theories of human-agent interaction that bridge research on robotic assistants, software agents, and autonomous systems  Leveraging educational contexts as testing grounds for HAI theories that could transfer to other domains</p>
<p>The concentration of research around specific AI types (chatbots, generative systems) and user groups (students, designers) also reveals gaps in our empirical coverage.Underexplored areas include long-term adaptation patterns, reciprocal learning between humans and AI systems, and the effects of AI integration in non-professional contexts.</p>
<p>Practice Implications</p>
<p>For practitioners, our findings suggest that effective human-AI interaction design requires attention to both technical features and contextual factors.The recurring relationship between explanation, trust, and performance indicates that transparency should be considered not merely as a technical feature but as part of a broader approach to establishing appropriate user expectations and relationships.</p>
<p>The observed patterns also suggest that designers should consider the entire cycle of interaction rather than isolated features.Initial expectation-setting appears to influence subsequent trust development, which in turn affects engagement patterns and ultimately system performance.This cyclical relationship highlights the importance of thoughtful onboarding and progressive disclosure in human-AI interaction design.</p>
<p>Future Direction</p>
<p>Several promising directions for future work could enhance its utility and scope.First, we envision continuous data addition and updates to maintain the Atlas's relevance and comprehensiveness.This includes not only incorporating new research findings as they emerge but also developing automated mechanisms for detecting and integrating relevant publications.Such a dynamic approach would help track the evolution of human-AI interaction patterns over time and ensure that the Atlas remains a current reflection of the field's understanding.</p>
<p>The current framework could be extended to accommodate more types of connections between findings.While our present implementation focuses on direct cause-and-effect relationships through INCREASES, DECREASES, and INFLUENCES connections, future work could incorporate more nuanced relationship types such as mediation effects, contextual dependencies, and temporal sequences.This expansion would enable more sophisticated analysis of how different aspects of human-AI interaction relate to and influence each other, potentially revealing more complex patterns and interdependencies.</p>
<p>Refinement of the extraction process presents another crucial area for future development.Our current LLM-based extractors, while effective, could be enhanced through several approaches.These include developing more sophisticated prompting strategies, implementing multi-stage verification processes, and incorporating domain-specific knowledge to improve the accuracy and granularity of extracted findings.Additionally, we plan to explore methods for automatically identifying and resolving contradictory findings across different studies, potentially through probabilistic reasoning or meta-analytical approaches.</p>
<p>Finally, we see significant potential in developing more intensive connectivity analysis methods.This involves not only examining direct relationships between findings but also identifying higherorder patterns and clusters of interaction effects.Advanced network analysis techniques could be applied to uncover hidden communities of related findings, identify key bridge concepts that connect different areas of human-AI interaction, and predict potential emerging patterns based on existing relationships.This enhanced connectivity analysis could provide deeper insights into the complex web of relationships that characterize human-AI interaction.</p>
<p>Conclusion</p>
<p>This paper has presented a meta-study of the human-AI interaction research landscape through the extraction and analysis of empirical findings from over 1,000 research papers.By representing these findings as structured relationship triplets and organizing them into a navigable knowledge graph, we have sought to provide a systematic overview of the current empirical understanding in this rapidly evolving field.</p>
<p>Our analysis reveals both areas of convergence and gaps in the research landscape.Educational contexts, transparency mechanisms, and trust development emerge as heavily studied areas with relatively consistent findings across multiple studies.In contrast, longitudinal effects, cross-domain applications, and reciprocal adaptation remain comparatively underexplored.The concentration of research on certain user groups (students, knowledge workers) and AI types (conversational, generative) suggests opportunities for broadening the empirical foundation.</p>
<p>This type of systematic knowledge mapping offers value for both research and practice.For researchers, the identified gaps and patterns may help inform research priorities and contextualize new findings within the broader empirical landscape.For practitioners, the recurring relationships between design features and human responses provide a starting point for more evidence-informed design decisions.</p>
<p>The stakes of this work extend far beyond academic synthesis.As technology reshape fundamental aspects of human experience within an evergrowing and increasingly complex information landscape, our ability to navigate and synthesize this knowledge will determine whether we realize Engelbart's vision of augmented human intellect or become overwhelmed by the very complexity we seek to understand.The Atlas demonstrates that AI can help us comprehend AI's effects, transforming large-scale corpora of scattered empirical findings into navigable knowledge structures that serve human understanding rather than adding to information overload.In a world where the pace of technological change and research output increasingly outstrips our capacity for manual knowledge synthesis, computational approaches to literature mapping may prove essential for maintaining human agency in shaping our technological future.</p>
<p>A Cluster Details</p>
<p>For each cluster, representative members were selected by identifying the five entities with the smallest Euclidean distance to their cluster's centroid vector.Cluster descriptions were generated by providing Claude Opus 4.1 with the 20 most representative terms from each cluster along with their subject type context.</p>
<p>A.1 Human-Related Clusters</p>
<p>C Data Extraction Prompts</p>
<p>This section provides the complete prompts used in our automated data extraction pipeline.</p>
<p>C.1 Findings Extraction Prompt</p>
<p>Extract key findings from the abstract as bullet points, following these rules: Formatting:</p>
<p>-Present only substantiated findings -Skip bullet points entirely if the abstract is purely theoretical or does not present findings (in this case, identify the paper type as described in the "Output Format" section below) -Limit to only the most important findings (typically 0-3 bullets) Each bullet must: -State a complete, specific conclusion (e.g., "AI chatbots foster creative collaboration by enabling anonymous idea sharing") -Be independently understandable without context -Use concise, precise, simple language -Focus solely on verified results and findings -Include metrics when available, but not required -Include ONLY findings where AI is directly involved in the result -Describes an AI-related element, explicitly name it as such (e.g."Design guidelines" "AI design guidelines", "Explanations" "AI explanations", "The system's outputs..." "The AI system's outputs..."); if encountering a non-well-known or product-specific name (e.g."NeuroSynthVision Pro" or "QuickScribe Assistant"), generalize it to the broader AI concept it represents (e.g."AI visual synthesis system" or "AI writing assistant").-Express each finding in Subject-Predicate-Object (SPO) format where possible Do NOT include: -Research methodology or process descriptions -References to "this paper" or "this study" -Vague comparisons without explanation -Framework descriptions or conceptual models -Hypotheses or future work Sample bullets: "The system showed improved performance over baseline"  "Virtual reality environments enable deeper emotional engagement in therapy sessions"  "AI-assisted writing tools reduce cognitive load by managing document structure" Output Format: -Summaries: Clear, quantified bullet points [Follow rules from the first section] -Note [if no bullets extracted]:</p>
<ol>
<li>type: state the paper type, e.g.</li>
</ol>
<p>-"Workshop announcement" -"Conceptual framework" -"Design methodology" -"Technical specification" -"Systematic review" -"System and methodology improvement" 2. description: summarize the content of the paper within 1 sentence In addition, extract 1-3 keywords from the abstract: -Keywords [1-3]: Main topics and themes of the paper include: -Domain terms (e.g., "Healthcare", "Education") -Target outcomes (e.g., "Team Efficiency", "Learning") -Specific contexts (e.g., "Emergency Response", "K-12") DO NOT include: -Generic HCI/AI-related terms (e.g., "Human-AI Interaction", "Human-Computer Interaction", "Artificial Intelligence")</p>
<p>C.2 Triplets Extraction Prompt</p>
<h1>Convert research statements about human-AI interaction into structured relationships Direct interactions include:</h1>
<p>-AI systems affecting human behavior/performance/perception -Human actions affecting AI system behavior/performance -Human-AI collaborative processes affecting outcomes -AI features influencing human-AI relationship dynamics</p>
<p>Examples of findings to PROCESS:</p>
<p>-"Primary school participants with higher trust in 2D AI teachers engaged in more dimensional interactions" Process (shows trust affecting interaction) -"XAI facilitates trust formation through affective information processing" Process (shows AI feature affecting human response) -"Users developed new interaction methods through Chains" Process (shows AI system enabling human behavior change)</p>
<p>Convert research statements into structured triplets using the format: [cause, relationship, effect, net_outcome] where cause and effect are structured Subject objects.</p>
<h2>Subject Structure Each Subject has three components: -<em>type</em>: One of only "human", "ai", or "co" (concepts/objects) -<em>subtype</em>: Broad taxonomical category (e.g., "student", "generative") -Should be included whenever possible (only omit for very general concepts) -Keep subtypes broad enough to be reusable across multiple instances -Avoid overly specific subtypes that can't serve as taxonomical categories -Use general model types (e.g., "llm", "transformer") not project-specific names -Exception: Widely-known models may use their common names (e.g., "chatgpt", "google") -<em>feature</em>: Specific attribute or property being affected ### Subject Categories Human: Individual actors (e.g., human with subtype "student" or "clinician") AI: AI systems/components (e.g., ai with subtype "generative" or "chatbot") CO: Concepts/Objects (e.g., co with subtype "project", "justice", or "interaction") ### feature Guidelines One word when possible Specific over general terms Use "#" prefix for perception features (e.g., "#trust")</h2>
<p>Fig. 2 .
2
Fig. 2. Graph generation pipeline comprising five stages: (1) research abstract collection, (2) findings triplet extraction, (3) triplet normalization and validation, (4) semantic entity clustering, and (5) structured graph construction with community detection.</p>
<p>Fig. 3 .
3
Fig. 3.The figure shows four views of the 3D graph Atlas (Left to Right): (Top) a complete network visualization (Full Graph View), the detailed display of the "human:designer" node with its direct connections and relationship information (Node View), (Bottom) the detailed view of the "Machine Learning Models and Explainability" cluster (Cluster View), and the detailed view of the relation between human:#trust and co:collaboration (Edge View).</p>
<p>Fig. 4 .
4
Fig. 4. Three visualization modes of the Atlas of Human-AI Interaction: (Left) 3D Graph View showing the complete network of empirical findings with nodes representing research concepts and edges showing cause-effect relationships; (Middle) Cause-Effect View using a Sankey diagram to emphasize directional causal flows from AI systems to documented effects; (Right) Paper View providing a searchable repository with detailed paper information and extracted findings for individual study exploration.</p>
<p>Fig. 5 .
5
Fig. 5. Sample graph data illustrating two nodes: ai&gt;explanation and human&gt; #trust, connected by an edge annotated with the original finding and source paper reference.</p>
<p>patterns, theoretical convergences, and methodological relationships across disparate research domains and contextual boundaries.(2) Methodological Contribution: A novel computational framework leveraging large language models to systematically extract, categorize, and synthesize empirical findings from extensive HCI literature corpora, establishing new methodological precedents for large-scale research literature sensemaking.(3) Structural Analysis of the Field: A comprehensive analysis of the current human-AI interaction research landscape's structure, identifying both well-established knowledge clusters and significant gaps that represent opportunities for future investigation.(4) Expert Validation and Design Principles: Empirical validation through a usability study with expert users/researchers, demonstrating how the Atlas supports research workflows and providing design recommendations for future literature analysis tools.</p>
<p>1) Open-Source Research Infrastructure: A comprehensive open-source system that systematically maps the empirical landscape of AI's documented effects on human experience, unveiling previously unidentified</p>
<p> AI technology clusters (8 clusters, 285 total members) encompass autonomous systems, information tools, visual/XR applications, creative support, embodied interfaces, educational platforms, medical diagnostics, and machine learning models.The largest cluster, AI-Powered Information and Translation Tools, contains 71 AI types (ai:chatbot, ai:assistant, ai:agent, ai:translator, ai:fact_checking), while the smallest, Visual and Extended Reality AI, represents 16 AI types (ai:visual, ai:ar, ai:vr, ai:xr, ai:3d). Concept and Objects clusters (9 clusters, 272 total members) comprise design tools, research collaboration, system integration, methodological strategies, governance, team dynamics, cognitive recognition, training, and risk assessment.The largest cluster, Design Tools and Interface Development, contains 50 concepts (co:interface, co:tool, co:prototype, co:framework, co:customization), while the smallest, Training and</p>
<p> Human clusters (7 clusters, 158 total members) include vulnerable populations, creative professionals, educators, healthcare practitioners, age demographics, organizational roles, and diverse stakeholders.The largest cluster, Diverse Stakeholders and User Roles, contains 78 members (human:user, human:participant, human:student, human:expert, human:decision_maker), while the smallest, Users with Disabilities and Vulnerabilities, contains 6 members (human:blind, human:autistic, human:visually_impaired, human:deaf, human:vulnerable).Skill Development, contains 19 concepts (co:training, co:coaching, co:skill, co:learning, co:assessment).</p>
<p>Explanation and Trust: We identified 14 distinct empirical relationships confirming the critical importance of transparency mechanisms in building user confidence.Decision Support Applications: Our analysis revealed 13 empirical relationships highlighting the importance of interpretability in professional contexts where significant decisions are supported by AI.Representative findings include: Research Integration Opportunities While human-AI interaction research has grown rapidly, much of this growth occurs within isolated research communities that rarely cross-reference each other's work.A researcher studying trust in medical AI systems, for example, may remain unaware of relevant findings about trust mechanisms developed in educational AI contexts.This fragmentation represents a significant missed opportunity: concepts that appear across multiple disconnected research areas often indicate where unified frameworks could accelerate progress across domains.
 Sofia Fled or Died? Design Fictional Explorations of Unintended and Unsus-tainable Consequences of Human-AI Interaction [8] found that crime sceneinvestigation metaphor workshops help designers identify unintendedconsequences of ai products by connecting speculative scenarios withreal-world user events. (co:workshop&gt;metaphor(crime)-[INCREASES]-&gt;human:designer&gt;identification(consequences)) Understanding User Perceptions, Collaborative Experience and User Engagementin Different Human-AI Interaction Designs for Co-Creative Systems [48] foundthat co-creative ai systems that contributed sketches as design inspirationsenhanced the design task experience when bidirectional communicationwas enabled. (ai:co-creative&gt;sketch(bidirectional)-[INCREASES]-&gt;human:designer&gt;experience)(4)Representativefindings include: Exploration of Explainable AI for Trust Development on Human-AI Interaction [6] foundthat explainable ai facilitates trust formation through affective processing mecha-nisms beyond cognitive explanations. (ai:explainable&gt;affective -[INCREASES]-&gt;human&gt;#trust) Help Me Help the AI: Understanding How Explainability Can Support Human-AI Interaction [26] found that end-users of ai applications prioritize practicallyuseful explanations that improve ai collaboration over technical system details.
) -[INCREASES]-&gt; human:student&gt;interaction(multidimensional))  AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts [65] found that llm chains significantly enhanced users' perceived system transparency, controllability, and sense of collaboration.(ai:llm(chains) -[INCREASES]-&gt; human:user&gt;#transparency) (2) (ai:explanation&gt;practical -[INCREASES]-&gt; co:collaboration&gt;human(ai)) (3) Design Relationships: This cluster encompasses 68 empirical findings examining how AI design characteristics influence designers themselves and design processes.Representative findings include:  "Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction [26] found that users employ ai explanations for multiple purposes: calibrating trust, improving task skills, optimizing inputs to the ai, and providing developer feedback.(ai:explanation -[INFLUENCES]-&gt; human:usage:multipurpose)  Measuring User Experience Inclusivity in Human-AI Interaction via Five User Problem-Solving Styles [2] found that ai products with greater user control features showed increased inclusivity across different problem-solving styles.(ai:product&gt;control -[INCREASES]-&gt; co:inclusivity&gt;problem-solving) (5) Self-Referential Research: We identified 6 empirical relationships examining recursive or evolutionary effects within participant populations, indicating research that examines how human behaviors and characteristics influence other human behaviors in AI-mediated contexts.Representative findings include:  The Human in the Infinite Loop: A Case Study on Revealing and Explaining Human-AI Interaction Loop Failures [35] found that human-ai loops in 3d model processing systems often fail to converge due to inconsistent human judgments and preferencebased optimization limitations.(human:judgment&gt;inconsistent -[DECREASES]-&gt; co:loop&gt;convergence)  The Human in the Infinite Loop: A Case Study on Revealing and Explaining Human-AI Interaction Loop Failure [35] found that ai system outcomes influence subsequent user inputs through cognitive biases including heuristic biases and loss aversion.(ai:outcome -[INFLUENCES]-&gt; human:input) 4.2</p>
<p>Table 1 .
1
1) Large Language Models (ai:llm): Healthcare research addresses prompt engineering for clinical accuracy, educational applications focus on scaffolding techniques for learning, Top five concepts with highest structural hole scores, indicating greatest potential for crosscommunity integration.(2) Generative AI Systems (ai:generative): Despite connecting 18 research communities, generative AI lacks coherent interaction paradigms across domains.The structural hole score (43.13) reflects this fragmentation: creative applications emphasize co-creation while analytical tools prioritize accuracy, yet underlying collaboration patterns remain unexamined systematically.(3) Trust Dynamics (human:trust): Trust mechanisms vary substantially across AI applications; medical systems emphasize clinical validation while educational systems prioritize transparency.The structural hole score (30.53) suggests that domain-specific approaches develop in isolation despite potentially shared psychological foundations.(
Nodecommunity constraint betweenness effective_size structural_hole_scoreai:llm650.0110.15194.31385.865ai:generative110.0150.13365.78543.138human:trust170.0180.11655.82130.529ai:chatbot550.0210.07148.50023.283ai:agent750.0230.06546.75020.631
and accessibility research develops adaptive interfaces.However, cross-domain knowledge transfer remains limited.The lowest constraint value (0.011) and highest structural hole score (85.86) indicate significant potential for developing unified human-LLM interaction frameworks.</p>
<p>Table 2 .
2
A.1.1 Cluster H0: Users with Disabilities and Vulnerabilities.Description: Individuals with physical, sensory, or cognitive disabilities who require accessible AI interfaces and adaptive technologies.This cluster focuses on inclusive design and accessibility considerations in human-AI interaction.Cluster H1: Creative and Technical Content Creators.Description: Artists, developers, and creative professionals who use AI as a tool for content creation and technical development.This cluster explores the collaboration between human creativity and AI-assisted production across various media.Representatives: human:composer, human:chemist, human:author, human:producer, human:songwriter A.1.3ClusterH2:Educators and Sports Professionals.Description: Teaching professionals and sports-related roles who leverage AI for training, instruction, and performance enhancement.This cluster represents the use of AI in pedagogical and athletic development contexts.Medical and Healthcare Professionals.Description: Healthcare practitioners and specialists across various medical domains who interact with AI systems for diagnosis, treatment, and patient care.This cluster represents the intersection of AI technology with clinical practice and medical expertise.Cluster H4: Age Groups and Family Relationships.Description: Different age demographics and family-based roles that interact with AI systems, from children to elderly users.This cluster emphasizes life-stage specific needs and intergenerational dynamics in AI adoption.Cluster H6: Diverse Stakeholders and User Roles.Description: A heterogeneous group of users representing various societal roles, occupations, and contexts where AI systems are deployed.This cluster captures the broad spectrum of non-specialist users who interact with AI in everyday scenarios.A.3.9ClusterC8: Risks and Challenges.Description: This cluster identifies potential barriers, risks, and negative consequences in human-AI interaction, including security threats, conflicts, and disparities.It addresses both technical limitations and social challenges like ostracism and transgression that can arise from AI deployment.Top 20 most frequent causes, effects, and cause-effect linkages in the research graph.B.2 Underdeveloped Areas and Research Integration Opportunities Analysis
Representatives:human:blind,human:autistic,human:disadvantaged,hu-man:visually_impaired, human:deafA.1.2 Representatives: human:coach, human:team, human:educator, human:instructor, hu-man:teacherA.1.4 Cluster H3: Representatives: human:biologist, human:pathologist, human:surgeon, human:ophthalmologist,human:practitionerA.1.5 Representatives: human:group, human:senior, human:family, human:older_adult, human:youthA.1.6 Cluster H5: Organizational and Administrative Professionals. Description: Workers in man-agement, administration, and public service roles who use AI for decision-making and organizationalprocesses. This cluster represents the integration of AI in workplace hierarchies and bureaucraticsystems.Representatives: human:civil_servant, human:employee, human:member, human:executive,human:leaderA.1.7</p>
<p>Table 3 .
3
Top 20 Nodes Spanning Structural Holes
Nodehome_community num_external_communities degree betweennessai:generative11181190.133489ai:llm65171260.150596ai:agent7516580.064660human:student55151040.069600ai:explanation1715610.058686human:designer1115710.051565human&gt;trust1714760.116269ai:chatbot5512650.070510ai:assistant4812360.040824human:participant7511600.049094co:interaction2610440.057347human:expert610230.018544co:collaboration939450.046421ai&gt;assistance1009230.025489ai:chatgpt558370.021821ai&gt;explainability548230.018313human:player768210.017318human:learner88260.016962human:researcher658210.014520ai:interactive68180.010258</p>
<p>Table 4 .
4
Top 20 community Bridges</p>
<p>Representatives: human:legal, human:crowdworker, human:immigrant, human:american, human:agent A.2 AI-Related Clusters A.2.1 Cluster A0: Autonomous Systems and Security Applications.Description: Focuses on AI in autonomous vehicles, drones, and security-critical applications including adversarial contexts and strategic planning.This cluster addresses both offensive and defensive AI capabilities in physical and computational domains.Representatives: ai:drone, ai:security, ai:uav, ai:adversarial, ai:navigation Output: { "cause": { "type": "ai", "subtype": "interactive", "feature": "interface" }, "relationship": "INCREASES", "effect": { "type": "human", "subtype": "artist", "feature": "creativity(writing)" }, } Input: "Engagement mechanisms improve users' positive perceptions of the robot" Output: { "cause": { "type": "ai", "subtype": "", "feature": "engagement" }, "relationship": "INCREASES", "effect": { "type": "human", "subtype": "", "feature": "#robot" }, } Input: "LLM-based tutoring systems significantly improve learning outcomes for medical students" Output: { "cause": { "type": "ai", "subtype": "llm", "feature": "tutoring" }, "relationship": "INCREASES", "effect": { "type": "human", "subtype": "student(medical)", "feature": "learning" }, } Input: "ChatGPT's explanation capability reduces the frequency of user misconceptions about complex topics" Output: { "cause": { "type": "ai", "subtype": "chatgpt", "feature": "explanation" }, "relationship": "DECREASES", "effect": { "type": "human", "subtype": "user", "feature": "misconception(complex)" }, } Input: "Collaborative projects between domain experts and generative AI tools lead to more novel solutions" Output: { "cause": { "type": "co", "subtype": "collaboration", "feature": "expert(ai)" }, "relationship": "INCREASES", "effect": { "type": "co", "subtype": "solution", "feature": "novelty" }, } Input: "Over-reliance on AI assistance reduces students' problem-solving skills" Output: { "cause": { "type": "ai", "subtype": "", "feature": "assistance" }, "relationship": "DECREASES", "effect": { "type": "human", "subtype": "student", "feature": "problem_solving" }, } ## Rules Focus on primary causal relationship Use specific terms over general ones -"system features"   "explanation interface" Standardize subjects -No redundant terms (e.g., "generative AI" -&gt; type="ai", subtype="generative") -Always use lowercase for all fields -Prefer using both type AND subtype when possible (only omit subtype for very generalized concepts) -Keep subtypes broad and taxonomical rather than overly specific Remove numerical metrics -"AI performance by 27%"   feature="performance" ## feature Special Cases Perception (#) -Use # prefix for beliefs/perspectives/ideas -Example: "perception of trust" "#trust" Nested features -Use parenthesis for nested features -Use fewest levels possible -Example: "reliance on AI" "reliance(ai)", "misconception about AI" "misconception(ai)"## Type and Subtype Rules For Subject.type, ONLY use "human", "ai", or "co" Empty string ("") for subtype is valid when no specific subtype applies Always parse nested subjects correctly: -"human:student" type="human", subtype="student" -"ai:generative" type="ai", subtype="generative"
Guidelines for human-AI interaction. Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, Proceedings of the 2019 chi conference on human factors in computing systems. the 2019 chi conference on human factors in computing systems2019</p>
<p>Measuring User Experience Inclusivity in Human-AI Interaction via Five User Problem-Solving Styles. Andrew Anderson, Jimena Noa Guevara, Fatima Moussaoui, Tianyi Li, Mihaela Vorvoreanu, Margaret Burnett, 10.1145/3663740ACM Trans. Interact. Intell. Syst. 14902024. Sept. 2024</p>
<p>Effects of communication directionality and AI agent differences in human-AI interaction. Zahra Ashktorab, Casey Dugan, James Johnson, Qian Pan, Wei Zhang, Sadhana Kumaravel, Murray Campbell, Proceedings of the 2021 CHI conference on human factors in computing systems. the 2021 CHI conference on human factors in computing systems2021</p>
<p>Scientometric analysis of the CHI proceedings. Christoph Bartneck, Jun Hu, 10.1145/1518701.1518810Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsBoston, MA, USA; New York, NY, USAAssociation for Computing Machinery2009CHI '09)</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Exploration of Explainable AI for Trust Development on Human-AI Interaction. L Ezekiel, Rosemary R Bernardo, Seva, 10.1145/3639592.3639625Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference. the 2023 6th Artificial Intelligence and Cloud Computing ConferenceKyoto, Japan; New York, NY, USAAssociation for Computing Machinery2024AICCC '23)</p>
<p>Co-authorship and bibliographic coupling network effects on citations. Claudio Biscaro, Carlo Giupponi, PloS one. 9e995022014. 2014</p>
<p>Sofia Fled or Died? Design Fictional Explorations of Unintended and Unsustainable Consequences of Human-AI Interaction. Johan Blomkvist, Wanjun Chu, Emma Mainza Chilufya, 10.1145/3706599.3719936Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25). the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25)New York, NY, USA, ArticleAssociation for Computing Machinery2025489</p>
<p>Fast unfolding of communities in large networks. Jean-Loup Vincent D Blondel, Renaud Guillaume, Etienne Lambiotte, Lefebvre, 10.1088/1742-5468/2008/10/p10008Journal of Statistical Mechanics: Theory and Experiment. 10P100082008. 2008. Oct. 2008</p>
<p>S Ronald, Burt, Structural Holes: The Social Structure of Competition. Harvard University Press1992</p>
<p>A Survey on Evaluation of Large Language Models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 15452024. March 2024</p>
<p>Scientometric analysis of scientific publications in CSCW. Antnio Correia, Hugo Paredes, Benjamim Fonseca, Scientometrics. 1142018. 2018</p>
<p>Wearable Reasoner: towards enhanced human rationality through a wearable device with an explainable AI assistant. Valdemar Danry, Pat Pataranutaporn, Yaoli Mao, Pattie Maes, Proceedings of the Augmented Humans International Conference. the Augmented Humans International Conference2020</p>
<p>Don't just tell me, ask me: Ai systems that intelligently frame explanations as questions improve human logical discernment accuracy over causal ai explanations. Valdemar Danry, Pat Pataranutaporn, Yaoli Mao, Pattie Maes, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Structured information extraction from complex scientific text with fine-tuned large language models. Alexander Dunn, John Dagdelen, Nicholas Walker, Sanghoon Lee, Gerbrand Andrew S Rosen, Kristin Ceder, Anubhav Persson, Jain, arXiv:2212.052382022. 2022arXiv preprint</p>
<p>Augmenting human intellect: A conceptual framework. Engelbart Douglas, Augmented education in the global age. Routledge2023</p>
<p>A density-based algorithm for discovering clusters in large spatial databases with noise. Martin Ester, Hans-Peter Kriegel, Jrg Sander, Xiaowei Xu, In kdd. 961996</p>
<p>Cathy Mengying Fang, Valdemar Auren R Liu, Eunhae Danry, Samantha Wt Lee, Pat Chan, Pattie Pataranutaporn, Jason Maes, Michael Phang, Lama Lampe, Ahmad, arXiv:2503.17473How ai and human behaviors shape psychosocial effects of chatbot use: A longitudinal randomized controlled study. 2025. 2025arXiv preprint</p>
<p>A typology of reviews: an analysis of 14 review types and associated methodologies. J Maria, Andrew Grant, Booth, Health information &amp; libraries journal. 262009. 2009</p>
<p>Exploring the impact of AI value alignment in collaborative ideation: Effects on perception, ownership, and output. Alicia Guo, Pat Pataranutaporn, Pattie Maes, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Mapping human-computer interaction research themes and trends from its existence to today: A topic modeling-based review of past 60 years. Fatih Gurcan, Nergiz Ercil Cagiltay, Kursat Cagiltay, International Journal of Human-Computer Interaction. 372021. 2021</p>
<p>What Is Interaction?. Kasper Hornbaek, Antti Oulasvirta, 10.1145/3025453.3025765Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. the 2017 CHI Conference on Human Factors in Computing SystemsDenver, Colorado, USA; New York, NY, USAAssociation for Computing Machinery2017</p>
<p>Societal-scale human-AI interaction design? How hospitals and companies are integrating pervasive sensing into mental healthcare. Angel Hsing, -Chi Hwang, Dan Adler, Meir Friedenberg, Qian Yang, Proceedings of the 2024 CHI conference on human factors in computing systems. the 2024 CHI conference on human factors in computing systems2024</p>
<p>Ai in your mind: Counterbalancing perceived agency and experience in human-ai interaction. Hsing-Chi Angel, Andrea Hwang, Stevenson Won, Chi conference on human factors in computing systems extended abstracts. 2022</p>
<p>Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration. Hayeon Jeon, Suhwoo Yoon, Keyeun Lee, Hyeong Seo, Esther Hehsun Kim, Seonghye Kim, Yena Cho, Soeun Ko, Laura Yang, John Dabbish, Zimmerman, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025</p>
<p>Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction. S Y Sunnie, Elizabeth Anne Kim, Olga Watkins, Ruth Russakovsky, Andrs Fong, Monroy-Hernndez, 10.1145/3544548.3581001Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing SystemsHamburg, Germany; New York, NY, USA, ArticleAssociation for Computing Machinery202317CHI '23)</p>
<p>Spiritual AI: Exploring the possibilities of a human-AI interaction beyond productive goals. Soonho Kwon, Dong Whi Yoo, Younah Kang, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Assessing human-AI interaction early through factorial surveys: a study on the guidelines for human-AI interaction. Tianyi Li, Mihaela Vorvoreanu, Derek Debellis, Saleema Amershi, ACM Transactions on Computer-Human Interaction. 302023. 2023</p>
<p>Chatbot companionship: a mixed-methods study of companion chatbot usage patterns and their relationship to loneliness in active users. Pat Auren R Liu, Pattie Pataranutaporn, Maes, arXiv:2410.215962024. 2024arXiv preprint</p>
<p>CHI 1994-2013: mapping two decades of intellectual progress through co-word analysis. Yong Liu, Jorge Goncalves, Denzil Ferreira, Bei Xiao, Simo Hosio, Vassilis Kostakos, 10.1145/2556288.2556969Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsToronto, Ontario, Canada; New York, NY, USAAssociation for Computing Machinery2014</p>
<p>Intelligent software. Pattie Maes, Proceedings of the 2nd international conference on Intelligent user interfaces. the 2nd international conference on Intelligent user interfaces1997</p>
<p>Knowledge structure transition in library and information science: topic modeling and visualization. Yosuke Miyata, Emi Ishita, Fang Yang, Michimasa Yamamoto, Azusa Iwase, Keiko Kurata, Scientometrics. 1252020. 2020</p>
<p>Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives. Meredith Ringel, Morris , Jed R Brubaker, 10.1145/3706598.3713758Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI '25). the 2025 CHI Conference on Human Factors in Computing Systems (CHI '25)New York, NY, USA, ArticleAssociation for Computing Machinery2025536</p>
<p>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, arXiv:2106.09700[cs.CL2021</p>
<p>The Human in the Infinite Loop: A Case Study on Revealing and Explaining Human-AI Interaction Loop Failures. Changkun Ou, Daniel Buschek, Sven Mayer, Andreas Butz, 10.1145/3543758.3543761Proceedings of Mensch Und Computer. Mensch Und ComputerDarmstadt, Germany; New York, NY, USAAssociation for Computing Machinery2022. 2022MuC '22)</p>
<p>Re-examining user burden in human-AI interaction: focusing on a domain-specific approach. Hyanghee Park, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Cyborg Psychology: The Art &amp; Science of Designing Human-AI Systems that Support Human Flourishing. Pat Pataranutaporn, 2024Massachusetts Institute of Technology</p>
<p>Slip Through the Chat: Subtle Injection of False Information in LLM Chatbot Conversations Increases False Memory Formation. Pat Pataranutaporn, Chayapatr Archiwaranguprok, Samantha Wt Chan, Elizabeth Loftus, Pattie Maes, Proceedings of the 30th International Conference on Intelligent User Interfaces. the 30th International Conference on Intelligent User Interfaces2025</p>
<p>Synthetic human memories: Ai-edited images and videos can implant false memories and distort recollection. Pat Pataranutaporn, Chayapatr Archiwaranguprok, Samantha Wt Chan, Elizabeth Loftus, Pattie Maes, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025</p>
<p>AI-generated characters for supporting personalized learning and well-being. Pat Pataranutaporn, Valdemar Danry, Joanne Leong, Parinya Punpongsanon, Dan Novy, Pattie Maes, Misha Sra, Nature Machine Intelligence. 32021. 2021</p>
<p>Influencing human-AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness. Pat Pataranutaporn, Ruby Liu, Ed Finn, Pattie Maes, Nature Machine Intelligence. 52023. 2023</p>
<p>Living Memory: Towards a Generative Conversational Agent using a Personalized Language Model. Pat Pataranutaporn, Lavanay Thakral, Pattie Maes, Misha Sra, Proceedings of ACM Augmented Humans (Under Review). ACM Augmented Humans (Under Review)ACM2021</p>
<p>Future you: a conversation with an AI-generated future self reduces anxiety, negative emotions, and increases future self-continuity. Pat Pataranutaporn, Kavin Winson, Peggy Yin, Auttasak Lapapirojn, Pichayoot Ouppaphan, Monchai Lertsutthiwong, Pattie Maes, Hal E Hershfield, 2024 IEEE Frontiers in Education Conference (FIE). IEEE2024</p>
<p>Jason Phang, Michael Lampe, Lama Ahmad, Sandhini Agarwal, Cathy Mengying Fang, Valdemar Auren R Liu, Danry, Eunhae Lee, Pat Pataranutaporn, and Pattie Maes. 2025. Investigating Affective Use and Emotional Well-being on ChatGPT. </p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. P Maciej, Dane Polak, Morgan, 10.1038/s41467-024-45914-8Nature Communications. 1512024. Feb. 2024</p>
<p>Effects of Proactive Interaction and Instructor Choice in AI-Generated Virtual Instructors for Financial Education. Thanawit Prasongpongchai, Pat Pataranutaporn, Auttasak Lapapirojn, Chonnipa Kanapornchai, Joanne Leong, Pichayoot Ouppaphan, Kavin Winson, Monchai Lertsutthiwong, Pattie Maes, 2024 IEEE Frontiers in Education Conference (FIE). IEEE2024</p>
<p>Talk to the Hand: an LLM-powered Chatbot with Visual Pointer as Proactive Companion for On-Screen Tasks. Thanawit Prasongpongchai, Pat Pataranutaporn, Monchai Lertsutthiwong, Pattie Maes, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025</p>
<p>Understanding User Perceptions, Collaborative Experience and User Engagement in Different Human-AI Interaction Designs for Co-Creative Systems. Jeba Rezwana, Mary Lou Maher, 10.1145/3527927.3532789Proceedings of the 14th Conference on Creativity and Cognition. the 14th Conference on Creativity and CognitionVenice, Italy; New York, NY, USAAssociation for Computing Machinery2022C&amp;C '22)</p>
<p>An Umbrella Review of Reporting Quality in CHI Systematic Reviews: Guiding Questions and Best Practices for HCI. Katja Rogers, Teresa Hirzle, Sukran Karaosmanoglu, Paula Toledo Palomino, Ekaterina Durmanova, Seiji Isotani, Lennart E Nacke, 10.1145/3685266ACM Trans. Comput.-Hum. Interact. 31552024. Nov. 2024</p>
<p>Human-Computer Interaction and AI: What practitioners need to know to design and build effective AI system from a human perspective. Daniel Russell, Chinmay Vera Liao, Elena L Kulkarni, Nikolas Glassman, Martelaro, Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. 2023</p>
<p>Bidirectional Human-AI Alignment: Emerging Challenges and Opportunities. Hua Shen, Tiffany Knearem, Reshmi Ghosh, Xieyang Michael, Andrs Liu, Tongshuang Monroy-Hernndez, Diyi Wu, Yun Yang, Tanushree Huang, Yang Mitra, Li, Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems2025</p>
<p>Mapping HCI research methods for studying social media interaction: A systematic literature review. Yuya Shibuya, Andrea Hamm, Teresa Cerratto Pargman, Computers in Human Behavior. 1291071312022. 2022</p>
<p>Direct manipulation vs. interface agents. interactions. Ben Shneiderman, Pattie Maes, 1997. 19974</p>
<p>A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models. Zihan Song, Gyo-Yeob Hwang, Xin Zhang, Shan Huang, Byung-Kwon Park, Scientific Reports. 1516082025. 2025</p>
<p>Challenges and Opportunities in the Responsible Use of AI and Human-Computer Interaction. Laura Steckman, Nandini Iyer, Boyoung Kim, Joseph Lyons, Massimiliano Cappuccio, Mark Richard Gaffley, Elizabeth Phillips, Sonia Sousa, Max Van Kleek, Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems2025</p>
<p>Literature Reviews in HCI: A Review of Reviews. Evropi Stefanidi, Marit Bentvelzen, Pawe W Woniak, Thomas Kosch, P Mikoaj, Thomas Woniak, Stefan Mildner, Heiko Schneegass, Jasmin Mller, Niess, 10.1145/3544548.3581332Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing SystemsHamburg, Germany; New York, NY, USA, ArticleAssociation for Computing Machinery2023509CHI '23)</p>
<p>Map of science with topic modeling: Comparison of unsupervised learning and human-assigned subject classification. Arho Suominen, Hannes Toivanen, Journal of the Association for Information Science and Technology. 672016. 2016</p>
<p>Quoc Huy, Ming To, Guangyan Liu, Huang, arXiv:2408.10729[cs.CLTowards Efficient Large Language Models for Scientific Text: A Review. 2024</p>
<p>Document co-citation analysis to enhance transdisciplinary research. M Caleb, Tammy M Trujillo, Long, Science advances. 4e17011302018. 2018</p>
<p>Evaluating interactive AI: Understanding and controlling placebo effects in human-AI interaction. Steeven Villa, Robin Welsch, Alena Denisova, Thomas Kosch, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>From human-human collaboration to Human-AI collaboration: Designing AI systems that can work together with people. Dakuo Wang, Elizabeth Churchill, Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi, Qianying Wang, Extended abstracts of the 2020 CHI conference on human factors in computing systems. 2020</p>
<p>Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant. Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, Ashok Goel, Proceedings of the 2021 CHI conference on human factors in computing systems. the 2021 CHI conference on human factors in computing systems2021</p>
<p>Theory of mind in human-ai interaction. Qiaosi Wang, Sarah Walsh, Mei Si, Jeffrey Kephart, Justin D Weisz, Ashok K Goel, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Does human-AI trust affect human-AI interaction in the metaverse? Insight from a pilot study. Tinghui Wu, Yanjie Song, Ching-Sing Chai, Ronnel B King, Yuehan Zhai, Xintian Ren, Xuesong Zhai, 10.1145/3723366.3723397Proceedings of the 2024 4th International Symposium on Big Data and Artificial Intelligence (ISBDAI '24). the 2024 4th International Symposium on Big Data and Artificial Intelligence (ISBDAI '24)New York, NY, USAAssociation for Computing Machinery2025</p>
<p>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. Tongshuang Wu, Michael Terry, Carrie , 10.1145/3491102.3517582Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing SystemsNew Orleans, LA, USA; New York, NY, USA, ArticleAssociation for Computing MachineryJun Cai. 2022385CHI '22)</p>
<p>Large language models for generative information extraction: A survey. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, Enhong Chen, Frontiers of Computer Science. 181863572024. 2024</p>
<p>Clustering scientific documents with topic modeling. Chyi-Kwei Yau, Alan Porter, Nils Newman, Arho Suominen, Scientometrics. 1002014. 2014</p>
<p>Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler, Michelle Vanni, Jiawei Han, arXiv:1812.09551TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering. 2018</p>
<p>UX research on conversational human-AI interaction: A literature review of the ACM digital library. Qingxiao Zheng, Yiliu Tang, Yiren Liu, Weizi Liu, Yun Huang, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>            </div>
        </div>

    </div>
</body>
</html>
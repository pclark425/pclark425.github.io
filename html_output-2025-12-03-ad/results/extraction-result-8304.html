<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8304 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8304</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8304</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-277151094</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.15848v2.pdf" target="_blank">Entropy-based Exploration Conduction for Multi-step Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Multi-step processes via large language models (LLMs) have proven effective for solving complex reasoning tasks. However, the depth of exploration of the reasoning procedure can significantly affect the task performance. Existing methods to automatically decide the depth often lead to high cost and a lack of flexibility. To address these issues, we propose Entropy-based Exploration Depth Conduction (Entro-duction), a novel method that dynamically adjusts the exploration depth during multi-step reasoning by monitoring LLM's output entropy and variance entropy. We employ these two features to capture the model's uncertainty of the current step and the fluctuation of uncertainty across consecutive reasoning steps. Based on the observed entropy changes, the LLM selects whether to deepen, expand, or stop exploration according to the probability, which facilitates the trade-off between the reasoning accuracy and exploration effectiveness. Experimental results across four benchmark datasets demonstrate the efficacy of Entro-duction.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8304.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8304.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entro-duction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy-based Exploration Depth Conduction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that dynamically adjusts multi-step reasoning exploration depth by monitoring token-level entropy and variance-entropy to choose actions (Deepen, Expand, Stop) via an epsilon-greedy policy, balancing accuracy and exploration cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8B-parameter Llama-3.1 instruct-tuned Transformer model used as the reasoner in experiments (temperature 0.7, max tokens 128).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Entropy- and variance-entropy guided dynamic multi-step reasoning', 'Deepen (extend a reasoning chain)', 'Expand (branch a reasoning chain)', 'Stop (terminate a chain)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At each reasoning step, token-level probabilities are converted to normalized entropy and variance-entropy metrics; their deltas (∆H, ∆σ^2_H) determine a preferred action via mapping Φ, and the system samples actions with an ϵ-greedy policy. Behaviors: Deepen appends a next node to the chain; Expand branches into multiple chains; Stop terminates a chain. A soft-stop option allows a small fixed extra number of steps before final termination.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Entro-duction itself supports both single-chain deepening and multi-branch expansion. Experiments compare Entro-duction to single-chain and multi-branch baselines (CoT, CoT-SC, ToT, Complex CoT) and include ablations: (1) Base / Entropy-only / Variance-only / Both; (2) w/ and w/o Expand behavior; (3) Stop strategies (hard Stop@1 vs soft Stop@2/Stop@3); (4) ϵ sweep (0.05,0.1,0.25,0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (multistep math), SVAMP (simpler math), StrategyQA (strategic commonsense QA), CommonsenseQA (CSQA, commonsense QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Entro-duction achieves accuracies (approx.) GSM8K 85.4%, SVAMP 92.0%, StrategyQA 70.3%, CSQA 79.6% while using relatively few steps (reported ~9.5, 11.2, 9.6, 7.1 steps respectively). It outperforms CoT (GSM8K ~75%), CoT-SC@maj64 (~80% on GSM8K), Complex CoT (~81% on GSM8K), Self-talk and DRR on most datasets; DRR and Self-talk show competitive performance on some datasets (e.g., DRR ~90.2% on SVAMP) but generally trail Entro-duction on GSM8K and StrategyQA.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Using both entropy and variance-entropy jointly yields best results: entropy-only can lead to premature stopping in high-outcome/low-fluctuation scenarios, variance-only slightly outperforms entropy but remains inadequate; removing Expand reduces accuracy especially on tasks needing branching (SVAMP, StrategyQA); hard stop harms performance, while soft stop (1-2 extra steps) improves outcomes; ϵ≈0.25 balances exploration and exploitation best across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Joint monitoring of entropy and variance-entropy enables dynamic selection among Deepen/Expand/Stop to improve accuracy while avoiding redundant reasoning; branching (Expand) is crucial for tasks requiring multiple solution paths; soft stopping and moderate stochastic exploration (ϵ≈0.25) further improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8304.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8304.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits step-by-step linear reasoning chains from LLMs to solve complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (as evaluated with CoT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 8B Llama-3.1-Instruct model used with 'Let's think step-by-step.' style CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (linear single-chain reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompted prompting to generate a single linear sequence of reasoning steps (fixed number of steps, e.g., 8 for math tasks, 5 for commonsense tasks in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>CoT is used as a baseline with fixed step counts; compared directly against multi-chain/self-consistency and tree-structured methods and Entro-duction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA, CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported approx. accuracies: GSM8K ~75%, SVAMP ~83.4%, StrategyQA ~57.7%, CSQA ~75.6%; number of steps set per task (e.g., 8 steps math, 5 steps commonsense).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Performs well on many problems but can fail when deeper exploration or branching is needed; fixed-step CoT can under- or over-reason if depth misaligned with problem requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Single-chain CoT is efficient but limited by fixed depth; dynamic or branching strategies (as in Entro-duction or ToT) can improve coverage and accuracy on tasks requiring multiple solution paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8304.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8304.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method sampling multiple CoT chains and aggregating via majority vote to improve robustness of reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (as evaluated with CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 8B Llama model; multiple CoT samples (maj8, maj64) are generated and majority vote selects final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Repeated chain-of-thought sampling with majority vote (self-consistency)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate many independent CoT chains (e.g., 8 or 64 samples) then aggregate answers by majority voting to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (many similar chains sampled to induce diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared CoT-SC@maj8 and CoT-SC@maj64 against CoT, ToT, Complex CoT, and Entro-duction; number of samples and resulting step counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA, CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported improvements over single CoT (e.g., CoT-SC@maj64 ~80% on GSM8K versus CoT ~75%), but with higher computational cost (more total steps/samples).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Sampling multiple similar reasoning chains increases robustness and accuracy but at substantial computational cost; still sometimes outperformed by approaches that use dynamic depth/branching more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency helps by aggregating diverse sampled chains but is less efficient than dynamic, entropy-guided exploration that focuses computation where needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8304.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8304.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-structured reasoning method that explores multiple branching thought trajectories to find better solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (as evaluated with ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same Llama-3.1-8B tuned model; ToT configuration in experiments generated 3 branches per step for 5 layers (leading to many steps overall).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Tree-structured, multi-branch exploration (explicit branching at each step)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At each layer, multiple branches are generated and expanded to form a tree of candidate reasoning trajectories; final answer chosen from leaf evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a baseline tree-structured method (3 branches/step, 5 layers) and compared with Entro-duction; ToT required many more steps (often >100) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA, CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported accuracies sometimes comparable but at much higher step counts (e.g., ToT reported requiring large numbers of steps, e.g., >100). Example accuracy on GSM8K ~72.6% with ~121 steps in table.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Tree search covers broader solution space and can find complex solutions, but is computationally expensive; Entro-duction seeks to capture branching benefits (Expand) with much lower step cost.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Branched exploration helps when multiple plausible solution paths exist, but costly; guided selective branching (entropy-driven Expand) recovers benefits more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8304.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8304.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complex CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complex Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CoT variant that engages with complex samples and selects the best solution from multiple intricate reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (as evaluated with Complex CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 8B Llama model; Complex CoT configured for longer/more intricate chains (settings per Zhou et al. referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Structured/complex chain-of-thought that attempts to capture multi-faceted reasoning within CoT-style outputs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate more detailed and/or structured CoT chains and select best answer among them; intended to handle multifaceted reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar/structured</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared as a baseline against Entro-duction and other methods using fixed-step complex CoT configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA, CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported example GSM8K accuracy ~81.4% (with ~8 steps), generally better than vanilla CoT but below Entro-duction on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Handles multifaceted problems better than simple CoT but still constrained by preset structure; lacks dynamic depth adjustment based on model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Complex CoT improves over basic CoT on harder problems, but entropy-guided dynamic strategies can further improve accuracy while controlling step cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8304.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8304.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-talk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-talk (exploratory questioning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where the LLM generates exploratory questions (self-talk) to elicit implicit background knowledge and use that to improve final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (as evaluated with Self-talk)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base Llama model; Self-talk procedure elicits auxiliary questions/knowledge related to the main problem and uses them to form responses.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Exploratory self-questioning to reveal implicit knowledge', 'Selection among generated responses']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model generates intermediate probing questions or subreasoning outputs to surface relevant knowledge; selects best answer from resulting outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared as a reasoning-depth optimization baseline (no additional training) against Entro-duction and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA, CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported accuracies approx.: GSM8K 79.1%, SVAMP 83.7%, StrategyQA 61.5%, CSQA 70.0%; generally below Entro-duction on several datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Helps uncover background knowledge and can improve answers without extra training, but may be less effective than entropy-guided exploration in balancing depth and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-talk improves reasoning by surfacing implicit context but is less consistent than Entro-duction's targeted exploration; Entro-duction attains higher accuracy without extra model training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8304.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8304.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distillation-Reinforcement-Reasoning (DRR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A depth-optimization method that distills LLM reasoning into synthetic data and trains a lightweight model to provide feedback for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct (as evaluated where DRR was used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DRR is a multi-step pipeline (distillation into data, train a small feedback model) used as a baseline for depth optimization comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Distillation of reasoning traces and learned feedback to guide depth/steps']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM outputs are distilled into synthetic training data used to train a lightweight model that provides feedback (e.g., when to stop or refine) to the LLM's reasoning process.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (method optimizes depth but can enable varied strategies via feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to Entro-duction; DRR requires additional training and separate model components while Entro-duction operates online without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA, CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported accuracies approx.: GSM8K 83.0%, SVAMP 90.2%, StrategyQA 67.7%, CSQA 82.1%; competitive on some datasets (SVAMP, CSQA) but overall trailing Entro-duction on GSM8K and StrategyQA.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>DRR benefits from offline training and distilled feedback but entails extra resource/training cost; Entro-duction claims similar or better accuracy in some cases with lower runtime overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>DRR can achieve strong results but requires auxiliary training; Entro-duction provides a lightweight, training-free alternative that dynamically adjusts depth via entropy signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8304.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8304.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3.1 8B Instruct-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The specific language model used as the reasoner for all experimental comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruct-tuned 8-billion parameter Llama-3.1 model (publicly available checkpoint referenced). Experiments used temperature 0.7 and max token limit 128 on an NVIDIA 4090 GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Used with various prompting/structuring methods: CoT, CoT-SC, ToT, Complex CoT, Self-talk, DRR, and Entro-duction']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The same model is driven by different prompting/structuring or algorithmic wrappers: single-chain CoT, multi-sample self-consistency, tree search, complexity-focused CoT, exploratory self-talk, distilled feedback pipelines, and entropy-guided dynamic exploration (Entro-duction).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (experiments evaluate both single-chain and multi-branch/diverse strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Baseline and proposed methods are evaluated on four datasets to compare single-chain vs multi-chain vs tree vs entropy-guided adaptive depth strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA, CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>See method-specific entries; Entro-duction with this model obtains best reported accuracy across most evaluated tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The Llama-3.1-8B-Instruct model benefits from dynamic, entropy-driven depth control to avoid over- or under-reasoning compared to fixed-step CoT or exhaustive ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>When used with Entro-duction, the Llama-3.1-8B-Instruct model attains higher accuracy with fewer or comparable steps than many baselines, indicating the effectiveness of entropy-guided dynamic exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entropy-based Exploration Conduction for Multi-step Reasoning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Unsupervised commonsense question answering with selftalk <em>(Rating: 1)</em></li>
                <li>Distilling system 2 into system 1 <em>(Rating: 1)</em></li>
                <li>Detecting hallucinations in large language models using semantic entropy <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8304",
    "paper_id": "paper-277151094",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Entro-duction",
            "name_full": "Entropy-based Exploration Depth Conduction",
            "brief_description": "A method that dynamically adjusts multi-step reasoning exploration depth by monitoring token-level entropy and variance-entropy to choose actions (Deepen, Expand, Stop) via an epsilon-greedy policy, balancing accuracy and exploration cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_description": "An 8B-parameter Llama-3.1 instruct-tuned Transformer model used as the reasoner in experiments (temperature 0.7, max tokens 128).",
            "reasoning_methods": [
                "Entropy- and variance-entropy guided dynamic multi-step reasoning",
                "Deepen (extend a reasoning chain)",
                "Expand (branch a reasoning chain)",
                "Stop (terminate a chain)"
            ],
            "reasoning_methods_description": "At each reasoning step, token-level probabilities are converted to normalized entropy and variance-entropy metrics; their deltas (∆H, ∆σ^2_H) determine a preferred action via mapping Φ, and the system samples actions with an ϵ-greedy policy. Behaviors: Deepen appends a next node to the chain; Expand branches into multiple chains; Stop terminates a chain. A soft-stop option allows a small fixed extra number of steps before final termination.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Entro-duction itself supports both single-chain deepening and multi-branch expansion. Experiments compare Entro-duction to single-chain and multi-branch baselines (CoT, CoT-SC, ToT, Complex CoT) and include ablations: (1) Base / Entropy-only / Variance-only / Both; (2) w/ and w/o Expand behavior; (3) Stop strategies (hard Stop@1 vs soft Stop@2/Stop@3); (4) ϵ sweep (0.05,0.1,0.25,0.5).",
            "task_or_benchmark": "GSM8K (multistep math), SVAMP (simpler math), StrategyQA (strategic commonsense QA), CommonsenseQA (CSQA, commonsense QA).",
            "performance_results": "Entro-duction achieves accuracies (approx.) GSM8K 85.4%, SVAMP 92.0%, StrategyQA 70.3%, CSQA 79.6% while using relatively few steps (reported ~9.5, 11.2, 9.6, 7.1 steps respectively). It outperforms CoT (GSM8K ~75%), CoT-SC@maj64 (~80% on GSM8K), Complex CoT (~81% on GSM8K), Self-talk and DRR on most datasets; DRR and Self-talk show competitive performance on some datasets (e.g., DRR ~90.2% on SVAMP) but generally trail Entro-duction on GSM8K and StrategyQA.",
            "qualitative_findings": "Using both entropy and variance-entropy jointly yields best results: entropy-only can lead to premature stopping in high-outcome/low-fluctuation scenarios, variance-only slightly outperforms entropy but remains inadequate; removing Expand reduces accuracy especially on tasks needing branching (SVAMP, StrategyQA); hard stop harms performance, while soft stop (1-2 extra steps) improves outcomes; ϵ≈0.25 balances exploration and exploitation best across datasets.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Joint monitoring of entropy and variance-entropy enables dynamic selection among Deepen/Expand/Stop to improve accuracy while avoiding redundant reasoning; branching (Expand) is crucial for tasks requiring multiple solution paths; soft stopping and moderate stochastic exploration (ϵ≈0.25) further improve robustness.",
            "uuid": "e8304.0",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (prompting)",
            "brief_description": "A prompting method that elicits step-by-step linear reasoning chains from LLMs to solve complex problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (as evaluated with CoT prompting)",
            "model_description": "Same 8B Llama-3.1-Instruct model used with 'Let's think step-by-step.' style CoT prompting.",
            "reasoning_methods": [
                "Chain-of-Thought (linear single-chain reasoning)"
            ],
            "reasoning_methods_description": "Prompted prompting to generate a single linear sequence of reasoning steps (fixed number of steps, e.g., 8 for math tasks, 5 for commonsense tasks in experiments).",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "CoT is used as a baseline with fixed step counts; compared directly against multi-chain/self-consistency and tree-structured methods and Entro-duction.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA, CSQA",
            "performance_results": "Reported approx. accuracies: GSM8K ~75%, SVAMP ~83.4%, StrategyQA ~57.7%, CSQA ~75.6%; number of steps set per task (e.g., 8 steps math, 5 steps commonsense).",
            "qualitative_findings": "Performs well on many problems but can fail when deeper exploration or branching is needed; fixed-step CoT can under- or over-reason if depth misaligned with problem requirements.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Single-chain CoT is efficient but limited by fixed depth; dynamic or branching strategies (as in Entro-duction or ToT) can improve coverage and accuracy on tasks requiring multiple solution paths.",
            "uuid": "e8304.1",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency",
            "brief_description": "A method sampling multiple CoT chains and aggregating via majority vote to improve robustness of reasoning outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (as evaluated with CoT-SC)",
            "model_description": "Same 8B Llama model; multiple CoT samples (maj8, maj64) are generated and majority vote selects final answer.",
            "reasoning_methods": [
                "Repeated chain-of-thought sampling with majority vote (self-consistency)"
            ],
            "reasoning_methods_description": "Generate many independent CoT chains (e.g., 8 or 64 samples) then aggregate answers by majority voting to improve reliability.",
            "reasoning_diversity": "both (many similar chains sampled to induce diversity)",
            "reasoning_diversity_experimental_setup": "Compared CoT-SC@maj8 and CoT-SC@maj64 against CoT, ToT, Complex CoT, and Entro-duction; number of samples and resulting step counts reported.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA, CSQA",
            "performance_results": "Reported improvements over single CoT (e.g., CoT-SC@maj64 ~80% on GSM8K versus CoT ~75%), but with higher computational cost (more total steps/samples).",
            "qualitative_findings": "Sampling multiple similar reasoning chains increases robustness and accuracy but at substantial computational cost; still sometimes outperformed by approaches that use dynamic depth/branching more efficiently.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency helps by aggregating diverse sampled chains but is less efficient than dynamic, entropy-guided exploration that focuses computation where needed.",
            "uuid": "e8304.2",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thought",
            "brief_description": "A tree-structured reasoning method that explores multiple branching thought trajectories to find better solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (as evaluated with ToT)",
            "model_description": "Same Llama-3.1-8B tuned model; ToT configuration in experiments generated 3 branches per step for 5 layers (leading to many steps overall).",
            "reasoning_methods": [
                "Tree-structured, multi-branch exploration (explicit branching at each step)"
            ],
            "reasoning_methods_description": "At each layer, multiple branches are generated and expanded to form a tree of candidate reasoning trajectories; final answer chosen from leaf evaluations.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Used as a baseline tree-structured method (3 branches/step, 5 layers) and compared with Entro-duction; ToT required many more steps (often &gt;100) in experiments.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA, CSQA",
            "performance_results": "Reported accuracies sometimes comparable but at much higher step counts (e.g., ToT reported requiring large numbers of steps, e.g., &gt;100). Example accuracy on GSM8K ~72.6% with ~121 steps in table.",
            "qualitative_findings": "Tree search covers broader solution space and can find complex solutions, but is computationally expensive; Entro-duction seeks to capture branching benefits (Expand) with much lower step cost.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Branched exploration helps when multiple plausible solution paths exist, but costly; guided selective branching (entropy-driven Expand) recovers benefits more efficiently.",
            "uuid": "e8304.3",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Complex CoT",
            "name_full": "Complex Chain-of-Thought",
            "brief_description": "A CoT variant that engages with complex samples and selects the best solution from multiple intricate reasoning paths.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (as evaluated with Complex CoT)",
            "model_description": "Same 8B Llama model; Complex CoT configured for longer/more intricate chains (settings per Zhou et al. referenced).",
            "reasoning_methods": [
                "Structured/complex chain-of-thought that attempts to capture multi-faceted reasoning within CoT-style outputs"
            ],
            "reasoning_methods_description": "Generate more detailed and/or structured CoT chains and select best answer among them; intended to handle multifaceted reasoning.",
            "reasoning_diversity": "similar/structured",
            "reasoning_diversity_experimental_setup": "Compared as a baseline against Entro-duction and other methods using fixed-step complex CoT configurations.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA, CSQA",
            "performance_results": "Reported example GSM8K accuracy ~81.4% (with ~8 steps), generally better than vanilla CoT but below Entro-duction on some datasets.",
            "qualitative_findings": "Handles multifaceted problems better than simple CoT but still constrained by preset structure; lacks dynamic depth adjustment based on model uncertainty.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Complex CoT improves over basic CoT on harder problems, but entropy-guided dynamic strategies can further improve accuracy while controlling step cost.",
            "uuid": "e8304.4",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-talk",
            "name_full": "Self-talk (exploratory questioning)",
            "brief_description": "A method where the LLM generates exploratory questions (self-talk) to elicit implicit background knowledge and use that to improve final answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (as evaluated with Self-talk)",
            "model_description": "Same base Llama model; Self-talk procedure elicits auxiliary questions/knowledge related to the main problem and uses them to form responses.",
            "reasoning_methods": [
                "Exploratory self-questioning to reveal implicit knowledge",
                "Selection among generated responses"
            ],
            "reasoning_methods_description": "The model generates intermediate probing questions or subreasoning outputs to surface relevant knowledge; selects best answer from resulting outputs.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared as a reasoning-depth optimization baseline (no additional training) against Entro-duction and other baselines.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA, CSQA",
            "performance_results": "Reported accuracies approx.: GSM8K 79.1%, SVAMP 83.7%, StrategyQA 61.5%, CSQA 70.0%; generally below Entro-duction on several datasets.",
            "qualitative_findings": "Helps uncover background knowledge and can improve answers without extra training, but may be less effective than entropy-guided exploration in balancing depth and coverage.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-talk improves reasoning by surfacing implicit context but is less consistent than Entro-duction's targeted exploration; Entro-duction attains higher accuracy without extra model training.",
            "uuid": "e8304.5",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DRR",
            "name_full": "Distillation-Reinforcement-Reasoning (DRR)",
            "brief_description": "A depth-optimization method that distills LLM reasoning into synthetic data and trains a lightweight model to provide feedback for reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct (as evaluated where DRR was used for comparison)",
            "model_description": "DRR is a multi-step pipeline (distillation into data, train a small feedback model) used as a baseline for depth optimization comparisons.",
            "reasoning_methods": [
                "Distillation of reasoning traces and learned feedback to guide depth/steps"
            ],
            "reasoning_methods_description": "LLM outputs are distilled into synthetic training data used to train a lightweight model that provides feedback (e.g., when to stop or refine) to the LLM's reasoning process.",
            "reasoning_diversity": "both (method optimizes depth but can enable varied strategies via feedback)",
            "reasoning_diversity_experimental_setup": "Compared to Entro-duction; DRR requires additional training and separate model components while Entro-duction operates online without retraining.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA, CSQA",
            "performance_results": "Reported accuracies approx.: GSM8K 83.0%, SVAMP 90.2%, StrategyQA 67.7%, CSQA 82.1%; competitive on some datasets (SVAMP, CSQA) but overall trailing Entro-duction on GSM8K and StrategyQA.",
            "qualitative_findings": "DRR benefits from offline training and distilled feedback but entails extra resource/training cost; Entro-duction claims similar or better accuracy in some cases with lower runtime overhead.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "DRR can achieve strong results but requires auxiliary training; Entro-duction provides a lightweight, training-free alternative that dynamically adjusts depth via entropy signals.",
            "uuid": "e8304.6",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Llama-3.1-8B-Instruct",
            "name_full": "Meta Llama-3.1 8B Instruct-tuned model",
            "brief_description": "The specific language model used as the reasoner for all experimental comparisons in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_description": "An instruct-tuned 8-billion parameter Llama-3.1 model (publicly available checkpoint referenced). Experiments used temperature 0.7 and max token limit 128 on an NVIDIA 4090 GPU.",
            "reasoning_methods": [
                "Used with various prompting/structuring methods: CoT, CoT-SC, ToT, Complex CoT, Self-talk, DRR, and Entro-duction"
            ],
            "reasoning_methods_description": "The same model is driven by different prompting/structuring or algorithmic wrappers: single-chain CoT, multi-sample self-consistency, tree search, complexity-focused CoT, exploratory self-talk, distilled feedback pipelines, and entropy-guided dynamic exploration (Entro-duction).",
            "reasoning_diversity": "both (experiments evaluate both single-chain and multi-branch/diverse strategies)",
            "reasoning_diversity_experimental_setup": "Baseline and proposed methods are evaluated on four datasets to compare single-chain vs multi-chain vs tree vs entropy-guided adaptive depth strategies.",
            "task_or_benchmark": "GSM8K, SVAMP, StrategyQA, CommonsenseQA",
            "performance_results": "See method-specific entries; Entro-duction with this model obtains best reported accuracy across most evaluated tasks in the paper.",
            "qualitative_findings": "The Llama-3.1-8B-Instruct model benefits from dynamic, entropy-driven depth control to avoid over- or under-reasoning compared to fixed-step CoT or exhaustive ToT.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "When used with Entro-duction, the Llama-3.1-8B-Instruct model attains higher accuracy with fewer or comparable steps than many baselines, indicating the effectiveness of entropy-guided dynamic exploration.",
            "uuid": "e8304.7",
            "source_info": {
                "paper_title": "Entropy-based Exploration Conduction for Multi-step Reasoning",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Unsupervised commonsense question answering with selftalk",
            "rating": 1,
            "sanitized_title": "unsupervised_commonsense_question_answering_with_selftalk"
        },
        {
            "paper_title": "Distilling system 2 into system 1",
            "rating": 1,
            "sanitized_title": "distilling_system_2_into_system_1"
        },
        {
            "paper_title": "Detecting hallucinations in large language models using semantic entropy",
            "rating": 2,
            "sanitized_title": "detecting_hallucinations_in_large_language_models_using_semantic_entropy"
        }
    ],
    "cost": 0.0163315,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Entropy-based Exploration Conduction for Multi-step Reasoning</p>
<p>Jinghan Zhang jinghanz@pdx.edu 
Portland State University</p>
<p>Xiting Wang xitingwang@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China Beijing
China</p>
<p>Corresponding Author. Beijing Key Laboratory of Re-search on Large Models and Intelligent Governance. Engineer-ing Research Center of Next-Generation Intelligent Search and Recommendation
MOE</p>
<p>Fengran Mo fengran.mo@umontreal.ca 
University of Montreal
4 Uber</p>
<p>Yeyang Zhou yeyang.zhou@uber.com 
Wanfu Gao 
Jilin University</p>
<p>Kunpeng Liu kunpeng@pdx.edu 
Portland State University</p>
<p>Entropy-based Exploration Conduction for Multi-step Reasoning
F71E58243033AA6150FF2469B7019347
Multi-step processes via large language models (LLMs) have proven effective for solving complex reasoning tasks.However, the depth of exploration of the reasoning procedure can significantly affect the task performance.Existing methods to automatically decide the depth often lead to high cost and a lack of flexibility.To address these issues, we propose Entropybased Exploration Depth Conduction (Entroduction), a novel method that dynamically adjusts the exploration depth during multi-step reasoning by monitoring LLM's output entropy and variance entropy.We employ these two features to capture the model's uncertainty of the current step and the fluctuation of uncertainty across consecutive reasoning steps.Based on the observed entropy changes, the LLM selects whether to deepen, expand, or stop exploration according to the probability, which facilitates the trade-off between the reasoning accuracy and exploration effectiveness.Experimental results across four benchmark datasets demonstrate the efficacy of Entro-duction.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated remarkable reasoning capabilities across various domains (Brown et al., 2020;Touvron et al., 2023;Patterson et al., 2022;Fu et al., 2022;Wei et al., 2022).However, they would still encounter challenges in generating accurate and effective multi-step reasoning in terms of complex downstream tasks.Specifically, LLMs may exhibit overreasoning or under-reasoning, which both imply the depth of exploration for a given problem does not match expectations (Ahn et al., 2024;Mirzadeh et al., 2024;Huang and Chang, 2022;Fu et al., 2023).This mismatch issue of reasoning path could lead to several issues: (1) inaccurate, insufficient, or redundant reasoning outcomes; (2) unnecessary computation costs (Yeo et al., 2025;Yang et al., 2024;Lightman et al., 2023).These issues might be attributed to two aspects: i) the lack of evaluation and regulatory mechanisms for LLMs' reasoning process; ii) there are significant variations in the reasoning process required for different tasks, and LLMs might not be unable to accurately judge and adjust the depth of exploration for a task solely based on their parametric knowledge during pretraining.Existing methods for optimizing the exploration depth of multi-step reasoning in LLMs can be categorized into two types: outcome-based optimiza-tion and process-based optimization.Outcomebased optimization aligns the LLMs' reasoning exploration with human expectation after generating a complete reasoning path and approaching the final conclusions (Jin et al., 2024;Liu et al., 2024a;Ton et al., 2024;Yu et al., 2024).These approaches rely on post-training techniques of LLMs, which are resource-intensive and do not provide an immediate improvement on the current task.Further, due to the diversity of reasoning tasks, the enhancement gain is task-specific.In process-based optimization, the LLMs supervise and evaluate their reasoning process through self-critique or by labeling reasoning steps to enhance outputs (Ma et al., 2025;Yang et al., 2024;Pan et al., 2023).The advantages of the process-based optimization methods are their immediacy and low cost.However, the LLMs should have high reasoning capabilities and a substantial knowledge base.Moreover, since the process is usually opaque, it is difficult for humans to effectively provide supervision signals and interpret the optimization processes.The model's illusions, biases, and errors may be reinforced during this process (Stechly et al., 2023(Stechly et al., , 2024b;;Liang et al., 2024b;Song et al., 2025).</p>
<p>Our Target.Given the existing challenges, we aim to develop a method that guides LLMs to automatically, concisely, and transparently determine the appropriate depth of exploration based on task information and the model's reasoning state.The goal is to enable the model to look ahead during reasoning, plan dynamically, and decide whether further exploration is necessary to achieve optimal reasoning outcomes.The whole procedure involves enhancing multi-step reasoning performance and reducing unnecessary exploration.</p>
<p>Our Method.To tackle these challenges, we propose Entropy-based Exploration Depth Conduction (Entro-duction), a novel method to help LLMs assess the adequacy of exploration during multi-step reasoning processes, thus enhancing the outcomes of reasoning.Inspired by Entropy Uncertainty Measurement (Coles et al., 2017;Farquhar et al., 2024;Zhang et al., 2024a;Rosenfeld et al., 1996), we employ entropy changes in the LLM's reasoning process to evaluate its uncertainty of reasoning, which reflects the reasoning confidence, and accordingly switch exploration strategy.Specifically, we use entropy and variance entropy as rewards to update the LLM's probability distribution for its next exploratory action, whether to deepen, stop, or expand exploration.This distribution subsequently guides a behavior selection mechanism that promotes reasoning when exploration is insufficient and avoids redundant reasoning when it is adequate.</p>
<p>In summary, our contributions involve:</p>
<ol>
<li>
<p>We propose Entro-duction to help LLMs dynamically evaluate the adequacy of exploration based on their reasoning uncertainty to enhance reasoning performance and avoid unnecessary exploration.</p>
</li>
<li>
<p>We further design an entropy-based exploration behavior selection mechanism, which refers to LLMs' expectancy and confidence in the reasoning procedure.</p>
</li>
<li>
<p>We conduct a series of experiments to demonstrate the effectiveness of Entro-duction on various reasoning tasks.Our results and analysis show that the Entro-duction outperforms baseline methods.</p>
</li>
</ol>
<p>Related Work</p>
<p>Reasoning Steps and Structures.When responding to queries, LLMs typically provide direct outputs.For complex questions, direct outputs often fail to deliver correct answers because they may overlook deeper logical connections or contextual information (Xia et al., 2024;Minaee et al., 2024).Multi-step reasoning involves instructing LLMs to decompose and progressively address problems, breaking down complex tasks into smaller, manageable units to significantly enhance reasoning capabilities (Chu et al., 2023).The simplest structure of multi-step reasoning is the Chain of Thought (CoT) (Wei et al., 2022;Wang and Zhou, 2024), which links reasoning steps by connecting distinct thoughts into a linear, coherent sequence (Li et al., 2024;Jin and Lu, 2024;Sprague et al., 2024).</p>
<p>To enable more comprehensive exploratory reasoning, some studies based on CoT have developed structured reasoning methods, such as selfconsistent CoT (CoT-SC), Complex CoT, and Treeof-Thought (ToT) (Wang et al., 2022;Zhang et al., 2024c;Yao et al., 2024;Mo et al., 2024;Liu et al., 2024b;Mo and Xin, 2024;Zhang et al., 2024b).These methods are called reasoning structures.They guide LLMs to do multi-directional exploration of problem solution spaces for superior reasoning solutions by capturing more complex and varied logical relationships (Xia et al., 2024;Stechly et al., 2024a;Mo et al., 2023;Liang et al., 2024a).However, the breadth and depth of these reasoning structures highly depend on predefined settings and vary greatly across different tasks, limiting their generalizability and flexibility.</p>
<p>Optimization of Reasoning Depth.The depth of reasoning structures refers to the number of layers or steps in the reasoning process, namely, the number of reasoning steps an LLM must undertake before reaching a final answer (Plaat et al., 2024;Gomez, 2023).For any given task, the optimal number of reasoning layers often correlates with the task's complexity and the level of detail required (Zhang et al., 2024c).Current methods in determining these layers or optimize reasoning structures automatically.These methods include using reinforcement learning algorithms to optimize the number of reasoning steps or dynamically adjusting the depth of exploration during the reasoning process (Jin et al., 2024;Liu et al., 2024a;Hoffmann et al., 2022).</p>
<p>The main issues of these methods include: (1) the automated algorithms may lack precision due to the lack of precision; (2) making dynamic adjustments without fully understanding task characteristics could harm the reasoning process; (3) for highly complex or novel tasks, preset reasoning structures may be inappropriate, and could limit the model's applicability and flexibility.These issues together ultimately affect the LLM's reasoning reliability and efficiency, and lead to inaccurate reasoning outcomes or redundant exploration.</p>
<p>Methodology</p>
<p>Problem Formulation</p>
<p>Given a reasoning task and an LLM R as the reasoner, the multi-step reasoning process is to generate a reasoning structure S. Structure S comprises directed links connecting sentence-level reasoning nodes.A reasoning chain is a unidirectional sequence that begins at an initial reasoning node and concludes at a terminal node:
C i = T i1 → T i2 → • • • → T i,j → • • • , i = 1, ..., m,(1)
where T i,j is the j-th node in the i-th chain,m is the total number of chains in S. The chain length |C i | is the total number of nodes within C i .We define the depth S d of S as the maximum length of any reasoning chain within the structure:
S d = max{|C i |}.
(2)</p>
<p>Since S could contain several valid reasoning chains, the reasoning conclusion L is made through a voting mechanism V :
L = V (S, R).
(3)</p>
<p>In this paper, our goal is to generate a structure S such that it achieves optimal reasoning accuracy, denoted as Acc(L), with an optimal depth S d .</p>
<p>Reasoning State Evaluation</p>
<p>The essence of the multi-step reasoning process is to explore the solution space of task Q.Our exploration goal is to cover as many potential reasoning paths as possible to ensure the accuracy and completeness of the solution.However, exhaustive exploration is inefficient and often impractical.As the problem's complexity grows, the solution space expands exponentially, driving the computational and time costs to untenable levels.Consequently, we must balance the breadth and depth of exploration.Achieving this balance calls for a method that can look forward from each current reasoning state, predicting and adjusting subsequent exploration steps, so that we do not miss crucial paths or waste resources on unnecessary ones.We employ uncertainty and stability to describe the reasoning state.Uncertainty measures the divergence of current thought processes, as shown in Figure 2. In a reasoning step, a high uncertainty indicates the presence of multiple possible directions or conclusions, this means a wide scope of exploration is necessary.Conversely, low uncertainty, where there are few or even a single possible outcome, indicates a more focused path.Specifically, we employ entropy as a metric for uncertainty to quantify the number of potential paths that need exploration at any given moment and to gauge the confidence level in the conclusions.</p>
<p>Definition 1: Entropy.Consider a reasoning step represented by a sentence, denoted as T i,j , which consists of a sequence of n tokens:
T i,j = {t ij1 , t ij2 , . . . , t ijn }.
(4)</p>
<p>Each token t ijk matches a logit l ijk , which is the model's raw output before the softmax function.</p>
<p>The collection of logits for the entire sentence is:
l ij = {l ij1 , l ij2 , . . . , l ijn }.(5)
We calculate the probability p ijk of each token t ijk by applying the softmax function to its corre- sponding logit l ijk :
p ijk = exp(l ijk ) n r=1 exp(l ijr ) . (6)
Then the entropy of the sentence T i,j is:
H(T i,j ) = − n k=1 p ijk log 2 (p ijk ).(7)
This measures the uncertainty or information content encoded in the probability distribution {p ij1 , p ij2 , . . ., p ijn }.</p>
<p>To compare the entropies across reasoning steps of varying lengths, we define the normalized entropy as:
H(T i,j ) = H(T i,j ) log 2 (n) .(8)
Here, log 2 (n) is the maximum possible entropy when all n tokens have uniform probability.Hence, the normalized entropy is bounded between 0 and 1 for consistent comparisons.</p>
<p>Similarly, we employ variance entropy to capture how much uncertainty fluctuates across consecutive reasoning steps.It indicates the consistency or divergence of the thought process.</p>
<p>Definition 2: Variance Entropy.For reasoning step T i,j of length n, let:
H(T i,j ) = 1 n n k=1 H(t ijk ),(9)
be the average token-level entropy in T i,j .We define the variance entropy as:
σ 2 H (T i,j ) = 1 n n k=1 H(t ijk ) − H(T i,j ) 2 . (10)
For comparisons, we define the normalized variance entropy:
σ 2 H (T i,j ) = σ 2 H (T i,j ) log 2 (n) . (11)
In this way, we have a normalized concise metric for tracking fluctuations in uncertainty within each reasoning step.</p>
<p>Exploration Behaviors</p>
<p>With two metrics for reasoning state defined above, we further consider how to use changes in these metrics to determine exploration behavior strategies.The possible scenarios are listed below:</p>
<ol>
<li>Entropy ↑ , Variance Entropy ↑: The reasoning process becomes simultaneously more complex and more unstable.This indicates that current exploration might have strategic deviations, but another possibility is that the model is exploring a new or more challenging direction.We need to consider avoiding ineffective exploration while maintaining the potential for the model to tackle challenging problems.</li>
</ol>
<p>Accordingly, we design a mechanism that adjusts the probability of exploration behaviors based on entropy and variance entropy changes.We define the exploration behaviors as follows:</p>
<p>Deepen: This behavior extends the current reasoning chain C i by adding a new node T i,j+1 :
C i → C i ∪ {T i,j+1 }. (12)
Expand: This behavior divides the current reasoning chain at T i,j , creates two separate chains C i and C ′ i .Each chain extending from the split point generates a new node:
C i → (C i ∪ {T i,j+1 }) , C ′ i → C i ∪ {T ′ i,j+1 } .
(13) Stop: This behavior terminates the extension of the current chain C i at the current node T i,j :
C i → C i \ {T i,j+1 }. (14)</p>
<p>Behavior Selection Mechanism</p>
<p>At the j-th reasoning step, we define:
∆H j = H(T j+1 ) − H(T j ),(15)∆σ 2 H,j = σ 2 H (T j+1 ) − σ 2 H (T j ),(16)
which denotes the changes in entropy and variance entropy, respectively.We define the state:
s j = ∆H j , ∆σ 2 H,j ,(17)
and the set of possible actions as
A = {Deepen, Expand, Stop}.(18)
We introduce a mapping function Φ : R 2 → A that assigns to each state (∆H j , ∆σ 2 H,j ) a "best" action a * j :
Φ(∆H, ∆σ 2 H ) =            Deepen, if (∆H &lt; 0, ∆σ 2 H &lt; 0) or (∆H &gt; 0, ∆σ 2 H &lt; 0), Expand, if ∆H &lt; 0, ∆σ 2 H &gt; 0, Stop, if ∆H &gt; 0, ∆σ 2 H &gt; 0. (19)
Algorithm 1 Entro-duction if all chains stopped or j = J then j ← j + 1 16: end while 17: L ← V (S, R) (final conclusion via consensus).18: return S, L Then, at each step j, we sample the actual action a j according to an ϵ-greedy rule:
π j a | s j =    1 − ϵ, a = a * j , ϵ |A| − 1 , a ̸ = a * j .(20)
Given the current state s j = (∆H j , ∆σ 2 H,j ), we first compute a * j = Φ(s j ), then draw a j from π j (• | s j ).If a j = Stop, the reasoning ends; otherwise, the system transitions to the next state s j+1 , where the new entropy measures yield an updated state.</p>
<p>Experiments</p>
<p>In this section, we first compare the reasoning performance and reasoning steps used between baseline methods and Entro-duction.Subsequently, we present ablation studies to analyze the contributions of each part of our strategies.Following this, we examine how different parameter settings impact Entro-duction's overall robustness.</p>
<p>Experiment Settings</p>
<p>Datasets.Entro-duction is a general approach applicable to various LLMs and reasoning tasks.Here, we test across four reasoning tasks with benchmark datasets, including two mathematical tasks (GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021)) and two commonsense questionanswering tasks (StrategyQA (Geva et al., 2021), CommonsenseQA (Talmor et al., 2018)).Here, GSM8K challenges language models with multistep math reasoning tasks, assessing their complex reasoning capabilities, while SVAMP focuses on simpler, one-step math reasoning tasks.Strate-gyQA tests strategic reasoning skills for deriving implicit strategies and using deductive reasoning to answer questions.CommonsenseQA (CSQA) tests the ability to handle commonsense reasoning with everyday knowledge.In evaluating performance on these datasets, we primarily focus on reasoning accuracy (%) as the key metric.</p>
<p>Baselines.We compare Entro-duction with two strong baseline types: (1) Reasoning structures, including Chain of Thought (CoT), Chain of Thought with Self-Consistency (CoT-SC), Tree of Thought (ToT) and Complex CoT:</p>
<p>• CoT: Guides the model to solve problems stepby-step and generates a coherent reasoning chain that leads to a conclusion.</p>
<p>• CoT-SC: Generates multiple reasoning chains and uses a majority vote to determine the final output.We sample answer 8 (CoT-SC@maj8) and 64 (CoT-SC@maj64 ) times to employ majority vote for selection.</p>
<p>• ToT: Expands the reasoning process into a tree-like structure where multiple branches represent different reasoning pathways.</p>
<p>• Complex CoT: Engages with complex samples and selects the best solution from various intricate reasoning paths for tackling multifaceted and challenging problems.and (2) Reasoning depth optimization methods, including Self-talk (Shwartz et al., 2020;Molfese et al., 2024) and Distillation-Reinforcement-Reasoning (DRR) (Yang et al., 2024):</p>
<p>• Self-talk: Enhances reasoning by eliciting LLMs to generate exploratory questions, uncovering implicit background knowledge and selecting the best answer.</p>
<p>• DRR: Distills LLM reasoning processes into synthetic data by training a lightweight model to provide feedback.</p>
<p>For detailed settings of baselines, please refer to Appendix A.</p>
<p>Implementation Details.We conduct the experiments utilizing the Llama-3.1-8B-Instruct1 .</p>
<p>The temperature for all models is set to the default value of 0.7, with a maximum token limit of 128.</p>
<p>All tasks are performed on an NVIDIA 4090 GPU.</p>
<p>Overall Performance</p>
<p>The overall performance is reported in Table 1.For the baselines, we compare reasoning accuracies across four datasets, and for reasoning structures, we additionally measure the number of steps required.Since each structure's steps and branches are predefined, we adopt configurations that can perform well and that further increasing the step count often does not bring significant gains.Specifically, for math tasks, CoT is set to 8 steps, CoT-SC adopts 3 parallel chains of 8 steps each, and ToT generates three branches per step for five layers.For commonsense tasks, CoT is set to 5 steps, CoT-SC adopts 3 parallel chains of 5 steps each, and ToT generates three branches per step for five layers.Compared to reasoning structures, our Entroduction approach achieves both higher accuracy and fewer reasoning steps.For instance, on GSM8K, CoT reaches 0.75 accuracy, CoT-SC@maj64 0.80, and Complex CoT 0.81, while Entro-duction attains 0.85.A similar advantage appears on SVAMP (up to 0.92), and on the commonsense tasks StrategyQA and CSQA, Entroduction scores 0.70 and 0.79 respectively, surpassing most baselines.Moreover, tree-structured methods ToT require hundreds of steps (over 100 in several cases), while Entro-duction needs much fewer steps, only more than CoT.Even compared to fixedstep approaches such as CoT and Complex CoT, Entro-duction delivers higher accuracy in a similar or slightly increased number of steps.</p>
<p>Compared to reasoning depth optimization methods, Entro-duction consistently attains higher per- To validate the necessity of jointly using entropy and variance entropy, we conduct experiments across four datasets to validate the necessity of jointly using entropy and variance entropy.We set four different scenarios: Base (neither used), Entropy (only entropy used), Variance (only variance entropy used), and Both (both used).As shown in Figure 3, when using only entropy, the model tends to stop reasoning prematurely in scenarios with many potential outcomes but fewer overall fluctuations (Scenario 2).Using only variance entropy can capture changes in fluctuations between reasoning steps.It slightly outperforms using entropy alone, but still proves inadequate for handling various uncertainty scenarios Scenario 3), with accuracies mostly close to or below Base.</p>
<p>Impact of Expansion in Reasoning</p>
<p>Compared with Deepen and Stop that directly affect reasoning depth, Expand is a key behavior in Entro-duction to branch out the current reasoning path to cover more potential solutions.We further validate the necessity of the behavior Expand by As shown in Figure 4, the accuracies of w/o are generally lower than those of w/, particularly in tasks requiring multiple reasoning paths or branching thought processes, such as SVAMP and Strate-gyQA.The result indicates that using only Deepen and Stop limits the exploration of potential directions, while expanding the exploration contributes to improving the completeness of the reasoning.</p>
<p>Impact of Soft Stop</p>
<p>In some complex tasks, we notice that the initial reasoning process could see an increase in both entropy and variance entropy.However, the model may still expect valid exploration in the continued reasoning.In this case, if we adopt a "hard stop", which means immediate shutdown, it could terminate exploration in advance of arriving at the correct conclusion.Instead, we introduce a "soft stop" mechanism to balance the need for thorough exploration and the risk of redundant reasoning.</p>
<p>The model continues for several additional steps before stopping.In our experiments, we implement four settings: Base (no stopping strategy), Stop@1 (hard stop with immediate termination), Stop@2 (soft stop with one more reasoning step before stopping) and Stop@3 (soft stop with two more reasoning steps before stopping).As shown in Figure 5, we can see that the hard stop strategy has the lowest outcomes across all four datasets, lower than not employing any stopping strategy.The soft stop strategy consistently has the best results.Moreover, on datasets SVAMP and CSQA, Stop@2 performs as well or better than Stop@3.This result suggests that a soft stop extending two to three steps is sufficient to complete effective exploration without the need for extensive further reasoning.</p>
<p>Robustness Study</p>
<p>We further discuss the impact of ϵ-greedy strategy by testing four ϵ values (0.05, 0.1, 0.25, 0.5).As shown in Figure 6, ϵ = 0.25 consistently achieves the highest accuracy across all four datasets.This result demonstrates that this value enhances the model's performance by effectively exploring the solution space.Specifically, when ϵ is set too low (ϵ = 0.05), the model's performance is poor.This is likely due to insufficient exploration that relies heavily on the known strategy, thus unable to explore potential solutions hidden in the space.Meanwhile, when ϵ = 0.5, it outperforms ϵ = 0.1 in mathematical tasks and underperforms in commonsense tasks.This result indicates that tasks requiring reasoning with stringent logical structure and relatively more steps need broader exploration to identify the correct solutions.For commonsense tasks, which require more precise adopting of knowledge for quick decision-making.In this type of task, over-exploration may lead the model away from the question background and thus miss the intuitive commonsense answers.</p>
<p>Conclusion</p>
<p>In this study, we introduce Entro-duction, a novel approach that dynamically adjusts the exploration depth during LLM multi-step reasoning by monitoring the entropy and variance entropy.Entroduction leverages the change of both metrics to select exploration behavior to enhance reasoning performance and avoid redundant reasoning steps.Our experiments across multiple reasoning datasets demonstrate the effectiveness of the Entro-duction and its components.</p>
<p>Limitations</p>
<p>We develop a framework with dynamic depth adjustment strategies for LLMs.If not precisely calibrated, it might lead to suboptimal reasoning performance.This may limit the Entro-duction method's ability to adaptively balance exploration and exploitation in real-time.Besides, the experiments are conducted on four benchmark datasets on one Llama model, which may not provide a comprehensive view of the Entro-duction's generalization capability across LLMs with varying sizes and pretraining processes.Moreover, the Entro-duction is mainly evaluated on specific tasks and these tasks cannot fully reflect the complexities of real-world scenarios where reasoning tasks can be variable and with more complex and external solution spaces.We left these potential explorations as future work.</p>
<p>Figure 1 :
1
Figure 1: Reasoning depth mismatching solution space.</p>
<p>Figure 2 :
2
Figure2: Framework of Entro-duction.We obtain two metrics, entropy and variance entropy, by calculating the probabilities of the logits at each reasoning step.Subsequently, we employ the epsilon-greedy method to select the appropriate exploration behavior based on changes in both metrics.</p>
<p>Figure 3 :
3
Figure 3: Comparison of adjusting with entropy and/or variance entropy.</p>
<p>Figure 4 :
4
Figure 4: Impact of the behavior Expand.</p>
<p>Figure 5 :
5
Figure 5: Comparison of stopping strategies.</p>
<p>Figure 6 :
6
Figure 6: Comparison of choice of ϵ.</p>
<p>Reasoning structure S and conclusion L. 3: Initialize S; set j ← 1; initialize chains {C i }.Sample action a j with probability π j (a | s j ) according to Eq. 20).Deepen: append T j+1 to C i .ii)Expand: branch C i into two chains with T j+1 and T ′ j+1 .iii) Stop: finalize C i (no further expansion).
4: while j ≤ J do5:for each active chain C i do6:Compute H(T j ) and σ 2 H (T j ) accord-ing to Eqs. 7, 10.7:Compute ∆H j and ∆σ 2 H,j accordingto Eqs. 15, 16).8:Determine a  *  j ← Φ(∆H j , ∆σ 2 H,j ) ac-cording to Eq. 19.9:10:Execute a j :i) 11: end for12:
1: Input: Reasoning task Q; LLM reasoner R; max steps J; exploration rate ϵ.2: Output:</p>
<p>Table 1 :
1
Performance comparison across different reasoning methods with accuracy and number of steps.
MathCommonsenseMethodGSM8KSVAMPStrategyQACSQAAccuracy # Steps Accuracy # Steps Accuracy # Steps Accuracy # StepsCoT75.28.083.48.057.75.075.65.0CoT-SC@maj878.124.087.524.068.315.078.215.0CoT-SC@maj6480.224.089.624.067.115.078.715.0ToT72.6121.083.3121.065.8121.073.5121.0Complex CoT81.48.086.28.065.75.073.95.0Self-talk79.1/83.7/61.5/70.0/DRR83.0/90.2/67.7/82.1/Entro-duction85.49.592.011.2070.39.679.67.1formance. Self-talk achieves accuracies of 0.79,0.61, and 0.70 on GSM8K, StrategyQA, andCSQA, all below Entro-duction 's 0.85, 0.70, and0.79. DRR demonstrates decent performance onSVAMP (0.90) and CSQA (0.82), but still trailsEntro-duction on GSM8K (0.83 vs. 0.85) and Strat-egyQA (0.67 vs. 0.70). Moreover, these methodsoften rely on additional training or separate mod-els, while Entro-duction balances accuracy and arelatively low reasoning overhead without training.4.3 Ablation Study4.3.1 Impact of Jointly Using Entropy andVariance Entropy
. Entropy ↓ , Variance Entropy ↓: The reasoning step becomes more certain, and the overall thought process more coherent. This indicates that information is becoming more focused, and the reasoning process is stable and effective. The LLM should continue to explore in this direction
.2. Entropy ↑ , Variance Entropy ↓: The reasoning step introduces more uncertainty, but the fluctuations between different steps are decreasing. This suggests that while a broader range of possibilities has emerged, the overall direction has not become dispersed. The LLM should continue to explore in this
direction. 3. Entropy ↓ , Variance Entropy ↑: The uncertainty of reasoning is decreasing, but the fluctuation between reasoning steps is increasing. This indicates potential divergences in local steps, and we should consider increasing exploration in different directions to cover possible solutions.
https://huggingface.co/meta-llama/Llama-3. 1-8B-Instruct
Acknowledgements Dr. Xiting Wang is supported by the National Natural Science Foundation of China (NSFC) (NO.62476279, NO. 92470205), Major Innovation &amp; Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China, the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China No. 24XNKJ18.Dr. Xiting Wang is supported by fund for building world-class universities (disciplines) of Renmin University of China and Public Computing Cloud, Renmin University of China.Appendix A Experimental SettingsA.1 BaselinesIn CoT, we prompt the model with the sentense "Let's think step-by-step.".For CoT-SC, we use a majority vote to identify the most probable correct solution, with the same few-shot examples as the standard CoT method.For Complex CoT we follow the setting in(Zhou et al., 2022).The setting of Self-talk and DRR method follows the setting in(Yang et al., 2024).A.2 Answer-CleaningWe showcase our answer-cleaning process with GSM8K as an example.In the context of the Algorithm 2 Answer Cleansing for GSM8K Dataset 1: Input: pred ▷ Raw prediction from the model 2: Output: cleansed_pred ▷ Cleansed numerical prediction 3: Remove commas from pred 4: Extract all numbers from pred using regex 5: Select the first or last number based on context 6: return cleansed_pred GSM8K dataset, the answer-cleansing process is crucial for ensuring the accuracy and usability of predictions from large language models.Initially, the raw prediction, referred to as pred, often contains numerical answers formatted with commas or mixed with textual content.To standardize these predictions, we first remove any commas to normalize the numbers.Subsequently, we use regular expressions to extract all numerical values from this cleaned string.Given the nature of GSM8K tasks, where a specific numerical answer is typically required, our algorithm strategically selects either the first or last number based on predefined logic tailored to the dataset's requirements.This selection process is designed to pick the most relevant number based on its position in the model's output.The final step produces a cleansed_pred, which is the processed and formatted numerical answer ready for evaluation against the dataset's ground truth.
Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, arXiv:2402.00157Large language models for mathematical reasoning: Progresses and challenges. 2024arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, arXiv:2309.15402A survey of chain of thought reasoning: Advances, frontiers and future. 2023arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Entropic uncertainty relations and their applications. Mario Patrick J Coles, Marco Berta, Stephanie Tomamichel, Wehner, Reviews of Modern Physics. 891150022017</p>
<p>Detecting hallucinations in large language models using semantic entropy. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal, Nature. 63080172024</p>
<p>Specializing smaller language models towards multi-step reasoning. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot, International Conference on Machine Learning. PMLR2023</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Kye Gomez, Tree of thoughts. 2023</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, arXiv:2401.04925The impact of reasoning step length on large language models. 2024arXiv preprint</p>
<p>Self-harmonized chain of thought. Ziqi Jin, Wei Lu, arXiv:2409.040572024arXiv preprint</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma, arXiv:2402.128752024arXiv preprint</p>
<p>Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong, Zhiyu Li, arXiv:2407.14507Internal consistency and selffeedback in large language models: A survey. 2024aarXiv preprint</p>
<p>Learning to trust your feelings: Leveraging self-awareness in llms for hallucination mitigation. Yuxin Liang, Zhuoyang Song, Hao Wang, Jiaxing Zhang, arXiv:2401.154492024barXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJanarXiv preprint</p>
<p>Can language models learn to skip steps?. Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, Zheng Zhang, arXiv:2411.018552024aPreprint</p>
<p>Logic-of-thought: Injecting logic into contexts for full reasoning in large language models. Tongxuan Liu, Wenjiang Xu, Weizhe Huang, Xingyu Wang, Jiaxing Wang, Hailong Yang, Jing Li, arXiv:2409.175392024barXiv preprint</p>
<p>Cot-valve: Lengthcompressible chain-of-thought tuning. Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, Xinchao Wang, arXiv:2502.096012025Preprint</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.06196Large language models: A survey. 2024arXiv preprint</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint</p>
<p>Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, Jian-Yun Nie, arXiv:2410.15576A survey of conversational search. 2024arXiv preprint</p>
<p>Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun Nie, arXiv:2305.15645Convgqr: Generative query reformulation for conversational search. 2023arXiv preprint</p>
<p>Tree of uncertain thoughts reasoning for large language models. Shentong Mo, Miao Xin, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Zebra: Zeroshot example-based retrieval augmentation for commonsense question answering. Francesco Maria Molfese, Simone Conia, Riccardo Orlando, Roberto Navigli, arXiv:2410.050772024arXiv preprint</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , arXiv:2308.031882023Preprint</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, Are nlp models really able to solve simple math word problems? arXiv preprint arXiv. 20212103</p>
<p>The carbon footprint of machine learning training will plateau, then shrink. David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, Maud David R So, Jeff Texier, Dean, Computer. 5572022</p>
<p>Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki Van Stein, Thomas Back, arXiv:2407.11511Reasoning with large language models, a survey. 2024arXiv preprint</p>
<p>A maximum entropy approach to adaptive statistical language modelling. Ronald Rosenfeld, Computer speech and language. 1031871996</p>
<p>Unsupervised commonsense question answering with selftalk. Vered Shwartz, Peter West, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:2004.054832020arXiv preprint</p>
<p>Progco: Program helps self-correction of large language models. Xiaoshuai Song, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, Bo Zheng, arXiv:2501.012642025arXiv preprint</p>
<p>To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett, arXiv:2409.121832024arXiv preprint</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, arXiv:2310.123972023Preprint</p>
<p>Chain of thoughtlessness: An analysis of cot in planning. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, arXiv:2405.047762024aarXiv preprint</p>
<p>On the self-verification limitations of large language models on reasoning and planning tasks. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, arXiv:2402.081152024barXiv preprint</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2018arXiv preprint</p>
<p>Understanding chain-of-thought in llms through information theory. Jean-Francois Ton, Muhammad Faaiz Taufiq, Yang Liu, arXiv:2411.119842024Preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-ofthought reasoning without prompting. Xuezhi Wang, Denny Zhou, arXiv:2402.102002024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Beyond chain-of-thought: A survey of chain-of-x paradigms for llms. Yu Xia, Rui Wang, Xu Liu, Mingyan Li, Tong Yu, Xiang Chen, Julian Mcauley, Shuai Li, arXiv:2404.156762024arXiv preprint</p>
<p>Reinforcing thinking through reasoningenhanced reward models. Diji Yang, Linda Zeng, Kezhen Chen, Yi Zhang, arXiv:2501.014572024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Demystifying long chain-of-thought reasoning in llms. Edward Yeo, Yuxuan Tong, Morry Niu, arXiv:2502.033732025arXiv preprintGraham Neubig, and Xiang Yue</p>
<p>Distilling system 2 into system 1. Ping Yu, Jing Xu, Jason Weston, Ilia Kulikov, arXiv:2407.060232024Preprint</p>
<p>Jinghan Zhang, Xiting Wang, Yiqiao Jin, Changyu Chen, Xinhao Zhang, Kunpeng Liu, arXiv:2406.06606Prototypical reward network for data-efficient rlhf. 2024aarXiv preprint</p>
<p>Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang, Dongjie Wang, Kunpeng Liu, arXiv:2406.02746Ratt: Athought structure for coherent and correct llmreasoning. 2024barXiv preprint</p>
<p>Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao, arXiv:2409.10038On the diagram of thought. 2024carXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
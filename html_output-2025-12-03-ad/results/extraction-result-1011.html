<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1011 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1011</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1011</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-22729745</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1705.06366v2.pdf" target="_blank">Automatic Goal Generation for Reinforcement Learning Agents</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1011.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1011.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ant (quadruped simulated robot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-degree-of-freedom simulated quadruped agent (8 actuated joints) trained with reinforcement learning (TRPO + GAE) to reach parametrized goal positions in a U-shaped maze using an automatically generated curriculum of goals (Goal GAN).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ant (quadruped simulated robot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated quadruped (8 actuated joints) whose policy is trained by reinforcement learning (TRPO with GAE). Policy is goal-conditioned (input includes goal parameters) and trained with goals generated by a Goal GAN and a replay buffer. Sparse binary indicator reward: +1 when goal region reached within T timesteps (episode ends on success).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic agent in Mujoco)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ant Maze (U-shaped maze)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 6 m x 6 m maze centered inside a 10 m x 10 m full state area; feasible goals lie within the interior of the U-shaped maze (two-dimensional goal space: (x,y) CoM positions). Complexity arises from the Ant's high DOF dynamics and motor coordination demands and from the maze geometry (requires navigating a U-turn to reach some goals). Rewards are sparse (indicator of reaching goal within up to 400 timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Agent DOF and dynamics (8 actuated joints); task complexity via maze topology (U-turn requiring nontrivial navigation) and required motor coordination; per-episode horizon (max T = 400 timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation in goal locations across the continuous 2D feasible goal set (uniform over maze interior during evaluation); curriculum causes generated goal distribution to shift over training. No procedural randomization of maze geometry reported.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (many distinct goal positions within the maze interior; environment geometry is fixed across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Coverage objective / average return over all goals (equivalently, average success probability within T timesteps) and per-goal success rate; also qualitative comparison to an oracle (rejection sampling) and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Goal GAN achieves substantially higher coverage than Uniform Sampling and other baselines and is reported to be close to the rejection-sampling oracle in this Ant maze task (no absolute numeric success rates provided in text). The fraction of generated goals that are at the appropriate difficulty level remains around 20% during training.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper emphasizes that high embodied-system complexity (Ant dynamics, high DOF) combined with a fixed but structured feasible goal set makes naive uniform sampling ineffective (many sampled goals yield no reward). Using an adaptive curriculum (Goal GAN) mitigates the difficulty by focusing training on goals of appropriate difficulty; no explicit analytic trade-off is given, but the empirical relationship is: increased agent/task complexity amplifies the need for curriculum/adaptive goal sampling to handle goal-space variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum learning via Goal GAN (adversarial goal generator + discriminator) combined with on-policy RL (TRPO + GAE) and a replay buffer of previously generated goals.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — policies are evaluated on coverage over the full feasible goal set (uniform grid inside maze). The Goal GAN trained policies cover most of the maze (including across the U-turn) and generalize to many goal positions within the feasible region; performance is much better than uniform-sampling baselines and close to an expensive oracle rejection-sampling baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively as more sample-efficient than baselines; training protocol: policy is trained in blocks (Appendix): policy trained for 5 iterations each consisting of 1000 episodes before GAN updates; per-goal empirical return estimates use 3 rollouts for coverage computation; max episode length 400 timesteps. Exact total environment interactions per run are not tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) For a complex embodied agent (Ant) in a maze with sparse binary rewards, Goal GAN yields faster and more efficient learning of coverage across goal-space than uniform sampling and several baselines. 2) The automatically generated curriculum shifts generated goals over training to follow the policy's expanding capability (goals remain at an intermediate difficulty). 3) Goal GAN performance is close to an oracle rejection-sampling upper bound in this domain, indicating that adaptive goal sampling largely overcomes the difficulties introduced by complex agent dynamics and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Goal Generation for Reinforcement Learning Agents', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1011.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1011.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>N-dim Point Mass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-dimensional Point Mass (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated point-mass agent used to study scaling with embedding dimension: the feasible goal region is a low-dimensional subset embedded in an increasing full state-space dimension N; Goal GAN generates feasible goals and maintains coverage as N increases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>N-dimensional Point Mass</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated point-mass whose policy is trained with reinforcement learning (TRPO + GAE). The agent can accelerate in each dimension and receives a sparse indicator reward when within a tolerance of a goal within a 400-timestep episode; the policy is goal-conditioned.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual point-mass)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>N-dimensional Point Mass embedding experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Full state-space is hypercube [-5,5]^N; feasible states are a low-dimensional subset: for N=2 the feasible region is a [−5,5] x [−1,1] strip (20% of full space); for N>2 the feasible region is Cartesian product of that 2D strip with small ranges in other dimensions ([−0.3,0.3]^(N-2)), keeping feasible volume approximately constant while increasing embedding dimension. Reward tolerance scales with sqrt(N) to account for increasing average distances.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State-space dimensionality N; ratio of feasible region volume to full state-space volume (quantified; e.g., drops to 0.00023:1 for N=6). Also agent dynamics simplicity (low DOF point-mass) vs. increased sampling difficulty because feasible region is sparse in higher-dimensional embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies with N; 'low' agent dynamics complexity but environment sampling complexity increases with embedding dimension (N up to at least 6 tested).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Embedding dimension N (procedural variation of ambient space) and consequent decrease in feasible-volume-to-full-volume ratio; explicit numeric ratio provided (e.g., 0.00023:1 for 6 dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>increases with N; for N=2 feasible fraction ~20%, for N up to 6 feasible fraction down to 0.00023 (very high variation/sparsity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Goal coverage (average success over goals) / per-goal expected return (success probability within T timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Goal GAN maintains substantially better coverage than Uniform Sampling as embedding dimension increases; uniform sampling performance decays dramatically with N because feasible fraction decreases. Exact numeric success rates are not provided in the text, but plots (Fig. 5) show Goal GAN performance degrades much less with increasing N compared to baselines; oracle and L2-loss baseline achieve perfect performance in this simple task.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: as embedding dimension N increases, the feasible portion of the state space becomes an increasingly smaller fraction of the full space (variation increases), so uniform sampling increasingly wastes samples on infeasible goals, degrading learning. The Goal GAN adapts to generate goals within the feasible region, thereby decoupling performance degradation from increasing ambient dimensionality. Thus, higher embedding complexity (larger N) increases variation (sparser feasible goals), harming baseline methods but not Goal GAN.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum learning via Goal GAN tailored to generate goals in the feasible set combined with on-policy RL (TRPO + GAE); generator initialization uses state-visitation of initial policy to produce feasible initial goals.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — evaluated for coverage across feasible grid of goals in the embedded feasible subspace. Goal GAN generalizes (i.e., the learned goal-conditioned policy attains high coverage across feasible goals) and is robust to increases in embedding dimension, unlike uniform sampling baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively: Goal GAN requires far fewer labeled/meaningful goal samples than uniform sampling as N increases. Training details: per-policy block: 5 iterations × 1000 episodes; GAN updates: 200 iterations per GAN phase; reward tolerance scaled with sqrt(N). Exact total environment interactions per run are not enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) When feasible states occupy a low-volume manifold embedded in a high-dimensional ambient space, naive uniform goal sampling becomes extremely sample-inefficient as ambient dimension grows. 2) Goal GAN effectively discovers and samples goals in the feasible manifold and maintains coverage as embedding dimension increases (feasible fraction declines to extremely small values). 3) This demonstrates that adaptive goal generation (curriculum) can mitigate the combinatorial explosion in sampling complexity caused by high-dimensional ambient spaces even when the feasible space volume is held approximately constant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Goal Generation for Reinforcement Learning Agents', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking deep reinforcement learning for continuous control <em>(Rating: 2)</em></li>
                <li>Learning and transfer of modulated locomotor controllers <em>(Rating: 2)</em></li>
                <li>Universal value function approximators <em>(Rating: 1)</em></li>
                <li>Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play <em>(Rating: 1)</em></li>
                <li>Autonomous skill acquisition on a mobile manipulator <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1011",
    "paper_id": "paper-22729745",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Ant",
            "name_full": "Ant (quadruped simulated robot)",
            "brief_description": "A high-degree-of-freedom simulated quadruped agent (8 actuated joints) trained with reinforcement learning (TRPO + GAE) to reach parametrized goal positions in a U-shaped maze using an automatically generated curriculum of goals (Goal GAN).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Ant (quadruped simulated robot)",
            "agent_description": "Simulated quadruped (8 actuated joints) whose policy is trained by reinforcement learning (TRPO with GAE). Policy is goal-conditioned (input includes goal parameters) and trained with goals generated by a Goal GAN and a replay buffer. Sparse binary indicator reward: +1 when goal region reached within T timesteps (episode ends on success).",
            "agent_type": "simulated agent (robotic agent in Mujoco)",
            "environment_name": "Ant Maze (U-shaped maze)",
            "environment_description": "A 6 m x 6 m maze centered inside a 10 m x 10 m full state area; feasible goals lie within the interior of the U-shaped maze (two-dimensional goal space: (x,y) CoM positions). Complexity arises from the Ant's high DOF dynamics and motor coordination demands and from the maze geometry (requires navigating a U-turn to reach some goals). Rewards are sparse (indicator of reaching goal within up to 400 timesteps).",
            "complexity_measure": "Agent DOF and dynamics (8 actuated joints); task complexity via maze topology (U-turn requiring nontrivial navigation) and required motor coordination; per-episode horizon (max T = 400 timesteps).",
            "complexity_level": "high",
            "variation_measure": "Variation in goal locations across the continuous 2D feasible goal set (uniform over maze interior during evaluation); curriculum causes generated goal distribution to shift over training. No procedural randomization of maze geometry reported.",
            "variation_level": "medium (many distinct goal positions within the maze interior; environment geometry is fixed across experiments)",
            "performance_metric": "Coverage objective / average return over all goals (equivalently, average success probability within T timesteps) and per-goal success rate; also qualitative comparison to an oracle (rejection sampling) and baselines.",
            "performance_value": "Goal GAN achieves substantially higher coverage than Uniform Sampling and other baselines and is reported to be close to the rejection-sampling oracle in this Ant maze task (no absolute numeric success rates provided in text). The fraction of generated goals that are at the appropriate difficulty level remains around 20% during training.",
            "complexity_variation_relationship": "The paper emphasizes that high embodied-system complexity (Ant dynamics, high DOF) combined with a fixed but structured feasible goal set makes naive uniform sampling ineffective (many sampled goals yield no reward). Using an adaptive curriculum (Goal GAN) mitigates the difficulty by focusing training on goals of appropriate difficulty; no explicit analytic trade-off is given, but the empirical relationship is: increased agent/task complexity amplifies the need for curriculum/adaptive goal sampling to handle goal-space variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum learning via Goal GAN (adversarial goal generator + discriminator) combined with on-policy RL (TRPO + GAE) and a replay buffer of previously generated goals.",
            "generalization_tested": true,
            "generalization_results": "Yes — policies are evaluated on coverage over the full feasible goal set (uniform grid inside maze). The Goal GAN trained policies cover most of the maze (including across the U-turn) and generalize to many goal positions within the feasible region; performance is much better than uniform-sampling baselines and close to an expensive oracle rejection-sampling baseline.",
            "sample_efficiency": "Reported qualitatively as more sample-efficient than baselines; training protocol: policy is trained in blocks (Appendix): policy trained for 5 iterations each consisting of 1000 episodes before GAN updates; per-goal empirical return estimates use 3 rollouts for coverage computation; max episode length 400 timesteps. Exact total environment interactions per run are not tabulated in the main text.",
            "key_findings": "1) For a complex embodied agent (Ant) in a maze with sparse binary rewards, Goal GAN yields faster and more efficient learning of coverage across goal-space than uniform sampling and several baselines. 2) The automatically generated curriculum shifts generated goals over training to follow the policy's expanding capability (goals remain at an intermediate difficulty). 3) Goal GAN performance is close to an oracle rejection-sampling upper bound in this domain, indicating that adaptive goal sampling largely overcomes the difficulties introduced by complex agent dynamics and sparse rewards.",
            "uuid": "e1011.0",
            "source_info": {
                "paper_title": "Automatic Goal Generation for Reinforcement Learning Agents",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "N-dim Point Mass",
            "name_full": "N-dimensional Point Mass (simulated)",
            "brief_description": "A simulated point-mass agent used to study scaling with embedding dimension: the feasible goal region is a low-dimensional subset embedded in an increasing full state-space dimension N; Goal GAN generates feasible goals and maintains coverage as N increases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "N-dimensional Point Mass",
            "agent_description": "A simulated point-mass whose policy is trained with reinforcement learning (TRPO + GAE). The agent can accelerate in each dimension and receives a sparse indicator reward when within a tolerance of a goal within a 400-timestep episode; the policy is goal-conditioned.",
            "agent_type": "simulated agent (virtual point-mass)",
            "environment_name": "N-dimensional Point Mass embedding experiments",
            "environment_description": "Full state-space is hypercube [-5,5]^N; feasible states are a low-dimensional subset: for N=2 the feasible region is a [−5,5] x [−1,1] strip (20% of full space); for N&gt;2 the feasible region is Cartesian product of that 2D strip with small ranges in other dimensions ([−0.3,0.3]^(N-2)), keeping feasible volume approximately constant while increasing embedding dimension. Reward tolerance scales with sqrt(N) to account for increasing average distances.",
            "complexity_measure": "State-space dimensionality N; ratio of feasible region volume to full state-space volume (quantified; e.g., drops to 0.00023:1 for N=6). Also agent dynamics simplicity (low DOF point-mass) vs. increased sampling difficulty because feasible region is sparse in higher-dimensional embedding.",
            "complexity_level": "varies with N; 'low' agent dynamics complexity but environment sampling complexity increases with embedding dimension (N up to at least 6 tested).",
            "variation_measure": "Embedding dimension N (procedural variation of ambient space) and consequent decrease in feasible-volume-to-full-volume ratio; explicit numeric ratio provided (e.g., 0.00023:1 for 6 dimensions).",
            "variation_level": "increases with N; for N=2 feasible fraction ~20%, for N up to 6 feasible fraction down to 0.00023 (very high variation/sparsity).",
            "performance_metric": "Goal coverage (average success over goals) / per-goal expected return (success probability within T timesteps).",
            "performance_value": "Goal GAN maintains substantially better coverage than Uniform Sampling as embedding dimension increases; uniform sampling performance decays dramatically with N because feasible fraction decreases. Exact numeric success rates are not provided in the text, but plots (Fig. 5) show Goal GAN performance degrades much less with increasing N compared to baselines; oracle and L2-loss baseline achieve perfect performance in this simple task.",
            "complexity_variation_relationship": "Explicitly discussed: as embedding dimension N increases, the feasible portion of the state space becomes an increasingly smaller fraction of the full space (variation increases), so uniform sampling increasingly wastes samples on infeasible goals, degrading learning. The Goal GAN adapts to generate goals within the feasible region, thereby decoupling performance degradation from increasing ambient dimensionality. Thus, higher embedding complexity (larger N) increases variation (sparser feasible goals), harming baseline methods but not Goal GAN.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum learning via Goal GAN tailored to generate goals in the feasible set combined with on-policy RL (TRPO + GAE); generator initialization uses state-visitation of initial policy to produce feasible initial goals.",
            "generalization_tested": true,
            "generalization_results": "Yes — evaluated for coverage across feasible grid of goals in the embedded feasible subspace. Goal GAN generalizes (i.e., the learned goal-conditioned policy attains high coverage across feasible goals) and is robust to increases in embedding dimension, unlike uniform sampling baselines.",
            "sample_efficiency": "Reported qualitatively: Goal GAN requires far fewer labeled/meaningful goal samples than uniform sampling as N increases. Training details: per-policy block: 5 iterations × 1000 episodes; GAN updates: 200 iterations per GAN phase; reward tolerance scaled with sqrt(N). Exact total environment interactions per run are not enumerated in main text.",
            "key_findings": "1) When feasible states occupy a low-volume manifold embedded in a high-dimensional ambient space, naive uniform goal sampling becomes extremely sample-inefficient as ambient dimension grows. 2) Goal GAN effectively discovers and samples goals in the feasible manifold and maintains coverage as embedding dimension increases (feasible fraction declines to extremely small values). 3) This demonstrates that adaptive goal generation (curriculum) can mitigate the combinatorial explosion in sampling complexity caused by high-dimensional ambient spaces even when the feasible space volume is held approximately constant.",
            "uuid": "e1011.1",
            "source_info": {
                "paper_title": "Automatic Goal Generation for Reinforcement Learning Agents",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking deep reinforcement learning for continuous control",
            "rating": 2,
            "sanitized_title": "benchmarking_deep_reinforcement_learning_for_continuous_control"
        },
        {
            "paper_title": "Learning and transfer of modulated locomotor controllers",
            "rating": 2,
            "sanitized_title": "learning_and_transfer_of_modulated_locomotor_controllers"
        },
        {
            "paper_title": "Universal value function approximators",
            "rating": 1,
            "sanitized_title": "universal_value_function_approximators"
        },
        {
            "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
            "rating": 1,
            "sanitized_title": "intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay"
        },
        {
            "paper_title": "Autonomous skill acquisition on a mobile manipulator",
            "rating": 1,
            "sanitized_title": "autonomous_skill_acquisition_on_a_mobile_manipulator"
        }
    ],
    "cost": 0.009843999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Goal Generation for Reinforcement Learning Agents
31 May 2017</p>
<p>David Held davheld@berkeley.edu 
Xinyang Geng 
Carlos Florensa florensad@berkeley.edu 
Pieter Abbeel pabbeel@berkeley.edu </p>
<p>Computer Science
UC Berkeley</p>
<p>Computer Science
UC Berkeley</p>
<p>Computer Science
UC Berkeley</p>
<p>OpenAI Computer Science
UC Berkeley International Computer Science Institute (ICSI)</p>
<p>Automatic Goal Generation for Reinforcement Learning Agents
31 May 201742EC1246F6947458593E9FA2987DCDB2arXiv:1705.06366v2[cs.LG]
Reinforcement learning is a powerful technique to train an agent to perform a task.However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function.Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment.We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized sub-set of the state-space.The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent.Our method thus automatically produces a curriculum of tasks for the agent to learn.We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment.Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) can be used to train an agent to perform a task by optimizing a reward function.Recently, a number of impressive results have been demonstrated by training agents using reinforcement learning: such agents have been trained to defeat a champion Go player (Silver et al., 2016), to outperform humans in 49 Atari games (Guo et al., 2016;Mnih et al., 2015), and to perform a variety of difficult robotics tasks (Lillicrap et al., 2015;Duan et al., 2016;Levine et al., 2016).</p>
<p>In each of the above cases, the agent is trained to optimize a single reward function in order to learn to perform a single task.However, there are many real-world environments in which a robot will need to be able to perform not a single task but a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.To automatically learn to achieve a diverse set of tasks, our algorithm allows an agent to generate its own reward functions, defined with respect to target sub-sets of the state space, called goals.Our method will propose different goals for the agent to try to reach, and then the agent will learn what actions are necessary to reach each proposed goal.After multiple training iterations, the agent learns to reach a wide variety of goals in its state space.Specifically, our approach learns a policy that takes as input not only the current observation but also the goal (i.e. the parameters describing the target state set) currently to be achieved, similar to the universal value functions in Schaul et al. (2015).We consider the problem of maximizing the average success rate of our agent over all possible goals, where success is defined as the probability of successfully reaching each goal by the current policy.In order to efficiently maximize this objective, the algorithm must intelligently choose which goals to focus on at every training stage: goals should be feasible and at the appropriate level of difficulty for the current policy.</p>
<p>We generate such goals using a Goal Generative Adversarial Network (Goal GAN), a variation of to the GANs introduced by Goodfellow et al. (2014).A goal discriminator is trained to evaluate whether a goal is feasible and at the appropriate level of difficulty for the current policy, and a goal generator is trained to generate goals that meet this criteria.We show that such a framework allows a policy to quickly learn to reach all feasible goals in its environment, with no prior knowledge about the environment or the tasks being performed.Our method automatically creates a curriculum, in which, at each step, the generator generates goals that are only slightly more difficult than the goals that the agent already knows how to achieve.Due to this curriculum, the policy is able to learn even with a sparse reward function, such as an indicator that rewards the agent for reaching the goal.</p>
<p>In summary, our main contribution is a method for automatic curriculum generation that considerably improves the sampling efficiency of learning to reach all feasible goals in the environment.Learning to reach multiple goals is useful for multi-task settings such as navigation or manipulation, in which we want the agent to perform a wide range of tasks.Our method naturally handles sparse reward functions, without needing to manually modify the reward function for every task, based on prior task knowledge.Instead, our method dynamically modifies the probability distribution from which goals are sampled to ensure that the generated goals are always at the appropriate difficulty level, until the agent learns to reach all goals within the feasible goal space.</p>
<p>Related Work</p>
<p>Intrinsic Motivation: Intrinsic motivation involves learning with an intrinsically specified objective (Schmidhuber, 1991(Schmidhuber, , 2010)).Recently there have been various formulations of intrinsic motivation, relating to optimizing surprise (Houthooft et al., 2016;Achiam &amp; Sastry, 2016) or surrogates of state-visitation counts (Bellemare et al., 2016;Tang et al., 2016).All these approaches improve learning in sparse tasks where naive exploration performs poorly.However, these formulations do not have a notion of what states are hard for the learner and the intrinsic motivation is independent of the current performance of the agent.In contrast, our formulation of intrinsic motivation directly relates to our policy improvement: the agent is motivated to train on tasks that push the boundaries of its capabilities.</p>
<p>Skill-learning:</p>
<p>We are often interested in training an agent to perform a collection of tasks rather than a single one, like reaching different positions in the agent's state-space.Skill learning is a common approach to this problem as it allows the agent to re-use skills, improving learning compared to training for every task from scratch.Discovering useful skills is a challenging task that has mostly been studied for discrete environments (Vigorito &amp; Barto, 2010;Mnih et al., 2016) or for continuous tasks where demonstrations are provided (Konidaris et al., 2011;Ranchod et al., 2015).More recent work overcomes some of these limitations by training low-level modulated locomotor controllers (Heess et al., 2016), or multimodal policies with an information theoretic regularizer to learn a fixed-size set of skills (Florensa et al., 2017).Nevertheless, previous work usually sees the skills learning as a pre-training step from which useful primitives are obtained and can later be used to achieve other tasks.Hence, additional downstream training is required to properly compose the skills in a purposeful way.On the other hand, our approach directly trains policies that take as input the desired goals, enabling generalization to new desired (test) goals without the need for fine tuning.</p>
<p>The problem that we are exploring has been referred to as "multi-task policy search" (Deisenroth et al., 2014) or "contextual policy search," in which the task is viewed as the context for the policy (Deisenroth et al., 2013;Fabisch &amp; Metzen, 2014).As opposed to the work of Deisenroth et al. (2014), our work uses a curriculum to perform efficient multi-task learning, even in sparse reward settings.In contrast to Fabisch &amp; Metzen (2014), which trains from a small number of discrete contexts / tasks, our method generates a training curriculum directly in continuous task space.</p>
<p>Curriculum Learning: The increasing interest on training single agents to perform multiple tasks is leading to new developments on how to optimally present the tasks to the agent during learning.The idea of using a curriculum has been explored in many prior works on supervised learning (Bengio et al., 2009;Zaremba &amp; Sutskever, 2014;Bengio et al., 2015).However, these curricula are usually hand-designed, using the expertise of the system designer.Another line of work takes into explicit consideration which examples are hard for the current learner and allows it to not consider them (Kumar et al., 2010;Jiang et al., 2015).However this has mainly been applied for supervised tasks and most curriculum learning in reinforcement learning still relies on fixed pre-specified sequences of tasks (Karpathy &amp; Van De Panne, 2012).In contrast, we automatically create a curriculum with no prior knowledge by discovering which tasks are easy or difficult for a given policy.</p>
<p>Other recent work has proposed using a given baseline performance for several tasks to gauge which tasks are the hardest and require more training (Sharma &amp; Ravindran, 2017), but the framework can only handle a finite set of tasks and cannot handle sparse rewards.Our method trains a policy that generalizes to a set of continuously parameterized tasks and is shown to perform well even under sparse rewards by not allocating training effort to tasks that are too hard for the current performance of the agent.Finally, an interesting self-play strategy has recently been proposed that is concurrent to our work (Sukhbaatar et al., 2017), but, contrary to our approach that is designed to generate many training tasks at the right level of difficulty, the asymmetric component of their method could lead to biased exploration.Furthermore, they view their work as simply an exploration bonus for a single target task (and they evaluate their method accordingly); in contrast, we define a new problem of efficiently optimizing a policy across a range of goals, as we explain below.</p>
<p>Problem Definition</p>
<p>Goal-parameterized Reward Functions</p>
<p>In the traditional reinforcement learning framework, at each timestep t, the agent in state s t ∈ S ⊆ R n takes an action a t ∈ A ⊆ R m , according to some policy π(a t | s t ) that maps from the current state s t to a probability distribution over actions.Taking this action causes the agent to enter into a new state s t+1 according to a transition distribution p(s t+1 |s t , a t ), and receive a reward r t = r(s t , a t , s t+1 ).The objective of the agent is to find the policy π that maximizes the expected return, defined as the sum of rewards R = T t=0 r t , where T is a maximal time given to perform the task.The learned policy corresponds to maximizing the expected return for a single reward function.</p>
<p>In our framework, instead of learning to optimize a single reward function, we consider a range of reward functions r g indexed or parametrized by a goal g ∈ G.Each goal g corresponds to a set of states S g ⊂ S such that goal g is considered to be achieved when the agent is in any state s t ∈ S g .Then the objective is to learn a policy that, given any goal g ∈ G, acts optimally with respect to r g .In this paper, we define a very simple reward function that measures whether the agent has reached the goal
r g (s t , a t , s t+1 ) = 1{s t+1 ∈ S g } , (1)
where 1 is the indicator function.In our case, we use
S g = {s t : d(f (s t ), g) ≤ }, where f (•) is a function that projects a state into goal space G, d(•, •
) is a distance metric in goal space, and is the acceptable tolerance that determines when the goal is reached.However, our method can handle generic sparse rewards (as in Eq. ( 1)) and does not require a distance metric for learning.Furthermore, we define our MDP such that each episode terminates when s t ∈ S g .Thus, the return R g = T t=0 r g t is a binary random variable whose value indicates whether the agent has reached the set S g in at most T time-steps.Hence, the return of a trajectory s 0 , s 1 , . . .can be expressed as R g = 1{ T t=0 s t ∈ S g }.Now, policies are also conditioned on the current goal g, written as π(a t | s t , g), and the expected return obtained when we take actions sampled from it can then be expressed as the probability of succeeding on each goal within T timesteps, as shown in Eq. ( 2).
R g (π) = E π(• | st,g) 1{ T t=0 s t ∈ S g } = P T t=0 s t ∈ S g π, g(2)
The sparse indicator reward function of Eq. ( 1) is not only simple but also represents a property of many real-world goal states: in many settings, it may be difficult to tell whether the agent is getting closer to achieving a goal, but easy to tell when a goal has been achieved.For example, for a robot moving in a maze, taking actions that maximally reduce the straight-line distance from the start to the goal is usually not a feasible approach for reaching the goal, due to the presence of obstacles along the path.In theory, one could hand-engineer a meaningful distance function for each task that could be used to create a dense reward function.Instead, we use the indicator function of Eq. ( 1), which simply captures our objective by measuring whether the agent has reached the goal state.We show that our method is able to learn even with such sparse rewards.</p>
<p>Overall Objective</p>
<p>We desire to find a policy π(a t | s t , g) that achieves a high reward for many goals g.We assume that there is a test distribution of goals p g (g) that we would like to perform well on.For simplicity, we assume that the test distribution samples goals uniformly from the set of goals G, although in practice any distribution can be used.The overall objective is then to find a policy π * such that
π * (a t | s t , g) = arg max π E g∼pg(•) R g (π)
.</p>
<p>(3)</p>
<p>Recall from Eq. ( 2) that R g (π) is the probability of success for each goal g.Thus the objective of Eq. ( 3) measures the average probability of success over all goals sampled from p g (g).We refer to the objective in Eq. ( 3) as the coverage objective.</p>
<p>Method</p>
<p>Our approach can be broken down into three parts: First, we label a set of goals based on whether they are at the appropriate level of difficulty for the current policy.Second, using these labeled goals, we construct and train a generator to output new goals that are at the appropriate level of difficulty.Finally, we use these new goals to efficiently train the policy, improving its coverage objective.We iterate through each of these steps until the policy converges.</p>
<p>Goal Labeling</p>
<p>As shown in our experiments, sampling goals from p g (g) directly, and training our policy on each sampled goal may not be the most sample efficient way to optimize the coverage objective of Eq. ( 3).Instead, we modify the distribution from which we sample goals during training: we wish to find the set of goals g in the set
G i = {g : R min ≤ R g (π i ) ≤ R max } ⊆ G.
The justification for this is as follows: due to the sparsity of the reward function, for most goals g, the current policy π i (at iteration i) obtains no reward.Instead, we wish to train our policy on goals g for which π i is able to receive some minimum expected return R g (π i ) &gt; R min such that the agent receives enough reward signal for learning.On the other hand, if we only sample from goals for which R g (π i ) &gt; R min , we might sample repeatedly from a small set of already mastered goals.To force our policy to train on goals that still need improvement, we train on the set of goals g for which R g (π i ) ≤ R max , where R max is a hyperparameter setting a maximum level of performance above which we prefer to concentrate on new goals.Thus, training our policy on goals in G i allows us to efficiently maximize the coverage objective of Eq. ( 3).Note that from Eq. ( 2), R min and R max can be interpreted as a minimum and maximum probability of reaching a goal over T timesteps.</p>
<p>Given a set of goals sampled from some distribution p data (g), we wish to estimate a label y g ∈ {0, 1} for each goal g that indicates whether g ∈ G i .To label a given goal, we empirically estimate the expected return for this goal Rg (π i ) by performing rollouts of our current policy π i .The label for this goal is then set to y g = 1 R min ≤ Rg (π i ) ≤ R max .In the next section we describe how we can generate more goals that also belong to G i , in addition to the goals that we have labeled.</p>
<p>Adversarial Goal Generation</p>
<p>In order to sample new goals g uniformly from G i , we introduce an adversarial training procedure called "goal GAN", which is a modification of to the procedure used for training Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).The modification allows to train the generative model both with positive examples from the distribution we want to approximate, and negative examples sampled from a distribution that does not share support with the desired one.This improves the accuracy of the generative model despite being trained with very few positive samples.Other generative models like Stochastic Neural Networks (Tang &amp; Salakhutdinov, 2013) don't accept negative examples and don't have the potential to scale up to higher dimensions as well as GAN approaches.</p>
<p>In our particular application, we use a neural network G(z) known as the "goal generator" to generate goals g from a noise vector z.We train the goal generator to uniformly output goals in G i , using a second network D(g) known as a "goal discriminator".The goal discriminator is trained to distinguish goals that are in G i from goals that are not in G i .We optimize our goal generator and discriminator in a manner similar to the training procedure for the Least-Squares GAN (LS-GAN) (Mao et al., 2016), but introducing the binary label y g indicating whether g ∈ G i .We found that the LSGAN works better than other forms of GAN for our problem.We use a = -1, b = 1, and c = 0, as in Mao et al. (2016).
min D V (D) = E g∼p data (g) y g (D(g) − b) 2 + (1 − y g )(D(g) − a) 2 + E z∼pz(z) [(D(G(z)) − a) 2 ] min G V (G) = E z∼pz(z) [D(G(z)) − c) 2 ] (4)
Unlike in the original LSGAN paper (Mao et al., 2016), we have three terms in our value function V (D) rather than the original two.For goals g for which y g = 1, the second term disappears and we are left with only the first and third terms, which are identical to that of the original LSGAN framework.Viewed in this manner, the discriminator is trained to discriminate between goals from p data (g) with a label y g = 1 and the generated goals G(z).Looking at the second term, our discriminator is also trained with "negative examples," i.e. goals with a label y g = 0 which our generator should not generate.The generator is trained to "fool" the discriminator, i.e. to output goals that match the distribution of goals in p data (g) for which y g = 1.  3) is shown in Algorithm 1.At each iteration i, we generate a set of goals from our goal generator G(z).We use these goals to train our policy using reinforcement learning, with the reward function given by Eq. ( 1).The training can be done with any Reinforcement Learning algorithm; in our case we use TRPO (Schulman et al., 2015) with GAE (Schulman et al., 2016).After training our policy, we evaluate our policy's performance on these goals; this performance is used to determine each goal's label y g , as described in Section 4.1.Next, we use these labels to train our goal generator and our goal discriminator, as described in Section 4.2.The generated goals from the previous iteration are used to compute the Monte Carlo estimate of the expectations with respect to the distribution p data (g) in Eq. ( 4).By training on goals within G i produced by the goal generator, our method efficiently finds a policy that optimizes the coverage objective.</p>
<p>Policy</p>
<p>In addition to training our policy on the goals that were generated in the current iteration, we also save a list ("regularized replay buffer") of goals that were generated during previous iterations and use these to train our policy as well, so that our policy does not forget how to achieve goals that it has previously learned.When we generate goals for our policy to train on, we sample two thirds of the goals from the Goal GAN and we sample the one third of the goals uniformly from the replay buffer.To prevent the replay buffer from concentrating in a small portion of goal space, we only insert new goals that are further away than from the goals already in the buffer, where we chose the goal-space metric and to be the same as the ones introduced in Section 3.1.The algorithm described above naturally creates a curriculum for our policy.The goal generator generates goals in G i , for which our policy obtains an intermediate level of return, and thus such goals are at the appropriate level of difficulty for our current policy π i .As our policy improves, the generator learns to generate goals in order of increasing difficulty.Hence, our method can be viewed as a way to automatically generate a curriculum of goals.However, the curriculum occurs as a by-product via our optimization, without requiring any prior knowledge of the environment or the tasks that the agent must perform.</p>
<p>Goal GAN Initialization</p>
<p>In order to begin our training procedure, we need to initialize our goal generator to produce an initial set of goals.If we initialize the goal generator randomly (or if we initialize it to sample uniformly from the goal space), it is likely that, for most (or all) of the sampled goals, our initial policy would receives no reward due to the sparsity of the reward function.Thus we might have that all of our initial goals g have Rg (π 0 ) &lt; R min , leading to very slow training.</p>
<p>To avoid this problem, we initialize our goal generator to output a set of goals that our initial policy is likely to be able to achieve with Rg (π i ) ≥ R min .To accomplish this, we run our initial policy π 0 (a t | s t , g) with goals sampled uniformly from the goal space.We then observe the set of states S v that are visited by our initial policy.These are states that can be easily achieved with the initial policy, π 0 , so the goals corresponding to such states will likely be contained within S I 0 .We then train the goal generator to produce goals that match the state-visitation distribution p v (g), defined as the uniform distribution over the set f (S v ).We can achieve this through traditional GAN training, with p data (g) = p v (g).This initialization of the generator allows us to bootstrap the Goal GAN training process, and our policy is able to quickly improve its performance.</p>
<p>Experimental Results</p>
<p>In this section we provide the experimental results to answer the following questions about our goal generation algorithm:</p>
<p>• Does the automatic curriculum yield faster maximization of the coverage objective?</p>
<p>• Does the Goal GAN dynamically shift to sample goals of the appropriate difficulty?</p>
<p>• Does it scale to a higher-dimensional state-space with a low-dimensional space of feasible goals?</p>
<p>To answer the first two questions, we demonstrate our method in a maze task where the goals are the (x, y) position of the Center of Mass (CoM) of a dynamically complex agent.Hence the feasible goal space is the interior of the maze, and we demonstrate how our goal generation can guide the agent around a turn in the maze.Then, we study how our method scales with the dimension of the state-space in an environment where the feasible region is kept of approximately constant volume in an embedding space that grows in dimension.For the implementation details refer to Appendices A.4 and A.5. Visualization of the learned policies are provided on our website2 .We compare our Goal GAN method against three baselines.Uniform Sampling (baseline 3) is a method that does not use a curriculum at all, training at every iteration on goals uniformly sampled from the state-space.Many goals are unfeasible or too difficult for the current performance of the agent and hence no reward is received, making this method very sample-inefficient.To demonstrate that a straight-forward distance reward can be prone to local minima, Uniform Sampling with L2 loss (baseline 2) samples goals in the same fashion as baseline 3, but instead of the indicator reward that our method uses, it receives the negative L2 distance to the goal as a reward at every step.Finally, On-policy GAN (baseline 1) samples goals from a GAN that is constantly retrained using the state visitation distribution (as in Section 4.4).Finally, we also show the performance of using Rejection Sampling (oracle), where we sample goals uniformly from the feasible state-space and only keep them if they satisfy the criterion defined in Section 4.1.This method is orders of magnitude more expensive in terms of labeling, but serves to estimate an upper-bound for our method in terms of performance.We test our method in the challenging environment of a complex robotic agent (Ant) navigating a U-shaped maze, as depicted in Fig. 1.Duan et al. (2016) describe the task of trying to reach the other end of the U-turn and they show that standard Reinforcement Learning methods are unable to solve it.We further extend the task to try to reach every point of the maze, still keeping the sparse indicator reward (here with = 1).Our goal GAN method successfully learn how to cover most of the maze, including the other end.The goal space is two dimensional, allowing us to study the goals generated by the automatic curriculum and how it drives the training through the U-turn.The visualizations and the experimental results will help us answer the first two questions from the previous section.</p>
<p>Ant Maze</p>
<p>We first explore whether, by training on goals that are generated by our Goal GAN, we are able to improve our policy's training efficiency, compared to the baselines described above.In Fig. 2 we see that our method leads to faster training compared to the baselines.Using the Goal we are always training our policy on goals that are at the appropriate level of difficulty, leading to more efficient learning.Interestingly, for this task our method is very close the oracle of rejection sampling.The On-policy GAN baseline is, in this environment, performing reasonably well because we inject high noise in the actions of the ant, making the current policy visit further and further regions.This baseline performs worse in other experiments presented in the supplementary material3 .The worse performing baseline is the one rewarding the L2 goal distance at every time-step, because the complex dynamics of the ant and the fact that goals are sampled all around make the optimization fall in the bad local optimal policy of not moving.The purpose of the Goal GAN is to always generate goals at the appropriate level of difficulty for the current policy, i.e. goals g for which R min ≤ Rg (π i ) ≤ R max .We explore the difficulty of the generated goals in Fig. 3.Note that the goals in this figure include a mix of newly generated goals as well as goals from previous iterations that we use to prevent our policy from "forgetting."It can be observed in these figures that the location of the generated goals shifts to different parts of the maze as the policy improves.The percentage of generated goals that are at the appropriate level of difficulty ("good goals") stays around 20% even as the policy improves.The Goal GAN learns to generate goals at the appropriate locations based on the policy performance such that many of the generated goals are at the appropriate level of difficulty.The corresponding performance of the policy can be shown in Fig. 4.</p>
<p>N-dimensional Point Mass</p>
<p>In most real-world reinforcement learning problems, the set of feasible states is a lower-dimensional subset of the full state space, defined by the constraints of the environment.For example, the kinematic constraints of a robot limit the set of feasible states that the robot can achieve.Therefore, uniformly sampling goals from the full state-space would yield very few achievable goals.In this section we use an N-dimensional Point Mass to explore this issue and demonstrate the performance of our method as the embedding dimension increases.3).For illustration purposes, the feasible state-space (i.e. the space within the maze) is divided into a grid, and a goal location is selected from the center of each grid cell.Each grid cell is colored according to the expected return achieved on this goal: Red indicates 100% success; blue indicates 0% success.</p>
<p>In our experiments, the full state-space of the N -dimensional Point Mass is the hypercube [−5, 5] N .However, the Point Mass can only move within a small subset of this state space.In the twodimensional case, the set of feasible states corresponds to the [−5, 5] × [−1, 1] rectangle, making up 20% of the full space.For N &gt; 2, the feasible space is the Cartesian product of this 2D strip with [− , ] N −2 , where = 0.3.In this higher-dimensional environment, our agent receives a reward of 1 when it moves within N = 0.3
√ N √
2 of the goal state, to account for the increase in average L2 distance between points in higher dimensions.The ratio of the volume of the embedded space to the volume of the full state space decreases as the state-dimension N increases, down to a ratio of 0.00023:1 for 6 dimensions.</p>
<p>Figure 5: Goal coverage obtained on the N-dim point mass environment, for an increasing number of dimensions.Each policy is trained for the same number of total iterations.All plots are an average over 5 random seeds.Fig. 5 shows the performance of our method compared to the baseline methods, as the number of dimensions increases.The uniform sampling baseline results in very poor performance as the number of dimensions increases.This is because the fraction of feasible states within the full state space decreases as the dimension increases.Thus, sampling uniformly results in sampling an increasing percentage of infeasible states as the dimension increases, leading to poor training.In contrast, the performance of our method does not decay as much as the state space dimension increases, because our Goal GAN always generates goals within the feasible portion of the state space, and at the appropriate level of difficulty.The On-policy GAN baseline suffers from the increase in dimension because it is not encouraged to keep exploring other parts of the space than the ones it has already visited.Finally, the oracle and the base-line with an L2 loss as reward have perfect performance, which is expected in this simple task where the optimal policy is just to go in a straight line towards the goal.</p>
<p>Figure 6: Point-mass in the same maze as described for Ant a GAE lambda of 0.995.The policy is trained with TRPO with Generalized Advantage Estimation implemented in rllab (Schulman et al., 2015(Schulman et al., , 2016;;Duan et al., 2016).Every "update policy" consists of 5 iterations this algorithm.</p>
<p>Figure 1 :
1
Figure 1: Ant Maze environment</p>
<p>Figure 2 :
2
Figure 2: Learning curves comparing the training efficiency of our method and the baselines.The y-axis indicates the average return over all goal positions in the maze, and the x-axis shows the number of times that new goals have been sampled.All plots are an average over 7 random seeds.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Goals sampled by the Goal GAN (same policy training as in Fig.4)."High rewards" (in green) are goals with Rg (π i ) ≥ R max ; "Good goals" (in blue) are those with the appropriate level of difficulty for the current policy (R min ≤ Rg (π i ) ≤ R max ).The red ones have R min ≥ Rg (π i )</p>
<p>https://sites.google.com/view/goalgeneration4rl
https://sites.google.com/view/goalgeneration4rl
Conclusions and Future WorkWe propose a new paradigm in reinforcement learning where the objective is to train a single policy to succeed on a variety of goals, under sparse rewards.To solve this problem we develop a method for automatic curriculum generation that dynamically adapts to the current performance of the agent.The curriculum is obtained without any prior knowledge of the environment or of the tasks being performed.We use generative adversarial training to automatically generate goals for our policy that are always at the appropriate level of difficulty (i.e.not too hard and not too easy).We demonstrated that our method can be used to efficiently train a policy to reach a range of goal states, which can be useful for multi-task learning scenarios.A Implementation details A.1 Ant Maze EnvironmentThe environment is implemented in Mujoco(Todorov et al., 2012).The agent is constrained to move within the maze environment, which has dimensions of 6m x 6m.The full state-space has an area of size 10 m x 10 m, within which the maze is centered.To compute the coverage objective, goals are sampled from within the maze according to a uniform grid on the maze interior.For each goal, we estimate the empirical return with three rollouts.The maximum time-steps given to reach the current goal are 400.A.2 Ant specificationsThe ant is a quadruped with 8 actuated joints, 2 for each leg.Besides the coordinates of the center of mass, the joint angles and joint velocities are also included in the observation of the agent.The high degrees of freedom and the required motor coordination make this task much harder than the point mass.More details found inDuan et al. (2016).A.3 Maze navigation with point-massWe have also conducted experiments with a point-mass instead of the complex ant inside the maze, to understand the dynamics of our curriculum generation.Results are reported in Fig.6.A.4 Point-mass specificationsFor both the N-dim point mass of Section 5.2 and the results from Figure6, in each episode (rollout) the point-mass has 400 timesteps to reach the goal, where each timestep is 0.02 seconds.The agent can accelerate in up to a rate of 5 m/s 2 in each dimension (N = 2 for the maze).The observations of the agent are 2N dimensional, including position and velocity of the point-mass.A.5 Goal GAN design and trainingAfter the generator generates goals, we add noise to each dimension of the goal sampled from a normal distribution with zero mean and unit variance.At each step of the algorithm, we train the policy for 5 iterations, each of which consists of 1000 episodes.After 5 policy iterations, we then train the GAN for 200 iterations, each of which consists of 1 iteration of training the discriminator and 1 iteration of training the generator.The generator receives as input 4 dimensional noise sampled from the standard normal distribution.The goal generator consists of two hidden layers with 128 nodes, and the goal discriminator consists of two hidden layers with 256 nodes, with relu nonlinearities.A.6 Policy and optimizationThe policy is defined by a neural network which receives as input the goal appended to the agent observations described above.The inputs are sent to two hidden layers of size 32 with tanh nonlinearities.The final hidden layer is followed by a linear N -dimensional output, corresponding to accelerations in the N dimensions.For policy optimization, we use a discount factor of 0.998 and
Surprise-based intrinsic motivation for deep reinforcement learning. Josh Achiam, Shankar Sastry, 2016 Deep Reinforcement Learning Workshop at NIPS. 2016</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc G Bellemare, Srinivasan, Sriram, Ostrovski, Georg, Schaul, Tom, David Saxton, Remi Munos, Advances in Neural Information Processing Systems. 2016</p>
<p>Scheduled sampling for sequence prediction with recurrent neural networks. Bengio, Samy, Vinyals, Oriol, Jaitly, Navdeep, Noam Shazeer, Advances in Neural Information Processing Systems. Curran Associates, Inc201528</p>
<p>Curriculum learning. Yoshua Bengio, Louradour, Jérôme, Collobert, Ronan, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACM2009</p>
<p>A survey on policy search for robotics. Marc Deisenroth, Peter, Gerhard Neumann, Jan Peters, Foundations and Trends in Robotics. 21-22013</p>
<p>Multi-task policy search for robotics. Marc Deisenroth, Peter, Englert, Peter, Jan Peters, Dieter Fox, Robotics and Automation (ICRA). IEEE2014. 2014</p>
<p>Benchmarking deep reinforcement learning for continuous control. Yan Duan, Chen, Xi, Houthooft, Rein, John Schulman, Pieter Abbeel, International Conference on Machine Learning (ICML). 2016</p>
<p>Active contextual policy search. Alexander Fabisch, Jan Metzen, Hendrik, Journal of Machine Learning Research. 1512014</p>
<p>Stochastic neural networks for hierarchical reinforcement learning. Carlos Florensa, Duan , Yan Abbeel, Pieter , International Conference in Learning Representations. 2017</p>
<p>Generative adversarial networks. Ian Goodfellow, Pouget-Abadie, Jean, Mirza, Mehdi, Bing Xu, Warde - Farley, David, Ozair, Sherjil, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. 2014</p>
<p>Deep learning for reward design to improve monte carlo tree search in atari games. Xiaoxiao Guo, Singh, Satinder, Richard Lewis, Honglak Lee, Conference on Uncertainty in Artificial Intelligence (UAI). 2016</p>
<p>Learning and transfer of modulated locomotor controllers. Nicolas Heess, Greg Wayne, Tassa, Yuval, Lillicrap, Timothy, Martin Riedmiller, David Silver, arXiv:1610.051822016</p>
<p>Variational information maximizing exploration. Rein Houthooft, Chen, Xi, Duan, Yan, John Schulman, De Turck, Filip Abbeel, Pieter , Advances in Neural Information Processing Systems. 2016</p>
<p>Self-paced curriculum learning. Lu Jiang, Meng, Deyu, Zhao, Qian, Shiguang Shan, Hauptmann, G Alexander, AAAI. 201526</p>
<p>Curriculum learning for motor skills. Andrej Karpathy, Van De Panne, Michiel, Canadian Conference on Artificial Intelligence. Springer2012</p>
<p>Autonomous skill acquisition on a mobile manipulator. George Konidaris, Kuindersma, Scott, Roderic A Grupen, Barto, G Andrew, AAAI. 2011</p>
<p>Self-paced learning for latent variable models. M P Kumar, Benjamin Packer, Daphne Koller, Advances in Neural Information Processing Systems. 201023</p>
<p>End-to-end of deep visuomotor policies. Sergey Levine, Chelsea Finn, Darrell , Trevor Abbeel, Pieter , Journal of Machine Learning Research. 2016</p>
<p>Timothy P Lillicrap, Jonathan J Hunt, Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa, Yuval, David Silver, Daan Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. 2015</p>
<p>Xudong Mao, Li, Qing, Xie, Haoran, Lau, Y K Raymond, Zhen Wang, Stephen Smolley, Paul, arXiv:1611.04076Least squares generative adversarial networks. 2016</p>
<p>Humanlevel control through deep reinforcement learning. Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Andrei A Rusu, Veness, Joel, Bellemare, G Marc, Alex Graves, Riedmiller, Martin, Andreas K Fidjeland, Ostrovski, Georg, Nature. 51875402015</p>
<p>Strategic attentive writer for learning macro-actions. Mnih, Volodymyr, Agapiou, John, Osindero, Simon, Alex Graves, Vinyals, Oriol, Kavukcuoglu, Koray, arXiv:1606.046952016</p>
<p>Nonparametric bayesian reward segmentation for skill discovery using inverse reinforcement learning. Pravesh Ranchod, Benjamin Rosman, George Konidaris, International Conference on Intelligent Robots and Systems. IEEE2015</p>
<p>Universal value function approximators. Tom Schaul, Horgan, Gregor Daniel, Karol Silver, David , Proceedings of the 32nd International Conference on Machine Learning (ICML-15). the 32nd International Conference on Machine Learning (ICML-15)2015</p>
<p>Curious model-building control systems. Jürgen Schmidhuber, International Joint Conference on Neural Networks. IEEE1991</p>
<p>Formal theory of creativity, fun, and intrinsic motivation. Jürgen Schmidhuber, IEEE Transactions on Autonomous Mental Development. 231990-2010. 2010</p>
<p>Trust region policy optimization. John Schulman, Levine, Sergey, Abbeel, Jordan Pieter, Michael Moritz, Philipp , Proceedings of The 32nd International Conference on Machine Learning. The 32nd International Conference on Machine Learning2015</p>
<p>Highdimensional continuous control using generalized advantage estimation. John Schulman, Moritz, Philipp, Levine, Jordan Sergey, Michael Abbeel, Pieter , 2016ICLR</p>
<p>Online Multi-Task Learning Using Biased Sampling. Sahil Sharma, Balaraman Ravindran, arXiv:1702.060532017</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Guez, Arthur, Sifre, Laurent, Van Den Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanctot, Marc, Nature. 52975872016</p>
<p>Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sukhbaatar, Sainbayar, Kostrikov, Ilya, Arthur Szlam, Rob Fergus, arXiv:1703.054072017</p>
<h1>exploration: A study of count-based exploration for deep reinforcement learning. Haoran Tang, Houthooft, Rein, Foote, Davis, Stooke, Adam, Chen, Xi, Duan, Yan, Schulman, John, Filip Turck, De, Pieter Abbeel, CoRR, abs/1611.047172016</h1>
<p>Yichuan Tang, Ruslan Salakhutdinov, doi: 10.1.1.63.1777Learning Stochastic Feedforward Neural Networks. Nips. 20132</p>
<p>Mujoco: A physics engine for model-based control. Todorov, Emanuel, Tom Erez, Tassa, IEEE. 2012. 2012IEEE</p>
<p>Intrinsically motivated hierarchical skill learning in structured environments. Christopher M Vigorito, Andrew G Barto, IEEE Transactions on Autonomous Mental Development. 222010</p>
<p>Learning to execute. Wojciech Zaremba, Ilya Sutskever, CoRR, abs/1410.46152014</p>            </div>
        </div>

    </div>
</body>
</html>
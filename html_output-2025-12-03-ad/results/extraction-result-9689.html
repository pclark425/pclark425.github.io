<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9689 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9689</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9689</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-259360395</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.03109v9.pdf" target="_blank">A Survey on Evaluation of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9689.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9689.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated evaluation (metrics-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation approach that computes task-specific metrics (accuracy, ROUGE, F1, BLEU, calibration, robustness) automatically without human raters; widely used for large-scale LLM benchmarking and reproducible comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various LLMs (e.g., GPT-3, GPT-3.5, GPT-4, Claude, LLaMA families)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>General-purpose large pre-trained transformer models of various sizes and training regimes; used across many benchmarks for zero-shot, few-shot, and fine-tuned evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / cross-domain (NLP, reasoning, math, science, medicine)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute automated metrics (Exact Match, F1, ROUGE, BLEU, calibration scores, attack success rate, performance drop rate) on held-out benchmark datasets or challenge sets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (Exact Match, F1), generation overlap (ROUGE/BLEU), calibration (ECE/AUC), robustness (ASR/PDR), fairness metrics where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used by MMLU, MATH, HELM, C-Eval, AlpacaEval, many task-specific datasets listed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports that automated evaluation is the most common method, enabling large-scale comparisons (e.g., MMLU, MATH). Automated metrics are efficient but often insufficient for open-ended or subjective scientific-theory outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Automated metrics often miss plausibility, novelty, explanatory power and can be gamed by surface overlap; they cannot reliably measure scientific-theory quality or hallucination without specialized probes or human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated evaluation is scalable but less reliable than human expert review for assessing deep correctness, novelty, and explanatory value of scientific theories.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine automated metrics with targeted probes and human evaluation; design domain-specific automatic metrics (e.g., fact-consistency, atomic-fact scoring) and use multiple complementary metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9689.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-in-the-loop / expert human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual evaluation where human annotators or domain experts rate outputs on criteria such as accuracy, relevance, fluency, transparency, safety, and human-alignment; used for openended generation and high-stakes domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various LLMs (GPT-3/3.5/4, Claude, Bard, Vicuna, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Large language models evaluated via human raters for nuanced judgments beyond automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / domain-specific (medical, legal, scientific research)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Crowdsourcing or expert panels score model outputs against rubrics (accuracy, relevance, fluency, transparency, safety, human alignment); sometimes use Elo or win-rate comparisons (Chatbot Arena, MT-Bench).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy, relevance, fluency, transparency, safety, human alignment; evaluator expertise and number of evaluators are emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied in Chatbot Arena, MT-Bench, domain studies (USMLE evaluations), and many case studies in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Human evaluation is necessary for open-ended and safety-critical assessments and reveals strengths (fluency, helpfulness) and weaknesses (hallucination, reasoning gaps) of LLMs; survey notes high variance and cultural/individual differences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High cost, evaluator bias/variance, difficulty scaling, and need for domain expertise for scientific-theory judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human evaluation is closer to traditional scientific peer review but lacks standardized rubrics across studies; necessary for assessing explanatory power and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Define clear rubrics (accuracy, novelty, explanatory power), recruit domain experts for scientific-theory evaluation, ensure adequate evaluator counts and training, and combine with automated pre-filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9689.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-EVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-EVAL (Unified Multi-Dimensional Automatic Evaluation for Conversations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic, multidimensional evaluator framework for open-domain conversations that uses LLMs themselves to score aspects of conversational quality, enabling reproducible automated judging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLM-based evaluators (LLM-as-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Approach uses large LMs to act as evaluators/judges (black-box) trained or prompted to score quality dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / general language generation; applicable to scientific-theory text generation evaluation as an automated judge proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt an LLM to score outputs along predefined dimensions (helpfulness, relevance, factuality, etc.) using structured prompts or scoring templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Multidimensional scores (helpfulness, coherence, factuality, style), reproducibility and multi-aspect automatic scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used for open-domain conversation datasets and as an automated judge in model comparison setups.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM-based evaluators can reproduce many human judgments and enable scalable automated scoring, but may inherit model biases and be unreliable without calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM evaluators can be biased toward models with similar training, may be overconfident, and are themselves subject to hallucination or miscalibration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Offers cheaper, scalable approximations to human evaluation but should be validated against expert raters; not a substitute for expert assessment of scientific theory novelty or correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Validate LLM-evaluators against human experts, calibrate scores, and use them as triage tools rather than final arbiters for scientific-theory evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9689.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PandaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PandaLM (Automatic evaluation model for instruction tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative LLM trained to judge and rank outputs from instruction-tuned LLMs across subjective dimensions like conciseness, clarity, and instruction following, enabling automated fine-tuning evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>PandaLM (an evaluation LLM) and target instruction-tuned LLMs (e.g., Vicuna, Alpaca variants)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>PandaLM is trained as a judge model to provide winrates or rankings among candidate LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General language generation and instruction-following; can be applied to evaluating generated scientific hypotheses or theories via relative ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Train or fine-tune an LLM classifier/judge on human preference data to output pairwise win rates or scores for candidate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relative win-rate, adherence to instructions, conciseness, clarity, comprehensiveness, formality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>PandaLM training uses human preference data and instruction-following datasets to learn evaluation behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports PandaLM offers reproducible automated judgments useful for tuning; it emphasizes subjective qualities beyond strict factual correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May replicate human annotator biases present in training data and is domain-dependent; not a full replacement for expert scientific evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides automated approximation to pairwise human preference tests; effective for scaling but requires human-labeled data for calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use PandaLM-like judges for large-scale tuning pipelines and validate against domain experts for scientific-theory judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9689.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PromptBench (adversarial prompt robustness benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified benchmark to evaluate LLM robustness against adversarial prompts across multiple perturbation levels (character, word, sentence, semantics), exposing sensitivity to prompt changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various LLMs (ChatGPT, GPT-3.5, GPT-4, open models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Evaluated LLMs under adversarial prompt transformations to probe robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General language tasks; relevant for evaluating robustness of LLM-generated scientific theories to prompt phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Apply adversarial transformations to prompts and measure performance degradation using task-specific metrics and unified robustness metrics (e.g., Performance Drop Rate).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Attack Success Rate (ASR), Performance Drop Rate (PDR), task-specific accuracy declines.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Adversarial prompt sets at multiple granularity levels constructed to test prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports contemporary LLMs are vulnerable to adversarial prompts; PromptBench demonstrates significant drops in performance under attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Adversarially constructed tests may be unfair as benchmarks if attackers optimize specifically; risk of overfitting evaluation to particular attack types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Adversarial prompting is a more challenging setting than standard test sets and reveals brittleness not captured by conventional metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include adversarial and paraphrase robustness tests when evaluating theory-generation pipelines; report relative PDR/ASR alongside absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9689.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factuality evaluation (TruthfulQA / FActScore / MQAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factuality and fact-consistency evaluation methods (TruthfulQA, FActScore, MQAG, BERTScore variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of datasets and metrics designed to probe and measure factual correctness and hallucination in LLM outputs, including adversarial datasets (TruthfulQA) and atomic-fact based scoring (FActScore).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5, GPT-4, InstructGPT, BingChat and other LLMs evaluated for factuality</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>LLMs assessed for capacity to recall verifiable knowledge and avoid fabrications; some models reach >80% on certain QA datasets per survey.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General factual domains and domain-specific knowledge (medical, science) important for scientific-theory validity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use adversarial truthfulness probes (TruthfulQA), decompose outputs into atomic facts and check correctness (FActScore), use QA-based or NLI-based fact-consistency checks (MQAG, BERTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary factual conflict labels, F1 on atomic facts, sentence-level factual consistency, probability-based uncertainty measures.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>TruthfulQA dataset, FActScore tasks derived from generated text, MQAG, Natural Questions and TriviaQA used for internal knowledge tests.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey notes GPT-4 and BingChat provide correct answers >80% on some QA datasets; however, factuality estimators still imperfect and hallucination remains a major issue.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Factuality metrics often depend on external knowledge, can be brittle, and no unified comparison framework exists; LLM estimators can be unreliable for black-box models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human evaluation remains gold standard for factuality in complex domains; automatic factuality tests are complementary but incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multi-pronged factuality evaluation (QA, atomic-fact checking, NLI), combine with external retrieval or tool use to ground theories, and employ human verification for high-stakes claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9689.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration metrics (ECE / AUC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calibration measures: Expected Calibration Error (ECE) and selective accuracy AUC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics that quantify whether a model's confidence estimates match actual correctness (ECE) and the trade-off between selective coverage and accuracy (AUC for selective classification).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On calibration of modern neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various LLMs including RLHF-tuned models (ChatGPT, GPT-4, Claude 1/2, Llama2)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models whose output probabilities/confidence are analyzed for calibration with ECE and selective-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General evaluation of predictive confidence across domains; relevant to trusting scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Bin predictions by reported/model confidence, compute difference between average confidence and accuracy per bin (ECE); compute AUC over selective accuracy-coverage curve.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>ECE (lower is better), AUC of selective accuracy vs coverage (higher is better).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to standard held-out evaluation sets and to RLHF-LM outputs in the survey's referenced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey notes ECE used to study calibration of RLHF models; calibration varies across models and affects trust in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLMs often miscalibrated, especially for open-ended generation; confidence estimates may be poor proxies for truthfulness of scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Calibration metrics provide a statistical measure of self-knowledge not present in traditional qualitative peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report calibration alongside accuracy; consider calibration-aware evaluation when assessing model-generated theories and when using model confidences to triage claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9689.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fairness metrics (DPD / EOD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fairness metrics: Demographic Parity Difference and Equalized Odds Difference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Group-based fairness measures assessing whether model predictions are distributed evenly across demographic groups (DPD) and whether error rates are similar across groups (EOD).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Equality of opportunity in supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLMs evaluated for bias and fairness (GPT family, ChatGPT, other foundation models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Large models whose outputs may reflect biases from training data; fairness metrics quantify disparate treatment/effects.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences, ethics, and any domain where demographic fairness matters; relevant to societal evaluation of scientific claims and theory dissemination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute probability differences of positive predictions across groups (DPD) and differences in true/false positive rates across groups (EOD) using labeled test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Demographic parity difference magnitude, equalized odds difference magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used in DecodingTrust and other fairness evaluations cited in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey highlights systematic biases and ethical risks in LLMs; fairness metrics used to quantify these harms and vulnerabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires well-specified demographic labels and ground-truth; not directly informative about scientific-theory correctness but crucial for equitable deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Traditional peer review rarely quantifies group-level biases; fairness metrics provide quantitative monitoring for LLM outputs disseminating scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include fairness audits when LLM outputs will target or affect demographic groups; report DPD/EOD for deployed systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9689.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATH (Mathematical problem solving benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark focused on evaluating models' mathematical reasoning and problem-solving abilities across grade-school and competition-style problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring mathematical problem solving with the MATH dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT family (GPT-3, GPT-3.5, GPT-4), PaLM, other LLMs evaluated on math tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Autoregressive LLMs tested for arithmetic, algebra, multi-step reasoning, and competition-level problems.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics and mathematical reasoning; a proxy for models' structured reasoning useful when evaluating formal scientific-theory derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measure exact-match correctness on math problems; sometimes broken down by sub-skills (algebra, calculus).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy / Exact Match, error rates by difficulty level and topic.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>MATH dataset containing competition-style math problems with ground-truth solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey reports GPT-4 substantially outperforms prior models on many math tasks but still struggles on high-complexity problems; performance declines with increasing cognitive complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Math benchmarks test symbolic manipulation and exactness, but may not capture plausibility or explanatory insight of scientific theories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides objective scoring similar to graded math exams; less applicable to open-ended theory novelty evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use math benchmarks to validate formal reasoning components of theory-generation pipelines and combine with human verification for conceptual novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9689.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HELM / BIG-bench / MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Holistic and multi-task benchmarks (HELM, BIG-bench, MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale, multi-task benchmark collections designed to probe LLM capabilities across many domains (HELM holistically, BIG-bench for hard tasks, MMLU for multitask accuracy across subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Holistic evaluation of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Large foundation models (GPT variants, PaLM, LLaMA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models evaluated across a spectrum of tasks to characterize wide-ranging abilities and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-disciplinary; includes science, math, humanities  useful for assessing broad competence needed for scientific theorizing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregate task-specific metrics into multi-metric evaluations; include accuracy, robustness, calibration and often human-evaluated dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Multi-metric scores (accuracy, calibration, robustness), per-domain breakdowns and aggregate summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>HELM suite, BIG-bench task collection, MMLU exam-style datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey highlights these benchmarks as standard references showing emergent abilities (e.g., GPT-4 strong across many tasks), but warns against over-reliance on static benchmarks due to memorization and contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Static, publicly-known datasets risk contamination and memorization; single aggregated metrics obscure task-specific failure modes critical for scientific-theory validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>These benchmarks provide broad quantitative baselines akin to multi-discipline exam batteries; human expert review remains necessary for theory novelty and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Complement holistic benchmarks with dynamic, domain-specific tests and human expert evaluation when assessing generated scientific theories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9689.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic / Crowdsourced evaluation (DynaBench)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DynaBench (dynamic, crowd-sourced benchmark platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform for continuously updated benchmarks that uses crowd-workers and model-in-the-loop data collection to create challenging, evolving test sets resistant to memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynabench: Rethinking benchmarking in NLP</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various LLMs evaluated iteratively on dynamically collected hard examples</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models are evaluated on data collected/adapted by humans in response to model outputs to reveal real-world weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General; particularly useful for evolving, adversarial, or domain-specific scientific evaluation where static tests fail.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human-in-the-loop sample generation and annotation to produce fresh, challenging evaluation examples; continual re-evaluation as models improve.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific accuracy under distribution shift, difficulty of examples, adversarial robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>DynaBench platform and datasets produced by it across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey emphasizes dynamic benchmarks address contamination and provide harder tests, better exposing failure modes as models improve.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires continuous human effort and infrastructure; potential inconsistencies in difficulty and annotation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>More similar to adversarial or peer-review stress-testing than static standardized testing; aligns well with iterative scientific critique.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Adopt dynamic evaluation when validating LLM-generated scientific theories to avoid memorization and to simulate critical peer scrutiny.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9689.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9689.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-augmented evaluation (API-Bank / ToolBench)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmarks for tool-augmented LLMs (API-Bank, ToolBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation suites that test LLMs' ability to use external tools (APIs, calculators, search engines), measuring execution success, planning and tool selection  critical for grounded scientific reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>API-Bank: A Benchmark for Tool-Augmented LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Tool-augmented LLMs (Toolformer, ToolLLM, ToolBench setups) and standard LLMs connected to APIs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models augmented with tool use capabilities (call external APIs, retrieval, calculators) to improve factuality and computation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific domains requiring retrieval, computation, or simulation (physics, chemistry, biology, engineering).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measure API call correctness, planning, retrieval success, end-to-end execution success rate, and correctness of outputs when tools are used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Execution success rate, correctness of tool-augmented outputs, planning accuracy, reduction in hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>API-Bank (53 APIs, 264 annotated dialogues), ToolBench projects testing tool use workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Survey notes tool-augmented evaluations show promise for improving factual grounding and complex computation (math, code), but tool use introduces new failure modes (incorrect calls, misinterpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tool integration complexity, evaluation of intermediate planning steps, and dependence on tool reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>More analogous to equipping a scientist with lab equipment; evaluation should measure both reasoning and correct tool operation; human oversight remains essential for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When assessing LLM-generated scientific theories, evaluate both the reasoning chain and tool usage correctness; include tool-level unit tests and end-to-end validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Evaluation of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models <em>(Rating: 2)</em></li>
                <li>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization <em>(Rating: 2)</em></li>
                <li>PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring how models mimic human falsehoods <em>(Rating: 2)</em></li>
                <li>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>Dynabench: Rethinking benchmarking in NLP <em>(Rating: 2)</em></li>
                <li>API-Bank: A Benchmark for Tool-Augmented LLMs <em>(Rating: 2)</em></li>
                <li>On calibration of modern neural networks <em>(Rating: 1)</em></li>
                <li>Equality of opportunity in supervised learning <em>(Rating: 1)</em></li>
                <li>PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9689",
    "paper_id": "paper-259360395",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Automatic evaluation",
            "name_full": "Automated evaluation (metrics-based)",
            "brief_description": "Evaluation approach that computes task-specific metrics (accuracy, ROUGE, F1, BLEU, calibration, robustness) automatically without human raters; widely used for large-scale LLM benchmarking and reproducible comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": "Various LLMs (e.g., GPT-3, GPT-3.5, GPT-4, Claude, LLaMA families)",
            "llm_description": "General-purpose large pre-trained transformer models of various sizes and training regimes; used across many benchmarks for zero-shot, few-shot, and fine-tuned evaluations.",
            "scientific_domain": "General / cross-domain (NLP, reasoning, math, science, medicine)",
            "evaluation_method": "Compute automated metrics (Exact Match, F1, ROUGE, BLEU, calibration scores, attack success rate, performance drop rate) on held-out benchmark datasets or challenge sets.",
            "evaluation_criteria": "Accuracy (Exact Match, F1), generation overlap (ROUGE/BLEU), calibration (ECE/AUC), robustness (ASR/PDR), fairness metrics where applicable.",
            "benchmark_or_dataset": "Used by MMLU, MATH, HELM, C-Eval, AlpacaEval, many task-specific datasets listed in survey.",
            "results_summary": "Survey reports that automated evaluation is the most common method, enabling large-scale comparisons (e.g., MMLU, MATH). Automated metrics are efficient but often insufficient for open-ended or subjective scientific-theory outputs.",
            "limitations_or_challenges": "Automated metrics often miss plausibility, novelty, explanatory power and can be gamed by surface overlap; they cannot reliably measure scientific-theory quality or hallucination without specialized probes or human judgment.",
            "comparison_to_human_or_traditional": "Automated evaluation is scalable but less reliable than human expert review for assessing deep correctness, novelty, and explanatory value of scientific theories.",
            "recommendations_or_best_practices": "Combine automated metrics with targeted probes and human evaluation; design domain-specific automatic metrics (e.g., fact-consistency, atomic-fact scoring) and use multiple complementary metrics.",
            "uuid": "e9689.0",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Human evaluation",
            "name_full": "Human-in-the-loop / expert human evaluation",
            "brief_description": "Manual evaluation where human annotators or domain experts rate outputs on criteria such as accuracy, relevance, fluency, transparency, safety, and human-alignment; used for openended generation and high-stakes domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": "Various LLMs (GPT-3/3.5/4, Claude, Bard, Vicuna, etc.)",
            "llm_description": "Large language models evaluated via human raters for nuanced judgments beyond automated metrics.",
            "scientific_domain": "General / domain-specific (medical, legal, scientific research)",
            "evaluation_method": "Crowdsourcing or expert panels score model outputs against rubrics (accuracy, relevance, fluency, transparency, safety, human alignment); sometimes use Elo or win-rate comparisons (Chatbot Arena, MT-Bench).",
            "evaluation_criteria": "Accuracy, relevance, fluency, transparency, safety, human alignment; evaluator expertise and number of evaluators are emphasized.",
            "benchmark_or_dataset": "Applied in Chatbot Arena, MT-Bench, domain studies (USMLE evaluations), and many case studies in the survey.",
            "results_summary": "Human evaluation is necessary for open-ended and safety-critical assessments and reveals strengths (fluency, helpfulness) and weaknesses (hallucination, reasoning gaps) of LLMs; survey notes high variance and cultural/individual differences.",
            "limitations_or_challenges": "High cost, evaluator bias/variance, difficulty scaling, and need for domain expertise for scientific-theory judgments.",
            "comparison_to_human_or_traditional": "Human evaluation is closer to traditional scientific peer review but lacks standardized rubrics across studies; necessary for assessing explanatory power and novelty.",
            "recommendations_or_best_practices": "Define clear rubrics (accuracy, novelty, explanatory power), recruit domain experts for scientific-theory evaluation, ensure adequate evaluator counts and training, and combine with automated pre-filtering.",
            "uuid": "e9689.1",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLM-EVAL",
            "name_full": "LLM-EVAL (Unified Multi-Dimensional Automatic Evaluation for Conversations)",
            "brief_description": "An automatic, multidimensional evaluator framework for open-domain conversations that uses LLMs themselves to score aspects of conversational quality, enabling reproducible automated judging.",
            "citation_title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
            "mention_or_use": "mention",
            "llm_name": "LLM-based evaluators (LLM-as-judge)",
            "llm_description": "Approach uses large LMs to act as evaluators/judges (black-box) trained or prompted to score quality dimensions.",
            "scientific_domain": "Dialogue / general language generation; applicable to scientific-theory text generation evaluation as an automated judge proxy.",
            "evaluation_method": "Prompt an LLM to score outputs along predefined dimensions (helpfulness, relevance, factuality, etc.) using structured prompts or scoring templates.",
            "evaluation_criteria": "Multidimensional scores (helpfulness, coherence, factuality, style), reproducibility and multi-aspect automatic scoring.",
            "benchmark_or_dataset": "Used for open-domain conversation datasets and as an automated judge in model comparison setups.",
            "results_summary": "LLM-based evaluators can reproduce many human judgments and enable scalable automated scoring, but may inherit model biases and be unreliable without calibration.",
            "limitations_or_challenges": "LLM evaluators can be biased toward models with similar training, may be overconfident, and are themselves subject to hallucination or miscalibration.",
            "comparison_to_human_or_traditional": "Offers cheaper, scalable approximations to human evaluation but should be validated against expert raters; not a substitute for expert assessment of scientific theory novelty or correctness.",
            "recommendations_or_best_practices": "Validate LLM-evaluators against human experts, calibrate scores, and use them as triage tools rather than final arbiters for scientific-theory evaluation.",
            "uuid": "e9689.2",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PandaLM",
            "name_full": "PandaLM (Automatic evaluation model for instruction tuning)",
            "brief_description": "A discriminative LLM trained to judge and rank outputs from instruction-tuned LLMs across subjective dimensions like conciseness, clarity, and instruction following, enabling automated fine-tuning evaluation.",
            "citation_title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
            "mention_or_use": "mention",
            "llm_name": "PandaLM (an evaluation LLM) and target instruction-tuned LLMs (e.g., Vicuna, Alpaca variants)",
            "llm_description": "PandaLM is trained as a judge model to provide winrates or rankings among candidate LLM outputs.",
            "scientific_domain": "General language generation and instruction-following; can be applied to evaluating generated scientific hypotheses or theories via relative ranking.",
            "evaluation_method": "Train or fine-tune an LLM classifier/judge on human preference data to output pairwise win rates or scores for candidate responses.",
            "evaluation_criteria": "Relative win-rate, adherence to instructions, conciseness, clarity, comprehensiveness, formality.",
            "benchmark_or_dataset": "PandaLM training uses human preference data and instruction-following datasets to learn evaluation behaviour.",
            "results_summary": "Survey reports PandaLM offers reproducible automated judgments useful for tuning; it emphasizes subjective qualities beyond strict factual correctness.",
            "limitations_or_challenges": "May replicate human annotator biases present in training data and is domain-dependent; not a full replacement for expert scientific evaluation.",
            "comparison_to_human_or_traditional": "Provides automated approximation to pairwise human preference tests; effective for scaling but requires human-labeled data for calibration.",
            "recommendations_or_best_practices": "Use PandaLM-like judges for large-scale tuning pipelines and validate against domain experts for scientific-theory judgments.",
            "uuid": "e9689.3",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PromptBench",
            "name_full": "PromptBench (adversarial prompt robustness benchmark)",
            "brief_description": "A unified benchmark to evaluate LLM robustness against adversarial prompts across multiple perturbation levels (character, word, sentence, semantics), exposing sensitivity to prompt changes.",
            "citation_title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "mention_or_use": "mention",
            "llm_name": "Various LLMs (ChatGPT, GPT-3.5, GPT-4, open models)",
            "llm_description": "Evaluated LLMs under adversarial prompt transformations to probe robustness.",
            "scientific_domain": "General language tasks; relevant for evaluating robustness of LLM-generated scientific theories to prompt phrasing.",
            "evaluation_method": "Apply adversarial transformations to prompts and measure performance degradation using task-specific metrics and unified robustness metrics (e.g., Performance Drop Rate).",
            "evaluation_criteria": "Attack Success Rate (ASR), Performance Drop Rate (PDR), task-specific accuracy declines.",
            "benchmark_or_dataset": "Adversarial prompt sets at multiple granularity levels constructed to test prompt sensitivity.",
            "results_summary": "Survey reports contemporary LLMs are vulnerable to adversarial prompts; PromptBench demonstrates significant drops in performance under attacks.",
            "limitations_or_challenges": "Adversarially constructed tests may be unfair as benchmarks if attackers optimize specifically; risk of overfitting evaluation to particular attack types.",
            "comparison_to_human_or_traditional": "Adversarial prompting is a more challenging setting than standard test sets and reveals brittleness not captured by conventional metrics.",
            "recommendations_or_best_practices": "Include adversarial and paraphrase robustness tests when evaluating theory-generation pipelines; report relative PDR/ASR alongside absolute performance.",
            "uuid": "e9689.4",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Factuality evaluation (TruthfulQA / FActScore / MQAG)",
            "name_full": "Factuality and fact-consistency evaluation methods (TruthfulQA, FActScore, MQAG, BERTScore variants)",
            "brief_description": "Set of datasets and metrics designed to probe and measure factual correctness and hallucination in LLM outputs, including adversarial datasets (TruthfulQA) and atomic-fact based scoring (FActScore).",
            "citation_title": "",
            "mention_or_use": "mention",
            "llm_name": "GPT-3.5, GPT-4, InstructGPT, BingChat and other LLMs evaluated for factuality",
            "llm_description": "LLMs assessed for capacity to recall verifiable knowledge and avoid fabrications; some models reach &gt;80% on certain QA datasets per survey.",
            "scientific_domain": "General factual domains and domain-specific knowledge (medical, science) important for scientific-theory validity.",
            "evaluation_method": "Use adversarial truthfulness probes (TruthfulQA), decompose outputs into atomic facts and check correctness (FActScore), use QA-based or NLI-based fact-consistency checks (MQAG, BERTScore).",
            "evaluation_criteria": "Binary factual conflict labels, F1 on atomic facts, sentence-level factual consistency, probability-based uncertainty measures.",
            "benchmark_or_dataset": "TruthfulQA dataset, FActScore tasks derived from generated text, MQAG, Natural Questions and TriviaQA used for internal knowledge tests.",
            "results_summary": "Survey notes GPT-4 and BingChat provide correct answers &gt;80% on some QA datasets; however, factuality estimators still imperfect and hallucination remains a major issue.",
            "limitations_or_challenges": "Factuality metrics often depend on external knowledge, can be brittle, and no unified comparison framework exists; LLM estimators can be unreliable for black-box models.",
            "comparison_to_human_or_traditional": "Human evaluation remains gold standard for factuality in complex domains; automatic factuality tests are complementary but incomplete.",
            "recommendations_or_best_practices": "Use multi-pronged factuality evaluation (QA, atomic-fact checking, NLI), combine with external retrieval or tool use to ground theories, and employ human verification for high-stakes claims.",
            "uuid": "e9689.5",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Calibration metrics (ECE / AUC)",
            "name_full": "Calibration measures: Expected Calibration Error (ECE) and selective accuracy AUC",
            "brief_description": "Metrics that quantify whether a model's confidence estimates match actual correctness (ECE) and the trade-off between selective coverage and accuracy (AUC for selective classification).",
            "citation_title": "On calibration of modern neural networks",
            "mention_or_use": "mention",
            "llm_name": "Various LLMs including RLHF-tuned models (ChatGPT, GPT-4, Claude 1/2, Llama2)",
            "llm_description": "Models whose output probabilities/confidence are analyzed for calibration with ECE and selective-AUC.",
            "scientific_domain": "General evaluation of predictive confidence across domains; relevant to trusting scientific claims.",
            "evaluation_method": "Bin predictions by reported/model confidence, compute difference between average confidence and accuracy per bin (ECE); compute AUC over selective accuracy-coverage curve.",
            "evaluation_criteria": "ECE (lower is better), AUC of selective accuracy vs coverage (higher is better).",
            "benchmark_or_dataset": "Applied to standard held-out evaluation sets and to RLHF-LM outputs in the survey's referenced studies.",
            "results_summary": "Survey notes ECE used to study calibration of RLHF models; calibration varies across models and affects trust in outputs.",
            "limitations_or_challenges": "LLMs often miscalibrated, especially for open-ended generation; confidence estimates may be poor proxies for truthfulness of scientific claims.",
            "comparison_to_human_or_traditional": "Calibration metrics provide a statistical measure of self-knowledge not present in traditional qualitative peer review.",
            "recommendations_or_best_practices": "Report calibration alongside accuracy; consider calibration-aware evaluation when assessing model-generated theories and when using model confidences to triage claims.",
            "uuid": "e9689.6",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Fairness metrics (DPD / EOD)",
            "name_full": "Fairness metrics: Demographic Parity Difference and Equalized Odds Difference",
            "brief_description": "Group-based fairness measures assessing whether model predictions are distributed evenly across demographic groups (DPD) and whether error rates are similar across groups (EOD).",
            "citation_title": "Equality of opportunity in supervised learning",
            "mention_or_use": "mention",
            "llm_name": "LLMs evaluated for bias and fairness (GPT family, ChatGPT, other foundation models)",
            "llm_description": "Large models whose outputs may reflect biases from training data; fairness metrics quantify disparate treatment/effects.",
            "scientific_domain": "Social sciences, ethics, and any domain where demographic fairness matters; relevant to societal evaluation of scientific claims and theory dissemination.",
            "evaluation_method": "Compute probability differences of positive predictions across groups (DPD) and differences in true/false positive rates across groups (EOD) using labeled test sets.",
            "evaluation_criteria": "Demographic parity difference magnitude, equalized odds difference magnitude.",
            "benchmark_or_dataset": "Used in DecodingTrust and other fairness evaluations cited in survey.",
            "results_summary": "Survey highlights systematic biases and ethical risks in LLMs; fairness metrics used to quantify these harms and vulnerabilities.",
            "limitations_or_challenges": "Requires well-specified demographic labels and ground-truth; not directly informative about scientific-theory correctness but crucial for equitable deployment.",
            "comparison_to_human_or_traditional": "Traditional peer review rarely quantifies group-level biases; fairness metrics provide quantitative monitoring for LLM outputs disseminating scientific claims.",
            "recommendations_or_best_practices": "Include fairness audits when LLM outputs will target or affect demographic groups; report DPD/EOD for deployed systems.",
            "uuid": "e9689.7",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "MATH benchmark",
            "name_full": "MATH (Mathematical problem solving benchmark)",
            "brief_description": "Benchmark focused on evaluating models' mathematical reasoning and problem-solving abilities across grade-school and competition-style problems.",
            "citation_title": "Measuring mathematical problem solving with the MATH dataset",
            "mention_or_use": "mention",
            "llm_name": "GPT family (GPT-3, GPT-3.5, GPT-4), PaLM, other LLMs evaluated on math tasks",
            "llm_description": "Autoregressive LLMs tested for arithmetic, algebra, multi-step reasoning, and competition-level problems.",
            "scientific_domain": "Mathematics and mathematical reasoning; a proxy for models' structured reasoning useful when evaluating formal scientific-theory derivations.",
            "evaluation_method": "Measure exact-match correctness on math problems; sometimes broken down by sub-skills (algebra, calculus).",
            "evaluation_criteria": "Accuracy / Exact Match, error rates by difficulty level and topic.",
            "benchmark_or_dataset": "MATH dataset containing competition-style math problems with ground-truth solutions.",
            "results_summary": "Survey reports GPT-4 substantially outperforms prior models on many math tasks but still struggles on high-complexity problems; performance declines with increasing cognitive complexity.",
            "limitations_or_challenges": "Math benchmarks test symbolic manipulation and exactness, but may not capture plausibility or explanatory insight of scientific theories.",
            "comparison_to_human_or_traditional": "Provides objective scoring similar to graded math exams; less applicable to open-ended theory novelty evaluation.",
            "recommendations_or_best_practices": "Use math benchmarks to validate formal reasoning components of theory-generation pipelines and combine with human verification for conceptual novelty.",
            "uuid": "e9689.8",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "HELM / BIG-bench / MMLU",
            "name_full": "Holistic and multi-task benchmarks (HELM, BIG-bench, MMLU)",
            "brief_description": "Large-scale, multi-task benchmark collections designed to probe LLM capabilities across many domains (HELM holistically, BIG-bench for hard tasks, MMLU for multitask accuracy across subjects).",
            "citation_title": "Holistic evaluation of language models",
            "mention_or_use": "mention",
            "llm_name": "Large foundation models (GPT variants, PaLM, LLaMA, etc.)",
            "llm_description": "Models evaluated across a spectrum of tasks to characterize wide-ranging abilities and limitations.",
            "scientific_domain": "Cross-disciplinary; includes science, math, humanities  useful for assessing broad competence needed for scientific theorizing.",
            "evaluation_method": "Aggregate task-specific metrics into multi-metric evaluations; include accuracy, robustness, calibration and often human-evaluated dimensions.",
            "evaluation_criteria": "Multi-metric scores (accuracy, calibration, robustness), per-domain breakdowns and aggregate summaries.",
            "benchmark_or_dataset": "HELM suite, BIG-bench task collection, MMLU exam-style datasets.",
            "results_summary": "Survey highlights these benchmarks as standard references showing emergent abilities (e.g., GPT-4 strong across many tasks), but warns against over-reliance on static benchmarks due to memorization and contamination.",
            "limitations_or_challenges": "Static, publicly-known datasets risk contamination and memorization; single aggregated metrics obscure task-specific failure modes critical for scientific-theory validation.",
            "comparison_to_human_or_traditional": "These benchmarks provide broad quantitative baselines akin to multi-discipline exam batteries; human expert review remains necessary for theory novelty and correctness.",
            "recommendations_or_best_practices": "Complement holistic benchmarks with dynamic, domain-specific tests and human expert evaluation when assessing generated scientific theories.",
            "uuid": "e9689.9",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Dynamic / Crowdsourced evaluation (DynaBench)",
            "name_full": "DynaBench (dynamic, crowd-sourced benchmark platform)",
            "brief_description": "A platform for continuously updated benchmarks that uses crowd-workers and model-in-the-loop data collection to create challenging, evolving test sets resistant to memorization.",
            "citation_title": "Dynabench: Rethinking benchmarking in NLP",
            "mention_or_use": "mention",
            "llm_name": "Various LLMs evaluated iteratively on dynamically collected hard examples",
            "llm_description": "Models are evaluated on data collected/adapted by humans in response to model outputs to reveal real-world weaknesses.",
            "scientific_domain": "General; particularly useful for evolving, adversarial, or domain-specific scientific evaluation where static tests fail.",
            "evaluation_method": "Human-in-the-loop sample generation and annotation to produce fresh, challenging evaluation examples; continual re-evaluation as models improve.",
            "evaluation_criteria": "Task-specific accuracy under distribution shift, difficulty of examples, adversarial robustness.",
            "benchmark_or_dataset": "DynaBench platform and datasets produced by it across tasks.",
            "results_summary": "Survey emphasizes dynamic benchmarks address contamination and provide harder tests, better exposing failure modes as models improve.",
            "limitations_or_challenges": "Requires continuous human effort and infrastructure; potential inconsistencies in difficulty and annotation quality.",
            "comparison_to_human_or_traditional": "More similar to adversarial or peer-review stress-testing than static standardized testing; aligns well with iterative scientific critique.",
            "recommendations_or_best_practices": "Adopt dynamic evaluation when validating LLM-generated scientific theories to avoid memorization and to simulate critical peer scrutiny.",
            "uuid": "e9689.10",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Tool-augmented evaluation (API-Bank / ToolBench)",
            "name_full": "Benchmarks for tool-augmented LLMs (API-Bank, ToolBench)",
            "brief_description": "Evaluation suites that test LLMs' ability to use external tools (APIs, calculators, search engines), measuring execution success, planning and tool selection  critical for grounded scientific reasoning.",
            "citation_title": "API-Bank: A Benchmark for Tool-Augmented LLMs",
            "mention_or_use": "mention",
            "llm_name": "Tool-augmented LLMs (Toolformer, ToolLLM, ToolBench setups) and standard LLMs connected to APIs",
            "llm_description": "Models augmented with tool use capabilities (call external APIs, retrieval, calculators) to improve factuality and computation.",
            "scientific_domain": "Scientific domains requiring retrieval, computation, or simulation (physics, chemistry, biology, engineering).",
            "evaluation_method": "Measure API call correctness, planning, retrieval success, end-to-end execution success rate, and correctness of outputs when tools are used.",
            "evaluation_criteria": "Execution success rate, correctness of tool-augmented outputs, planning accuracy, reduction in hallucination.",
            "benchmark_or_dataset": "API-Bank (53 APIs, 264 annotated dialogues), ToolBench projects testing tool use workflows.",
            "results_summary": "Survey notes tool-augmented evaluations show promise for improving factual grounding and complex computation (math, code), but tool use introduces new failure modes (incorrect calls, misinterpretation).",
            "limitations_or_challenges": "Tool integration complexity, evaluation of intermediate planning steps, and dependence on tool reliability.",
            "comparison_to_human_or_traditional": "More analogous to equipping a scientist with lab equipment; evaluation should measure both reasoning and correct tool operation; human oversight remains essential for scientific claims.",
            "recommendations_or_best_practices": "When assessing LLM-generated scientific theories, evaluate both the reasoning chain and tool usage correctness; include tool-level unit tests and end-to-end validation.",
            "uuid": "e9689.11",
            "source_info": {
                "paper_title": "A Survey on Evaluation of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
            "rating": 2,
            "sanitized_title": "llmeval_unified_multidimensional_automatic_evaluation_for_opendomain_conversations_with_large_language_models"
        },
        {
            "paper_title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
            "rating": 2,
            "sanitized_title": "pandalm_an_automatic_evaluation_benchmark_for_llm_instruction_tuning_optimization"
        },
        {
            "paper_title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "rating": 2,
            "sanitized_title": "promptbench_towards_evaluating_the_robustness_of_large_language_models_on_adversarial_prompts"
        },
        {
            "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "rating": 2,
            "sanitized_title": "truthfulqa_measuring_how_models_mimic_human_falsehoods"
        },
        {
            "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
            "rating": 2,
            "sanitized_title": "factscore_finegrained_atomic_evaluation_of_factual_precision_in_long_form_text_generation"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2,
            "sanitized_title": "holistic_evaluation_of_language_models"
        },
        {
            "paper_title": "Dynabench: Rethinking benchmarking in NLP",
            "rating": 2,
            "sanitized_title": "dynabench_rethinking_benchmarking_in_nlp"
        },
        {
            "paper_title": "API-Bank: A Benchmark for Tool-Augmented LLMs",
            "rating": 2,
            "sanitized_title": "apibank_a_benchmark_for_toolaugmented_llms"
        },
        {
            "paper_title": "On calibration of modern neural networks",
            "rating": 1,
            "sanitized_title": "on_calibration_of_modern_neural_networks"
        },
        {
            "paper_title": "Equality of opportunity in supervised learning",
            "rating": 1,
            "sanitized_title": "equality_of_opportunity_in_supervised_learning"
        },
        {
            "paper_title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "rating": 1,
            "sanitized_title": "promptbench_towards_evaluating_the_robustness_of_large_language_models_on_adversarial_prompts"
        }
    ],
    "cost": 0.02475875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Evaluation of Large Language Models
29 Dec 2023</p>
<p>Yupeng Chang 
Xu U Wang 
Jindong Wang jindong.wang@microsoft.com 
Microsoft Research 
China Yuan Wu 
Linyi Yang 
Westlake University 
China Kaijie Zhu 
Hao Chen 
Y I Chang 
Philip S Yu 
Qiang Yang 
Hong Kong 
Xing Xie 
Yuan Wu yuanwu@jlu.edu.cn 
Xiaoyuan Yi 
Yi Chang 
Yupeng Chang 
Kaijie Zhu 
Cunxiang Wang 
Yidong Wang 
Wei Ye 
Yue Zhang 
Yi Chang </p>
<p>School of Artificial Intelligence
Jilin University
China</p>
<p>School of Artificial Intelligence
Jilin University
China</p>
<p>Institute of Automation
Chinese Academy of Sciences
China</p>
<p>Carnegie Mellon University
USA</p>
<p>XIAOYUAN YI
Microsoft Research Asia
China</p>
<p>Westlake University
China</p>
<p>School of Artificial Intelligence
Jilin University
China</p>
<p>University of Illinois at Chicago
USA</p>
<p>University of Science and Technology
China</p>
<p>Microsoft Research Asia
China</p>
<p>School of Artificial Intelligence
Jilin University
2699 Qianjin St, Jindong Wang130012ChangchunChina</p>
<p>Microsoft Research Asia
BeijingChina</p>
<p>School of Artificial Intelligence
Jilin University
ChangchunChina</p>
<p>Institute of Automation
Westlake University
Kaijie ZhuLinyi Yang, HangzhouChina</p>
<p>Chinese Academy of Sciences
BeijingChina</p>
<p>Carnegie Mellon University
PennsylvaniaUSA</p>
<p>Microsoft Research Asia
Cunxiang WangBeijingChina</p>
<p>Yidong Wang
Westlake University
HangzhouChina</p>
<p>Peking University
China; Wei YeBeijing</p>
<p>Peking University
Beijing, Yue ZhangChina</p>
<p>Westlake University
HangzhouChina</p>
<p>School of Artificial Intelligence
Jilin University
ChangchunChina</p>
<p>University of Illinois at Chicago
Illinois, Qiang YangUSA</p>
<p>Hong Kong University of Science and Technology
China; Xing XieKowloon, Hong Kong</p>
<p>Microsoft Research Asia
BeijingChina</p>
<p>A Survey on Evaluation of Large Language Models
29 Dec 20230E92C33C9157442F086C195528A6B78EarXiv:2307.03109v9[cs.CL]large language modelsevaluationmodel assessmentbenchmark
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications.As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks.Over the past years, significant efforts have been made to examine LLMs from various perspectives.This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate.Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas.Secondly, we answer the 'where' and 'how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs.Then, we summarize the success and failure cases of LLMs in different tasks.Finally, we shed light on several</p>
<p>INTRODUCTION</p>
<p>Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists.It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities [92].In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed to biologically based intellect [136].Proper measurement helps to understand intelligence.For instance, measures for general intelligence in human individuals often encompass IQ tests [12].</p>
<p>Within the scope of AI, the Turing Test [193], a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution.It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent.Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and algorithms.With each emergence of a novel AI model or algorithm, researchers invariably scrutinize its capabilities in real-world scenarios through evaluation using specific and challenging tasks.For instance, the Perceptron algorithm [49], touted as an Artificial General Intelligence (AGI) approach in the 1950s, was later revealed as inadequate due to its inability to resolve the XOR problem.The subsequent rise and application of Support Vector Machines (SVMs) [28] and deep learning [104] have marked both progress and setbacks in the AI landscape.A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models.</p>
<p>Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains [11,219,257].As demonstrated by existing work [15], the great performance of LLMs has raised promise that they could be AGI in this era.LLMs possess the capabilities to solve diverse tasks, contrasting with prior models confined to solving specific tasks.Due to its great performance in handling different applications such as general natural language tasks and domain-specific ones, LLMs are increasingly used by individuals with critical information needs, such as students or patients.</p>
<p>Evaluation is of paramount prominence to the success of LLMs due to several reasons.First, evaluating LLMs helps us better understand the strengths and weakness of LLMs.For instance, the PromptBench [264] benchmark illustrates that current LLMs are sensitive to adversarial prompts, thus a careful prompt engineering is necessary for better performance.Second, better evaluations can provide better guidance for human-LLMs interaction, which could inspire future interaction design and implementation.Third, the broad applicability of LLMs underscores the paramount</p>
<p>LLMs evaluation</p>
<p>What to evaluate (Sec.3)</p>
<p>Natural language processing</p>
<p>Natural language understanding: (1) Sentiment analysis: Bang et al. [6]/ Liang et al. [114]/ Lopez-Lira and Tang [129]/ Qin et al. [159]/ Wang et al. [218]/ Zhang et al. [251] (2) Text classification: Liang et al. [114] / Pea et al. [154] / Yang and Menczer [233] (3) Natural language inference: Lee et al. [105] / Qin et al. [159] (4) Others: Choi et al. [23] / Riccardi and Desai [166] / Tao et al. [184] Fig. 1.Structure of this paper.</p>
<p>importance of ensuring their safety and reliability, particularly in safety-sensitive sectors such as financial institutions and healthcare facilities.Finally, as LLMs are becoming larger with more emergent abilities, existing evaluation protocols may not be enough to evaluate their capabilities and potential risks.Therefore, we aim to raise awareness in the community of the importance to LLMs evaluations by reviewing the current evaluation protocols and most importantly, shed light on future research about designing new LLMs evaluation protocols.</p>
<p>With the introduction of ChatGPT [145] and , there have been a number of research efforts aiming at evaluating ChatGPT and other LLMs from different aspects (Figure 2), encompassing a range of factors such as natural language tasks, reasoning, robustness, trustworthiness, medical applications, and ethical considerations.Despite these efforts, a comprehensive overview capturing the entire gamut of evaluations is still lacking.Furthermore, the ongoing evolution of LLMs has also presented novel aspects for evaluation, thereby challenging existing evaluation protocols and reinforcing the need for thorough, multifaceted evaluation techniques.While existing research such as Bubeck et al. [15] claimed that GPT-4 can be seen as sparks of AGI, others contest this claim due to the human-crafted nature of its evaluation approach.</p>
<p>This paper serves as the first comprehensive survey on the evaluation of large language models.As depicted in Figure 1, we explore existing work in three dimensions: 1) What to evaluate, 2) Where to evaluate, and 3) How to evaluate.Specifically, "what to evaluate" encapsulates existing evaluation tasks for LLMs, "where to evaluate" involves selecting appropriate datasets and benchmarks for evaluation, while "how to evaluate" is concerned with the evaluation process given appropriate tasks and datasets.These three dimensions are integral to the evaluation of LLMs.We subsequently discuss potential future challenges in the realm of LLMs evaluation.</p>
<p>The contributions of this paper are as follows:</p>
<p>(1) We provide a comprehensive overview of LLMs evaluations from three aspects: what to evaluate, where to evaluate, and how to evaluate.Our categorization is general and encompasses the entire life cycle of LLMs evaluation.(2) Regarding what to evaluate, we summarize existing tasks in various areas and obtain insightful conclusions on the success and failure case of LLMs (Sec.6), providing experience for future research.(3) As for where to evaluate, we summarize evaluation metrics, datasets, and benchmarks to provide a profound understanding of current LLMs evaluations.In terms of how to evaluate, we explore current protocols and summarize novel evaluation approaches.(4) We further discuss future challenges in evaluating LLMs.We open-source and maintain the related materials of LLMs evaluation at https://github.com/MLGroupJLU/LLM-eval-survey to foster a collaborative community for better evaluations.</p>
<p>The paper is organized as follows.In Sec. 2, we provide the basic information of LLMs and AI model evaluation.Then, Sec. 3 reviews existing work from the aspects of "what to evaluate".After that, Sec. 4 is the "where to evaluate" part, which summarizes existing datasets and benchmarks.Sec. 5 discusses how to perform the evaluation.In Sec. 6, we summarize the key findings of this paper.We discuss grand future challenges in Sec.7 and Sec. 8 concludes the paper.</p>
<p>BACKGROUND</p>
<p>Large Language Models</p>
<p>Language models (LMs) [36,51,96] are computational models that have the capability to understand and generate human language.LMs have the transformative ability to predict the likelihood of word sequences or generate new text based on a given input.N-gram models [13], the most common type of LM, estimate word probabilities based on the preceding context.However, LMs also face challenges, such as the issue of rare or unseen words, the problem of overfitting, and the difficulty in capturing complex linguistic phenomena.Researchers are continuously working on improving LM architectures and training methods to address these challenges.</p>
<p>Large Language Models (LLMs) [19,91,257] are advanced language models with massive parameter sizes and exceptional learning capabilities.The core module behind many LLMs such as GPT-3 [43], InstructGPT [149], and  is the self-attention module in Transformer [197] J. ACM, Vol.37  that serves as the fundamental building block for language modeling tasks.Transformers have revolutionized the field of NLP with their ability to handle sequential data efficiently, allowing for parallelization and capturing long-range dependencies in text.One key feature of LLMs is in-context learning [14], where the model is trained to generate text based on a given context or prompt.This enables LLMs to generate more coherent and contextually relevant responses, making them suitable for interactive and conversational applications.Reinforcement Learning from Human Feedback (RLHF) [25,268] is another crucial aspect of LLMs.This technique involves fine-tuning the model using human-generated responses as rewards, allowing the model to learn from its mistakes and improve its performance over time.</p>
<p>In an autoregressive language model, such as GPT-3 and PaLM [24], given a context sequence  , the LM tasks aim to predict the next token .The model is trained by maximizing the probability of the given token sequence conditioned on the context, i.e.,  (| ) =  (| 1 ,  2 , ...,   1 ), where  1 ,  2 , ...,   1 are the tokens in the context sequence, and  is the current position.By using the chain rule, the conditional probability can be decomposed into a product of probabilities at each position:
 (| ) =   =1  (  | 1 ,  2 , ...,   1 ),
where  is sequence length.In this way, the model predicts each token at each position in an autoregressive manner, generating a complete text sequence.</p>
<p>One common approach to interacting with LLMs is prompt engineering [26,222,263], where users design and provide specific prompt texts to guide LLMs in generating desired responses or completing specific tasks.This is widely adopted in existing evaluation efforts.People can also engage in question-and-answer interactions [83], where they pose questions to the model and receive answers, or engage in dialogue interactions, having natural language conversations with LLMs.In conclusion, LLMs, with their Transformer architecture, in-context learning, and RLHF capabilities, have revolutionized NLP and hold promise in various applications.Model Fig. 3.The evaluation process of AI models.</p>
<p>AI Model Evaluation</p>
<p>AI model evaluation is an essential step in assessing the performance of a model.There are some standard model evaluation protocols, including -fold cross-validation, holdout validation, leave one out cross-validation (LOOCV), bootstrap, and reduced set [8,95].For instance, -fold crossvalidation divides the dataset into  parts, with one part used as a test set and the rest as training sets, which can reduce training data loss and obtain relatively more accurate model performance evaluation [48]; Holdout validation divides the dataset into training and test sets, with a smaller calculation amount but potentially more significant bias; LOOCV is a unique -fold cross-validation method where only one data point is used as the test set [223]; Reduced set trains the model with one dataset and tests it with the remaining data, which is computationally simple, but the applicability is limited.The appropriate evaluation method should be chosen according to the specific problem and data characteristics for more reliable performance indicators.Figure 3 illustrates the evaluation process of AI models, including LLMs.Some evaluation protocols may not be feasible to evaluate deep learning models due to the extensive training size.Thus, evaluation on a static validation set has long been the standard choice for deep learning models.For instance, computer vision models leverage static test sets such as ImageNet [33] and MS COCO [120] for evaluation.LLMs also use GLUE [200] or SuperGLUE [199] as the common test sets.</p>
<p>As LLMs are becoming more popular with even poorer interpretability, existing evaluation protocols may not be enough to evaluate the true capabilities of LLMs thoroughly.We will introduce recent evaluations of LLMs in Sec. 5.</p>
<p>WHAT TO EVALUATE</p>
<p>What tasks should we evaluate LLMs to show their performance?On what tasks can we claim the strengths and weaknesses of LLMs?In this section, we divide existing tasks into the following categories: natural language processing, robustness, ethics, biases and trustworthiness, social sciences, natural science and engineering, medical applications, agent applications (using LLMs as agents), and other applications. 1 Survey on Evaluation of Large Language Models 111:7</p>
<p>Natural Language Processing Tasks</p>
<p>The initial objective behind the development of language models, particularly large language models, was to enhance performance on natural language processing tasks, encompassing both understanding and generation.Consequently, the majority of evaluation research has been primarily focused on natural language tasks.Table 2 summarizes the evaluation aspects of existing research, and we mainly highlight their conclusions in the following. 2.1.1Natural language understanding.Natural language understanding represents a wide spectrum of tasks that aims to obtain a better understanding of the input sequence.We summarize recent efforts in LLMs evaluation from several aspects.</p>
<p>Sentiment analysis is a task that analyzes and interprets the text to determine the emotional inclination.It is typically a binary (positive and negative) or triple (positive, neutral, and negative) class classification problem.Evaluating sentiment analysis tasks is a popular direction.Liang et al. [114] and Zeng et al. [243] showed that the performance of the models on this task is usually high.ChatGPT's sentiment analysis prediction performance is superior to traditional sentiment analysis methods [129] and comes close to that of GPT-3.5 [159].In fine-grained sentiment and emotion cause analysis, ChatGPT also exhibits exceptional performance [218].In low-resource learning environments, LLMs exhibit significant advantages over small language models [251], but the ability of ChatGPT to understand low-resource languages is limited [6].In conclusion, LLMs have demonstrated commendable performance in sentiment analysis tasks.Future work should focus on enhancing their capability to understand emotions in under-resourced languages.</p>
<p>Text classification and sentiment analysis are related fields, text classification not only focuses on sentiment, but also includes the processing of all texts and tasks.The work of Liang et al. [114] showed that GLM-130B was the best-performed model, with an overall accuracy of 85.8% for miscellaneous text classification.Yang and Menczer [233] found that ChatGPT can produce credibility ratings for a wide range of news outlets, and these ratings have a moderate correlation with those from human experts.Furthermore, ChatGPT achieves acceptable accuracy in a binary classification scenario (AUC=0.89).Pea et al. [154] discussed the problem of topic classification for public affairs documents and showed that using an LLM backbone in combination with SVM classifiers is a useful strategy to conduct the multi-label topic classification task in the domain of public affairs with accuracies over 85%.Overall, LLMs perform well on text classification and can even handle text classification tasks in unconventional problem settings as well.</p>
<p>Natural language inference (NLI) is the task of determining whether the given "hypothesis" logically follows from the "premise".Qin et al. [159] showed that ChatGPT outperforms GPT-3.5 for NLI tasks.They also found that ChatGPT excels in handling factual input that could be attributed to its RLHF training process in favoring human feedback.However, Lee et al. [105] observed LLMs perform poorly in the scope of NLI and further fail in representing human disagreement, which indicates that LLMs still have a large room for improvement in this field.</p>
<p>Semantic understanding refers to the meaning or understanding of language and its associated concepts.It involves the interpretation and comprehension of words, phrases, sentences, and the relationships between them.Semantic processing goes beyond the surface level and focuses on understanding the underlying meaning and intent.Tao et al. [184] comprehensively evaluated the event semantic processing abilities of LLMs covering understanding, reasoning, and prediction about the event semantics.Results indicated that LLMs possess an understanding of individual events, but their capacity to perceive the semantic similarity among events is constrained.In reasoning tasks, LLMs exhibit robust reasoning abilities in causal and intentional relations, yet their  Zhang et al. [244]  performance in other relation types is comparatively weaker.In prediction tasks, LLMs exhibit enhanced predictive capabilities for future events with increased contextual information.Riccardi and Desai [166] explored the semantic proficiency of LLMs and showed that these models perform poorly in evaluating basic phrases.Furthermore, GPT-3.5 and Bard cannot distinguish between meaningful and nonsense phrases, consistently classifying highly nonsense phrases as meaningful.GPT-4 shows significant improvements, but its performance is still significantly lower than that of humans.In summary, the performance of LLMs in semantic understanding tasks is poor.In the future, we can start from this aspect and focus on improving its performance on this application.</p>
<p>In social knowledge understanding, Choi et al. [23] evaluated how well models perform at learning and recognizing concepts of social knowledge and the results revealed that despite being much smaller in the number of parameters, finetuning supervised models such as BERT lead to much better performance than zero-shot models using state-of-the-art LLMs, such as GPT [162], GPT-J-6B [202] and so on.This statement demonstrates that supervised models significantly outperform zero-shot models in terms of performance, highlighting that an increase in parameters does not necessarily guarantee a higher level of social knowledge in this particular scenario.</p>
<p>3.1.2Reasoning.The task of reasoning poses significant challenges for an intelligent AI model.To effectively tackle reasoning tasks, the models need to not only comprehend the provided information but also utilize reasoning and inference to deduce answers when explicit responses are absent.Table 2 reveals that there is a growing interest in evaluating the reasoning ability of LLMs, as evidenced by the increasing number of articles focusing on exploring this aspect.Currently, the evaluation of reasoning tasks can be broadly categorized into mathematical reasoning, commonsense reasoning, logical reasoning, and domain-specific reasoning.</p>
<p>ChatGPT exhibits a strong capability for arithmetic reasoning by outperforming GPT-3.5 in the majority of tasks [159].However, its proficiency in mathematical reasoning still requires improvement [6,45,265].On symbolic reasoning tasks, ChatGPT is mostly worse than GPT-3.5, which may be because ChatGPT is prone to uncertain responses, leading to poor performance [6].Through the poor performance of LLMs on task variants of counterfactual conditions, Wu et al. [227] showed that the current LLMs have certain limitations in abstract reasoning ability.On abstract reasoning, Gendron et al. [56] found that existing LLMs have very limited ability.In logical reasoning, Liu et al. [124] indicated that ChatGPT and GPT-4 outperform traditional fine-tuning methods on most benchmarks, demonstrating their superiority in logical reasoning.However, both models face challenges when handling new and out-of-distribution data.ChatGPT does not perform as well as other LLMs, including GPT-3.5 and BARD [159,229].This is because ChatGPT is designed explicitly for chatting, so it does an excellent job of maintaining rationality.FLAN-T5, LLaMA, GPT-3.5, and PaLM perform well in general deductive reasoning tasks [170].GPT-3.5 is not good at keeping oriented for reasoning in the inductive setting [229].For multi-step reasoning, Fu et al. [47] showed PaLM and Claude2 are the only two model families that achieve similar performance (but still worse than the GPT model family).Moreover, LLaMA-65B is the most robust open-source LLMs to date, which performs closely to code-davinci-002.Some papers separately evaluate the performance of ChatGPT on some reasoning tasks: ChatGPT generally performs poorly on commonsense reasoning tasks, but relatively better than non-text semantic reasoning [6].Meanwhile, ChatGPT also lacks spatial reasoning ability, but exhibits better temporal reasoning.Finally, while the performance of ChatGPT is acceptable on causal and analogical reasoning, it performs poorly on multi-hop reasoning ability, which is similar to the weakness of other LLMs on complex reasoning [148].In professional domain reasoning tasks, zero-shot InstructGPT and Codex are capable of complex medical reasoning tasks, but still need to be further improved [117].In terms of language insight issues, Orr et al. [147] demonstrated the potential of ChatGPT for solving verbal insight problems, as ChatGPT's performance was comparable to that of human participants.It should be noted that most of the above conclusions are obtained for specific data sets.In contrast, more complex tasks have become the mainstream benchmarks for assessing the capabilities of LLMs.These include tasks such as mathematical reasoning [226,237,244] and structured data inference [86,151].Overall, LLMs show great potential in reasoning and show a continuous improvement trend, but still face many challenges and limitations, requiring more in-depth research and optimization.</p>
<p>3.1.3Natural language generation.NLG evaluates the capabilities of LLMs in generating specific texts, which consists of several tasks, including summarization, dialogue generation, machine translation, question answering, and other open-ended generation tasks.</p>
<p>Summarization is a generation task that aims to learn a concise abstract for the given sentence.In this evaluation, Liang et al. [114] found that TNLG v2 (530B) [179] achieved the highest score in both scenarios, followed by OPT (175B) [247] in second place.The fine-tuned Bart [106] is still better than zero-shot ChatGPT.Specifically, ChatGPT demonstrates comparable zero-shot performance to the text-davinci-002 [6], but performs worse than GPT-3.5 [159].These findings indicate that LLMs, particularly ChatGPT, have a general performance in summarization tasks.</p>
<p>Evaluating the performance of LLMs on dialogue tasks is crucial to the development of dialogue systems and improving human-computer interaction.Through such evaluation, the natural language processing ability, context understanding ability and generation ability of the model can be improved, so as to realize a more intelligent and more natural dialogue system.Both Claude and ChatGPT generally achieve better performance across all dimensions when compared to GPT-3.5 [121,159].When comparing the Claude and ChatGPT models, both models demonstrate competitive performance across different evaluation dimensions, with Claude slightly outperforming ChatGPT in specific configurations.Research by Bang et al. [6] underscores that fully fine-tuned models tailored for specific tasks surpass ChatGPT in both task-oriented and knowledge-based dialogue contexts.Additionally, Zheng et al. [259] have curated a comprehensive LLMs conversation dataset, LMSYS-Chat-1M, encompassing up to one million samples.This dataset serves as a valuable resource for evaluating and advancing dialogue systems.</p>
<p>While LLMs are not explicitly trained for translation tasks, they can still demonstrate strong performance.Wang et al. [208] demonstrated that ChatGPT and GPT-4 exhibit superior performance in comparison to commercial machine translation (MT) systems, as evaluated by humans.Additionally, they outperform most document-level NMT methods in terms of sacreBLEU scores.During contrastive testing, ChatGPT shows lower accuracy in comparison to traditional translation models.However, GPT-4 demonstrates a robust capability in explaining discourse knowledge, even though it may occasionally select incorrect translation candidates.The findings from Bang et al. [6] indicated that ChatGPT performs X  Eng translation well, but it still lacks the ability to perform Eng  X translation.Lyu et al. [130] investigated several research directions in MT utilizing LLMs.This study significantly contributes to the advancement of MT research and highlights the potential of LLMs in enhancing translation capabilities.In summary, while LLMs perform satisfactorily in several translation tasks, there is still room for improvement, e.g., enhancing the translation capability from English to non-English languages.</p>
<p>Question answering is a crucial technology in the field of human-computer interaction, and it has found wide application in scenarios like search engines, intelligent customer service, and QA systems.The measurement of accuracy and efficiency in QA models will have significant implications for these applications.According to Liang et al. [114], among all the evaluated models, InstructGPT davinci v2 (175B) exhibited the highest performance in terms of accuracy, robustness, and fairness across the 9 QA scenarios.Both GPT-3.5 and ChatGPT demonstrate significant advancements compared to GPT-3 in their ability to answer general knowledge questions.In most domains, ChatGPT surpasses GPT-3.5 by more than 2% in terms of performance [9,159].However, ChatGPT performs slightly weaker than GPT-3.5 on the CommonsenseQA and Social IQA benchmarks.This can be attributed to ChatGPT's cautious nature, as it tends to decline to provide an answer when there is insufficient information available.Fine-tuned models, such as Vcuna and ChatGPT, exhibit exceptional performance with near-perfect scores, surpassing models that lack supervised fine-tuning by a significant margin [5,6].Laskar et al. [102] evaluated the effectiveness of ChatGPT on a range of academic datasets, including various tasks such as answering questions, summarizing text, generating code, reasoning with commonsense, solving math problems, translating languages, detecting bias, and addressing ethical issues.Overall, LLMs showcase flawless performance on QA tasks and hold the potential for further enhancing their proficiency in social, event, and temporal commonsense knowledge in the future.</p>
<p>There are also other generation tasks to explore.In the field of sentence style transfer, Pu and Demberg [158] demonstrated that ChatGPT surpasses the previous SOTA supervised model through training on the same subset for few-shot learning, as evident from the higher BLEU score.However, when it comes to controlling the formality of sentence style, ChatGPT's performance still differs significantly from human behavior.In writing tasks, Chia et al. [22] discovered that LLMs exhibit consistent performance across various categories such as informative, professional, argumentative, and creative writing.This finding implies that LLMs possess a general proficiency in writing capabilities.In text generation quality, Chen et al. [20] revealed that ChatGPT excels in assessing text quality from multiple angles, even in the absence of reference texts, surpassing the performance of most existing automated metrics.Employing ChatGPT to generate numerical scores for text quality emerged as the most reliable and effective approach among the various testing methods studied.</p>
<p>Multilingual tasks.</p>
<p>While English is the predominant language, many LLMs are trained on mixed-language training data.The combination of multilingual data indeed helps LLMs gain the ability to process inputs and generate responses in different languages, making them widely adopted and accepted across the globe.However, due to the relatively recent emergence of this technology, LLMs are primarily evaluated on English data, leading to a potential oversight of evaluating their multilingual performance.To address this, several articles have provided comprehensive, open, and independent evaluations of LLMs' performance on various NLP tasks in different non-English languages.These evaluations offer valuable insights for future research and applications.</p>
<p>Abdelali et al. [1] evaluated the performance of ChatGPT in standard Arabic NLP tasks and observed that ChatGPT exhibits lower performance compared to SOTA models in the zero-shot setting for most tasks.Ahuja et al. [2], Bang et al. [6], Lai et al. [100], Zhang et al. [250] utilized a greater number of languages across multiple datasets, encompassing a wider range of tasks, and conducted a more comprehensive evaluation of LLMs, including BLOOM, Vicuna, Claude, ChatGPT, and GPT-4.The results indicated that these LLMs perform poorly when it came to non-Latin languages and languages with limited resources.Despite translating the input to English and using it as the query, generative LLMs still displays subpar performance across tasks and languages compared to SOTA models [2].Furthermore, Bang et al. [6] highlighted that ChatGPT still faces a limitation in translating sentences written in non-Latin script languages with rich linguistic resources.The aforementioned demonstrates that there are numerous challenges and ample opportunities for enhancement in multilingual tasks for LLMs.Future research should prioritize achieving multilingual balance and addressing the challenges faced by non-Latin languages and low-resource languages, with the aim of better supporting users worldwide.At the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications.</p>
<p>3.1.5Factuality.Factuality in the context of LLMs refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts.Factuality in LLMs significantly impacts a variety of tasks and downstream applications, such as QA systems, information extraction, text summarization, dialogue systems, and automated fact-checking, where incorrect or inconsistent information could lead to substantial misunderstandings and misinterpretations.Evaluating factuality is of great importance in order to trust and efficiently use these models.This includes the ability of these models to maintain consistency with known facts, avoid generating misleading or false information (known as "factual hallucination"), and effectively learn and recall factual knowledge.A range of methodologies have been proposed to measure and improve the factuality of LLMs.</p>
<p>Wang et al. [204] assessed the internal knowledge capabilities of several large models, namely InstructGPT, ChatGPT-3.5,GPT-4, and BingChat [137], by examining their ability to answer open questions based on the Natural Questions [98] and TriviaQA [88] datasets.The evaluation process involved human assessment.The results of the study indicated that while GPT-4 and BingChat can provide correct answers for more than 80% of the questions, there is still a remaining gap of over 15% to achieve complete accuracy.In the work of Honovich et al. [74], they conducted a review of current factual consistency evaluation methods and highlighted the absence of a unified comparison framework and the limited reference value of related scores compared to binary labels.To address this, they transformed existing fact consistency tasks into binary labels, specifically considering only whether there is a factual conflict with the input text, without factoring in external knowledge.The research discovered that fact evaluation methods founded on natural language inference and question generation answering exhibit superior performance and can complement each other.Pezeshkpour [156] proposed a novel metric, based on information theory, to assess the inclusion of specific knowledge in LLMs.The metric utilized the concept of uncertainty in knowledge to measure factualness, calculated by LLMs filling in prompts and examining the probability distribution of the answer.The paper discussed two methods for injecting knowledge into LLMs: explicit inclusion of knowledge in the prompts and implicit fine-tuning of the LLMs using knowledge-related data.The study demonstrated that this approach surpasses traditional ranking methods by achieving an accuracy improvement of over 30%.Gekhman et al. [55] improved the method for evaluating fact consistency in summarization tasks.It proposed a novel approach that involved training student NLI models using summaries generated by multiple models and annotated by LLMs to ensure fact consistency.The trained student model was then used for summarization fact consistency evaluation.Manakul et al. [133] operated on two hypotheses regarding how LLMs generate factual or hallucinated responses.It proposed the use of three formulas (BERTScore [249], MQAG [134] and n-gram) to evaluate factuality and employed alternative LLMs to gather token probabilities for black-box language models.The study discovered that simply computing sentence likelihood or entropy helped validate the factuality of the responses.Min et al. [138] broke down text generated by LLMs into individual "atomic" facts, which were then evaluated for their correctness.The FActScore is used to measure the performance of estimators through the calculation of F1 scores.The paper tested various estimators and revealed that current estimators still have some way to go in effectively addressing the task.Lin et al. [119] introduced the TruthfulQA dataset, designed to cause models to make mistakes.Multiple language models were tested by providing factual answers.The findings from these experiments suggest that simply scaling up model sizes may not necessarily improve their truthfulness, and recommendations are provided for the training approach.This dataset has become widely used for evaluating the factuality of LLMs [89,146,192,220].</p>
<p>Robustness, Ethic, Bias, and Trustworthiness</p>
<p>The evaluation encompasses crucial aspects of robustness, ethics, biases, and trustworthiness.These factors have gained increasing importance in assessing the performance of LLMs comprehensively.</p>
<p>Robustness.</p>
<p>Robustness studies the stability of a system when facing unexpected inputs.Specifically, out-of-distribution (OOD) [207] and adversarial robustness are two popular research topics for robustness.Wang et al. [206] is an early work that evaluated ChatGPT and other LLMs from both the adversarial and OOD perspectives using existing benchmarks such as AdvGLUE [203], ANLI [140], and DDXPlus [41] datasets.Zhuo et al. [267] evaluated the robustness of semantic parsing.Yang et al. [234] evaluated OOD robustness by extending the GLUE [200] dataset.The results of this study emphasize the potential risks to the overall system security when manipulating visual input.For vision-language models, Zhao et al. [258] evaluated LLMs on visual input and transferred them to other visual-linguistic models, revealing the vulnerability of visual input.Li et al. [111] provided an overview of OOD evaluation for language models: adversarial robustness, domain generalization, and dataset biases.Bridging these lines of research, the authors conducted a comparative analysis, unifying the three approaches.They succinctly outlined the data-generation processes and evaluation protocols for each line of study, all while emphasizing the prevailing challenges and future research prospects.Additionally, Liu et al. [123] introduced a large-scale robust visual instruction dataset to enhance the performance of large-scale multi-modal models in handling relevant images and human instructions.</p>
<p>For adversarial robustness, Zhu et al. [264] evaluated the robustness of LLMs to prompts by proposing a unified benchmark called PromptBench.They comprehensively evaluated adversarial text attacks at multiple levels (character, word, sentence, and semantics).The results showed that contemporary LLMs are vulnerable to adversarial prompts, highlighting the importance of the models' robustness when facing adversarial inputs.As for new adversarial datasets, Wang et al. [201] introduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a new evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.</p>
<p>3.2.2Ethic and bias.LLMs have been found to internalize, spread, and potentially magnify harmful information existing in the crawled training corpora, usually, toxic languages, like offensiveness, hate speech, and insults [53], as well as social biases like stereotypes towards people with a particular demographic identity (e.g., gender, race, religion, occupation, and ideology) [175].More recently, Zhuo et al. [266] used conventional testing sets and metrics [37,53,153] to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend.Taking a further step, Deshpande et al. [35] introduced role-playing into the model and observed an increase in generated toxicity up to 6x.Furthermore, such role-playing also caused biased toxicity towards specific entities.Different from simply measuring social biases, Ferrara [42] investigated the sources, underlying mechanisms, and corresponding ethical consequences of these biases potentially produced by ChatGPT.Beyond social biases, LLMs have also been assessed by political tendency and personality traits [65,167] based questionnaires like the Political Compass Test and MBTI test, demonstrating a propensity for progressive views and an ENFJ personality type.In addition, LLMs like GPT-3 were found to have moral biases [176] in terms of the Moral Foundation theory [58]; The study conducted by [69] reveals that existing LMs have potential in ethical judgment, but still need improvement.[256] proposes a Chinese conversational bias evaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing methods.Moreover, in the assessment of GPT-4 alignment, [209] discovered a systematic bias.ChatGPT is also observed to exhibit somewhat bias on cultural values [16].Wang et al. [201] also incorporated an evaluation dataset specifically aimed at gauging stereotype bias, using both targeted and untargeted system prompts.All these ethical issues might elicit serious risks, impeding the deployment of LLMs and having a profound negative impact on society.</p>
<p>3.2.3</p>
<p>Trustworthiness.Some work focuses on other trustworthiness problems in addition to robustness and ethics. 3In their 2023 study, DecodingTrust, Wang et al. [201] offered a multifaceted exploration of trustworthiness vulnerabilities in the GPT models, especially GPT-3.5 and GPT-4.Their evaluation expanded beyond the typical trustworthiness concerns to include eight critical aspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness.DecodingTrust's investigation employs an array of newly constructed scenarios, tasks, and metrics.They revealed that while GPT-4 often showcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously more susceptible to attacks.</p>
<p>In another study by Hagendorff and Fabi [62], LLMs with enhanced cognitive abilities were evaluated.They found that these models can avoid common human intuitions and cognitive errors, demonstrating super-rational performance.By utilizing cognitive reflection tests and semantic illusion experiments, the researchers gained insights into the psychological aspects of LLMs.This method offers new perspectives for evaluating model biases and ethical issues that may not have been previously identified.Furthermore, a study by [228] brings attention to a significant concern: the consistency of judgment in LLMs diminishes notably when faced with disruptions such as questioning, negation, or misleading cues, even if their initial judgments were accurate.The research delves into various prompting methods designed to mitigate this issue and successfully demonstrates their efficacy.</p>
<p>LLMs are capable of generating coherent and seemingly factual text.However, the information generated can include factual inaccuracies or statements ungrounded in reality, a phenomenon known as hallucination [163,253].Evaluating these issues helps improve the training methods of LLMs to reduce the occurrence of hallucinations.For the evaluation of illusions in large-scale visual models, Liu et al. [123] introduced a comprehensive and robust large-scale visual instruction dataset: LRV-Instruction.Through the GAVIE method, they fine-tuned the evaluation visual instructions, and experimental results demonstrated that LRV-Instruction effectively alleviates illusions in LLMs.In addition, Li et al. [113] conducted an assessment of illusions in large-scale visual language models, revealing through experiments that the distribution of objects in visual instructions significantly impacts object illusions in LVLMs.To enhance the assessment of object illusions in LVLMs, they introduced a polling-based query method, known as POPE.This method provides an improved evaluation of object illusions in LVLMs.</p>
<p>Social Science</p>
<p>Social science involves the study of human society and individual behavior, including economics, sociology, political science, law, and other disciplines.Evaluating the performance of LLMs in social science is important for academic research, policy formulation, and social problem-solving.Such evaluations can help improve the applicability and quality of models in the social sciences, increasing understanding of human societies and promoting social progress.</p>
<p>Wu et al. [224] evaluated the potential use of LLMs in addressing scaling and measurement issues in social science and found that LLMs can generate meaningful responses regarding political ideology and significantly improve text-as-data methods in social science.</p>
<p>In computational social science (CSS) tasks, Ziems et al. [269] presented a comprehensive evaluation of LLMs on several CSS tasks.During classification tasks, LLMs exhibit the lowest absolute performance on event argument extraction, character tropes, implicit hate, and empathy classification, achieving accuracy below 40%.These tasks either involve complex structures (event arguments) or subjective expert taxonomies with semantics that differ from those learned during LLM pretraining.Conversely, LLMs achieve the best performance on misinformation, stance, and emotion classification.When it comes to generation tasks, LLMs often produce explanations that surpass the quality of gold references provided by crowd workers.In summary, while LLMs can greatly enhance the traditional CSS research pipeline, they cannot completely replace it.</p>
<p>Some articles also evaluate LLMs on legal tasks.The zero-shot performance of LLMs is mediocre in legal case judgment summarization.LLMs have several problems, including incomplete sentences and words, meaningless sentences merge, and more serious errors such as inconsistent and hallucinated information [34].The results showed that further improvement is necessary for LLMs to be useful for case judgment summarization by legal experts.Nay et al. [139] indicated that LLMs, particularly when combined with prompting enhancements and the correct legal texts, could perform better but not yet at expert tax lawyer levels.</p>
<p>Lastly, within the realm of psychology, Frank [44] adopted an interdisciplinary approach and drew insights from developmental psychology and comparative psychology to explore alternative methods for evaluating the capabilities of LLMs.By integrating different perspectives, researchers can deepen their understanding of the essence of cognition and effectively leverage the potential of advanced technologies such as large language models, while mitigating potential risks.</p>
<p>In conclusion, the utilization of LLMs has significantly benefited individuals in addressing social science-related tasks, leading to improved work efficiency.The outputs produced by LLMs serve as valuable resources for enhancing productivity.However, it is crucial to acknowledge that existing LLMs cannot completely replace human professionals in this domain.</p>
<p>Natural Science and Engineering</p>
<p>Evaluating the performance of LLMs in natural science and engineering can help guide applications and development in scientific research, technology development, and engineering studies.</p>
<p>Mathematics.</p>
<p>For fundamental mathematical problems, most large language models (LLMs) demonstrate proficiency in addition and subtraction, and possess some capability in multiplication.However, they face challenges when it comes to division, exponentiation, trigonometry functions, and logarithm functions.On the other hand, LLMs exhibit competence in handling decimal numbers, negative numbers, and irrational numbers [241].In terms of performance, ChatGPT and GPT-4 outperform other models significantly, showcasing their superiority in solving mathematical tasks [221].These two models have a distinct advantage in dealing with large numbers (greater than 1e12) and complex, lengthy mathematical queries.GPT-4 outperforms ChatGPT by achieving a significant increase in accuracy of 10 percentage points and a reduction in relative error by 50%, due to its superior division and trigonometry abilities, proper understanding of irrational numbers, and consistent step-by-step calculation of long expressions.When confronted with complex and challenging mathematical problems, LLMs exhibit subpar performance.Specifically, GPT-3 demonstrates nearly random performance, while GPT-3.5 shows improvement, and GPT-4 performs the best [3].Despite the advancements made in the new models, it is important to note that the peak performance remains relatively low compared to that of experts and these models lack the capability to engage in mathematical research [15].The specific tasks of algebraic manipulation and calculation continue to pose challenges for GPTs [15,27].The primary reasons behind GPT-4's low performance in these tasks are errors in algebraic manipulation and difficulties in retrieving pertinent domain-specific concepts.Wu et al. [225] evaluated the use of GPT-4 on difficult high school competition problems and GPT-4 reached 60% accuracy on half of the categories.Intermediate algebra and precalculus can only be solved with a low accuracy rate of around 20%.ChatGPT is not good at answering questions on topics including derivatives and applications, Oxyz spatial calculus, and spatial geometry [31].Dao and Le [31], Wei et al. [221] showed that ChatGPT's performance worsens as task difficulty increases: it correctly answered 83% of the questions at the recognition level, 62% at the comprehension level, 27% at the application level, and only 10% at the highest cognitive complexity level.Given those problems at higher knowledge levels tend to be more complex, requiring in-depth understanding and problem-solving skills, such results are to be expected.</p>
<p>These results indicate that the effectiveness of LLMs is highly influenced by the complexity of problems they encounter.This finding holds significant implications for the design and development of optimized artificial intelligence systems capable of successfully handling these challenging tasks.</p>
<p>General science.</p>
<p>Further improvements are needed in the application of LLMs in the field of chemistry.Castro Nascimento and Pimentel [18] presented five straightforward tasks from various subareas of chemistry to assess ChatGPT's comprehension of the subject, with accuracy ranging from 25% to 100%.Guo et al. [61] created a comprehensive benchmark that encompasses 8 practical chemistry tasks, which is designed to assess the performance of LLMs (including GPT-4, GPT-3.5, and Davinci-003) for each chemistry task.Based on the experiment results, GPT-4 demonstrates superior performance compared to the other two models.[3] showed that LLMs perform worse on physics problems than chemistry problems, probably because chemistry problems have lower inference complexity than physics problems in this setting.There are limited evaluation studies on LLMs in the field of general science, and the current findings indicate that further improvement is needed in the performance of LLMs within this domain.</p>
<p>Engineering.</p>
<p>Within engineering, the tasks can be organized in ascending order of difficulty, including code generation, software engineering, and commonsense planning.</p>
<p>In code generation tasks, the smaller LLMs trained for the tasks are competitive in performance, and CodeGen-16B [141] is comparable in performance to ChatGPT using a larger parameter setting, reaching about a 78% match [125].Despite facing challenges in mastering and comprehending certain fundamental concepts in programming languages, ChatGPT showcases a commendable level of coding level [265].Specifically, ChatGPT has developed superior skills in dynamic programming, greedy algorithm, and search, surpassing highly capable college students, but it struggles in data structure, tree, and graph theory.GPT-4 demonstrates an advanced ability to generate code based on given instructions, comprehend existing code, reason about code execution, simulate the impact of instructions, articulate outcomes in natural language, and execute pseudocode effectively [15].</p>
<p>In software engineering tasks, ChatGPT generally performs well and provides detailed responses, often surpassing both human expert output and SOTA output.However, for certain tasks such as code vulnerability detection and information retrieval-based test prioritization, the current version of ChatGPT fails to provide accurate answers, rendering it unsuitable for these specific tasks [181].</p>
<p>In commonsense planning tasks, LLMs may not perform well, even in simple planning tasks where humans excel [194,195].Pallagani et al. [150] demonstrated that the fine-tuned CodeT5 [214] performs the best across all considered domains, with the shortest inference time.Moreover, it explored the capability of LLMs for plan generalization and found that their generalization capabilities appear to be limited.It turns out that LLMs can handle simple engineering tasks, but they perform poorly on complex engineering tasks.</p>
<p>Medical Applications</p>
<p>The application of LLMs in the medical field has recently received significant attention.As a result, this section aims to provide a comprehensive review of the ongoing efforts dedicated to implementing LLMs in medical applications.We have categorized these applications into three aspects as shown in Table 5: medical query, medical examination, and medical assistants.A detailed  Wang et al. [217]  examination of these categories will enhance our understanding of the potential impact and advantages that LLMs can bring to the medical domain.</p>
<p>Medical queries.</p>
<p>The significance of evaluating LLMs on medical queries lies in providing accurate and reliable medical answers to meet the needs of healthcare professionals and patients for high-quality medical information.As shown in Table 5, the majority of LLMs evaluations in the medical field concentrate on medical queries.ChatGPT generated relatively accurate information for various medical queries, including genetics [39], radiation oncology physics [73], biomedicine [81], and many other medical disciplines [63,87,169], demonstrating its effectiveness in the field of medical queries to a certain extent.As for the limitations, Thirunavukarasu et al. [186] assessed ChatGPT's performance in primary care and found that its average score in the student comprehensive assessment falls below the passing score, indicating room for improvement.Chervenak et al. [21] highlighted that while ChatGPT can generate responses similar to existing sources in fertility-related clinical prompts, its limitations in reliably citing sources and potential for fabricating information restrict its clinical utility.</p>
<p>Medical examination.</p>
<p>The studies by Gilson et al. [57], Kung et al. [97] have evaluated the performance of LLMs in medical examination assessment through the United States Medical Licensing Examination (USMLE) 4 .In the study of [57], ChatGPT's performance in answering USMLE Step 1 and Step 2 exam questions was assessed using novel multiple-choice question sets.</p>
<p>The results indicated that ChatGPT achieves varying accuracies across different datasets.However, the presence of out-of-context information was found to be lower compared to the correct answer in the NBME-Free-Step1 and NBME-Free-Step2 datasets.Kung et al. [97] showed that ChatGPT achieves or approaches the passing threshold in these exams with no tailored training.The model demonstrates high consistency and insight, indicating its potential to assist in medical education and clinical decision-making.ChatGPT can be used as a tool to answer medical questions, provide explanations, and support decision-making processes.This offers additional resources and support for medical students and clinicians in their educational and clinical practices.Moreover, Sharma et al. [173] found that answers generated by ChatGPT are more context-aware with better deductive reasoning abilities compared to Google search results.</p>
<p>Medical assistants.</p>
<p>In the field of medical assistance, LLMs demonstrate potential applications, including research on identifying gastrointestinal diseases [99], dementia diagnosis [217], accelerating the evaluation of COVID-19 literature [93], and their overall potential in healthcare [17].However, there are also limitations and challenges, such as lack of originality, high input requirements, resource constraints, uncertainty in answers, and potential risks related to misdiagnosis and patient privacy issues.Moreover, several studies have evaluated the performance and feasibility of ChatGPT in the medical education field.In the study by Oh et al. [143], ChatGPT, specifically GPT-3.5 and GPT-4 models, were evaluated in terms of their understanding of surgical clinical information and their potential impact on surgical education and training.The results indicate an overall accuracy of 46.8% for GPT-3.5 and 76.4% for GPT-4, demonstrating a significant performance difference between the two models.Notably, GPT-4 consistently performs well across different subspecialties, suggesting its capability to comprehend complex clinical information and enhance surgical education and training.Another study by Lyu et al. [131] explores the feasibility of utilizing ChatGPT in clinical education, particularly in translating radiology reports into easily understandable language.The findings demonstrate that ChatGPT effectively translates radiology reports into accessible language and provides general recommendations.Furthermore, the quality of ChatGPT has shown improvement compared to GPT-4.These findings suggest that employing LLMs in clinical education is feasible, although further efforts are needed to address limitations and unlock their full potential.</p>
<p>Agent Applications</p>
<p>Instead of focusing solely on general language tasks, LLMs can be utilized as powerful tools in various domains.Equipping LLMs with external tools can greatly expand the capabilities of the model [160].ToolLLM [161] provides a comprehensive framework to equip open-source large language models with tool use capabilities.Huang et al. [77] introduced KOSMOS-1, which is capable of understanding general patterns, following instructions, and learning based on context.The study by MRKL Karpas et al. [90] emphasized the importance of understanding when and how to utilize external symbolic tools, as this knowledge is dependent on the capabilities of LLMs, particularly when these tools can reliably perform functions.Additionally, two other studies, Toolformer [172] and TALM [152], explored the utilization of tools to enhance language models.Toolformer employs a training approach to determine the optimal usage of specific APIs and integrates the obtained results into subsequent token predictions.On the other hand, TALM combines indistinguishable tools with text-based methods to augment language models and employs an iterative technique known as "self-play", guided by minimal tool demonstrations.Furthermore, Shen et al. [174] proposed the HuggingGPT framework, which leverages LLMs to connect various AI models within the machine learning community (such as Hugging Face), aiming to address AI tasks.</p>
<p>Other Applications</p>
<p>In addition to above areas, there have been evaluations in various other domains, including education, search and recommendation, personality testing, and specific applications.</p>
<p>Education.</p>
<p>LLMs have shown promise in revolutionizing the field of education.They have the potential to make significant contributions in several areas, such as assisting students in improving their writing skills, facilitating better comprehension of complex concepts, expediting the delivery of information, and providing personalized feedback to enhance student engagement.These applications aim to create more efficient and interactive learning experiences, offering  Zhang et al. [246]  students a broader range of educational opportunities.However, to fully harness the potential of LLMs in education, extensive research, and ongoing refinement are necessary.The evaluation of LLMs for educational assistance aims to investigate and assess their potential contributions to the field of education.Such evaluations can be conducted from various perspectives.According to Dai et al. [30], ChatGPT demonstrates the ability to generate detailed, fluent, and coherent feedback that surpasses that of human teachers.It can accurately assess student assignments and provide feedback on task completion, thereby assisting in the development of student skills.However, ChatGPT's responses may lack novelty or insightful perspectives regarding teaching improvement [210].Additionally, the study conducted by Hellas et al. [67] revealed that LLMs can successfully identify at least one actual problem in student code, although instances of misjudgment are also observed.In conclusion, the utilization of LLMs shows promise in addressing program logic issues, although challenges remain in achieving proficiency in output formatting.It is important to note that while these models can provide valuable insights, they may still generate errors similar to those made by students.</p>
<p>In educational exams, researchers aim to evaluate the application effectiveness of LLMs, including automatic scoring, question generation, and learning guidance.de Winter [32] showed that ChatGPT achieves an average of 71.8% correctness, which is comparable to the average score of all participating students.Subsequently, the evaluation was conducted using GPT-4, and it achieved a score of 8.33.Furthermore, this evaluation showed the effectiveness of leveraging bootstrapping that combines randomness via the "temperature" parameter in diagnosing incorrect answers.Zhang et al. [248] claimed that GPT-3.5 can solve MIT math and EECS exams with GPT-4 achieving better performance.However, it turned out to be not fair since they accidentally included the correct answers into the prompts.</p>
<p>Search and recommendation.</p>
<p>The assessment of LLMs in search and recommendation can be broadly categorized into two areas.Firstly, in the realm of information retrieval, Sun et al. [183] investigated the effectiveness of generative ranking algorithms, such as ChatGPT and GPT-4, for information retrieval tasks.Experimental results demonstrate that guided ChatGPT and GPT-4 exhibit competitive performance on popular benchmark tests, even outperforming supervised methods.Additionally, the extraction of ChatGPT's ranking functionality into a specialized model shows superior performance when trained on 10K ChatGPT-generated data compared to training on 400K annotated MS MARCO data in the BEIR dataset [185].Furthermore, Xu et al. [232] conducted a randomized online experiment to investigate the behavioral differences of users when performing information retrieval tasks using search engines and chatbot tools.Participants were divided into two groups: one using tools similar to ChatGPT and the other using tools similar to Google Search.The results show that the ChatGPT group spent less time on all tasks and the difference between these two groups is not significant.</p>
<p>Secondly, moving to the domain of recommendation systems, LLMs have emerged as essential components that leverage their natural language processing capabilities to comprehend user preferences, item descriptions, and contextual information [40].By incorporating LLMs into recommendation pipelines, these systems can offer more accurate and personalized recommendations, thereby improving user experience and overall recommendation quality.However, it is crucial to address the potential risks associated with using LLMs for recommendations.Recent research by Zhang et al. [246] has highlighted the issue of unfair recommendations generated by ChatGPT.This emphasizes the importance of evaluating fairness when employing LLMs in recommendation scenarios.Dai et al. [29] suggest that ChatGPT exhibits strong performance in recommender systems.The use of listwise ranking is found to strike the best balance between cost and performance.Furthermore, ChatGPT shows promise in addressing the cold-start problem and providing interpretable recommendations.Moreover, the research by Yuan et al. [240] and Li et al. [110] demonstrated the promising potential of the modality-based recommendation model (MoRec) and text-based collaborative filtering (TCF) in recommendation systems.</p>
<p>Personality testing.</p>
<p>Personality testing aims to measure individuals' personality traits and behavioral tendencies, and LLMs as powerful natural language processing models have been widely applied in such tasks.</p>
<p>Research conducted by Bodroza et al. [10] investigated the personality features of using Davinci-003 as a chatbot and found variations in the consistency of its answers, despite exhibiting prosocial characteristics.However, there remains uncertainty regarding whether the chatbot's responses are driven by conscious self-reflection or algorithmic processes.Song et al. [180] examined the manifestation of personality in language models and discovered that many models perform unreliably in self-assessment tests and exhibit inherent biases.Therefore, it is necessary to develop specific machine personality measurement tools to enhance reliability.These studies offer vital insights to better understand LLMs in personality testing.Safdari et al. [168] proposed a comprehensive approach to conduct effective psychometric testing for the personality traits in the text generated by LLMs.In order to evaluate the emotional intelligence of LLMs, Wang et al. [212] developed a new psychometric assessment method.By referencing a framework constructed from over 500 adults, the authors tested various mainstream LLMs.The results showed that most LLMs achieve above-average scores in emotional quotient (EQ), with GPT-4 scoring 117, surpassing 89% of human participants.However, a multivariate pattern analysis indicated that certain LLMs achieve human-level performance without relying on mechanisms resembling those found in humans.This is evident from the distinct differences in the quality of their representational patterns, as compared to humans.Liang et al. [115] employed the word guessing game to evaluate LLMs' language and theory of mind intelligences, a more engaging and interactive assessment method.Jentzsch and Kersting [84] discussed the challenges of incorporating humor into LLMs, particularly ChatGPT.They found that while ChatGPT demonstrates impressive capabilities in NLP tasks, it falls short Table 7. Summary of existing LLMs evaluation benchmarks (ordered by the name of the first author).</p>
<p>Benchmark</p>
<p>Focus Domain Evaluation Criteria SOCKET [23] Social knowledge Specific downstream task Social language understanding MME [46] Multimodal LLMs Multi-modal task Ability of perception and cognition Xiezhi [59] Comprehensive domain knowledge General language task Overall performance across multiple benchmarks Choice-75 [75] Script learning Specific downstream task Overall performance of LLMs CUAD [71] Legal contract review Specific downstream task Legal contract understanding TRUSTGPT [79] Ethic Specific downstream task Toxicity, bias, and value-alignment MMLU [70] Text models General language task Multitask accuracy MATH [72] Mathematical problem Specific downstream task Mathematical ability APPS [68] Coding challenge competence Specific downstream task Code generation ability CELLO [66] Complex instructions Specific downstream task Four designated evaluation criteria C-Eval [78] Chinese evaluation General language task 52 Exams in a Chinese context EmotionBench [76] Empathy ability Specific downstream task Emotional changes OpenLLM [80] Chatbots General language task Leaderboard rankings DynaBench [94] Dynamic evaluation General language task NLI, QA, sentiment, and hate speech Chatbot Arena [128] Chat assistants General language task Crowdsourcing and Elo rating system AlpacaEval [112] Automated evaluation General language task Metrics, robustness, and diversity CMMLU [108] Chinese multi-tasking Specific downstream task Multi-task language understanding capabilities HELM [114] Holistic evaluation General language task Multi-metric API-Bank [109] Tool utilization Specific downstream task API call, retrieval, and planning M3KE [122] Multi-task Specific downstream task Multi-task accuracy MMBench [126] Large vision-language models(LVLMs) Multi-modal task Multifaceted capabilities of VLMs SEED-Bench [107] Multimodal Large Language Models Multi-modal task Generative understanding of MLLMs UHGEval [116] Hallucination of Chinese LLMs Specific downstream task Form, metric and granularity ARB [171] Advanced reasoning ability Specific downstream task Multidomain advanced reasoning ability BIG-bench [182] Capabilities and limitations of LMs General language task Model performance and calibration MultiMedQA [177] Medical QA Specific downstream task Accuracy and human evaluation CVALUES [230] Safety and responsibility Specific downstream task Alignment ability of LLMs LVLM-eHub [231] LVLMs Multi-modal task Multimodal capabilities of LVLMs ToolBench [191] Software tools Specific downstream task Execution success rate FRESHQA [198] Dynamic QA Specific downstream task Correctness and hallucination CMB [211] Chinese comprehensive medicine Specific downstream task Expert evaluation and automatic evaluation PandaLM [216] Instruction tuning General language task Winrate judged by PandaLM MINT [213] Multi-turn interaction Specific downstream task Success rate with k-turn budget   Dialogue CoT [205] In-depth dialogue Specific downstream task Helpfulness and acceptness of LLMs BOSS [239] OOD robustness in NLP General language task OOD robustness MM-Vet [238] Complicated multi-modal tasks Multi-modal task Integrated vision-language capabilities LAMM [235] Multi-modal point clouds Multi-modal task Task-specific metrics GLUE-X [234] OOD robustness for NLP tasks General language task OOD robustness KoLA [236] Knowledge-oriented evaluation General language task Self-contrast metrics AGIEval [262] Human-centered foundational models General language task General PromptBench [264] Adversarial prompt resilience General language task Adversarial robustness MT-Bench [260] Multi-turn conversation General language task Winrate judged by GPT-4 M3Exam [250] Multilingual, multimodal and multilevel Specific downstream task Task-specific metrics GAOKAO-Bench [245] Chinese Gaokao examination Specific downstream task Accuracy and scoring rate SafetyBench [254] Safety Specific downstream task Safety abilities of LLMs LLMEval2 [252] LLM Evaluator General language task Acc, macro-f1 and kappa correlation coefficient in generating humorous responses.This study emphasizes the importance of humor in human communication and the difficulties that LLMs face in capturing the subtleties and context-dependent nature of humor.It discusses the limitations of current approaches and highlights the need for further research on more sophisticated models that can effectively understand and generate humor.</p>
<p>Specific applications.</p>
<p>Moreover, various research endeavors have been conducted to explore the application and evaluation of LLMs across a wide spectrum of tasks, such as game design [101], model performance assessment [216], and log parsing [103].Collectively, these findings enhance our comprehension of the practical implications associated with the utilization of LLMs across diverse tasks.They shed light on the potential and limitations of these models while providing valuable insights for performance improvement.</p>
<p>WHERE TO EVALUATE: DATASETS AND BENCHMARKS</p>
<p>LLMs evaluation datasets are used to test and compare the performance of different language models on various tasks, as depicted in Sec. 3.These datasets, such as GLUE [200] and SuperGLUE [199], aim to simulate real-world language processing scenarios and cover diverse tasks such as text classification, machine translation, reading comprehension, and dialogue generation.This section will not discuss any single dataset for language models but benchmarks for LLMs.</p>
<p>A variety of benchmarks have emerged to evaluate their performance.In this study, we compile a selection of 46 popular benchmarks, as shown in Table 7. 5 Each benchmark focuses on different aspects and evaluation criteria, providing valuable contributions to their respective domains.For a better summarization, we divide these benchmarks into three categories: benchmarks for general language tasks, benchmarks for specific downstream tasks, and benchmarks for multi-modal tasks.</p>
<p>Benchmarks for General Tasks</p>
<p>LLMs are designed to solve a vast majority of tasks.To this end, existing benchmarks tend to evaluate the performance in different tasks.</p>
<p>Chatbot Arena [128] and MT-Bench [260] are two significant benchmarks that contribute to the evaluation and advancement of chatbot models and LLMs in different contexts.Chatbot Arena provides a platform to assess and compare diverse chatbot models through user engagement and voting.Users can engage with anonymous models and express their preferences via voting.a significant volume of votes, facilitating the evaluation of models' performance in realistic scenarios.Chatbot Arena provides valuable insights into the strengths and limitations of chatbot models, thereby contributing to the progress of chatbot research and advancement.</p>
<p>Meanwhile, MT-Bench evaluates LLMs on multi-turn dialogues using comprehensive questions tailored to handling conversations.It provides a comprehensive set of questions specifically designed for assessing the capabilities of models in handling multi-turn dialogues.MT-Bench possesses several distinguishing features that differentiate it from conventional evaluation methodologies.Notably, it excels in simulating dialogue scenarios representative of real-world settings, thereby facilitating a more precise evaluation of a model's practical performance.Moreover, MT-Bench effectively overcomes the limitations in traditional evaluation approaches, particularly in gauging a model's competence in handling intricate multi-turn dialogue inquiries.</p>
<p>Instead of focusing on specific tasks and evaluation metrics, HELM [114] provides a comprehensive assessment of LLMs.It evaluates language models across various aspects such as language understanding, generation, coherence, context sensitivity, common-sense reasoning, and domainspecific knowledge.HELM aims to holistically evaluate the performance of language models across different tasks and domains.For LLMs Evaluator, Zhang et al. [252] introduces LLMEval2, which encompasses a wide range of capability evaluations.In addition, Xiezhi [59] presents a comprehensive suite for assessing the knowledge level of large-scale language models in different subject areas.The evaluation conducted through Xiezhi enables researchers to comprehend the notable limitations inherent in these models and facilitates a deeper comprehension of their capabilities in diverse fields.For evaluating language models beyond their existing capacities, BIG-bench [182] introduces a diverse collection of 204 challenging tasks contributed by 450 authors from 132 institutions.These tasks cover various domains such as math, childhood development, linguistics, biology, common-sense reasoning, social bias, physics, software development, etc.</p>
<p>Recent work has led to the development of benchmarks for evaluating language models' knowledge and reasoning abilities.The Knowledge-Oriented Language Model Evaluation KoLA [236] focuses on assessing language models' comprehension and utilization of semantic knowledge for inference.As such, KoLA serves as an important benchmark for evaluating the depth of language understanding and reasoning in language models, thereby driving progress in language comprehension.To enable crowd-sourced evaluations of language tasks, DynaBench [94] supports dynamic benchmark testing.DynaBench explores new research directions including the effects of closed-loop integration, distributional shift characteristics, annotator efficiency, influence of expert annotators, and model robustness to adversarial attacks in interactive settings.Furthermore, to evaluate language models' ability to learn and apply multidisciplinary knowledge across educational levels, the Multidisciplinary Knowledge Evaluation M3KE [122] was recently introduced.M3KE assesses knowledge application within the Chinese education system.</p>
<p>The development of standardized benchmarks for evaluating LLMs on diverse tasks has been an important research focus.MMLU [70] provides a comprehensive suite of tests for assessing text models in multi-task contexts.AlpacaEval [112] stands as an automated evaluation benchmark, which places its focus on assessing the performance of LLMs across various natural language processing tasks.It provides a range of metrics, robustness measures, and diversity evaluations to gauge the capabilities of LLMs.AlpacaEval has significantly contributed to advancing LLMs in diverse domains and promoting a deeper understanding of their performance.Furthermore, AGIEval [262], serves as a dedicated evaluation framework for assessing the performance of foundation models in the domain of human-centric standardized exams.Moreover, OpenLLM [80] functions as an evaluation benchmark by offering a public competition platform for comparing and assessing different LLM models' performance on various tasks.It encourages researchers to submit their models and compete on different tasks, driving progress and competition in LLM research.</p>
<p>As for tasks beyond standard performance, there are benchmarks designed for OOD, adversarial robustness, and fine-tuning.GLUE-X [234] is a novel attempt to create a unified benchmark aimed at evaluating the robustness of NLP models in OOD scenarios.This benchmark emphasizes the significance of robustness in NLP and provides insights into measuring and enhancing the robustness of models.In addition, Yuan et al. [239] presents BOSS, a benchmark collection for assessing out-of-distribution robustness in natural language processing tasks.PromptBench [264] centers on the importance of prompt engineering in fine-tuning LLMs.It provides a standardized evaluation framework to compare different prompt engineering techniques and assess their impact on model performance.PromptBench facilitates the enhancement and optimization of fine-tuning methods for LLMs.To ensure impartial and equitable evaluation, PandaLM [216] is introduced as a discriminative large-scale language model specifically designed to differentiate among multiple high-proficiency LLMs through training.In contrast to conventional evaluation datasets that predominantly emphasize objective correctness, PandaLM incorporates crucial subjective elements, including relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality.</p>
<p>Benchmarks for Specific Downstream Tasks</p>
<p>Other than benchmarks for general tasks, there exist benchmarks specifically designed for certain downstream tasks.</p>
<p>Question-answering benchmarks have become a fundamental component in the assessment of LLMs and their overall performance.MultiMedQA [177] is a medical QA benchmark that focuses on medical examinations, medical research, and consumer healthcare questions.It consists of seven datasets related to medical QA, including six existing datasets and one new dataset.The goal of this benchmark is to evaluate the performance of LLMs in terms of clinical knowledge and QA abilities.To assess the ability of LLMs in dynamic QA about current world knowledge, Vu et al. [198] introduced FRESHQA.By incorporating relevant and current information retrieved from search engines into prompts, there is a significant enhancement in the performance of LLMs on FRESHQA.To effectively assess in-depth dialogue, Wang et al. [205] introduced the Dialogue CoT, incorporating two efficient dialogue strategies: Explicit CoT and CoT.</p>
<p>The assessment of LLMs in diverse and demanding tasks has garnered substantial attention in recent research.To this end, a range of specialized benchmarks have been introduced to evaluate LLMs' capabilities in specific domains and applications.Among these, ARB, as presented by Sawada et al. [171], focuses on probing the performance of LLMs in advanced reasoning tasks spanning multiple domains.Additionally, ethical considerations in LLMs have become an area of paramount importance.TRUSTGPT, as tailored by Huang et al. [79], addresses critical ethical dimensions, including toxicity, bias, and value alignment, within the context of LLMs.Furthermore, the simulation of human emotional reactions by LLMs remains an area with significant potential for improvement, as highlighted by the EmotionBench benchmark by Huang et al. [76].In terms of security evaluation, Zhang et al. [254] have introduced SafetyBench, a benchmark specifically designed to test the security performance of a range of popular Chinese and English LLMs.The results of this evaluation reveal substantial security flaws in current LLMs.To evaluate the daily decision-making capabilities of intelligent systems, Hou et al. [75] introduced Choice-75.Additionally, to assess LLMs' aptitude in understanding complex instructions, He et al. [66] have introduced CELLO.This benchmark encompasses the design of eight distinctive features, the development of a comprehensive evaluation dataset, and the establishment of four evaluation criteria alongside their respective measurement standards.</p>
<p>Other specific benchmarks such as C-Eval [78], which is the first extensive benchmark to assess the advanced knowledge and reasoning capabilities of foundation models in Chinese.Additionally, Li et al. [108] introduces CMMLU as a comprehensive Chinese proficiency standard and evaluates the performance of 18 LLMs across various academic disciplines.The findings reveal that the majority of LLMs demonstrate suboptimal performance in Chinese language environments, highlighting areas for improvement.M3Exam [250] provides a unique and comprehensive evaluation framework that incorporates multiple languages, modalities, and levels to test the general capabilities of LLMs in diverse contexts.Additionally, GAOKAO-Bench [245] provides a comprehensive evaluation benchmark for gauging the proficiency of large language models in intricate and context-specific tasks, utilizing questions sourced from the Chinese Gaokao examination.On the other hand, SOCKET [23] serves as an NLP benchmark designed to evaluate the performance of LLMs in learning and recognizing social knowledge concepts.It consists of several tasks and case studies to assess the limitations of LLMs in social capabilities.MATH [72] concentrates on assessing reasoning and problem-solving proficiencies of AI models within the domain of mathematics.APPS [68] is a more comprehensive and rigorous benchmark for evaluating code generation, measuring the ability of language models to generate python code according to natural language specifications.CUAD [71] is an expert-annotated, domain-specific legal contract review dataset that presents a challenging research benchmark and potential for enhancing deep learning models' performance in contract understanding tasks.CVALUES [230] introduces a humanistic evaluation benchmark to assess the alignment of LLMs with safety and responsibility standards.In the realm of comprehensive Chinese medicine, Wang et al. [211] introduced CMB, a medical evaluation benchmark rooted in the Chinese language and culture.It addresses the potential inconsistency in the local context that may arise from relying solely on English-based medical assessments.In the realm of hallucination assessment, [116] has developed UHGEval, a benchmark specifically designed to evaluate the performance of Chinese LLMs in text generation without being constrained by hallucination-related limitations.</p>
<p>In addition to existing evaluation benchmarks, there is a research gap in assessing the effectiveness of utilizing tools for LLMs.To address this gap, the API-Bank benchmark [109] is introduced as the first benchmark explicitly designed for tool-augmented LLMs.It comprises a comprehensive Tool-Augmented LLM workflow, encompassing 53 commonly used API tools and 264 annotated</p>
<p>Method</p>
<p>References Human-in-the-loop AdaVision [50], AdaTest [164] Crowd-sourcing testing DynaBench [94], DynaBoard [132], DynamicTempLAMA [135], DynaTask [188] More challenging tests HELM [114], AdaFilter [157], CheckList [165], Big-Bench [182], DeepTest [190] dialogues, encompassing a total of 568 API calls.Furthermore, the ToolBench project [191] aims to empower the development of large language models that effectively leverage the capabilities of general-purpose tools.By providing a platform for creating optimized instruction datasets, the ToolBench project seeks to drive progress in language models and enhance their practical applications.To evaluate LLMs in multi-turn interactions, Wang et al. [213] proposed MINT, which utilizes tools and natural language feedback.</p>
<p>Benchmarks for Multi-modal task</p>
<p>For the evaluation of Multimodal Large Language Models (MLLMs), MME [46] serves as an extensive evaluative benchmark, aiming to assess their perceptual and cognitive aptitudes.It employs meticulously crafted instruction-answer pairs alongside succinct instruction design, thereby guaranteeing equitable evaluation conditions.To robustly evaluate large-scale vision-language models, Liu et al. [126] introduced MMBench, which comprises a comprehensive dataset and employs a CircularEval assessment method.Additionally, MMICL [255] enhances visual language models for multimodal inputs and excels in tasks such as MME and MMBench.Furthermore, LAMM [235] extends its research to encompass multimodal point clouds.LVLM-eHub [231] undertakes an exhaustive evaluation of LVLMs using an online competitive platform and quantitative capacity assessments.To comprehensively assess the generative and understanding capabilities of Multi-modal Large Language Models (MLLMs), Li et al. [107] introduced a novel benchmark named SEED-Bench.This benchmark consists of 19,000 multiple-choice questions that have been annotated by human assessors.Additionally, the evaluation covers 12 different aspects, including the models' proficiency in understanding patterns within images and videos.In summary, recent works have developed robust benchmarks and improved models that advance the study of multimodal languages.</p>
<p>HOW TO EVALUATE</p>
<p>In this section, we introduce two common evaluation methods: automatic evaluation and human evaluation.Our categorization is based on whether or not the evaluation criterion can be automatically computed.If it can be automatically calculated, we categorize it into automatic evaluation; otherwise, it falls into human evaluation.</p>
<p>Automatic Evaluation</p>
<p>Automated evaluation is a common, and perhaps the most popular, evaluation method that typically uses standard metrics and evaluation tools to evaluate model performance.Compared with human evaluation, automatic evaluation does not require intensive human participation, which not only saves time, but also reduces the impact of human subjective factors and makes the evaluation process more standardized.For example, both Qin et al. [159] and Bang et al. [6] use automated evaluation methods to evaluate a large number of tasks.Recently, with the development of LLMs, some advanced automatic evaluation techniques are also designed to help evaluate.Lin and Chen [121] proposed LLM-EVAL, a unified multidimensional automatic evaluation method for open-domain conversations with LLMs.PandaLM [216] can achieve reproducible and automated language model assessment by training an LLM that serves as the "judge" to evaluate different models.Proposing a Attack success rate [203], Performance drop rate [264] self-supervised evaluation framework, Jain et al. [82] enabled a more efficient form of evaluating models in real-world deployment by eliminating the need for laborious labeling of new data.In addition, many benchmarks also apply automatic evaluation, such as MMLU [70], HELM [114], C-Eval [78], AGIEval [262], AlpacaFarm [38], Chatbot Arena [128], etc.</p>
<p>Based on the literature that adopted automatic evaluation, we summarized the main metrics in automatic evaluation in Table 9.The key metrics include the following four aspects:</p>
<p>(1) Accuracy is a measure of how correct a model is on a given task.The concept of accuracy may vary in different scenarios and is dependent on the specific task and problem definition.</p>
<p>It can be measured using various metrics such as Exact Match, F1 score, and ROUGE score.</p>
<p> Exact Match (EM) is a metric used to evaluate whether the model's output in text generation tasks precisely matches the reference answer.In question answering tasks, if the model's generated answer is an exact match with the manually provided answer, the EM is 1; otherwise, it is 0.  The F1 score is a metric for evaluating the performance of binary classification models, combining the model's precision and recall.The formula for calculation is as follows:  1 =</p>
<p>2</p>
<ul>
<li>.</li>
</ul>
<p> ROUGE is primarily employed to assess the performance of tasks such as text summarization and machine translation, involving considerations of overlap and matching between texts.(2) Calibrations pertains to the degree of agreement between the confidence level of the model output and the actual prediction accuracy.</p>
<p> Expected Calibration Error (ECE) is one of the commonly used metrics to evaluate model calibration performance [60].Tian et al. [189] utilized ECE to study the calibration of RLHF-LMs, including ChatGPT, GPT-4, Claude 1, Claude 2 and Llama2.For the calculation of ECE, they categorize model predictions based on confidence and measure the average accuracy of the predictions within each confidence interval. Area Under the Curve of selective accuracy and coverage (AUC) [54] is another commonly used metric.(3) Fairness refers to whether the model treats different groups consistently, that is, whether the model's performance is equal across different groups.This can include attributes such as gender, race, age, and more.DecodingTrust [201] employs the following two metrics for measuring fairness:</p>
<p> Demographic Parity Pifference (DPD) measures whether the model's predictions are distributed equally across different population groups.If predictions differ significantly between groups, the DPD is high, indicating that the model may be unfairly biased against different groups.The calculation of DPD involves the prediction of the model and the true label, and the following formula can be used:  =  ( | = 1)   ( | = 0), where  is the binary classification prediction of the model, Z is the identifier of the population group (usually binary, indicating two different groups, such as men and women),  ( | = 1) and  ( | = 0) respectively represent the probabilities of predicting the positive class in population  = 1 and  = 0.</p>
<p> Equalized Odds Difference (EOD) aims to ensure that the model provides equal error rates across different populations, that is, the model's prediction error probability distribution is similar for different populations.The calculation of EOD involves probabilities related to true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.The formula for EOD is as follows:  { (  = 1| = 1,  = 1)   (  = 1| = 1,  = 0),  (  = 1| = 0,  = 1)   (  = 1| = 0,  = 0)} where  is the binary classification prediction of the model,  is the true label,  is the demographic group identifier (typically binary, representing two different groups), and  (  = 1| = 1,  = 1) denotes the probability of the model predicting a positive class when the true label is positive and belongs to group  = 1.(4) Robustness evaluates the performance of a model in the face of various challenging inputs, including adversarial attacks, changes in data distribution, noise, etc.</p>
<p> Attack Success Rate (ASR) serves as a metric for evaluating the adversarial robustness of LLMs [206].Specifically, consider a dataset D = {(  ,   )}  =1 containing  pairs of samples   and ground truth   .For an adversarial attack method A, given an input , this method can produce adversarial examples A () to attack surrogate model  , with the success rate is calculated as:  = (,  )
I [  ( A ( ) ) ] I [  ( )= ]
, where I is the indicator function [203].</p>
<p> Performance Drop Rate (PDR), a new unified metric, effectively assesses the robustness of prompt in LLMs [264].PDR quantifies the relative performance degradation after a prompt attack, and the formula is as follows:  = 1  (,)  M [  ( [( ), ] ), ] (,)  M [  ( [, ] ), ] , where  represents the adversarial attack applied to prompt , and  denotes the evaluation function, which varies across different tasks [264].</p>
<p>Human Evaluation</p>
<p>The increasingly strengthened capabilities of LLMs have certainly gone beyond standard evaluation metrics on general natural language tasks.Therefore, human evaluation becomes a natural choice in some non-standard cases where automatic evaluation is not suitable.For instance, in open-generation tasks where embedded similarity metrics (such as BERTScore) are not enough, human evaluation is more reliable [142].While some generation tasks can adopt certain automatic evaluation protocols, human evaluation in these tasks is more favorable as generation can always go better than standard answers.</p>
<p>Human evaluation is a way to evaluate the quality and accuracy of model-generated results through human participation.Compared with automatic evaluation, manual evaluation is closer to the actual application scenario and can provide more comprehensive and accurate feedback.In the manual evaluation of LLMs, evaluators (such as experts, researchers, or ordinary users) are usually invited to evaluate the results generated by the model.For example, Ziems et al. [269] used the annotations from experts for generation.By human evaluation, Liang et al. [114] assessed on summarization and disinformation scenarios on 6 models and Bang et al. [6] evaluated analogical reasoning tasks.Bubeck et al. [15] did a series of human-crafted tests using GPT-4 and they found that GPT-4 performs close to or even exceeds human performance on multiple tasks.This evaluation requires human evaluators to actually test and compare the performance of the models, not just evaluate the models through automated evaluation metrics.Note that even human evaluations can have high variance and instability, which could be due to cultural and individual differences [155].In practical applications, these two evaluation methods are considered and weighed in combination with the actual situation.</p>
<p>Exploring the human evaluation methods of LLMs requires thoughtful attention to various crucial factors to guarantee the dependability and precision of assessments [178].Table 10 provides a concise overview of the essential aspects of human evaluation, including the number of evaluators,  [178], Relevance [261], Fluency [196], Transparency, Safety [85], Human alignment Evaluator's expertise level Relevant domain expertise [144], Task familiarity, Methodological training evaluation criteria, and evaluator's expertise level.Primarily, the number of evaluators emerges as a crucial factor intricately intertwined with adequate representation and statistical significance.A judiciously chosen number of evaluators contributes to a more nuanced and comprehensive understanding of the LLMs under scrutiny, enabling a more reliable extrapolation of the results to a broader context.</p>
<p>Furthermore, evaluation criteria are fundamental components of the human assessment process.Expanding upon the principles of the 3H rule (Helpfulness, Honesty, and Harmlessness) [4], we have elaborated them into the following 6 human assessment criteria.These criteria include accuracy, relevance, fluency, transparency, safety, and human alignment.Through the application of these standards, a thorough analysis of LLMs' performance in syntax, semantics, and context is achieved, allowing for a more comprehensive evaluation of the quality of generated text.</p>
<p>(1) Accuracy [178] stands out as a pivotal criterion that assesses the precision and correctness of the generated text.It involves scrutinizing the extent to which the language model produces information that aligns with factual knowledge, avoiding errors and inaccuracies.(2) Relevance [261] focuses on the appropriateness and significance of the generated content.</p>
<p>This criterion examines how well the text addresses the given context or query, ensuring that the information provided is pertinent and directly applicable.(3) Fluency [196] assesses the language model's ability to produce content that flows smoothly, maintaining a consistent tone and style.A fluent text is not only grammatically correct but also ensures readability and a seamless user experience.Analysts evaluate how well the model avoids awkward expressions and abrupt shifts in language or topic, contributing to effective communication with users.(4) Transparency delves into the clarity and openness of the language model's decision-making process.It involves assessing how well the model communicates its thought processes, enabling users to understand how and why certain responses are generated.A transparent model provides insights into its inner workings.(5) Safety [85] emerges as a critical criterion concerned with the potential harm or unintended consequences arising from the generated text.It examines the language model's ability to avoid producing content that may be inappropriate, offensive, or harmful, ensuring the well-being of users and avoiding misinformation.(6) Human alignment assesses the degree to which the language model's output aligns with human values, preferences, and expectations.It considers the ethical implications of the generated content, ensuring that the language model produces text that respects societal norms and user expectations, promoting a positive interaction with human users.</p>
<p>Lastly, the expertise level of evaluators is a critical consideration, encompassing relevant domain knowledge, task familiarity, and methodological training.Delineating the requisite expertise level for evaluators ensures that they possess the necessary background knowledge to accurately comprehend and assess the domain-specific text generated by LLMs.This strategy adds a layer of rigor to the evaluation process, reinforcing the credibility and validity of the findings.</p>
<p>SUMMARY</p>
<p>In this section, we summarize the key findings based on our review in sections 3, 4, and 5.</p>
<p>First of all, we would like to highlight that despite all the efforts spent on summarizing existing works on evaluation, there is no evidence to explicitly show that one certain evaluation protocol or benchmark is the most useful and successful, but with different characteristics and focuses.This also demonstrates that not a single model can perform best in all kinds of tasks.The purpose of this survey is to go beyond simply determining the "best" benchmark or evaluation protocol.By summarizing and analyzing existing efforts on LLMs evaluation, we may identify the current success and failure cases of LLMs, derive new trends for evaluation protocols, and most importantly, propose new challenges and opportunities for future research.</p>
<p>Task: Success and Failure Cases of LLMs</p>
<p>We now summarize the success and failure cases of LLMs in different tasks.Note that all the following conclusions are made based on existing evaluation efforts and the results are only dependent on specific datasets.</p>
<p>What can LLMs do well?</p>
<p> LLMs demonstrate proficiency in generating text [11,14,24] by producing fluent and precise linguistic expressions. LLMs obtain impressive performance in tasks involving language understanding, including sentiment analysis [52,129,159], text classification [114,154,233], as well as the handling of factual input [159]. LLMs demonstrate robust arithmetic reasoning capabilities [159] and excel in logical reasoning [124].Moreover, they exhibit noteworthy proficiency in temporal reasoning [6].Furthermore, more intricate tasks such as mathematical reasoning [226,237,244] and structured data inference [86,151] have emerged as the prevailing benchmarks for evaluation. LLMs exhibit robust contextual comprehension, enabling them to generate coherent responses that align with the given input [187]. LLMs also achieve satisfying performance across several natural language processing tasks, including machine translation [6,130,208], text generation [20], and question answering [102,114].</p>
<p>When can LLMs fail?</p>
<p> Within the realm of NLI, LLMs exhibit subpar performance and encounter challenges in accurately representing human disagreements [105]. LLMs exhibit restricted proficiency in discerning semantic similarity between events [184] and demonstrate substandard performance in evaluating fundamental phrases [166]. LLMs have limited abilities on abstract reasoning [56], and are prone to confusion or errors in complex contexts [148]. In linguistic contexts featuring non-Latin scripts and limited resources, LLMs manifest suboptimal performance [2,6,100,250].Furthermore, generative LLMs consistently display proficiency levels below the expected standards across various tasks and languages [2]. LLMs demonstrate susceptibility when processing visual modal information [258].Furthermore, they have the capacity to assimilate, disseminate, and potentially magnify detrimental content found within the acquired training datasets, frequently encompassing toxic linguistic elements, including offensive, hostile, and derogatory language [53]. LLMs may exhibit social biases and toxicity [37,53,153] during the generation process, resulting in the production of biased outputs.</p>
<p> LLMs may manifest credibility deficits [201], potentially giving rise to fabricated information or erroneous facts within dialogues [163,253]. LLMs have limitations in incorporating real-time or dynamic information [127], making them less suitable for tasks that require up-to-date knowledge or rapid adaptation to changing contexts. LLMs is sensitive to prompts, especially adversarial prompts [264], which trigger new evaluations and algorithms to improve its robustness.</p>
<p>Benchmark and Evaluation Protocol</p>
<p>With the rapid development and widespread use of LLMs, the importance of evaluating them in practical applications and research has become crucial.This evaluation process should include not only task-level evaluation but also a deep understanding of the potential risks they pose from a societal perspective.In this section, we summarize existing benchmarks and protocols in Table 8.First, a shift from objective calculation to human-in-the-loop testing, allowing for greater human feedback during the evaluation process.AdaVision [50], an interactive process for testing vision models, enables users to label a small amount of data for model correctness, which helps users identify and fix coherent failure modes.In AdaTest [164], the user filters test samples by only selecting high-quality tests and organizing them into semantically related topics.</p>
<p>Second, a move from static to crowd-sourcing test sets is becoming more common.Tools like DynaBench [94], DynaBoard [132], and DynaTask [188] rely on crowdworkers to create and test hard samples.Additionally, DynamicTempLAMA [135] allows for dynamically constructed time-related tests.</p>
<p>Third, a shift from a unified to a challenging setting in evaluating machine learning models.While unified settings involve a test set with no preference for any specific task, challenging settings create test sets for specific tasks.Tools like DeepTest [190] use seeds to generate input transformations for testing, CheckList [165] builds test sets based on templates, and AdaFilter [157] adversarially constructs tests.However, it is worth noting that AdaFilter may not be entirely fair as it relies on adversarial examples.HELM [114] evaluates LLMs from different aspects, while the Big-Bench [182] platform is used to design hard tasks for machine learning models to tackle.PromptBench [264] aims to evaluate the adversarial robustness of LLMs by creating adversarial prompts, which is more challenging and the results demonstrated that current LLMs are not robust to adversarial prompts.</p>
<p>GRAND CHALLENGES AND OPPORTUNITIES FOR FUTURE RESEARCH</p>
<p>Evaluation as a new discipline: Our summarization inspires us to redesign a wide spectrum of aspects related to evaluation in the era of LLMs.In this section, we present several grand challenges.Our key point is that evaluation should be treated as an essential discipline to drive the success of LLMs and other AI models.Existing protocols are not enough to thoroughly evaluate the true capabilities of LLMs, which poses grand challenges and triggers new opportunities for future research on LLMs evaluation.</p>
<p>Designing AGI Benchmarks</p>
<p>As we discussed earlier, while all tasks can potentially serve as evaluation tools for LLMs, the question remains as to which can truly measure AGI capabilities.As we expect LLMs to demonstrate AGI abilities, a comprehensive understanding of the differences between human and AGI capacities becomes crucial in the creation of AGI benchmarks.The prevailing trend seems to conceptualize AGI as a superhuman entity, thereby utilizing cross-disciplinary knowledge from fields such as education, psychology, and social sciences to design innovative benchmarks.Nonetheless, there remains a plethora of unresolved issues.For instance, does it make sense to use human values as a starting point for test construction, or should alternative perspectives be considered?Developing suitable AGI benchmarks presents many open questions demanding further exploration.</p>
<p>Complete Behavioral Evaluation</p>
<p>An ideal AGI evaluation should contain not only standard benchmarks on common tasks, but also evaluations on open tasks such as complete behavioral tests.By behavioral test, we mean that AGI models should also be evaluated in an open environment.For instance, by treating LLMs as the central controller, we can construct evaluations on a robot manipulated by LLMs to test its behaviors in real situations.By treating LLMs as a completely intelligent machine, the evaluations of its multi-modal dimensions should also be considered.In fact, complete behavioral evaluations are complementary to standard AGI benchmarks and they should work together for better testing.</p>
<p>Robustness Evaluation</p>
<p>Beyond general tasks, it is crucial for LLMs to maintain robustness against a wide variety of inputs in order to perform optimally for end-users, given their extensive integration into daily life.For instance, the same prompts but with different grammars and expressions could lead ChatGPT and other LLMs to generate diverse results, indicating that current LLMs are not robust to the inputs.While there are some prior work on robustness evaluation [206,264], there are much room for advancement, such as including more diverse evaluation sets, examining more evaluation aspects, and developing more efficient evaluations to generate robustness tasks.Concurrently, the concept and definition of robustness are constantly evolving.It is thus vital to consider updating the evaluation system to better align with emerging requirements related to ethics and bias.</p>
<p>Dynamic and Evolving Evaluation</p>
<p>Existing evaluation protocols for most AI tasks rely on static and public benchmarks, i.e., the evaluation datasets and protocols are often publicly available.While this facilitates rapid and convenient evaluation within the community, it is unable to accurately assess the evolving abilities of LLMs, given their rapid rate of development.The capabilities of LLMs may enhance over time which cannot be consistently evaluated by existing static benchmarks.On the other hand, as LLMs grow increasingly powerful with larger model sizes and training set sizes, static and public benchmarks are likely to be memorized by LLMs, resulting in potential training data contamination.Therefore, developing dynamic and evolving evaluation systems is the key to providing a fair evaluation of LLMs.</p>
<p>Principled and Trustworthy Evaluation</p>
<p>When introducing an evaluation system, it is crucial to ascertain its integrity and trustworthiness.Therefore, the necessity for trustworthy computing extends to the requirement for reliable evaluation systems as well.This poses a challenging research question that intertwines with measurement theory, probability, and numerous other domains.For instance, how can we ensure that dynamic testing truly generates out-of-distribution examples?There is a scarcity of research in this domain, and it is hoped that future work will aim to scrutinize not only the algorithms but the evaluation system itself.</p>
<p>Unified Evaluation that Supports All LLMs Tasks</p>
<p>There are many other research areas of LLMs and we need to develop evaluation systems that can support all kinds of tasks such as value alignment, safety, verification, interdisciplinary research, fine-tuning, and others.For instance, PandaLM [216] is an evaluation system that assists LLMs fine-tuning by providing an open-source evaluation model, which can automatically assess the performance of fine-tuning.We expect that more evaluation systems are becoming more general and can be used as assistance in certain LLMs tasks.</p>
<p>Beyond Evaluation: LLMs Enhancement</p>
<p>Ultimately, evaluation is not the end goal but rather the starting point.Following the evaluation, there are undoubtedly conclusions to be drawn regarding performance, robustness, stability, and other factors.A proficient evaluation system should not only offer benchmark results but should also deliver an insightful analysis, recommendations, and guidance for future research and development.For instance, PromptBench [264] provides not only robustness evaluation results on adversarial prompts but also a comprehensive analysis through attention visualization, elucidating how adversarial texts can result in erroneous responses.The system further offers a word frequency analysis to identify robust and non-robust words in the test sets, thus providing prompt engineering guidance for end users.Subsequent research can leverage these findings to enhance LLMs.Another example is that Wang et al. [215] first explored the performance of large vision-language models on imbalanced (long-tailed) tasks, which demonstrates the limitation of current large models.Then, they explored different methodologies to enhance the performance on these tasks.In summary, enhancement after evaluation helps to build better LLMs and much can be done in the future.</p>
<p>CONCLUSION</p>
<p>Evaluation carries profound significance, becoming imperative in the advancement of AI models, especially within the context of large language models.This paper presents the first survey to give a comprehensive overview of the evaluation on LLMs from three aspects: what to evaluate, how to evaluate, and where to evaluate.By encapsulating evaluation tasks, protocols, and benchmarks, our aim is to augment understanding of the current status of LLMs, elucidate their strengths and limitations, and furnish insights for future LLMs progression.</p>
<p>Our survey reveals that current LLMs exhibit certain limitations in numerous tasks, notably reasoning and robustness tasks.Concurrently, the need for contemporary evaluation systems to adapt and evolve remains evident, ensuring the accurate assessment of LLMs' inherent capabilities and limitations.We identify several grand challenges that future research should address, with the aspiration that LLMs can progressively enhance their service to humanity.</p>
<p>ACKNOWLEDGEMENTS</p>
<p>This work is supported in part by NSF under grant III-2106758.</p>
<p>DISCLAIMER</p>
<p>The goal of this paper is mainly to summarize and discuss existing evaluation efforts on large language models.Results and conclusions in each paper are original contributions of their corresponding authors, particularly for potential issues in ethics and biases.This paper may discuss some side effects of LLMs and the only intention is to foster a better understanding.</p>
<p>Due to the evolution of LLMs especially online services such as Claude and ChatGPT, it is very likely that they become stronger and some of the limitations described in this paper are mitigated (and new limitations may arise).We encourage interested readers to take this survey as a reference for future research and conduct real experiments in current systems when performing evaluations.</p>
<p>Finally, the evaluation of LLMs is continuously developing, thus we may miss some new papers or benchmarks.We welcome all constructive feedback and suggestions.</p>
<p>Fig. 2 .
2
Fig. 2. Trend of LLMs evaluation papers over time (2020 -Jun.2023, including Jul. 2023.).</p>
<p>Table 1
1
provides a brief comparison of traditional ML, deep learning, and LLMs.</p>
<p>Table 1 .
1
Comparison of Traditional ML, Deep Learning, and LLMs
ComparisonTraditional ML Deep LearningLLMsTraining Data SizeLargeLargeVery largeFeature EngineeringManualAutomaticAutomaticModel ComplexityLimitedComplexVery ComplexInterpretabilityGoodPoorPoorerPerformanceModerateHighHighestHardware RequirementsLowHighVery HighWhatWhereHow(Task)(Data)(Process)</p>
<p>Table 2 .
2
Summary of evaluation on natural language processing tasks: NLU (Natural Language Understanding, including SA (Sentiment Analysis), TC (Text Classification), NLI (Natural Language Inference) and other NLU tasks), Reasoning, NLG (Natural Language Generation, including Summ.(Summarization), Dlg.(Dialogue), Tran (Translation), QA (Question Answering) and other NLG tasks), and Multilingual tasks (ordered by the name of the first author).
ReferenceNLU SA TC NLI OthersRNG.NLG Summ. Dlg. Tran. QA OthersMul.Abdelali et al. [1]Ahuja et al. [2]Bian et al. [9]Bang et al. [6]Bai et al. [5]Chen et al. [20]Choi et al. [23]Chia et al. [22]Frieder et al. [45]Fu et al. [47]Gekhman et al. [55]Gendron et al. [56]Honovich et al. [74]Jiang et al. [86]Lai et al. [100]Laskar et al. [102]Lopez-Lira and Tang [129] Liang et al. [114]Lee et al. [105]Lin and Chen [121]Livin et al. [117]Liu et al. [124]Lyu et al. [130]Manakul et al. [133]Min et al. [138]Orr et al. [147]Pan et al. [151]Pea et al. [154]Pu and Demberg [158]Pezeshkpour [156]Qin et al. [159]Riccardi and Desai [166]Saparov et al. [170]Tao et al. [184]Wang et al. [208]Wang et al. [218]Wang et al. [204]Wu et al. [227]Wu et al. [226]Xu et al. [229]Yang and Menczer [233]Zheng et al. [259]Zhang et al. [251]Zhang et al. [250]Zhuang et al. [265]</p>
<p>Table 3 .
3
Summary of LLMs evaluation on robustness, ethics, biases, and trustworthiness (ordered by the name of the first author).
ReferenceRobustness Ethics and biases TrustworthinessCao et al. [16]Dhamala et al. [37]Deshpande et al. [35]Ferrara [42]Gehman et al. [53]Hartmann et al. [65]Hendrycks et al. [69]Hagendorff and Fabi [62]Li et al. [111]Liu et al. [123]Liu et al. [123]Li et al. [113]Parrish et al. [153]Rutinowski et al. [167]Rawte et al. [163]Sheng et al. [175]Simmons [176]Wang et al. [207]Wang et al. [206]Wang et al. [201]Wang et al. [209]Xie et al. [228]Yang et al. [234]Zhao et al. [258]Zhuo et al. [267]Zhu et al. [264]Zhuo et al. [266]Zhang et al. [253]</p>
<p>Table 4 .
4
Summary of evaluations on natural science and engineering tasks based on three aspects: Mathematics, General science and Engineering (ordered by the name of the first author).
ReferenceMathematics General science EngineeringArora et al. [3]Bubeck et al. [15]Castro Nascimento and Pimentel [18]Collins et al. [27]Dao and Le [31]Guo et al. [61]Liu et al. [125]Pallagani et al. [150]Sridhara et al. [181]Valmeekam et al. [194]Valmeekam et al. [195]Wei et al. [221]Wu et al. [225]Yuan et al. [241]Yu et al. [237]Zhuang et al. [265]</p>
<p>Table 5 .
5
Summary of evaluations on medical applications based on the three aspects: Medical queries, Medical assistants, and Medical examination (ordered by the name of the first author).
ReferenceMedical queries Medical examination Medical assistantsCascella et al. [17]Chervenak et al. [21]Duong and Solomon [39]Gilson et al. [57]Hamidi and Roberts [63]Holmes et al. [73]Jahan et al. [81]Johnson et al. [87]Khan et al. [93]Kung et al. [97]Lahat et al. [99]Lyu et al. [131]Oh et al. [143]Samaan et al. [169]Thirunavukarasu et al. [186]</p>
<p>Table 6 .
6
Summary of evaluations on other applications based on the four aspects: Education, Search and recommendation, Personality testing and Specific applications (ordered by the name of the first author).
ReferenceEducation Search and recommendation Personality testing Specific applicationsBodroza et al. [10]Dai et al. [30]de Winter [32]Dai et al. [29]Fan et al. [40]Hellas et al. [67]Jentzsch and Kersting [84]Lanzi and Loiacono [101]Le and Zhang [103]Li et al. [110]Liang et al. [115]Sun et al. [183]Song et al. [180]Safdari et al. [168]Thakur et al. [185]Wang and Demszky [210]Wang et al. [212]Wang et al. [216]Xu et al. [232]Yuan et al. [240]</p>
<p>Table 8 .
8
Summary of new LLMs evaluation protocols.</p>
<p>Table 9 .
9
Key metrics of automatic evaluation.
General metricsMetricsAccuracyExact match, Quasi-exact match, F1 score, ROUGE score [118]CalibrationsExpected calibration error [60], Area under the curve [54]FairnessDemographic parity difference [242], Equalized odds difference [64]Robustness</p>
<p>Table 10 .
10
Summary of key factors in human evaluation
Evaluation CriteriaKey FactorNumber of evaluatorsAdequate representation [7], Statistical significanceEvaluation rubricsAccuracy
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
Note that LLMs are evaluated in various tasks and the categorization in this paper is only one possible way for classification of these works. There are certainly other taxonomies.
Several NLP areas have intersections and thus our categorization of these areas is only one possible way to categorize. J. ACM, Vol.
, No. 4, Article 111. Publication date: August 2018.
The term 'trustworthiness' in this section refers to other work that contains more than robustness and ethics. J. ACM, Vol. 37, No.
, Article 111. Publication date: August 2018.
https://www.usmle.org/
Note that as the evaluation of LLMs is a hot research area, it is very likely that we cannot cover all benchmarks. We welcome suggestions and comments to make this list perfect.</p>
<p>Ahmed Abdelali, Hamdy Mubarak, Absar Shammur, Maram Chowdhury, Basel Hasanain, Sabri Mousi, Yassine El Boughorbel, Daniel Kheir, Fahim Izham, Majd Dalvi, Hawasly, arXiv:2305.14982Benchmarking Arabic AI with Large Language Models. 2023. 2023arXiv preprint</p>
<p>Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, arXiv:2303.12528Mega: Multilingual evaluation of generative ai. 2023. 2023arXiv preprint</p>
<p>Daman Arora, Himanshu Gaurav Singh, arXiv:2305.15074Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. 2023. 2023arXiv preprint</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, arXiv:2112.00861A general language assistant as a laboratory for alignment. 2021. 2021arXiv preprint</p>
<p>Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, arXiv:2306.04181Benchmarking Foundation Models with Language-Model-as-an-Examiner. 2023. 2023arXiv preprint</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023. 2023arXiv preprint</p>
<p>Comparing automatic and human evaluation of NLG systems. Anja Belz, Ehud Reiter, 2006In 11th conference of the european chapter of the association for computational linguistics</p>
<p>. Daniel Berrar, 2019Cross-Validation</p>
<p>Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. Xianpei Ning Bian, Le Han, Hongyu Sun, Yaojie Lin, Ben Lu, He, arXiv:2303.164212023. 2023arXiv preprint</p>
<p>Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. Bojana Bodroza, M Bojana, Ljubisa Dinic, Bojic, arXiv:2306.043082023. 2023arXiv preprint</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021. 2021arXiv preprint</p>
<p>What is intelligence?. Nathan Brody, International Review of Psychiatry. 111999. 1999</p>
<p>Class-based n-gram models of natural language. Vincent J Peter F Brown, Della Pietra, Jennifer C Peter V Desouza, Robert L Lai, Mercer, Computational linguistics. 181992. 1992</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023. 2023arXiv preprint</p>
<p>Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study. Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, Daniel Hershcovich, Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP). the First Workshop on Cross-Cultural Considerations in NLP (C3NLP)2023</p>
<p>Evaluating the feasibility of ChatGPT in healthcare: an analysis of multiple clinical and research scenarios. Marco Cascella, Jonathan Montomoli, Valentina Bellini, Elena Bignami, Journal of Medical Systems. 47332023. 2023</p>
<p>Do Large Language Models Understand Chemistry? A Conversation with ChatGPT. Cayque Monteiro, Castro Nascimento, Andr Silva Pimentel, Journal of Chemical Information and Modeling. 632023. 2023</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021. 2021arXiv preprint</p>
<p>Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu, arXiv:2304.007232023. 2023arXiv preprint</p>
<p>The promise and peril of using a large language model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with limitations. Joseph Chervenak, Harry Lieman, Miranda Blanco-Breindel, Sangita Jindal, Fertility and Sterility. 2023. 2023</p>
<p>Ken Yew, Pengfei Chia, Hong, arXiv:2306.04757Lidong Bing, and Soujanya Poria. 2023. INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. 2023arXiv preprint</p>
<p>Publication date. J Acm, August 201837</p>
<p>Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, David Jurgens, arXiv:2305.14938Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark. 2023. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022. 2022arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 2017. 201730Advances in neural information processing systems</p>
<p>Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. Benjamin Clavi, Alexandru Ciceu, Frederick Naylor, Guillaume Souli, Thomas Brightwell, International Conference on Applications of Natural Language to Information Systems. Springer2023</p>
<p>Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, arXiv:2306.01694Evaluating Language Models for Mathematics through Interactions. 2023. 2023arXiv preprint</p>
<p>Support-vector networks. Corinna Cortes, Vladimir Vapnik, Machine learning. 201995. 1995</p>
<p>Uncovering ChatGPT's Capabilities in Recommender Systems. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, arXiv:2305.02182Jun Xu. 2023. 2023arXiv preprint</p>
<p>Can large language models provide feedback to students? a case study on chatgpt. Wei Dai, Jionghao Lin, Flora Jin, Tongguang Li, Yi-Shan Tsai, Dragan Gasevic, Guanliang Chen, 2023. 2023</p>
<p>Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination. Xuan-Quy Dao, Ngoc-Bich Le, arXiv:2306.063312023. 2023arXiv preprint</p>
<p>Can ChatGPT pass high school exams on English language comprehension. Joost Cf De Winter, 2023. 2023Researchgate. Preprint</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. Aniket Deroy, Kripabandhu Ghosh, Saptarshi Ghosh, arXiv:2306.012482023. 2023arXiv preprint</p>
<p>Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, arXiv:2304.05335Toxicity in chatgpt: Analyzing persona-assigned language models. 2023. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Bold: Dataset and metrics for measuring biases in open-ended language generation. Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, arXiv:2305.143872023. 2023arXiv preprint</p>
<p>Analysis of large-language model versus human performance for genetics questions. Dat Duong, Benjamin D Solomon, European Journal of Human Genetics. 2023. 2023</p>
<p>Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, Qing Li, arXiv:2307.02046[cs.IR]Recommender Systems in the Era of Large Language Models (LLMs). 2023</p>
<p>Ddxplus: A new dataset for automatic medical diagnosis. Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, Joumana Ghosn, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Emilio Ferrara, arXiv:2304.03738Should chatgpt be biased? challenges and risks of bias in large language models. 2023. 2023arXiv preprint</p>
<p>GPT-3: Its nature, scope, limits, and consequences. Minds and Machines. Luciano Floridi, Massimo Chiriatti, 2020. 202030</p>
<p>Baby steps in evaluating the capacities of large language models. Frank Michael, Nature Reviews Psychology. 2023. 2023</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, arXiv:2301.13867Mathematical capabilities of chatgpt. 2023. 2023arXiv preprint</p>
<p>Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, arXiv:2306.13394MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. 2023. 2023arXiv preprint</p>
<p>Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, arXiv:2305.173062023. 2023arXiv preprint</p>
<p>Publication date. J Acm, August 201837</p>
<p>Estimation of prediction error by using K-fold cross-validation. Tadayoshi Fushiki, Statistics and Computing. 212011. 2011</p>
<p>Perceptron-based learning algorithms. I Stephen, Gallant, IEEE Transactions on neural networks. 11990. 1990</p>
<p>Irena Gao, Gabriel Ilharco, Scott Lundberg, Marco Tulio, Ribeiro , arXiv:2212.02774Adaptive Testing of Computer Vision Models. 2022. 2022arXiv preprint</p>
<p>Introduction to the special issue on statistical language modeling. Jianfeng Gao, Chin-Yew Lin, 2004</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, arXiv:2012.157232020. 2020arXiv preprint</p>
<p>RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Selective classification for deep neural networks. Yonatan Geifman, Ran El-Yaniv, 2017. 201730Advances in neural information processing systems</p>
<p>Trueteacher: Learning factual consistency evaluation with large language models. Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, Idan Szpektor, arXiv:2305.111712023. 2023arXiv preprint</p>
<p>Large Language Models Are Not Abstract Reasoners. Gal Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie, arXiv:2305.195552023. 2023arXiv preprint</p>
<p>How does CHATGPT perform on the United States Medical Licensing Examination? the implications of large language models for medical education and knowledge assessment. Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash, JMIR Medical Education. 9e453122023. 2023</p>
<p>Moral foundations theory: The pragmatic validity of moral pluralism. Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, Peter H Ditto, Advances in experimental social psychology. 472013Elsevier</p>
<p>Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, arXiv:2306.05783Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. 2023. 2023arXiv preprint</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2305.183652023. 2023arXiv preprint</p>
<p>Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -and Disappeared in GPT-4. Thilo Hagendorff, Sarah Fabi, arXiv:2306.07622[cs.CL]2023</p>
<p>Alaleh Hamidi, Kirk Roberts, arXiv:2306.02549Evaluation of AI Chatbots for Patient-Specific EHR Questions. 2023. 2023arXiv preprint</p>
<p>Equality of opportunity in supervised learning. Moritz Hardt, Eric Price, Nati Srebro, Advances in neural information processing systems. 292016. 2016</p>
<p>The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation. Jochen Hartmann, Jasper Schwenzow, Maximilian Witte, arXiv:2301.017682023. 2023arXiv preprint</p>
<p>Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, arXiv:2309.09150Can Large Language Models Understand Real-World Complex Instructions?. 2023. 2023arXiv preprint</p>
<p>Exploring the Responses of Large Language Models to Beginner Programmers. Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja Kujanp, Juha Sorva, arXiv:2306.057152023. 2023Help Requests. arXiv preprint</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, arXiv:2105.09938Measuring coding challenge competence with apps. 2021. 2021arXiv preprint</p>
<p>Aligning ai with shared human values. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt, arXiv:2008.022752020. 2020arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020. 2020arXiv preprint</p>
<p>Cuad: An expert-annotated nlp dataset for legal contract review. Dan Hendrycks, Collin Burns, Anya Chen, Spencer Ball, arXiv:2103.062682021. 2021arXiv preprint</p>
<p>Publication date. J Acm, August 201837</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021. 2021arXiv preprint</p>
<p>Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T Sio, Lisa A Mcgee, Jonathan B Ashman, Xiang Li, Tianming Liu, Jiajian Shen, arXiv:2304.01938Evaluating large language models on a highly-specialized topic, radiation oncology physics. 2023. 2023arXiv preprint</p>
<p>Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias, arXiv:2204.04991TRUE: Re-evaluating factual consistency evaluation. 2022. 2022arXiv preprint</p>
<p>Choice-75: A Dataset on Decision Branching in Script Learning. Joey Zhaoyi, Li Hou, Chris Zhang, Callison-Burch, arXiv:2309.117372023. 2023arXiv preprint</p>
<p>Jen-Tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R Lyu, arXiv:2308.03656Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench. 2023. 2023arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, arXiv:2302.140452023. 2023arXiv preprint</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, arXiv:2305.083222023. 2023arXiv preprint</p>
<p>Yue Huang, Qihui Zhang, Philip S Y , Lichao Sun, arXiv:2306.11507[cs.CL]TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models. 2023</p>
<p>Open-source Large Language Models Leaderboard. Huggingface, 2023</p>
<p>Israt Jahan, Md Tahmid Rahman, Chun Laskar, Jimmy Peng, Huang, arXiv:2306.04504Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. 2023. 2023arXiv preprint</p>
<p>Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein, arXiv:2306.136512023. 2023arXiv preprint</p>
<p>Online question and answer sessions: How students support their own and other students' processes of inquiry in a text-based learning environment. Malin Jansson, Stefan Hrastinski, Stefan Stenbom, Fredrik Enoksson, The Internet and Higher Education. 511008172021. 2021</p>
<p>ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. Sophie Jentzsch, Kristian Kersting, arXiv:2306.045632023. 2023arXiv preprint</p>
<p>Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Ruiyang Sun, Yizhou Wang, Yaodong Yang, arXiv:2307.046572023. 2023arXiv preprint</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.09645Structgpt: A general framework for large language model to reason over structured data. 2023. 2023arXiv preprint</p>
<p>Assessing the accuracy and reliability of AI-generated medical responses: an evaluation of the Chat-GPT model. Douglas Johnson, Rachel Goodman, Cosby Patrinely, Eli Stone, Rebecca Zimmerman, Sam Donald, Sean Chang, Avni Berkowitz, Eiman Finn, Jahangir, 2023. 2023</p>
<p>TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Language Models (Mostly) Know What They Know. Saurav Kadavath, Tom Conerly, Amanda Askell, T J Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zachary Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, ArXiv abs/2207.05221Josh Jacobson, John Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, Jack Clark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan2022. 2022</p>
<p>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, arXiv:2205.004452022. 2022arXiv preprint</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seler, Stefan Kchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Gnnemann, Eyke Hllermeier, Learning and Individual Differences. 10342023. 2023. August 2018102274. J. ACM. Publication date</p>
<p>What is intelligence?. Jean Khalfa, 1994. 1994</p>
<p>Clarisse Yousuf A Khan, Jennifer Hokia, Ben Xu, Ehlert, arXiv:2306.04926covLLM: Large Language Models for COVID-19 Biomedical Literature. 2023. 2023arXiv preprint</p>
<p>Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, arXiv:2104.14337Dynabench: Rethinking benchmarking in NLP. 2021. 2021arXiv preprint</p>
<p>A study of cross-validation and bootstrap for accuracy estimation and model selection. Ron Kohavi, In Ijcai. 141995</p>
<p>Stefan Kombrink, Tomas Mikolov, Martin Karafit, Luks Burget, Recurrent Neural Network Based Language Modeling in Meeting Recognition. 201111</p>
<p>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepao, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, PLoS digital health. 22e00001982023. 2023</p>
<p>Natural Questions: a Benchmark for Question Answering Research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, Transactions of the Association of Computational Linguistics. 2019. 2019</p>
<p>Adi Lahat, Eyal Shachar, Benjamin Avidan, Zina Shatz, Benjamin S Glicksberg, Eyal Klang, Evaluating the use of large language model in identifying top research questions in gastroenterology. 2023. 2023134164</p>
<p>Dac Viet, Lai, Trung Nghia, Amir Ngo, Ben Pouran, Hieu Veyseh, Franck Man, Trung Dernoncourt, Thien Huu Bui, Nguyen, arXiv:2304.05613ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. 2023. 2023arXiv preprint</p>
<p>Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. Pier Luca, Lanzi , Daniele Loiacono, arXiv:2303.021552023. 2023arXiv preprint</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang, arXiv:2305.18486A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. 2023. 2023arXiv preprint</p>
<p>Van-Hoang Le, Hongyu Zhang, arXiv:2306.01590An Evaluation of Log Parsing with ChatGPT. 2023. 2023arXiv preprint</p>
<p>Deep learning. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, nature. 5212015. 2015</p>
<p>Can Large Language Models Infer and Disagree Like Humans?. Noah Lee, Na Min An, James Thorne, arXiv:2305.137882023. 2023arXiv preprint</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019. 2019BartarXiv preprint</p>
<p>Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, arXiv:2307.16125Seed-bench: Benchmarking multimodal llms with generative comprehension. 2023. 2023arXiv preprint</p>
<p>Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, arXiv:2306.09212CMMLU: Measuring massive multitask language understanding in Chinese. 2023. 2023arXiv preprint</p>
<p>Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, arXiv:2304.08244[cs.CL]API-Bank: A Benchmark for Tool-Augmented LLMs. 2023</p>
<p>Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights. Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, Fajie Yuan, arXiv:2305.117002023. 2023arXiv preprint</p>
<p>Xinzhe Li, Ming Liu, Shang Gao, Wray Buntine, arXiv:2306.15261[cs.CL]A Survey on Out-of-Distribution Evaluation of Neural NLP Models. 2023</p>
<p>AlpacaEval: An Automatic Evaluator of Instruction-following Models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023. 2023arXiv preprint</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022. 2022arXiv preprint</p>
<p>. Tian Liang, Zhiwei He, Jen-Tes Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang, 2023Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</p>
<p>Publication date. J Acm, arXiv:2310.20499A Survey on Evaluation of Large Language Models. 374August 2018. 202339 arXiv preprint</p>
<p>UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation. Xun Liang, Shichao Song, Simin Niu, Zhiyu Li, Feiyu Xiong, Bo Tang, Zhaohui Wy, Dawei He, Peng Cheng, Zhonghao Wang, arXiv:2311.152962023. 2023arXiv preprint</p>
<p>Can large language models reason about medical questions?. Christoffer Egeberg Valentin Livin, Ole Hother, Winther, arXiv:2207.081432022. 2022arXiv preprint</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2109.07958Truthfulqa: Measuring how models mimic human falsehoods. 2021. 2021arXiv preprint</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringer2014. September 6-12, 2014Proceedings, Part V 13</p>
<p>LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models. Yen-Ting Lin, Yun-Nung Chen, arXiv:2305.137112023. 2023arXiv preprint</p>
<p>Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, Xiaowen Su, Qun Liu, Deyi Xiong, arXiv:2305.10263[cs.CL]M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. 2023</p>
<p>Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang, arXiv:2306.14565[cs.CV]Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. 2023</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439[cs.CL]Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. 2023</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, arXiv:2305.012102023. 2023arXiv preprint</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin, arXiv:2307.06281[cs.CV]MMBench: Is Your Multi-modal Model an All-around Player. 2023</p>
<p>Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, arXiv:2304.01852Summary of chatgpt/gpt-4 research and perspective towards the future of large language models. 2023. 2023arXiv preprint</p>
<p>LMSYS. 2023Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings. </p>
<p>Can chatgpt forecast stock price movements? Return predictability and large language models. Alejandro Lopez, - Lira, Yuehua Tang, arXiv:2304.076192023. 2023arXiv preprint</p>
<p>New trends in machine translation using large language models: Case examples with chatgpt. Chenyang Lyu, Jitao Xu, Longyue Wang, arXiv:2305.011812023. 2023arXiv preprint</p>
<p>Qing Lyu, Josh Tan, Mike E Zapadka, Janardhana Ponnatapuram, Chuang Niu, Ge Wang, Christopher T Whitlow, arXiv:2303.09038Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: Promising results, limitations, and potential. 2023. 2023arXiv preprint</p>
<p>Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking. Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, Douwe Kiela, Advances in Neural Information Processing Systems. 342021. 2021</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark, Gales, arXiv:2303.088962023. 2023arXiv preprint</p>
<p>MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization. Potsawee Manakul, Adian Liusie, Mark J F Gales, arXiv:2301.12307[cs.CL]2023</p>
<p>Katerina Margatina, Shuai Wang, Yogarshi Vyas, Anna Neha, Yassine John, Miguel Benajiba, Ballesteros, arXiv:2302.12297Dynamic benchmarking of masked language models on temporal concept drift with multiple views. 2023. 2023arXiv preprint</p>
<p>What is artificial intelligence. John Mccarthy, 2007. 2007</p>
<p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, arXiv:2305.142512023. 2023arXiv preprint</p>
<p>Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities Emergence. David John J Nay, Sarah B Karamardian, Wenting Lawsky, Meghana Tao, Raghav Bhat, Aaron Travis Jain, Jonathan H Lee, Jungo Choi, Kasai, arXiv:2306.070752023. 2023arXiv preprint</p>
<p>Publication date. J Acm, August 201837</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, arXiv:1910.14599Adversarial NLI: A new benchmark for natural language understanding. 2019. 2019arXiv preprint</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.13474Codegen: An open large language model for code with multi-turn program synthesis. 2022. 2022arXiv preprint</p>
<p>Jekaterina Novikova, Ondej Duek, Amanda Cercas Curry, Verena Rieser, arXiv:1707.06875Why we need new evaluation metrics for NLG. 2017. 2017arXiv preprint</p>
<p>ChatGPT goes to the operating room: evaluating GPT-4 performance and its potential in surgical education and training in the era of large language models. Namkee Oh, Gyu-Seong Choi, Woo Yong, Lee , Annals of Surgical Treatment and Research. 1042692023. 2023</p>
<p>Generating multiple choice questions from a textbook: Llms match human performance on most metrics. Andrew M Olney, AIED Workshops. 2023</p>
<p>Human-like problem-solving abilities in large language models using ChatGPT. Graziella Orr, Andrea Piarulli, Ciro Conversano, Angelo Gemignani, Frontiers in Artificial Intelligence. 62023. 2023</p>
<p>Simon Ott, Konstantin Hebenstreit, Valentin Livin, Egeberg Christoffer, Milad Hother, Maximilian Moradi, Robert Mayrhauser, Ole Praas, Matthias Winther, Samwald, arXiv:2301.11596ThoughtSource: A central hub for large language model reasoning data. 2023. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Understanding the Capabilities of Large Language Models for Automated Planning. Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, Andrea Loreggia, arXiv:2305.161512023. 2023arXiv preprint</p>
<p>Unifying Large Language Models and Knowledge Graphs: A Roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.08302[cs.CL]2023</p>
<p>Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022. 2022arXiv preprint</p>
<p>BBQ: A hand-built bias benchmark for question answering. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, Samuel Bowman, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Alejandro Pea, Aythami Morales, Julian Fierrez, Ignacio Serna, Javier Ortega-Garcia, Iigo Puente, Jorge Cordova, Gonzalo Cordova, arXiv:2306.02864Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs. 2023. 2023arXiv preprint</p>
<p>Validity problems comparing values across cultures and possible solutions. Kaiping Peng, Richard E Nisbett, Nancy Yc Wong, Psychological methods. 23291997. 1997</p>
<p>Pouya Pezeshkpour, arXiv:2306.06264Measuring and Modifying Factual Knowledge in Large Language Models. 2023. 2023arXiv preprint</p>
<p>Adversarially constructed evaluation sets are more challenging, but may not be fair. Jason Phang, Angelica Chen, William Huang, Samuel R Bowman, arXiv:2111.081812021. 2021arXiv preprint</p>
<p>Dongqi Pu, Vera Demberg, arXiv:2306.07799[cs.CL]ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. 2023</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, arXiv:2302.06476Is ChatGPT a general-purpose natural language processing task solver? arXiv preprint. 2023. 2023</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, arXiv:2304.08354[cs.CL]Tool Learning with Foundation Models. Zhiyuan Liu, and Maosong Sun2023</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun, arXiv:2307.16789[cs.AI]ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. 2023</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018. 2018</p>
<p>Amit Vipula Rawte, Amitava Sheth, Das, arXiv:2309.05922A Survey of Hallucination in Large Foundation Models. 2023. 2023arXiv preprint</p>
<p>Publication date. J Acm, August 201837</p>
<p>Adaptive testing and debugging of nlp models. Marco Tulio, Ribeiro , Scott Lundberg, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh, arXiv:2005.04118Beyond accuracy: Behavioral testing of NLP models with CheckList. 2020. 2020arXiv preprint</p>
<p>Nicholas Riccardi, H Rutvik, Desai, arXiv:2306.04610The Two Word Test: A Semantic Benchmark for Large Language Models. 2023. 2023arXiv preprint</p>
<p>Jrme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, Markus Pauly, arXiv:2304.07333The Self-Perception and Political Biases of ChatGPT. 2023. 2023arXiv preprint</p>
<p>Mustafa Safdari, Greg Serapio-Garca, Clment Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, Maja Matari, arXiv:2307.00184Personality Traits in Large Language Models. 2023. 2023arXiv preprint</p>
<p>Assessing the accuracy of responses by the language model ChatGPT to questions regarding bariatric surgery. Yee Jamil S Samaan, Nithya Hui Yeo, Lauren Rajeev, Stuart Hawley, Wee Abel, Han Ng, Nitin Srinivasan, Justin Park, Miguel Burch, Rabindra Watson, Obesity Surgery. 2023. 2023</p>
<p>Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, arXiv:2305.152692023. 2023arXiv preprint</p>
<p>Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, Aran Komatsuzaki, arXiv:2307.13692[cs.CL]ARB: Advanced Reasoning Benchmark for Large Language Models. 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023. 2023arXiv preprint</p>
<p>Mala Deep Upadhaya, Santosh Adhikari, and Salik Ram Khanal. Prabin Sharma, Kisan Thapa, Prastab Dhakal, arXiv:2307.00112Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. 2023. 2023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023. 2023arXiv preprint</p>
<p>Societal Biases in Language Generation: Progress and Challenges. Emily Sheng, Kai-Wei Chang, Prem Natarajan, Nanyun Peng, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Moral mimicry: Large language models produce moral rationalizations tailored to political identity. Gabriel Simmons, arXiv:2209.121062022. 2022arXiv preprint</p>
<p>Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, arXiv:2212.13138Large Language Models Encode Clinical Knowledge. 2022. 2022arXiv preprint</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 6202023. 2023</p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022. 2022arXiv preprint</p>
<p>Have Large Language Models Developed a Personality?. Xiaoyang Song, Akshat Gupta, Kiyan Mohebbizadeh, Shujie Hu, Anant Singh, arXiv:2305.14693Applicability of Self-Assessment Tests in Measuring Personality in LLMs. 2023. 2023arXiv preprint</p>
<p>Giriprasad Sridhara, Sourav Mazumdar, arXiv:2305.16837ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks. 2023. 2023arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adri Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022. 2022arXiv preprint</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, Zhaochun Ren, arXiv:2304.09542Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. 2023. 2023arXiv preprint</p>
<p>EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models. Zhengwei Tao, Zhi Jin, Xiaoying Bai, Haiyan Zhao, Yanlin Feng, Jia Li, Wenpeng Hu, arXiv:2305.152682023. 2023arXiv preprint</p>
<p>Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas Rckl, Abhishek Srivastava, Iryna Gurevych, arXiv:2104.086632021. 2021arXiv preprint</p>
<p>Publication date. J Acm, August 201837</p>
<p>Trialling a large language model (ChatGPT) in general practice with the Applied Knowledge Test: observational study demonstrating opportunities and limitations in primary care. Arun James Thirunavukarasu, Refaat Hassan, Shathar Mahmood, Rohan Sanghera, Kara Barzangi, Mohanned El Mukashfi, Sachin Shah, JMIR Medical Education. 9e465992023. 2023</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022. 2022arXiv preprint</p>
<p>Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas, Peter Mattson, Adina Williams, Douwe Kiela, arXiv:2204.01906Dynatask: A framework for creating dynamic AI benchmark tasks. 2022. 2022arXiv preprint</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D Manning, arXiv:2305.149752023. 2023arXiv preprint</p>
<p>Deeptest: Automated testing of deep-neural-networkdriven autonomous cars. Yuchi Tian, Kexin Pei, Suman Jana, Baishakhi Ray, Proceedings of the 40th international conference on software engineering. the 40th international conference on software engineering2018</p>
<p>Open-source tools learning benchmarks. Toolbench, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Computing machinery and intelligence. Alan M Turing, 2009Springer</p>
<p>Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.15771On the Planning Abilities of Large Language Models-A Critical Investigation. 2023. 2023arXiv preprint</p>
<p>Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022. 2022arXiv preprint</p>
<p>Best practices for the human evaluation of automatically generated text. Chris Van Der Lee, Albert Gatt, Emiel Van Miltenburg, Sander Wubben, Emiel Krahmer, Proceedings of the 12th International Conference on Natural Language Generation. the 12th International Conference on Natural Language Generation2019</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong, arXiv:2310.03214[cs.CL]FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. 2023</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 322019. 2019</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2018. 2018arXiv preprint</p>
<p>Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li, arXiv:2306.11698[cs.CL]DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. 2023</p>
<p>Ben Wang, Aran Komatsuzaki, GPT-J-6B: A 6 billion parameter autoregressive language model. 2021</p>
<p>Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li, arXiv:2111.02840Adversarial glue: A multi-task benchmark for robustness evaluation of language models. 2021. 2021arXiv preprint</p>
<p>Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, Yue Zhang, arXiv:2305.12421Evaluating open question answering evaluation. 2023. 2023arXiv preprint</p>
<p>Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. Hongru Wang, Rui Wang, Fei Mi, Zezhong Wang, Ruifeng Xu, Kam-Fai Wong, arXiv:2305.11792[cs.CL]2023</p>
<p>On the robustness of chatgpt: An adversarial and out-of-distribution perspective. Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, ICLR workshop on Trustworthy and Reliable Large-Scale Machine Learning Models. 2023</p>
<p>Generalizing to unseen domains: A survey on domain generalization. Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, Philip Yu, IEEE Transactions on Knowledge and Data Engineering. 2022. 2022</p>
<p>Publication date. J Acm, August 20183743</p>
<p>Documentlevel machine translation with large language models. Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, Zhaopeng Tu, arXiv:2304.022102023. 2023arXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023. 2023arXiv preprint</p>
<p>E Rose, Dorottya Wang, Demszky, arXiv:2306.03090Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction. 2023. 2023arXiv preprint</p>
<p>Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, arXiv:2308.08833CMB: A Comprehensive Medical Benchmark in Chinese. 2023. 2023arXiv preprint</p>
<p>Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia, arXiv:2307.09042[cs.AI]Emotional Intelligence of Large Language Models. 2023</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, arXiv:2309.10691MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. 2023. 2023arXiv preprint</p>
<p>Codet5: Identifier-aware unified pre-trained encoderdecoder models for code understanding and generation. Yue Wang, Weishi Wang, Shafiq Joty, Steven Ch Hoi, arXiv:2109.008592021. 2021arXiv preprint</p>
<p>Exploring Vision-Language Models for Imbalanced Learning. Yidong Wang, Zhuohao Yu, Jindong Wang, Qiang Heng, Hao Chen, Wei Ye, Rui Xie, Xing Xie, Shikun Zhang, arXiv:2304.014572023. 2023arXiv preprint</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, arXiv:2306.05087PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. 2023. 2023arXiv preprint</p>
<p>Zhuo Wang, Rongzhen Li, Bowen Dong, Jie Wang, Xiuxing Li, Ning Liu, Chenhui Mao, Wei Zhang, Liling Dong, Jing Gao, arXiv:2306.01499Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today. 2023. 2023arXiv preprint</p>
<p>Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, Rui Xia, arXiv:2304.04339[cs.CL]Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. 2023</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022. 2022arXiv preprint</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai Hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 20222022. 2022</p>
<p>CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, Bin Wang, arXiv:2306.16636[cs.CL]2023</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023. 2023arXiv preprint</p>
<p>Performance evaluation of classification algorithms by k-fold and leave-one-out cross validation. Tzu-Tsung Wong, Pattern Recognition. 482015. 2015</p>
<p>Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. Joshua A Patrick Y Wu, Jonathan Tucker, Solomon Nagler, Messing, arXiv:2303.120572023. 2023arXiv preprint</p>
<p>. Yiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Chi Wang, arXiv:2306.01337An Empirical Study on Challenging Math Problem Solving with GPT-4. 2023. 2023arXiv preprint</p>
<p>Autoformalization with large language models. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, Christian Szegedy, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyrek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, arXiv:2307.024772023. 2023arXiv preprint</p>
<p>Qiming Xie, Zengzhi Wang, Yi Feng, Rui Xia, arXiv:2310.02174[cs.CL]Ask Again, Then Fail: Large Language Models' Vacillations in Judgement. 2023</p>
<p>Are Large Language Models Really Good Logical Reasoners?. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, arXiv:2306.09841A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. Jun Liu, and Erik Cambria. 2023. 2023arXiv preprint</p>
<p>Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, Jingren Zhou, arXiv:2307.09705[cs.CL]CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility. 2023</p>
<p>Publication date. J Acm, August 201837</p>
<p>Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo, arXiv:2306.09265[cs.CV]LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models. 2023</p>
<p>Ruiyun Xu, Yue Feng, Hailiang Chen, arXiv:2307.01135ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience. 2023. 2023arXiv preprint</p>
<p>Large language models can rate news outlet credibility. Kai-Cheng Yang, Filippo Menczer, arXiv:2304.002282023. 2023arXiv preprint</p>
<p>Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang, arXiv:2211.080732022. 2022arXiv preprint</p>
<p>Jiong Zhenfei Yin, Jianjian Wang, Zhelun Cao, Dingning Shi, Mukai Liu, Lu Li, Lei Sheng, Xiaoshui Bai, Zhiyong Huang, Wang, arXiv:2306.06687LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark. 2023. 2023arXiv preprint</p>
<p>Jifan Yu, Shangqing Wang, Shulin Tu, Daniel Cao, Xin Zhang-Li, Hao Lv, Zijun Peng, Xiaohan Yao, Hanming Zhang, Li, arXiv:2306.09296KoLA: Carefully Benchmarking World Knowledge of Large Language Models. 2023. 2023arXiv preprint</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. 2023. 2023arXiv preprint</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023. 2023arXiv preprint</p>
<p>Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, Maosong Sun, arXiv:2306.04618[cs.CL]Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. 2023</p>
<p>Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, Yongxin Ni, arXiv:2303.13835[cs.IR]Where to Go Next for Recommender Systems? ID-vs. Modality-based Recommender Models Revisited. 2023</p>
<p>How well do Large Language Models perform in Arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.020152023. 2023arXiv preprint</p>
<p>Learning fair representations. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, International conference on machine learning. PMLR2013</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022. 2022arXiv preprint</p>
<p>Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen, arXiv:2306.02408Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning. 2023. 2023arXiv preprint</p>
<p>Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen, arXiv:2306.02408Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning. 2023. 2023arXiv preprint</p>
<p>Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He, arXiv:2305.07609Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. 2023. 2023arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022. 2022arXiv preprint</p>
<p>Sarah J Zhang, Samuel Florin, Ariel N Lee, Eamon Niknafs, Andrei Marginean, Annie Wang, Keith Tyser, Zad Chin, Yann Hicke, Nikhil Singh, arXiv:2306.08997Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. 2023. 2023arXiv preprint</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019. 2019arXiv preprint</p>
<p>Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, arXiv:2306.05179M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. 2023. 2023arXiv preprint</p>
<p>Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing, arXiv:2305.15005Sentiment Analysis in the Era of Large Language Models: A Reality Check. 2023. 2023arXiv preprint</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li, arXiv:2308.01862Wider and deeper llm networks are fairer llm evaluators. 2023. 2023arXiv preprint</p>
<p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi, arXiv:2309.01219[cs.CL]Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. 2023</p>
<p>Publication date. J Acm, A Survey on Evaluation of Large Language Models. 37445August 2018</p>
<p>SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions. Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang, arXiv:2309.070452023. 2023arXiv preprint</p>
<p>Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang, arXiv:2309.07915MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. 2023. 2023arXiv preprint</p>
<p>Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, Mykola Pechenizkiy, arXiv:2305.11262[cs.CL]CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models. 2023</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin, arXiv:2305.16934On Evaluating Adversarial Robustness of Large Vision-Language Models. 2023. 2023arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, arXiv:2309.11998LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. 2023. 2023arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, arXiv:2306.05685[cs.CL]and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. </p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, arXiv:2210.071972022. 2022arXiv preprint</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023. 2023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022. 2022arXiv preprint</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. 2023. 2023arXiv preprint</p>
<p>Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, arXiv:2306.10512Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. 2023. 2023arXiv preprint</p>
<p>Exploring ai ethics of chatgpt: A diagnostic analysis. Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing, arXiv:2301.128672023. 2023arXiv preprint</p>
<p>On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, Fatemeh Shiri, arXiv:2301.128682023. 2023arXiv preprint</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019. 2019arXiv preprint</p>
<p>Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, arXiv:2305.03514Can Large Language Models Transform Computational Social Science?. 2023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
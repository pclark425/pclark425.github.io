<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7326 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7326</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7326</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-267755280</p>
                <p><strong>Paper Title:</strong> PromptAD: Zero-shot Anomaly Detection using Text Prompts</p>
                <p><strong>Paper Abstract:</strong> We consider the problem of zero-shot anomaly detection in which a model is pre-trained to detect anomalies in images belonging to seen classes, and expected to detect anomalies from unseen classes at test time. State-of-the-art anomaly detection (AD) methods can often achieve exceptional results when training images are abundant, but they catastrophically fail in zero-shot scenarios with a lack of real examples. However, with the emergence of multi-modal models such as CLIP, it is possible to use knowledge from other modalities (e.g. text) to compensate for the lack of visual information and improve AD performance. In this work, we propose PromptAD, a dual-branch framework which uses prior knowledge about both normal and abnormal behaviours in the form of text prompts to detect anomalies even in unseen classes. More speci ﬁ cally, it uses CLIP as a backbone encoder network and an additional dual-branch vision-language decoding network for both normality and abnormality information. The normality branch establishes a pro ﬁ le of normality, while the abnormality branch models anomalous behaviors, guided by natural language text prompts. As the two branches capture complementary information or ‘views’, we propose a ‘cross-view contrastive learning’ (CCL) component which regularizes each view with additional reference information from the other view. We further propose a cross-view mutual interaction (CMI) strategy to promote the mutual exploration of useful knowledge from each branch. We show that PromptAD outperforms existing baselines in zero-shot anomaly detection on key benchmark datasets and analyse the</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7326",
    "paper_id": "paper-267755280",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0033545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PromptAD: Zero-shot Anomaly Detection using Text Prompts</p>
<p>Yiting Li 
Institute for Infocomm Research (I 2 R)
A * STARSingapore</p>
<p>Adam Goodge 
Institute for Infocomm Research (I 2 R)
A * STARSingapore</p>
<p>Fayao Liu 
Institute for Infocomm Research (I 2 R)
A * STARSingapore</p>
<p>Chuan-Sheng Foo 
Institute for Infocomm Research (I 2 R)
A * STARSingapore</p>
<p>Centre for Frontier AI Research (CFAR)
A * STARSingapore</p>
<p>PromptAD: Zero-shot Anomaly Detection using Text Prompts
213595F40218D44905D6F0921418145E10.1109/WACV57701.2024.00113
We consider the problem of zero-shot anomaly detection in which a model is pre-trained to detect anomalies in images belonging to seen classes, and expected to detect anomalies from unseen classes at test time.State-ofthe-art anomaly detection (AD) methods can often achieve exceptional results when training images are abundant, but they catastrophically fail in zero-shot scenarios with a lack of real examples.However, with the emergence of multimodal models such as CLIP, it is possible to use knowledge from other modalities (e.g.text) to compensate for the lack of visual information and improve AD performance.In this work, we propose PromptAD, a dual-branch framework which uses prior knowledge about both normal and abnormal behaviours in the form of text prompts to detect anomalies even in unseen classes.More specifically, it uses CLIP as a backbone encoder network and an additional dual-branch vision-language decoding network for both normality and abnormality information.The normality branch establishes a profile of normality, while the abnormality branch models anomalous behaviors, guided by natural language text prompts.As the two branches capture complementary information or 'views', we propose a 'cross-view contrastive learning' (CCL) component which regularizes each view with additional reference information from the other view.We further propose a cross-view mutual interaction (CMI) strategy to promote the mutual exploration of useful knowledge from each branch.We show that PromptAD outperforms existing baselines in zero-shot anomaly detection on key benchmark datasets and analyse the role of each component in ablation studies.</p>
<p>Introduction</p>
<p>Anomaly detection (AD) is important in a wide range of applications, such as industrial product inspection, net-Figure 1.The ZSAD problem addressed in this work.The model is trained to detect anomaly types like "crack" and "scratch" in the set of known object classes "hazelnut" and "metal nut".At testtime, the model is tasked with detecting similar anomaly types in previously unseen object classes "tile" and "capsule".</p>
<p>work security and autonomous driving.Although existing anomaly detection techniques have achieved impressive performance in many cases, they are only evaluated on classes of data that the model has observed during training.In real-world scenarios, a model is often required to evaluate samples in an open-world setting: that is, to detect anomalies in previously unseen and novel classes.Despite its importance, this cross-category generalization has been largely overlooked in existing literature.Thus, in this work, we target this problem of zero-shot anomaly detection (ZSAD) (Fig. 1), where the model is trained to detect anomalies in the "hazelnut" and "metal nut" classes and expected to generalise to detect anomalies in the previously unseen "tile" and "capsule" classes without further training.</p>
<p>ZSAD is a difficult task in the absence of real data from the unseen classes.However, prior information about the expected behaviour of normal data, as well as the potential behaviour of anomalies, is often known even without visual examples [22,23].For example, quality control engineers can describe how a manufactured product should and should not appear in the form of natural language (text).With the emergence of vision-language models like CLIP [18], which has demonstrated its capability for image-level zeroshot classification, this natural language information can be encoded to obtain semantic representations of both normality as well as abnormality, which can compensate the lack of visual examples and help in the anomaly detection task.</p>
<p>To this end, we propose an effective and flexible framework named PromptAD, which equips the model with both a normality view and abnormality view that leverage rich information from the language mode (text) to reduce dependence on image data.Information sharing between the two views further refines the representation learning and improves generalization.PromptAD is built on the CLIP model, which is used to encode data from both image and text modalities.On top of this backbone, PromptAD decodes these representations into feature maps for anomaly detection through both 'intra-view' and 'cross-view' modeling.Intra-view modeling captures knowledge specific to each view through two parallel vision-language decoding networks (one each for normality and abnormality).Crossview modeling shares information between the two views via a cross-view contrastive learning strategy (CCL), which regularizes the intra-view training with additional reference information from the other complementary view, and a cross-view mutual interaction (CMI) strategy is proposed to facilitate the explicit knowledge transfer from each other.</p>
<p>Existing uni-modal AD methods rely exclusively on visual information, which hinders their generalization beyond the base training classes.Moreover, uni-modal training often utilizes a considerable amount of training data, which incurs extra data collection cost.In contrast, PromptAD has two main advantages: strong transferability and high data-efficiency.It utilizes semantic knowledge from text prompts to learn representations that are more reliable and more transferable to new classes.We further observe that PromptAD requires much less training data to achieve comparable performance to methods trained on a large amount of data.This is because a well-designed text prompt can contain rich semantic information that effectively distills the information contained in a large number of images.We demonstrate these advantages of PromptAD through extensive experiments.In summary, the contributions of this paper are as follows:</p>
<ol>
<li>
<p>We propose PromptAD which efficiently adapt the pre-trained CLIP features aligned with language for zero-shot anomaly detection.The proposed approach enables training one unified and generalizable model without any fine-tuning when adapting to new classes.</p>
</li>
<li>
<p>Our framework effectively aggregate the semantic knowledge from both normality view and the abnormality view.A Cross-View Contrastive Learning (CCL) strategy is proposed to readjust the optimiza-tion difficulty of the intra-view modeling, and a Cross-View Mutual Interaction (CMI) approach is proposed to promote the explicit knowledge transfer between two complementary branches.</p>
</li>
<li>
<p>We show that our approach significantly improves the anomaly detection performance over existing methods on widely used AD benchmarks.</p>
</li>
</ol>
<p>Related Works</p>
<p>Existing methods can be divided into unsupervised and supervised methods.Unsupervised methods model the normal sample distribution and exclude anomalies in training.Reconstruction-based methods [2,6,19,26] generate reconstructed images and then use reconstruction errors between the input image and its reconstruction to detect anomalies.GAN-based models detect anomalies based on the ability of the generator to generate a given test sample [1,20,24].In contrast, supervised methods find a better decision boundary between normal and anomalous samples by leveraging synthetically generated anomalies [13] or a small number of real anomalies.DevNet [17] encourages the anomaly score of normal samples towards a common center, whereas MLEP [14] maximises the pair-wise distances between normal and anomalous features.DRA [7] uses a multi-head neural network to learn disentangled representations for different types of anomalies separately.</p>
<p>As mentioned, these methods rely on the availability of training data from all classes and are ill-equipped to detect anomalies from previously unseen classes at test time.On the other hand, PromptAD can detect anomalous samples from novel classes not observed during training; it achieves this by incorporating additional domain knowledge from natural language descriptions of normal and abnormal behaviour to compensate for the lack of real samples.</p>
<p>It worth to take note that WinCLIP [10] also uses CLIP for zero-shot anomaly detection.However, it uses representations of query images directly from the fixed CLIP model parameters without further training.This may be suitable for domains which are closely related to the original data used to train CLIP, however, as noted in its original paper [18], CLIP under-performs when applied to domains that are not well represented in this training distribution.In contrast, PromptAD builds two trainable branches on top of a frozen CLIP model, which combines CLIP's multi-modal capabilities with task-specific representation learning and directly optimises the model for the anomaly detection task at hand.Therefore, PromptAD can effectively ameliorate the distribution shift issue and thus achieves better pixellevel AD performance than WinCLIP (Table 2).  3 and Sec.3.4) approach is proposed to incorporate complementary information from the opposite view for better anomaly targeting in each branch.The two branches further explore knowledge from each other through cross-view mutual interaction (CMI, Sec.3.5).</p>
<p>Anomaly Detection with Anomaly-Aware</p>
<p>Text Prompts</p>
<p>Problem Description</p>
<p>We focus on the ZSAD problem formulated as follows.We train a model on the base data, consisting of N normal samples and a few anomalous samples from a base (or seen) class C b .We then test the model on a set of novel classes, C u , without additional training to evaluate zero-shot performance, where C b ∩ C u = ∅.Additionally, for each class in C = C b ∪ C u , we define two types of semantic knowledge in the form of text prompts, where the normality prompt P nor describes the visual appearance of normal patterns, and the abnormality prompt P abn describes typical anomaly appearances (details on how to construct text prompts are given in Sec.4.1).Our goal is to train an anomaly detector f : (X, P nor , P abn ) → R from a single base class C b to detect anomalies on unseen classes C u , by assigning larger scores to anomalies than normal samples.</p>
<p>PromptAD Framework</p>
<p>Here we describe the PromptAD framework (Fig. 2), giving an overview before detailing the individual components.PromptAD has two complementary branches built on top of a shared CLIP model.The abnormality branch directly models the distribution of the available anomaly samples (Sec.3.3.1)while the normality branch models the distribution of the normal samples, directly measuring the conformity of a query image to the normality descriptions (Sec.3.3.2).To promote complementary knowledge transfer between the two views, a cross-view contrastive learning (CCL, Sec.3.4) approach is proposed to regularize intraview training with additional reference information from the complementary view.Furthermore, the two branches also explicitly explore knowledge from each other through cross-view mutual interaction (CMI, Sec.3.5).</p>
<p>Intra-view Modeling</p>
<p>Abnormality Branch</p>
<p>The abnormality branch (lower part of Fig. 2) learns to detect anomalies according to information provided by the abnormality text prompts.In this branch, we obtain the visual embedding of a given image x from the CLIP image encoder and the semantic embedding of the abnormality prompt P abn from the CLIP text encoder.We then fuse these embeddings using FiLM [8] as in [15].</p>
<p>The fused feature embeddings are fed as input to a transformer-based decoder to generate output features q intra abn = q 1 abn , q 2 abn , • • • , q L abn , where each q i abn ∈ R D corresponds to a local area of the input image and D is the  channel size of the output feature space.A linear classifier (1x1 convolution) is then applied to each q i abn ∈ q intra abn to output an anomaly score for each spatial location.We denote the obtained 2D score map as s intra abn , which is referred as the Intra-view Abnormality Score.Regions that contain anomalies recorded by the abnormality prompt P abn should produce higher responses than normal regions.</p>
<p>Normality Intra-view Modeling</p>
<p>The normality branch (upper part of Fig. 2) is a similar decoder network, but it is instead designed to directly measure the conformity of a query image to the normal profile defined by the normality prompts, in an adjacent process to the abnormality branch.As the abnormality prompts cannot describe every possible type of anomaly (which could be infinite), modeling normality is also important to detect anomalies as those samples that do not conform with the normal profile.Similarly, the produced prediction s intra nor is referred as Intra-view Normality Score.</p>
<p>Cross-view Contrastive Learning</p>
<p>As an anomaly usually refers an irregularity or deviation from the standard pattern, the single-view approach (intraview modeling) may not be optimal for detecting certain anomalies that do not exhibit obvious irregularities.For example, in the popular MVTec dataset [3], the "transistor" class has a type of anomalies named "misplaced", where a transistor in good condition is shifted away from the right location.Such anomalies can only be effectively detected given a reference standard, e.g.,"a normal transistor should be placed vertically in the middle line of the circuit board".</p>
<p>To better recognize such anomalies, we propose a crossview contrastive learning (CCL) approach (illustrated in Fig. 3) which enhances the intra-view modeling with additional reference information from the opposing view.In particular, CCL learns anomaly scores based on the residuals between the two complementary views in a learned feature space.This process will now be detailed for each branch.</p>
<p>Abnormality Branch</p>
<p>The abnormality branch is regularized with complementary information from the normality view.After extracting the semantic embedding of the normality prompt from the CLIP text encoder, we first reduce its channel dimension with a 1x1 convolution layer to match with the output-space dimension.The obtained semantic feature is regarded as the normality prototype φ nor .Given the intra-view abnormality features q intra abn extracted from a query image, we also compute a Cross-view Normality Score s cross nor to measure the closeness between the normality prototype φ nor and q intra abn .In particular, s cross nor is a 2D score map with the same size as the intra-view abnormality score s intra abn , which is obtained by measuring inner-product similarity between φ nor and the feature vector of each spatial location in q intra abn , where s cross nor = φ nor • q intra abn (• denotes inner product).Oppositely from s intra abn , images with high responses in s cross nor are those that have higher possibilities of being normal.The final prediction s a of the abnormality branch is given by an element-wise subtraction of the two scores:
s a = s intra abn − s cross nor . (1)
In doing so, our model learns generalized and anomaly-aware representations rather than over-fitting to the limited anomaly modes described in abnormality prompts.</p>
<p>Normality Branch</p>
<p>Anomalies often share many compositional patterns with normal samples and their anomalousness may be very subtle, for example "an anomaly glass bottle with small crack" vs. "a normal sample with small scratch within acceptable limits" is difficult to distinguish.As the intra-view learning of the normality branch focuses on the normality reference, it struggles to detect such anomalies; carefully-defined abnormality prompts are useful for defining this boundary.</p>
<p>For this reason, we similarly obtain the abnormality semantic prototype φ abn to highlight those hard anomalies.Formally, given the extracted intra-view features q intra nor , we compute its inner-product similarity with φ abn to obtain the Cross-view Abnormality Score s cross abn , where s cross abn = φ abn • q intra nor .Higher values in s cross abn indicate that the corresponding locations in q intra nor are more similar to the abnormality prototype φ abn , and are more likely to be anomalous.On the other hand, the normal regions in the query features q intra nor can hardly match the abnormality features in φ abn with high similarity, leading to lower values in s cross abn .To this end, the final anomaly score map of normality branch is obtained by:
s n = s cross abn − s intra nor .
(2)</p>
<p>Cross-view Mutual Interaction</p>
<p>The two branches capture complementary information: the abnormality branch is more effective for anomalies that are well defined by the abnormality prompts while the normality branch is effective for more general anomalies that are missed by the abnormality prompts; we believe that the attention maps 1 learned by one branch are helpful in discovering overlooked anomalies by the other [27].Therefore, rather than separate training, the two branches can each benefit through mutual interaction [16,28].In particular, attention maps from the intermediate layers can highlight those regions in the image that were important for the network's decisions [11,12].Hence, we propose to utilize the intermediate attentions of one branch to extend possible anomalous regions for the other branch, so that the other branch can better detect anomalies from regions that it has missed.</p>
<p>For example, the detection score map s n output by the normality branch can serve as a soft attention map M sof t for guiding the abnormality branch to discover hard anomalies that are neglected by itself.In particular, the intermediate feature map q intra abn of the abnormality branch is refined through spatial-wise multiplication with the attention 1 Attention maps here refer to the generated anomaly score maps [21].We will use these two terms interchangeably.map M sof t .Such auxiliary attention then serves as feature selectors to highlight anomalous regions on query features during the forward pass, as well as gradient selectors to correct such false-negative errors during the backward pass.</p>
<p>To ensure that the attention maps generated from middle layers can cover most of the anomalous regions, we additionally train the intermediate layers with supervision signal, i.e., the symmetric deviation loss detailed in Sec 3.6.However, the obtained raw score map s n could be noisy due to its unbounded prediction values and diverse backgrounds.To highlight the true anomaly regions, regions with activation values less than σ in s n will all be treated as the background zone and masked out, such that s n is tailored towards the real anomalies.Therefore, we define the remodeled foreground attention map M fg as:
M fg = ReLU(s n − σ).
(
)3
Here we use σ = 0.5 for all experiments.The ReLU function ensures that only highly scoring regions (greater than a threshold σ) are activated.We observed that the resulting candidate regions cover most of the real anomalous regions for the vast majority of inputs.Then M fg is then used to generate a soft mask via a sigmoid function:
M sof t = sigmoid(M fg ).(4)
Next, M sof t obtained from the normality branch is used as an attention map to re-weight the intra-view features q intra abn of the abnormality branch, so that the neglected anomalies can be highlighted (Fig. 3).This drives the two branches into a mutual-guidance state so that anomalies missed by one branch can be effectively captured through the complementary attentions from the other branch.</p>
<p>Training and Inference</p>
<p>Symmetric Deviation Loss The goal of training is to enforce statistically significant deviations between the scores of anomalies from those of normal samples.Inspired by the success of the deviation loss in [17], we use a predefined Gaussian distribution to generate a set of reference scores R, which serves as an anchor to guide the learning of anomaly scores: dev(x) = S−μR δR , where μ R is the mean and δ R the standard deviation of scores in R. We then introduce a symmetric deviation loss to enlarge the gap between intra-view and cross-view scores:
L(x) = (1 − y) • max(0, α + dev(x)) +y • max(0, α − dev(x)), (5)
where y = 1 if x is an anomaly and y = 0 if x is normal.α is a hyper-parameter defining the margin between the scores of normal and anomalous samples.The mean value of the reference score set serves as the classification boundary.The proposed loss enforces a positive deviation of at least α between the classification boundary μ and the anomaly scores of anomalies in the upper tail, while enforcing a negative deviation of at least −α between the classification boundary μ and the anomaly scores of normal samples in the lower tail.In our experiments, we follow previous works and set μ R = 0, δ R = 1 and α = 5.Importantly, the symmetric deviation loss is applied and backpropagated through multiple layers for the anomaly scores calculated from the abnormality branch in Eq. 1 and the normality branch in Eq. 2 separately.</p>
<p>Inference To perform unbiased anomaly classification, we combine the scores from the normality branch s n and abnormality branch s a as they contain complementary information.The first learns normality representation that enables the universal anomaly detection on unseen categories.The latter incorporates more specific knowledge of abnormality retrieved via language, which helps learn discriminative features for the detection of hard anomalies (e.g., anomalies similar to normal samples).We thus compute the final anomaly score as s = s n + s a .</p>
<p>Experiments</p>
<p>Experiment Settings</p>
<p>We experiment on two challenging real-world benchmark datasets for industrial anomaly detection which are MVTec AD and AITEX.MVTec AD consists of 4096 normal and 1258 anomalous images split across 15 object categories, while AITEX contains 7 types of fabric defects.Experiments were conducted using the leave-one-categoryout setting [9], i.e., a target category was chosen to be tested, while other categories in the dataset are used for training.As labelled anomalies are difficult to obtain, we use only one or ten labelled anomaly from a randomly chosen anomaly type of each seen class during model training.In contrast to the previous work [7] which samples labelled anomalies from every anomaly type, our method achieves better data efficiency.The Area Under ROC Curve is used as the performance criteria and the reported experimental results are averaged over 10 trial runs.Text Prompts Formulation We use the Oxford English Dictionary definition to construct the normality prompt for each object class.For example, for "screw" class, the normality prompt is "a short metal pin with a helical thread running round it and a slotted head".Oppositely, abnormality prompts describe our prior knowledge of the visual appearance of potential anomalies from that same class.Our abnormality prompt is constructed as: "abnormality of {class label} is [anomaly description]".Using our earlier example of "screw", example anomaly descriptions include "scratch, tear, crack, cut, defect", which are provided as anomaly type labels in the dataset [4].Patches that align closely with such an abnormality prompt are likely to contain some type of anomaly, meaning the sample from which the patches come is likely to be anomalous.Implementation Details We use the pre-trained CLIP (ViT-B/16) model for the image and text encoders, and the two branches consist of three transformer blocks.The parameters of two branches are initialized from scratch.The top-K setting is the same as in [7], which is set to be 10 percent of the number of last-layer output tokens.We do not use any image augmentation techniques nor pseudo anomaly samples.For model training, we learn our PromptAD in the support-query manner.To simulate the real-world ZSAD scenario, we sample a few query images with ground-truth labels in each training episode and utilize the two views of text prompts as support information.Training is performed on each transformer layer with the proposed symmetric deviation loss.To prevent the vast number of normal samples from overwhelming the training loss, we up-sample the positive samples (anomalies) by 10 times for balanced model training.After training, the model can be directly applied to novel classes without further updating.</p>
<p>Results</p>
<p>We compare PromptAD against the following baseline methods: DevNet [17], MLEP [14], DRA [7] , WinCLIP [10] and the original CLIP model [18].Classification Table 1 shows the performance of Promp-tAD compared with other methods.Our method outperforms all its competitors on of all the unseen classes, and this is especially significant in the 1-shot setting, demonstrating that our method can effectively exploit rich semantic information in the two-branch learning framework to boost the anomaly detection performance.Furthermore, in more difficult object classes, such as the transistor images which contain many components and a complex background, the advantages of our proposed method become more significant.This may be attributed to the incorporation of text prompts, which provide important semantic information for generalization to unseen classes.In comparison, MLEP [14], DevNet [17] and DRA [7] rely on images solely and therefore do not generalise as successfully.The CLIP [18] baseline focuses more on semantic classification by aligning the relevant textual information with the image, failing to preserve the necessary spatial information to produce fine-grained and position-sensitive anomaly detection, which is resolved by the two-branch learning mechanism used in our model.In addition, we further Localization We also show the pixel-wise AUC performance for anomaly localization in Table 2.We can see that PromptAD works well on anomaly localization and performs better than the state-of-the-art method WinCLIP [10].This is because that PromptAD are explicitly designed for learning domain-specific features and thus ensures the vision-language correspondence on AD tasks.In contrast, Table 1.Average AUC performance under the zero-shot anomaly detection setting for the MVTec dataset and the AITEX dataset.The best score is highlighted in red.</p>
<p>Dataset One Anomaly Example</p>
<p>Ten Anomaly Examples CLIP [18] DevNet [17] MLEP [14] DRA [7] Ours CLIP [18] DevNet [17] MLEP [14]  WinCLIP relies on features generated by the frozen CLIP model, which may not be optimal for anomaly localization.Some qualitative results are shown in Fig. 5. Fig. 4 visualizes the representations learned by Promp-tAD in comparison to those by DRA [7].We see that normal samples and anomalies exhibit greater separability in the latent space obtained through our dual-branch framework with semantic knowledge injected.Few-shot AD We also evaluate our method under the general few-shot AD setting used by RegAD [9].This setting assumes that a few normal examples of a new class are available for model testing.Therefore, we replace the normality prototype φ nor with the visual embeddings of the available normal examples.The evaluation results are presented in Table 3.The sampled examples are then removed from the test set during evaluation.Under the case of K = 2, the performance gains observed in our proposed method over the other baselines of CLIP, RegAD [9] and DRA [7] are 18.0, 5.5, 12.0 points, respectively, demonstrating that our method can exploit the rich semantic information effectively to boost the overall detection performance.When the number of available visual samples increases, the gap between our method and the other methods become smaller.This may be because visual modal might be richer and more discriminative than text ones for AD task when more visual examples are available.</p>
<p>Ablation Study</p>
<p>Importance of Each Component After validating the overall effectiveness of our approach, we further investigate the importance of each component.The results are presented in Table 4, where we denote the abnormality branch coupled with abnormality prompts as ABN and the normality branch coupled with normality prompts as NOR.Sim-   ply adopting the single-view training without any constraint leads to poor generalization performance due to semantic confusion.In contrast, the proposed CCL alleviates this issue, outperforming the naive single-view training approach up to 10.2 percentage points in the 10-shot scenario.We then compare the performances of the single-branch and double-branch frameworks.The results show that the double-branch framework outperforms the single branch by a significant margin since it takes into account both normal and anomaly data distribution.This is also validated by the visualizations in Sec.4.3.</p>
<p>CMI contributes to forming a virtuous feedback cycle between the two branches via exchanging complementary information, and is thus able to improve the detection accuracy.As a result, one branch is able to learn better abstract representations by referring to the other branch's predictions.In contrast to the baselines where this feedback cycle is absent, either branch tends to overfit to its own view and suffers from significant performance degradation.Visualizing Detection Maps of Each Branch To gain deeper insights into the anomaly detection capabilities of the abnormality branch (ABN Branch) and normality branch (NOR Branch) in PromptAD, we visualize the AD score maps from each branch in Fig. 5 for the unseen classes of "wood," "metal nut," "zipper," and "grid".We see the abnormality branch focuses on the most discriminative anomaly regions but does not identify all potential anomaly areas.In contrast, the normality Branch presents a more comprehensive segmentation mask, localizing most of the anomaly regions.This can be attributed to the guidance by normality semantic information, which allows it to identify visual patterns that deviate from the expected normality profile as anomalies.Moreover, we observe that score fusion effectively enhances the AD performance and mitigates the impact of incorrect detections (false positives).This validates that the discriminative capabilities of the abnormality and normality branches complement each other and both contribute to improved ZSAD performance through the dual-branch mechanism.</p>
<p>Conclusion</p>
<p>The closed-set nature of existing anomaly detection methods limit their generalization capabilities to new distributions, such as for previously unseen classes.To address this, we study the ZSAD problem where a model is trained to detect anomalies from seen classes and tested on its ability to detect anomalies in unseen classes without additional training.We propose PromptAD: a dual branch framework that incorporates rich semantic knowledge from both abnormality and normality views in the form of natural language text prompts.PromptAD refines the representations from a fixed CLIP encoder backbone for the anomaly detection task using the a dual-branch framework, each making use of abnormality and normality information in synergy.Importantly, information from one view is used to help and complement the learning of the other view, through cross-view contrastive learning and cross-view mutual interaction.Extensive experiments show that PromptAD improves zeroshot anomaly detection over existing baselines on key industrial benchmark datasets and can also maintain its strong performance in few-shot settings.</p>
<p>Figure 2 .
2
Figure 2.An overview of our PromptAD framework.It uses a dual-branch network design, which aims to detect anomalies from both normality (upper part of the figure) and abnormality (lower part of the figure) views.The normality branch learns to detect out-ofdistribution patterns by modeling the normal data conditioned on images and normality text prompts.The abnormal branch attempts to directly identify anomalies by conditioning on images and abnormality text prompts.A cross-view contrastive learning (CCL, details in Fig.3and Sec.3.4) approach is proposed to incorporate complementary information from the opposite view for better anomaly targeting in each branch.The two branches further explore knowledge from each other through cross-view mutual interaction (CMI, Sec.3.5).</p>
<p>1084</p>
<p>Figure 3 .
3
Figure 3. (a).An illustration of the proposed Cross-view Contrastive Learning (CCL) pipeline.Using the normality branch as an example, to generate the intra-view normality score, a linear classifier is applied on each spatial location of q intra nor for point-wise normality score predictions.To generate the cross-view abnormality score, the normality-aware features q intra nor are then compared with the semantic prototype of the abnormality view φ abn with inner-product similarity.Finally, the two-view scores are fused together through elementwise subtraction for unbiased anomaly detection sn.(b).An illustration of the proposed Cross-view Mutual Interaction (CMI).The detection score map sn from the normality branch are then used as an auxiliary attention to discover those anomalies that are neglected by the abnormality branch.</p>
<p>Figure 4 .
4
Figure 4. Visualizations of features learned by DRA and our proposed PromptAD on the unseen test classes of MVTec dataset.Green indicates anomalies while red indicates normal samples.Features learned by PromptAD show better separability.</p>
<p>Figure 5 .
5
Figure 5. Qualitative anomaly localization results on MVTec AD dataset.For each example, the images from left to right are the anomaly image, the ground-truth mask, the anomaly score map produced by the abnormality branch and normality branch, and the fused score map.</p>
<p>Table 2 .
2
Pixel-level AUC performance on MVTec AD dataset.
DRA [7] Ours</p>
<p>Table 3 .
3
Average AUC performance under the general anomaly detection setting on the MVTec AD dataset."K-shot" denotes the number of normal samples used for training.
K-shot CLIP [18] RegAD [9] DRA [7] PromptAD80.7600.9120.8530.93140.7580.8820.8210.92720.7320.8570.7920.912</p>
<p>Table 4 .
4
Ablation study for PromptAD.ABN and NOR are the abnormality and normality branches respectively.
Components1-shot 10-shotABN0.741 0.765NOR0.735 0.763ABN + NOR0.746 0.789ABN + NOR + CCL0.838 0.866ABN + NOR + CCL + CMI 0.908 0.912
Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
This research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds (Grant No. A20H6b0151).
Ganomaly: Semi-supervised anomaly detection via adversarial training. Samet Akcay, Toby P Amir Atapour-Abarghouei, Breckon, Asian conference on computer vision. Springer2018</p>
<p>Variational autoencoder based anomaly detection using reconstruction probability. Jinwon An, Sungzoon Cho, Special Lecture on IE. 212015</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Generic attentionmodel explainability for interpreting bi-modal and encoderdecoder transformers. Hila Chefer, Shir Gur, Lior Wolf, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Autoencoder-based network anomaly detection. Zhaomin Chen, Kiat Chai, Bu Yeo, Sung Lee, Chiew Tong Lau, Wireless telecommunications symposium (WTS). IEEE2018. 2018</p>
<p>Catching both gray and black swans: Open-set supervised anomaly detection. Choubo Ding, Guansong Pang, Chunhua Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022. 3, 789</p>
<p>Harm de Vries, Aaron Courville, and Yoshua Bengio. Feature-wise transformations. Ethan Vincent Dumoulin, Nathan Perez, Florian Schucher, Strub, Distill. 37e112018</p>
<p>Registration based few-shot anomaly detection. Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael W Spratling, Yan-Feng Wang, European Conference on Computer Vision. Springer2022136849</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition19606-19616, 2023. 3, 7, 8</p>
<p>Adversarial discriminative attention for robust anomaly detection. Daiki Kimura, Subhajit Chaudhury, Minori Narita, Asim Munawar, Ryuki Tachibana, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2020</p>
<p>Regularizing attention networks for anomaly detection in visual question answering. Doyup Lee, Yeongjae Cheon, Wook-Shin Han, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Cutpaste: Self-supervised learning for anomaly detection and localization. Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Margin learning embedded prediction for video anomaly detection with a few anomalies. Wen Liu, Weixin Luo, Zhengxin Li, Peilin Zhao, Shenghua Gao, IJCAI. 20197</p>
<p>Image segmentation using text and image prompts. Timo Lüddecke, Alexander Ecker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>A two-stream mutual attention network for semi-supervised biomedical segmentation with noisy labels. Xuejin Shaobo Min, Zheng-Jun Chen, Feng Zha, Yongdong Wu, Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Anton van den Hengel. Deep anomaly detection with deviation networks. Guansong Pang, Chunhua Shen, Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining2019. 3, 6, 7, 8</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR, 2021. 3, 7, 8, 9</p>
<p>Anomaly detection using autoencoders with nonlinear dimensionality reduction. Mayu Sakurada, Takehisa Yairi, Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis. the MLSDA 2014 2nd workshop on machine learning for sensory data analysis2014</p>
<p>Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. Thomas Schlegl, Philipp Seeböck, Ursula Sebastian M Waldstein, Georg Schmidt-Erfurth, Langs, International conference on information processing in medical imaging. Springer2017</p>
<p>Attention guided anomaly localization in images. Shashanka Venkataramanan, Kuan-Chuan Peng, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XVII</p>
<p>Few-shot learning with visual distribution calibration and cross-modal distribution alignment. Runqi Wang, Hao Zheng, Xiaoyue Duan, Jianzhuang Liu, Yuning Lu, Tian Wang, Songcen Xu, Baochang Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Adaptive cross-modal few-shot learning. Chen Xing, Negar Rostamzadeh, Boris Oreshkin, Pedro O O Pinheiro, Advances in Neural Information Processing Systems. 201932</p>
<p>Efficient gan-based anomaly detection. Houssam Zenati, Chuan Sheng Foo, Bruno Lecouat, Gaurav Manek, Vijay Ramaseshan Chandrasekhar, arXiv:1802.062222018arXiv preprint</p>
<p>Extract free dense labels from clip. Chong Zhou, Chen Change Loy, Bo Dai, European Conference on Computer Vision. Springer2022</p>
<p>Anomaly detection with robust deep autoencoders. Chong Zhou, Randy C Paffenroth, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2017</p>
<p>Few-shot partial multi-view learning. Yuan Zhou, Yanrong Guo, Shijie Hao, Richang Hong, Jiebo Luo, IEEE Transactions on Pattern Analysis and Machine Intelligence. 62023</p>
<p>Binocular mutual learning for improving few-shot classification. Ziqi Zhou, Xi Qiu, Jiangtao Xie, Jianan Wu, Chi Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>            </div>
        </div>

    </div>
</body>
</html>
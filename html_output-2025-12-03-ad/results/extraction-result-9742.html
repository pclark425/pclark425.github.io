<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9742 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9742</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9742</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-278166056</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.19076v1.pdf" target="_blank">LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly used to evaluate information retrieval (IR) systems, generating relevance judgments traditionally made by human assessors. Recent empirical studies suggest that LLM-based evaluations often align with human judgments, leading some to suggest that human judges may no longer be necessary, while others highlight concerns about judgment reliability, validity, and long-term impact. As IR systems begin incorporating LLM-generated signals, evaluation outcomes risk becoming self-reinforcing, potentially leading to misleading conclusions. This paper examines scenarios where LLM-evaluators may falsely indicate success, particularly when LLM-based judgments influence both system development and evaluation. We highlight key risks, including bias reinforcement, reproducibility challenges, and inconsistencies in assessment methodologies. To address these concerns, we propose tests to quantify adverse effects, guardrails, and a collaborative framework for constructing reusable test collections that integrate LLM judgments responsibly. By providing perspectives from academia and industry, this work aims to establish best practices for the principled use of LLMs in IR evaluation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9742.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9742.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Circularity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Circularity (Eval Trope #1 & #2: Leaking evaluation signal / LLM-eval as ranker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When LLM-based evaluators influence system development or are reused inside systems (e.g., as rerankers), evaluation becomes self-reinforcing, inflating scores and diverging from human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Information retrieval (retrieval-augmented generation / ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Umbrela (open-source reproduction of Bing relevance assessor) used in TREC RAG 2024 demonstration</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Umbrela generated relevance grades used as qrels; reranking executed by sorting by Umbrela grade (primary) then original score (secondary); Umbrela also used to evaluate reranked runs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Manual relevance labels from TREC RAG 2024 human assessors (official TREC manual qrels; exact annotator counts/protocol not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau between system rankings: tau@60 = 0.84 (original Umbrela vs human), after Umbrela-based reranking tau@60 = 0.63, discordant pairs rose from ~8% to 18% for top 60; tau falls to 0.44 for top 20 systems; twelve systems score Umbrela-NDCG > 0.95 while manual NDCG for same systems is 0.68-0.72.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Validity and external generalization are lost: evaluations become inflated and misleading, leaderboard rankings diverge from human judgments, and claimed near-perfect scores mask substantial differences seen by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>TREC RAG 2024 simulation: reranking with Umbrela improved human NDCG (valid system improvement), but evaluating reranked systems with Umbrela created large divergence from human evaluation (more discordant pairs, inflated NDCG for many systems).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Umbrela correlates well with human judges on original systems (tau@60 = 0.84), showing LLM-evaluators can be valid when they are not embedded in the system; reranking with Umbrela genuinely improved human-evaluated performance, indicating LLM signals can be useful on the system side if evaluation circularity is avoided.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Eval Trope #1: Circularity', 'Eval Trope #2: LLM-Eval as a Ranker', and 'Demonstration of Circularity' (TREC RAG 2024 analysis, Figures 2-4)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9742.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Narcissism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Narcissism (Eval Trope #3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM evaluators systematically prefer text that resembles their own generations or family of models, biasing judgments toward outputs similar to the judge's own style and token-likelihood patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General text evaluation across IR and generative responses (ranking, conversational quality)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Generic LLMs (examples in paper: GPT-4 and other LLM families); specific experiments referenced in literature (e.g., GPT-4 favoring GPT-4 outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLMs used to assign relevance/quality scores to outputs; often single-model evaluation or single-family evaluators; mitigation suggested by reserving an LLM for evaluation or ensembling multiple LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human assessors described as better at recognizing nuance, novelty, and contextual diversity; specifics vary by cited study and task.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No single numeric agreement provided for narcissism; paper cites studies showing systematic favoritism and suggests measuring frequency that evaluator favors outputs from same underlying model (protocol from Liu et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of fairness and openness to novelty: systems that produce novel, non-standard but valid responses may be penalized; rankings biased toward systems sharing lineage with the evaluator; sets an implicit performance ceiling.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Mentioned empirical observations where GPT-4 evaluators favor GPT-4-generated responses even when humans find no meaningful difference; general phenomenon described in industry case study (Valence) where narcissism inflated effectiveness estimates versus human assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Mitigations: reserve an LLM family for evaluation only, or ensemble multiple diverse LLM evaluators and exclude votes from shared-lineage evaluators; human judge diversity is more robust for nuance but harder to emulate with LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Eval Trope #3: LLM Narcissism' and 'Valence' case study</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9742.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ignored Label Correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ignored Label Correlation (Meta-Eval Trope #5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High system-level correlation between LLM and human evaluations can mask wide disagreement on individual query-document labels; label-level agreement is often much lower than system-level metrics imply.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Conversational systems and general IR relevance labeling</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Generic LLM evaluators (specific evaluators vary across cited meta-evaluation studies).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLMs generate relevance labels for query-document or query-response pairs; system-level metrics (e.g., NDCG) are then computed from those labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Manual relevance labels used as gold standard; referenced meta-evaluations compare system-level rankings and individual label agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>System-level Spearman or Kendall correlation often reported (examples: perfect Spearman = 1.0 in some setups), but label-level agreement reported to range from 0.12 to 0.61 in cited conversational-system study (Mehri & Eskenazi).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of label-level reliability and fine-grained judgment fidelity: relying solely on system-level agreement ignores per-item disagreements that matter for dataset robustness and downstream development.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Mehri & Eskenazi: even with perfect system-level correlation, label-level agreement varied widely (0.12–0.61), showing LLMs can reproduce leaderboard ordering while disagreeing on many individual labels.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>When label-level agreement is explicitly measured and found high, LLM evaluators can be considered more trustworthy; the paper recommends always measuring label-level agreement in addition to system-level metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Meta-Eval Trope #5: Ignored Label Correlation' (references Mehri & Eskenazi and general meta-evaluation discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9742.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rubber-Stamp Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rubber-Stamp Effect (Judge Trope #12)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When humans verify LLM-generated labels, they can be unduly influenced by the model's output and conform to incorrect assessments, producing automated-but-human-validated labels that still reflect LLM errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Hybrid human-LLM annotation workflows across IR and QA tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various LLM evaluators (paper cites conformity studies and experiments showing assessor bias when exposed to model outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLM generates labels or answers, which are then shown to human annotators for verification; human verification can be superficial.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators asked to verify or correct LLM labels; paper notes assessor fatigue and task repetition reduce critical oversight (exact annotator counts/protocols not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Suggested measurement: compare fully manual labels vs human-verified LLM labels; specific numeric results not provided in paper but references studies on conformity and influence.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of independent human judgment and critical oversight; label quality can degrade because humans default to model outputs, reducing the benefit of human-in-the-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Cited experimental findings (papers [7,24,31,66]) and classic conformity literature (Asch) demonstrate people conform to model suggestions; proposed vigilance tests where adversarial or flipped labels detect rubber-stamping.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Guardrails can mitigate the effect: embed vigilance tests, reward flagging of errors, and random adversarial inserts to ensure annotator engagement; when such guardrails are applied, human verification can remain effective.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Judge Trope #12: Rubber-Stamp Effect' (discussion and proposed vigilance guardrails)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9742.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Black-box Labeling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Black-box Labeling (Judge Trope #13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-generated relevance labels often lack interpretable rationales; without transparent explanations, it's difficult to audit why a label was assigned, increasing the risk of uncritical acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Complex relevance judgments in IR and QA; any task where labels compress multifaceted criteria into single scores</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM evaluators capable of producing labels and optional rationales; specific model/version not required by trope.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLMs produce a relevance label and (optionally) a rationale; the paper cautions these rationales can be flawed and need critical assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judges typically produce labels with implicit or explicit reasoning; recommended to require stepwise reasoning and multi-stage justification.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No single numeric metric provided; paper recommends measuring variability between independent manual labels and human-verified LLM labels to detect black-box behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of interpretability and auditability: LLM rationales can be incorrect or misleading, making it hard to detect errors or to decompose complex judgments for reproducibility and debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Discussion of opaque LLM rationales and how flawed explanations can still be accepted by humans (ties to Rubber-Stamp); no single numeric case provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Mitigation: break complex labeling into smaller steps, require articulated reasoning from both LLMs and humans (chain-of-thought-like protocols) to increase transparency and enable auditing.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Judge Trope #13: Black-box Labeling' (discussion and guardrails)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9742.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predictable Secrets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictable Secrets (Judge Trope #14)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Secrets or guardrail signals intended to be known only to human judges (to prevent leakage) can be inferred or guessed by LLMs, undermining human-only checks and enabling systems to exploit evaluation signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>IR evaluation paradigms that rely on hidden human-only signals or secrets (nuggets, rubrics, graded secrets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various LLMs capable of inferring patterns or predicting withheld information; no single model specified.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLM is used to predict secrets or to infer patterns from structured labels; per-token likelihood or direct prediction can reveal guessability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judges supply secrets, rubrics, or nugget annotations intended to be hidden from systems; exact human protocol varies.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Suggested measures: have an LLM predict the secret directly or compute per-token likelihood; then replace the manual secret with predicted secret and observe ranking changes — no numeric example provided.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of the protective barrier provided by human-only secrets: LLMs can guess patterns, enabling leakage, circularity, and inflated system performance that exploits predictable test structure.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Conceptual example and recommended quantification; no empirical numeric case directly in paper but ties to test-set leakage and circularity sections.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Guardrails: make secrets complex, varied, require contextual understanding or subjective reasoning that resists LLM inference; design tasks to reduce predictability.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Judge Trope #14: Predictable Secrets' (discussion and recommended quantification)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9742.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Evolution / Reproducibility Drift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Evolution (Meta-Eval Trope #7)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM behavior changes over time as models are updated or retired, causing drift in evaluator behavior and hindering longitudinal reproducibility of evaluation results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Any domain relying on LLM evaluators across time (IR, summarization, dialogue, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Any cloud/hosted LLM subject to versioning and updates (paper notes providers may retire or update models without notice).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Meta-evaluations often rely on a single prompt or model snapshot; the paper warns that future iterations may judge differently due to training on new data or architecture changes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments are comparatively stable and reproducible when protocols are archived, whereas LLM versions may not be accessible later for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No single numeric metric; recommended to periodically repeat meta-evaluations and track changes in top-system rankings, label inconsistencies, and variability on previously unjudged documents.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of reproducibility and longitudinal comparability: inability to re-run prior evaluations with the same LLM behavior undermines experiment auditing and cumulative science.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Discussion of providers retiring models and model drift; cited studies on ChatGPT behavior over time and model evolution issues (e.g., references to Chen et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Mitigation: continual re-validation, recurring meta-evaluation protocols, archiving model outputs and relying on human-verified benchmarks for long-term comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Meta-Eval Trope #7: LLM Evolution' (discussion and guardrails)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9742.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Canva case study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canva: Synthetic known-item LLM-evaluator case study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A product case where an LLM-evaluator synthesizes known-items and queries using aggregated anonymized statistics, enabling private, repeatable offline evaluation that later aligns directionally with human A/B tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Private/enterprise search (known-item / re-finding tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Unspecified LLM used to synthesize items and queries (paper anonymizes company details; LLM not embedded in system under test).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLM synthesizes a target known-item and generation of distractor items and user queries grounded in aggregated, anonymized user statistics; produced a synthetic test collection with known relevance at creation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human A/B and online interleaving experiments conducted later to validate offline LLM-driven signals; human involvement used in later-stage validation (specifics not detailed).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Directional alignment between synthetic LLM evaluation and subsequent human-centered A/B experiments reported qualitatively; no numeric agreement provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When properly isolated (no LLM inside the search system), losses are minimized; however, potential risks (e.g., LLM Narcissism, Circularity) are acknowledged if guardrails fail.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Not a failure case: Canva used human later-stage validation to derisk biases; shows that LLM-driven offline evaluation can substitute earlier human annotation stages if validated later with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Success caveat: effectiveness required no LLM in the evaluated system and later human experiments to confirm directional alignment; demonstrates one practical pathway to retain validity while leveraging LLM scale.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Case Studies' — 'Canva' (description of the private known-item evaluation workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9742.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9742.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Valence case study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Valence: AI-assisted enterprise coaching evaluation case study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Enterprise coaching system that used LLMs for generation and evaluation; LLM Narcissism inflated effectiveness estimates compared to human assessment, prompting use of separate LLMs for generation and scoring and calibration against human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Conversational coaching dialogs (dialogue quality and coaching effectiveness metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple large commercial LLMs integrated with specialized dialogue components (specific model names not disclosed).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLM-based evaluator scored dialog quality and turn-level metrics, including CSAT-like scores and coaching-effectiveness rubrics; evaluator used for privacy-preserving offline optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Subject-matter expert-created rubrics used to calibrate LLM scoring against human-labeled datasets; later-stage human validation used for calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Quantitative agreement not reported; qualitative note that LLM Narcissism led to inflated effectiveness estimates vs human assessment, leading to mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-based evaluation overestimated coaching effectiveness relative to humans (inflation), vulnerability to reward-hacking when used as reward model for RL, and risk of amplifying circularity in real-time turn-level evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Observed inflation of effectiveness estimates compared to human assessments; led to explicit mitigation: separate LLMs for generation and scoring and calibration on human-labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>When calibrated against human labels and using separate LLMs for generation and scoring, the system maintained usefulness for scalability and privacy; still flagged long-term risks (reward hacking, self-training collapse).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Case Studies' — 'Valence' (description, challenges, and mitigations)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look <em>(Rating: 2)</em></li>
                <li>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor <em>(Rating: 2)</em></li>
                <li>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores <em>(Rating: 2)</em></li>
                <li>A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment <em>(Rating: 1)</em></li>
                <li>LLM-based relevance assessment still can't replace human relevance assessment <em>(Rating: 2)</em></li>
                <li>In search of verifiability: Explanations rarely enable complementary performance in AI-advised decision making <em>(Rating: 1)</em></li>
                <li>The Curse of Recursion: Training on Generated Data Makes Models Forget <em>(Rating: 1)</em></li>
                <li>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9742",
    "paper_id": "paper-278166056",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Circularity",
            "name_full": "Circularity (Eval Trope #1 & #2: Leaking evaluation signal / LLM-eval as ranker)",
            "brief_description": "When LLM-based evaluators influence system development or are reused inside systems (e.g., as rerankers), evaluation becomes self-reinforcing, inflating scores and diverging from human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Information retrieval (retrieval-augmented generation / ranking)",
            "llm_judge_model": "Umbrela (open-source reproduction of Bing relevance assessor) used in TREC RAG 2024 demonstration",
            "llm_judge_setup": "Umbrela generated relevance grades used as qrels; reranking executed by sorting by Umbrela grade (primary) then original score (secondary); Umbrela also used to evaluate reranked runs.",
            "human_evaluation_setup": "Manual relevance labels from TREC RAG 2024 human assessors (official TREC manual qrels; exact annotator counts/protocol not specified in paper).",
            "agreement_metric": "Kendall's tau between system rankings: tau@60 = 0.84 (original Umbrela vs human), after Umbrela-based reranking tau@60 = 0.63, discordant pairs rose from ~8% to 18% for top 60; tau falls to 0.44 for top 20 systems; twelve systems score Umbrela-NDCG &gt; 0.95 while manual NDCG for same systems is 0.68-0.72.",
            "losses_identified": "Validity and external generalization are lost: evaluations become inflated and misleading, leaderboard rankings diverge from human judgments, and claimed near-perfect scores mask substantial differences seen by humans.",
            "examples_of_loss": "TREC RAG 2024 simulation: reranking with Umbrela improved human NDCG (valid system improvement), but evaluating reranked systems with Umbrela created large divergence from human evaluation (more discordant pairs, inflated NDCG for many systems).",
            "counterexamples_or_caveats": "Umbrela correlates well with human judges on original systems (tau@60 = 0.84), showing LLM-evaluators can be valid when they are not embedded in the system; reranking with Umbrela genuinely improved human-evaluated performance, indicating LLM signals can be useful on the system side if evaluation circularity is avoided.",
            "paper_reference": "Section 'Eval Trope #1: Circularity', 'Eval Trope #2: LLM-Eval as a Ranker', and 'Demonstration of Circularity' (TREC RAG 2024 analysis, Figures 2-4)",
            "uuid": "e9742.0",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM Narcissism",
            "name_full": "LLM Narcissism (Eval Trope #3)",
            "brief_description": "LLM evaluators systematically prefer text that resembles their own generations or family of models, biasing judgments toward outputs similar to the judge's own style and token-likelihood patterns.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "General text evaluation across IR and generative responses (ranking, conversational quality)",
            "llm_judge_model": "Generic LLMs (examples in paper: GPT-4 and other LLM families); specific experiments referenced in literature (e.g., GPT-4 favoring GPT-4 outputs).",
            "llm_judge_setup": "LLMs used to assign relevance/quality scores to outputs; often single-model evaluation or single-family evaluators; mitigation suggested by reserving an LLM for evaluation or ensembling multiple LLMs.",
            "human_evaluation_setup": "Human assessors described as better at recognizing nuance, novelty, and contextual diversity; specifics vary by cited study and task.",
            "agreement_metric": "No single numeric agreement provided for narcissism; paper cites studies showing systematic favoritism and suggests measuring frequency that evaluator favors outputs from same underlying model (protocol from Liu et al.).",
            "losses_identified": "Loss of fairness and openness to novelty: systems that produce novel, non-standard but valid responses may be penalized; rankings biased toward systems sharing lineage with the evaluator; sets an implicit performance ceiling.",
            "examples_of_loss": "Mentioned empirical observations where GPT-4 evaluators favor GPT-4-generated responses even when humans find no meaningful difference; general phenomenon described in industry case study (Valence) where narcissism inflated effectiveness estimates versus human assessment.",
            "counterexamples_or_caveats": "Mitigations: reserve an LLM family for evaluation only, or ensemble multiple diverse LLM evaluators and exclude votes from shared-lineage evaluators; human judge diversity is more robust for nuance but harder to emulate with LLM prompting.",
            "paper_reference": "Section 'Eval Trope #3: LLM Narcissism' and 'Valence' case study",
            "uuid": "e9742.1",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Ignored Label Correlation",
            "name_full": "Ignored Label Correlation (Meta-Eval Trope #5)",
            "brief_description": "High system-level correlation between LLM and human evaluations can mask wide disagreement on individual query-document labels; label-level agreement is often much lower than system-level metrics imply.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Conversational systems and general IR relevance labeling",
            "llm_judge_model": "Generic LLM evaluators (specific evaluators vary across cited meta-evaluation studies).",
            "llm_judge_setup": "LLMs generate relevance labels for query-document or query-response pairs; system-level metrics (e.g., NDCG) are then computed from those labels.",
            "human_evaluation_setup": "Manual relevance labels used as gold standard; referenced meta-evaluations compare system-level rankings and individual label agreement.",
            "agreement_metric": "System-level Spearman or Kendall correlation often reported (examples: perfect Spearman = 1.0 in some setups), but label-level agreement reported to range from 0.12 to 0.61 in cited conversational-system study (Mehri & Eskenazi).",
            "losses_identified": "Loss of label-level reliability and fine-grained judgment fidelity: relying solely on system-level agreement ignores per-item disagreements that matter for dataset robustness and downstream development.",
            "examples_of_loss": "Mehri & Eskenazi: even with perfect system-level correlation, label-level agreement varied widely (0.12–0.61), showing LLMs can reproduce leaderboard ordering while disagreeing on many individual labels.",
            "counterexamples_or_caveats": "When label-level agreement is explicitly measured and found high, LLM evaluators can be considered more trustworthy; the paper recommends always measuring label-level agreement in addition to system-level metrics.",
            "paper_reference": "Section 'Meta-Eval Trope #5: Ignored Label Correlation' (references Mehri & Eskenazi and general meta-evaluation discussion)",
            "uuid": "e9742.2",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Rubber-Stamp Effect",
            "name_full": "Rubber-Stamp Effect (Judge Trope #12)",
            "brief_description": "When humans verify LLM-generated labels, they can be unduly influenced by the model's output and conform to incorrect assessments, producing automated-but-human-validated labels that still reflect LLM errors.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Hybrid human-LLM annotation workflows across IR and QA tasks",
            "llm_judge_model": "Various LLM evaluators (paper cites conformity studies and experiments showing assessor bias when exposed to model outputs).",
            "llm_judge_setup": "LLM generates labels or answers, which are then shown to human annotators for verification; human verification can be superficial.",
            "human_evaluation_setup": "Human annotators asked to verify or correct LLM labels; paper notes assessor fatigue and task repetition reduce critical oversight (exact annotator counts/protocols not provided).",
            "agreement_metric": "Suggested measurement: compare fully manual labels vs human-verified LLM labels; specific numeric results not provided in paper but references studies on conformity and influence.",
            "losses_identified": "Loss of independent human judgment and critical oversight; label quality can degrade because humans default to model outputs, reducing the benefit of human-in-the-loop verification.",
            "examples_of_loss": "Cited experimental findings (papers [7,24,31,66]) and classic conformity literature (Asch) demonstrate people conform to model suggestions; proposed vigilance tests where adversarial or flipped labels detect rubber-stamping.",
            "counterexamples_or_caveats": "Guardrails can mitigate the effect: embed vigilance tests, reward flagging of errors, and random adversarial inserts to ensure annotator engagement; when such guardrails are applied, human verification can remain effective.",
            "paper_reference": "Section 'Judge Trope #12: Rubber-Stamp Effect' (discussion and proposed vigilance guardrails)",
            "uuid": "e9742.3",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Black-box Labeling",
            "name_full": "Black-box Labeling (Judge Trope #13)",
            "brief_description": "LLM-generated relevance labels often lack interpretable rationales; without transparent explanations, it's difficult to audit why a label was assigned, increasing the risk of uncritical acceptance.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Complex relevance judgments in IR and QA; any task where labels compress multifaceted criteria into single scores",
            "llm_judge_model": "LLM evaluators capable of producing labels and optional rationales; specific model/version not required by trope.",
            "llm_judge_setup": "LLMs produce a relevance label and (optionally) a rationale; the paper cautions these rationales can be flawed and need critical assessment.",
            "human_evaluation_setup": "Human judges typically produce labels with implicit or explicit reasoning; recommended to require stepwise reasoning and multi-stage justification.",
            "agreement_metric": "No single numeric metric provided; paper recommends measuring variability between independent manual labels and human-verified LLM labels to detect black-box behavior.",
            "losses_identified": "Loss of interpretability and auditability: LLM rationales can be incorrect or misleading, making it hard to detect errors or to decompose complex judgments for reproducibility and debugging.",
            "examples_of_loss": "Discussion of opaque LLM rationales and how flawed explanations can still be accepted by humans (ties to Rubber-Stamp); no single numeric case provided in paper.",
            "counterexamples_or_caveats": "Mitigation: break complex labeling into smaller steps, require articulated reasoning from both LLMs and humans (chain-of-thought-like protocols) to increase transparency and enable auditing.",
            "paper_reference": "Section 'Judge Trope #13: Black-box Labeling' (discussion and guardrails)",
            "uuid": "e9742.4",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Predictable Secrets",
            "name_full": "Predictable Secrets (Judge Trope #14)",
            "brief_description": "Secrets or guardrail signals intended to be known only to human judges (to prevent leakage) can be inferred or guessed by LLMs, undermining human-only checks and enabling systems to exploit evaluation signals.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "IR evaluation paradigms that rely on hidden human-only signals or secrets (nuggets, rubrics, graded secrets)",
            "llm_judge_model": "Various LLMs capable of inferring patterns or predicting withheld information; no single model specified.",
            "llm_judge_setup": "LLM is used to predict secrets or to infer patterns from structured labels; per-token likelihood or direct prediction can reveal guessability.",
            "human_evaluation_setup": "Human judges supply secrets, rubrics, or nugget annotations intended to be hidden from systems; exact human protocol varies.",
            "agreement_metric": "Suggested measures: have an LLM predict the secret directly or compute per-token likelihood; then replace the manual secret with predicted secret and observe ranking changes — no numeric example provided.",
            "losses_identified": "Loss of the protective barrier provided by human-only secrets: LLMs can guess patterns, enabling leakage, circularity, and inflated system performance that exploits predictable test structure.",
            "examples_of_loss": "Conceptual example and recommended quantification; no empirical numeric case directly in paper but ties to test-set leakage and circularity sections.",
            "counterexamples_or_caveats": "Guardrails: make secrets complex, varied, require contextual understanding or subjective reasoning that resists LLM inference; design tasks to reduce predictability.",
            "paper_reference": "Section 'Judge Trope #14: Predictable Secrets' (discussion and recommended quantification)",
            "uuid": "e9742.5",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM Evolution / Reproducibility Drift",
            "name_full": "LLM Evolution (Meta-Eval Trope #7)",
            "brief_description": "LLM behavior changes over time as models are updated or retired, causing drift in evaluator behavior and hindering longitudinal reproducibility of evaluation results.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Any domain relying on LLM evaluators across time (IR, summarization, dialogue, etc.)",
            "llm_judge_model": "Any cloud/hosted LLM subject to versioning and updates (paper notes providers may retire or update models without notice).",
            "llm_judge_setup": "Meta-evaluations often rely on a single prompt or model snapshot; the paper warns that future iterations may judge differently due to training on new data or architecture changes.",
            "human_evaluation_setup": "Human judgments are comparatively stable and reproducible when protocols are archived, whereas LLM versions may not be accessible later for verification.",
            "agreement_metric": "No single numeric metric; recommended to periodically repeat meta-evaluations and track changes in top-system rankings, label inconsistencies, and variability on previously unjudged documents.",
            "losses_identified": "Loss of reproducibility and longitudinal comparability: inability to re-run prior evaluations with the same LLM behavior undermines experiment auditing and cumulative science.",
            "examples_of_loss": "Discussion of providers retiring models and model drift; cited studies on ChatGPT behavior over time and model evolution issues (e.g., references to Chen et al.).",
            "counterexamples_or_caveats": "Mitigation: continual re-validation, recurring meta-evaluation protocols, archiving model outputs and relying on human-verified benchmarks for long-term comparability.",
            "paper_reference": "Section 'Meta-Eval Trope #7: LLM Evolution' (discussion and guardrails)",
            "uuid": "e9742.6",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Canva case study",
            "name_full": "Canva: Synthetic known-item LLM-evaluator case study",
            "brief_description": "A product case where an LLM-evaluator synthesizes known-items and queries using aggregated anonymized statistics, enabling private, repeatable offline evaluation that later aligns directionally with human A/B tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Private/enterprise search (known-item / re-finding tasks)",
            "llm_judge_model": "Unspecified LLM used to synthesize items and queries (paper anonymizes company details; LLM not embedded in system under test).",
            "llm_judge_setup": "LLM synthesizes a target known-item and generation of distractor items and user queries grounded in aggregated, anonymized user statistics; produced a synthetic test collection with known relevance at creation.",
            "human_evaluation_setup": "Human A/B and online interleaving experiments conducted later to validate offline LLM-driven signals; human involvement used in later-stage validation (specifics not detailed).",
            "agreement_metric": "Directional alignment between synthetic LLM evaluation and subsequent human-centered A/B experiments reported qualitatively; no numeric agreement provided in paper.",
            "losses_identified": "When properly isolated (no LLM inside the search system), losses are minimized; however, potential risks (e.g., LLM Narcissism, Circularity) are acknowledged if guardrails fail.",
            "examples_of_loss": "Not a failure case: Canva used human later-stage validation to derisk biases; shows that LLM-driven offline evaluation can substitute earlier human annotation stages if validated later with humans.",
            "counterexamples_or_caveats": "Success caveat: effectiveness required no LLM in the evaluated system and later human experiments to confirm directional alignment; demonstrates one practical pathway to retain validity while leveraging LLM scale.",
            "paper_reference": "Section 'Case Studies' — 'Canva' (description of the private known-item evaluation workflow)",
            "uuid": "e9742.7",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Valence case study",
            "name_full": "Valence: AI-assisted enterprise coaching evaluation case study",
            "brief_description": "Enterprise coaching system that used LLMs for generation and evaluation; LLM Narcissism inflated effectiveness estimates compared to human assessment, prompting use of separate LLMs for generation and scoring and calibration against human labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Conversational coaching dialogs (dialogue quality and coaching effectiveness metrics)",
            "llm_judge_model": "Multiple large commercial LLMs integrated with specialized dialogue components (specific model names not disclosed).",
            "llm_judge_setup": "LLM-based evaluator scored dialog quality and turn-level metrics, including CSAT-like scores and coaching-effectiveness rubrics; evaluator used for privacy-preserving offline optimization.",
            "human_evaluation_setup": "Subject-matter expert-created rubrics used to calibrate LLM scoring against human-labeled datasets; later-stage human validation used for calibration.",
            "agreement_metric": "Quantitative agreement not reported; qualitative note that LLM Narcissism led to inflated effectiveness estimates vs human assessment, leading to mitigation.",
            "losses_identified": "LLM-based evaluation overestimated coaching effectiveness relative to humans (inflation), vulnerability to reward-hacking when used as reward model for RL, and risk of amplifying circularity in real-time turn-level evaluation.",
            "examples_of_loss": "Observed inflation of effectiveness estimates compared to human assessments; led to explicit mitigation: separate LLMs for generation and scoring and calibration on human-labeled data.",
            "counterexamples_or_caveats": "When calibrated against human labels and using separate LLMs for generation and scoring, the system maintained usefulness for scalability and privacy; still flagged long-term risks (reward hacking, self-training collapse).",
            "paper_reference": "Section 'Case Studies' — 'Valence' (description, challenges, and mitigations)",
            "uuid": "e9742.8",
            "source_info": {
                "paper_title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "rating": 2,
            "sanitized_title": "a_largescale_study_of_relevance_assessments_with_large_language_models_an_initial_look"
        },
        {
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "rating": 2,
            "sanitized_title": "umbrela_umbrela_is_the_opensource_reproduction_of_the_bing_relevance_assessor"
        },
        {
            "paper_title": "LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores",
            "rating": 2,
            "sanitized_title": "llms_as_narcissistic_evaluators_when_ego_inflates_evaluation_scores"
        },
        {
            "paper_title": "A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment",
            "rating": 1,
            "sanitized_title": "a_humanai_comparative_analysis_of_prompt_sensitivity_in_llmbased_relevance_judgment"
        },
        {
            "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
            "rating": 2,
            "sanitized_title": "llmbased_relevance_assessment_still_cant_replace_human_relevance_assessment"
        },
        {
            "paper_title": "In search of verifiability: Explanations rarely enable complementary performance in AI-advised decision making",
            "rating": 1,
            "sanitized_title": "in_search_of_verifiability_explanations_rarely_enable_complementary_performance_in_aiadvised_decision_making"
        },
        {
            "paper_title": "The Curse of Recursion: Training on Generated Data Makes Models Forget",
            "rating": 1,
            "sanitized_title": "the_curse_of_recursion_training_on_generated_data_makes_models_forget"
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
            "rating": 1,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.01502075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations
27 Apr 2025</p>
<p>Laura Dietz 
Peter Bailey 
Charles Clarke 
Jeff Dalton 
Mark Sanderson 
Nick Craswell 
Oleg Zendel 
Ellese Cotterill 
Faegheh Hasibi 
ZendelDietz 
Bailey 
Cotterill, DaltonClarke 
HasibiCraswell Sanderson </p>
<p>University of New Hampshire
USA</p>
<p>Oleg Zendel RMIT University
Australia</p>
<p>CanvaAustralia</p>
<p>University of Waterloo
Canada Ellese Cotterill Canva Australia</p>
<p>University of Edinburgh United Kingdom Faegheh Hasibi Radboud University
Netherlands</p>
<p>RMIT University
Australia</p>
<p>MicrosoftUSA</p>
<p>LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations
27 Apr 202581FBC04E051CD8282F7AB5BDE595ACB8arXiv:2504.19076v1[cs.IR]LLM-Based Evaluation, Validity of Experimentation
Large Language Models (LLMs) are increasingly used to evaluate information retrieval (IR) systems, generating relevance judgments traditionally made by human assessors.Recent empirical studies suggest that LLM-based evaluations often align with human judgments, leading some to suggest that human judges may no longer be necessary, while others highlight concerns about judgment reliability, validity, and long-term impact.As IR systems begin incorporating LLM-generated signals, evaluation outcomes risk becoming self-reinforcing, potentially leading to misleading conclusions.This paper examines scenarios where LLM-evaluators may falsely indicate success, particularly when LLM-based judgments influence both system development and evaluation.We highlight key risks, including bias reinforcement, reproducibility challenges, and inconsistencies in assessment methodologies.To address these concerns, we propose tests to quantify adverse effects, guardrails, and a collaborative framework for constructing reusable test collections that integrate LLM judgments responsibly.By providing perspectives from academia and industry, this work aims to establish best practices for the principled use of LLMs in IR evaluation.CCS Concepts• Information systems → Evaluation of retrieval results.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) are increasingly used in the evaluation of information retrieval (IR) systems, generating relevance judgments that were traditionally the domain of human assessors.Given an information need (or topic) and a set of documents, assessors determine the relevance of each document to the topic.A process that forms the foundation of retrieval evaluation.However, due to the vast number of topic-document pairs, traditional assessment relies on pooling methods to identify a subset of documents for judgment.LLMs offer an alternative approach, with the potential to scale relevance assessments far beyond the limits of human annotation.However, Soboroff [64] writes: "Letting the LLM write your truth data handicaps the evaluation by setting that LLM as a ceiling on performance." In this paper we aim for a middle ground by discussing 13 ways in which LLMs can negatively impact the evaluation and how avoid this adverse effect.</p>
<p>Pro.One of the most impactful advantages of LLM-based evaluation is speed.Unlike human assessors, who require coordination, training, and extensive annotation time, LLMs can generate relevance labels almost instantly.This dramatically lowers the cost of evaluation, making it possible to assess larger datasets, cover a wider range of retrieval tasks, and conduct evaluations more frequently.These benefits have led to the rapid adoption of LLMs in large-scale evaluation pipelines.Microsoft, for example, now uses OpenAI's GPT models for relevance assessment in Bing [68].More recently, Upadhyay et al. [72] introduced Umbrela, an open-source toolkit based on a similar prompt that uses proprietary LLMs to label unjudged documents.Its application in a recent TREC task, further reinforces the notion that human assessors could be replaced [70].Beyond labeling, LLMs have been proposed for fully synthetic test collections, where they replace human users in both query generation and relevance judgment [51].</p>
<p>Con.Although empirical studies demonstrate the effectiveness of LLM-based judgments, concerns remain regarding their reliability, validity, and long-term implications for IR evaluation [22].The increasing reliance on LLMs for test collection creation raises fundamental questions about reproducibility.At the same time traditional pooled judgment methodologies are becoming impractical for assessing generative and multi-modal systems.Furthermore, there is no consensus on how to mitigate the risks associated with these models, including biases, inconsistencies, and potential vulnerabilities such as susceptibility to query stuffing [3].Without a shared principled framework for responsible adoption, the unchecked use of LLMs in evaluation studies may lead to misleading conclusions, where prior work is cited without appropriate consideration of its limitations.</p>
<p>Below we outline our contributions by specifying questions that are addressed in this paper.</p>
<p>Can I Use LLM-judgments in My Next</p>
<p>Research Paper?</p>
<p>Contributions.This paper explores validity challenges posed by using LLMs as a evaluator, aims to codify the best practices for ensuring that LLM-based evaluation remains a valid experimentation approach for IR research.Maintaining scientific rigor requires identifying and recognizing the risks, implementing guardrails, and continuously confirming their validity with human judgments.</p>
<p>Based on many discussions and reviewing the literature, we establish a taxonomy of reoccurring issues with LLM evaluations, which we refer to as LLM Evaluation Tropes (Figure 1).To this end, we outline guardrails that should be applied when using LLMbased evaluation for research publications. 1We underscore the truthfulness and relevance of these tropes by case studies from academia and industry.This paper is co-authored by a team of established international researchers from who hold a spectrum of viewpoints.</p>
<p>What is a Valid Experiment?</p>
<p>We take valid LLM evaluators to mean evaluators whose measurements align with human intuition and quantify the utility for real people.We follow the guidance from Spärck Jones and van Rijsbergen [65]: "It is apparent in particular that it is most important that the ideal collection(s) should be a means of relating valid abstract studies of information retrieval and those of operational systems and user behaviour."</p>
<p>In industry, the effectiveness of systems is typically validated through A/B testing and manual assessments.In academia, evaluation helps to determine which approaches constitute research advances and should be submitted to conferences and shared tasks.In both industry and academia, it is essential to obtain quantitative quality measures that credibly reflect relevance while mitigating unintentional biases, such as test data leakage, which may compromise the validity of drawn conclusions.</p>
<p>In this paper, we discuss conditions under which the use of LLM evaluators may (inadvertently) threaten the validity of the experiment.</p>
<p>Figure 1: LLM-evaluation tropes that can lead to invalid conclusions about evaluation, systems, and the efficacy of human judges that oversee the process.Overarching patterns are circularity ⟳, Goodhart's law ♥, and loss of variety .</p>
<p>Evaluation Tropes:</p>
<p>⟳ #1 Circularity: Leaking the evaluation signal into the IR system.⟳ #2 LLM-Eval as a Ranker: Using the same approach in the system and the evaluation.♥ #3 LLM Narcissism:</p>
<p>Why Does the Old Evaluation Paradigm No Longer Apply?</p>
<p>The long standing and widely accepted testing paradigm, as practiced in TREC,2 CLEF, 3 FIRE, 4 and NTCIR, 5 assumes that all effective IR systems will identify a similar set of relevant documents.</p>
<p>To approximate a comprehensive relevance set, pooling techniques select top-ranked documents from multiple systems for manual assessment [73].While this approach remains foundational, it has diverged from the original Cranfield experiment [17], in which all documents were judged rather than a limited subset of potentially relevant ones.Since Cranfield's inception in 1967, the scale of document collections, including the Web, has expanded exponentially, making exhaustive manual assessment infeasible.Moreover, when synthesis or generation is introduced into IR systems, many of the existing assumptions collapse.Instead of a static search engine result page (SERP) containing a fixed set of documents, generative models produce paraphrased or alternative responses, many of which may be equally valid.Assessing all such variations manually is impractical.At the same time, a single word change can render a relevant response to be non-relevant.</p>
<p>Even without generative models, the traditional evaluation setup faces limitations.Retrieval systems that are not included in the original pooling process often return unjudged documents, leading to incomplete evaluations.This issue is particularly acute for newer retrieval paradigms -such as dense retrieval, neural reranking, and query reformulation -which retrieve documents in ways that diverge from those originally used to construct the test collections [52].As a result, many established test collections are becoming less useful for evaluating newer retrieval models, especially when these models differ substantially from those used in the pooling process.</p>
<p>To address these challenges, the IR community has long relied on approximations to make the evaluation tractable, while continuously refining evaluation methodologies to uphold validity [25,40,45,46,54,56].The extensive body of methodological research developed over the years provides valuable insights for designing guardrails and validation techniques in emerging evaluation paradigms.</p>
<p>We call for the development of novel evaluation paradigms that uphold the validity of experimental results, and for reusable test collection construction methods that minimize the need for additional human assessments.</p>
<p>Outline</p>
<p>We start by exploring different ways in which the use of LLMs in evaluation can inadvertently negatively impact the validity of the evaluation.Section 3 supports this with case studies from two companies who identified and overcame validity issues in their experimentation.We include a demonstration of how circularity can arise with data from a recent TREC task in Section 4 and suggest a annual effort to cooperatively build test collections with recent systems, evaluators, and content modifiers in Section 5, before concluding the paper.</p>
<p>LLM-Eval Tropes and Guardrails</p>
<p>During system development, various forms of Goodhart's Law [28] and test data leakage may compromise evaluation integrity.A critical hazard is circularity, a feedback loop in which evaluator assumptions and system design decisions reinforce one another, distorting evaluation outcomes.As a result, evaluation metrics may no longer reflect human preferences under realistic conditions, thereby invalidating the evaluation paradigm.</p>
<p>We present a taxonomy of recurring tropes observed in LLMbased evaluations (see Figure 1 for an overview).These tropes can negatively affect different stages of the evaluation process, and we discuss several types that pose particular challenges to evaluation validity:</p>
<p>(1) Eval: Tropes that lead to misleading or incorrect evaluation measurements.(2) Meta-Eval: Tropes that give the false impression of high evaluation quality or reliability.(3) System: Tropes that cause an IR system to perform poorly or unreliably in real-world scenarios.(4) Judge: Tropes that inadvertently undermine the effectiveness of human judgment in the evaluation process.</p>
<p>Below, we examine these common trope patterns, highlighting their pitfalls, and propose guardrails to mitigate their shortcomings.</p>
<p>Evaluation Tropes</p>
<p>We begin by describing a set of common evaluation tropes that can undermine the validity of LLM-based evaluation systems.These issues arise when the design or application of the evaluation process produces misleading metrics, circular validation, or inflated estimates of system performance.</p>
<p>Eval Trope #1: Circularity</p>
<p>-Leaking the evaluation signal into the IR system.-When the output of an LLM evaluator influences the design or training of the IR system, the evaluation risks becoming selfreinforcing rather than genuinely informative [2,8,15,76].</p>
<p>Even when system developers and LLM evaluators work independently, unintentional contamination can occur [22].A system may unknowingly integrate aspects of an LLM evaluator's methodology, either through training data, algorithmic choices, or shared heuristics.This accidental feedback loop can result in inflated performance in LLM-based evaluations without corresponding gains for real-world applications [8,53,55,62].</p>
<p>A related concern is that repeated circularity, i.e., when systems are trained and evaluated with signals from the same LLM evaluator, may result in effects of the Model Collapse trope [60].</p>
<p>Quantify effect.The impact of this trope can be assessed by comparing against a manual evaluation paradigm and measuring divergence in leaderboard rankings -particularly for systems that may have been influenced by evaluation signal leakage.We present one such study in Section 4.</p>
<p>Guardrail.Involving human annotators offers independent against self-reinforcing evaluation loops.Care must be taken to steer clear of the Predictable Secret trope.</p>
<p>Eval Trope #2: LLM-Eval as a Ranker</p>
<p>-Using the same approach in the system and the evaluation.-This trope refers to a special case of circularity that arises when the same LLM evaluator is used both within the ranking system and as the evaluation metric.This trope effects from any form of self-refinement procedure [78].In the context of IR, this arises when the same procedure generates both the rank scores and the evaluation scores, the result is a superficial alignment that yields an artificially perfect evaluation-despite the system potentially performing poorly under human judgment [8,15,26].</p>
<p>A similar failure mode can be illustrated with BM25: if the top ten documents retrieved by BM25 were assumed to define groundtruth relevance, then a BM25 ranker would trivially achieve perfect P@10.Clearly, no one would accept such a circular and and invalid evaluation paradigm -except in limited contexts such as improving the efficiency of a system [14].</p>
<p>We recognize that system developers will want to include notions of LLM-evaluation in their system.In Section 4 we examine this scenario in the context of the TREC RAG 2024 track, which employed the Umbrela LLM evaluator.We show that reranking submitted runs using Umbrela improves performance under manual assessment -a valid and actionable finding that supports system-side use of LLM evaluators.</p>
<p>However, evaluating such reranked systems using the same Umbrela metric introduces circularity and leads to invalid evaluation outcomes.In our analysis, this reuse of the evaluator causes significant divergence from human assessments: human and LLM evaluators disagree on the relative quality of 18% of system pairsmore than twice as many as found on original systems.Moreover, while twelve systems score above 0.95 in Umbrela-NDCG, their manual NDCG scores range from only 0.68 to 0.72.</p>
<p>Quantify effect.This effect can be quantified by repeating the analysis in Section 4, directly comparing system rankings under LLM and human evaluation.</p>
<p>Guardrail.Avoid using an LLM evaluation procedure if the same (or closely related) procedure may be embedded within the system under evaluation.</p>
<p>Eval Trope #3: LLM Narcissism</p>
<p>-LLMs prefer text from their own model.-Being language models, LLM evaluators tend to assign higher scores to text that aligns closely with their own generation patterns-effectively equating textual quality with per-token likelihood.This leads to a preference for outputs produced by the same model family.For instance, GPT-4 may systematically favor responses generated by GPT-4-based systems, even when human assessors detect no meaningful quality difference [38,39,49,78].This results in distorted system rankings and compromises the validity of the evaluation outcomes.</p>
<p>Models may be optimized to align with the LLM's biases rather than real-world relevance assessments, undermining the credibility of the evaluation.</p>
<p>Quantify effect.The experimental protocol from Liu et al. [38] can quantify this effect.It involves recording the LLM versions used in both systems and evaluators, and analyzing how often evaluators favor systems built with the same underlying model.</p>
<p>Guardrail.One mitigation strategy is to reserve a specific LLM (or family of LLMs) exclusively for evaluation purposes, ensuring it is not used in any system under test.However, due to overlapping training corpora across models, this bias may still persist.A more robust alternative is to involve multiple LLMs in the evaluation and aggregate relevance judgments using majority voting -while omitting the vote of any evaluator that shares lineage with the system under consideration.[11,48].In contrast, human assessors are better equipped to recognize nuance, novelty, and contextual diversity, which LLMs frequently overlook [61].</p>
<p>More fundamentally, when LLMs define what is relevant across the board, they implicitly set a ceiling for what systems can achieve.This can penalize systems that offer innovative or non-standard responses that fall outside the LLM's implicit norms [64].</p>
<p>Quantify effect.This trope's impact can only be assessed through independent evaluations involving human judges from diverse socio-cultural backgrounds.</p>
<p>Guardrail.While human annotation workflows can be designed to ensure a variety of perspectives, achieving this with LLMs is far more difficult.Persona-based prompting strategies [69] have been proposed as a mitigation, but emerging evidence highlights their limitations [33].We recommend rigorous quantification of this effect before relying on such methods in high-stakes evaluation.</p>
<p>Meta-Evaluation Tropes</p>
<p>The quality of different LLM-judge approaches is often validated through meta-evaluation -a paradigm that measures how well LLM judgments reproduce either manually created relevance labels or leaderboard rankings under an official evaluation metric [22,84].However, such meta-evaluations can foster a false sense of reliability or progress, masking deeper issues in metrics, methodology, or evaluator behavior.</p>
<p>Meta-Eval Trope #5: Ignored Label Correlation</p>
<p>-When human and LLM judges disagree on relevance labels.-Meta-evaluations of LLM-based judges often rely on measuring the correlation between system rankings (or document rankings for individual queries) based on an evaluation metric (e.g., using NDCG or DCG) using manual and LLM-based labels [47,72].However, this high-level agreement can mask important differences at the level of individual judgments.</p>
<p>For example, in the context of conversational systems, Mehri and Eskenazi [44] demonstrate that even when system-level Spearman correlation is perfect (i.e.,  = 1), agreement on individual relevance labels can vary widely -from as low as 0.12 to 0.61 -depending on the underlying metric.This highlights the risk of relying solely on system-level comparisons.</p>
<p>To establish that LLM-generated judgments are robust and futureproof -and do not artificially constrain the performance of emerging systems -agreement should be assessed directly at the label level, i.e., for each query-document pair.</p>
<p>Quantify effect.Measure correlation between human and LLMgenerated relevance labels directly, to assess the reliability of LLM judgments at the label level.</p>
<p>Guardrail.Incorporating label-level agreement analysis alongside system-level metrics ensures that inconsistencies or biases in LLM evaluations are not overlooked, providing a more complete view of evaluator reliability.</p>
<p>Meta-Eval Trope #6: Old Systems</p>
<p>-Evaluators need to identify the best systems of the future.-The primary goal of evaluation is to identify the next generation of state-of-the-art systems.Accordingly, meta-evaluations of LLM-based judges aim to show that these evaluators can correctly identify the best-performing systems.However, this claim is often tested on legacy systems -those that were state-of-the-art at the time the test collection was created.</p>
<p>As new IR paradigms emerge, they are rarely reflected in existing test collections.As a result, a meta-evaluation on a dataset can only confirm whether the LLM evaluator recognizes high-performing systems that era.</p>
<p>Yet such studies are frequently used to argue that LLM evaluators will also be effective for future systems.This assumption remains untested for future systems, which are expected to differ significantly.Such systems are likely to employ LLMs more extensively, integrate higher-quality models, or adopt innovations that differ significantly from past approaches.There is a real danger that LLM evaluators -especially those themselves based on outdated LLMsmay fail to recognize these future breakthroughs.</p>
<p>Quantify effect.A simple change in community practice is to collect implementations of the recent IR systems and release an expanded judgment pools (e.g. as suggested in Section 5).By repeating meta-evaluations on these new systems using test collection artifacts, one can assess whether the LLM evaluator still identifies best performing systems.</p>
<p>Guardrail.Older TREC collections remain relevant, because of available manual runs [74].In addition, the community should regularly collect recent system implementations, expand the relevance pool, and re-run meta-evaluations to detect and mitigate the effects of this trope.</p>
<p>Meta-Eval Trope #7: LLM Evolution</p>
<p>-LLMs are not static; they can improve or degrade over time.-Meta-evaluations of LLM-based judges often rely on a single prompt or a single LLM family, despite the wide variety of models available.Crucially, LLMs are not static -model behavior evolves over time as new versions are released [13].Future iterations of an LLM may judge relevance differently than earlier ones, introducing inconsistencies in longitudinal evaluations.This drift becomes especially problematic when newer models are trained on data that includes outputs from earlier versions, potentially leading to feedback loops and self-training collapse [60].</p>
<p>These issues are compounded by the fact that LLM providers may seamlessly retire older versions or update models without notice. 6This makes it difficult -or even impossible -to reproduce prior evaluation findings using the same version of the evaluator.</p>
<p>Quantify effect.To track the impact of model evolution, metaevaluations should be periodically repeated using updated LLM versions.Key indicators of behavioral drift include changes in the ranking of top systems, inconsistencies in relevance labels compared to human judgments, and increased variability in the labeling of previously unjudged documents [1,5].</p>
<p>Guardrail.</p>
<p>Because access to specific model versions cannot be guaranteed over time, LLM-based evaluation methods must be continually re-validated.In Section 5, we recommend that the community adopt a recurring meta-evaluation protocol to ensure the ongoing reliability and relevance of LLM-based evaluators.</p>
<p>System Tropes</p>
<p>Next, we examine tropes that degrade IR system performance as a result of reliance on artifacts such as LLM-generated relevance labels.While synthetic data and automated evaluators can improve scalability, their improper use can introduce systemic biases and encourage overfitting to unreliable or unstable evaluation signals.</p>
<p>System Trope #8: Test Set Leak</p>
<p>-LLMs trained on test collections create the illusion of quality.-Some LLMs are trained on publicly available test collections used in IR evaluation.This contaminates evaluation outcomes by inflating the performance of systems that incorporate such LLMs, creating the illusion of high accuracy that fails to generalize to real-world scenarios [83].</p>
<p>There are, however, legitimate reasons to train an LLM on relevance labels, for example, when developing an LLM evaluator specifically designed to support assessment.Prior work, such as the Autotar evaluation framework [18,19], demonstrates that targeted training can yield valid and scalable evaluation systems.</p>
<p>Nevertheless, if such test collections are also used in meta-evaluation, training-induced memorization can create a misleading appearance of alignment between LLM and human judgments-undermining the credibility of the evaluation approach [20,77].</p>
<p>Quantify effect.After collecting fresh manual relevance labels on new topics for the task, a drop in performance on fresh topics would signal potential overfitting or memorization.The protocol of Bordt et al. [12], developed in the context of table learning, provides a useful template for quantifying this effect.The increasing use of LLM-generated content as training data for other LLMs raises serious concerns about concept drift and long-term quality degradation [79].Rather than fostering diversity or nuance, repeated training on synthetic outputs may entrench biases and amplify systematic errors [21,27,34,59].</p>
<p>In the context of IR, this phenomenon arises when LLM-based evaluators are used to generate synthetic training data for IR systems.The problem compounds when the outputs of these synthetically trained systems are then used to fine-tune the next generation of LLM evaluators, forming a recursive feedback loop.This recursive co-training process can amplify subtle biases and lead to concept drift -and, ultimately, model collapse [60].This is a concrete manifestation of unintended circularity during system development, wherein models achieve high training or evaluation scores but fail to generalize in real-world scenarios.</p>
<p>Quantify effect.This effect can be quantified [59] by tracking evaluation performance on a fixed set of held-out, human-labeled data across multiple rounds of recursive training.A consistent decline in agreement with manual judgments would signal the onset of model collapse.</p>
<p>Guardrail. One should adopt guardrails from Reinforcement</p>
<p>Learning from AI Feedback [21] and generative AI [59] to detect when systems are degrading.</p>
<p>To avoid inadvertently exercising this trope, training data should be released with proper documentation of how training data was obtained, and to which extent LLMs were used in the generation.</p>
<p>System Trope #10: Goodhart's Overfitting</p>
<p>-IR systems that are "trained for the test".-When a metric becomes a target, it ceases to be a reliable measure of success [28].In IR, this effect arises when systems overfit to artifacts of a specific LLM-based evaluator, rather than optimizing for genuine user satisfaction.Industry experience [67,82] has shown that systems tuned solely for a single metric (such as high NDCG) may underperform on user-centered outcomes, including click-through rate, dwell time, and robustness to spam.</p>
<p>We anticipate similar issues for systems optimized against a single LLM evaluator, e.g., Umbrela, which may achieve high scores under that metric while degrading on alternative evaluation measures or in real-world effectiveness.</p>
<p>Quantify effect.Siska et al. [63] proposes a protocol for quantifying overfitting by using a range of evaluation metrics.</p>
<p>Guardrail.To mitigate overfitting in this trope, evaluations should include multiple complementary metrics, along with manual relevance labels.This approach helps detect when systems are narrowly optimizing for a specific LLM-derived signal while failing to generalize across other important evaluation dimensions [57,80,81].</p>
<p>System Trope #11: Adversarial Threats -Bad actors want to manipulate the systems and evaluation.-Adversarial behavior is an increasing concern [10,32] as LLMs become central to both retrieval and evaluation pipelines.These models are susceptible to manipulation, particularly via the LLM Narcisissm Trope, which can be exploited for search engine optimization (SEO).</p>
<p>Recent studies demonstrate that LLMs can be guided to rewrite content to improve evaluation scores [9,75].Such techniques can distort rankings, spread misinformation, or amplify propaganda.System developers, aware of evaluation setups, may optimize their outputs to align with known evaluator biases -effectively training for the test.This risk increases when evaluation models and prompts are publicly disclosed, enabling targeted reverse-engineering.</p>
<p>LLMs can also be deceived into labeling irrelevant documents as relevant using simple adversarial attacks [3,50,58].These vulnerabilities threaten the integrity of evaluation pipelines and call into question the trustworthiness and reliability of LLM-based assessments.</p>
<p>Quantify effect.This effect can be measured by analyzing performance changes when outputs are explicitly optimized for a specific LLM evaluator, prompt, or configuration.Comparative studies help estimate to which extent evaluation scores are inflated by evaluation-aware tuning.</p>
<p>Guardrail.We advocate developing adversarial test inputs, e.g., targeted content rewrites, to assess the resilience of evaluation metrics under manipulation (cf.Section 5).</p>
<p>To reduce vulnerability, evaluation campaigns (e.g., TREC) should avoid exposing evaluator identities and prompt designs.Blind evaluation setups, where system developers are unaware of the specific LLM and prompt, can reduce gaming.Rotating or ensembling multiple evaluators adds further robustness.Where feasible, human judgments should remain part of the evaluation loop to validate and audit automated assessments.</p>
<p>Judge Tropes</p>
<p>A common solution to many evaluation and system tropes is to incorporate human judges into the evaluation process.Rather than relying solely on pristine manual judgments, many current approaches involve a collaboration between human assessors and LLMs to generate relevance labels.However, this hybrid setup introduces new risks: subtle forms of bias that can arise during human verification.We refer to these as "judge tropes".</p>
<p>While human involvement is often viewed as the gold standard, misalignment between task design, instructions, or expectations can inadvertently render human judgments ineffective -or even misleading.These issues can compromise the integrity of relevance assessments and, in severe cases, invalidate experimental findings.</p>
<p>Biases may stem from overreliance on LLM outputs, cognitive fatigue, or inadequate oversight -emphasizing the need for robust guardrails and diverse, well-calibrated evaluation protocols.</p>
<p>Judge Trope #12: Rubber-Stamp Effect -Lack of critical oversight when humans blindly trust LLM labels.-Experimental studies show that when human assessors are shown LLM-generated answers before making their own judgments, they are significantly more likely to conform to the model's assessmenteven when it is demonstrably incorrect [7,24,31,66].In psychology this is known as Ash conformity experiments [6,29].Moreover, as assessor fatigue and task repetition set in, human verification of LLM-generated labels often turns into passive agreement, driven more by trust than by critical scrutiny.</p>
<p>This creates a feedback loop: despite involving human judges, evaluations increasingly mirror LLM outputs -even when those outputs diverge from human intuition or real-world utility.</p>
<p>Quantify effect.This effect can be measured by comparing outcomes under fully manual relevance labels versus human-verified LLM labels.Divergence in label quality or ranking decisions will help quantify the degree of automation bias introduced.</p>
<p>Guardrail.To counteract this effect, we draw inspiration from vigilance protocols in security contexts [16].We propose embedding vigilance tests in the annotation workflow-rewarding annotators for identifying errors in LLM outputs.Randomly flipped or adversarial labels can be inserted to test whether annotators are critically engaged.If assessors fail to flag introduced errors, this signals a breakdown in oversight and provides a measurable indicator of rubber-stamping behavior.</p>
<p>Judge Trope #13: Black-box Labeling -When relevance is complex, labels may be difficult to interpret.-Relevance labels are often used to represent complex judgments in a simplified form.Whether assigned by humans or LLMs, it can be difficult to determine why a particular passage received a given label -especially when the decision is based on multiple, opaque criteria.This challenge is exacerbated when LLMs provide relevance judgments without clear or trustworthy rationales, increasing the risk of uncritical acceptance by human verifiers [30,35].</p>
<p>Lack of transparency in LLM-generated relevance labels is a concern.Although LLM evaluators can generate explanations alongside labels, these rationales may themselves be flawed and must be critically assessed [60] while avoiding the Rubber-Stamp trope.</p>
<p>Quantify effect.Variability between independent manual relevance labels and human-verified LLM labels can reveal the extent of black-box behavior.</p>
<p>Guardrail.To mitigate this issue, complex labeling tasks should be broken into smaller steps, each with explicit reasoning guidelines [23,43].Both LLMs and human judges should articulate their reasoning at multiple stages, which makes decisions more interpretable and auditable.Stepwise reasoning, inspired by chain-ofthought prompting in GPT models, can increase transparency and robustness in evaluation.</p>
<p>Judge Trope #14: Predictable Secrets</p>
<p>-When human data can be guessed by an LLM.-Many evaluation paradigms incorporate secrets, information known only to human judges and withheld from the system, to prevent evaluation leakage.These include human-generated relevance labels, grading rubrics [23], or nugget annotations [36].Such mechanisms are designed to guard against the negative effects of LLM-based evaluation.</p>
<p>However, these guardrails become ineffective when an LLM can reliably infer the secret.This introduces inadvertent circularity and undermines the purpose of human oversight [20].Predictable secrets typically signal that test points are too simplistic or follow an obvious pattern.Evaluation labels generated or structured by LLMs may exhibit consistent patterns that make them predictable and leakable.This allows systems to infer and exploit the evaluation signal, even in good-faith settings [34,64].</p>
<p>If an IR system can use an LLM to anticipate the secret and incorporate it into its output, it may achieve inflated scores, despite the apparent use of human judgment in the evaluation pipeline.</p>
<p>Quantify effect.The guessability of a secret can be measured by having an LLM predict secrets directly or by computing its pertoken likelihood.Downstream effects can be evaluated by replacing the manually created secret with the predicted secret and observing its impact on system rankings.</p>
<p>Guardrails.When validity of the evaluation relies on secret information known only to human judges, it is essential to ensure that the secret is complex and varied enough to resist LLM inference.Designing tasks where secrets require true contextual understanding or subjective reasoning can help maintain this barrier.</p>
<p>Case Studies</p>
<p>To demonstrate that our listed LLM-Evaluation tropes are in fact real issues, we explore two case studies of LLM evaluation methods used in industry and which guardrails were implemented to combat the risks. 7</p>
<p>Canva</p>
<p>In this case study (detailed in Anonymous [4]), the LLM-evaluator is used in a known-item or re-finding task.It is particularly valuable in a private or enterprise search environment, in which queries and documents are not readily available, let alone viewable by system developers due to privacy restrictions.Rather than providing relevance labels over a corpus of items, instead the LLM is used to synthesize a known-item according to some desired properties.Any number of additional items are also generated, both ones that are "distant" from the target item and ones that are "near" to the target item.It generates one or more queries that represent a user trying to re-find the target item.The characteristics of items and queries are derived from anonymized aggregated statistics over the real user data, thereby grounding the LLM-evaluator, avoiding Circularity trope.In this way, we generated a conventional test collection, but with the properties that the relevance judgments are known at inception, rather than requiring subsequent human annotation.</p>
<p>Conference acronym 'XX, Sometime, Somewhere Dietz, Zendel, Bailey, Clarke, Cotterill, Dalton, Hasibi, Sanderson, and Craswell</p>
<p>The goals of this setup are three-fold:</p>
<p>• Eliminate privacy challenges allowing conventional eyes-on analysis and debugging of search systems.• Directly construct retrieval and ranking challenges that match specific areas for product improvement e.g., spell correction.• Ensure repeatability of evaluation, through archiving of generated test collections.Changes to the search system are efficiently and deterministically evaluated offline, accelerating rejection of bad improvements before testing with people.</p>
<p>Although this offline evaluation was then succeeded by online interleaving and A/B experiments, we demonstrated that, provided the improvements we observe in the entirely synthetic LLM evaluation framework are directionally aligned to these later-stage human-centered evaluations, we have no need to also involve humans in the first stage.Both the known-item task (exactly one right answer) and involvement of humans at later-stage evaluations derisk LLM Narcissism and Circularity; we had no LLM involved in the search system either.</p>
<p>Valence</p>
<p>This case study examines AI-assisted enterprise coaching, where sessions address complex workplace challenges.The system integrates multiple large commercial LLMs with specialized dialogue components for domain expertise, personalized memory, and user work profiles.</p>
<p>An LLM-based evaluator assesses dialogue quality at both conversational and turn levels, incorporating Client Satisfaction (CSAT) scores and proprietary coaching effectiveness measures.The evaluation methodology follows a rubric-based framework, similar to Lin et al. [37] but manually adapted to coaching tasks by subject matter experts.The evaluation framework serves key purposes including:</p>
<p>• Privacy-Preserving Evaluation: Assesses dialogue quality without exposing sensitive conversations to human reviewers.• LLM-Based User Simulation: Tests alternative prompts, system configurations, and model components through synthetic interactions.• LLM as an Autonomous Judge: Enables offline optimization of coaching effectiveness across key conversational dimensions.</p>
<p>A key challenge is LLM Narcissism, leading to inflated effectiveness estimates compared to human assessments.To mitigate this, we use separate LLMs for generation and scoring, calibrated against human-labeled datasets.</p>
<p>As our system evolves, enhancing LLM-based evaluation is critical for scalability, privacy, and expert-level assessment quality.Two emerging challenges stand out: 1) LLM as a Reward Model for RL Optimization, while promising, risks reward hacking, where the model optimizes for evaluation heuristics rather than genuine coaching effectiveness.2) There is a shift to real-time evaluation where evaluation moves from offline to online assessment at the turn-level to enable active intervention when dialogue quality drops, but could further amplify Circularity biases potentially creating negative feedback loops such as Self-training Collapse and Overfitting.Original System (tau@60:0.84)</p>
<p>Figure 3: Reproduction of Upadhyay et al. [72]: On top 60 original TREC RAG 24 systems and data, the Umbrela LLM evaluator correlates highly with manual assessors.Only few submitted retrieval systems included approaches from LLM evaluators.Each system represents one dot.Red dots mark systems known to contain LLM evaluators [15].</p>
<p>Demonstration of Circularity</p>
<p>We simulate the effects of circularity on data from the TREC Retrievalaugmented Generation track (TREC RAG 2024) [71].Given the recency systems used in this dataset, we reduce the risk of the Old Systems trope.Omitting very low performing systems, we consider the top 60 submitted results of TREC RAG 24 retrieval systems, called "original" systems in the following.We simulate the LLM Eval as a Ranker trope by re-ranking all original systems with the Umbrela system, which we refer to as re-ranked systems for short.Specifically each system run is re-ranked using the official qrels obtained via Umbrela, with the Umbrela grade as the primary key and the original rank score as the secondary key.By TREC standards, the result would qualify as an automatic run.Our findings corroborate the discussion of Clarke and Dietz [15] and show the effects of Circularity.</p>
<p>LLM-evaluators are good re-rankers.We confirm that it is objectively beneficial for a system to incorporate approaches from LLM-evaluators as suggested in prior work [41,42].We simulate this by reranking the outputs of all retrieval systems with Umbrela.Comparing original and re-ranked systems on manual judgments in Figure 2, we see that re-ranking with Umbrela indeed has a positive impact on the system performance.The (valid) conclusion is that any system developer should be encouraged to adopt this approach.We expect more IR systems to adopt this approach in the near-future.</p>
<p>LLM-evaluators are valid on systems that don't use LLM evaluators.On the original systems, track organizers [71] demonstrate with a Kendall's tau leaderboard correlation test that using Umbrela as LLM-evaluator leads to very similar results as when using human judges.Using the top 60 systems in Figure 3, we confirm that this results in a relatively high Kendall's tau measure of 0.84, which relates to about 8% of discordant system pairs, i.e., a pair of systems where the two evaluators disagree on which system is better.Overall Re-ranked with Umbrela (tau@60: 0.63)</p>
<p>Figure 4: Demonstration of the effects of circularity when using Umbrela as both evluator and ranker using TREC RAG 24 data.Each submitted retrieval system is first re-ranked with Umbrela, then evaluated under NDCG with relevance labels from human judges and the Umbrela evaluator.We see that especially among top ranked systems, the evaluation strategy no longer agrees with human judges on which system is better.Showing same top 60 (of 75) systems as in Figure 3.</p>
<p>this confirms the findings of Upadhyay et al. [71].We note one a few outliers, such as one system in Figure 3 has a high LLM evaluation score (0.81) but a lower manual score (0.63).This systems includes an LLM evaluator as part of the system, further described in Clarke and Dietz [15].</p>
<p>Circularity arises when LLM evaluators are used both by the system (for re-ranking) and for evaluation.We would obtain an invalid evaluation paradigm if we were to use the Umbrela evaluation metric to evaluate these Umbrela re-ranked systems.This effect is demonstrated in Figure 4, where the Umbrela-reranked system runs are evaluated with NDCG under Umbrela relevance labels and manual relevance judgments -the latter measurement also being presented in Figure 2.</p>
<p>We observe an increased disagreement between both evaluation systems, resulting in a higher number of discordant pairs (18% within the top 60).This results in a much lower Kendall's tau metric of only 0.63.</p>
<p>This statistic further decreases to 0.44 when only the top 20 systems are compared.We note that for most research publications, it matters to demonstrate that all best systems are significantly outperformed by the proposed new system.Under this circular evaluation setup, such findings are not believable.</p>
<p>Additionally, we find that under the Umbrela evaluation, twelve systems now obtain an NDCG score above 0.95 which would suggest a near-perfect ranking quality that cannot be further improved.However, same systems only obtain manual NDCG scores between 0.68-0.72,demonstrating the inflation of evaluation scores due to circularity.</p>
<p>Taking manually created relevance labels as the gold standard, we conclude that using the Umbrela LLM evaluator on systems that internally (may) utilize an Umbrela re-ranking approach leads to an invalid experimental evaluation.</p>
<p>Suggested Experimentation Infrastructure</p>
<p>Comparing LLM-based and traditional evaluation metrics on the same IR systems is essential to quantify biases and inconsistencies.</p>
<p>We propose a TREC-style collaborative competition, a so-called "Coopetition",8 structured around a shared task with a predefined set of topics.Participants would submit contributions in three categories:</p>
<ol>
<li>IR Systems that attempt to solve the task using retrieval-based, generative, or mixed-modality approaches.2. LLM Evaluators that assess system outputs, either by ranking systems or generating relevance judgments.3. Content Modification Strategies to deliberately alter documents with the goal of testing system and evaluator robustness.</li>
</ol>
<p>The expected outcome is a test collection with human-verified relevance labels, along with one or more LLM-based evaluators that demonstrate strong performance.The inclusion of modified content introduces an adversarial component, stress-testing both retrieval systems and evaluators.This setup encourages the development of more robust methods while helping to identify potential risks and vulnerabilities in a controlled environment.Ideally, the top-performing LLM evaluator(s) should have an open-source implementation, ensuring transparency and enabling their use for addressing future gaps in judgment coverage.Beyond identifying the best-performing systems, this initiative should serve as an ongoing benchmarking effort, similar to leaderboard-style evaluations.Rather than emphasizing marginal performance gains, the goal is to build a diverse set of baseline systems and evaluators for meaningful comparisons.</p>
<p>We envision the Coopetition as an annual effort, introducing new tasks, topics, modified content, and fresh human relevance judgments in each iteration.If a superior LLM evaluator emerges, it will replace the previous best, updating the test collection accordingly.Between iterations, researchers would be encouraged to use the best available LLM evaluator for offline experiments, method development, and publications.A key aspect of this initiative is to obtain a resilient evaluation even when content is modified in adversarial ways.It will also help identify what makes an LLM-based evaluation system effective, where it fails, and how it can be improved; followed by defining standardized guard rails for validating experiments.</p>
<p>Conclusions</p>
<p>With this paper we aim to codify the best practices for ensuring that LLM-based evaluation remains a valid experimentation approach for IR research.Maintaining scientific rigor requires identifying and recognizing the risks associated with synthetic training data and LLM-based ranking, while ensuring they are cross-validated with human-verified benchmarks.Automatic evaluation methodologies must be adopted cautiously, treating them as validation tools rather than definitive measures of system performance.</p>
<p>To address these challenges, we propose a new form of TRECstyle Coopetition which annually identifies the best LLM-evaluation approaches measuring state-of-the-art IR systems on fresh test collections.This would ensure we are using (1) the best technology, and (2) continuously confirming LLM-evaluation validity with manual judgments.</p>
<p>Can I use LLM-judgments in my next conference paper?We conclude that LLM-based judgments can be used, but only under certain conditions that ensure evaluation validity:</p>
<p>• If the LLM-based evaluation metrics have been recently validated against real user judgments and are supported by diverse complementary metrics to prevent overfitting or bias.• If the evaluation setup ensures that LLM evaluators do not inadvertently influence system development, with potential risks such as test signal leakage and circularity being demonstrably mitigated.• If potential biases and evaluation tropes associated with the LLM evaluator are thoroughly discussed, quantified, and addressed through effective guardrails.Applying this framework ensures that LLM-based evaluations remain trustworthy, reproducible, and scientifically groundedreducing the danger of developing and deploying systems that are of limited utility for (human) users.</p>
<p>Figure 2 :
2
Figure 2: Reranking with an LLM evaluator (Umbrela) improves performance under human relevance labels.This plot compares the original and reranked versions of all TREC RAG 24 systems based on manual assessment.</p>
<p>Guardrail.</p>
<p>Avoid conducting IR research on test collections likely to have been included in LLM training data, as this risks measuring memorization rather than generalization.Regularly collect fresh human relevance judgments on new topics to track performance drift.Because most LLM training corpora are not disclosed, a trusted entity should maintain a hidden subset of evaluation topics to safeguard against future test set leakage.System Trope #9: Self-Training Collapse -Concept drift from training LLMs on LLM output.-</p>
<p>We hope our recommendations offer a balanced compromise that satisfies both authors and reviewers.
https://trec.nist.gov/
https://www.clef-initiative.eu/
https://fire.irsi.org.in
https://research.nii.ac.jp/ntcir/index-en.html
https://openai.com/index/gpt-4-api-general-availability/
Company names anonymized for review.
Coopetition refers to cooperative competition, where multiple research groups collaboratively compete to identify the most effective LLM-based evaluator, grounded in manual assessments, for use in academic research.</p>
<p>Can We Use Large Language Models to Fill Relevance Judgment Holes?. Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, Mohammad Aliannejadi, EMTCIR '24: The First Workshop on Evaluation Methodologies. Testbeds and Community for Information Access Research2024</p>
<p>Generative information retrieval evaluation. Marwah Alaofi, Negar Arabzadeh, L A Charles, Mark Clarke, Sanderson, Information Access in the Era of Generative AI. Springer2024</p>
<p>LLMs can be Fooled into Labelling a Document as Relevant: best café near me; this paper is perfectly relevant. Marwah Alaofi, Paul Thomas, Falk Scholer, Mark Sanderson, Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP 2024. the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP 20242024</p>
<p>Author Anonymous. 2024. How to improve search without looking at queries or results. anonymized/for/review. </p>
<p>Negar Arabzadeh, L A Charles, Clarke, arXiv:2504.12408A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment. 2025. 2025arXiv preprint</p>
<p>Effects of group pressure on the modification and distortion. Readings in social psychology. Solomon Asch, Holt, Rinehart and Winston1958. 1958New York</p>
<p>Authors, arXiv:2307.13601The Importance of Distrust in AI. 2023. 2023arXiv preprint</p>
<p>Krisztian Balog, Donald Metzler, Zhen Qin, arXiv:2503.19092Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation. 2025. 2025arXiv preprint</p>
<p>Niv Bardas, Tommy Mordo, Oren Kurland, Moshe Tennenholtz, Gal Zur, arXiv:2502.07315Prompt-Based Document Modifications in Ranking Competitions. 2025. 2025arXiv preprint</p>
<p>The Probability Ranking Principle is Not Optimal in Adversarial Retrieval Settings. Ran Ben Basat, Moshe Tennenholtz, Oren Kurland, Proceedings of the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15). the 2015 International Conference on The Theory of Information Retrieval (ICTIR '15)2015</p>
<p>Extrinsic Evaluation of Cultural Competence in Large Language Models. Shaily Bhatt, Fernando Diaz, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich Caruana, arXiv:2404.06209Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models. 2024. 2024arXiv preprint</p>
<p>How is ChatGPT's behavior changing over time?. Lingjiao Chen, Matei Zaharia, James Zou, arXiv:2307.090092023. 2023arXiv preprint</p>
<p>Assessing efficiency-effectiveness tradeoffs in multi-stage retrieval systems without using relevance judgments. L A Charles, Shane Clarke, Alistair Culpepper, Moffat, Information Retrieval Journal. 192016. 2016</p>
<p>L A Charles, Laura Clarke, Dietz, arXiv:2412.17156LLM-based relevance assessment still can't replace human relevance assessment. 2024. 2024arXiv preprint</p>
<p>The effects of event rate on a cognitive vigilance task. Daryn A Victoria L Claypoole, Kody L Dever, James L Denues, Szalma, Human factors. 612019. 2019</p>
<p>The Cranfield Tests on Index Language Devices. Cyril Cleverdon, Aslib Proceedings. 191967. 1967</p>
<p>V Gordon, Maura R Cormack, Grossman, arXiv:1504.06868Autonomy and reliability of continuous active learning for technology-assisted review. 2015. 2015arXiv preprint</p>
<p>Scalability of continuous active learning for reliable high-recall text classification. V Gordon, Maura R Cormack, Grossman, Proceedings of the 25th ACM international on conference on information and knowledge management. the 25th ACM international on conference on information and knowledge management2016</p>
<p>Investigating Data Contamination in Modern Benchmarks for Large Language Models. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Model Collapse Demystified: The Case of Regression. Elvis Dohmatob, Yunzhen Feng, Julia Kempe, arXiv:2402.077122024. 2024arXiv preprint</p>
<p>Perspectives on large language models for relevance judgment. Guglielmo Faggioli, Laura Dietz, L A Charles, Gianluca Clarke, Matthias Demartini, Claudia Hagen, Noriko Hauff, Evangelos Kando, Martin Kanoulas, Benno Potthast, Stein, Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval. the 2023 ACM SIGIR International Conference on Theory of Information Retrieval2023</p>
<p>Exam++: Llm-based answerability metrics for ir evaluation. Naghmeh Farzi, Laura Dietz, Proceedings of LLM4Eval: The First Workshop on Large Language Models for Evaluation in Information Retrieval. LLM4Eval: The First Workshop on Large Language Models for Evaluation in Information Retrieval2024</p>
<p>In search of verifiability: Explanations rarely enable complementary performance in AI-advised decision making. Raymond Fok, Daniel S Weld, AI Magazine. 452024. 2024</p>
<p>Bootstrapped nDCG Estimation in the Presence of Unjudged Documents. Maik Fröbe, Lukas Gienapp, Martin Potthast, Matthias Hagen, Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023. Dublin, Ireland2023. April 2-6, 2023Proceedings, Part I</p>
<p>Jingtong Gao, Bo Chen, Xiangyu Zhao, Weiwen Liu, Xiangyang Li, Yichao Wang, Zijian Zhang, Wanyu Wang, Yuyang Ye, Shanru Lin, arXiv:2406.12433Huifeng Guo, and Ruiming Tang. 2024. LLM-enhanced Reranking in Recommender Systems. 2024arXiv preprint</p>
<p>Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, Daniel A Roberts, Diyi Yang, David L Donoho, Sanmi Koyejo, arXiv:2404.01413Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data. 2024. 2024arXiv preprint</p>
<p>Problems of monetary management: the UK experience in papers in monetary economics. Charles Goodhart, Monetary Economics. 11975. 1975</p>
<p>Bennett Lewis D Griffin, Maximilian Kleinberg, Kimberly T Mozes, Maria Mai, Matthew Vau, Augustine Caldwell, Marvor-Parker, arXiv:2303.06074Susceptibility to influence of large language models. 2023. 2023arXiv preprint</p>
<p>Jiazhou Ji, Ruizhe Li, Shujun Li, Jie Guo, Weidong Qiu, Zheng Huang, Chiyu Chen, Xiaoyu Jiang, Xinru Lu, arXiv:2406.18259Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated. 2024. 2024arXiv preprint</p>
<p>Sebastian Krügel, Andreas Ostermaier, Matthias Uhl, arXiv:2106.16122Zombies in the Loop? Humans Trust Untrustworthy AI-Advisors for Ethical Decisions. 2021. 2021arXiv preprint</p>
<p>Competitive Search. Oren Kurland, Moshe Tennenholtz, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)2022</p>
<p>Ang Li, Haozhe Chen, Hongseok Namkoong, Tianyi Peng, arXiv:2503.16527LLM Generated Persona is a Promise with a Catch. 2025. 2025arXiv preprint</p>
<p>Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, Huan Liu, arXiv:2502.01534Preference Leakage: A Contamination Problem in LLM-as-a-judge. 2025. 2025arXiv preprint</p>
<p>Q , Vera Liao, Jennifer Wortman Vaughan, arXiv:2306.01941AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap. 2023. 2023arXiv preprint</p>
<p>Different structures for evaluating answers to complex questions: Pyramids won't topple, and neither will human assessors. Jimmy Lin, Dina Demner-Fushman, Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. the 45th Annual Meeting of the Association of Computational Linguistics2007</p>
<p>Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models. Ying-Chun Lin, Jennifer Neville, Jack Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, Saurabh Tiwary, Brent Hecht, Jaime Teevan, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics2024</p>
<p>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores. Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Can Deep Effectiveness Metrics Be Evaluated Using Shallow Judgment Pools. Xiaolu Lu, Alistair Moffat, Shane Culpepper, Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval2017</p>
<p>. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin, arXiv:2310.083192023. 2023Fine-Tuning LLaMA for Multi-Stage Text Retrieval. arXiv preprint</p>
<p>Zero-Shot Listwise Document Reranking with a Large Language Model. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin, arXiv:2305.021562023. 2023arXiv preprint</p>
<p>On the evaluation of machine-generated reports. James Mayfield, Eugene Yang, Dawn Lawrie, Sean Macavaney, Paul Mcnamee, Douglas W Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Kayi, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024. 1904-1915</p>
<p>USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation. Shikib Mehri, Maxine Eskenazi, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Pooled Evaluation Over Query Variations: Users Are as Diverse as Systems. Alistair Moffat, Falk Scholer, Paul Thomas, Peter Bailey, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. the 24th ACM International on Conference on Information and Knowledge Management2015</p>
<p>Rank-Biased Precision for Measurement of Retrieval Effectiveness. Alistair Moffat, Justin Zobel, ACM Trans. Inf. Syst. 272008. 2008</p>
<p>Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I. Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Vishakh Padmakumar, He He, arXiv:2309.05196Does Writing with Language Models Reduce Content Diversity?. 2023. 2023arXiv preprint</p>
<p>Llm evaluators recognize and favor their own generations. Arjun Panickssery, Samuel Bowman, Shi Feng, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models. Andrew Parry, Maik Fröbe, Sean Macavaney, Martin Potthast, Matthias Hagen, Advances in Information Retrieval. 46th European Conference on IR Research (ECIR 2024). Lecture Notes in Computer Science. Berlin Heidelberg New YorkSpringer2024</p>
<p>Synthetic Test Collections for Retrieval Evaluation. A Hossein, Nick Rahmani, Emine Craswell, Yilmaz, Mitra Bhaskar, Daniel Campos, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>TREC-COVID: rationale and structure of an information retrieval shared task for COVID-19. Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen Voorhees, Lucy Lu Wang, William R Hersh, Journal of the American Medical Informatics Association. 272020. 2020</p>
<p>Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley, arXiv:2310.10628Data Contamination Through the Lens of Time. 2023. 2023arXiv preprint</p>
<p>Preferences on a Budget: Prioritizing Document Pairs When Crowdsourcing Relevance Judgments. Kevin Roitero, Alessandro Checco, Stefano Mizzaro, Gianluca Demartini, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Oier Lopez de Lacalle, and Eneko Agirre. Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, arXiv:2310.18018NLP Evaluation in Trouble: On the Need to Measure LLM Data Contamination for Each Benchmark. 2023. 2023arXiv preprint</p>
<p>WWW3E8: 259,000 Relevance Labels for Studying the Effect of Document Presentation Order for Relevance Assessors. Tetsuya Sakai, Sijie Tao, Zhaohao Zeng, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval2021</p>
<p>Recommender Model Evaluation: Offline vs. Nic Scheltema, Online. Shaped Blog. 2024. 2024</p>
<p>Optimization-based prompt injection attack to llmas-a-judge. Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, Neil Zhenqiang, Gong , Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security. the 2024 on ACM SIGSAC Conference on Computer and Communications Security2024</p>
<p>Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson, arXiv:2305.17493The Curse of Recursion: Training on Generated Data Makes Models Forget. 2023. 2023arXiv preprint</p>
<p>AI models collapse when trained on recursively generated data. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, Yarin Gal, Nature. 6312024. 2024</p>
<p>Evaluating Large Language Model Biases in Persona-Steered Generation. Chenglei Si, Findings of the Association for Computational Linguistics: ACL 2024. 2024</p>
<p>K Aaditya, Muhammed Singh, Andrew Yusuf Kocyigit, David Poulton, Maria Esiobu, Gergely Lomeli, Szilvasy, arXiv:2411.03923Evaluation Data Contamination in LLMs: How Do We Measure It and (When) Does It Matter?. 2024. 2024arXiv preprint</p>
<p>Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks. Charlotte Siska, Katerina Marazopoulou, Melissa Ailem, James Bono, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Don't use LLMs to make relevance judgments. Ian Soboroff, Information retrieval research journal. 12025. 2025</p>
<p>Report on the need for and provision of an 'ideal' information retrieval test collection. Karen Spärck, Jones , C J Van Rijsbergen, Computer Laboratory. 1975. 1975</p>
<p>What large language models know and what people think they know. Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas W Mayer, Padhraic Smyth, Nature Machine Intelligence. 2025. 2025</p>
<p>What matters in a measure? A perspective from large-scale search evaluation. Paul Thomas, Gabriella Kazai, Nick Craswell, Seth Spielman, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Large Language Models can Accurately Predict Searcher Preferences. Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization. Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen, Chao-Wei Huang, Yu Meng, Yun-Nung Chen, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look. Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa , Trang Dang, Jimmy Lin, arXiv:2411.082752024. 2024arXiv preprint</p>
<p>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look. Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa , Trang Dang, Jimmy Lin, arXiv:2411.082752024. 2024arXiv preprint</p>
<p>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor. Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Nick Craswell, Jimmy Lin, arXiv:2406.065192024. 2024arXiv preprint</p>
<p>The Evolution of Cranfield. Ellen M Voorhees, In Information Retrieval Evaluation in a Changing World. Nicola Ferro and Carol Peters412019Springer International Publishing</p>
<p>Can Old TREC Collections Reliably Evaluate Modern Neural Retrieval Models?. Ellen M Voorhees, Ian Soboroff, Jimmy Lin, arXiv:2201.110862022. 2022arXiv preprint</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv:2212.03533Text Embeddings by Weakly-Supervised Contrastive Pre-training. 2024. 2024arXiv preprint</p>
<p>Rethinking generative large language model evaluation for semantic comprehension. Fangyun Wei, Xi Chen, Lin Luo, arXiv:2403.078722024. 2024arXiv preprint</p>
<p>Cheng Xu, Shuhao Guan, Derek Greene, Kechadi, arXiv:2406.04244Benchmark data contamination of large language models: A survey. 2024. 2024arXiv preprint</p>
<p>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Self-training Large Language Models through Knowledge Detection. Wei Jie Yeo, Teddy Ferdinan, Przemyslaw Kazienko, Ranjan Satapathy, Erik Cambria, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Towards a Better Understanding of Evaluation Metrics. Fan Zhang, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval2020</p>
<p>Constructing and Meta-Evaluating State-Aware Evaluation Metrics for Information Retrieval. Fan Zhang, Information Retrieval Journal. 2023. 2023</p>
<p>Models versus satisfaction: Towards a better understanding of evaluation metrics. Fan Zhang, Jiaxin Mao, Yiqun Liu, Xiaohui Xie, Weizhi Ma, Min Zhang, Shaoping Ma, Proceedings of the 43rd international acm sigir conference on research and development in information retrieval. the 43rd international acm sigir conference on research and development in information retrieval2020</p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.01964Don't Make Your LLM an Evaluation Benchmark Cheater. 2023. 2023arXiv preprint</p>
<p>Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, Shafiq Joty, arXiv:2504.15253Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators. 2025. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
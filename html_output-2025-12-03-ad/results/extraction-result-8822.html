<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8822 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8822</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8822</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270063290</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.16800v1.pdf" target="_blank">TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations</a></p>
                <p><strong>Paper Abstract:</strong> Text-Attributed Graphs (TAGs) enhance graph structures with natural language descriptions, enabling detailed representation of data and their relationships across a broad spectrum of real-world scenarios. Despite the potential for deeper insights, existing TAG representation learning primarily relies on supervised methods, necessitating extensive labeled data and limiting applicability across diverse contexts. This paper introduces a new self-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA), which overcomes these constraints by integrating TAGs' structural and semantic dimensions. TAGA constructs two complementary views: Text-of-Graph view, which organizes node texts into structured documents based on graph topology, and the Graph-of-Text view, which converts textual nodes and connections into graph data. By aligning representations from both views, TAGA captures joint textual and structural information. In addition, a novel structure-preserving random walk algorithm is proposed for efficient training on large-sized TAGs. Our framework demonstrates strong performance in zero-shot and few-shot scenarios across eight real-world datasets.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8822.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8822.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Text / HDL (TAGA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph2Text Encoding Module with Hierarchical Document Layout (HDL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text encoding introduced in this paper that converts a node's k-hop ego-graph into a hierarchical textual document mirroring the graph BFS-tree backbone and encoding cross-edges as intra-document references, designed to align with pretrained language model (PLM) corpora distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Hierarchical Document Layout (HDL) / Graph2Text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms a node's k-hop ego-graph into a multi-section hierarchical document by (1) extracting a BFS tree rooted at the target node, (2) assigning sections by a preorder traversal (root, then subtrees), and (3) inserting references at source sections to earlier sections for each cross-edge, producing a natural-looking document whose latent structure mirrors the original graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs) / general document-graph networks (e.g., citation, book, e-commerce networks)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Construct ego-graph G(v,k); compute BFS tree T(v,k) rooted at v; perform preorder traversal to order nodes and assign sections; for each cross-edge (u,w) where destination w precedes u in the traversal, add an in-document reference at section u pointing to section of w; concatenate node texts into hierarchical sections forming a document.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Unsupervised representation pre-training for node classification (zero-shot and few-shot), transfer learning across TAG datasets; embeddings are later used for similarity-based zero-shot classification and few-shot adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot node classification: TAGA (UAE-Large-V1 text encoder) per-dataset examples: Arxiv 0.537 ± 0.003, Children 0.224 ± 0.001, Computers 0.498 ± 0.004, Cora 0.682 ± 0.005, History 0.351 ± 0.009, Photo 0.419 ± 0.001, Pubmed 0.616 ± 0.009, Sports 0.448 ± 0.003 (accuracy). Reported aggregate statements: TAGA surpasses other graph pre-training methods by 47.84% on average and exceeds the second-best model by 6.78% on average in zero-shot; improves over direct PLM textual embeddings by 20.76% on average. Few-shot: average improvement 15.55% over comparisons and 6.28% over second-best; in <=5-shot scenarios average advantage 19.79% over other methods and 7.91% over second-best.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared (in experiments) against PLM-only text embeddings and unsupervised graph pretrain baselines (GraphMAE, GraphCL, GRACE) and G2P2. TAGA consistently outperforms these baselines in zero-shot and few-shot node classification on eight TAG datasets; the paper reports TAGA > second-best by ~6–7% on average and substantially > older graph-pretraining methods in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves original graph topology in a natural document layout that better matches PLM pretraining distributions (reduces distributional shift); encodes both hierarchical backbone and cross-edge connectivity; enables PLM to produce higher-quality text embeddings that capture structural relations; empirical gains in zero-shot and few-shot node classification; supports transfer across datasets; can be combined with GNNs via multi-view alignment to distill PLM knowledge into efficient GNN inference.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Direct PLM processing of fully constructed TofG documents is expensive due to transformer quadratic complexity in sequence length (TofG can grow quickly with neighborhood hops), causing high computation and memory costs at pre-training and inference if used naively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Full-document TofG generation and direct PLM encoding becomes infeasible (OOM) beyond modest hop depths (paper reports infeasible beyond ~3 hops in experiments). The method can still be limited when source and target domains are very different (e.g., social networks vs chemical networks) as noted in limitations. No explicit failure modes in prediction accuracy are reported beyond domain-transfer limitations and computational scaling issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8822.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8822.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAGA-rw</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAGA with Structure-Preserving Random Walk Acceleration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable approximation for Graph2Text encoding that samples multiple structure-preserving traversals of the hierarchical document to generate sub-corpora, emulating human reading patterns to approximate full-document information while drastically reducing PLM input length and training cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structure-Preserving Random Walk Traversal (sub-corpus sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Segments the hierarchical TofG document into multiple related sub-documents via randomized traversals: start at root, probabilistically walk down child links, with probability p jump along cross-edges, stopping at predefined length L; repeat multiple traversals and average representations to approximate the full-k-hop document embedding while preserving hierarchical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs), applied to ego-graph neighborhoods (k-hop)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use BFS tree and cross-edge set; for each traversal, start at root node, at each step either move to a randomly chosen child or with probability p jump to a cross-edge neighbor; collect the sequence of visited nodes to form a sub-document; perform multiple traversals and average embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Same as TAGA: unsupervised pre-training for node classification (zero-shot, few-shot), enabling efficient PLM-based neighborhood encoding and distillation into GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirically shown to achieve near-parity with full TAGA in prediction tasks while greatly reducing input size and training time; example transfer performance (zero-shot): Cora→Arxiv: TAGA 0.406 vs TAGA-rw 0.398 (accuracy). Efficiency results: random-walk method grows near-linearly in number of input words and training time with hops, while full TofG grows exponentially and becomes OOM beyond ~3 hops in experiments (Cora dataset). Exact wall-clock numbers are shown in Figure 3 (paper) but not tabulated numerically in text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to full Graph2Text HDL, TAGA-rw trades a small drop in predictive performance for much improved scalability and feasibility on deeper neighborhoods; compared to naive node/edge sampling, TAGA-rw preserves hierarchical and cross-edge structure and therefore better approximates full-document semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Near-linear growth in input size and training time with neighborhood hops; maintains structure-preserving properties (hierarchy + cross-edges); avoids OOM issues of full-document PLM encoding; empirically competitive predictive performance (often close to full TAGA).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Approximation can incur slight degradation in predictive accuracy compared to full HDL; introduces hyperparameters (jump probability p, path length L, number of traversals) that require tuning (though paper reports low sensitivity).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May not perfectly recover full-document information; small but measurable drop in some transfer tasks versus full TAGA; if p and L are chosen poorly the traversal may miss key cross-edges or sections, reducing embedding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8822.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8822.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-based Graph-to-Text Linearization (explicit node/edge listing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of prior methods that serialize graph neighborhoods into plain-language templates by explicitly listing nodes and their connection statements to create input text for LLMs (e.g., 'The first node is ... First node connects to third node ...').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Template-based serialization / explicit connection listing</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Manually designed natural-language templates that state node texts and explicit connection relations in sequence (e.g., enumerating nodes and then listing connection sentences), effectively forming a flat serialization of the local graph as plain text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs), local graph neighborhoods</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearize nodes into numbered statements describing each node's text content, then append sentences describing edges (e.g., 'Node A connects to Node B') or local adjacency descriptions in plain text templates.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used as input to large language models for tasks such as prediction/classification or question answering on graph-structured data by prompting the LLM with the serialized graph text and a task question.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prior work (cited Fatemi et al. 2023 and others) report that these methods produce lower-quality embeddings and high variance in downstream performance (no consistent numerical advantage reported in this paper). The current paper reports that TAGA improves zero-shot performance over direct PLM/text-embedding baselines (which would include naive serialized text) by an average of 20.76%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper contrasts template-based linearization unfavorably with HDL/Graph2Text: template methods often do not present structure in natural-language-speaking manner and cause distributional shift away from PLM pretraining data; HDL produces more natural hierarchical documents and yields better PLM embeddings and downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement and directly compatible with LLM prompts; explicit and interpretable text describing nodes and edges.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>The produced text is unnatural relative to PLM pretraining corpora (distributional shift), often verbose or awkward; tends to not fully leverage PLM pretraining leading to lower-quality embeddings and higher variance in downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported to produce lower-quality embeddings and high variance (Fatemi et al. 2023); loses ability to exploit PLM priors because serialization is not similar to natural documents PLMs were trained on.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8822.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8822.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G2P2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G2P2 (Graph-to-Pretrained-PLM Alignment, Wen & Fang 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior pretraining approach that aligns GNN representations with text encoder outputs by averaging node text embeddings across neighborhood hops and applying contrastive alignment objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmenting low-resource text classification with graph-grounded pre-training and prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Neighborhood-averaged text-embedding aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents a node's neighborhood for alignment by averaging the PLM embeddings of individual node texts across various neighborhood hops (simple aggregation), and then uses contrastive objectives to align averaged text embeddings with GNN embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs), neighborhoods (multi-hop)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No graph-to-long-text document conversion; instead, compute PLM text embeddings for individual nodes and average these embeddings across nodes within neighborhood hops to obtain a neighborhood textual representation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Pre-training for node classification and representation learning; used in comparison baselines for zero-shot and few-shot node classification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper tables (zero-shot, UAE-Large-V1): example G2P2 zero-shot accuracies per dataset: Arxiv 0.453 ± 0.002, Children 0.201 ± 0.001, Computers 0.453 ± 0.001, Cora 0.644 ± 0.004, History 0.322 ± 0.003, Photo 0.452 ± 0.001, Pubmed 0.576 ± 0.006, Sports 0.436 ± 0.001. In transfer/ few-shot experiments G2P2 is generally competitive but inferior to TAGA on average.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared in experiments: G2P2 often performs better than older graph-only pretraining methods but worse than TAGA. Paper criticizes G2P2 for simplifying neighborhood textual representation by averaging, losing neighborhood internal interactions and structural nuances.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and computationally cheap (averaging embeddings); leverages PLM embeddings without long-sequence PLM encoding; performs reasonably well compared to some graph-only pretraining baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Averaging erases intra-neighborhood interaction patterns and the ordering/hierarchy of nodes, potentially losing structural signals that matter for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Over-simplifies neighborhood structure leading to degradation in alignment objectives and downstream tasks where neighborhood interaction patterns are important; paper reports it overlooks underlying interactions within neighborhoods, leading to information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Grenade: Graph-centric language model for self-supervised representation learning on text-attributed graphs <em>(Rating: 2)</em></li>
                <li>Augmenting low-resource text classification with graph-grounded pre-training and prompting <em>(Rating: 2)</em></li>
                <li>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8822",
    "paper_id": "paper-270063290",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Graph2Text / HDL (TAGA)",
            "name_full": "Graph2Text Encoding Module with Hierarchical Document Layout (HDL)",
            "brief_description": "A graph-to-text encoding introduced in this paper that converts a node's k-hop ego-graph into a hierarchical textual document mirroring the graph BFS-tree backbone and encoding cross-edges as intra-document references, designed to align with pretrained language model (PLM) corpora distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Hierarchical Document Layout (HDL) / Graph2Text",
            "representation_description": "Transforms a node's k-hop ego-graph into a multi-section hierarchical document by (1) extracting a BFS tree rooted at the target node, (2) assigning sections by a preorder traversal (root, then subtrees), and (3) inserting references at source sections to earlier sections for each cross-edge, producing a natural-looking document whose latent structure mirrors the original graph topology.",
            "graph_type": "Text-Attributed Graphs (TAGs) / general document-graph networks (e.g., citation, book, e-commerce networks)",
            "conversion_method": "Construct ego-graph G(v,k); compute BFS tree T(v,k) rooted at v; perform preorder traversal to order nodes and assign sections; for each cross-edge (u,w) where destination w precedes u in the traversal, add an in-document reference at section u pointing to section of w; concatenate node texts into hierarchical sections forming a document.",
            "downstream_task": "Unsupervised representation pre-training for node classification (zero-shot and few-shot), transfer learning across TAG datasets; embeddings are later used for similarity-based zero-shot classification and few-shot adaptation.",
            "performance_metrics": "Zero-shot node classification: TAGA (UAE-Large-V1 text encoder) per-dataset examples: Arxiv 0.537 ± 0.003, Children 0.224 ± 0.001, Computers 0.498 ± 0.004, Cora 0.682 ± 0.005, History 0.351 ± 0.009, Photo 0.419 ± 0.001, Pubmed 0.616 ± 0.009, Sports 0.448 ± 0.003 (accuracy). Reported aggregate statements: TAGA surpasses other graph pre-training methods by 47.84% on average and exceeds the second-best model by 6.78% on average in zero-shot; improves over direct PLM textual embeddings by 20.76% on average. Few-shot: average improvement 15.55% over comparisons and 6.28% over second-best; in &lt;=5-shot scenarios average advantage 19.79% over other methods and 7.91% over second-best.",
            "comparison_to_others": "Directly compared (in experiments) against PLM-only text embeddings and unsupervised graph pretrain baselines (GraphMAE, GraphCL, GRACE) and G2P2. TAGA consistently outperforms these baselines in zero-shot and few-shot node classification on eight TAG datasets; the paper reports TAGA &gt; second-best by ~6–7% on average and substantially &gt; older graph-pretraining methods in zero-shot.",
            "advantages": "Preserves original graph topology in a natural document layout that better matches PLM pretraining distributions (reduces distributional shift); encodes both hierarchical backbone and cross-edge connectivity; enables PLM to produce higher-quality text embeddings that capture structural relations; empirical gains in zero-shot and few-shot node classification; supports transfer across datasets; can be combined with GNNs via multi-view alignment to distill PLM knowledge into efficient GNN inference.",
            "disadvantages": "Direct PLM processing of fully constructed TofG documents is expensive due to transformer quadratic complexity in sequence length (TofG can grow quickly with neighborhood hops), causing high computation and memory costs at pre-training and inference if used naively.",
            "failure_cases": "Full-document TofG generation and direct PLM encoding becomes infeasible (OOM) beyond modest hop depths (paper reports infeasible beyond ~3 hops in experiments). The method can still be limited when source and target domains are very different (e.g., social networks vs chemical networks) as noted in limitations. No explicit failure modes in prediction accuracy are reported beyond domain-transfer limitations and computational scaling issues.",
            "uuid": "e8822.0",
            "source_info": {
                "paper_title": "TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "TAGA-rw",
            "name_full": "TAGA with Structure-Preserving Random Walk Acceleration",
            "brief_description": "A scalable approximation for Graph2Text encoding that samples multiple structure-preserving traversals of the hierarchical document to generate sub-corpora, emulating human reading patterns to approximate full-document information while drastically reducing PLM input length and training cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Structure-Preserving Random Walk Traversal (sub-corpus sampling)",
            "representation_description": "Segments the hierarchical TofG document into multiple related sub-documents via randomized traversals: start at root, probabilistically walk down child links, with probability p jump along cross-edges, stopping at predefined length L; repeat multiple traversals and average representations to approximate the full-k-hop document embedding while preserving hierarchical structure.",
            "graph_type": "Text-Attributed Graphs (TAGs), applied to ego-graph neighborhoods (k-hop)",
            "conversion_method": "Use BFS tree and cross-edge set; for each traversal, start at root node, at each step either move to a randomly chosen child or with probability p jump to a cross-edge neighbor; collect the sequence of visited nodes to form a sub-document; perform multiple traversals and average embeddings.",
            "downstream_task": "Same as TAGA: unsupervised pre-training for node classification (zero-shot, few-shot), enabling efficient PLM-based neighborhood encoding and distillation into GNNs.",
            "performance_metrics": "Empirically shown to achieve near-parity with full TAGA in prediction tasks while greatly reducing input size and training time; example transfer performance (zero-shot): Cora→Arxiv: TAGA 0.406 vs TAGA-rw 0.398 (accuracy). Efficiency results: random-walk method grows near-linearly in number of input words and training time with hops, while full TofG grows exponentially and becomes OOM beyond ~3 hops in experiments (Cora dataset). Exact wall-clock numbers are shown in Figure 3 (paper) but not tabulated numerically in text.",
            "comparison_to_others": "Compared to full Graph2Text HDL, TAGA-rw trades a small drop in predictive performance for much improved scalability and feasibility on deeper neighborhoods; compared to naive node/edge sampling, TAGA-rw preserves hierarchical and cross-edge structure and therefore better approximates full-document semantics.",
            "advantages": "Near-linear growth in input size and training time with neighborhood hops; maintains structure-preserving properties (hierarchy + cross-edges); avoids OOM issues of full-document PLM encoding; empirically competitive predictive performance (often close to full TAGA).",
            "disadvantages": "Approximation can incur slight degradation in predictive accuracy compared to full HDL; introduces hyperparameters (jump probability p, path length L, number of traversals) that require tuning (though paper reports low sensitivity).",
            "failure_cases": "May not perfectly recover full-document information; small but measurable drop in some transfer tasks versus full TAGA; if p and L are chosen poorly the traversal may miss key cross-edges or sections, reducing embedding quality.",
            "uuid": "e8822.1",
            "source_info": {
                "paper_title": "TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Template-based linearization",
            "name_full": "Template-based Graph-to-Text Linearization (explicit node/edge listing)",
            "brief_description": "A family of prior methods that serialize graph neighborhoods into plain-language templates by explicitly listing nodes and their connection statements to create input text for LLMs (e.g., 'The first node is ... First node connects to third node ...').",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "mention",
            "representation_name": "Template-based serialization / explicit connection listing",
            "representation_description": "Manually designed natural-language templates that state node texts and explicit connection relations in sequence (e.g., enumerating nodes and then listing connection sentences), effectively forming a flat serialization of the local graph as plain text.",
            "graph_type": "Text-Attributed Graphs (TAGs), local graph neighborhoods",
            "conversion_method": "Linearize nodes into numbered statements describing each node's text content, then append sentences describing edges (e.g., 'Node A connects to Node B') or local adjacency descriptions in plain text templates.",
            "downstream_task": "Used as input to large language models for tasks such as prediction/classification or question answering on graph-structured data by prompting the LLM with the serialized graph text and a task question.",
            "performance_metrics": "Prior work (cited Fatemi et al. 2023 and others) report that these methods produce lower-quality embeddings and high variance in downstream performance (no consistent numerical advantage reported in this paper). The current paper reports that TAGA improves zero-shot performance over direct PLM/text-embedding baselines (which would include naive serialized text) by an average of 20.76%.",
            "comparison_to_others": "Paper contrasts template-based linearization unfavorably with HDL/Graph2Text: template methods often do not present structure in natural-language-speaking manner and cause distributional shift away from PLM pretraining data; HDL produces more natural hierarchical documents and yields better PLM embeddings and downstream performance.",
            "advantages": "Simple to implement and directly compatible with LLM prompts; explicit and interpretable text describing nodes and edges.",
            "disadvantages": "The produced text is unnatural relative to PLM pretraining corpora (distributional shift), often verbose or awkward; tends to not fully leverage PLM pretraining leading to lower-quality embeddings and higher variance in downstream tasks.",
            "failure_cases": "Reported to produce lower-quality embeddings and high variance (Fatemi et al. 2023); loses ability to exploit PLM priors because serialization is not similar to natural documents PLMs were trained on.",
            "uuid": "e8822.2",
            "source_info": {
                "paper_title": "TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "G2P2",
            "name_full": "G2P2 (Graph-to-Pretrained-PLM Alignment, Wen & Fang 2023)",
            "brief_description": "A prior pretraining approach that aligns GNN representations with text encoder outputs by averaging node text embeddings across neighborhood hops and applying contrastive alignment objectives.",
            "citation_title": "Augmenting low-resource text classification with graph-grounded pre-training and prompting",
            "mention_or_use": "mention",
            "representation_name": "Neighborhood-averaged text-embedding aggregation",
            "representation_description": "Represents a node's neighborhood for alignment by averaging the PLM embeddings of individual node texts across various neighborhood hops (simple aggregation), and then uses contrastive objectives to align averaged text embeddings with GNN embeddings.",
            "graph_type": "Text-Attributed Graphs (TAGs), neighborhoods (multi-hop)",
            "conversion_method": "No graph-to-long-text document conversion; instead, compute PLM text embeddings for individual nodes and average these embeddings across nodes within neighborhood hops to obtain a neighborhood textual representation.",
            "downstream_task": "Pre-training for node classification and representation learning; used in comparison baselines for zero-shot and few-shot node classification.",
            "performance_metrics": "Reported in paper tables (zero-shot, UAE-Large-V1): example G2P2 zero-shot accuracies per dataset: Arxiv 0.453 ± 0.002, Children 0.201 ± 0.001, Computers 0.453 ± 0.001, Cora 0.644 ± 0.004, History 0.322 ± 0.003, Photo 0.452 ± 0.001, Pubmed 0.576 ± 0.006, Sports 0.436 ± 0.001. In transfer/ few-shot experiments G2P2 is generally competitive but inferior to TAGA on average.",
            "comparison_to_others": "Compared in experiments: G2P2 often performs better than older graph-only pretraining methods but worse than TAGA. Paper criticizes G2P2 for simplifying neighborhood textual representation by averaging, losing neighborhood internal interactions and structural nuances.",
            "advantages": "Simple and computationally cheap (averaging embeddings); leverages PLM embeddings without long-sequence PLM encoding; performs reasonably well compared to some graph-only pretraining baselines.",
            "disadvantages": "Averaging erases intra-neighborhood interaction patterns and the ordering/hierarchy of nodes, potentially losing structural signals that matter for downstream tasks.",
            "failure_cases": "Over-simplifies neighborhood structure leading to degradation in alignment objectives and downstream tasks where neighborhood interaction patterns are important; paper reports it overlooks underlying interactions within neighborhoods, leading to information loss.",
            "uuid": "e8822.3",
            "source_info": {
                "paper_title": "TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Grenade: Graph-centric language model for self-supervised representation learning on text-attributed graphs",
            "rating": 2,
            "sanitized_title": "grenade_graphcentric_language_model_for_selfsupervised_representation_learning_on_textattributed_graphs"
        },
        {
            "paper_title": "Augmenting low-resource text classification with graph-grounded pre-training and prompting",
            "rating": 2,
            "sanitized_title": "augmenting_lowresource_text_classification_with_graphgrounded_pretraining_and_prompting"
        },
        {
            "paper_title": "Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning",
            "rating": 1,
            "sanitized_title": "harnessing_explanations_llmtolm_interpreter_for_enhanced_textattributed_graph_representation_learning"
        }
    ],
    "cost": 0.01652825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations</p>
<p>Zheng Zhang zheng.zhang@emory.edu 
Emory University
AtlantaGA</p>
<p>Yuntong Hu 
Emory University
AtlantaGA</p>
<p>Bo Pan 
Emory University
AtlantaGA</p>
<p>Chen Ling 
Emory University
AtlantaGA</p>
<p>Liang Zhao liang.zhao@emory.edu 
Emory University
AtlantaGA</p>
<p>TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations
331C63A1FBB7DA26FDAA9779069C03D7
Text-Attributed Graphs (TAGs) enhance graph structures with natural language descriptions, enabling detailed representation of data and their relationships across a broad spectrum of real-world scenarios.Despite the potential for deeper insights, existing TAG representation learning primarily relies on supervised methods, necessitating extensive labeled data and limiting applicability across diverse contexts.This paper introduces a new self-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA), which overcomes these constraints by integrating TAGs' structural and semantic dimensions.TAGA constructus two complementary views: Text-of-Graph view, which organizes node texts into structured documents based on graph topology, and the Graph-of-Text view, which converts textual nodes and connections into graph data.By aligning representations from both views, TAGA captures joint textual and structural information.In addition, a novel structure-preserving random walk algorithm is proposed for efficient training on large-sized TAGs.Our framework demonstrates strong performance in zero-shot and few-shot scenarios across eight real-world datasets.2 Related Works 2.1 Unsupervised Graph Pre-Train Methods Existing unsupervised graph pre-training methods can be categorized into several categories based on their objectives and architectures.Graph autoencoder methods, graph autoencoder methods Kipf and Welling [2016], Hou et al. [2022] convert node and edge features into low-dimensional embeddings,</p>
<p>Introduction</p>
<p>Text-Attributed Graphs (TAGs) are text documents that are connected in graph structures, allowing for deeper analysis and interpretation of complex relationships Zhang et al. [2024], Jin et al. [2023b,a].TAGs are prevalently used in numerous real-world applications, such as social networks Paranyushkin [2019], Myers et al. [2014], citation networks Liu et al. [2013], and recommendation systems Wu et al. [2022], Huang et al. [2004].TAGs encompass textual content in both nodes and edges that elucidate the meaning of individual documents and who they are semantically correlated with.For instance, scientific article network is a type of TAGs that store the texts of research papers and how they are citing, criticizing, and summarizing each other in paragraphs.As exemplified in Figure 1(a), to elicit knowledge like "the first law proposed in Paper A is a special case of Paper B's Theorem 1 when it is under macro scale and low velocity" from scientific article network, it requires jointly considering semantics, topology, and their entanglement in the TAG.</p>
<p>Representation learning of TAGs is a promising, yet open research area that starts to attract fastincreasing attention Ye et al. [2023], Wang et al. [2024], Chen et al. [2024], Hu et al. [2023], Huang et al. [2023], Fatemi et al. [2023], Tang et al. [2023], Li et al. [2023], He et al. [2023].Existing works typically use Pre-trained Language Models (PLMs) to generate textual embeddings from node texts, which are then processed by Graph Neural Networks (GNNs) to produce embedding of the TAG.However, these methods predominantly rely on supervised learning paradigms, which require extensively labeled data that is often unavailable in real-world scenarios.Moreover, the reliance on supervised tasks means that models are usually optimized for specific tasks and domains reflected in the training dataset, which significantly constrains their applicability to new domains or broader tasks.This limitation undermines the unique advantage of TAGs to leverage their universal linguistic attributes effectively.Although there are some graph pre-training models Hou et al. [2022]  Text-of-Graph view organizes the text node and their connection description in a hierarchical layout document.These two views can be mutually transformed to each other.Veličković et al. [2018], You et al. [2020], Li et al. [2023] operate in an unsupervised manner, they often focus on either graph topology or node features independently, neglecting the crucial interplay between textual semantics and structural information inherent in TAGs.</p>
<p>Therefore, there is a pressing need for a method that comprehensively addresses the unique nature of TAGs, seamlessly integrating both their structural and semantic dimensions within a unified unsupervised framework.This presents a significant research challenge with several substantial hurdles to overcome.Primarily, developing a representation that can simultaneously leverage the textual semantic content, the graph structure, and their complex interplay presents significant difficulties.</p>
<p>The scarcity of labeled training data further exacerbates this issue, making traditional supervised approaches impractical and necessitating innovative unsupervised strategies.Furthermore, the computational demands of such representation learning are substantial.The integration of large PLMs for textual corpus processing to be considered in TAGs creates a significant computational burden.</p>
<p>In order to address the aforementioned challenges, this paper proposes a new self-supervised learning framework named Text-And-Graph Multi-View Alignment (TAGA).TAGA jointly preserves rich semantic information, topology information, and their interplay by aligning representations of TAGs from two complementary views: the Text-of-Graph view and the Graph-of-Text view.As illustrated in Figure 1, these two views offer different representation formats of a TAG yet contain equivalent information.Specifically, the Text-of-Graph view organizes node texts into a structured textual document according to the TAG's topology.As exemplified in Figure 1(b), structured textual documents are universal ways to represent the relations among different text pieces in large corpus, especially in books, long articles, web files, etc.Here we propose a novel Graph2Text encoding module to automatically transfer a TAG to a structured textual document, which is readily to be processed by language models.Conversely, the Graph-of-Text view transforms textual nodes and topology into graph-structured data, which is then processed by a graph representation learning module (e.g.graph neural network).By aligning the representations learned from these two views, we encourage the learned representation to capture both textual and structural information, resulting in a unified, comprehensive representation of the TAG.Furthermore, to accelerate the training process, we propose a novel structure-preserving random walk algorithm.Finally, we demonstrate the strength of our proposed representation learning framework through extensive experiments on eight real-world datasets in zero-shot and few-shot prediction scenarios.</p>
<p>which are then used to reconstruct the original graph data.Contrastive learning approaches, like DGI Veličković et al. [2018], GraphCL You et al. [2020], GRACE Zhu et al. [2020], and S 3 -CL Ding et al. [2023b], generate perturbed graph pairs by altering structural features, such as adding or removing nodes and edges or masking features, aiming to align the embeddings of these modified graphs closer in the embedding space.However, these methods often produce domain-specific embeddings with limited generalization ability across different domains, reducing their effectiveness in data-scarce or label-limited scenarios.</p>
<p>Recent developments have also seen efforts Wen and Fang [2023], Tang et al. [2023], Li et al. [2023] in aligning graph representations with textual representations.For instance, G2P2 Wen and Fang [2023] employs contrastive learning to align GNN representations with text encoder outputs by averaging individual node text embeddings across various neighborhood hops during its pre-training phase.However, these methods often simplify the treatment of textual encoder embeddings for neighborhoods by averaging the embeddings of individual nodes.Similarly, GRENADE Li et al. [2023] implements a dual-level alignment strategy.This approach not only aligns GNN and text encoder embeddings but also encourages embeddings of connected node pairs to exhibit similarity.This approach overlooks the underlying interactions within neighborhoods, leading to a loss of information that could be crucial for the contrastive objectives of alignment models.</p>
<p>Graph2Text Encoding Methods</p>
<p>Recently, research include approaches Ye et al. [2023], Wang et al. [2024], Chen et al. [2024], Hu et al. [2023], Huang et al. [2023], Fatemi et al. [2023] that first transform the text-attributed graph into text sequence and then directly utilize LLMs as the predictor given the transformed text and corresponding question as input prompt.These methods typically designs text templates to explicitly describe local graph structure by stating nodes and how they are connected in plain text.For example, "The first node is . . . .The second node is . . . . . . . .First node connects to third node.Second node connects to . . .".However, these methods do not present the structure in a natural language-speaking manner, which fails to fully leverage the pretrained capabilities of language models.This is due to the distributional shift between the transformed text from the graph and the original pretrained corpus, resulting in lower quality embeddings and high variance of performance Fatemi et al. [2023].</p>
<p>Efficient and Scalable Methods for Large-Size Graph Neighborhoods</p>
<p>Efficiency and scalability are crucial for deep graph learning, particularly when dealing with large graphs or high-order interactions.Traditional graph sampling techniques, such as node sampling Chen et al. [2018], edge sampling Hamilton et al. [2017], or subgraph sampling Zeng et al. [2019], aim to reduce neighborhood size.However, these methods may not be suitable for TAGs, as they can result in the loss of important hierarchical interactive connection during the random sampling process.Meanwhile, in the NLP domain, some efforts Peng et al. [2023], Han et al. [2023], Chen et al. [2023a], Jiang et al. [2023], Ding et al. [2023a] have been made to address the long context issue of PLMs.These approaches typically involve compressing input tokens into latent vectors Jiang et al. [2023] or modifying the attention mask Chen et al. [2023b], Han et al. [2023], Ding et al. [2023a] to reduce significant interactions.However, these methods often fail to preserve the original structure of the input corpus and might alter the hierarchical layout.</p>
<p>Preliminaries</p>
<p>In our study, a Text-Attributed Graph (TAG) can be represented as
G = (V, E, C), where V = {v 1 , v 2 , ..., v N } is a set of N nodes and E ⊆ V × V is the set of M edges. e ij ∈ E is an edge connecting nodes v i and v j ∈ V. C = {C 1 , C 2 , .
. ., C N } is the set of node textual features where each C i is the textual corpus associated with node v i ∈ V.</p>
<p>The main goal of this paper is to learn the representation f (G) of a TAG G = (V, E, C), which is an open research problem with several subsantial and unique challenges to be resolved.First, how the representation can jointly preserve the rich semantic information, graph information, and their interplay in TAG?Moreover, note that it is prohibitive to prepare label data so the unavailability of the training labels further troubles the representation learning.Second, the efficiency and scalability present a big challenge in representation learning of TAG because of the synergization of computational overhead of LLMs and the large corpus to be considered in the subgraph of TAG.</p>
<p>Methodology</p>
<p>To effectively and efficiently address the substantial challenges of unsupervised representation learning on Text-Attributed Graphs (TAGs), we propose a novel self-supervised learning framework called Text-And-Graph Multi-View Alignment (TAGA).Specifically, to jointly preserve both rich semantic information, topology information, and their interplay, we propose to learn and align the representations of TAG in two complementary views, namely text view and graph view.In particular, the text view is a Text-of-Graph, where the TAG's node texts are organized according to the TAG's topology into a textual document format, which inherently has the power to encompass logic and relational information.The graph view is a Graph-of-Text, where the TAG's nodes and topology are turned into a graph structured data.Then the text view can be transformed by pretrained language models, which are adept at preserving textual information, while the graph view can be transformed by graph neural network, which are designed to guarantee preserving graph information.Therefore, by aligning the representations learned from these two views, we encourage the graph view's representation to also capture textual information and the text view's representation to also capture graph information.The above new idea is shown in Figure 2, where Figure 2</p>
<p>Text-and-Graph Multi-View Alignment via TAG Hierarchical Self-Supervised Learning</p>
<p>Existing methods for learning representations in TAGs use Pre-trained Language Models (PLMs) to generate textual embeddings from node texts, which are then processed by GNNs for an aggregated TAG embedding.These methods typically require supervised labels for training, which are hard to obtain in real-world scenarios.Moreover, the resulting embeddings often lack generalization capabilities beyond their training data's specific domain and task.</p>
<p>To address the challenge of unsupervised representation learning in TAGs, TAGA first constructs two complementary views of a TAG: Text-of-Graph and Graph-of-Text, detailed in Section 4.1.1.These views contain equivalent information but in different formats, allowing them to mutually supervise each other.To leverage the strengths of both views, a hierarchical self-supervised learning module, described in Section 4.1.2,aligns the embeddings from both views, effectively capturing the rich semantic and structural information within TAGs.</p>
<p>Text-and-Graph Multi-View Construction</p>
<p>Our proposed framework TAGA first leverages two views of a TAG: Text-of-Graph (TofG) and Graph-of-Text (GofT).Each view can be defined at different neighborhood orders, allowing for a multi-order hierarchical representation that captures both the structural and semantic information within the TAG.Specifically, a k-order TofG view represents a node's k-hop neighborhood as a single textual corpus that encompasses all nodes and their connections within that neighborhood.This corpus is then processed by a PLM to extract semantic embeddings that capture the combined content and structure within that k-hop neighborhood.In contrast, the corresponding k-order GofT view is constructed as a graph structure, where nodes represent lower order TofGs within the k-hop neighborhood.A GNN model is then applied to aggregate information from these connected lower order TofGs, capturing the overall neighborhood context.This ensures that both TofG and GofT views at the same order encode equivalent information about the neighborhood.</p>
<p>To illustrate, consider a node with a 3-hop neighborhood, as shown in Figure 2(a).Its 3-order TofG is constructed by transforming the entire 3-hop neighborhood as a single text corpus.Three distinct 3-order GofT views can then be created using TofGs of orders 0, 1, and 2 as nodes in the graph structure.To maintain information consistency, the number of GNN aggregation layers decreases with increasing TofG order: 3 layers for 0-order TofGs, 2 for 1-order TofGs, and 1 for 2-order TofGs.This ensures that each 3-order GofT view captures the same 3-hop neighborhood information as the 3-order TofG view, facilitating information equivalent self-supervised learning.</p>
<p>Multi-View Alignment via TAG Hierarchical Self-Supervised Learning</p>
<p>Upon construction of both views at different orders, a hierarchical self-supervised learning module is proposed to align the embeddings from both views.Given a TAG G with at most K-hop neighborhood size, for each node v i ∈ V, its k-hop neighborhood can be denoted as N k (v i ) and its corresponding k-order TofG view embedding can be represented as:
h k (v i ) = PLM (TofG(v i ; k)) , TofG(v i ; k) = Graph2Text (v i ∪ N (v i , k)) ,(1)
where PLM is a pre-trained language model (e.g.BERT Devlin et al. [2018] or LlaMA Touvron et al. [2023]).Graph2Text is an encoding template function that can transform individual nodes and edges text into a textual corpus.Meanwhile, its corresponding k-order GofT views embeddings can be denoted as GNN aggregated representations of lower order TofGs:
b l k (v i ) = f (k−l) ({h l (v b )|v b ∈ v i ∪ N (v i , k − l)}) ,(2)
where l covers from 0 to k − 1 and f (k−l) denotes the GNN model with k − l layers.</p>
<p>By aggregating k − l layers of information over the connected l-order TofGs, the obtained k-order GofT embeddings cover equivalent information with the k-order TofG view embedding.Therefore, given all the embeddings from level 1 to K, the supervision objective function can be written as:
L positive = − 1 K|B| vi∈B k∈[1,K] l∈[0,k−1] ρ b l k (v i ), h k (v i ) ,(3)
where B represents the minibatch and ρ denotes a similarity function, such as cosine similarity.</p>
<p>Additionally, we include the negative samples that chosen from other nodes within the minibatch:
L negative = 1 K|B| vi,vj ∈B,v1̸ =v2 k∈[1,K] l∈[0,k−1] ρ b l k (v i ), h k (v j ) ,(4)
Thus, the overall objective function can be denoted as:
L = L positive + L negative (5)
Time Complexity Analysis.Consider a TAG with a maximum K-hop neighborhood size, where each node has an average degree d and text attribute length L. Assume the feature dimensionality is F .In the case of transformer-based PLMs, the time complexity for processing the TofG view of a node would be O((dL) 2 K 2 ), due to the quadratic complexity of self-attention mechanisms with respect to input sequence length.In contrast, our method employs a GNN to aggregate information from lower-order TofGs, each of length dL.Assuming a GNN with constant complexity per layer, the time complexity for aggregating information from all K levels of the GofT view would be O(L 2 dK).</p>
<p>Our method achieves significantly higher efficiency than directly using PLMs for TofG views, with details available in the Appendix B.</p>
<p>Algorithm 1 Hierarchical Document Layout (HDL) for Graph2Text The key to our proposed self-supervised learning framework is ensuring that the two distinct graph views (TofG and GofT) contain equivalent information.This necessitates constructing a TofG view through the Graph2Text module in Equation 1 that preserves all connectivity information present in the original TAG.Existing methods Fatemi et al. [2023], Huang et al. [2023], Wen and Fang [2023], Tang et al. [2023] often struggle to effectively represent the structural information of graphs in a way that is both comprehensive and natural to language model understanding.Some methods Tang et al. [2023], Wen and Fang [2023] omit crucial connectivity information between nodes, while others Fatemi et al. [2023], Huang et al. [2023] explicitly list all connections in a manner that is unnatural and difficult for language models to process.This discrepancy between the transformed graph text and the original pre-training corpus leads to a distributional shift, hindering the language model's ability to generate high-quality embeddings that accurately reflect both the semantic and structural aspects of the graph.
Input: Graph G, target node v, hop count k Output: Hierarchical text document D 1: Ĝ(v, k) ← Construct ego-graph of v up to k hops in G 2: T (v, k) ← BFS tree of Ĝ(v, k) rooted at v 3: Êcross (v, k) ← Cross-edges in Ĝ(v, k) 4: D ← Assign document
To address this issue, we introduce a novel Graph2Text approach that transforms a graph neighborhood into a hierarchical text document.This hierarchical structure mirrors the original graph's topology, ensuring that the document's latent structure is equivalent to the graph itself.Crucially, the resulting document resembles a natural document, aligning with the distribution of majority text data used to pre-train PLMs.This alignment mitigates the distributional shift issue, allowing PLMs to generate embeddings that accurately reflect both the semantic and structural aspects of the graph.Specifically, the structure of a node and its k-hop neighborhood can be represented as an ego graph, with the node itself as the root.This ego graph can be decomposed into a hierarchical tree backbone and a set of cross-edges, as illustrated in Figure 2(b).The reading order is established for the TofG document through a pre-order traversal of this tree structure (first visit the root, then the left subtree, then the right subtree), capturing the hierarchical relationships between nodes.To fully represent the neighborhood's structure, we then incorporate cross-edges into the document.These cross-edges indicate connections from later sections of the document back to earlier ones, effectively mirroring the original graph's topology within the text format.</p>
<p>As shown in Algorithm 1, the k-hop neighborhood of a target node v in graph G is represented as an ego-graph G(v, k).A breadth-first search (BFS) tree T (v, k), rooted at v, provides a hierarchical structure for the document, while cross-edges (edges outside the BFS tree) are identified.A preorder traversal of T (v, k) establishes the document's hierarchical layout, assigning each node a section number.Cross-edges are then integrated by adding references at source nodes to the sections containing their respective destination nodes, if the destination node appears earlier in the traversal.This approach ensures that the document faithfully reflects the graph's structure.</p>
<p>Accelerating Training on Large TAGs with Structure-Preserving Random Walk</p>
<p>While TAGA significantly improves efficiency during inference by transferring knowledge from the PLM to a GNN model, the pre-training stage still encounters computational bottlenecks due to the quadratic complexity of transformers with respect to context length when generating TofG view embeddings.Existing graph sampling methods (e.g., node or edge dropping) can partially alleviate this issue, but at the cost of sacrificing some valuable neighborhood structure information, which is crucial for capturing the intricate relationships within TAGs.</p>
<p>To address this issue while preserving the structure of corpus, we propose a novel approach inspired by human reading patterns.Our method segments the hierarchical corpus into multiple related subcorpora, mirroring how humans naturally engage with complex documents: starting with a general overview (top of the hierarchy) and delving into specific sections (sub-corpora).By navigating the corpus multiple times, focusing on different sub-corpora each time, the combined insights gained can effectively approximate the understanding achieved from processing the entire corpus.</p>
<p>To facilitate this behavior, we introduce a neighborhood traversal algorithm based on a random walk.This algorithm simulates a reader starting at the root node and progressing towards leaf nodes in the BFS tree, transitioning from general to specific information.Additionally, at each step, there is a probability p of jumping to another node via cross-edges, imitating the non-linear navigation often observed in human reading (e.g., jumping to related topics or backtracking).By averaging multiple random walk traversals, the generated paths can approximate the complete corpus.As detailed in Algorithm 2, each traversal begins at the root node v and iteratively samples child nodes to form a path down the hierarchy.At each step, a jump to another node via cross-edges is possible with probability p.This traversal continues until reaching a predefined length or a leaf.</p>
<p>Experiments</p>
<p>In this section, the experimental settings are introduced first in Section 5.1, then the zero-shot and few-shot performances are presented in Section 5.2.We further present the effectiveness under transfer learning settings in Section 5.3.In addition, we measure model efficiency in Section 5.5.We verify the effectiveness of framework components through ablation studies in Section 5.4.The parameter sensitivity experiments are present in Appendix A.2 due to space limit.</p>
<p>Experimental Settings</p>
<p>Datasets.We evaluate on eight real-world text-attributed graph datasets across different domains.Specifically, three citation networks (Cora Yang et al. [2016], Pubmed Yang et al. [2016] and Arxiv Hu et al. [2020]), two book networks (Children Shchur et al. [2018] and History Shchur et al. [2018]), and three E-commerce networks (Computers Shchur et al. [2018], Photo Shchur et al. [2018], and Sports Yan et al. [2023]) are chosen as our evaluation datasets.Datasets statistics can be found in Table 1.</p>
<p>Comparison Methods.We choose the textual embedding of the text corpus as the baseline, which is denoted as "PLM" in our experimental results tables.Additionally, we compare our proposed framework with four state-of-the-art unsupervised graph training methods that across different pretrain strategies.Specifically, GraphMAE Kipf and Welling [2016] -utilizes masked autoencoder technique to predict of graph structure and node features.GraphCL You et al. [2020] and GRACE Zhu et al. [2020] applies various graph augmentations to generate contrastive pairs.G2P2 Wen and Fang [2023] aligns GNN embeddings and text encoder embeddings through contrastive learning across various neighborhood hops.</p>
<p>Implementation Details.We choose two different pre-trained language models (OpenAI's text-embedding-3-small and UAE-Large-V1 Li and Li [2023]) to generate text embeddings for robust results.Commonly used GNN models (GCN Kipf and Welling [2017], GIN Hamilton et al. [2017], GraphSAGE Xu et al. [2018]) are chosen as the backbone model as the backbone model for both our method and all comparison methods.For a fair comparison, all models are required to adhere to the same GNN architecture, including the number of convolution layers and hidden dimensions.More details about hyperparameters can be found in Appendix A.1.For space considerations, further technical details regarding the implementation of zero-shot and few-shot learning can be found in Appendix B.</p>
<p>Effectiveness Results</p>
<p>In this section, we assess the effectiveness of our proposed unsupervised representation learning framework compared to other methods under conditions of label scarcity.Our representation learning models are initially pre-trained on each TAG dataset without any supervised labels.After the pre-training phase, we evaluate the quality of the obtained node embeddings under zero-shot conditions by measuring the similarity between these embeddings and the corresponding text label embeddings.To further gauge performance in scenarios with limited labeled data, we conduct evaluations using 1, 3, 5, 10, 20, 50, and 100-shot settings.Due to space limitation, the results with text encoder UAE-Large-V1 under zero-shot and 1, 3, 5, 10, and 100-shot settings is reported in Table 1.Our acceleration method with random walk is denoted as "TAGA-rw".The results with text-embedding-3-small and other few-shot settings can be found in Appendix A.3.</p>
<p>Zero-shot performance.Table 1 presents node classification accuracy under zero-shot conditions, where our method consistently outperforms all comparison methods in seven out of eight datasets.On average, our method surpasses other graph pre-training methods by 47.84% and exceeds the second-best model by 6.78%.These findings demonstrate the enhanced ability of our pre-trained model to effectively learn representations that enable zero-shot predictions.Furthermore, compared to direct textual embeddings from the PLM, our method improves zero-shot performance by an average of 20.76%.This demonstrates our method's capacity in integrating structural and textual information from neighborhoods over directly using the PLM.Interestingly, our method exhibits a stronger performance advantage when dealing with data rich in textual information.Specifically, for the two citation networks (Arxiv and Cora), which possess significantly longer text attributes compared to other datasets, our method surpasses the second-best performing graph pretrained model by an average of 10.33%.This proves our method can effectively leverage the rich textual information.</p>
<p>Few-shot performance.For few-shot experiments, our method consistently outperforms all comparison methods, achieving a 15.55% average improvement and surpassing the second-best model by 6.28% on average.Notably, our method exhibits a more pronounced advantage in scenarios with limited labeled data (&lt;=5 shots), where it outperforms all other methods by an average of 19.79% and exceeds the second-best model by 7.91% on average.This underscores the effectiveness of our method, particularly in settings where few-shot learning is essential due to data labels constraints.</p>
<p>Remarks.It is worth noting that for some datasets, the zero-shot performance of our method can match or even exceed few-shot predictive results, particularly when the number of training samples for few-shot learning is limited.For example, on five datasets (Arxiv, Children, Computers, Cora, and Pubmed), the zero-shot performance surpasses 1-shot performance by an average of 23.54%.</p>
<p>Remarkably, the zero-shot performance can even be comparable to that of 5-shot.This demonstrates the strong potential of our method in scenarios where labeled data is scarce or unreachable.In real-world applications, scenarios may arise where not only are labels difficult to obtain, but the data itself is also scarce.This necessitates the generalization of a pre-trained model to a data domain distinct from the pre-training data.Here we evaluate the zero-shot and few-shot performance under transfer learning settings.Specifically, the model is unsupervisedly pre-trained on the source data domain and then transferred to the target data domain.No further fine-tuning is performed for zero-shot prediction, and is fine-tuned using the limited training samples for few-shot prediction.</p>
<p>Transfer Ability Analysis</p>
<p>In Table 2, we present the performance of zero-shot and five-shot predictions across eight pairs of source and target datasets.The results demonstrate a clear advantage for our method in the zero-shot setting, where it consistently outperforms all other methods across all dataset pairs.Notably, our method achieves an average improvement of 26.5% over the second-best performing method.In the five-shot setting, our method continues outperforming the second-best performing method by 4.53% on average.Particularly when transferring from Cora to Arxiv and Pubmed, and Children to History, our method achieves significant performance gain by 6.30% on average, demonstrating its ability to effectively leverage limited labeled data in the target domain.Table 3: Ablation studies results of zero-and five-shot settings.Here "Full" denotes our full model.</p>
<p>Ablation Study</p>
<p>To investigate the effectiveness of our proposed model compared to simpler heuristics, we conducted a series of ablation analyses.We began by considering textual embeddings obtained directly by applying the PLM to the Text of Graph views' corpus at different orders.This allowed us to assess the impact of our training procedure compared to a simpler approach that relies solely on Text-of-Graph view representations.In addition, we compare our full model with a variant, Glo-GofT, which only aligns the GNN embeddings that aggregate individual node's text embeddings but removes all higher-order Graphof-Text embeddings.The results of these ablation studies are presented in Table 3, which reveals that removing components of our full model generally leads to a decrease in performance.In the zero-shot setting, the full model outperforms the variant models by 2.79% to 8.49% on average, and ranges from 1.74% to 9.71% in the five-shot setting.These results underscore the contribution of each component to TAGA's overall effectiveness.</p>
<p>Efficiency Analysis</p>
<p>To validate the efficiency and scalability of our proposed full method and random walk algorithm during both training and inference phases, we conduct experiments on the Cora dataset.We vary the number of hops from 0 to 5 and record the number of words in the input corpus, training time, and inference time.The results are presented in Figure 3.As depicted in top figure, the exponential growth in input size for the full method compared to the near-linear growth of the random walk method demonstrates the our's superior scalability in managing larger graph neighborhoods.The middle figure further demonstrates the efficiency advantage of the random walk algorithm, as its training time increases linearly with the number of hops, whereas the full method experiences a much steeper increase, becoming infeasible beyond 3 hops due to out-of-memory (OOM) errors.Finally, the bottom figure highlights the speedup achieved by our proposed method during inference compared to directly using a PLM.The inference time for our method remains linear growth trend across different hops, while the PLM-based approach suffers from rapidly increasing inference time with the hops number.</p>
<p>Conclusions</p>
<p>In this paper, we introduce TAGA, a novel self-supervised learning framework designed to address the challenges of unsupervised representation learning on TAGs.TAGA integrates both textual and structural information within TAGs by aligning representations from two complementary views: Text-of-Graph and Graph-of-Text.To enhance the preservation of structural information in the Text-of-Graph view, we propose a natural hierarchical document layout that mirrors the graph's topology.Additionally, we introduce a structure-preserving random walk algorithm to accelerate the training process on large TAGs.Extensive experiments on eight real-world datasets demonstrate TAGA's superior performance in zero-shot and few-shot learning scenarios, showcasing its strong generalization capabilities across diverse domains.can observe that our method TAGA consistently achieve the best performance on two different choices of text encoder models.This demonstrates the effectiveness and robustness of our proposed method.</p>
<p>B Additional Technical Details</p>
<p>Efficiency Comparison with Directly Using PLM Embeddings.It is worth noting that the textual embeddings of TofG views h(v i ) can directly represent the entire TAG.However, it may cause significant scalability and efficiency issue during the inference phase.Existing PLMs typically adopts transformer architecture and it has a quadratic complexity with the input number of text tokens, this is especially important to TAGs since the number of input size grows exponentially with the number of neighborhood hops.By aligning the knowledge from PLM with GNN model through our framework, we can simultaneously maintain generalization ability of TAG embeddings and high efficiency and scalability to large-sized graphs.</p>
<p>Enabling Zero-Shot and Few-Shot Predictions.Our pretrained strategy ensures that the embeddings obtained from the GNN models at each layer remain aligned within the textual embedding space.This alignment enables direct zero-shot predictions using the self-supervised trained embeddings without requiring any additional fine-tuning.Specifically, suppose there are L prediction labels {l 1 , l 2 , . . ., l L }. Their textual embeddings are obtained through the pretrained language model (PLM) as follows:</p>
<p>h (l) (l i ) = PLM(l i ) for i ∈ {1, . . ., L}</p>
<p>The probability that node v i belongs to class l j is computed in an unsupervised manner by measuring the cosine similarity (or another appropriate similarity measure) between the learned GNN embeddings h (g) (v i ) and the label textual embeddings h (l) (l j ):</p>
<p>p(v i → l j ) = e ρ(h (g) (vi),h (l) (lj ))</p>
<p>L k=1 e ρ(h (g) (vi),h (l) (l k ))</p>
<p>(7)</p>
<p>The final predicted class of node v i is determined as follows:
l(v i ) = argmax j p(v i → l j )(8)
where l(v i ) is the predicted class label for node v i , determined by selecting the class l that maximizes the similarity measure ρ between the GNN embedding of the node h (g) (v i ) and each of the label embeddings h (l) (l j ).</p>
<p>Additionally, to further refine the learned embeddings, we introduce a learnable transformation function for few-shot learning adaptation:
h (g) adapted (v i ) = g(h (g) (v i ), D support ) (9)
where g represents a transformation function with learnable parameters (e.g., a multi-layer perceptron), and D support denotes a set of support examples for few-shot learning.This adapted embedding h (g) adapted is then utilized to compute the updated predictive probabilities:</p>
<p>p(v i → l j ) = e ρ(h (g) adapted (vi),h (l) (lj ))</p>
<p>L k=1 e ρ(h (g) adapted (vi),h (l) (l k ))</p>
<p>(10)</p>
<p>C Limitations</p>
<p>This work aims to pioneer unsupervised representation learning in the text-attributed graph research domain.Our approach demonstrates significant performance improvements over existing state-of-theart methods in zero-shot and few-shot prediction tasks.However, we acknowledge certain limitations.While our work pushes the boundaries of graph foundation models, the model's transfer capabilities may be limited when training and inference domains are vastly different (e.g., from social networks to chemical networks).We consider the development of a universal graph foundation model, capable of generalizing across diverse domains, to be an important direction for future research.</p>
<p>Figure 1 :
1
Figure 1: Illustration of the two distinct views of TAGs: (left) Graph-of-Text and (right) Text-of-Graph.Graph-of-Text view constructs a graph-structured data over the individual text corpora, while Text-of-Graph view organizes the text node and their connection description in a hierarchical layout document.These two views can be mutually transformed to each other.</p>
<p>Ø</p>
<p>Galileo develops the theory of projectile trajectories, which discusses … Ø [1] Newton's three laws of motion … Ø [1.1]First law of motion describes … Ø [1.1.1]An example of first law is … Ø [1.2]As stated in 1.1.1,an object's motion remains constant without external force.Additionally, second law describes how force affects … Ø [2] Einstein found that Newton's laws in Section 1 are valid only for under conditions of macro scales and low relative velocities.For micro scales and high speed, … Ø [2.1]The development of relativity occurred in two stages … Ø [</p>
<p>Figure 2 :
2
Figure 2: Illustration of the proposed self-supervised learning framework.(a) Generation of different orders of Graph-of-Text views; (b) The Graph2Text module that transforms a Graph-of-Text view into a Graph-of-Text view; (c) The alignment module via hierarchical self-supervised learning.</p>
<p>(a) illustrates Graphof-Text view while Figure 2(b) illustrates Text-of-Graph view, and their respectively transformed embeddings are aligned by our new TAG-hierarchical self-supervised learning framework, as detailed in Section 4.1.The details of our Text-of-Graph view are elaborated in Section 4.2.Finally, the acceleration of our learning process is detailed in Section 4.3.</p>
<p>Figure 3 :
3
Figure 3: (top) Comparison of the full method and the random walk algorithm in terms of the number of words, and (middle) training time, and (bottom) inference time comparison between PLM and TAGA in terms of the number of hops.</p>
<p>,</p>
<p>Preprint.Under review.
Ø Galileo develops the theory of projectiletrajectories, which discusses …Ø [1] Newton's theory consists of …Ø [1.1] First law of motion is …Ø [1.1.1] Follows by first law,Only valid for low velocityhere we further … Ø [1.2] Second law of motionand macro scaleEquivalentdescribes … Ø [2] Newton's laws in Sec. 1 are onlyvalid only for low velocity and macroscale. Conversely, Einstein's relativitygeneralize the laws to the cases …Ø [2.1] Revolution of relativity starts..arXiv:2405.16800v1 [cs.LG] 27 May 2024Ø [2.1.1] The second law discussed in Sec.1.2 is invalid when approaching the speed of light. Special relativity ... Ø [2.1.2] General relativity is …</p>
<p>sections to nodes following pre-order traversal 5: for each cross-edge e = (u, w) do
Algorithm 2 Structure-Preserving RandomWalk TraversalInput: Root node v, cross-edge probabilityp, maximum length LOutput: Traversal path P1: P ← [v]2: while |P | &lt; L and v has children do3:if random() &lt; p and v has cross-edges then4:v ← Random neighbor bycross-edge6: 7: 8: 9: end for if w precedes u then Add reference at u to section containing w in D end if 10: return D5: 6: 7: 8: 9: end while else v ← Random child of v end if P ← P + [v] 10: return P4.2 Represent Text Neighborhood Information via Hierarchical Document Layout</p>
<p>Table 2 :
2
Transfer learning results.The best-performing model is highlighted in bold.
SourceCora ArxivCoraPubmed Children History ComputersPhoto↓↓↓↓↓↓↓↓TargetArxiv Cora PubmedCoraHistory ChildrenPhotoComputersGRACE0.021 0.1730.3600.3020.0730.0650.0990.070GraphMAE 0.012 0.1530.4340.2390.0090.0300.0820.0040-shotGraphCL0.015 0.2320.3680.1780.0450.0240.0940.135G2P20.241 0.6470.4210.5330.2040.1000.2970.340TAGA0.406 0.6790.4840.5590.1840.2000.4520.372TAGA-rw0.398 0.6240.4080.5260.1760.2030.4550.348GRACE0.426 0.7210.5910.6570.6090.2190.4830.382GraphMAE 0.426 0.6450.5780.5150.5270.1600.3670.2945-shotGraphCL0.107 0.6780.4360.4160.5980.1780.3950.345G2P20.395 0.7490.6330.7080.6230.2390.5090.429TAGA0.475 0.7540.6550.7340.6510.2570.5280.448TAGA-rw0.443 0.7640.6440.6740.6170.2500.4820.436</p>
<p>Table 4 :
4
± 0.001 0.094 ± 0.003 0.427 ± 0.001 0.624 ± 0.005 0.169 ± 0.001 0.387 ± 0.009 0.475 ± 0.008 0.316 ± 0.002 GraphMAE 0.104 ± 0.001 0.021 ± 0.001 0.049 ± 0.001 0.194 ± 0.006 0.019 ± 0.001 0.152 ± 0.001 0.438 ± 0.001 0.112 ± 0.001 GraphCL 0.089 ± 0.001 0.037 ± 0.001 0.173 ± 0.001 0.176 ± 0.003 0.191 ± 0.001 0.174 ± 0.001 0.368 ± 0.001 0.140 ± 0.001 GRACE 0.045 ± 0.001 0.034 ± 0.001 0.169 ± 0.001 0.146 ± 0.004 0.079 ± 0.001 0.025 ± 0.001 0.335 ± 0.001 0.057 ± 0.001 G2P2 0.453 ± 0.002 0.201 ± 0.001 0.453 ± 0.001 0.644 ± 0.004 0.322 ± 0.003 0.452 ± 0.001 0.576 ± 0.006 0.436 ± 0.001 TAGA 0.537 ± 0.003 0.224 ± 0.001 0.498 ± 0.004 0.682 ± 0.005 0.351 ± 0.009 0.419 ± 0.001 0.616 ± 0.009 0.448 ± 0.003 Zero-shot node classification performance.
arxivchildrencomputerscorahistoryphotopubmedsportsText EncoderModelUAE-Large-V1 0.500 Text-embedding-3-small PLM PLM 0.351 ± 0.001 0.098 ± 0.002 0.434 ± 0.005 0.561 ± 0.006 0.125 ± 0.001 0.321 ± 0.001 0.306 ± 0.001 0.424 ± 0.002GraphMAE 0.101 ± 0.001 0.025 ± 0.001 0.108 ± 0.001 0.162 ± 0.003 0.158 ± 0.001 0.033 ± 0.001 0.205 ± 0.001 0.364 ± 0.001GraphCL0.127 ± 0.001 0.045 ± 0.001 0.282 ± 0.001 0.197 ± 0.004 0.106 ± 0.001 0.163 ± 0.001 0.383 ± 0.001 0.240 ± 0.003GRACE0.023 ± 0.001 0.022 ± 0.001 0.117 ± 0.001 0.085 ± 0.004 0.039 ± 0.001 0.037 ± 0.001 0.319 ± 0.001 0.088 ± 0.001G2P20.332 ± 0.001 0.092 ± 0.001 0.449 ± 0.001 0.637 ± 0.006 0.168 ± 0.001 0.298 ± 0.001 0.569 ± 0.001 0.511 ± 0.003TAGA0.369 ± 0.001 0.084 ± 0.001 0.615 ± 0.001 0.668 ± 0.005 0.264 ± 0.001 0.423 ± 0.001 0.639 ± 0.001 0.548 ± 0.003k-ShotModelArxivChildrenComputersCoraHistoryPhotoPubmedSportsPLM0.280 ± 0.044 0.122 ± 0.042 0.238 ± 0.039 0.412 ± 0.080 0.284 ± 0.078 0.230 ± 0.051 0.503 ± 0.067 0.282 ± 0.068GraphMAE 0.255 ± 0.041 0.128 ± 0.028 0.300 ± 0.052 0.474 ± 0.058 0.231 ± 0.052 0.304 ± 0.066 0.492 ± 0.076 0.270 ± 0.0421GRACE G2P20.263 ± 0.034 0.138 ± 0.035 0.336 ± 0.051 0.435 ± 0.071 0.266 ± 0.085 0.295 ± 0.053 0.514 ± 0.095 0.282 ± 0.045 0.308 ± 0.052 0.145 ± 0.029 0.359 ± 0.044 0.477 ± 0.082 0.361 ± 0.092 0.372 ± 0.066 0.522 ± 0.085 0.356 ± 0.042TAGA0.323 ± 0.040 0.180 ± 0.073 0.380 ± 0.062 0.509 ± 0.089 0.413 ± 0.114 0.417 ± 0.077 0.563 ± 0.062 0.440 ± 0.070PLM0.436 ± 0.036 0.194 ± 0.029 0.318 ± 0.038 0.588 ± 0.036 0.448 ± 0.071 0.352 ± 0.044 0.611 ± 0.051 0.392 ± 0.041GraphMAE 0.379 ± 0.039 0.182 ± 0.025 0.389 ± 0.035 0.634 ± 0.044 0.362 ± 0.050 0.432 ± 0.051 0.597 ± 0.061 0.363 ± 0.0503GRACE G2P20.398 ± 0.031 0.200 ± 0.038 0.442 ± 0.045 0.622 ± 0.043 0.404 ± 0.057 0.447 ± 0.053 0.620 ± 0.055 0.398 ± 0.045 0.430 ± 0.027 0.207 ± 0.038 0.469 ± 0.042 0.623 ± 0.033 0.508 ± 0.073 0.528 ± 0.049 0.641 ± 0.064 0.464 ± 0.050TAGA0.445 ± 0.035 0.241 ± 0.062 0.497 ± 0.035 0.695 ± 0.050 0.551 ± 0.094 0.551 ± 0.045 0.659 ± 0.058 0.586 ± 0.057PLM0.500 ± 0.019 0.210 ± 0.025 0.377 ± 0.027 0.641 ± 0.031 0.557 ± 0.040 0.420 ± 0.037 0.632 ± 0.040 0.478 ± 0.056GraphMAE 0.425 ± 0.028 0.212 ± 0.029 0.434 ± 0.036 0.704 ± 0.038 0.459 ± 0.038 0.489 ± 0.038 0.625 ± 0.049 0.452 ± 0.0375GRACE G2P20.445 ± 0.028 0.227 ± 0.031 0.472 ± 0.040 0.685 ± 0.027 0.481 ± 0.061 0.515 ± 0.042 0.628 ± 0.047 0.482 ± 0.040 0.466 ± 0.025 0.240 ± 0.034 0.510 ± 0.039 0.703 ± 0.032 0.617 ± 0.053 0.583 ± 0.051 0.640 ± 0.051 0.565 ± 0.055TAGA0.483 ± 0.022 0.263 ± 0.031 0.543 ± 0.038 0.752 ± 0.028 0.636 ± 0.046 0.602 ± 0.041 0.649 ± 0.044 0.664 ± 0.061PLM0.526 ± 0.0130.240 ± 0.018 0.463 ± 0.029 0.690 ± 0.017 0.639 ± 0.038 0.491 ± 0.028 0.679 ± 0.023 0.535 ± 0.038GraphMAE 0.461 ± 0.017 0.234 ± 0.014 0.511 ± 0.028 0.761 ± 0.023 0.535 ± 0.042 0.543 ± 0.035 0.659 ± 0.028 0.508 ± 0.02810GRACE G2P20.488 ± 0.018 0.251 ± 0.015 0.552 ± 0.028 0.754 ± 0.018 0.567 ± 0.054 0.567 ± 0.031 0.670 ± 0.025 0.529 ± 0.033 0.527 ± 0.014 0.269 ± 0.018 0.598 ± 0.031 0.753 ± 0.020 0.649 ± 0.046 0.632 ± 0.037 0.691 ± 0.029 0.618 ± 0.037TAGA0.521 ± 0.017 0.288 ± 0.025 0.622 ± 0.025 0.788 ± 0.021 0.679 ± 0.041 0.651 ± 0.048 0.714 ± 0.024 0.705 ± 0.045PLM0.526 ± 0.0130.240 ± 0.018 0.463 ± 0.029 0.690 ± 0.017 0.639 ± 0.038 0.491 ± 0.028 0.679 ± 0.023 0.535 ± 0.038GraphMAE 0.501 ± 0.009 0.264 ± 0.013 0.558 ± 0.015 0.801 ± 0.014 0.597 ± 0.033 0.596 ± 0.016 0.689 ± 0.021 0.572 ± 0.02520GRACE G2P20.521 ± 0.011 0.277 ± 0.013 0.605 ± 0.017 0.791 ± 0.017 0.556 ± 0.010 0.301 ± 0.015 0.649 ± 0.015 0.813 ± 0.012 0.716 ± 0.025 0.672 ± 0.015 0.726 ± 0.025 0.690 ± 0.025 0.640± 0.037 0.615 ± 0.02 0.704 ± 0.029 0.607 ± 0.027TAGA0.561 ± 0.010 0.319 ± 0.023 0.673 ± 0.014 0.814 ± 0.012 0.721 ± 0.035 0.694 ± 0.021 0.745 ± 0.022 0.759 ± 0.026PLM0.526 ± 0.0130.240 ± 0.018 0.463 ± 0.029 0.690 ± 0.017 0.639 ± 0.038 0.491 ± 0.028 0.679 ± 0.023 0.535 ± 0.038GraphMAE 0.541 ± 0.0070.300± 0.010 0.612 ± 0.015 0.815 ± 0.008 0.657 ± 0.012 0.631 ± 0.010 0.729 ± 0.011 0.631 ± 0.01850GRACE G2P20.553 ± 0.007 0.314 ± 0.012 0.649 ± 0.012 0.818 ± 0.012 0.706 ± 0.017 0.661 ± 0.019 0.732 ± 0.014 0.678 ± 0.022 0.578 ± 0.009 0.340 ± 0.011 0.692 ± 0.012 0.827 ± 0.013 0.738 ± 0.009 0.700 ± 0.014 0.758 ± 0.009 0.725 ± 0.014TAGA0.586 ± 0.010 0.348 ± 0.015 0.712 ± 0.012 0.836 ± 0.010 0.743 ± 0.022 0.715 ± 0.016 0.771 ± 0.011 0.784 ± 0.016PLM0.592 ± 0.005 0.337 ± 0.013 0.610 ± 0.008 0.753 ± 0.014 0.753 ± 0.008 0.634 ± 0.015 0.771 ± 0.005 0.690 ± 0.013GraphMAE 0.573 ± 0.005 0.319 ± 0.008 0.650 ± 0.008 0.835 ± 0.007 0.684 ± 0.011 0.655 ± 0.012 0.744 ± 0.010 0.677 ± 0.009100GRACE G2P20.579 ± 0.007 0.339 ± 0.009 0.681 ± 0.006 0.838 ± 0.008 0.725 ± 0.014 0.678 ± 0.010 0.753 ± 0.010 0.712 ± 0.014 0.578 ± 0.007 0.360 ± 0.009 0.711 ± 0.007 0.838 ± 0.010 0.748 ± 0.009 0.710 ± 0.008 0.758 ± 0.009 0.725 ± 0.010TAGA0.631 ± 0.008 0.375 ± 0.021 0.731 ± 0.006 0.849 ± 0.008 0.754 ± 0.022 0.738 ± 0.015 0.787 ± 0.007 0.802 ± 0.014</p>
<p>Table 5 :
5
Performance of all few-shot node classification for each dataset.The text encoder choice is UAE-Large-V1.</p>
<p>Table 6 :
6
± 0.044 0.106 ± 0.025 0.347 ± 0.084 0.486 ± 0.095 0.285 ± 0.108 0.339 ± 0.055 0.491 ± 0.066 0.443 ± 0.098 GraphMAE 0.167 ± 0.041 0.112 ± 0.052 0.257 ± 0.037 0.447 ± 0.095 0.268 ± 0.063 0.263 ± 0.080 0.456 ± 0.069 0.331 ± 0.090 GRACE 0.224 ± 0.038 0.136 ± 0.034 0.329 ± 0.046 0.403 ± 0.067 0.304 ± 0.096 0.312 ± 0.049 0.513 ± 0.086 0.287 ± 0.039 G2P2 0.308 ± 0.052 0.145 ± 0.029 0.359 ± 0.044 0.477 ± 0.082 0.361 ± 0.092 0.372 ± 0.066 0.522 ± 0.085 0.356 ± 0.042 TAGA 0.306 ± 0.057 0.173 ± 0.072 0.430 ± 0.067 0.523 ± 0.101 0.395 ± 0.101 0.431 ± 0.083 0.581 ± 0.073 0.510 ± 0.099 3 PLM 0.322 ± 0.046 0.148 ± 0.024 0.495 ± 0.061 0.66 ± 0.037 0.422 ± 0.075 0.438 ± 0.044 0.608 ± 0.033 0.577 ± 0.082 GraphMAE 0.276 ± 0.033 0.169 ± 0.051 0.339 ± 0.038 0.657 ± 0.038 0.425 ± 0.097 0.347 ± 0.048 0.553 ± 0.060 0.398 ± 0.064 GRACE 0.360 ± 0.030 0.191 ± 0.037 0.455 ± 0.045 0.580 ± 0.041 0.448 ± 0.067 0.461 ± 0.045 0.623 ± 0.064 0.426 ± 0.± 0.024 0.189 ± 0.026 0.627 ± 0.025 0.741 ± 0.018 0.586 ± 0.056 0.541 ± 0.022 0.667 ± 0.025 0.708 ± 0.039 GraphMAE 0.375 ± 0.017 0.208 ± 0.011 0.469 ± 0.029 0.763 ± 0.027 0.564 ± 0.047 0.491 ± 0.034 0.613 ± 0.034 0.539 ± 0.028 GRACE 0.449 ± 0.018 0.249 ± 0.019 0.577 ± 0.027 0.714 ± 0.023 0.601 ± 0.047 0.578 ± 0.030 0.682 ± 0.025 0.569 ± 0.039 G2P2 0.527 ± 0.014 0.269 ± 0.018 0.598 ± 0.031 0.753 ± 0.020 0.649 ± 0.046 0.632 ± 0.037 0.691 ± 0.029 0.618 ± 0.037 TAGA 0.509 ± 0.020 0.315 ± 0.028 0.661 ± 0.028 0.781 ± 0.018 0.67 ± 0.049 0.646 ± 0.033 0.724 ± 0.022 0.756 ± 0.032 20 PLM 0.434 ± 0.016 0.223 ± 0.032 0.659 ± 0.014 0.767 ± 0.015 0.641 ± 0.04 0.581 ± 0.015 0.712 ± 0.021 0.761 ± 0.026 GraphMAE 0.429 ± 0.011 0.236 ± 0.020 0.535 ± 0.023 0.799 ± 0.014 0.625 ± 0.024 0.559 ± 0.017 0.655 ± 0.030 0.602 ± 0.028 GRACE 0.486 ± 0.014 0.282 ± 0.015 0.613 ± 0.019 0.770 ± 0.017 0.654 ± 0.027 0.629 ± 0.016 0.697 ± 0.022 0.657 ± 0.025 G2P2 0.556 ± 0.010 0.301 ± 0.015 0.649 ± 0.015 0.813 ± 0.012 0.716 ± 0.025 0.672 ± 0.015 0.726 ± 0.025 0.690 ± 0.025 TAGA 0.547 ± 0.010 0.332 ± 0.023 0.691 ± 0.017 0.805 ± 0.011 0.708 ± 0.039 0.682 ± 0.015 0.745 ± 0.027 0.808 ± 0.022 50 PLM 0.480 ± 0.007 0.252 ± 0.022 0.695 ± 0.010 0.785 ± 0.009 0.702 ± 0.02 0.609 ± 0.013 0.749 ± 0.011 0.784 ± 0.014 GraphMAE 0.477 ± 0.010 0.278 ± 0.012 0.603 ± 0.012 0.819 ± 0.011 0.675 ± 0.019 0.630 ± 0.015 0.692 ± 0.016 0.673 ± 0.021 GRACE 0.520 ± 0.006 0.324 ± 0.012 0.664 ± 0.013 0.806 ± 0.014 0.694 ± 0.022 0.668 ± 0.020 0.727 ± 0.015 0.712 ± 0.020 G2P2 0.578 ± 0.009 0.340 ± 0.011 0.692 ± 0.012 0.827 ± 0.013 0.738 ± 0.009 0.700 ± 0.014 0.758 ± 0.009 0.725 ± 0.014 TAGA 0.576 ± 0.009 0.368 ± 0.014 0.734 ± 0.007 0.826 ± 0.009 0.738 ± 0.021 0.717 ± 0.016 0.773 ± 0.009 0.828 ± 0.014 100 PLM 0.508 ± 0.005 0.272 ± 0.010 0.722 ± 0.007 0.800 ± 0.014 0.73 ± 0.015 0.629 ± 0.009 0.772 ± 0.008 0.802 ± 0.006 GraphMAE 0.499 ± 0.008 0.298 ± 0.014 0.634 ± 0.008 0.844 ± 0.010 0.704 ± 0.015 0.652 ± 0.017 0.721 ± 0.007 0.709 ± 0.011 GRACE 0.546 ± 0.007 0.344 ± 0.008 0.693 ± 0.006 0.823 ± 0.013 0.714 ± 0.011 0.688 ± 0.011 0.745 ± 0.006 0.753 ± 0.010 G2P2 0.578 ± 0.007 0.360 ± 0.009 0.711 ± 0.007 0.838 ± 0.010 0.748 ± 0.009 0.710 ± 0.008 0.758 ± 0.009 0.725 ± 0.010 TAGA 0.602 ± 0.007 0.400 ± 0.017 0.747 ± 0.009 0.838 ± 0.009 0.755 ± 0.017 0.738 ± 0.010 0.786 ± 0.006 0.846 ± 0.013 Performance of all few-shot node classification for each dataset.The text encoder choice is Text-embedding-3-small.
k-ShotModelArxivChildrenComputersCoraHistoryPhotoPubmedSports1PLM0.199 045G2P20.430 ± 0.027 0.207 ± 0.038 0.469 ± 0.042 0.623 ± 0.033 0.508 ± 0.073 0.528 ± 0.049 0.641 ± 0.064 0.464 ± 0.050TAGA0.442 ± 0.023 0.248 ± 0.052 0.548 ± 0.058 0.702 ± 0.032 0.523 ± 0.08 0.575 ± 0.047 0.683 ± 0.056 0.67 ± 0.062PLM0.365 ± 0.037 0.174 ± 0.039 0.55 ± 0.0360.705 ± 0.02 0.522 ± 0.094 0.502 ± 0.039 0.601 ± 0.0320.67 ± 0.05GraphMAE 0.308 ± 0.030 0.196 ± 0.059 0.384 ± 0.026 0.711 ± 0.030 0.511 ± 0.058 0.412 ± 0.032 0.563 ± 0.068 0.484 ± 0.0385GRACE G2P20.399 ± 0.026 0.223 ± 0.028 0.501 ± 0.043 0.635 ± 0.028 0.513 ± 0.051 0.527 ± 0.040 0.640 ± 0.052 0.521 ± 0.049 0.466 ± 0.025 0.240 ± 0.034 0.510 ± 0.039 0.703 ± 0.032 0.617 ± 0.053 0.583 ± 0.051 0.640 ± 0.051 0.565 ± 0.055TAGA0.468 ± 0.023 0.299 ± 0.034 0.584 ± 0.040.74 ± 0.031 0.618 ± 0.0670.6 ± 0.0410.676 ± 0.048 0.735 ± 0.063PLM0.39810
A Additional Experimental Results and SettingsIn this section, we present additional experimental settings and results due to the space limitation of the main paper.A.1 Additional Implementation SettingsAll experiments are conducted on a 64-bit machine with a 16GB NVIDIA GPU.Each experiment involves running the models 20 times with different random seeds to minimize variance due to specific data splits.Accuracy is adopted as the evaluation metric for node classification tasks.Specifically, for smaller datasets such as Cora and PubMed, we employ 3 convolution layers, while for larger datasets, we utilize 2 layers.Latent dimension is aligned with the PLM embedding dimension.During the pre-train stage, the model is trained with 40,000 steps on each dataset with minibatch size 8.The learning rate is initialized as 1e −3 and with decay rate 0.999 each 10 steps.For zero-shot predictions, we utilize the entire dataset as the test set.In the case of k-shot predictions, we randomly select k samples from each class to form the training set, dividing the remaining data into validation and test sets at a ratio of 1:9.All models undergo finetune for 100 epochs, and testing is based on the best validation results.In this section, we investigate the sensitivity of the key hyperparameters and their impact on TAGA's performance.Specifically, we first evaluate how different GNN backbones (GCN, GIN, and Graph-SAGE) affect performance.Then we evaluate how jumping ratio (p) and maximum walk length (L) would affect random walk's performance.The results are presented in Figure4.The sensitivity analysis conducted on TAGA's performance demonstrates that the method is robust across a range of hyperparameters.Specifically, the variance in performance across different GNN backbones is 0.84%, indicating a stable behavior regardless of the backbone employed.Similarly, adjustments in the jumping ratio (p) and maximum walk length (L) exhibit 0.33% and 0.76% variance on average, which underscores that our method is not sensitive to the hyperparameters chosen.A.2 Sensitivity AnalysisA.3 Additional Effectiveness AnalysisWe present additional zero-shot and few-shot performance under two different text encoders UAE-Large-V1 and Text-embedding-3-small.The zero-shot results are present in Table 4.The few-shot results with text encoder UAE-Large-V1 is present in Table5, and few-shot results with text encoderText-embedding-3-small is present in Table6.From the table, we
Fastgcn: fast learning with graph convolutional networks via importance sampling. Jie Chen, Tengfei Ma, Cao Xiao, arXiv:1801.102472018arXiv preprint</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, arXiv:2306.155952023aarXiv preprint</p>
<p>Longlora: Efficient fine-tuning of long-context large language models. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia, arXiv:2309.123072023barXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, ACM SIGKDD Explorations Newsletter. 2522024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei, arXiv:2307.02486Longnet: Scaling transformers to 1,000,000,000 tokens. 2023aarXiv preprint</p>
<p>Eliciting structural and semantic global knowledge in unsupervised graph contrastive learning. Kaize Ding, Yancheng Wang, Yingzhen Yang, Huan Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023b37</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, Advances in neural information processing systems. 201730</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Ji Heng, Sinong Wang, arXiv:2308.16137The Twelfth International Conference on Learning Representations. Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, 2023. 2023arXiv preprintLm-infinite: Simple on-the-fly length generalization for large language models</p>
<p>Graphmae: Self-supervised masked graph autoencoders. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, Jie Tang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.006872020arXiv preprint</p>
<p>Beyond text: A deep dive into large language models' ability on understanding graph data. Yuntong Hu, Zheng Zhang, Liang Zhao, arXiv:2310.049442023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: when and why. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, arXiv:2309.165952023arXiv preprint</p>
<p>A graph model for e-commerce recommender systems. Zan Huang, Wingyan Chung, Hsinchun Chen, Journal of the American Society for information science and technology. 5532004</p>
<p>Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu, arXiv:2310.068392023arXiv preprint</p>
<p>Large language models on graphs: A comprehensive survey. Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.027832023aarXiv preprint</p>
<p>Edgeformers: Graph-empowered transformers for representation learning on textual-edge networks. Jin Bowen, Yu Zhang, Yu Meng, Jiawei Han, The Eleventh International Conference on Learning Representations,{ICLR} 2023. OpenReview. net. 2023b</p>
<p>Variational graph auto-encoders. N Thomas, Max Kipf, Welling, arXiv:1611.073082016arXiv preprint</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, Proceedings of the 5th International Conference on Learning Representations. the 5th International Conference on Learning Representations2017</p>
<p>. Xianming Li, Jing Li, arXiv:2309.128712023Angle-optimized text embeddings. arXiv preprint</p>
<p>Grenade: Graph-centric language model for self-supervised representation learning on text-attributed graphs. Yichuan Li, Kaize Ding, Kyumin Lee, arXiv:2310.151092023arXiv preprint</p>
<p>Full-text citation analysis: A new method to enhance scholarly networks. Xiaozhong Liu, Jinsong Zhang, Chun Guo, Journal of the American Society for Information Science and Technology. 6492013</p>
<p>Information network or social network? the structure of the twitter follow graph. Seth A Myers, Aneesh Sharma, Pankaj Gupta, Jimmy Lin, Proceedings of the 23rd international conference on world wide web. the 23rd international conference on world wide web2014</p>
<p>Infranodus: Generating insight using text network analysis. Dmitry Paranyushkin, The world wide web conference. 2019</p>
<p>Yarn: Efficient context window extension of large language models. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole, arXiv:2309.000712023arXiv preprint</p>
<p>Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan Günnemann, arXiv:1811.05868Pitfalls of graph neural network evaluation. 2018arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Petar Veličković, William Fedus, Pietro William L Hamilton, Yoshua Liò, Devon Bengio, Hjelm, arXiv:1809.10341Deep graph infomax. 2018arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Advances in Neural Information Processing Systems. 362024</p>
<p>Augmenting low-resource text classification with graph-grounded pre-training and prompting. Zhihao Wen, Yuan Fang, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Graph neural networks in recommender systems: a survey. Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, Bin Cui, ACM Computing Surveys. 5552022</p>
<p>How powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, International Conference on Learning Representations. 2018</p>
<p>A comprehensive study on text-attributed graphs: Benchmarking and rethinking. Chaozhuo Hao Yan, Ruosong Li, Chao Long, Jianan Yan, Wenwen Zhao, Jun Zhuang, Peiyan Yin, Weihao Zhang, Hao Han, Sun, Advances in Neural Information Processing Systems. 202336</p>
<p>Revisiting semi-supervised learning with graph embeddings. Zhilin Yang, William Cohen, Ruslan Salakhudinov, International conference on machine learning. PMLR2016</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.071342023arXiv preprint</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in neural information processing systems. 202033</p>
<p>Graphsaint: Graph sampling based inductive learning method. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, Viktor Prasanna, arXiv:1907.049312019arXiv preprint</p>
<p>Text-attributed graph representation learning: Methods, applications, and challenges. Ce Delvin, Menglin Zhang, Rex Yang, Hady W Ying, Lauw, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, arXiv:2006.04131Deep graph contrastive representation learning. 2020arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
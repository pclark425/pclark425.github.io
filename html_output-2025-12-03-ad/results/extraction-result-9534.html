<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9534 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9534</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9534</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-272550739</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.06433v1.pdf" target="_blank">Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</a></p>
                <p><strong>Paper Abstract:</strong> The increasing amount of published scholarly articles, exceeding 2.5 million yearly, raises the challenge for researchers in following scientific progress. Integrating the contributions from scholarly articles into a novel type of cognitive knowledge graph (CKG) will be a crucial element for accessing and organizing scholarly knowledge, surpassing the insights provided by titles and abstracts. This research focuses on effectively conveying structured scholarly knowledge by utilizing large language models (LLMs) to categorize scholarly articles and describe their contributions in a structured and comparable manner. While previous studies explored language models within specific research domains, the extensive domain-independent knowledge captured by LLMs offers a substantial opportunity for generating structured contribution descriptions as CKGs. Additionally, LLMs offer customizable pathways through prompt engineering or fine-tuning, thus facilitating to leveraging of smaller LLMs known for their efficiency, cost-effectiveness, and environmental considerations. Our methodology involves harnessing LLM knowledge, and complementing it with domain expert-verified scholarly data sourced from a CKG. This strategic fusion significantly enhances LLM performance, especially in tasks like scholarly article categorization and predicate recommendation. Our method involves fine-tuning LLMs with CKG knowledge and additionally injecting knowledge from a CKG with a novel prompting technique significantly increasing the accuracy of scholarly knowledge extraction. We integrated our approach in the Open Research Knowledge Graph (ORKG), thus enabling precise access to organized scholarly knowledge, crucially benefiting domain-independent scholarly knowledge exchange and dissemination among policymakers, industrial practitioners, and the general public.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9534.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9534.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IQCK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Injecting Query-specific Context Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-engineering method that injects task- and query-specific context retrieved from a Cognitive Knowledge Graph (CKG) into Chain-of-Thought style prompts to improve LLM performance on scholarly tasks such as research-field prediction and predicate recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Mistral 7B (and other evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Mistral 7B (7-billion-parameter transformer) was used as a pre-trained LLM in experiments alongside Llama 2 (7B, 13B) and Gemini Pro; models were used either in zero/few-shot/CoT prompting with injected CKG context or fine-tuned variants were created (e.g., LoRA-based fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly communication / research contribution description (cross-domain across many research fields)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Graphlets extracted from the Open Research Knowledge Graph (ORKG): training sets of 1,894 graphlets for research-field annotations and 1,740 graphlets for predicate lists; a held-out test set of 100 curated graphlets; additionally abstracts from the CORE dataset were used for contextual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Reusable cognitive-structure templates (graphlets) representing common patterns of research contributions (i.e., structural/generalizable templates for contribution descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>A graphlet template connecting typed entities and properties such as hasTitle → followsMethodology → usesDataset → hasContribution → belongsToResearchField, used as a reusable pattern to represent research contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Query-specific CKG graphlets were retrieved via SPARQL and injected into CoT prompts (IQCK). Task-aware prefixes (e.g., 'Research field prediction') were optionally prepended. For some experiments, models were fine-tuned with CKG-derived data using LoRA, quantization, and TRL trainer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Two-pronged evaluation: (1) LLM-based automated evaluation using GPT-4 (gpt-4-1106-preview) to score outputs on clarity, coverage, relevance, granularity (0-3 scale aggregated to Mean Average Score (MAS)); (2) human expert evaluation following the same criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Injecting CKG context into prompts (IQCK) improved research-field prediction accuracy for Mistral 7B from baseline 67–73% to 76% MAS on sample graphlets. Across models, pre-trained models with prefix+IQCK achieved high MAS (e.g., Gemini Pro with Prefix reached 80% by GPT-4 evaluation). For predicate recommendation, pre-trained Llama 2 13B with Prefix achieved MAS ≈ 65% (GPT-4) and 60% (human). Fine-tuned variants were mixed: some fine-tuned models produced competitive predicate recommendation scores (e.g., ORKG Llama 2 13B reached ~67% MAS) but fine-tuning did not uniformly improve research-field prediction and in some cases performed worse than pre-trained+IQCK.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to baseline prompting strategies (Zero-shot, Few-shot, CoT, zero-shot CoT with no CKG hierarchy), IQCK improved Mistral 7B research-field MAS from 67–73% up to 76% (sample comparison). Pre-trained models with IQCK often outperformed vanilla prompting; fine-tuning sometimes underperformed pre-trained+IQCK for research-field prediction but could match or exceed baselines for predicate recommendation depending on settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Domain expert-verified ORKG data contained only a single research field per article (limiting evaluation for interdisciplinary works) and incomplete predicate lists (CKG incompleteness). Human evaluators had only high-level guidelines and were not experts across all domains, introducing variability. Fine-tuning improvements were inconsistent and require more task-specific data and tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The paper explicitly notes general LLM challenges including bias, intransparency, and confabulation (generation of plausible but factually incorrect outputs). These are cited as motivations for using CKG-backed context and human-verified data; no quantitative hallucination rates are reported, but confabulation is identified as a known failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9534.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9534.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CKG graphlets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognitive Knowledge Graph graphlets (reusable cognitive units)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured, connected small subgraph patterns (graphlets) in a Cognitive Knowledge Graph that represent reusable information structures for scholarly contributions (e.g., problem, approach, evaluation), used as contextual knowledge to guide LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama 2 13B, Llama 2 7B, Mistral 7B, Gemini Pro (used with graphlet injection)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Pretrained transformer LLMs of specified sizes (Llama 2 7B/13B, Mistral 7B) and a commercial model (Gemini Pro) were provided CKG graphlet context via prompting; some Llama variants were also fine-tuned on ORKG data (LoRA/quantization).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly knowledge organization; meta-analysis of research contributions across many scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Graphlets constructed from ORKG entries (metadata and predicate-object structures) and abstracts from CORE; training sets included 1,894 graphlets for research-field, 1,740 graphlets for predicate lists, and 100 curated graphlets for testing.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Structural/generalizable contribution templates (graphlet motifs) that serve as domain-independent organizational rules for representing scholarly contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>A canonical graphlet representing a contribution: {hasTitle, followsMethodology, usesDataset, hasContribution, belongsToResearchField} which can be reused to describe many papers' contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Graphlets were retrieved via SPARQL from ORKG and used as prompt context (IQCK) to condition LLM outputs; additionally, graphlet-derived pairs were used as fine-tuning examples to teach LLMs to recommend predicates and fields.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Model outputs informed by graphlets were evaluated by GPT-4 and human experts on clarity, coverage, relevance, and granularity, aggregated to MAS; direct comparisons made to outputs from baselines without injected graphlets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using graphlets as injected context improved LLM performance on the targeted tasks (notably research-field prediction). The approach demonstrated that supplying explicit, structured cognitive patterns to LLMs helps in domains with sparse LLM training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Graphlet-injected prompts outperformed baseline prompt styles (Zero/Few-shot and CoT without hierarchy) for research-field prediction (e.g., Mistral 7B: baseline 67–73% vs IQCK 76% MAS).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>CKG/graphlet coverage is incomplete; ORKG entries may not contain all relevant predicates or multi-field annotations, limiting the ground truth available for training and evaluation. Building larger, higher-coverage CKG graphlet corpora is noted as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Graphlet injection reduces some hallucination by grounding prompts in curated CKG content, but risks remain because LLMs can still generate plausible-but-incorrect predicates or fields when the CKG is incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9534.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9534.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 2 13B (pretrained + IQCK)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 13-billion-parameter model used with injected CKG context</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama 2 13B was evaluated both with and without task-aware prefixes and with IQCK prompt injection; it was also fine-tuned in ORKG-variants. It was applied to research-field prediction and list-of-predicate recommendation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama 2 13B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A 13-billion-parameter transformer LLM (Llama 2 13B) used in pre-trained form with prompt injection and optionally fine-tuned on ORKG graphlet datasets (LoRA+quantization in some experiments, trained with TRL trainer).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly article metadata extraction and organization across diverse research fields.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>ORKG-derived graphlets (1,894 and 1,740 for training tasks), 100 test graphlets, and CORE abstracts for grounding; prompts included title, abstract, and research field hierarchy when IQCK was used.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Predicate recommendation lists and research-field assignment patterns; implicitly, structural rules for mapping textual article content to graphlet slots.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>From an article about 'Transfer Learning', the model recommended domain-specific predicates such as usesTrainingCorpus, usesTokenization, hasNumberofParameters, hasModelFamily, followsMethodology.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Pretrained Llama 2 13B was prompted using IQCK (SPARQL-extracted graphlets injected into CoT prompts), with optional task-aware prefixes; separate experiments fine-tuned the model on ORKG graphlet data using LoRA/quantization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated scoring via GPT-4 on clarity, coverage, relevance, granularity (0-3 scores aggregated to MAS), and human evaluation by domain experts using the same criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Pre-trained Llama 2 13B with Prefix achieved MAS ~65% for predicate recommendation (GPT-4) and ~60% human; for research-field prediction it achieved moderate scores (reported lower than top performers like Gemini Pro or Mistral in some evaluations). Fine-tuned ORKG variants sometimes improved predicate recommendation (e.g., ORKG Llama 2 13B reached ~67% MAS) but fine-tuning did not consistently improve research-field prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Pretrained+IQCK generally outperformed vanilla prompting baselines; fine-tuning provided mixed gains and in some RF (research field) setups yielded lower MAS than pre-trained+IQCK baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Performance varied by task and prefix use; improvements depend on quality and completeness of CKG graphlets. Fine-tuning resource requirements and tuning hyperparameters impacted outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Paper reiterates general LLM risks (confabulation/bias) and notes that incomplete CKG labels can cause correct LLM outputs to be judged as incorrect against limited expert-verified ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9534.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9534.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini Pro (pretrained + IQCK)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini Pro model used with injected CKG context and optional task prefixes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial LLM (Gemini Pro) evaluated in the study; when used with task-aware prefixes and CKG context injection it achieved the highest reported pre-trained-model MAS for research-field prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A commercial, large-scale transformer language model (details not specified in the paper) evaluated in pre-trained form with prompt engineering (IQCK) and optional task prefixing.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly knowledge organization across multiple research domains.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>ORKG graphlets (as above: training graphlets and 100 test graphlets) and CORE abstracts injected into prompts as contextual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Research-field assignment patterns and predicate recommendation behavior (structural rules for mapping article content to contribution descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Using IQCK and a task-aware prefix, Gemini Pro correctly assigned a sample article to the 'machine learning' research field and recommended relevant predicates for contribution description.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>IQCK: SPARQL-extracted CKG graphlets and research-field hierarchies were injected into CoT-style prompts with optional task-aware prefixes; Gemini Pro processed these augmented prompts to output field assignments and predicate lists.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated evaluation via GPT-4 and human evaluation; outputs were scored on clarity, coverage, relevance, and granularity (0-3), aggregated into MAS.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Gemini Pro with Prefix achieved a reported MAS of ~80% (GPT-4 evaluation) for research-field prediction, making it the top pre-trained performer in those experiments; human-evaluation MAS for Gemini Pro was reported ~78% in some tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Gemini Pro with IQCK and prefix outperformed baseline prompt styles and other pre-trained models in research-field prediction according to GPT-4 and human MAS metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Model-specific internals and training data for Gemini Pro are not detailed; as with other models, evaluation is constrained by single-field expert labels and incomplete predicate coverage in ORKG.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>General concerns about LLM bias and confabulation are noted; no model-specific hallucination statistics are provided, but grounding via CKG is presented as a mitigation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llm-assisted content analysis: Using large language models to support deductive coding <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>From answers to insights: Unveiling the strengths and limitations of chatgpt and biomedical knowledge graphs <em>(Rating: 1)</em></li>
                <li>Prefix propagation: Parameterefficient tuning for long sequences <em>(Rating: 1)</em></li>
                <li>Qa-lora: Quantization-aware low-rank adaptation of large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9534",
    "paper_id": "paper-272550739",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "IQCK",
            "name_full": "Injecting Query-specific Context Knowledge",
            "brief_description": "A prompt-engineering method that injects task- and query-specific context retrieved from a Cognitive Knowledge Graph (CKG) into Chain-of-Thought style prompts to improve LLM performance on scholarly tasks such as research-field prediction and predicate recommendation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Mistral 7B (and other evaluated LLMs)",
            "llm_model_description": "Mistral 7B (7-billion-parameter transformer) was used as a pre-trained LLM in experiments alongside Llama 2 (7B, 13B) and Gemini Pro; models were used either in zero/few-shot/CoT prompting with injected CKG context or fine-tuned variants were created (e.g., LoRA-based fine-tuning).",
            "application_domain": "Scholarly communication / research contribution description (cross-domain across many research fields)",
            "input_corpus_description": "Graphlets extracted from the Open Research Knowledge Graph (ORKG): training sets of 1,894 graphlets for research-field annotations and 1,740 graphlets for predicate lists; a held-out test set of 100 curated graphlets; additionally abstracts from the CORE dataset were used for contextual grounding.",
            "qualitative_law_type": "Reusable cognitive-structure templates (graphlets) representing common patterns of research contributions (i.e., structural/generalizable templates for contribution descriptions).",
            "qualitative_law_example": "A graphlet template connecting typed entities and properties such as hasTitle → followsMethodology → usesDataset → hasContribution → belongsToResearchField, used as a reusable pattern to represent research contributions.",
            "extraction_methodology": "Query-specific CKG graphlets were retrieved via SPARQL and injected into CoT prompts (IQCK). Task-aware prefixes (e.g., 'Research field prediction') were optionally prepended. For some experiments, models were fine-tuned with CKG-derived data using LoRA, quantization, and TRL trainer.",
            "evaluation_method": "Two-pronged evaluation: (1) LLM-based automated evaluation using GPT-4 (gpt-4-1106-preview) to score outputs on clarity, coverage, relevance, granularity (0-3 scale aggregated to Mean Average Score (MAS)); (2) human expert evaluation following the same criteria.",
            "results_summary": "Injecting CKG context into prompts (IQCK) improved research-field prediction accuracy for Mistral 7B from baseline 67–73% to 76% MAS on sample graphlets. Across models, pre-trained models with prefix+IQCK achieved high MAS (e.g., Gemini Pro with Prefix reached 80% by GPT-4 evaluation). For predicate recommendation, pre-trained Llama 2 13B with Prefix achieved MAS ≈ 65% (GPT-4) and 60% (human). Fine-tuned variants were mixed: some fine-tuned models produced competitive predicate recommendation scores (e.g., ORKG Llama 2 13B reached ~67% MAS) but fine-tuning did not uniformly improve research-field prediction and in some cases performed worse than pre-trained+IQCK.",
            "comparison_to_baseline": "Compared to baseline prompting strategies (Zero-shot, Few-shot, CoT, zero-shot CoT with no CKG hierarchy), IQCK improved Mistral 7B research-field MAS from 67–73% up to 76% (sample comparison). Pre-trained models with IQCK often outperformed vanilla prompting; fine-tuning sometimes underperformed pre-trained+IQCK for research-field prediction but could match or exceed baselines for predicate recommendation depending on settings.",
            "reported_limitations": "Domain expert-verified ORKG data contained only a single research field per article (limiting evaluation for interdisciplinary works) and incomplete predicate lists (CKG incompleteness). Human evaluators had only high-level guidelines and were not experts across all domains, introducing variability. Fine-tuning improvements were inconsistent and require more task-specific data and tuning.",
            "bias_or_hallucination_issues": "The paper explicitly notes general LLM challenges including bias, intransparency, and confabulation (generation of plausible but factually incorrect outputs). These are cited as motivations for using CKG-backed context and human-verified data; no quantitative hallucination rates are reported, but confabulation is identified as a known failure mode.",
            "uuid": "e9534.0",
            "source_info": {
                "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CKG graphlets",
            "name_full": "Cognitive Knowledge Graph graphlets (reusable cognitive units)",
            "brief_description": "Structured, connected small subgraph patterns (graphlets) in a Cognitive Knowledge Graph that represent reusable information structures for scholarly contributions (e.g., problem, approach, evaluation), used as contextual knowledge to guide LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Llama 2 13B, Llama 2 7B, Mistral 7B, Gemini Pro (used with graphlet injection)",
            "llm_model_description": "Pretrained transformer LLMs of specified sizes (Llama 2 7B/13B, Mistral 7B) and a commercial model (Gemini Pro) were provided CKG graphlet context via prompting; some Llama variants were also fine-tuned on ORKG data (LoRA/quantization).",
            "application_domain": "Scholarly knowledge organization; meta-analysis of research contributions across many scientific domains.",
            "input_corpus_description": "Graphlets constructed from ORKG entries (metadata and predicate-object structures) and abstracts from CORE; training sets included 1,894 graphlets for research-field, 1,740 graphlets for predicate lists, and 100 curated graphlets for testing.",
            "qualitative_law_type": "Structural/generalizable contribution templates (graphlet motifs) that serve as domain-independent organizational rules for representing scholarly contributions.",
            "qualitative_law_example": "A canonical graphlet representing a contribution: {hasTitle, followsMethodology, usesDataset, hasContribution, belongsToResearchField} which can be reused to describe many papers' contributions.",
            "extraction_methodology": "Graphlets were retrieved via SPARQL from ORKG and used as prompt context (IQCK) to condition LLM outputs; additionally, graphlet-derived pairs were used as fine-tuning examples to teach LLMs to recommend predicates and fields.",
            "evaluation_method": "Model outputs informed by graphlets were evaluated by GPT-4 and human experts on clarity, coverage, relevance, and granularity, aggregated to MAS; direct comparisons made to outputs from baselines without injected graphlets.",
            "results_summary": "Using graphlets as injected context improved LLM performance on the targeted tasks (notably research-field prediction). The approach demonstrated that supplying explicit, structured cognitive patterns to LLMs helps in domains with sparse LLM training data.",
            "comparison_to_baseline": "Graphlet-injected prompts outperformed baseline prompt styles (Zero/Few-shot and CoT without hierarchy) for research-field prediction (e.g., Mistral 7B: baseline 67–73% vs IQCK 76% MAS).",
            "reported_limitations": "CKG/graphlet coverage is incomplete; ORKG entries may not contain all relevant predicates or multi-field annotations, limiting the ground truth available for training and evaluation. Building larger, higher-coverage CKG graphlet corpora is noted as future work.",
            "bias_or_hallucination_issues": "Graphlet injection reduces some hallucination by grounding prompts in curated CKG content, but risks remain because LLMs can still generate plausible-but-incorrect predicates or fields when the CKG is incomplete.",
            "uuid": "e9534.1",
            "source_info": {
                "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama 2 13B (pretrained + IQCK)",
            "name_full": "Llama 2 13-billion-parameter model used with injected CKG context",
            "brief_description": "Llama 2 13B was evaluated both with and without task-aware prefixes and with IQCK prompt injection; it was also fine-tuned in ORKG-variants. It was applied to research-field prediction and list-of-predicate recommendation tasks.",
            "citation_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
            "mention_or_use": "use",
            "llm_model_name": "Llama 2 13B",
            "llm_model_description": "A 13-billion-parameter transformer LLM (Llama 2 13B) used in pre-trained form with prompt injection and optionally fine-tuned on ORKG graphlet datasets (LoRA+quantization in some experiments, trained with TRL trainer).",
            "application_domain": "Scholarly article metadata extraction and organization across diverse research fields.",
            "input_corpus_description": "ORKG-derived graphlets (1,894 and 1,740 for training tasks), 100 test graphlets, and CORE abstracts for grounding; prompts included title, abstract, and research field hierarchy when IQCK was used.",
            "qualitative_law_type": "Predicate recommendation lists and research-field assignment patterns; implicitly, structural rules for mapping textual article content to graphlet slots.",
            "qualitative_law_example": "From an article about 'Transfer Learning', the model recommended domain-specific predicates such as usesTrainingCorpus, usesTokenization, hasNumberofParameters, hasModelFamily, followsMethodology.",
            "extraction_methodology": "Pretrained Llama 2 13B was prompted using IQCK (SPARQL-extracted graphlets injected into CoT prompts), with optional task-aware prefixes; separate experiments fine-tuned the model on ORKG graphlet data using LoRA/quantization.",
            "evaluation_method": "Automated scoring via GPT-4 on clarity, coverage, relevance, granularity (0-3 scores aggregated to MAS), and human evaluation by domain experts using the same criteria.",
            "results_summary": "Pre-trained Llama 2 13B with Prefix achieved MAS ~65% for predicate recommendation (GPT-4) and ~60% human; for research-field prediction it achieved moderate scores (reported lower than top performers like Gemini Pro or Mistral in some evaluations). Fine-tuned ORKG variants sometimes improved predicate recommendation (e.g., ORKG Llama 2 13B reached ~67% MAS) but fine-tuning did not consistently improve research-field prediction.",
            "comparison_to_baseline": "Pretrained+IQCK generally outperformed vanilla prompting baselines; fine-tuning provided mixed gains and in some RF (research field) setups yielded lower MAS than pre-trained+IQCK baselines.",
            "reported_limitations": "Performance varied by task and prefix use; improvements depend on quality and completeness of CKG graphlets. Fine-tuning resource requirements and tuning hyperparameters impacted outcomes.",
            "bias_or_hallucination_issues": "Paper reiterates general LLM risks (confabulation/bias) and notes that incomplete CKG labels can cause correct LLM outputs to be judged as incorrect against limited expert-verified ground truth.",
            "uuid": "e9534.2",
            "source_info": {
                "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Gemini Pro (pretrained + IQCK)",
            "name_full": "Gemini Pro model used with injected CKG context and optional task prefixes",
            "brief_description": "A commercial LLM (Gemini Pro) evaluated in the study; when used with task-aware prefixes and CKG context injection it achieved the highest reported pre-trained-model MAS for research-field prediction.",
            "citation_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
            "mention_or_use": "use",
            "llm_model_name": "Gemini Pro",
            "llm_model_description": "A commercial, large-scale transformer language model (details not specified in the paper) evaluated in pre-trained form with prompt engineering (IQCK) and optional task prefixing.",
            "application_domain": "Scholarly knowledge organization across multiple research domains.",
            "input_corpus_description": "ORKG graphlets (as above: training graphlets and 100 test graphlets) and CORE abstracts injected into prompts as contextual grounding.",
            "qualitative_law_type": "Research-field assignment patterns and predicate recommendation behavior (structural rules for mapping article content to contribution descriptors).",
            "qualitative_law_example": "Using IQCK and a task-aware prefix, Gemini Pro correctly assigned a sample article to the 'machine learning' research field and recommended relevant predicates for contribution description.",
            "extraction_methodology": "IQCK: SPARQL-extracted CKG graphlets and research-field hierarchies were injected into CoT-style prompts with optional task-aware prefixes; Gemini Pro processed these augmented prompts to output field assignments and predicate lists.",
            "evaluation_method": "Automated evaluation via GPT-4 and human evaluation; outputs were scored on clarity, coverage, relevance, and granularity (0-3), aggregated into MAS.",
            "results_summary": "Gemini Pro with Prefix achieved a reported MAS of ~80% (GPT-4 evaluation) for research-field prediction, making it the top pre-trained performer in those experiments; human-evaluation MAS for Gemini Pro was reported ~78% in some tables.",
            "comparison_to_baseline": "Gemini Pro with IQCK and prefix outperformed baseline prompt styles and other pre-trained models in research-field prediction according to GPT-4 and human MAS metrics.",
            "reported_limitations": "Model-specific internals and training data for Gemini Pro are not detailed; as with other models, evaluation is constrained by single-field expert labels and incomplete predicate coverage in ORKG.",
            "bias_or_hallucination_issues": "General concerns about LLM bias and confabulation are noted; no model-specific hallucination statistics are provided, but grounding via CKG is presented as a mitigation strategy.",
            "uuid": "e9534.3",
            "source_info": {
                "paper_title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llm-assisted content analysis: Using large language models to support deductive coding",
            "rating": 2,
            "sanitized_title": "llmassisted_content_analysis_using_large_language_models_to_support_deductive_coding"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "From answers to insights: Unveiling the strengths and limitations of chatgpt and biomedical knowledge graphs",
            "rating": 1,
            "sanitized_title": "from_answers_to_insights_unveiling_the_strengths_and_limitations_of_chatgpt_and_biomedical_knowledge_graphs"
        },
        {
            "paper_title": "Prefix propagation: Parameterefficient tuning for long sequences",
            "rating": 1,
            "sanitized_title": "prefix_propagation_parameterefficient_tuning_for_long_sequences"
        },
        {
            "paper_title": "Qa-lora: Quantization-aware low-rank adaptation of large language models",
            "rating": 1,
            "sanitized_title": "qalora_quantizationaware_lowrank_adaptation_of_large_language_models"
        }
    ],
    "cost": 0.01365225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization
10 Sep 2024</p>
<p>Gollam Rabby gollam.rabby@l3s.de 
L3S Research Center
Leibniz University Hannover
HanoverGermany</p>
<p>Sören Auer auer@tib.eu 
Jennifer D'souza jennifer.dsouza@tib.eu 
Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Allard Oelen allard.oelen@tib.eu 
Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization
10 Sep 2024B3C4AC43B1BFBB1A2877E5B1363E199EarXiv:2409.06433v1[cs.DL]
The increasing amount of published scholarly articles, exceeding 2.5 million yearly, raises the challenge for researchers in following scientific progress.Integrating the contributions from scholarly articles into a novel type of cognitive knowledge graph (CKG) will be a crucial element for accessing and organizing scholarly knowledge, surpassing the insights provided by titles and abstracts.This research focuses on effectively conveying structured scholarly knowledge by utilizing large language models (LLMs) to categorize scholarly articles and describe their contributions in a structured and comparable manner.While previous studies explored language models within specific research domains, the extensive domainindependent knowledge captured by LLMs offers a substantial opportunity for generating structured contribution descriptions as CKGs.Additionally, LLMs offer customizable pathways through prompt engineering or fine-tuning, thus facilitating to leveraging of smaller LLMs known for their efficiency, cost-effectiveness, and environmental considerations.Our methodology involves harnessing LLM knowledge, and complementing it with domain expert-verified scholarly data sourced from a CKG.This strategic fusion significantly enhances LLM performance, especially in tasks like scholarly article categorization and predicate recommendation.Our method involves fine-tuning LLMs with CKG knowledge and additionally injecting knowledge from a CKG with a novel prompting technique significantly increasing the accuracy of scholarly knowledge extraction.We integrated our approach in the Open Research Knowledge Graph (ORKG), thus enabling precise access to organized scholarly knowledge, crucially benefiting domain-independent scholarly knowledge exchange and dissemination among policymakers, industrial practitioners, and the general public.</p>
<p>Introduction</p>
<p>In recent years, we saw a steeply rising popularity of Large Language Models (LLMs) for a variety of applications in natural language understanding and generation, content creation, programming assistance, translation, etc.However, for many applications, also a number of challenges with respect to the application of LLMs became apparent.Besides potential bias and intransparency these include in particular: (a) the limited context information that can be exploited by the LLM, (b) confabulation, where the LLM generates information or narratives that are plausible-sounding but factually incorrect or misleading, and (c) difficulty in handling specific highly specialized or niche domains where training data is limited or the language used is very specific or technical.</p>
<p>These issues are particularly pressing for applications in scholarly communication, i.e., the representation, organization, exchange, and usage of scholarly knowledge.Traditionally, scholarly knowledge is represented primarily in scientific articles of which several hundred million are already available and approx.2.5 million are furthermore added every year.While LLMs can and are being trained with scientific articles [Jungherr, 2023;Lee et al., 2023] scholarly communication is inherently complex, involving intricate processes and specialized knowledge.LLMs struggle to capture the nuances and depth required in academic discourse [Asher et al., 2023].</p>
<p>In this work, we address the context and specialized knowledge issues of LLMs with a Neuro-Symbolic approach intertwining two complementary methods -(1) fine-tuning LLMs with background knowledge obtained from knowledge graphs and (2) injecting query-specific context knowledge into the prompt.For both strategies, we leverage a novel type of contextualized knowledge graphs -cognitive knowledge graphs, which organize knowledge not only as entities and relationships but also in small reusable cognitive units.A cognitive knowledge graph is a knowledge graph equipped with an overlay structure, which determines reusable patterns (so-called graphlets), that represent common cognitive information structures, such as research contributions comprising entities such as the tacked research problem, the approach, the evaluation, etc.</p>
<p>We leverage cognitive knowledge graphs for (a) fine-tuning existing base models with scholarly knowledge obtained from a cognitive knowledge graph and (b) injecting contextual knowledge from the CKG into the prompt in order to exploit additional context during inferencing.</p>
<p>We evaluate our approach with a comprehensive set of experiments with four different LLMs, viz.Llama 2 (7B and 13B model variants), Mistral (7B), and finally Gemini Pro.Our subsequent model evaluations follow a two-fold methodology: 1) automatic evaluations using GPT as an evaluator, and 2) manual evaluations using a human expert evaluator.We observe that tasks relying on sparse background knowledge such as from the scholarly domain significantly benefit from injecting contextual knowledge from a CKG into the prompt, especially the research field prediction task.</p>
<p>In summary, the contributions of this work comprise:</p>
<ol>
<li>the definition of the notion of cognitive knowledge graphs, which are capable of capturing contextual knowledge to bridge between neural and symbolic processing as well as human curation, 2. the conceptualization and implementation of a method for injecting contextual knowledge into prompting as well as fine-tuning, 3. a comprehensive empirical evaluation of the method with four different LLM with human and LLM assessment.</li>
</ol>
<p>Related Work</p>
<p>The related work is roughly categorized along the dimensions of Knowledge Graphs, Prompt Engineering for LLMs, and fine-tuning LLMs for specific tasks.</p>
<p>Knowledge Graphs</p>
<p>Knowledge graphs primarily focus on representing explicit facts and relationships between entities, often lacking the depth to capture complex, abstract concepts, or contextual aspects [Choudhary and Reddy, 2023].They have limitations in representing evolving or multifaceted domains due to challenges in dynamic information updates and integrating multidisciplinary knowledge.These limitations reduce the effectiveness especially for capturing the evolving nature of real-world systems and handling complex information domains, such as scholarly communication.Knowledge graphs are mostly universal and stationary distributed, which does not consider the dynamic nature of many realworld domains [Wang et al., 2023].Additionally, knowledge graphs face difficulties in updating information in real-time, which is required as domains evolve over time [Hou et al., 2023].Furthermore, integrating multidisciplinary knowledge into knowledge graphs is challenging, limiting their ability to represent diverse domains effectively [Peng et al., 2023].</p>
<p>With CKG, we address knowledge graph limitations with dynamic updates and multidisciplinary integration and also enhance their effectiveness in representing evolving or multifaceted domains.</p>
<p>Traditional Prompt Engineering: Effective But Limited</p>
<p>Traditional prompt engineering methods, such as Zero-Shot [Wei et al., 2021], Few-Shot [Brown et al., 2020],</p>
<p>Chain-of-Thought (CoT) [Wei et al., 2022], Tree-of-Thoughts (ToT) [Yao et al., 2023]  To address these issues within the scholarly domain, we propose a novel knowledge-driven prompt engineering approach.This approach injects query-specific context knowledge from a domain-specific CKG directly into the prompts.This not only bridges the knowledge capture gap but also alleviates the need for readily available domain experts and reduces the time required for crafting effective prompts.</p>
<p>Fine</p>
<p>Method</p>
<p>Our method comprises three core elements: the novel notion of cognitive knowledge graphs (CKGs), injecting queryspecific context knowledge from CKGs into the prompts, and finally fine-tuning LLMs with CKG knowledge.To illustrate our method, let us consider a scholarly article titled "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" [Raffel et al., 2020], for which we aim to perform the following two tasks:</p>
<p>(1) predicting the research field and (2) recommending a list of predicates.In this scenario, the CKG Open Research Knowledge Graph (ORKG) [Jaradeh et al., 2019] holds metadata and a CKG graphlet for this scholarly ar-  Exploring the Limits of...</p>
<p>Benchmark CNN</p>
<p>Machine learning T5</p>
<p>Rouge-1 Cnn daily mail  ticle1 .A visual representation of this specific graphlet is depicted in Figure 2. The graphlet is essentially a set of typed entities connected through properties such as hasTitle, followsMethodology, usesDataset, hasContribution, belongsToResearchField etc.Using ORKG as a CKG, we retrieve this graphlet via SPARQL.</p>
<p>Simultaneously, we improve the prompts by incorporating real-world context, leveraging abstracts from the CORE dataset.This enriches the LLM understanding of scholarly articles and their properties.For prompt engineering in this scholarly article, we use the existing CoT prompt framework, injecting it with the extracted query-specific context knowledge for the research field prediction task.We include the title, abstract, and research field hierarchy to guide the LLM.Additionally, we examine the impact of task-aware prefixes, such as "Research field prediction", to monitor the LLM for specific tasks.Integrating CKG graphlets into prompts enhances the LLM's performance for certain cases without needing additional fine-tuning.For instance, using the pretrained Llama 2 13b, this scholarly article can accurately be assigned to the research field of "machine learning".Furthermore, our methodology involves fine-tuning to optimize the LLM's performance, especially in scenarios where prompts lack query-and domain-specific knowledge for a specific task.In the case of this scholarly article, without fine-tuning, it is not possible to determine the most suitable domainspecific properties for describing the research contribution, such as usesTrainingCorpus, usesTokenization, hasNumberofParameters etc.Through CKG-based fine-tuning, like ORKG Llama 2 13b, the LLM can extract both domain-specific and domain-independent properties, including followsMethodology, usesDataset, hasContribution, usesTrainingCorpus, usesTokenization, hasNumberofParameters, hasModelFamily, hasLicense etc.This integrated approach demonstrates the effectiveness of our methodology in improving the LLM's performance across various scholarly tasks.</p>
<p>Cognitive Knowledge Graphs</p>
<p>Traditional knowledge graphs primarily focus on representing explicit facts and relationships between entities, often lacking the depth to capture complex, abstract concepts or contextual nuances.Additionally, they struggle with dynamic information updates and integrating multidisciplinary knowledge, limiting their effectiveness in representing evolving or multifaceted domains, such as scholarly communication.</p>
<p>Our concept of a Cognitive Knowledge Graph now builds upon traditional knowledge graphs by integrating an overlay structure that identifies and utilizes reusable patterns, referred to as graphlets.These patterns represent common cognitive information structures.The motivation for the development and use of CKGs in addition to traditional knowledge graphs is multifaceted.Among others, we aim for enhanced representation of complex concepts like research methodologies, arguments, or narratives.CKGs, with their overlay structures, can better represent these multifaceted and complex concepts.In particular, we envision CKGs to capture and represent the context in which information exists thus facilitating the moderation between machine and human intelligence.This is particularly important in domains like research, where under-standing the context (e.g., the problem addressed, methodologies used, and the nature of the conclusions) is crucial for accurate interpretation.</p>
<p>The base constituents of cognitive knowledge graphs are more complex fabrics of entity descriptions arranged according to certain patterns -graphlets.In network analysis and graph theory, the notions graphlet [Pržulj et al., 2004;Pržulj, 2007] and motif [Milo et al., 2002] were introduced to provide a structuring element between whole graphs and individual nodes and edges.Hence, in order to be able to effectively represent and manage more complex knowledge artefacts, we translate and apply the notion of graphlets to knowledge graphs.</p>
<p>Formally, a CKG graphlet is a tuple of sets of types (classes) and roles (properties) (C, P ), where:</p>
<ol>
<li>for each role p ∈ P , the domain (either explicitly defined or implicitly inferred from a concrete CKG) includes at least one of the types c ∈ C: domain(p) ⊂ C and 2. all types c ∈ C are connected via a property chain in P :
∀c 1 , c 2 ∈ C, ∃p 1 , ..., pj, ..., p n ∈ P : domain(p 1 ) = c 1 ∧range(p n ) = c 2 ∧ n−1 i=1 range(p i ) = domain(p i+1 )
Alternatively, we can also view CKG graphlets as (a) a special type of connected graph patterns (according to the SPARQL algebra), where variables occur in the positions of concrete instances and literals, or (b) as specific sets of SHACL shapes.</li>
</ol>
<p>Injecting Query-specific Context Knowledge into the Prompts</p>
<p>Our method is based on injecting query-specific context knowledge alongside task-specific context into LLMs prompts to empower them for tasks with sparse knowledge in the foundational models.We exploit the unique capabilities of CKGs, which represent knowledge not just as isolated facts but as interconnected patterns called graphlets.These graphlets can capture more complex concepts and nuanced relationships, providing a richer scaffolding for the LLM's reasoning process.Our rationale is to inject query-specific contexts and verified task-driven knowledge with a structured hierarchy from a CKG, such that when both are incorporated into the prompts, the LLMs can achieve increased performance on tasks with sparse prior knowledge or limited training without requiring further fine-tuning.</p>
<p>For this experiment, we utilized a multi-stage pipeline (cf. Figure 1) to inject query-specific context knowledge into the prompts:</p>
<ol>
<li>
<p>CKG Knowledge Extraction: We tap into the vast knowledge reservoir of a CKG, extracting relevant graphlets that align with the specific scholarly task.This goes beyond simple fact retrieval, capturing the intricate cognitive structures and relationships within the domain.reasoning, is enriched with the extracted CKG graphlets.This new step aims to craft prompts that not only guide the LLM's thought process but also equip it with the precise domain-specific knowledge needed for accurate and insightful responses.</p>
</li>
<li>
<p>Targeted Knowledge Retrieval: Precise SPARQL queries act as filters, sifting through the vast CKG and extracting only the most relevant information pertinent to each specific prompt.This ensures that the injected knowledge directly aligns with the task.</p>
</li>
</ol>
<p>Task-Aware Prefixes:</p>
<p>To explore the impact of task identification, we utilized optional prefixes attached to the knowledge-driven prompts.These prefixes explicitly signal the scholarly domain and task nature, thus potentially further enhancing the LLM's ability to focus its reasoning and deliver even more refined responses.</p>
<p>By systematically constructing this knowledge-driven pipeline, we aim to improve the performance of LLMs for complex tasks with sparse domain knowledge.Injecting query-specific context enriched with structured knowledge from CKGs empowers LLMs to achieve increased performance without the need for additional fine-tuning.This approach harnesses the power of LLMs while equipping them with the precise domain-specific knowledge they need to excel in challenging domains with sparse domain knowledge such as scholarly communication.</p>
<p>LLM Fine-tuning</p>
<p>In order to increase the LLM performance, we intertwine prompt knowledge injection and fine-tuning leveraging CKGs, specifically in scenarios where the LLM lacks domain-specific knowledge for a task.Enriching prompts with query-specific context is powerful, but for domainspecific tasks, injecting additional domain-specific knowledge through fine-tuning might further increase performance.This process not only supplements the LLM knowledge but integrates its interaction, expanding its capabilities, especially for domain-specific tasks.When faced with domain knowledge gaps, targeted fine-tuning with external domainspecific knowledge proves effective in unlocking the full potential of the LLM.Domain-specific knowledge from a knowledge base like CKG, verified by domain experts, equips the LLM for navigating domain-driven challenges.The LoRA [Hu et al., 2021] attention mechanism efficiently handles intricate details in domain-specific knowledge, ensuring focused involvement.To optimize the resource constraints, quantization [Xu et al., 2023] is utilized, and the TRL trainer [von Werra et al., 2020] orchestrates the fine-tuning process by setting optimal parameters to overcome learning challenges.</p>
<p>Experimental Setup</p>
<p>Dataset.Our dataset originates from a large-scale CKG, the Open Research Knowledge Graph (ORKG), encompassing diverse research fields and a comprehensive list of predicates.To enhance query-specific content injection and facilitate fine-tuning, we utilized 1,894 and 1,740 graphlets related to the research fields and a list of predicates (Table 1), respectively.This dataset creates a knowledge-rich environment for the LLMs, with a comprehensive understanding of various academic fields.Additionally, we incorporated 100 graphlets in the test set, each carefully curated to showcase a list of research fields and list of predicates.These graphlets encapsulate the essence of their respective fields, emphasizing key concepts with a list of predicates.The curated CKG graphlets dataset is employed to unveil the true potential of LLMs when faced with intricate scholarly tasks.The choice of 100 graphlets for the test set serves multiple purposes.It not only enables us to assess the LLM's performance through automated-generated metrics but also facilitates a rigorous human validation process [Shen et al., 2023] [Chew et al., 2023].By comparing the LLM's generated responses with the judgments of human evaluators, we gain a deeper understanding of its strengths and weaknesses.This cross-validation approach, integrating AI and human evaluation, ensures a comprehensive and granular assessment of the LLM's capabilities, ultimately paving the way for its further development and refinement.</p>
<p>Evaluation criteria The evaluation criteria encompass two primary dimensions: (1) LLM-based evaluation [Liusie et al., 2023] [Zhang et al., 2023] and (2) human-based evaluation [Wu andAji, 2023] [Shen et al., 2023].In LLM-based evaluation, the research field prediction is assessed based on clarity, coverage, relevance to the research field predicted by a domain expert, and granularity of the specified LLMgenerated research field, each scored within a range of 0-3.Similarly, the LLM's list of predicate recommendations is evaluated considering clarity, coverage, relevance for domain expert recommended contexts for a scholarly article, granularity, and the LLM's ability to recognize all contexts, each scored within the same 0-3 scale.Human-based evaluation mirrors LLM-based criteria, ensuring a parallel human assessment of clarity, coverage, relevance, and granularity in both research field categorization and a list of predicate recommendations.This approach aims to comprehensively evaluate the LLM's performance, combining automated metrics with human judgment to provide valuable insights and an in-depth understanding of its strengths and weaknesses in scholarly-related tasks.Tasks for Tuning This research explores two different approaches for injecting query-specific knowledge into prompts: task-independent and task-driven variants.By employing these two distinct prompt variants in our experiment, we assess the strengths and weaknesses associated with task-independent and task-driven approaches, analyzing how LLMs handle different graphlet structures.</p>
<p>• Task-Independent Prompts: We extract query-specific context from the CKG without imposing any taskrelated constraints.LLMs equipped with general knowledge can readily address diverse tasks.Additionally, the LLM undergoes fine-tuning with this general CKG knowledge, further enhancing its understanding of underlying concepts and relationships.• Task-Driven Prompts: In the second approach, we harness the power of CKGs to formulate task-specific prompts, explicitly incorporating task-oriented prefixes that guide the LLM's reasoning toward the desired outcome.For instance, in a research field prediction task, the prompt might commence with "Research field prediction" along with all the pertinent CKG-derived context.This targeted approach aims to improve the LLM's precision in applying its knowledge to the specific task.</p>
<p>Evaluation</p>
<p>We performed a comparative analysis of different prompt engineering approaches (Table 2) and LLM performance for both pre-trained (Table 3a) and fine-tuned (Table 3b) models across various tasks, which helps to clarify the capabilities and limitations of these LLMs regarding the CKG knowledge injection approach.The comparison of our Injection of Query-specific Context Knowledge (IQCK) method with four baseline methods (for a sample set of the CKG graphlets) for the research field prediction task (RF) with Mistral 7b is shown in Table 2.For the baseline methods, we tried different prompting styles but omitted the injection of a research field hierarchy to choose predictions from.We can observe, that IQCK with the injection of the research field hierarchies into the prompt as contextual knowledge significantly improves the accuracy of the research field prediction to 76% from the 67-73% achieved with the baseline methods.</p>
<p>The evaluation of pre-trained LLMs (Table 3a), including Gemini pro (without and with Prefix) and Mistral 7b (without and with Prefix) shows impressive mean scores, signifying their capabilities in capturing and leveraging queryspecific context knowledge from within the prompts.Specifically, Gemini pro (with Prefix) achieved a remarkable mean score of 80% (GPT4 evaluation), positioning it as a top performer in this task.Mistral 7b (without Prefix) follows closely with a substantial mean score of 79% (GPT4 evaluation) and 60% (human evaluation), demonstrating robust  competency.The models Llama 2 13b (without and with Prefix) and Llama 2 7b (without and with Prefix) achieve lower mean average scores than others by the GPT4 evaluation and are similarly scored by human evaluation.This pattern indicates that the incorporation of injecting queryspecific context knowledge during the prompt engineering phase provides these LLMs with a solid foundation for understanding and recommending research fields.</p>
<p>The evaluation of fine-tuned LLMs (Table 3b) in the research field prediction reveals different dynamics.For instance, ORKG Llama 2 13b (With Prefix) achieves a mean score of 49% (GPT4 evaluation), showing competitive performance with regard to other fine-tuned LLMs but falling short from the previous approach.Similarly, ORKG Llama 13b (Without Prefix) and ORKG Mistral 7B (With Prefix) achieve mean scores of 42% (GPT4 evaluation) and 46% (GPT4 evaluation), respectively, suggesting a noteworthy but comparatively lower proficiency.This variance scores between pre-trained and fine-tuned LLMs underscores the substantial impact of injecting query-specific context knowledge in the pre-train LLMs for research field prediction tasks.</p>
<p>For the list of predicate recommendations, Llama 2 13b (with Prefix) achieved a mean score of 65% (GPT4 evaluation) and 60% (human evaluation), emphasizing its proficiency in suggesting predicate labels.Llama 2 7b (With Prefix) follows closely with a mean score of 63% (GPT4 evaluation), while Llama 2 13b (With Prefix) and Mistral 7b (With Prefix) achieve mean scores of 63% (GPT4 evaluation) and 64% (GPT4 evaluation) which is also quite similar to a human evaluation, respectively.</p>
<p>Regarding the fine-tuned LLMs in the list of predicates recommendation unveils a more diverse scenario.ORKG Llama 2 13b (Without Prefix) achieves 67% of the mean average score, suggesting that fine-tuning can yield competitive results in this task.However, ORKG Llama 13b (With Prefix) and ORKG Mistral 7B (With Prefix) experience lower mean scores, with 57% and 52%, respectively.This variability in performance indicates that the effectiveness of fine-tuned models in predicate label recommendation is more contingent on specific training conditions, and adding a prefix seems to introduce additional complexity.</p>
<p>This comprehensive analysis establishes a solid foundation for LLMs, particularly in the scholarly domain for tasks like research field prediction and list of predicate recommendations.The injection of query-specific context knowledge into the prompts from CKG improves the LLM performance and also provides a robust understanding of complex relationship prompts and LLMs.Fine-tuned LLMs showcase competitive performance, especially for list of predicate recommendations, highlighting the relation between the LLMs and finetuned requirements.</p>
<p>Limitations and Future Work</p>
<p>In order to evaluate our approach, we utilized a domain expert to verify data using the previously described approach.</p>
<p>Although the domain expert verified data contains human curated data, it can be misleading to determine to performance of our models.For the research field prediction task, the domain expert verified data only includes a single field per article.In the case of interdisciplinary fields, still, only a single field is contained.Consequently, the LLM recommendation can be correct based on the contents of the title and abstract but is evaluated as incorrect because of the domain expertverified data.For the list of predicate recommendations, the domain expert verified data only includes a subset of relevant predicates, since knowledge descriptions in the CKG are generally incomplete.Therefore, an LLM-recommended list of properties might be relevant based on the title and abstract, but evaluated as incorrect when comparing them to the domain expert-verified data.</p>
<p>Furthermore, there are several limitations regarding the human evaluation of the data.The evaluators were instructed on how to score the outcomes, but only high-level guidelines were provided.Therefore, scores might vary between different articles.Despite this shortcoming, the human evaluation still provides valuable insights, especially in comparison with the machine evaluation.Another aspect of the human evaluation is the heterogeneous set of research domains.Since our approach does not focus on specific domains, the set of selected articles comes from a variety of domains.The human evaluators were not experts in all domains and therefore had to judge the results to the best of their ability.However, we believe that a high-level assessment of the resulting LLM responses is also possible without domain knowledge, albeit in lesser granularity.</p>
<p>In addition to the two tasks performed in our approach, we plan to extend this to various other tasks in future work.In addition to the prediction of predicates, we explored the prediction of objects to form complete contribution description facts.However, further work is required to perform this task at the desired accuracy.In the future, we also plan to continue working on this specific task by leveraging other approaches, such as fine-tuning the LLMs for each specific task and increasing the data from the CKG which needs to be collected from the domain experts.</p>
<p>Conclusion</p>
<p>This work is part of a larger research agenda aiming at creating a comprehensive neuro-symbolic system for describing research contributions in a cognitive knowledge graph ultimately giving rise to novel AI-based research assistance systems, which enable researchers to obtain comprehensive answers to research questions based on the recent corpus of scientific knowledge.Due to the limitations of LLMs in extracting information in domains with sparse training data, we developed two methods for (1) injecting contextual information into prompts from a preexisting CKG and (2) fine-tuning LLMs with such knowledge for specific tasks.Our evaluation showed, that in particular the first method is very well suited to improve the accuracy of LLMs for scholarly communication tasks.More research is required to further improve the fine-tuning methods and expand the evaluation of the approach to further knowledge extraction and augmentation tasks, such as object prediction.</p>
<p>Figure 1 :
1
Figure 1: Overview on the method for knowledge augmentation and discovery comprising context knowledge prompt injection and finetuning leveraging a Cognitive Knowledge Graph.</p>
<p>Figure 2 :
2
Figure 2: Example of a graphlet retrieved from the ORKG, displaying a scholarly article, the metadata, and the respective properties and entities (simplified version).</p>
<p>Table 1 :
1
2. Contextual Grounding: We leverage abstracts from the CORE dataset [Knoth et al., 2023] as factual summaries, providing rich contextual grounding for the subsequent prompt construction.3. Knowledge-Infused CoT Prompts: The existing CoT prompt framework, a proven way to guide the LLM's Evaluation dataset comprising research field annotations and predicates for research contribution descriptions.
Train Data Test DataResearch Field1,894100List of Predicates1,740100Prompt StyleMAS (RF)Zero-Shot Prompting67%Few-Shot Prompting68%Chain-of-Thought Prompting69%Zero-shot COT Prompting73%IQCK into COT Prompting76%</p>
<p>Table 2 :
2
Comparison of our Injecting Query-specific Context Knowledge (IQCK) method with four baseline methods (for a sample set of the CKG graphlets) for the research field prediction (RF) Mistral 7b.CoT shares the same input as Few-Shot, with the only difference being the inclusion of additional examples; MAS = Mean Average Score.</p>
<p>Table 3 :
3
AI (gpt-4-1106-preview)Perspective on LLM (Pre-trained and Fine-tuned) Evaluation; MAS = Mean Average Score; LP = List of predicates recommendation, RF = Research field prediction.(a)Table2.1:AIPerspective on LLM (Pre-trained) Evaluation.Table2.2:AI Perspective on LLM (Fine-tuned) Evaluation.
TaskPrefixLLMMAS(b) TaskPrefixLLMMASGemini pro64%ORKG Llama 13b 67%LPWithout PrefixLlama 2 13b 64% Mistral 7b 64%LPWithout PrefixORKG Llama 2 7b 42% ORKG Mistral 7B 42%Llama 2 7b59%ORKG Llama 13b 57%Llama 2 13b 65%LPWith PrefixORKG Mistral 7B52%LPWith PrefixMistral 7b Llama 2 7b Gemini pro64% 63% 62%RFWithout PrefixORKG Llama 2 7b 37% ORKG Llama 2 7b 42% ORKG Mistral 7B 38%Mistral 7b79%ORKG Llama 13b13%RFWithout PrefixGemini pro Llama 2 13b 62% 78% Llama 2 7b 50%RFWith PrefixORKG Llama 13b 49% ORKG Mistral 7B 46% ORKG Llama 2 7b 19%Gemini pro80%RFWith PrefixMistral 7b Llama 2 13b 61% 77%Llama 2 7b43%</p>
<p>Table 4 :
4
Human Perspective on LLM (Pre-trained and Fine-tune) Evaluation; MAS = Mean Average Score; LP = List of predicates recommendation, RF = Research field prediction.(a)Table3.1:Human Perspective on LLM (Pre-trained) Evaluation.
(b) Table 3.2: Human Perspective on LLM (Fine-trained) Evaluation.TaskPrefixLLMMASTaskPrefixLLMMASLlama 2 13b 60%ORKG Mistral 7B 53%LPWith PrefixMistral 7b Llama 2 7b60% 59%LPWith PrefixORKG Llama 13b ORKG Llama 2 7b 45% 50%Gemini pro57%ORKG Llama 13b 60%Llama 2 13b 60%LPWithout PrefixORKG Llama 2 7b 47%LPWithout PrefixLlama 2 7b Mistral 7b Gemini pro59% 59% 58%RFWith PrefixORKG Mistral 7B ORKG Llama 13b 50% 46% ORKG Mistral 7B 48%Gemini pro78%ORKG Llama 2 7b 33%RFWith PrefixMistral 7b Llama 2 13b 70% 78% Llama 2 7b 49%RFWithout PrefixORKG Mistral 7B 55% ORKG Llama 2 7b 41% ORKG Llama 13b 18%Mistral 7b82%RFWithout PrefixGemini pro Llama 2 13b 66% 75%Llama 2 7b56%
https://orkg.org/paper/R161808</p>
<p>Limits for learning with language models. Asher, arXiv:2306.122132023. 2023. 2020. 202033Tom Brown, Benjamin Mann, Nick Ry-arXiv preprintLanguage models are few-shot learners. Advances in neural information processing systems</p>
<p>Llm-assisted content analysis: Using large language models to support deductive coding. Chew, arXiv:2306.14924arXiv:2305.01157Complex logical reasoning over knowledge graphs using large language models. 2023. 2023. 2023arXiv preprintChoudhary and Reddy, 2023</p>
<p>From answers to insights: Unveiling the strengths and limitations of chatgpt and biomedical knowledge graphs. medRxiv. D' Souza, arXiv:2305.13264arXiv:2304.01933Proceedings of the 10th International Conference on Knowledge Capture. Mohamad Yaser, Jaradeh , Allard Oelen, Kheir Eddine Farfar, Manuel Prinz, D' Jennifer, Gábor Souza, Markus Kismihók, Sören Stocker, Auer, the 10th International Conference on Knowledge CaptureSpringer2023. 2023. 2023. 2023. 2023. 2021. 2021. 2023. 2019. 2019arXiv preprintInternational Conference on Database and Expert Systems Applications</p>
<p>Prefix propagation: Parameterefficient tuning for long sequences. Knoth Jungherr, arXiv:2305.12086arXiv:2307.07889Adian Liusie, Potsawee Manakul, and Mark JF Gales. Zero-shot nlg evaluation through pairware comparisons with llms. Feng Xia, Mehdi Naseriparsa, Francesco Osborne, Ciyuan Peng2023. 2023. 2023. June 2023. 2023. 2023. 2023. 2023. 2023. 2023. 2002. 2002. 2023. 2023. 200410arXiv preprintNature Scientific Data. Pržulj et al., 2004] N. Pržulj, D. G. Corneil, and I. Jurisica. Modeling interactome: scale-free or geometric? Bioinformatics</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Nataša Pržulj, ; Pržulj, Raffel, The Journal of Machine Learning Research. 2322007. 01 2007. 2020. 2020Bioinformatics</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Shen, arXiv:2305.13091arXiv:2309.14717Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation biases for large language models. Wu and Aji2023. 2023. 2020. 2020. 2023. 2023. 2021. 2021. 2022. 2022. 2023. 2023. 202335arXiv preprintQa-lora: Quantization-aware low-rank adaptation of large language models</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Yao, arXiv:2305.10601arXiv:2307.07705Zhengyan Zhang, and Maosong Sun. Cpet: Effective parameter-efficient tuning for compressed large language models. 2023. 2023. 2023. 2023. 20232arXiv preprintProceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8520 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8520</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8520</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-2069aaaa281eb13bcd9330fc4d43f24f6b436a53</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2069aaaa281eb13bcd9330fc4d43f24f6b436a53" target="_blank">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> DSPy is introduced, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules, and a compiler is designed that will optimize any DSPy pipeline to maximize a given metric.</p>
                <p><strong>Paper Abstract:</strong> The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded"prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8520",
    "paper_id": "paper-2069aaaa281eb13bcd9330fc4d43f24f6b436a53",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0058255,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DSPY: COMPILING DECLARATIVE LANGUAGE Model Calls into Self-Improving Pipelines</h1>
<p>Omar Khattab, ${ }^{1}$ Arnav Singhvi, ${ }^{2}$<br>Paridhi Maheshwari, ${ }^{4}$ Zhiyuan Zhang, ${ }^{1}$<br>Keshav Santhanam, ${ }^{1}$ Sri Vardhamanan, ${ }^{6}$ Saiful Haq, ${ }^{6}$<br>Ashutosh Sharma, ${ }^{6}$ Thomas T. Joshi, ${ }^{7}$ Hanna Moazam, ${ }^{8}$<br>Heather Miller, ${ }^{3,9}$ Matei Zaharia, ${ }^{2}$ Christopher Potts ${ }^{1}$<br>${ }^{1}$ Stanford University, ${ }^{2}$ UC Berkeley, ${ }^{3}$ Carnegie Mellon University, ${ }^{4}$ Amazon Alexa AI, ${ }^{5}$ Dashworks Technologies, Inc., ${ }^{6}$ IIT Bombay, ${ }^{7}$ Calera Capital, ${ }^{8}$ Microsoft, ${ }^{9}$ Two Sigma Investments<br>okhattab@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computation graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multihop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to selfbootstrap pipelines that outperform standard few-shot prompting (generally by over $25 \%$ and $65 \%$, respectively) and pipelines with expert-created demonstrations (by up to $5-46 \%$ and $16-40 \%$, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy.</p>
<h2>1 INTRODUCTION</h2>
<p>Language models (LMs) are enabling researchers to build NLP systems at higher levels of abstraction and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an exploding space of "prompting" techniques-and lightweight finetuning techniques-for adapting LMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022; Wang et al., 2022b), and augmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al., 2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these techniques are explored in isolation, but interest has been growing in building multi-stage pipelines and agents that decompose complex tasks into more manageable calls to LMs in an effort to improve performance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot et al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza \&amp; Rafiei, 2023; Shinn et al., 2023).</p>
<p>Unfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is exacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM</p>
<p>calls in existing LM pipelines and in popular developer frameworks are generally implemented using hard-coded 'prompt templates', that is, long strings of instructions and demonstrations that are hand crafted through manual trial and error. We argue that this approach, while pervasive, can be brittle and unscalable-conceptually akin to hand-tuning the weights for a classifier. A given string prompt might not generalize to different pipelines or across different LMs, data domains, or even inputs.</p>
<p>Toward a more systematic approach to designing AI pipelines, we introduce the DSPy programming model. ${ }^{1}$ DSPy pushes building new LM pipelines away from manipulating free-form strings and closer to programming (composing modular operators to build text transformation graphs) where a compiler automatically generates optimized LM invocation strategies and prompts from a program. We draw inspiration from the consensus that emerged around neural network abstractions (Bergstra et al., 2013), where (1) many general-purpose layers can be modularly composed in any complex architecture and (2) the model weights can be trained using optimizers instead of being hand-tuned.</p>
<p>To this end, we propose the DSPy programming model (Sec 3). We first translate string-based prompting techniques, including complex and task-dependent ones like Chain of Thought (Wei et al., 2022) and ReAct (Yao et al., 2022), into declarative modules that carry natural-language typed signatures. DSPy modules are task-adaptive components-akin to neural network layers-that abstract any particular text transformation, like answering a question or summarizing a paper. We then parameterize each module so that it can learn its desired behavior by iteratively bootstrapping useful demonstrations within the pipeline. Inspired directly by PyTorch abstractions (Paszke et al., 2019), DSPy modules are used via expressive define-by-run computational graphs. Pipelines are expressed by (1) declaring the modules needed and (2) using these modules in any logical control flow (e.g., if statements, for loops, exceptions, etc.) to logically connect the modules.</p>
<p>We then develop the DSPy compiler (Sec 4), which optimizes any DSPy program to improve quality or cost. The compiler inputs are the program, a few training inputs with optional labels, and a validation metric. The compiler simulates versions of the program on the inputs and bootstraps example traces of each module for self-improvement, using them to construct effective few-shot prompts or finetuning small LMs for steps of the pipeline. Optimization in DSPy is highly modular: it is conducted by teleprompters, ${ }^{2}$ which are general-purpose optimization strategies that determine how the modules should learn from data. In this way, the compiler automatically maps the declarative modules to high-quality compositions of prompting, finetuning, reasoning, and augmentation.</p>
<p>Programming models like DSPy could be assessed along many dimensions, but we focus on the role of expert-crafted prompts in shaping system performance. We are seeking to reduce or even remove their role through DSPy modules (e.g., versions of popular techniques like Chain of Thought) and teleprompters. We report on two expansive case studies: math word problems (GMS8K; Cobbe et al. 2021) and multi-hop question answering (HotPotQA; Yang et al. 2018) with explorations of chain of thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and agent loops. Our evaluations use a number of different compiling strategies effectively and show that straightforward DSPy programs outperform systems using hand-crafted prompts, while also allowing our programs to use much smaller and hence more efficient LMs effectively.</p>
<p>Overall, this work proposes the first programming model that translates prompting techniques into parameterized declarative modules and introduces an effective compiler with general optimization strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contributions are empirical and algorithmic: with DSPy, we have found that we can implement very short programs that can bootstrap self-improving multi-stage NLP systems using LMs as small as llama2-13b-chat and T5-Large ( 770 M parameters). Without hand-crafted prompts and within minutes to tens of minutes of compiling, compositions of DSPy modules can raise the quality of simple programs from $33 \%$ to $82 \%$ (Sec 6) and from $32 \%$ to $46 \%$ (Sec 7) for GPT-3.5 and, similarly, from $9 \%$ to $47 \%$ (Sec 6) and from $22 \%$ to $41 \%$ (Sec 7) for llama2-13b-chat.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 Related Work</h1>
<p>This work is inspired by the role that Torch (Collobert et al., 2002), Theano (Bergstra et al., 2010; 2011; Al-Rfou et al., 2016), Chainer (Tokui et al., 2015), and others played in the development in deep learning by providing powerful abstractions. A similar transformation is emerging with higherlevel pipelines of LMs, and we are seeking to offer a solid conceptual framework and programming abstractions for what we call foundation model programming. We draw on differentiable programming (Wang et al., 2018) but applied to LM calls rather than neural networks, and borrow syntactic elements from PyTorch (Paszke et al., 2019).</p>
<p>In-context learning (McCann et al. 2018; Radford et al. 2018; Brown et al. 2020) is a key mechanism for foundation model programming. A growing body of work has revealed that, especially with instruction tuning (Ouyang et al., 2022), we can elicit sophisticated behavior via prompting (Wei et al., 2022; Wang et al., 2022b; Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Madaan et al., 2023). Similarly, forms of weak supervision that would normally require task-specific (Khattab et al., 2021a;b) or hand-built (Ratner et al., 2016; Hancock et al., 2018) heuristics are now done by LMs (Wang et al., 2022b; Zelikman et al., 2022; Zhang et al., 2022; Shao et al., 2023).</p>
<p>In-context learning methods now routinely invoke tools, leading to LM pipelines that use retrieval models (Chen et al., 2017; Lewis et al., 2020; Guu et al., 2020; Lazaridou et al., 2022; Izacard et al., 2022), multimodal foundation models, and more traditional tools like APIs (Nakano et al., 2021) and calculators. A number of toolkits have been developed to facilitate this, including LangChain (Chase, 2022), Semantic Kernel (Microsoft, 2023), LlamaIndex (Liu, 2022), and many other retrieval and agent libraries. These toolkits provide pre-packaged chains and agents that connect LMs with numerous accessible tools. However, they suffer from the pervasive prompt engineering challenges we address in DSPy: they express task-specific behavior through hand-written prompt templates (for detailed discussion, see Appendix B).</p>
<p>Researchers are starting to apply discrete optimization and RL to find effective prompts, generally for a single logical LM call (Guo et al., 2023; Pryzant et al., 2023; Huang et al., 2022; Yang et al., 2023). DSPy seeks to generalize this space: it offers a rich framework for optimizing arbitrary pipelines from high-level declarative signatures, by bootstrapping high-quality multi-stage demonstrations with constraints. In this framework, DSPy teleprompters may apply optimization using model selection techniques like cross-validation or, in principle, with sophisticated techniques involving RL and LM feedback (Hu et al., 2023; Zhao et al., 2023a; Shinn et al., 2023) or learned or Bayesian hyperparameter optimization methods (Bergstra et al., 2013; Akiba et al., 2019).</p>
<p>The present paper seeks to motivate DSPy as a programming model and to report new empirical findings from applying the DSPy compiler. This is inspired by formative work by Bergstra et al. (2010; 2013), Paszke et al. (2019), and Wolf et al. (2020), who support their respective programming models with a mix of benchmark numbers and some qualitative measures. For the current paper, we focus on showing that DSPy and its compiler allow us to build outstanding LM systems without hand-crafted prompt strings, but instead from truly modular units, and that this opens up doors for systematically exploring a rich design space at a very high programmatic level of abstraction.</p>
<h2>3 The DSPy Programming Model</h2>
<p>We present DSPy, which treats LMs as abstract devices for text generation, ${ }^{3}$ and optimizes their usage in arbitrary computational graphs. DSPy programs are expressed in Python: each program takes the task input (e.g., a question to answer or a paper to summarize) and returns the output (e.g., an answer or a summary) after a series of steps. DSPy contributes three abstractions toward automatic optimization: signatures, modules, and teleprompters. Signatures abstract the input/output behavior of a module; modules replace existing hand-prompting techniques and can be composed in arbitrary pipelines; and teleprompters optimize all modules in the pipeline to maximize a metric.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.1 Natural Language Signatures Can abstract Prompting \&amp; finetuning</h1>
<p>Instead of free-form string prompts, DSPy programs use natural language signatures to assign work to the LM. A DSPy signature is natural-language typed declaration of a function: a short declarative spec that tells DSPy what a text transformation needs to do (e.g., "consume questions and return answers"), rather than how a specific LM should be prompted to implement that behavior. More formally, a DSPy signature is a tuple of input fields and output fields (and an optional instruction). A field consists of field name and optional metadata. ${ }^{4}$ In typical usage, the roles of fields are inferred by DSPy as a function of field names. For instance, the DSPy compiler will use in-context learning to interpret question differently from answer and will iteratively refine its usage of these fields.</p>
<p>Signatures offer two benefits over prompts: they can be compiled into self-improving and pipelineadaptive prompts or finetunes. This is primarily done by bootstrapping (Sec 4) useful demonstrating examples for each signature. Additionally, they handle structured formatting and parsing logic to reduce (or, ideally, avoid) brittle string manipulation in user programs.</p>
<p>In practice, DSPy signatures can be expressed with a shorthand notation like question -&gt; answer, so that line 1 in the following is a complete DSPy program for a basic question-answering system (with line 2 illustrating usage and line 3 the response when GPT-3.5 is the LM):</p>
<div class="codehilite"><pre><span></span><code><span class="o">:</span><span class="n">qa</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dspy</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="s">&quot;question -&gt; answer&quot;</span><span class="p">)</span>
<span class="o">:</span><span class="n">qa</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s">&quot;Where is Guarani spoken?&quot;</span><span class="p">)</span>
<span class="o">:</span><span class="err">#</span><span class="w"> </span><span class="n">Out</span><span class="o">:</span><span class="w"> </span><span class="n">Prediction</span><span class="p">(</span><span class="n">answer</span><span class="o">=</span><span class="s">&#39;Guarani is spoken mainly in South America.&#39;</span><span class="p">)</span>
</code></pre></div>

<p>In the shorthand notation, each field's name indicates the semantic role that the input (or output) field plays in the transformation. DSPy will parse this notation and expand the field names into meaningful instructions for the LM, so that english_document -&gt; french_translation would prompt for English to French translation. When needed, DSPy offers more advanced programming interfaces for expressing more explicit constraints on signatures (Appendix A).</p>
<h3>3.2 PARAMETERIZED \&amp; TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES</h3>
<p>Akin to type signatures in programming languages, DSPy signatures simply define an interface and provide type-like hints on the expected behavior. To use a signature, we must declare a module with that signature, like we instantiated a Predict module above. A module declaration like this returns a function having that signature.</p>
<p>The Predict Module The core module for working with signatures in DSPy is Predict (simplified pseudocode in Appendix D.1). Internally, Predict stores the supplied signature, an optional LM to use (initially None, but otherwise overrides the default LM for this module), and a list of demonstrations for prompting (initially empty). Like layers in PyTorch, the instantiated module behaves as a callable function: it takes in keyword arguments corresponding to the signature input fields (e.g., question), formats a prompt to implement the signature and includes the appropriate demonstrations, calls the LM, and parses the output fields. When Predict detects it's being used in compile mode, it will also internally track input/output traces to assist the teleprompter at bootstrapping the demonstrations.</p>
<p>Other Built-in Modules DSPy modules translate prompting techniques into modular functions that support any signature, contrasting with the standard approach of prompting LMs with task-specific details (e.g., hand-written few-shot examples). To this end, DSPy includes a number of more sophisticated modules like ChainOfThought, ProgramOfThought, MultiChainComparison, and ReAct. ${ }^{5}$ These can all be used interchangeably to implement a DSPy signature. For instance, simply chang-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ing Predict to ChainOfThought in the above program leads to a system that thinks step by step before committing to its output field.</p>
<p>Importantly, all of these modules are implemented in a few lines of code by expanding the userdefined signature and calling Predict one or more times on new signatures as appropriate. For instance, we show a simplified implementation of the built-in ChainOfThought below.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">ChainOfThought</span>(<span class="n">dspy</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="nb">signature</span>):
        <span class="c1"># Modify signature from &#39;*inputs -&gt; *outputs&#39; to &#39;*inputs -&gt; rationale, *outputs&#39;.</span>
        <span class="n">rationale_field</span> = <span class="n">dspy</span>.<span class="n">OutputField</span>(<span class="nb">prefix</span>=<span class="s">&quot;Reasoning: Let&#39;s think step by step.&quot;</span>)
        <span class="nb">signature</span> = <span class="n">dspy</span>.<span class="nb">Signature</span>(<span class="nb">signature</span>).<span class="n">prepend_output_field</span>(<span class="n">rationale_field</span>)
        <span class="c1"># Declare a sub-module with the modified signature.</span>
        <span class="nb">self</span>.<span class="n">predict</span> = <span class="n">dspy</span>.<span class="n">Predict</span>(<span class="nb">signature</span>)
    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, **<span class="n">kwargs</span>):
        <span class="c1"># Just forward the inputs to the sub-module.</span>
        <span class="k">return</span> <span class="nb">self</span>.<span class="n">predict</span>(**<span class="n">kwargs</span>)
</code></pre></div>

<p>This is a fully-fledged module capable of learning effective few-shot prompting for any LM or task. We contrast that with Appendix C, which copies long reasoning prompts hand-written by sources ranging from recent research to popular prompting libraries.</p>
<p>Parameterization Uniquely, DSPy parameterizes these prompting techniques. To understand this parameterization, observe that any LM call seeking to implement a particular signature needs to specify parameters that include: (1) the specific LM to call (Chen et al., 2023), (2) the prompt instructions (Yang et al., 2023) and the string prefix of each signature field and, most importantly, (3) the demonstrations used as few-shot prompts (for frozen LMs) or as training data (for finetuning). We focus primarily on automatically generating and selecting useful demonstrations. In our case studies, we find that bootstrapping good demonstrations gives us a powerful way to teach sophisticated pipelines of LMs new behaviors systematically.</p>
<p>Tools DSPy programs may use tools, which are modules that execute computation. We support retrieval models through a dspy. Retrieve module. At the time of writing, DSPy has built-in support for ColBERTv2, Pyserini, and Pinecone retrievers, and we have explored experimental dspy.SQL for executing SQL queries and dspy. PythonInterpreter for executing Python code in a sandbox.</p>
<p>Programs DSPy modules can be composed in arbitrary pipelines in a define-by-run interface. Inspired directly by PyTorch and Chainer, one first declares the modules needed at initialization, allowing DSPy to keep track of them for optimization, and then one expresses the pipeline with arbitrary code that calls the modules in a forward method. As a simple illustration, we offer the following simple but complete retrieval-augmented generation (RAG) system.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">RAG</span>(<span class="n">dspy</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">num_p</span> <span class="n">passages</span>=<span class="mi">3</span>):
        <span class="c1"># &#39;Retrieve&#39; will use the user&#39;s default retrieval settings unless overriden.</span>
        <span class="nb">self</span>.<span class="n">retrieve</span> = <span class="n">dspy</span>.<span class="n">Retrieve</span>(<span class="n">k</span>=<span class="n">num_passages</span>)
        <span class="c1"># &#39;ChainOfThought&#39; with signature that generates answers given retrieval &amp; question.</span>
        <span class="nb">self</span>.<span class="n">generate_answer</span> = <span class="n">dspy</span>.<span class="n">ChainOfThought</span>(<span class="s">&quot;context, question -&gt; answer&quot;</span>)
    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="n">question</span>):
        <span class="n">context</span> = <span class="nb">self</span>.<span class="n">retrieve</span>(<span class="n">question</span>).<span class="n">passages</span>
        <span class="k">return</span> <span class="nb">self</span>.<span class="n">generate_answer</span>(<span class="n">context</span>=<span class="n">context</span>, <span class="n">question</span>=<span class="n">question</span>)
</code></pre></div>

<p>To highlight modularity, we use ChainOfThought as a drop-in replacement of the basic Predict. One can now simply write RAG()("Where is Guaran√≠ spoken?") to use it. Notice that, if we use a signature "context, question -&gt; search_query", we get a system that generates search queries rather than answers.</p>
<h1>3.3 TELEPROMPTERS CAN AUTOMATE PROMPTING FOR ARBITRARY PIPELINES</h1>
<p>When compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that takes the program, a training set, and a metric-and returns a new optimized program. Different teleprompters (Sec 4) apply different strategies for optimization.</p>
<p>In DSPy, training sets may be small, potentially a handful of examples, though larger data enables more powerful optimization. Training examples may be incomplete, i.e., only input values are necessary. Labels for the pipeline steps are not required, unless they need to be used in the metric. In practice, we typically assume labels only for (at most) the program's final output, not the intermediate steps. This label-efficiency is critical for modularity: building a new pipeline in DSPy requires simply recompiling the new pipeline's code, not annotating data specific to the new pipeline.</p>
<p>Metrics can be simple notions like exact match (EM) or F1, but they can be entire DSPy programs that balance multiple concerns. For example, we may compile the RAG module above against a dataset of question-answer pairs qa_trainset and the metric EM. The goal of optimization here is to effectively bootstrap few-shot demonstrations. The following code achieves this:</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Small training set with only questions and final answers.
qa_trainset = [dspy.Example(question=&quot;What is the capital of France?&quot;, answer=&quot;Paris&quot;)]
<span class="gh">#</span> The teleprompter will bootstrap missing labels: reasoning chains and retrieval contexts.
teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)
compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)
</code></pre></div>

<p>In this example, the BootstrapFewShot teleprompter (Sec 4, Appendix E.1) simulates RAG on the training example(s). It will collect demonstrations of each module (i.e., examples of its input-output behavior) that collectively lead to valid output (i.e., respecting the signatures and the metric).</p>
<p>If one wanted to push the compiled program to be extractive given its retrieved contexts, one could define a custom metric to use in place of dspy.evaluate.answer_exact_match:</p>
<div class="codehilite"><pre><span></span><code>def answer_and_context_match(example, pred, trace=None):
    answer_match = dspy.evaluate.answer_exact_match(example, pred)
    # Is the prediction a substring of some passage?
    context_match = any((pred.answer.lower() in c) for c in pred.context)
    return answer_match and context_match
</code></pre></div>

<p>Notice that behavior like this might be more accurately checked by another DSPy program that checks for faithful grounding of answers. Such metrics are fully supported and encouraged in DSPy.
Teleprompters can be composed by specifying a teacher program. DSPy will sample demonstrations from this program for prompt optimization. This composition can enable very rich pipelines, where expensive programs (e.g., complex expensive ensembles using large LMs) supervise cheap programs (e.g., simple pipelines using smaller LMs). One may start with compiled_rag from above (say, compiled to use a large Llama2-13b-chat LM) but now fine-tune Flan-T5-large to create an efficient program:</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Larger set of questions with *no labels*. Labels for all steps will be bootstrapped.
unlabeled_questions = [dspy.Example(question=&quot;What is the capital of Germany?&quot;), ...]
<span class="gh">#</span> As we assumes no answer, we use &#39;answer_passage_match&#39; to filter ungrounded answers.
finetuning_teleprompter = BootstrapFinetune(metric=dspy.evaluate.answer_passage_match)
<span class="gh">#</span> We set &#39;teacher=compiled_rag&#39; to compose. Bootstrapping will now use &#39;compiled_rag&#39;.
compiled_rag_via_finetune = finetuning_teleprompter.compile(RAG(), teacher=compiled_rag,
    trainset=unlabeled_questions, target=&#39;google/flan-t5-large&#39;)
</code></pre></div>

<h1>4 THE DSPY COMPILER</h1>
<p>A key source of DSPy's expressive power is its ability to compile-or automatically optimize-any program in this programming model. Compiling relies on a teleprompter, which is an optimizer for DSPy programs that improves the quality (or cost) of modules via prompting or finetuning, which are unified in DSPy. While DSPy does not enforce this when creating new teleprompters, typical teleprompters go through three stages.</p>
<p>Stage 1: Candidate Generation The compiler first (recursively) finds all unique Predict modules (predictors) in a program, including those nested under other modules. For each unique predictor $p$, the teleprompter may generate candidate values for the parameters of $p$ : the instructions, field descriptions, or-most importantly-demonstrations (i.e., example input-output pairs). In this iter-</p>
<p>ation of DSPy, we focus on demonstrations and find that simple rejection-sampling-like approaches can help bootstrap highly effective multi-stage systems.</p>
<p>Consider the simplest non-trivial teleprompter in DSPy, BootstrapFewShot (simplified pseudocode in Appendix E.1). This teleprompter will simulate a teacher program (or, if unset, the zero-shot version of the program being compiled) on some training inputs, possibly one or more times with a high temperature. When running in compile mode, multi-stage traces are tracked transparently and in a thread-safe fashion throughout execution. The program's metric is used to filter for multistage traces that together help the pipeline pass the metric. We thus obtain potential labels for all signatures in the program by throwing away the bad examples and using the good examples as potential demonstrations, though these design decisions are under user control.</p>
<p>While LMs can be highly unreliable, we find they can be rather efficient at searching the space of solutions for multi-stage designs. A well-decomposed program can typically find at least a few training examples where the LM can pass the constraints enforced by the signatures and metrics, allowing us to bootstrap iteratively if needed.</p>
<p>Stage 2: Parameter Optimization Now each parameter has a discrete set of candidates: demonstrations, instructions, etc. Many hyperparameter tuning algorithms (e.g., random search or Treestructured Parzen Estimators as in HyperOpt (Bergstra et al., 2013) and Optuna (Akiba et al., 2019)) can be applied for selection among candidates. We report simplified implementations of DSPy's BootstrapFewShotWithRandomSearch and BootstrapFewShotWithOptuna in Appendix E. 2 and Appendix E.3.</p>
<p>Another type of optimization is finetuning with BootstrapFinetune, where the demonstrations are used to update the LM's weights for each predictor. When this is applied, the LM parameter of each module is updated to the new LM weights. Typically, we are optimizing average quality using the metric with cross-validation over the training set or a validation set. This is applicable even with no labels for any stages, depending on the nature of metric.</p>
<p>Stage 3: Higher-Order Program Optimization A different type of optimization that the DSPy compiler supports is modifying the control flow of the program. One of the simplest forms of these is ensembles, which we use in the case studies in this work. An ensemble will bootstrap multiple copies of the same program, and then replace the program with a new one that runs them all in parallel and reduces their predictions into one with a custom function (e.g., majority voting). In future work, this stage can easily accommodate techniques for more dynamic (i.e., test-time) bootstrapping as well as automatic backtracking-like logic.</p>
<h1>5 GOALS OF EVALUATION</h1>
<p>Programming frameworks can be evaluated along many dimensions: computational efficiency, developer efficiency, intuitiveness of the code and concepts, and so forth. In this paper, we focus on perhaps the most pressing issue for current LM pipelines: the role of hand-written, task-specific prompts in achieving performant systems. Our evaluations seek to test the following hypotheses:</p>
<p>H1 With DSPy, we can replace hand-crafted prompt strings with concise and well-defined modules, without reducing quality or expressive power.</p>
<p>H2 Parameterizing the modules and treating prompting as an optimization problem makes DSPy better at adapting to different LMs, and it may outperform expert-written prompts.</p>
<p>H3 The resulting modularity makes it possible to more thoroughly explore complex pipelines that have useful performance characteristics or that fit nuanced metrics.</p>
<p>Our evaluation will explore these hypotheses using diverse task-program pairs. We hope this begins a shift from underspecified questions like "how do different LMs compare on GSM8K" toward "how they compare on GSM8K with program P when compiled with strategy S", which is a well-defined and reproducible run. Ultimately, our goal is to reduce the role of artful prompt construction in modern AI in favor of the development of new modular, composable programs and optimizers.</p>
<p>Table 1: Results with in-context learning on GSM8K math word problems. Each row represents a separate pipeline: the module in the Program column is compiled against the examples in the Training set. The programs, compilers, and (small) training sets are defined in Section 6. Rows with ensemble build on the immediately preceding row. Notably, all programs in this table are expressed by composing two to four DSPy modules and teleprompters. Compiling the correct modules, instead of string prompts, improves different LMs from 4-20\% accuracy to 49-88\% accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Program</th>
<th style="text-align: center;">Compilation</th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Llama2-13b-chat</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">vanilla</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">9.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fewshot</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap $\times 2$</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">36.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ensemble</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">34.6</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fewshot</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fewshot</td>
<td style="text-align: center;">+human.CoT</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">33.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ensemble</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">reflection</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fewshot</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">40.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ensemble</td>
<td style="text-align: center;">trainset</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">46.9</td>
</tr>
</tbody>
</table>
<h1>6 Case Study: Math Word Problems</h1>
<p>We evaluate on the popular GSM8K dataset with grade school math questions (Cobbe et al., 2021). We sample 200 and 300 question-answer pairs from the official training set for training and development, respectively. Our final evaluations use the 1.3 k official test set examples. We report extensive comparisons on the development set to avoid overfitting on test. Following prior work on GSM8K, we evaluate the accuracy of the final numerical value that appears in the LM output.</p>
<p>Programs Considered For this task, we consider three simple DSPy programs: a one-step Predict module (vanilla), a two-step ChainOfThought module (CoT), and finally a multi-stage ComparerOfThoughts module (ThoughtReflection). These are fully defined by the following code:</p>
<div class="codehilite"><pre><span></span><code><span class="n">vanilla</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dspy</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="s">&quot;question -&gt; answer&quot;</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">GSM8K</span><span class="w"> </span><span class="n">Program</span><span class="w"> </span><span class="s">&#39;vanilla&#39;</span>
<span class="n">CoT</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dspy</span><span class="p">.</span><span class="n">ChainOfThought</span><span class="p">(</span><span class="s">&quot;question -&gt; answer&quot;</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">GSM8K</span><span class="w"> </span><span class="n">Program</span><span class="w"> </span><span class="s">&#39;CoT&#39;</span>
<span class="n">class</span><span class="w"> </span><span class="n">ThoughtReflection</span><span class="p">(</span><span class="n">dspy</span><span class="p">.</span><span class="kr">Module</span><span class="p">)</span><span class="o">:</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="p">(</span><span class="kr">self</span><span class="p">,</span><span class="w"> </span><span class="n">num_attempts</span><span class="p">)</span><span class="o">:</span>
<span class="w">        </span><span class="kr">self</span><span class="p">.</span><span class="n">predict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dspy</span><span class="p">.</span><span class="n">ChainOfThought</span><span class="p">(</span><span class="s">&quot;question -&gt; answer&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="o">+</span><span class="n">num_attempts</span><span class="p">)</span>
<span class="w">        </span><span class="kr">self</span><span class="p">.</span><span class="n">compare</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dspy</span><span class="p">.</span><span class="n">MultiChainComparison</span><span class="p">(</span><span class="s">&#39;question -&gt; answer&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="o">+</span><span class="n">num_attempts</span><span class="p">)</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="kr">self</span><span class="p">,</span><span class="w"> </span><span class="n">question</span><span class="p">)</span><span class="o">:</span>
<span class="w">        </span><span class="n">completions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kr">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">question</span><span class="o">+</span><span class="n">question</span><span class="p">).</span><span class="n">completions</span>
<span class="w">        </span><span class="kr">return</span><span class="w"> </span><span class="kr">self</span><span class="p">.</span><span class="n">compare</span><span class="p">(</span><span class="n">question</span><span class="o">+</span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">completions</span><span class="o">+</span><span class="n">completions</span><span class="p">)</span>
<span class="n">reflection</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ThoughtReflection</span><span class="p">(</span><span class="n">num_attempts</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">GSM8K</span><span class="w"> </span><span class="n">Program</span><span class="w"> </span><span class="s">&#39;reflection&#39;</span>
</code></pre></div>

<p>In reflection, five reasoning chains are sampled from the LM (alongside their answers) and they are compared in parallel by a built-in MultiChainComparison module, which generalizes Yoran et al. (2023). This generates a new answer taking into account the patterns from the five attempts. Critically, the modules used are all generic, none is specific math problems or particular LM.</p>
<p>Compiling As we discussed in Section 4, DSPy programs can be compiled into new, optimized programs. In our experiments, we evaluate the programs zero-shot (no compiling) as well as a number of strategies for compiling. Our simplest compiler is LabeledFewShot:</p>
<div class="codehilite"><pre><span></span><code>fewshot = dspy.LabeledFewShot(k=8).compile(program, trainset=trainset)
</code></pre></div>

<p>Here, program can be any DSPy module. This simply samples $k=8$ random demonstrations from the trainset for the fields common to the training examples and the signature(s), in this case, question and answer, but not the reasoning for instance. We report the average of 3-5 runs (depending on the setting) when applying such random sampling.</p>
<p>Next, we also consider bootstrapping few-shot examples with random search:</p>
<div class="codehilite"><pre><span></span><code>: tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_accuracy)
: bootstrap = tp.compile(program, trainset=trainset, valset=devset)
</code></pre></div>

<p>This will generate demonstration chains for examples in the training set and optimize the selection of demonstrations (from this set) to self-improve the program's modules. As the name indicates, this is done with random search, treating the selection of demonstrations as a parameter to optimize.</p>
<p>Next, if desired, this bootstrapping process can be nested in DSPy. In particular, we can use the optimized bootstrap program itself to further bootstrap another program. This is relevant, for example, whenever the original zero-shot program performs relatively poorly.</p>
<div class="codehilite"><pre><span></span><code>:bootstrap2 = tp.compile(program, teacher=bootstrap, trainset=trainset, valset=devset)
</code></pre></div>

<p>And lastly, we consider ensembling these bootstraps:</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> A program that ensembles the top-7 candidate programs from a bootstrapping compiler run
    (in particular &#39;bootstrap&#39; or, when applicable, &#39;bootstrap2&#39;) with majority voting.
ensemble = Ensemble(reduce_fn=dspy.majority).compile(bootstrap.programs[:7])
</code></pre></div>

<p>GSM8K includes human reasoning chains. Above, trainset does not include these reasoning chains. We also evaluate with trainset_human_CoT, which extends the examples in trainset with the human reasoning string. These two datasets can be used interchangeably as the value for the trainset parameter above. We note here that compiling generally runs on the order of minutes (or tens of minutes) as even the more expensive settings only require running the program a few thousand times (e.g., 10-20 trials over 150-300 validation examples) and they can occur in parallel.</p>
<p>Results Our results are summarized in Table 1, which includes dev results as well as our evaluation of promising representatives of each approach on the test set. First, the vanilla program results show that GPT-3.5 and llama2-13b-chat struggle with math word problems when they have to predict the answers directly, that is, without using a reasoning chain first. This is most pronounced in the absence of good demonstrations, which can be seen in the none compilation setting (i.e., zero-shot instruction) and the fewshot setting (i.e., sampling random question-answer pairs). Interestingly, however, vanilla is helped substantially by compiling with bootstrap and by iterating this process into bootstrap $\times 2$. On inspecting the prompts bootstrapped (Appendix F), we see that the prompt allows the LM to leverage the answer field for reasoning first, which is permitted as the metric extracts the final numerical value for evaluation.</p>
<p>Next, we consider the CoT program. While the expert human reasoning chains (+human.CoT) provide a large boost when available, we can match or surpass this using bootstrap, substantiating our hypothesis that DSPy can cut the need for hand-crafted prompts. Beyond this, we see that the reflection program, while only a few lines longer than the others, is a clear winner, though CoT is quite effective with ensemble. Overall, the bootstrap compilation procedure leads to large gains for every program, across both LMs. Indeed, all programs in this table are expressed by composing two to four DSPy modules and teleprompters, and they reveal overall that-in the new paradigm prescribed by DSPy-it's composing the right generic modules, rather than manipulating string prompts, that improves different LMs from $4-20 \%$ accuracy to $49-88 \%$ accuracy.
We can informally compare with the following. Zhang et al. (2022) reports $48 \%$ for text-davinci-002, which aligns closely with our llama2-13b-chat results, and reports $59.4 \%$ with codex when employing a manual CoT approach and $62.8 \%$ with an automatic CoT method. Wang et al. (2022b) report $57 \%$ for CoT prompting with PaLM 540-B, which becomes $74 \%$ upon adding self-consistency. The Llama2 authors (Touvron et al., 2023) presents $28.7 \%$ for llama2-13b, $42.2 \%$ for llama2-34b, and $56.8 \%$ for llama2-70b. Intriguingly, our program with the 13b variant of the model is competitive with their 34b-based results even though we don't use human reasoning chains in our program. Zhao et al. (2023b) reports $80.8 \%$ for CoT with gpt-3.5-turbo from April 2023. The GPT-4 authors (OpenAI, 2023) reports that GPT-3.5 scores $57.1 \%$ and GPT-4 elevates this to $92 \%$ but they note that GPT-4 was in fact pre-trained on a subset of GSM8K's training set.</p>
<h1>7 Case Study: Complex Question Answering</h1>
<p>In this case study, we explore the multi-hop question answering task with the HotPotQA (Yang et al., 2018) dataset in the open-domain "fullwiki" setting. For retrieval, we use a search index of the official Wikipedia 2017 "abstracts" dump of HotPotQA. Search is conducted by a ColBERTv2 (Santhanam et al., 2021) retriever. The HotPotQA test set is hidden, so we reserve the official validation set for our testing, and sample 1000 examples for that. We sub-divide the training set into $70 \% / 30 \%$ train/validation splits. In the training (and thus validation) split, we keep only examples marked as "hard" in the original dataset, which matches the designation of the official validation and test sets. For training and for reporting development results, we sample 200 and 300 examples respectively.</p>
<p>Programs Considered Our simplest baseline is the vanilla program used in the previous case study on GSM8K (Sec 6); the "question -&gt; answer" signature is universal enough that it will work for this task (and many others) when compiled appropriately.
Our baseline RAG program is the one given in Section 3.2 as a simple example of RAG with a dspy. ChainOfThought layer. We will see that this program does not excel at HotPotQA, and this motivates us to evaluate two multi-hop programs.
To that end, we first test ReAct (Yao et al., 2022), a multi-step agent for tool use, which is implemented as a built-in module in DSPy. In the simplest case, a ReAct module for a particular signature can be declared as follows in DSPy:</p>
<div class="codehilite"><pre><span></span><code><span class="n">react</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dspy</span><span class="o">.</span><span class="n">ReAct</span><span class="p">(</span><span class="s2">&quot;question -&gt; answer&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">dspy</span><span class="o">.</span><span class="n">Retrieve</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)],</span><span class="w"> </span><span class="n">max_iters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>

<p>We also test the following custom program, which simulates the information flow in Baleen (Khattab et al., 2021a) and IRRR (Qi et al., 2020) and has similarities to IRCoT (Trivedi et al., 2022).</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">BasicMultiHop</span>(<span class="n">dspy</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">passages_per_hop</span>):
        <span class="nb">self</span>.<span class="n">retrieve</span> = <span class="n">dspy</span>.<span class="n">Retrieve</span>(<span class="n">k</span>=<span class="n">passages_per_hop</span>)
        <span class="nb">self</span>.<span class="n">generate_query</span> = <span class="n">dspy</span>. <span class="n">ChainOfThought</span>(<span class="s">&quot;context, question -&gt; search_query&quot;</span>)
        <span class="nb">self</span>.<span class="n">generate_answer</span> = <span class="n">dspy</span>. <span class="n">ChainOfThought</span>(<span class="s">&quot;context, question -&gt; answer&quot;</span>)
    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="n">question</span>):
        <span class="n">context</span> = []
        <span class="k">for</span> <span class="n">hop</span> <span class="nb">in</span> <span class="nb">range</span>(<span class="mi">2</span>):
            <span class="n">query</span> = <span class="nb">self</span>.<span class="n">generate_query</span>(<span class="n">context</span>=<span class="n">context</span>, <span class="n">question</span>=<span class="n">question</span>).<span class="n">search_query</span>
            <span class="n">context</span> += <span class="nb">self</span>.<span class="n">retrieve</span>(<span class="n">query</span>).<span class="n">passages</span>
        <span class="k">return</span> <span class="nb">self</span>.<span class="n">generate_answer</span>(<span class="n">context</span>=<span class="n">context</span>, <span class="n">question</span>=<span class="n">question</span>)
<span class="n">multihop</span> = <span class="n">BasicMultiHop</span>(<span class="n">passages_per_hop</span>=<span class="mi">3</span>)
</code></pre></div>

<p>Compiling For compilers, we continue to use the ones that we used for GSM8K (see Sec 6). We also consider two compositions of our teleprompters. For ReAct, we consider bootstrapping with BootstrapFewShotWithRandomSearch starting from an earlier bootstrap of the ReAct program. For the simple multihop program, we also consider fine-tuning with T5-Large starting from the earlier bootstrap of that program.</p>
<div class="codehilite"><pre><span></span><code>multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program,
    teacher=bootstrap, trainset=trainset, target=&#39;t5-large&#39;)
</code></pre></div>

<p>Results Table 2 summarizes our results. Compared with the vanilla few-shot prompting, a chain-of-thought and retrieval-augmented generation (CoT_RAG) program can self-bootstrap in DSPy to increase answer EM substantially. However, this relies entirely on the ColBERTv2 retriever to find relevant passages directly from the original questions, limiting its passage recall. This is tackled in the react and multihop programs, which will generate queries for the retriever in multiple iterative "hops". Indeed, overall, a simple multihop program performs the best, and in general bootstrap again proves to be very effective at raising its quality relative to its fewshot variant for both LMs.
In particular, we can see that bootstrap (and/or bootstrap $\times 2$ ) can outperform both fewshot prompting (for multihop) and expert human reasoning (for react; adapted slightly from Yao et al. (2022) to our retrieval setting). Perhaps most importantly, we can make llama2-13b-chat competitive with GPT-3.5 by simply compiling our programs.
To assess the finetuning capacity of DSPy, we also evaluated the compiler multihop_t5 defined above which produces a T5-Large ( 770 M parameter) model. This program scores $39.3 \%$ answer EM and $46.0 \%$ passage accuracy on the dev set, using only 200 labeled inputs and 800 unlabeled</p>
<p>Table 2: Results with in-context learning on HotPotQA multi-hop retrieval question answering. We report answer exact match (Ans) and pair-retrieval accuracy (Psg). Each row represents a separate pipeline: the module in the Program column is compiled against the examples in the Training set. The programs, compilers, and (small) training sets are defined in the main text. For HotPotQA, we use the training set (and not dev) directly for cross-validation. *The marked result is evaluated on $50 \%$ of our test set due to cost.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Program</th>
<th style="text-align: center;">Compiler</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Llama2-13b-chat</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev <br> Ans</td>
<td style="text-align: center;">Psg</td>
<td style="text-align: center;">Test <br> Psg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev <br> Ans</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test <br> Ans</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">vanilla</td>
<td style="text-align: center;">fewshot</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">CoT_RAG</td>
<td style="text-align: center;">fewshot</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: center;">react</td>
<td style="text-align: center;">none</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+human_r</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap $\times 2$</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">multihop</td>
<td style="text-align: center;">fewshot</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bootstrap</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ensemble</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">45.6*</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">$-$</td>
</tr>
</tbody>
</table>
<p>questions. For compiling, we use a teacher program consisting of an ensemble (union) of two multihop with llama2-13b-chat. Considering its extremely small size and local availability, this compiled program with T5-Large would impose orders of magnitude lower costs for inference than a proprietary LM like GPT-3.5.</p>
<p>Our results may be pegged against the evaluation on HotPotQA in a number of recent papers, though there is significant variation in evaluation methodology and test set samples across studies in this space. Using CoT prompting, Si et al. (2022) achieve 25.2\% EM. With a "recite-and-answer" technique that uses PaLM-62B (Chowdhery et al., 2022) to recite evidence passages, Sun et al. (2022) achieve $26.5 \%$ EM. Wang et al. (2022a) achieve $33.8 \%$ EM and $44.6 \%$ F1 when applying selfconsistency for PaLM-540B. Yao et al. (2022) achieve $27.4 \%$ EM using ReAct with PaLM-540B and 30.8 with text-davinci-002, with a tool giving it the ability for search using a Wikipedia API. They push their PaLM results to $35.1 \%$ EM by applying an additional CoT step with selfconsistency, which may resemble our ensemble approach in the sense of aggregating multiple answers. Trivedi et al. (2022) reports $49 \%$ using a pipeline with code-davinci-002 LM on a sample of 500 HotPotQA questions.</p>
<h1>8 CONCLUSION</h1>
<p>This paper introduced DSPy, a new programming model for designing AI systems using pipelines of pretrained LMs and other tools. We presented three new concepts introduced in this abstraction (DSPy signatures, modules, and teleprompters), and showed in two very different case studies that it supports rapid development of highly effective systems that use relatively small LMs. We have maintained open-source versions of this framework for close to a year. In this period, we have seen and created a large number of programs that were compiled to high-quality systems by DSPy, spanning tasks from information extraction to low-resource synthetic data generation. In the interest of space and to maintain reasonable scope in this paper, we leave reporting on such tasks under controlled experimental conditions to future work. While in-context learning has proved transformative over the past $2-3$ years of LM research, we argue that the true expressive power in this emerging paradigm is in building sophisticated text transformation graphs in which composable modules and optimizers (teleprompters) come together to leverage LMs in more systematic and reliable ways.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Josh Purtell for suggesting the apt name "text transformation graph" for the computational graph abstraction of DSPy. We thank Rick Battle, Igor Kotenkov, Lisa Li, David Hall, Ashwin Paranjape, Chris Manning, Percy Liang, and many researchers, developers, and users for valuable</p>
<p>discussions and feedback. We thank Giuseppe Attanasio for his public $\mathrm{LT}_{\mathrm{E}} \mathrm{X}$ GitHub-style Python code formatting gist. ${ }^{6}$</p>
<p>This work was partially supported by IBM as a founding member of the Stanford Institute for Human-Centered Artificial Intelligence (HAI), Oracle, Virtusa, and Cigna Healthcare. It was also partially supported by an HAI Azure compute grant. This research was supported in part by affiliate members and other supporters of the Stanford DAWN project-Facebook, Google, and VMware-as well as the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Omar Khattab is supported by the Apple Scholars in AI/ML fellowship.
\usepackage[pdftex]{graphicx} ...
\includegraphics[width=0.8\linewidth]{myfile.pdf}</p>
<h1>REFERENCES</h1>
<p>Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pp. 2623-2631, 2019.</p>
<p>Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Fr√©d√©ric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, pp. arXiv-1605, 2016.</p>
<p>James Bergstra, Olivier Breuleux, Fr√©d√©ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A CPU and GPU math compiler in Python. In Proc. 9th python in science conf, volume 1, pp. 3-10, 2010.</p>
<p>James Bergstra, Fr√©d√©ric Bastien, Olivier Breuleux, Pascal Lamblin, Razvan Pascanu, Olivier Delalleau, Guillaume Desjardins, David Warde-Farley, Ian Goodfellow, Arnaud Bergeron, et al. Theano: Deep learning on gpus with Python. In NIPS 2011, BigLearning Workshop, Granada, Spain, volume 3. Citeseer, 2011.</p>
<p>James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning, pp. 115-123. PMLR, 2013.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Harrison Chase. Hwchase17/langchain. 2022. URL https://github.com/hwchase17/ langchain.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1870-1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https: //aclanthology.org/P17-1171.</p>
<p>Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Ronan Collobert, Samy Bengio, and Johnny Mari√©thoz. Torch: a modular machine learning software library. Technical report, Idiap, 2002.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477-16508, 2023a.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023b.</p>
<p>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. URL https: //arxiv.org/abs/2002.08909.</p>
<p>Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher R√©. Training classifiers with natural language explanations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 18841895. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/ P18-1175.</p>
<p>Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach. arXiv preprint arXiv:2306.03604, 2023. URL https://arxiv.org/abs/2306.03604.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.</p>
<p>Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445, 2022.</p>
<p>Omar Khattab, Christopher Potts, and Matei Zaharia. Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021a.</p>
<p>Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with ColBERT. Transactions of the Association for Computational Linguistics, 9:929-944, 2021b.</p>
<p>Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9459-9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper.pdf.</p>
<p>Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv:1806.08730, 2018. URL https: //arxiv.org/abs/1806.08730.</p>
<p>Microsoft. Semantic kernel. 2023. URL https://learn.microsoft.com/semantic-kernel/.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback, 2021. URL https: //arxiv.org/abs/2112.09332.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.</p>
<p>Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-tosql with self-correction. arXiv preprint arXiv:2304.11015, 2023.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.</p>
<p>Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023.</p>
<p>Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D. Manning. Answering complex open-domain questions through iterative query generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2590-2602, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1261. URL https://aclanthology.org/D19-1261.</p>
<p>Peng Qi, Haejun Lee, Oghenetegiri Sido, Christopher D Manning, et al. Retrieve, rerank, read, then iterate: Answering open-domain questions of arbitrary complexity from text. arXiv preprint arXiv:2010.12527, 2020. URL https://arxiv.org/abs/2010.12527.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. Ms, OpenAI, 2018. URL https://openai.com/blog/ language-unsupervised/.</p>
<p>Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R√©. Data programming: Creating large training sets, quickly. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3567-3575. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper/ 6523-data-programming-creating-large-training-sets-quickly.</p>
<p>Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. arXiv preprint arXiv:2112.01488, 2021.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. arXiv preprint arXiv:2302.00618, 2023.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150, 2022.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022.</p>
<p>Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS), volume 5, pp. 1-6, 2015.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022.</p>
<p>Fei Wang, James Decker, Xilun Wu, Gregory Essertel, and Tiark Rompf. Backpropagation with callbacks: Foundations for efficient and expressive differentiable programming. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/ 34e157766f31db3d2099831d348a7933-Paper.pdf.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022a.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007, 2023.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.</p>
<p>Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. ExpeL: LLM agents are experiential learners. arXiv preprint arXiv:2308.10144, 2023a. URL https: //arxiv.org/pdf/2308.10144.</p>
<p>Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333, 2023b.</p>
<h1>A AdVANCED SignATURES</h1>
<p>When more control is desired, one can express signatures as Python classes to provide explicit instructions of the transformation and describe the format or role of each field more directly. For instance, the following signature generates search queries using context and an optional question:</p>
<div class="codehilite"><pre><span></span><code><span class="kd">class</span><span class="w"> </span><span class="nx">GenerateSearchQuery</span><span class="p">(</span><span class="nx">dspy</span><span class="p">.</span><span class="nx">Signature</span><span class="p">):</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;Write a simple search query that will help answer a complex question.&quot;&quot;&quot;</span>
<span class="w">    </span><span class="nx">context</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">dspy</span><span class="p">.</span><span class="nx">InputField</span><span class="p">(</span><span class="nx">desc</span><span class="p">=</span><span class="s">&quot;may contain relevant facts&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="nx">question</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">dspy</span><span class="p">.</span><span class="nx">InputField</span><span class="p">()</span>
<span class="w">    </span><span class="nx">query</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">dspy</span><span class="p">.</span><span class="nx">OutputField</span><span class="p">(</span><span class="nx">dtype</span><span class="p">=</span><span class="nx">dspy</span><span class="p">.</span><span class="nx">SearchQuery</span><span class="p">)</span>
</code></pre></div>

<p>Using the above, we can specify a complete system for the generation of a synthetic IR dataset where the queries are mediated by a question generated by the LM:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">query_gen</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">dspy</span><span class="p">.</span><span class="nx">Predict</span><span class="p">(</span><span class="nx">GenerateSearchQuery</span><span class="p">)</span>
<span class="nx">query_gen</span><span class="p">(</span><span class="nx">context</span><span class="p">=</span><span class="s">&quot;Language typology&quot;</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="nx">Out</span><span class="p">:</span><span class="w"> </span><span class="nx">Prediction</span><span class="p">(</span><span class="nx">question</span><span class="p">=</span><span class="err">&#39;</span><span class="nx">What</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">main</span><span class="w"> </span><span class="nx">types</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="nx">classification</span><span class="p">?</span><span class="err">&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="nx">query</span><span class="p">=</span><span class="s">&quot;&quot;</span><span class="nx">language</span><span class="w"> </span><span class="nx">classification</span><span class="s">&quot; OR &quot;</span><span class="nx">language</span><span class="w"> </span><span class="nx">typology</span><span class="err">&quot;</span><span class="w"> </span><span class="o">-</span><span class="nx">wikipedia</span><span class="err">&#39;</span><span class="p">)</span>
</code></pre></div>

<p>If questions are available, they can be supplied as shown: query_gen(context="Language typology", question="What are the primary language families of South America?").
As a work in progress feature, users can optionally specify the type of output fields as bool, int, float, list, or dict instead of the default free-form string type, as in contexts, question -&gt; answer_found: bool.</p>
<h2>B COMPARISON WITH EXISTING LIBRARIES LIKE LANGCHAIN AND LLAMAINDEX</h2>
<p>LangChain and LlamaIndex are perhaps the most popular library in the general space of prompting LMs. These libraries have a different focus compared to DSPy and they suffer internally from the prompt engineering challenges that DSPy aims to resolve. In particular, whereas the goal of DSPy is to tackle the fundamental challenges of prompt engineering for building new LM computational graphs, LangChain and LlamaIndex generally help application developers who need pre-packaged components and chains, e.g., implementations of popular and reusable pipelines (e.g., particular agents and specific retrieval pipelines) and tools (e.g., connections to various databases and implementations of long- and short-term memory for agents).</p>
<p>These off-the-shelf higher-level abstractions contrast with DSPy's focus on introducing core composable operators. In particular, DSPy introduces signatures (to abstract prompts), modules (to abstract prompting techniques), and teleprompters to act as optimizers for arbitrary imperative code (DSPy programs) that chain modules together. Its goal is to help researchers and practitioners build new LM pipelines quickly and achieve very high quality through automatic compilation (selfimprovement) instead of manual prompt engineering.</p>
<p>In contrast, typical existing research implementations and existing libraries like LangChain and LlamaIndex are implemented using manual prompt engineering, which is the key problem that DSPy tackles. We conducted an informal study to highlight this. In late September 2023, we found that the LangChain codebase contains 50 strings exceeding 1000 characters, which are generally prompts, compared to none at all in DSPy. Indeed, a substantial number of LangChain's Python files are singularly dedicated to task-related templating and prompt engineering with 12 prompts.py files and and 42 prompt.py files. DSPy, on the other hand, provides a structured framework that automatically bootstraps prompts. The library itself does not contain a single hand-written prompt demonstration for any tasks at the time of writing, despite the very high quality with various LMs.</p>
<p>To review the typical forms of prompt engineering in existing libraries, we consider the following in LangChain. The LangChain Program-Aided Language Model Gao et al. (2023a) chain program uses few-shot learning, leveraging a template that is 3982 characters long with 8 math word problems (Prompt 2) and corresponding outputted programs as learning examples for the language model. LangChain also contains a prompt for SQL query tasks for each of the databases like Oracle, GoogleSQL, DuckDB, Crate, and MySQL, with the average length of these prompts at 1058 characters. Other task areas such as QA with sources (Prompt B) and Graph_QA also have signif-</p>
<p>icantly lengthy prompt templates, with averages of 1337 and 722 characters, respectively. While expert-written prompts can be useful, we believe that LM- and task-adaptive prompts bootstrapped automatically can offer far more power (and are far more modular) than hard-coding a prompt per database provider inside the code base. The next appendix section contains a number of prompts copied from related research papers and existing libraries.</p>
<h1>C SAMPLE LARGE PROMPTS</h1>
<p>This section highlights a few popular existing frameworks that structure prompts with extensive prompt engineering templates. The primary objective is to capture how many words and characters are used for such large multi-line prompts defined for tasks or tools and present these example prompts retrieved from open-sourced papers and repositories. The formatting of these example prompts is adapted from Gao et al. (2023a).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task/Tool Prompt</th>
<th style="text-align: left;">Source</th>
<th style="text-align: center;">Words</th>
<th style="text-align: center;">Characters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prompt 1: Text-evidence checker</td>
<td style="text-align: left;">Gao et al. (2023a)</td>
<td style="text-align: center;">818</td>
<td style="text-align: center;">4964</td>
</tr>
<tr>
<td style="text-align: left;">Prompt 2: Math word problems (PAL)</td>
<td style="text-align: left;">LangChain \&amp; Gao et al. (2023b)</td>
<td style="text-align: center;">566</td>
<td style="text-align: center;">3957</td>
</tr>
<tr>
<td style="text-align: left;">Prompt 3: ReAct</td>
<td style="text-align: left;">Yao et al. (2022)</td>
<td style="text-align: center;">593</td>
<td style="text-align: center;">3889</td>
</tr>
<tr>
<td style="text-align: left;">Prompt 4: Zero-shot ReAct</td>
<td style="text-align: left;">LangChain</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: left;">Prompt 5: QA with sources</td>
<td style="text-align: left;">LangChain</td>
<td style="text-align: center;">992</td>
<td style="text-align: center;">6197</td>
</tr>
<tr>
<td style="text-align: left;">Prompt 6: SQL MyScale querying</td>
<td style="text-align: left;">LangChain</td>
<td style="text-align: center;">343</td>
<td style="text-align: center;">2239</td>
</tr>
<tr>
<td style="text-align: left;">Prompt 7: Relevant docs retrieval</td>
<td style="text-align: left;">LlamaIndex</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">719</td>
</tr>
<tr>
<td style="text-align: left;">Prompt 8: IRS chatbot</td>
<td style="text-align: left;">LlamaIndex</td>
<td style="text-align: center;">389</td>
<td style="text-align: center;">2258</td>
</tr>
</tbody>
</table>
<p>[web] I will check some things you said.
(1) You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This is to prevent a buildup of mucus. It's called the nasal cycle.
I checked: How often do your nostrils switch?
I found this article: Although we don't usually notice it, during the nasal cycle one nostril becomes congested and thus contributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every 2 hours, according to a small 2016 study published in the journal PLOS One.
Your nose's switching time is about every 2 hours, not 45 minutes.
This disagrees with what you said.
(2) You said: The Little House books were written by Laura Ingalls Wilder. The books were published by HarperCollins.</p>
<p>I checked: Who published the Little House books?
I found this article: These are the books that started it all -- the stories that captured the hearts and imaginations of children and young adults worldwide. Written by Laura Ingalls Wilder and published by HarperCollins, these beloved books remain a favorite to this day.
The Little House books were published by HarperCollins.
This agrees with what you said.
(3) You said: The Stanford Prison Experiment was conducted in the basement of Jordan Hall, Stanford's psychology building.</p>
<p>I checked: Where was Stanford Prison Experiment conducted?
I found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set out to examine the psychological effects of authority and powerlessness in a prison environment.
The Stanford Prison Experiment was conducted in Jordan Hall.
This agrees with what you said.
(4) You said: Social work is a profession that is based in the philosophical tradition of humanism. It is an intellectual discipline that has its roots in the 1880s.
I checked: When did social work have its roots?
I found this article: The Emergence and Growth of the Social work Profession<br>rbr&gt; Social work's roots were planted in the 1880s, when charity organization societies (COS) were created to organize municipal voluntary relief associations and settlement houses were established.
Social work has its roots in the 1880s, not 1800s.
This disagrees with what you said.
(5) You said: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency list. It is named after Vaclav Havel and Samih Hakimi.
I checked: What is the Havel-Hakimi algorithm?
I found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree sequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm. The algorithm was published by Havel (1955), and later by Hakimi (1962).
Havel-Hakimi algorithm is for constructing a special solution if a simple graph for the given degree sequence exists, or proving that one cannot find a positive answer, not converting the adjacency matrix of a graph into its adjacency list. This disagrees with what you said.
(6) You said: "Time of My Life" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film Dirty Dancing. The song was produced by Michael Lloyd.
I checked: Who was the producer of "[I've Had] The Time of My Life"?
I found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd, was released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at Stanford University.
"Time of My Life" was produced by Michael Lloyd.
This agrees with what you said.
(7) You said: Kelvin Hopins was suspended from the Labor Party because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party activist, Ava Etemadzadeh.
I checked: Why was Kelvin Hopins suspended from the Labor Party?
I found this article: A former Labour MP has left the party before an inquiry into sexual harassment allegations against him was able to be concluded, the party has confirmed. Kelvin Hopkins was accused in 2017 of inappropriate physical contact and was suspended by the Labour party pending an investigation. This agrees with what you said.
Kelvin Hopins was suspended because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party activist, Ava Etemadzadeh.
This agrees with what you said.
(8) You said: In the battles of Lexington and Concord, the British side was led by General Thomas Smith.</p>
<p>I checked: Who led the British side in the battle of Lexington and Concord?
I found this article: Interesting Facts about the Battles of Lexington and Concord. The British were led by Lieutenant Colonel Francis Smith. There were 700 British regulars.
The British side was led by Lieutenant Colonel Francis Smith, not General Thomas Hall.
This disagrees with what you said.
(9) You said: {text}</p>
<p>I checked: {query}
I found this article: {evidence}
$------$</p>
<p>Figure 1: Example few-shot prompt using a reasoning chain for agreement model that identifies inconsistencies between text and evidence (Gao et al., 2023a).</p>
<div class="codehilite"><pre><span></span><code>    <span class="s">Q: Olivia has $23. She bought five bagels for $</span><span class="mi">3</span> <span class="n">each</span>. <span class="n">How</span> <span class="n">much</span> <span class="n">money</span> <span class="k">does</span> <span class="n">she</span> <span class="n">have</span> <span class="n">left</span>?
    <span class="c1"># solution in Python:</span>
    <span class="n">def</span> <span class="n">solution</span>():
        <span class="s">&quot;&quot;&quot;Olivia has $23. She bought five bagels for $3 each. How much money does she have left?&quot;&quot;&quot;</span>
        <span class="n">money</span>.<span class="n">initial</span> = <span class="mi">23</span>
        <span class="n">bagels</span> = <span class="mi">5</span>
        <span class="n">bagel_cost</span> = <span class="mi">3</span>
        <span class="n">money</span>.<span class="n">spent</span> = <span class="n">bagels</span> + <span class="n">bagel_cost</span>
        <span class="n">money</span>.<span class="n">left</span> = <span class="n">money</span>.<span class="n">initial</span> - <span class="n">money</span>.<span class="n">spent</span>
        <span class="nb">result</span> = <span class="n">money</span>.<span class="n">left</span>
        <span class="k">return</span> <span class="nb">result</span>
    <span class="s">Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls.</span> <span class="n">On</span> <span class="n">wednesday</span>, <span class="n">he</span> <span class="n">lost</span> <span class="mi">2</span> <span class="n">more</span>. <span class="n">How</span> <span class="n">many</span> <span class="n">golf</span> <span class="n">balls</span> <span class="n">did</span> <span class="n">he</span>
    <span class="n">have</span> <span class="nb">at</span> <span class="n">the</span> <span class="nb">end</span> <span class="nb">of</span> <span class="n">wednesday</span>?
    <span class="c1"># solution in Python:</span>
    <span class="n">def</span> <span class="n">solution</span>():
        <span class="s">&quot;&quot;&quot;Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls</span>
<span class="s">    did he have at the end of wednesday?&quot;&quot;&quot;</span>
        <span class="n">golf</span>.<span class="n">balls_initial</span> = <span class="mi">58</span>
        <span class="n">golf</span>.<span class="n">balls_lost_tuesday</span> = <span class="mi">23</span>
        <span class="n">golf</span>.<span class="n">balls_lost_wednesday</span> = <span class="mi">2</span>
        <span class="n">golf</span>.<span class="n">balls_left</span> = <span class="n">golf</span>.<span class="n">balls_initial</span> - <span class="n">golf</span>.<span class="n">balls_lost_tuesday</span> - <span class="n">golf</span>.<span class="n">balls_lost_wednesday</span>
        <span class="nb">result</span> = <span class="n">golf</span>.<span class="n">balls</span>.<span class="n">left</span>
        <span class="k">return</span> <span class="nb">result</span>
    <span class="s">Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday.</span>
    <span class="n">How</span> <span class="n">many</span> <span class="n">computers</span> <span class="n">are</span> <span class="nb">now</span> <span class="nb">in</span> <span class="n">the</span> <span class="n">server</span> <span class="n">room</span>?
    <span class="c1"># solution in Python:</span>
    <span class="n">def</span> <span class="n">solution</span>():
        <span class="s">&quot;&quot;&quot;There were nine computers in the server room. Five more computers were installed each day, from monday to thursday.</span>
<span class="s">    How many computers are now in the server room?&quot;&quot;&quot;</span>
        <span class="n">computers</span>.<span class="n">initial</span> = <span class="mi">9</span>
        <span class="n">computers_per_day</span> = <span class="mi">5</span>
        <span class="n">num_days</span> = <span class="mi">4</span>
        <span class="n">computers_added</span> = <span class="n">computers_per_day</span> + <span class="n">num_days</span>
        <span class="n">computers_total</span> = <span class="n">computers_initial</span> + <span class="n">computers_added</span>
        <span class="nb">result</span> = <span class="n">computers</span>.<span class="nb">total</span>
        <span class="k">return</span> <span class="nb">result</span>
    <span class="s">Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad.</span> <span class="n">How</span> <span class="n">many</span> <span class="n">toys</span> <span class="k">does</span> <span class="n">he</span> <span class="n">have</span> <span class="nb">now</span>?
    <span class="c1"># solution in Python:</span>
    <span class="n">def</span> <span class="n">solution</span>():
        <span class="s">&quot;&quot;&quot;Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?&quot;</span>&quot;
        <span class="n">toys_initial</span> = <span class="mi">5</span>
        <span class="n">mom_toys</span> = <span class="mi">2</span>
        <span class="n">dad_toys</span> = <span class="mi">2</span>
        <span class="n">total_received</span> = <span class="n">mom_toys</span> + <span class="n">dad_toys</span>
        <span class="n">total_toys</span> = <span class="n">toys_initial</span> + <span class="n">total_received</span>
        <span class="nb">result</span> = <span class="n">total_toys</span>
        <span class="k">return</span> <span class="nb">result</span>
    <span class="s">Q: Jason had 20 lollipops. He gave Denny some lollipops.</span> <span class="n">Now</span> <span class="n">Jason</span> <span class="k">has</span> <span class="mi">12</span> <span class="n">lollipops</span>. <span class="n">How</span> <span class="n">many</span> <span class="n">lollipops</span> <span class="n">did</span> <span class="n">Jason</span> <span class="n">give</span> <span class="nb">to</span>
    <span class="n">Denny</span>?
    <span class="c1"># solution in Python:</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>def solution():
    &quot;&quot;&quot;Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give
    to Denny?&quot;&quot;&quot;
        Jason,lollipops,initial = 20
        Jason,lollipops,after = 12
        denny,lollipops = jason,lollipops,initial - jason,lollipops,after
        result = denny,lollipops
        return result
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
<span class="gh">#</span> solution in Python:
def solution():
    &quot;&quot;&quot;Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?&quot;&quot;&quot;
    Leah_chocolates = 32
    sister_chocolates = 42
    total_chocolates = Leah_chocolates + sister_chocolates
    chocolates_eaten = 35
    chocolates_left = total_chocolates - chocolates_eaten
    result = chocolates_left
    return result
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
<span class="gh">#</span> solution in Python:
def solution():
    &quot;&quot;&quot;If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?&quot;&quot;
    cars_initial = 3
    cars_arrived = 2
    total_cars = cars_initial + cars_arrived
    result = total_cars
    return result
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be
21 trees. How many trees did the grove workers plant today?
<span class="gh">#</span> solution in Python:
def solution():
    &quot;&quot;&quot;There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will
be 21 trees. How many trees did the grove workers plant today?&quot;&quot;&quot;
    trees_initial = 15
    trees_after = 21
    trees_added = trees_after - trees_initial
    result = trees_added
    return result
Q: {question}
<span class="gh">#</span> solution in Python:
</code></pre></div>

<p>Figure 2: PAL example few-shot prompt for solving math questions by generating code.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://gist.github.com/g8a9/87c2be12ae82cfad4aa438d77dc948cb&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
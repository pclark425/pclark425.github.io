<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8336 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8336</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8336</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278789523</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16114v1.pdf" target="_blank">Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language</a></p>
                <p><strong>Paper Abstract:</strong> Solving puzzles in natural language poses a long-standing challenge in AI. While large language models (LLMs) have recently shown impressive capabilities in a variety of tasks, they continue to struggle with complex puzzles that demand precise reasoning and exhaustive search. In this paper, we propose Logic-of-Thought (Logot), a novel framework that bridges LLMs with logic programming to address this problem. Our method leverages LLMs to translate puzzle rules and states into answer set programs (ASPs), the solution of which are then accurately and efficiently inferred by an ASP interpreter. This hybrid approach combines the natural language understanding of LLMs with the precise reasoning capabilities of logic programs. We evaluate our method on various grid puzzles and dynamic puzzles involving actions, demonstrating near-perfect accuracy across all tasks. Our code and data are available at: https://github.com/naiqili/Logic-of-Thought.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8336.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8336.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought (Logot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid framework introduced in this paper that uses large language models to translate natural-language puzzle rules and states into Answer Set Programs (ASP), then uses an ASP solver (Clingo) to perform exact, exhaustive inference and produce puzzle solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logot (LLM+ASP pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A two-stage pipeline: (1) few-shot in-context prompting of an LLM to translate puzzle rule specifications and puzzle states into declarative ASP code; (2) grounding and solving of the generated ASP program with Clingo, followed by lightweight postprocessing to produce human-readable answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku, Hitori, Fillomino, Blocks World tasks (Projection, Legality, Plan Verification, Goal Recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Classic grid-based logic puzzles (Sudoku, Hitori, Fillomino) requiring 2D spatial constraints; dynamic action-based spatial planning (Blocks World) requiring reasoning about 'on' relations and clear/topology of blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Few-shot in-context examples provided separately for rule-specification translation (D_r) and puzzle-state translation (D_q). The LLM is prompted to output ASP translations of rules and states; the translations are concatenated, executed by Clingo to get answer sets; a decoder converts ASP atoms to the puzzle's expected answer format. Baselines: Standard Prompting, Chain-of-Thought (CoT), and Finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Declarative translation: LLMs produce ASP rules (choice rules, constraints, facts) and state encodings; Clingo performs exhaustive search/inference. Uses few-shot demonstration examples for translation. Also discusses potential use of Program-of-Thought (PoT) to improve translation robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Logot+GPT-4o: near-perfect accuracy (reported as >99% across all evaluated puzzles). Logot+GPT-4o-mini: reported 100% accuracy on the three grid puzzles and strong performance on Blocks World (example table shows 95.5% BW-GR, 99.5% BW-LG, 98.5% BW-PV, 95.0% BW-PJ). The paper states Logot with smaller models (Deepseek-V3 or GPT-4o-mini) still achieves strong accuracy (reported as over 95% and 98%, respectively). Standard prompting produced 0% on grid puzzles; CoT is N/A for grid puzzles and competitive on some dynamic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The paper demonstrates that the ASP solver enforces spatial constraints (row/column/subgrid uniqueness, adjacency/connectedness for Hitori, polyomino regions for Fillomino, and 'on'/clear relations for Blocks World). The success of Logot indicates correct handling of spatial constraints, but the paper attributes precise spatial inference to the ASP solver rather than to pure LLM internal spatial reasoning; no direct probing or ablation shows the LLM itself performing spatial reasoning beyond translation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to baselines (Standard Prompting, CoT, Finetuning), Logot substantially outperforms them on grid puzzles and achieves higher accuracy on dynamic Blocks World tasks. Paper reports Standard prompting and direct LLM answers often fail (0% on grid puzzles), CoT was competitive on some Blocks World subtasks, and finetuned models reported in prior work had mixed performance; specific numerical comparisons are provided in Table 1 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Remaining failures mainly originate from incorrect puzzle-state translations produced by the LLM (i.e., wrong parsing/encoding of the instance), not from the ASP solver. Representative failure cases shown for Sudoku and Blocks World - Legality. Method relies on manually curated few-shot examples and accurate translation; occasional translation errors occur even with strong LLMs (e.g., GPT-4o). The evaluated puzzle set is modest in variety; generalization to broader puzzle families is left for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8336.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8336.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability variant of the GPT-4 family used as an LLM backbone in experiments, serving as the translator from natural language to ASP in the Logot pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art GPT family model referenced and used as the prompting backbone to perform few-shot translation of puzzle rules and states into ASP; exact architecture/training details and parameter count are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku, Hitori, Fillomino, Blocks World (GR, LG, PV, PJ)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles and dynamic action-based spatial planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Used within Logot: few-shot prompts for rule and state translation; output ASP executed by Clingo. Evaluated against direct prompting, CoT baselines, and finetuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Few-shot in-context translation to ASP; relied on Clingo for exact spatial/integrity constraints. Postprocessing decodes ASP answer sets to puzzle answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Logot+GPT-4o: reported to achieve near-perfect accuracy (stated as over 99% across all puzzles). Table 1 reports 100% on the three grid puzzles and near- or perfect scores on dynamic Blocks World subtasks (e.g., 97.5% on BW-GR and 100% on other BW subtasks in the table).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High solution accuracy indicates correct resolution of spatial constraints, but the paper emphasizes that the ASP engine provides the exact spatial reasoning (LLM's role is translation). No ablation demonstrates GPT-4o performing spatial reasoning without ASP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>When combined with Logot, GPT-4o outperforms smaller LLMs in accuracy (higher than GPT-4o-mini, Deepseek-V3) and outperforms baseline prompting/CoT/finetuning approaches reported. Also higher cost than smaller models per the paper's cost analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still susceptible to translation errors in the puzzle-state encoding stage; occasional mistakes in translating instances to ASP noted even with GPT-4o (Figures 7 and 8 show representative failure cases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8336.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8336.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller/cheaper GPT-4o family variant used as an LLM backbone in experiments with Logot, showing strong performance when paired with the ASP solver.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A smaller/cheaper variant of GPT-4o used for in-context few-shot translation to ASP; paper gives no implementation or parameter-size details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku, Hitori, Fillomino, Blocks World (GR, LG, PV, PJ)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles and dynamic Blocks World tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Used within Logot pipeline with few-shot prompts; ASP solver (Clingo) executes generated programs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Same translation-to-ASP strategy as other LLM backbones; relies on declarative ASP solving for spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Logot+GPT-4o-mini: paper reports 100% accuracy on Sudoku, Hitori, Fillomino and strong dynamic-task results (example table: 95.5% BW-GR, 99.5% BW-LG, 98.5% BW-PV, 95.0% BW-PJ). Paper text states Logot with GPT-4o-mini achieves strong accuracy (one of the reported figures: over 98% for one of the smaller models).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>As with GPT-4o, the spatial reasoning is realized via ASP. High task accuracy indicates correct enforcement of spatial constraints when the translation is correct; no direct probe of intrinsic LLM spatial reasoning is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performance substantially exceeds Standard prompting and many baseline configurations; somewhat lower than GPT-4o in some dynamic subtasks but still strong and more cost-effective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Subject to the same translation failures as other LLMs; dependent on curated few-shot examples; occasional state-translation errors reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8336.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8336.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open/academic LLM referenced and used in experiments as a more affordable model; used as a Logot backbone in some evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM (technical report cited in refs) used as a lower-cost translation backbone; the paper does not provide architecture or parameter counts but cites Deepseek-V3 as one of the evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku, Hitori, Fillomino, Blocks World tasks</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles and dynamic Blocks World tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Used within the Logot pipeline and evaluated under the same few-shot translation + ASP solving setup; also used in baseline prompting/CoT variants in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>LLM-based translation to ASP with few-shot examples; ASP solver enforces spatial constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used with Logot, the paper reports strong accuracy for smaller models; the text specifically states Logot with Deepseek-V3 achieves strong accuracy (paper phrases examples as 'over 95%' for one smaller model). Exact per-task numbers for Deepseek-V3 appear in experimental tables (paper indicates robust performance though less than GPT-4o in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Correct solutions achieved indicate that the ASP solver handles spatial reasoning; Deepseek-V3's role is translation. No direct evidence that Deepseek-V3 alone performs spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to direct prompting with Deepseek-V3 (which fails on grid puzzles), Logot+Deepseek-V3 substantially improves results. Logot with larger models yields higher accuracy but at higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same translation-stage sensitivity; occasional mis-encodings of puzzle state. Lower raw capability than GPT-4o so may produce more translation errors without careful prompts/examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8336.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8336.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuned LMs (RoBERTa, GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuned models (examples: RoBERTa, GPT-2) reported from prior work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuned language models reported in the literature and used as baselines in comparisons; they represent task-specific supervised approaches rather than in-context translation to ASP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Finetuned RoBERTa / GPT-2 (baseline references)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task-specific finetuned models reported from prior work (He et al., 2023 and other sources) used as baseline comparisons; the paper does not finetune these models itself but uses reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Blocks World dynamic tasks (reported baselines); grid puzzles often N/A for finetuned baselines in this work</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Dynamic action-based spatial tasks (Blocks World)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Finetuned on task-specific datasets (prior work); used as a baseline in Table 1 (results reported from He et al. 2023 or other sources).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Supervised finetuning on labeled instances to perform the target task directly (as opposed to translation-to-ASP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported finetune baselines achieve reasonable performance on some Blocks World tasks (paper cites reported numbers, e.g., finetuned models achieving high 80sâ€“99% on certain dynamic tasks in prior work), but finetuned approaches are not directly comparable on grid puzzles where configuration differs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Finetuned models may learn task-specific patterns for dynamic reasoning; paper does not provide new probes or ablations for their spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Logot outperforms many baseline finetuned and prompting approaches on grid puzzles and matches or surpasses performance on dynamic tasks when using strong LLM backbones plus ASP.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Finetuned approaches can require large labeled datasets and may not generalize across diverse puzzle types; not explored in depth in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8336.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8336.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Minesweeper mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (assessed on Minesweeper in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is mentioned in related work as having been assessed on Minesweeper puzzles and found to have consistent reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced prior-evaluation of GPT-4 on Minesweeper from Yinghao Li et al., 2024; the present paper cites that study as evidence that state-of-the-art LLMs can struggle on combinatorial/structured puzzles without external symbolic tools.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Minesweeper (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based puzzle requiring local adjacency reasoning and spatial inference</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Prior work (cited) evaluated GPT-4 directly on Minesweeper and observed reasoning failures; details are in the cited work, not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Direct prompting/evaluation of GPT-4 on Minesweeper (per cited study); no ASP integration in that cited evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited prior study reported consistent reasoning failures for GPT-4 on Minesweeper (no numeric metrics reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The cited study is used as evidence that pure LLM prompting can fail on combinatorial spatial puzzles; this paper uses that motivation to justify integrating LLMs with symbolic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Used as contrast: pure LLM (GPT-4) evaluations on Minesweeper show failures, whereas Logot (LLM->ASP) achieves high accuracy on evaluated grid puzzles here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper cites prior findings that GPT-4 exhibits consistent reasoning failures on Minesweeper; details and failure modes are in the referenced study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study <em>(Rating: 2)</em></li>
                <li>PuzzleBench: Can LLMs solve challenging first-order combinatorial reasoning problems? <em>(Rating: 2)</em></li>
                <li>Mathematical definition and systematization of puzzle rules <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8336",
    "paper_id": "paper-278789523",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Logot",
            "name_full": "Logic-of-Thought (Logot)",
            "brief_description": "A hybrid framework introduced in this paper that uses large language models to translate natural-language puzzle rules and states into Answer Set Programs (ASP), then uses an ASP solver (Clingo) to perform exact, exhaustive inference and produce puzzle solutions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Logot (LLM+ASP pipeline)",
            "model_description": "A two-stage pipeline: (1) few-shot in-context prompting of an LLM to translate puzzle rule specifications and puzzle states into declarative ASP code; (2) grounding and solving of the generated ASP program with Clingo, followed by lightweight postprocessing to produce human-readable answers.",
            "model_size": null,
            "puzzle_name": "Sudoku, Hitori, Fillomino, Blocks World tasks (Projection, Legality, Plan Verification, Goal Recognition)",
            "puzzle_type": "Classic grid-based logic puzzles (Sudoku, Hitori, Fillomino) requiring 2D spatial constraints; dynamic action-based spatial planning (Blocks World) requiring reasoning about 'on' relations and clear/topology of blocks.",
            "task_setup": "Few-shot in-context examples provided separately for rule-specification translation (D_r) and puzzle-state translation (D_q). The LLM is prompted to output ASP translations of rules and states; the translations are concatenated, executed by Clingo to get answer sets; a decoder converts ASP atoms to the puzzle's expected answer format. Baselines: Standard Prompting, Chain-of-Thought (CoT), and Finetuning.",
            "mechanisms_or_strategies": "Declarative translation: LLMs produce ASP rules (choice rules, constraints, facts) and state encodings; Clingo performs exhaustive search/inference. Uses few-shot demonstration examples for translation. Also discusses potential use of Program-of-Thought (PoT) to improve translation robustness.",
            "performance_metrics": "Logot+GPT-4o: near-perfect accuracy (reported as &gt;99% across all evaluated puzzles). Logot+GPT-4o-mini: reported 100% accuracy on the three grid puzzles and strong performance on Blocks World (example table shows 95.5% BW-GR, 99.5% BW-LG, 98.5% BW-PV, 95.0% BW-PJ). The paper states Logot with smaller models (Deepseek-V3 or GPT-4o-mini) still achieves strong accuracy (reported as over 95% and 98%, respectively). Standard prompting produced 0% on grid puzzles; CoT is N/A for grid puzzles and competitive on some dynamic tasks.",
            "evidence_of_spatial_reasoning": "The paper demonstrates that the ASP solver enforces spatial constraints (row/column/subgrid uniqueness, adjacency/connectedness for Hitori, polyomino regions for Fillomino, and 'on'/clear relations for Blocks World). The success of Logot indicates correct handling of spatial constraints, but the paper attributes precise spatial inference to the ASP solver rather than to pure LLM internal spatial reasoning; no direct probing or ablation shows the LLM itself performing spatial reasoning beyond translation.",
            "comparisons": "Compared to baselines (Standard Prompting, CoT, Finetuning), Logot substantially outperforms them on grid puzzles and achieves higher accuracy on dynamic Blocks World tasks. Paper reports Standard prompting and direct LLM answers often fail (0% on grid puzzles), CoT was competitive on some Blocks World subtasks, and finetuned models reported in prior work had mixed performance; specific numerical comparisons are provided in Table 1 of the paper.",
            "limitations_or_failure_cases": "Remaining failures mainly originate from incorrect puzzle-state translations produced by the LLM (i.e., wrong parsing/encoding of the instance), not from the ASP solver. Representative failure cases shown for Sudoku and Blocks World - Legality. Method relies on manually curated few-shot examples and accurate translation; occasional translation errors occur even with strong LLMs (e.g., GPT-4o). The evaluated puzzle set is modest in variety; generalization to broader puzzle families is left for future work.",
            "uuid": "e8336.0",
            "source_info": {
                "paper_title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (as used in experiments)",
            "brief_description": "A high-capability variant of the GPT-4 family used as an LLM backbone in experiments, serving as the translator from natural language to ASP in the Logot pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "State-of-the-art GPT family model referenced and used as the prompting backbone to perform few-shot translation of puzzle rules and states into ASP; exact architecture/training details and parameter count are not provided in the paper.",
            "model_size": null,
            "puzzle_name": "Sudoku, Hitori, Fillomino, Blocks World (GR, LG, PV, PJ)",
            "puzzle_type": "Grid-based logic puzzles and dynamic action-based spatial planning",
            "task_setup": "Used within Logot: few-shot prompts for rule and state translation; output ASP executed by Clingo. Evaluated against direct prompting, CoT baselines, and finetuned models.",
            "mechanisms_or_strategies": "Few-shot in-context translation to ASP; relied on Clingo for exact spatial/integrity constraints. Postprocessing decodes ASP answer sets to puzzle answers.",
            "performance_metrics": "Logot+GPT-4o: reported to achieve near-perfect accuracy (stated as over 99% across all puzzles). Table 1 reports 100% on the three grid puzzles and near- or perfect scores on dynamic Blocks World subtasks (e.g., 97.5% on BW-GR and 100% on other BW subtasks in the table).",
            "evidence_of_spatial_reasoning": "High solution accuracy indicates correct resolution of spatial constraints, but the paper emphasizes that the ASP engine provides the exact spatial reasoning (LLM's role is translation). No ablation demonstrates GPT-4o performing spatial reasoning without ASP.",
            "comparisons": "When combined with Logot, GPT-4o outperforms smaller LLMs in accuracy (higher than GPT-4o-mini, Deepseek-V3) and outperforms baseline prompting/CoT/finetuning approaches reported. Also higher cost than smaller models per the paper's cost analysis.",
            "limitations_or_failure_cases": "Still susceptible to translation errors in the puzzle-state encoding stage; occasional mistakes in translating instances to ASP noted even with GPT-4o (Figures 7 and 8 show representative failure cases).",
            "uuid": "e8336.1",
            "source_info": {
                "paper_title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT-4o-mini (as used in experiments)",
            "brief_description": "A smaller/cheaper GPT-4o family variant used as an LLM backbone in experiments with Logot, showing strong performance when paired with the ASP solver.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "A smaller/cheaper variant of GPT-4o used for in-context few-shot translation to ASP; paper gives no implementation or parameter-size details.",
            "model_size": null,
            "puzzle_name": "Sudoku, Hitori, Fillomino, Blocks World (GR, LG, PV, PJ)",
            "puzzle_type": "Grid-based logic puzzles and dynamic Blocks World tasks",
            "task_setup": "Used within Logot pipeline with few-shot prompts; ASP solver (Clingo) executes generated programs.",
            "mechanisms_or_strategies": "Same translation-to-ASP strategy as other LLM backbones; relies on declarative ASP solving for spatial reasoning.",
            "performance_metrics": "Logot+GPT-4o-mini: paper reports 100% accuracy on Sudoku, Hitori, Fillomino and strong dynamic-task results (example table: 95.5% BW-GR, 99.5% BW-LG, 98.5% BW-PV, 95.0% BW-PJ). Paper text states Logot with GPT-4o-mini achieves strong accuracy (one of the reported figures: over 98% for one of the smaller models).",
            "evidence_of_spatial_reasoning": "As with GPT-4o, the spatial reasoning is realized via ASP. High task accuracy indicates correct enforcement of spatial constraints when the translation is correct; no direct probe of intrinsic LLM spatial reasoning is provided.",
            "comparisons": "Performance substantially exceeds Standard prompting and many baseline configurations; somewhat lower than GPT-4o in some dynamic subtasks but still strong and more cost-effective.",
            "limitations_or_failure_cases": "Subject to the same translation failures as other LLMs; dependent on curated few-shot examples; occasional state-translation errors reported.",
            "uuid": "e8336.2",
            "source_info": {
                "paper_title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Deepseek-V3",
            "name_full": "Deepseek-V3",
            "brief_description": "An open/academic LLM referenced and used in experiments as a more affordable model; used as a Logot backbone in some evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Deepseek-V3",
            "model_description": "An LLM (technical report cited in refs) used as a lower-cost translation backbone; the paper does not provide architecture or parameter counts but cites Deepseek-V3 as one of the evaluated models.",
            "model_size": null,
            "puzzle_name": "Sudoku, Hitori, Fillomino, Blocks World tasks",
            "puzzle_type": "Grid-based logic puzzles and dynamic Blocks World tasks",
            "task_setup": "Used within the Logot pipeline and evaluated under the same few-shot translation + ASP solving setup; also used in baseline prompting/CoT variants in experiments.",
            "mechanisms_or_strategies": "LLM-based translation to ASP with few-shot examples; ASP solver enforces spatial constraints.",
            "performance_metrics": "When used with Logot, the paper reports strong accuracy for smaller models; the text specifically states Logot with Deepseek-V3 achieves strong accuracy (paper phrases examples as 'over 95%' for one smaller model). Exact per-task numbers for Deepseek-V3 appear in experimental tables (paper indicates robust performance though less than GPT-4o in some cases).",
            "evidence_of_spatial_reasoning": "Correct solutions achieved indicate that the ASP solver handles spatial reasoning; Deepseek-V3's role is translation. No direct evidence that Deepseek-V3 alone performs spatial reasoning.",
            "comparisons": "Compared to direct prompting with Deepseek-V3 (which fails on grid puzzles), Logot+Deepseek-V3 substantially improves results. Logot with larger models yields higher accuracy but at higher cost.",
            "limitations_or_failure_cases": "Same translation-stage sensitivity; occasional mis-encodings of puzzle state. Lower raw capability than GPT-4o so may produce more translation errors without careful prompts/examples.",
            "uuid": "e8336.3",
            "source_info": {
                "paper_title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Finetuned LMs (RoBERTa, GPT-2)",
            "name_full": "Finetuned models (examples: RoBERTa, GPT-2) reported from prior work",
            "brief_description": "Finetuned language models reported in the literature and used as baselines in comparisons; they represent task-specific supervised approaches rather than in-context translation to ASP.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Finetuned RoBERTa / GPT-2 (baseline references)",
            "model_description": "Task-specific finetuned models reported from prior work (He et al., 2023 and other sources) used as baseline comparisons; the paper does not finetune these models itself but uses reported numbers.",
            "model_size": null,
            "puzzle_name": "Blocks World dynamic tasks (reported baselines); grid puzzles often N/A for finetuned baselines in this work",
            "puzzle_type": "Dynamic action-based spatial tasks (Blocks World)",
            "task_setup": "Finetuned on task-specific datasets (prior work); used as a baseline in Table 1 (results reported from He et al. 2023 or other sources).",
            "mechanisms_or_strategies": "Supervised finetuning on labeled instances to perform the target task directly (as opposed to translation-to-ASP).",
            "performance_metrics": "Reported finetune baselines achieve reasonable performance on some Blocks World tasks (paper cites reported numbers, e.g., finetuned models achieving high 80sâ€“99% on certain dynamic tasks in prior work), but finetuned approaches are not directly comparable on grid puzzles where configuration differs.",
            "evidence_of_spatial_reasoning": "Finetuned models may learn task-specific patterns for dynamic reasoning; paper does not provide new probes or ablations for their spatial reasoning.",
            "comparisons": "Logot outperforms many baseline finetuned and prompting approaches on grid puzzles and matches or surpasses performance on dynamic tasks when using strong LLM backbones plus ASP.",
            "limitations_or_failure_cases": "Finetuned approaches can require large labeled datasets and may not generalize across diverse puzzle types; not explored in depth in this paper.",
            "uuid": "e8336.4",
            "source_info": {
                "paper_title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4 (Minesweeper mention)",
            "name_full": "GPT-4 (assessed on Minesweeper in prior work)",
            "brief_description": "GPT-4 is mentioned in related work as having been assessed on Minesweeper puzzles and found to have consistent reasoning failures.",
            "citation_title": "Assessing logical puzzle solving in large language models: Insights from a minesweeper case study",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Referenced prior-evaluation of GPT-4 on Minesweeper from Yinghao Li et al., 2024; the present paper cites that study as evidence that state-of-the-art LLMs can struggle on combinatorial/structured puzzles without external symbolic tools.",
            "model_size": null,
            "puzzle_name": "Minesweeper (mentioned in related work)",
            "puzzle_type": "Grid-based puzzle requiring local adjacency reasoning and spatial inference",
            "task_setup": "Prior work (cited) evaluated GPT-4 directly on Minesweeper and observed reasoning failures; details are in the cited work, not in this paper.",
            "mechanisms_or_strategies": "Direct prompting/evaluation of GPT-4 on Minesweeper (per cited study); no ASP integration in that cited evaluation.",
            "performance_metrics": "Cited prior study reported consistent reasoning failures for GPT-4 on Minesweeper (no numeric metrics reproduced in this paper).",
            "evidence_of_spatial_reasoning": "The cited study is used as evidence that pure LLM prompting can fail on combinatorial spatial puzzles; this paper uses that motivation to justify integrating LLMs with symbolic solvers.",
            "comparisons": "Used as contrast: pure LLM (GPT-4) evaluations on Minesweeper show failures, whereas Logot (LLM-&gt;ASP) achieves high accuracy on evaluated grid puzzles here.",
            "limitations_or_failure_cases": "Paper cites prior findings that GPT-4 exhibits consistent reasoning failures on Minesweeper; details and failure modes are in the referenced study.",
            "uuid": "e8336.5",
            "source_info": {
                "paper_title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Assessing logical puzzle solving in large language models: Insights from a minesweeper case study",
            "rating": 2,
            "sanitized_title": "assessing_logical_puzzle_solving_in_large_language_models_insights_from_a_minesweeper_case_study"
        },
        {
            "paper_title": "PuzzleBench: Can LLMs solve challenging first-order combinatorial reasoning problems?",
            "rating": 2,
            "sanitized_title": "puzzlebench_can_llms_solve_challenging_firstorder_combinatorial_reasoning_problems"
        },
        {
            "paper_title": "Mathematical definition and systematization of puzzle rules",
            "rating": 1,
            "sanitized_title": "mathematical_definition_and_systematization_of_puzzle_rules"
        }
    ],
    "cost": 0.01789575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language
22 May 2025</p>
<p>Naiqi Li 
Tsinghua Shenzhen International Graduate School</p>
<p>Peiyuan Liu 
Tsinghua Shenzhen International Graduate School</p>
<p>Zheng Liu 
Tao Dai 
Tsinghua Shenzhen International Graduate School</p>
<p>Shenzhen University</p>
<p>Yong Jiang jiangy@sz.tsinghua.edu.cn 
Tsinghua Shenzhen International Graduate School</p>
<p>Shu-Tao Xia 
Tsinghua Shenzhen International Graduate School</p>
<p>Tom Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Wenhu Chen 
Xueguang Ma 
Xinyi Wang 
William W 2022 Cohen 
William W 2023 Cohen 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Martin Gebser 
Benjamin Kaufmann 
Roland Kaminski 
Max Ostrowski 
Torsten Schaub 
Marius Schnei- Der 
Potassco 
Nicola Leone 
Francesco 2015 Ricca </p>
<p>Maarten Bosma
Gaurav Mishra, Hyung Won Chung, SebasAdam Roberts, Paul Barham, Charles Sutton</p>
<p>Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language
22 May 20253D05158B14EA3DBF2F242FBBB031BD1BarXiv:2505.16114v1[cs.AI]
Solving puzzles in natural language poses a long-standing challenge in AI.While large language models (LLMs) have recently shown impressive capabilities in a variety of tasks, they continue to struggle with complex puzzles that demand precise reasoning and exhaustive search.In this paper, we propose Logic-of-Thought (Logot), a novel framework that bridges LLMs with logic programming to address this problem.Our method leverages LLMs to translate puzzle rules and states into answer set programs (ASPs), the solution of which are then accurately and efficiently inferred by an ASP interpreter.This hybrid approach combines the natural language understanding of LLMs with the precise reasoning capabilities of logic programs.We evaluate our method on various grid puzzles and dynamic puzzles involving actions, demonstrating nearperfect accuracy across all tasks.Our code and data are available at: https://github.com/naiqili/Logic-of-Thought.</p>
<p>Introduction</p>
<p>Solving puzzles expressed in natural language has long been regarded as a cornerstone task for artificial intelligence, which requires a diverse set of advanced skills, including natural language understanding, logical reasoning, abstract thinking, and the ability to plan and infer (Mittal et al., 2024;Li et al., 2024).</p>
<p>In recent years, large language models (LLMs) such as GPT (Brown et al., 2020), PaLM (Chowdhery et al., 2023), andLLaMA (Touvron et al., 2023) have shown remarkable progress in tasks like numerical and commonsense reasoning.These models have demonstrated impressive capabilities in few-shot and zero-shot learning, which lead to the development of several prompting strategies including Chain-of-Thought (CoT) (Wei et al., 2022), Tree-of-Thought (ToT) (Yao et al., 2023), and Program-of-Thought (PoT) (Chen et al., 2023).</p>
<p>Puzzle Example: Hitori</p>
<p>Rule Specification: Hitori is a puzzle on a grid filled with numbers.The objective is to eliminate duplicates by marking some cells as black in each row and column according to following rules: 1) Eliminate numbers by marking them black, so that no row or column has duplicate numbers; 2) Blackened cells cannot be horizontally or vertically adjacent; 3) Not blackened cells must form a single connected group.Question Instance: Answer Instance: These prompting techniques aim to guide LLMs to reason more systematically, and have further enhanced LLMs' ability in handling simple tasks.</p>
<p>However, when facing more involved puzzles such as Hitori in Figure 1, current LLM-based methods are still inadequate.The puzzles considered in this work require a combination of deep reasoning and precise understanding.Even small misinterpretations can lead to entirely incorrect solutions, as there is little tolerance for ambiguity or error in the problem representation.The need for precise understanding, extensive search, and exact inference exceeds the capabilities of current LLMs and prompting methods.</p>
<p>To address these challenges, we propose to bridge LLM and logic programming.Logic programs have been well-investigated and are with broad applications in planning, diagnosis, and knowledge representation (Lifschitz, 2019;Kowalski, 2014).A key principle of logic programming is its declarative nature: instead of specifying how to solve a problem, one specifies what the problem is, which contrasts with popular algorithmic languages.The advantages of logic programs include elaboration tolerance (small changes in specification result in minimal changes to the program), separation of knowledge and reasoning algorithms, and the use of powerful off-the-shelf solvers that can perform systematic and exhaustive search (Gelfond and Kahl, 2014).</p>
<p>In this paper, we propose a novel framework called Logic-of-Thought (Logot), which combines the strengths of large language models with the reasoning capabilities of logic programs.Our key idea is to first translate puzzle descriptions into formal logic programs using LLMs.Then logic program interpreters are used to search for solutions with utmost accuracy and high efficiency.This hybrid approach leverages the language understanding and generative capacity of LLMs, while offloading the heavy reasoning and search tasks to well-established logic program engines.</p>
<p>Our main contributions are as follows:</p>
<p>â€¢ We propose a novel framework to solve puzzles in natural language, which is a task that remains to be a major challenge.</p>
<p>â€¢ We introduce a new paradigm that integrates LLMs with logic programming.</p>
<p>â€¢ We curate a diverse set of benchmark puzzles to evaluate our method, including three grid puzzles and four dynamic puzzles involving actions.</p>
<p>â€¢ Our experiments demonstrate near-perfect accuracy across all puzzle types, showing the effectiveness and generality of our method.</p>
<p>â€¢ We make our code and dataset publicly available to facilitate future research.</p>
<p>2 Related Work</p>
<p>Puzzle Solving in Computer Science</p>
<p>The endeavor of employing machines for puzzle solving can date back to Alan Turing, who cracked the Enigma code during World War II.In modern research communities, solving puzzles expressed in natural language has long been regarded as a cornerstone task, as it require a diverse set of advanced cognitive skills, including natural language understanding, logical reasoning, abstract thinking, and the ability to plan and infer.Successfully solving such tasks is crucial for showing an agent can approach human-level intelligence (Lake et al., 2017;Bisk et al., 2020).</p>
<p>Early AI research attempt puzzle solving via formal logic and mathematical modeling.Some recent works still follow this paradigm: Costa and PoÃ§as (2018) introduced a tableaux-based deductive system capable of solving Smullyan's puzzles involving propositional logic and first-order logic.Groza (2021) illustrated how various logic puzzles can be modeled in first-order logic, and then be solved by mathematical proving tools.Maeda and Inoue (2024) proposed a mathematical framework to formalize pencil puzzles like Slither and Sudoku, allowing for the application of constraint-based solvers.</p>
<p>Meanwhile, planning tasks that require searching a sequence of actions to transition from an initial state to a goal state, can also be viewed as a type of structured puzzles.Formal approaches to planning have leveraged logical representations such as STRIPS (Fikes and Nilsson, 1971) and the situation calculus (Reiter, 2001).The Blocks World domain, in particular, has served as a long-standing benchmark for evaluating planning algorithms, due to its simplicity in structure and implications in various reasoning capacities (Li et al., 2013;Li and Liu, 2015).</p>
<p>However, these methods face several practical limitations.First, they are tailored to specific puzzle types, making them difficult to generalize.More critically, translating puzzles from natural language into formal representations requires substantial manual effort and expertise.</p>
<p>Large Language Models</p>
<p>Large Language Models (LLMs) have witnessed rapid development in recent years since the introduction of the Transformer architecture (Vaswani et al., 2017), which laid the foundation for modern neural language models.GPT-2 (Radford et al., 2019) marked a significant leap by demonstrating the power of large-scale autoregressive training, which further leads to ChatGPT and GPT-4 (Mao et al., 2024;Kalyan, 2024).Current LLMs include both commercial models such as Claude and Gemini (Minaee et al., 2024), as well as open-source models like LLaMA (Touvron et al., 2023), Mistral (Jiang et al., 2023), and DeepSeek (Liu et al., 2024).</p>
<p>A remarkable property of LLMs is their ability to perform in-context learning and few-shot reasoning, i.e., a model can solve unseen tasks based on a few examples provided in the prompt without any parameter updates.Chain-of-Thought (CoT) (Wei et al., 2022) enables models to decompose problems into intermediate reasoning steps.Self-consistency (Wang et al., 2023) aggregates multiple reasoning paths to improve robustness.Least-to-Most prompting (Zhou et al., 2023) guides LLMs through progressive refinement.Tree-of-Thought (ToT) (Yao et al., 2023) introduces explicit decision paths with branching logic.Most relevant to our work is Program-of-Thought (PoT) (Chen et al., 2022), which use LLMs to generate algorithmic code (e.g., Python) to solve complex problems.However, our method differs fundamentally: instead of generating procedural programs, we generate declarative logic programs.Declarative programs specify what the solution should satisfy, rather than how to compute it.These prompting techniques have been successfully applied to a variety of tasks, including commonsense reasoning and arithmetic reasoning (Cobbe et al., 2021;Ling et al., 2017;Talmor et al., 2019).Nevertheless, recent studies have shown that even state-of-the-art LLMs struggle with solving combinatorial and structured puzzles.For instance, Li et al. (2024) assessed GPT-4's capability on Minesweeper puzzles and revealed consistent reasoning failures.Mittal et al. (2024) proposed PuzzleBench to systematically benchmark LLMs on first-order logic puzzles, concluding that current models still underperform without external tools.</p>
<p>Answer Set Programming</p>
<p>Answer Set Programming (ASP) is a wellestablished paradigm in the field of declarative programming and non-monotonic reasoning.It has a broad range of applications, due to its expressive power, clear semantics, and availability of efficient solvers (Baral, 2003;Gelfond and Kahl, 2014;Leone and Ricca, 2015).</p>
<p>An ASP program consists of a set of rules of the form:
L 0 ; ... ; L k :- L k+1 , ..., L m , not L m+1 , ..., not L n .
where each L i is a literal.Intuitively, if all literals in the body are true and none of the negated literals can be proven, then at least one literal in the head must be true.A rule with an empty body is called a fact, while a rule with an empty head is a constraint.ASP also supports choice rules, such as:
m {p(X) : q(X)} n :-L 1 , ..., not L n ,
which allows flexible inclusion of subsets of atoms in answer sets.To solve ASP programs, we employ Clingo (Gebser et al., 2011), a state-of-the-art ASP interpreter that performs both grounding and solving efficiently.</p>
<p>ASP has also been applied to puzzle solving (Mitra and Baral, 2015), where handcrafted programs were written to solve logic grid puzzles.However, the types of puzzles addressed are narrow and rely on highly regular logical structures.ASP programs must be manually designed, a process that is often non-trivial and requires deep expertise in both the puzzle and logic programming.</p>
<p>Our work addresses these limitations by proposing a novel method that combines the reasoning power of ASP with the natural language understanding capabilities of large language models.Instead of manually writing ASP programs, we leverage the in-context learning ability of LLMs to automatically translate natural language puzzle descriptions into executable ASP logic programs.This ushers in a new paradigm for solving complex puzzles that require precise understanding, deep reasonin, and exhaustive search.</p>
<p>Method</p>
<p>Overview</p>
<p>Problem formulation: We define a puzzle instance as a tuple P = âŸ¨B, R, I q âŸ©, where B represents background information (e.g., a short textual introduction of the puzzle), R = {R i } K i=1 represents a set of rules (e.g., blackened cells cannot be horizontally or vertically adjacent), and I q represents a particular question instance (e.g., the initial state of the grid).The goal is to design a function f (P ) = I a , such that the output answer instance I a is a proper solution for the puzzle P .</p>
<p>Figure 2 presents the overall framework of our method.The key idea is to translate the puzzle's rule specifications and question instance into the corresponding logic program separately.Subsequently, off-the-shelves logic programming interpreters are used to ground and search for solutions.During the translation, the in-context and few-shot learning abilities of large language models are utilized.To facilitate this, two sets of few-shot examples
D r = {âŸ¨R â€² i , Râ€² i âŸ©} N i=1 and D q = {âŸ¨Q â€² i , Qâ€² i âŸ©} M i=1
are introduced, which correspond to the translations of rule specifications and question instances.</p>
<p>Here âŸ¨R â€² i , Râ€² i âŸ© denotes the translation between the rule's textual form and logic program representation (similar for
âŸ¨Q â€² i , Qâ€² i âŸ©).
With these notations, the ultimate objective is to design a function f (P ; D r , D q ) = I a , such that the answer I a resolves the puzzle P .</p>
<p>In what follows, we use L(s) to denote the output of the LLM when s is given as input, and use s 1 |s 2 |...|s k to denote the concatenation of strings.</p>
<p>Few-Shot Rule Specification Translation Module</p>
<p>One of the core components of our framework is the Few-Shot Rule Specification Translation Module, which is designed to transform natural language rule descriptions R = {R i } K i=1 into formal logic programs { Ri } K i=1 that can be interpreted and executed by logic programming engines.These rules define the constraints and structure of the puzzle, and are expressed in natural language.</p>
<p>To accomplish this, we exploit the in-context learning capabilities of large language models (LLMs), using a set of few-shot examples D r = {âŸ¨R â€² i , Râ€² i âŸ©} N i=1 as demonstrations.Each pair in D r consists of a textual rule R â€² i and its corresponding logic representation Râ€² i .These demonstrations are drawn from puzzles of similar domains and serve as guidance for translating new rules.</p>
<p>Given a new rule R i âˆˆ R and the full puzzle instance P = âŸ¨B, R, I q âŸ©, the rule translation function is defined as:
RT (R i ; P, D r ) = L(R â€² 1 | Râ€² 1 | . . . |R â€² N | Râ€² N |B|R i ), RT (R; P, D r ) = {RT (R i ; P, D r ) | R i âˆˆ R} K i=1 ,
where L(â€¢) denotes the output of the LLM given the concatenated string of examples and the rule R i to be translated.The inclusion of the background description B provides additional context about the specific puzzle domain, helping the model ground its understanding of the rule semantics.</p>
<p>--(Few-shot examples from other puzzles)--R â€² 1 : Fill each row with the numbers 1 through 9 without repeating any number.Râ€² 1 : :-pos(X, Y1, N), pos(X, Y2, N), Y1 != Y2.R â€² 2 : Fill each column with the numbers 1 through 9 without repeating any number.----------------------B: Hitori is a puzzle on a grid filled with ... Ri: Eliminate numbers by marking them black, so no row or column has duplicate numbers.</p>
<p>-------(LLM output, Ri)-------:-pos(X, Y1, N), pos(X, Y2, N), Y1 != Y2, not black(X, Y1), not black(X, Y2).:-pos(X1, Y, N), pos(X2, Y, N), X1 != X2, not black(X1, Y), not black(X2, Y).Q â€² 1 : 112435 115446 ... Qâ€² 1 : pos(1, 1, 1).pos(1, 2, 1).pos(1, 3, 2).pos(1, 4, 4).pos(1, 5, 3).pos (1,6,5).pos(2, 1, 1).pos(2, 2, 1).pos (2,3,5) (1,3,5).pos (1,4,4).pos(1, 5, 4).pos(1, 6, 4).pos(1, 7, 6).pos(1, 8, 1).pos(1, 9, 4).pos(2, 1, 5).pos(2, 2, 7).pos(2, 3, 4).... This translation process allows the LLM to infer the appropriate formal structure for a rule based on analogical reasoning from the provided demonstrations.An example of such a translation is shown in Figure 3.
. ... ...... ---------------------- Iq: 875444614 594728321 ... -------(LLM output, Ä¨q)------- pos(1, 1, 8). pos(1, 2, 7). pos
By leveraging LLMs for rule translation, Logot presents a novel method for converting humanreadable rules into executable logic, serving as a critical bridge between natural language and symbolic reasoning in our system.</p>
<p>Few-Shot Puzzle State Translation Module</p>
<p>The Few-Shot Puzzle State Translation Module is responsible for converting the puzzle's initial state I q into ASP statements Ä¨q .</p>
<p>Given a puzzle instance P = âŸ¨B, R, I q âŸ© and a set of few-shot examples
D q = {âŸ¨Q â€² i , Qâ€² i âŸ©} M i=1
from the same puzzle domain, where Q â€² i is a textual representation of a puzzle's initial state and Qâ€² i is its corresponding logic encoding, the mod-ule generates the translated logic program Ä¨q by prompting the LLM as follows:
ST (I q ; P, D q ) = L(Q â€² 1 | Qâ€² 1 | . . . |Q â€² M | Qâ€² M |I q ).
Here L denotes the output of the language model given the prompt, which is constructed from concatenating M few-shot examples and the input I q .This formulation leverages the LLM's ability to learn translation patterns from a small number of demonstration pairs.An illustrative example of this translation process in the Hitori puzzle is shown in Figure 4.</p>
<p>Inference and Postprocessing</p>
<p>Once the puzzle rules and the question instance have been translated into their respective logic program representations, the next step is to perform automated reasoning to obtain a solution.</p>
<p>Specifically, we combine the outputs of the Rule Specification Translation Module and the Puzzle State Translation Module to form a complete answer set programming specification.We then invoke a state-of-the-art ASP solver, Clingo (Gebser et al., 2011), to compute the answer sets that satisfy all constraints in the program.Finally, a lightweight postprocessing component D(â€¢) is applied to convert the ASP output into a humanreadable answer instance I a .This decoder interprets the logic atoms in the answer set (e.g., black(2,3), pos(1,1,5)) and formats them into a structured representation consistent with the puzzle's original format.</p>
<p>Formally, the overall pipeline of our method can be represented as:
I a = f (P ; D r , D q ) = D (ASP (RT (R; P, D r ) âˆª {ST (I q ; P, D q )})) ,
where RT (R; P, D r ) denotes the translated set of puzzle rules, and ST (I q ; P, D q ) represents the logic encoding of the initial puzzle state.The union of these two components yields a complete ASP program that fully specifies the puzzle constraints and configuration.</p>
<p>To summarize, the key advantage of our method lies in its ability to seamlessly integrate the natural language understanding capabilities of large language models with the rigorous reasoning framework of logic programming, establishing a new paradigm for puzzle solving and reasoning.</p>
<p>Comparison</p>
<p>To demonstrate the effectiveness of our method, we compare it against several baselines across a variety of puzzles.</p>
<p>Baselines.We consider the following baseline methods:</p>
<p>â€¢ Standard Prompting: Directly prompts the language model with the puzzle description and expects the answer.</p>
<p>â€¢ Chain-of-Thought (CoT) (Wei et al., 2022): Prompts the model to generate intermediate reasoning steps before arriving at an answer.</p>
<p>â€¢ Finetuning: Finetunes language models on task-specific datasets.We adopt the results reported in (He et al., 2023).</p>
<p>Each baseline is evaluated using different LLM backbones, including Deepseek-V3, RoBERTa, GPT-2, GPT-4o-mini, and GPT-4o.</p>
<p>Accuracy Analysis.The comparison results are shown in Table 1."N/A" indicates that the method is not applicable or is difficult to adapt.For example, it is challenging to design effective chainof-thought (CoT) prompts for grid puzzles such as Sudoku.The results show that:</p>
<p>â€¢ Logot consistently achieves superior performance across both classic grid puzzles and dynamic puzzles with actions.</p>
<p>â€¢ Notably, Logot+GPT-4o achieves near-perfect accuracy on all tasks.</p>
<p>â€¢ Even when using smaller or less capable models like Deepseek-V3 or GPT-4o-mini, Logot still demonstrates robust performance.</p>
<p>â€¢ Although CoT performs competitively on some dynamic puzzles (e.g., BW-LG, BW-PJ), such applications were not explored in previous work (He et al., 2023), suggesting that further investigation into prompting strategies may yield useful insights.An example of the CoT prompts we used is shown in Figure 5.   Cost Analysis.We further analyze the costefficiency tradeoff of different methods, based on Table 2 and Figure 6.</p>
<p>Table 2 reports the total cost (in USD) required to solve 200 instances for each puzzle category.Figure 6 provides a clearer summary by illustrating the trade-off between cost and accuracy averaged over all instances in each puzzle category.</p>
<p>From the results, we observe that our proposed method is highly cost-effective.When paired with GPT-4o, Logot achieves near-perfect accuracy (over 99%) across all puzzles.Although this comes at a relatively higher cost, the performance gain is significant.</p>
<p>More importantly, Logot can also be deployed with more affordable language models, such as Deepseek-V3 or GPT-4o-mini.In these settings, it Translated ASP Program: ...... pos(1, 2, 5).pos(1, 5, 2).pos (1,6,3).pos (1,8,6).pos (1,9,4).pos(2, 1, 1).pos (2,3,4).pos(3, 1, 8).pos (3,2,6).pos(4, 1, 9).pos(4, 3, 5).pos(4, 9, 2).pos (5,4,5).pos (5,6,7).pos(6, 1, 4).pos (6,7,9).pos (7,2,9).pos (7,4,4).pos (7,7,8).pos (8,3,6).pos (8,4,7).pos(8, 8, 1).pos(9, 7, 5).pos(9, 8, 9).still achieves strong accuracy (over 95% and 98%, respectively), while incurring significantly lower costs.All evidences suggest that, Logot is not only highly accurate but also resource-efficient, making it practical and accessible for solving various challenging puzzles.</p>
<p>Failure Case Study</p>
<p>In this section, we analyze representative failure cases of our method to better understand its limitations and potential areas for improvement.Specifically, we focus on two puzzle categories: Sudoku (Figure 7) and Blocks World -Legality (Figure 8), which are representative of grid puzzles and dynamic puzzles involving actions.</p>
<p>Across all examined cases, we find that the few errorsstem entirely from the state translation stage, i.e., incorrect interpretation of the initial state.These mistakes are generally straightforward to identify and are easy to be corrected by humans.</p>
<p>Interestingly, even with powerful LLMs such as GPT-4o, these seemingly simple translation errors can still occur.This highlights a key challenge in achieving full robustness: while logic interpreters are precise and reliable, they depend heavily on accurate input representations, making the translation step a critical bottleneck.</p>
<p>To address this, one promising direction is to integrate more advanced prompting methods such as  Program-of-Thought (PoT), which uses language models to generate intermediate algorithmic code (e.g., Python) to assist with the transaction of rules and states.By combining PoT with our pipeline, it is possible to further enhance translation accuracy and eliminate residual errors, which we leave for future investigation.</p>
<p>Conclusion</p>
<p>In this paper, we proposed Logic-of-Thought (Logot), a novel framework that combines large language models with logic programming to solve puzzles expressed in natural language, which remains a challenging task.Our method leverages the fewshot in-context learning capabilities of LLMs to translate both puzzle rules and instances into declarative logic programs, which are then executed using an Answer Set Programming interpreter to accurately and efficiently infer solutions.Experiments show that Logot achieves near-perfect accuracy across a range of puzzle at a reasonable cost.Overall, Logot introduces a new paradigm that combines the strengths of LLM and logic programming, with potential applications extending well beyond the puzzle domain.</p>
<p>Limitations</p>
<p>While our proposed method demonstrates strong performance across a range of puzzle types, we acknowledge a few minor limitations that open up promising directions for future research.First, the number of puzzle types and categories evaluated in this work is relatively modest.Future work can further enhance generality by curating a broader and richer collection of puzzles, particularly by exploring more tasks in the planning domain.Second, our method currently relies on manually annotated few-shot examples to bootstrap performance, which is a common setup in in-context learning.Nonetheless, future research could explore the information retrieval techniques Rubin et al. (2022) to automatically retrieve high-quality examples, potentially reducing the annotation effort even further.Finally, occasional errors still occur during the puzzle state translation stage.The accuracy of our method can be further improved using advanced prompting techniques such as Program-of-Thought (PoT), or by leveraging diagnostic tools from the logic programming community.</p>
<p>A Task Description</p>
<p>A.1 Classic Grid Puzzles Sudoku Sudoku is a popular logic-based number placement puzzle.The puzzle is played on a 9Ã—9 grid divided into nine 3Ã—3 subgrids (also called regions, boxes, or blocks).The goal is to fill in the grid so that each number appears exactly once in each row, column, and 3Ã—3 subgrid.Specifically, the puzzle requires completing grid under the following rules:</p>
<ol>
<li>
<p>Fill each row with the numbers 1 through 9 without repeating any number.</p>
</li>
<li>
<p>Fill each column with the numbers 1 through 9 without repeating any number.</p>
</li>
<li>
<p>Fill each 3Ã—3 subgrid with the numbers 1 through 9 without repeating any number.</p>
</li>
</ol>
<p>Figure 9 presents a question-answer pair of the Sudoku puzzle.</p>
<p>Hitori</p>
<p>Hitori is a logic-based puzzle game originating from Japan.The name "Hitori" means "alone" or "one person" in Japanese, reflecting the puzzle's goal of isolating numbers.It is typically played on a square grid filled with numbers, and the objective is to eliminate duplicates in each row and column according to following rules:</p>
<ol>
<li>
<p>Eliminate numbers by marking them (usually shaded or blacked out) so that no row or column has duplicate numbers.</p>
</li>
<li>
<p>You cannot shade two adjacent cells (cells sharing an edge) -shaded cells must not touch horizontally or vertically.</p>
</li>
<li>
<p>All unshaded (white) cells must form a single connected group, meaning you can move from any unshaded cell to any other through neighboring unshaded cells.</p>
</li>
<li>
<p>A shaded cell is considered "eliminated" and cannot be part of the connected group.</p>
</li>
</ol>
<p>Figure 10 presents a question-answer pair of the Hitori puzzle.</p>
<p>Fillomino</p>
<p>Fillomino is a logic puzzle played on a rectangular grid where some cells may initially contain numbers.The goal is to divide the grid into regions, or "polyominoes," such that each region contains exactly one number and has an area (number of cells) equal to that number.The specific rules are:</p>
<ol>
<li>
<p>Divide the grid into regions where each region consists of connected cells (horizontally or vertically adjacent).</p>
</li>
<li>
<p>Each region must contain exactly one number that matches the total number of cells in that region.</p>
</li>
<li>
<p>Regions of the same size must not be orthogonally adjacent (they cannot share a side).</p>
</li>
<li>
<p>Empty cells must be filled with numbers during solving to satisfy the above conditions.</p>
</li>
</ol>
<p>Figure 11 presents a question-answer pair of the Fillomino puzzle.</p>
<p>A.2 Task with Actions</p>
<p>Blocks World is a classic domain in knowledge representation and action reasoning.It involves a set of blocks placed on a table, where blocks can be stacked or moved by an agent under certain physical constraints.</p>
<p>Specifically, the domain is formalized using the following fluents-state properties that may change over time due to actions-and a set of corresponding actions:</p>
<p>Fluents:</p>
<p>â€¢ on(x, y): block x is on top of y, where y is a location that can be a block or the table.</p>
<p>â€¢ clear(x): block x has nothing on top of it.</p>
<p>Actions:</p>
<p>â€¢ move(x, y, z): move a block x from location y to location z.</p>
<p>Our dataset is based on the recent work of He et al. ( 2023), which introduces a structured and large-scale Blocks World benchmark for exploring the capacity of pretrained language models for reasoning about actions and change.We consider the four sub-tasks proposed in their paper, i.e., projection, legality, plan verification, and goal recognition.</p>
<p>Projection</p>
<p>The projection task focuses on reasoning about the outcomes of actions.It involves determining whether a given proposition will be true after performing a specific sequence of actions starting from an initial state.Example 1 (Projection Task).The following is an example of negative cases, i.e., the query proposition does not hold after performing the action sequence from the given initial state.â€¢ Action sequence: Jane moves the turquoise block from the teal block to the brown block.Jane moves the turquoise block from the brown block onto the table.Jane moves the tan block from the table to the turquoise block.</p>
<p>â€¢ Query: The teal block is on top of the brown block.The green block is clear.</p>
<p>â€¢ Label: False.</p>
<p>Legality</p>
<p>This task focuses on action preconditions.It requires determining whether a given sequence of actions can be executed in order, starting from an initial state, without violating any preconditions.</p>
<p>Example 2 (Legality Task).The following is an example of negative cases, i.e., the query action sequence is not executable in the given state.â€¢ Query: Jane moves the green block from the blue block onto the table.Jane moves the green block from the purple block to the blue block.Jane moves the pink block from the green block to the blue block.</p>
<p>â€¢ Label: False.</p>
<p>Plan Verification</p>
<p>Planning involves identifying a sequence of actions that leads to a desired outcome.In this task, the focus is on verifying whether a given sequence of actions, when applied to an initial state, successfully results in the intended goal.</p>
<p>Example 3 (Plan Verification Task).The following is an example of negative cases, i.e., starting from the given state, the query action sequence leads to a state satisfying the goal.navy block.The silver block is on the table.The magenta block is clear.The violet block is clear.The magenta block is on the table.</p>
<p>â€¢ Goal: The silver block is not on the table and the red block is on the table.</p>
<p>â€¢ Query: Jane moves the violet block from the navy block to the magenta block.Jane moves the navy block from the silver block to the red block.Jane moves the silver block from the table to the navy block.</p>
<p>â€¢ Label: True.</p>
<p>Goal Recognition</p>
<p>This task focuses on identifying the intended goal based on a partial observation of actions.Starting from an initial state and given a candidate goal along with an observed sequence of actions, the objective is to determine whether the observed actions are consistent with pursuing that goal-specifically, whether they form the beginning of an optimal plan to achieve it.</p>
<p>Example 4 (Goal Recognition Task).The following is an example of negative cases, i.e., the observed sequence of actions is not a prefix of any optimal plan that achieves the goal.pos_(1, 1, 0). pos_(1, 2, 0).pos_(1, 3, 0).pos_ (1,4,8).pos_(1, 5, 5)... pos_(2, 1, 0). pos_(2, 2, 2).pos_(2, 3, 0).pos_(2, 4, 0).pos_(2, 5, 0)... pos_(3, 1, 0). pos_(3, 2, 1).pos_(3, 3, 0).pos_ (3,4,9).pos_(3, 5, 0)... pos_(4, 1, 0). pos_(4, 2, 7).pos_(4, 3, 0).pos_(4, 4, 0).pos_(4, 5, 2)... pos_(5, 1, 4).pos_(5, 2, 0).pos_(5, 3, 2).pos_(5, 4, 0).pos_(5, 5, 0)... pos_(6, 1, 0). pos_(6, 2, 0).pos_(6, 3, 0).pos_(6, 4, 0).pos_(6, 5, 0)... pos_(7, 1, 0). pos_ (7,2,9).pos_ (7,3,7).pos_ (7,4,5).pos_(7, 5, 0)... pos_(8, 1, 5).pos_ (8,2,6).pos_ (8, 3, 3). pos_(8, 4, 0).pos_(8, 5, 0)... pos_(9, 1, 0). pos_(9, 2, 0).pos_(9, 3, 0).pos_(9, 4, 0).pos_(9, 5, 0)... Output: pos (1,4,8).pos(1, 5, 1).pos(2, 2, 2).pos(3, 2, 1).pos (3,4,9).pos(3, 7, 7).pos(4, 2, 7).pos(4, 5, 2).pos(4, 6, 5).pos(4, 8, 9).pos(4, 9, 3).pos(5, 1, 4).pos(5, 3, 2).pos(6, 7, 5).pos (7,2,9).pos (7,3,7).pos (7,4,5).pos(8, 1, 5).pos (8,2,6).pos (8,3,3).pos(8, 9, 4).pos(9, 7, 6).pos (9,8,8).</p>
<p>Input: {input}</p>
<p>In the output, pos(x, y, n) means the number of row x and column y is n.A cell with 0 is skipped.Complete the text of Thought and Output.Only show the result, do not explain.You are a Sudoku-solving assistant.Given a 9x9 Sudoku puzzle, where each row is a string of 9 digits and '0' represents an empty cell, solve the puzzle and print the completed Sudoku grid.</p>
<p>Input: {input}</p>
<p>Figure 14: A standard prompt for solving the Sudoku puzzle.</p>
<p>Figure 1 :
1
Figure 1: An example of the Hitori puzzle.</p>
<p>Figure 2 :
2
Figure 2: The overall framework of Logot.</p>
<p>Râ€² 2 : :-pos(X1, Y, N), pos(X2, Y, N), X1 != X2. ......</p>
<p>Figure 3 :
3
Figure 3: An example of rule translation in the Hitori puzzle.</p>
<p>Figure 4 :
4
Figure 4: An example of puzzle state translation in the Hitori puzzle.</p>
<p>Figure 6 :
6
Figure 6: Accuracy-cost tradeoff of different methods.</p>
<p>Figure 7 :
7
Figure 7: A failure case of the Sudoku puzzle.</p>
<p>Figure 8 :
8
Figure 8: A failure case of the Blocks World -Legality puzzle.</p>
<p>Figure 9 :
9
Figure 9: An example of the Sudoku puzzle.(Left) Puzzle question; (Right) Puzzle answer.</p>
<p>Figure 10 :
10
Figure 10: An example of the Hitori puzzle.(Left) Puzzle question; (Right) Puzzle answer.</p>
<p>Figure 11 :
11
Figure 11: An example of the Fillomino puzzle.(Left) Puzzle question; (Right) Puzzle answer.</p>
<p>Figure 13 :
13
Figure 13: Prompts for translating state representations to ASP programs in the Sudoku puzzle.</p>
<p>Table 1 :
1
Accuracy of different methods on grid puzzles(Sudoku, Hitori, Fillomino)and dynamic puzzles in the Blocks World domain (Goal Recognition -GR, Legality -LG, Plan Verification -PV, Projection -PJ).We evaluate our method on two types of puzzles: grid puzzles and dynamic puzzles involving actions.Grid Puzzles.These include Sudoku, Hitori, and Fillomino, which are widely known and require nontrivial combinatorial search.Solving them demands precise rule interpretation and reasoning over a large search space.For each of the three puzzles, we collect 200 instances with corresponding ground-truth solutions.
MethodModelClassic Grid Puzzles Sudoku Hitori Fillomino BW-GR BW-LG BW-PV Dynamic Puzzles with Actions BW-PJStandardDeepseek-V30%0%0%59.0%88.5%79.0%88.5%PromptGPT-4o-mini0%0%0%47.5%50.5%71.0%85.0%CoTDeepseek-V3N/AN/AN/A62.5%97.0%97.0%100.0%Finetune*RoBERTa GPT-2N/A N/AN/A N/AN/A N/A96.8% 97.4%99.7% 99.4%87.6% 90.1%87.4% 85.1%Deepseek-V399.0%100.0%91.0%92.0%92.5%98.0%98.0%LogotGPT-4o-mini 100.0% 100.0%100.0%95.5%99.5%98.5%95.0%GPT-4o100.0% 100.0%100.0%97.5%100.0% 100.0% 100.0%4 Experiment4.1 Task DescriptionDynamic Puzzles. We use four tasks from theBlocks World domain introduced in He et al.(2023): Projection, Legality, Plan Verification, andGoal Recognition. The Blocks World consists ofan unbounded table surface and a set of blocks thatcan be stacked to form towers. Each block caneither be on another block or directly on the table.Movement of a block is constrained by precondi-tions, such as whether it is clear. As an example,the Legality task asks whether a given sequence ofactions can be executed from an initial state. In theoriginal dataset of (He et al., 2023), each task con-tains 12,000 training and 1,000 test instances. Inour experiment, we randomly sample 200 instancesper task for evaluation.Due to space constraints, detailed descriptionsand examples for all puzzles are left in the Ap-pendix.</p>
<ol>
<li>Identify which block is being moved.2. Find where that block currently is. 3. Check whether the block is clear (nothing on top of it). 4. Check whether the destination block is clear.5.If the block is clear, is currently in the stated location, and the destination is clear, then the action is legal.Otherwise, it is not.The red block is on the table.The tan block is on the table.The green block is on the red block.The green block is clear.The tan block is clear.
â†’6. Conclude with "True" if legal, or "False" if not.### Example 1:state:â†’query:Jane moves the green block from the red block to the tan block.Reasoning:The green block is the one being moved.It is currently on the red block.It is clear, meaning there is nothing on top of it.The tan block is clear as well.All conditions are satisfied to move the green block from the red block to the tan block.Final output: TrueMethodModelClassic Grid Puzzles Sudoku Hitori Fillomino BW-GR BW-LG BW-PV BW-PJ Dynamic Puzzles with ActionsStandardDeepseek-V3 0.0048 0.00890.00870.01190.01010.01200.0105PromptGPT-4o-mini0.0073 0.01130.01060.01290.01090.01300.0114CoTDeepseek-V3N/AN/AN/A0.08950.03030.09570.0629Deepseek-V3 0.0920 0.05140.03930.04740.04180.04780.0506LogotGPT-4o-mini0.1619 0.08000.05070.06070.05420.06200.0654GPT-4o4.9572 2.45681.55651.86591.66181.90282.0023</li>
</ol>
<h3>......(other examples) Figure 5: A chain-of-thought (CoT) prompt example for solving the Blocks World -Legality puzzle.</h3>
<p>Table 2 :
2
Cost of solving each puzzle type across different methods and models (in USD).</p>
<p>Rule Specification: In the Blocks World domain... Init State: The silver block is on top of the teal block.The green block is on top of the indigo block.The silver block is clear.The indigo block is on the table.The teal block is on the table.The green block is clear.The aquamarine block is clear.The aquamarine block is on the table.Action Sequence: Jane moves the aquamarine block from the teal block to the silver block.
Translated ASP Program:......% State Encoding:holds(on(silver, teal), 0).holds(on(green, indigo), 0).holds(clear(silver), 0).holds(on(indigo, table), 0).holds(on(teal, table), 0).holds(clear(green), 0).holds(clear(aquamarine), 0).holds(on(aquamarine, table), 0).% All Blocks:block(silver).block(green).block(indigo).block(teal). block(aquamarine).% Query Encoding:occurs(move(aquamarine, table, silver), 0).% Steps:#const num_step=1.</p>
<p>â€¢</p>
<p>State: The tan block is on the table.The turquoise block is clear.The teal block is on the table.The brown block is clear.The green block is clear.The brown block is on the table.The green block is on the table.The tan block is clear.The turquoise block is on top of the teal block.</p>
<p>â€¢</p>
<p>State: The tan block is clear.The olive block is on top of the green block.The brown block is on the table.The magenta block is on top of the brown block.The magenta block is clear.The green block is on the table.The tan block is on top of the olive block.
Input:000850000020000000010900700070025093402000000000000500097500000563000004000000680Thought:
â€¢ Observations: Jane moves the magenta block from the brown block to the tan block.Jane moves the magenta block from the tan block onto the table.Jane moves the tan block from the olive block onto the table.â€¢ Goal: The brown block is not on the table and the olive block is on top of the magenta block.â€¢ Label: False.B Prompt ExamplesIn Figure12-5, we present various prompt examples for Sudoku and Blocks Would -Legality, which are representative puzzles in classic grid puzzles and dynamic puzzles with action.â€¢ Figure12shows the few-shot learning prompts for translating rule specifications in Sudoku.â€¢ Figure13shows the few-shot learning prompts for translating puzzle state in Sudoku.â€¢ Figure14shows the standard prompt for solving Sudoku.â€¢ Figure15shows the few-shot learning prompts for translating rule specifications in Blocks World -Legality.â€¢ Figure16shows the few-shot learning prompts for translating puzzle state in Blocks World -Legality.â€¢ Figure17shows the standard prompt for solving Blocks World -Legality.). reachable(X1, Y1, X1, Y1) :--black(X1, Y1).reachable(X1, Y1, X3, Y3) :-reachable(X1, Y1, X2, Y2), adj(X2, Y2, X3, Y3), -black(X1, Y1), -black(X2, Y2), -black(X3, Y3).reachable(X2, Y2, X1, Y1) :-reachable(X1, Y1, X2, Y2).:--black(X1, Y1), -black(X2, Y2), not reachable(X1, Y1, X2, Y2).......(similar few-shot example for the Fillomino puzzle) % Introduction: Sudoku is a popular logic-based number placement puzzle.% The puzzle is played on a 9Ã—9 grid divided into nine 3Ã—3 subgrids (also called regions).% The goal is to fill in the grid so that each number appears exactly once in each % row, column, and 3Ã—3 subgrid.% The logic program includes the following atoms: % coord(X, Y) : The coordination of the grid.% pos(X, Y, N) : The number in the cell (X, Y) is N. % num(N) : Possible numbers in the cells, N=1..9 % adj(X1, Y1, X2, Y2) : Cell (X1, Y1) and cell (X2, Y2) are adjacent to each other.% reachable(X1, Y1, X2, Y2) : Cell (X1, Y1) and cell (X2, Y2) are reachable to each other.% Description of a 9x9 grid.ASP_RULES % Fill each row with the numbers 1 through 9 without repeating any number.ASP_RULES % Fill each column with the numbers 1 through 9 without repeating any number.ASP_RULES % Fill each 3x3 subgrid with the numbers 1 through 9 without repeating any number.ASP_RULES Complete the ASP_RULES.
Reasoning about state constraints in the situation calculus. Naiqi Li, Yi Fan, Yongmei Liu, Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial Intelligence2013</p>
<p>Automatic verification of partial correctness of golog programs. Naiqi Li, Yongmei Liu, Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial Intelligence2015</p>
<p>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study. Yinghao Li, Haorui Wang, Chao Zhang, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Answer set programming. Vladimir Lifschitz, 2019Springer3Cham</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the. the 55th Annual Meeting of theAssociation for Computational Linguistics20171</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, arXiv:2412.19437Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint</p>
<p>Mathematical definition and systematization of puzzle rules. Itsuki Maeda, Yasuhiro Inoue, arXiv:2501.014332024arXiv preprint</p>
<p>GPTEval: A survey on assessments of ChatGPT and GPT-4. Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation2024</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.06196Large language models: A survey. 2024arXiv preprint</p>
<p>Learning to automatically solve logic grid puzzles. Arindam Mitra, Chitta Baral, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Puzzlebench: Can LLMs solve challenging first-order combinatorial reasoning problems?. Chinmay Mittal, Krishna Kartik, arXiv:2402.02611Parag Singla, and 1 others. 2024arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 201919</p>
<p>Knowledge in action: logical foundations for specifying and implementing dynamical systems. Raymond Reiter, 2001MIT press</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, and 1 others. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, International Conference on Learning Representations. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235and 1 others</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202336</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
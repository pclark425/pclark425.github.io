<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7860 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7860</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7860</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-277066688</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.18771v3.pdf" target="_blank">CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</a></p>
                <p><strong>Paper Abstract:</strong> Existing LLM-as-a-Judge approaches for evaluating text generation suffer from rating inconsistencies, with low agreement and high rating variance across different evaluator models. We attribute this to subjective evaluation criteria combined with Likert scale scoring in existing protocols. To address this issue, we introduce CheckEval, a checklist-based evaluation framework that improves rating reliability via decomposed binary questions. Through experiments with 12 evaluator models across multiple datasets, we first demonstrate that CheckEval strongly correlates with human judgments. More importantly, CheckEval dramatically improves the average agreement across evaluator models by 0.45 and reduces the score variance. CheckEval scores furthermore have the benefit of being more interpretable because it decomposes evaluation criteria into traceable binary decisions, allowing analyses of specific attributes driving quality judgments.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7860.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7860.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CheckEval_vs_Human_overall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CheckEval (LLM-as-a-Judge checklist) compared to human evaluation (overall findings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate comparisons reported in the paper between the CheckEval checklist-based LLM evaluator and human judgments, covering correlation, inter-evaluator agreement, stability, and failure modes across summarization and dialogue datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization and knowledge-grounded dialogue evaluation (multi-dimensional: coherence, consistency, fluency, relevance / naturalness, coherence, engagingness, groundedness)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval; Topical-Chat (primary reported datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Multiple LLM evaluators (12 models including Mistral-Large, Qwen2.5-72B, Llama3.1-70B, GPT-4-Turbo/GPT-4o variants)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Mixture of open-source models of varying sizes (7–123B) and closed-source GPT models (GPT-4-Turbo, GPT-4o, GPT-4o-mini); temperature=0, n=1 for evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Original SummEval and Topical-Chat human ratings (benchmark authors) plus three PhD-level human annotators used for checklist validation and agreement/correlation studies</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho / Pearson's r / Kendall's tau for correlation with human judgments; Krippendorff's alpha and Fleiss' kappa for inter-evaluator agreement (IEA)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>sensitivity to evaluator-model choice; difficulty discriminating subtle differences among high-quality texts under Likert protocols; Likert-scale calibration inconsistencies across models; occasional misalignment with human annotation practices (e.g., humans emphasize readability over strict grammatical correctness); binary checklist may be insufficient for long-form text with mixed-quality parts</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CheckEval maintains competitive or higher correlation with human judgments compared to Likert-based LLM evaluators while substantially improving inter-evaluator agreement (IEA) and reducing variance across evaluator models; CheckEval's checklist answers are more interpretable/traceable; failure cases include reduced granularity for long-form/mixed-quality outputs and sensitivity when human annotation practices deviate from checklist definitions</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>scalable and low-cost relative to human annotation; faster and reproducible; CheckEval in particular adds interpretability (traceable binary criteria) and improved consistency across different LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Checklist-based pointwise evaluation: boolean (yes/no) questions grouped by sub-dimensions; final score = proportion of 'yes' answers; multi-question prompts evaluated together; temperature=0, model outputs constrained to 'yes'/'no'; comparisons to G-Eval and SEEval using their original Likert prompts</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7860.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7860.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CheckEval_Mistral-Large_numeric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CheckEval evaluated using Mistral-Large compared to human judgments (reported numeric correlations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete numeric correlation results for CheckEval when using Mistral-Large as the evaluator on SummEval and Topical-Chat reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization (SummEval) and knowledge-grounded dialogue (Topical-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval; Topical-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mistral-Large</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Mistral-Large (reported as one of the large open-source evaluators in the experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Benchmark human ratings (SummEval and Topical-Chat) used as ground truth; additional PhD-level annotators used for checklist validation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (SummEval); Pearson's r (Topical-Chat) (also report Kendall's tau where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5486</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>minor sensitivity to additional human filtering of checklist questions (very small changes only); potential mismatch with human annotation practices for some dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Mistral-Large + CheckEval achieved one of the highest correlations among models (ρ = 0.5486 on SummEval) and strong performance on Topical-Chat (r ≈ 0.6451); CheckEval preserved or improved correlation while increasing evaluator-model agreement</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High-performing open-source model achieves strong alignment with human scores while benefiting from CheckEval's improved consistency</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>CheckEval prompts (binary checklist), multi-question grouping per prompt, max_length=200, temperature=0; correlations computed at sample-level for SummEval and turn-level for Topical-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7860.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7860.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IEA_CheckEval_vs_G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inter-evaluator agreement (IEA) comparisons: CheckEval vs G-Eval/SEEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported inter-evaluator agreement metrics showing that CheckEval yields much higher agreement across different LLM evaluators than Likert-based methods (G-Eval and SEEval).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization and dialogue evaluation (multi-dimension)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval; Topical-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Multiple LLM groups (Large / Medium / Small / GPT / Top-3)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Groupings by model size used to compute IEA; 12 LLM evaluators in total including open and closed-source models</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human IEA reported from Fabbri et al. (SummEval) for comparison (human κ ≈ 0.7 cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha and Fleiss' kappa (used to measure IEA across evaluator models)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.67</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>G-Eval and SEEval show low IEA across models (IEA often << CheckEval); Likert-scale scoring is sensitive to model choice and calibration, causing inconsistent scores</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CheckEval achieves IEA ≈ 0.67 (α and κ) on SummEval for large models — comparable to human rater agreement (κ ≈ 0.7); across model groups CheckEval's IEA (e.g., All ~0.48 reported) is substantially higher than G-Eval/SEEval</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Using CheckEval reduces inter-evaluator variability, enabling more reliable comparisons across systems without requiring human raters for every experiment</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>IEA computed by treating different LLMs within the same model-capacity group as annotators; comparisons include grouped averages (All, Large, Medium, Small, GPT, Top-3)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7860.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7860.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binarize_G-Eval_experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binarization of Likert outputs (G-Eval) and effect on IEA compared to CheckEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiment converting G-Eval's Likert outputs to binary (0/1) to test whether output format alone explains IEA differences; binarization improves G-Eval IEA but a large gap to CheckEval remains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization (SummEval) IEA analysis</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>G-Eval (various LLM backends) after binarization</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>G-Eval Likert outputs (1-5) converted to binary under different mappings; reported for model-size groups</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>N/A (analysis among LLM evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha (primary reported metric in these tables)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.3204</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Output-format binarization improves IEA for G-Eval but does not match CheckEval; implies that protocol differences (question design/criteria ambiguity) — not only output format — drive IEA gap</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Binarizing G-Eval's Likert scores increased its IEA relative to original Likert outputs (example: Large-group alpha from 0.0512 to 0.3204 under one binarization scheme), but CheckEval's IEA remained substantially higher (e.g., CheckEval alpha ≈ 0.6731 for Large), indicating deeper protocol-driven differences</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Shows that simple output-format fixes (binarization) can yield some gains but structured checklist design yields larger, more robust agreement improvements</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>G-Eval outputs converted to binary under multiple mapping schemes (e.g., mapping top Likert values to 1); IEA recomputed and compared to CheckEval IEA</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7860.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7860.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human_Validation_checklist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human validation of CheckEval protocol: correlation and agreement analyses between human checklist scores and LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human studies where three PhD-level annotators applied the CheckEval checklist to samples; correlations between human-generated checklist scores and LLM scores were computed and inter-annotator agreement (and human-LLM agreement) analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Summarization (SummEval) — correlation study and relevance-only agreement study</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval (20-sample correlation study; 100-sample relevance agreement study)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Large LLM group (Mistral-Large, Qwen2.5-72B, Llama3.1-70B aggregated in reported analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Large-model group used for comparison to human checklist annotations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three PhD-level annotators (experts) recruited by the authors for checklist validation (compensated, trained); original SummEval crowd/human ratings also used in benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Correlation coefficients (Pearson r, Spearman ρ, Kendall τ) for human vs LLM; Exact Match and IEA metrics for checklist validation (Exact Match reported for checklist validation IAA)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Checklist alignment can be affected if human annotation practice deviates from predefined checklist criteria; annotators asked for additional filtering for 1-2 redundant questions per dimension</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>All reported human-LLM correlations were statistically significant; high agreement observed among humans and between humans and LLMs for relevance in the agreement study; human validation suggests the checklist generation (augmentation + filtering) is high-quality (augmentation >90% pass rate)</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>CheckEval's checklist format can be directly used by humans and LLMs, enabling direct cross-comparisons and interpretable aggregation of binary judgments</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Correlation study: 20 stratified summaries, 3 annotators answered ~25 binary questions per dimension (≈2,000 binary annotations per annotator); Agreement study: 100 summaries annotated for relevance only (~10K questions per annotator); aggregation by summing 'yes' responses per dimension</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7860.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7860.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CaseStudy_TopicalChat_naturalness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topical-Chat naturalness case study: normalized scores human vs G-Eval vs CheckEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Example case showing concrete normalized scores on a single Topical-Chat sample comparing human, G-Eval, and CheckEval evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Knowledge-grounded dialogue — naturalness dimension (Topical-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Topical-Chat (single example case study)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Mistral-Large (system response was from Mistral-Large in the case study)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Case study evaluated Mistral-Large system response; scores normalized to 0-1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human benchmark score for that sample (normalized)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Normalized score comparison (0-1 scale)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.88</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>G-Eval assigned a much lower naturalness score in this instance (0.25) relative to human (1.0) and CheckEval (0.88), illustrating inconsistent Likert-style scoring by LLM evaluators on some samples</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CheckEval score (0.88) aligned much more closely with human normalized score (1.0) than G-Eval (0.25) for the presented sample; CheckEval additionally provided checklist-level 'yes'/'no' answers enabling traceability of why it rated naturalness highly</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>CheckEval provides explainability (which checklist items passed/failed) in addition to numeric alignment with humans</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Single-case comparison on Topical-Chat naturalness; scores normalized to 0-1 for direct comparison (human, G-Eval single Likert->normalized, CheckEval proportion of 'yes' answers)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpteval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>SEEval: Advancing LLM text evaluation efficiency and accuracy through self-explanation prompting <em>(Rating: 2)</em></li>
                <li>Summeval: Re-evaluating summarization evaluation <em>(Rating: 2)</em></li>
                <li>Large language models are inconsistent and biased evaluators <em>(Rating: 2)</em></li>
                <li>A closer look into using large language models for automatic evaluation <em>(Rating: 1)</em></li>
                <li>HD-EVAL: Aligning large language model evaluators through hierarchical criteria decomposition <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7860",
    "paper_id": "paper-277066688",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "CheckEval_vs_Human_overall",
            "name_full": "CheckEval (LLM-as-a-Judge checklist) compared to human evaluation (overall findings)",
            "brief_description": "Aggregate comparisons reported in the paper between the CheckEval checklist-based LLM evaluator and human judgments, covering correlation, inter-evaluator agreement, stability, and failure modes across summarization and dialogue datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
            "evaluation_task": "Summarization and knowledge-grounded dialogue evaluation (multi-dimensional: coherence, consistency, fluency, relevance / naturalness, coherence, engagingness, groundedness)",
            "dataset_name": "SummEval; Topical-Chat (primary reported datasets)",
            "judge_model_name": "Multiple LLM evaluators (12 models including Mistral-Large, Qwen2.5-72B, Llama3.1-70B, GPT-4-Turbo/GPT-4o variants)",
            "judge_model_details": "Mixture of open-source models of varying sizes (7–123B) and closed-source GPT models (GPT-4-Turbo, GPT-4o, GPT-4o-mini); temperature=0, n=1 for evaluations",
            "human_evaluator_type": "Original SummEval and Topical-Chat human ratings (benchmark authors) plus three PhD-level human annotators used for checklist validation and agreement/correlation studies",
            "agreement_metric": "Spearman's rho / Pearson's r / Kendall's tau for correlation with human judgments; Krippendorff's alpha and Fleiss' kappa for inter-evaluator agreement (IEA)",
            "agreement_score": null,
            "reported_loss_aspects": "sensitivity to evaluator-model choice; difficulty discriminating subtle differences among high-quality texts under Likert protocols; Likert-scale calibration inconsistencies across models; occasional misalignment with human annotation practices (e.g., humans emphasize readability over strict grammatical correctness); binary checklist may be insufficient for long-form text with mixed-quality parts",
            "qualitative_findings": "CheckEval maintains competitive or higher correlation with human judgments compared to Likert-based LLM evaluators while substantially improving inter-evaluator agreement (IEA) and reducing variance across evaluator models; CheckEval's checklist answers are more interpretable/traceable; failure cases include reduced granularity for long-form/mixed-quality outputs and sensitivity when human annotation practices deviate from checklist definitions",
            "advantages_of_llm_judge": "scalable and low-cost relative to human annotation; faster and reproducible; CheckEval in particular adds interpretability (traceable binary criteria) and improved consistency across different LLM evaluators",
            "experimental_setting": "Checklist-based pointwise evaluation: boolean (yes/no) questions grouped by sub-dimensions; final score = proportion of 'yes' answers; multi-question prompts evaluated together; temperature=0, model outputs constrained to 'yes'/'no'; comparisons to G-Eval and SEEval using their original Likert prompts",
            "uuid": "e7860.0",
            "source_info": {
                "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CheckEval_Mistral-Large_numeric",
            "name_full": "CheckEval evaluated using Mistral-Large compared to human judgments (reported numeric correlations)",
            "brief_description": "Concrete numeric correlation results for CheckEval when using Mistral-Large as the evaluator on SummEval and Topical-Chat reported in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
            "evaluation_task": "Summarization (SummEval) and knowledge-grounded dialogue (Topical-Chat)",
            "dataset_name": "SummEval; Topical-Chat",
            "judge_model_name": "Mistral-Large",
            "judge_model_details": "Mistral-Large (reported as one of the large open-source evaluators in the experiments)",
            "human_evaluator_type": "Benchmark human ratings (SummEval and Topical-Chat) used as ground truth; additional PhD-level annotators used for checklist validation",
            "agreement_metric": "Spearman's rho (SummEval); Pearson's r (Topical-Chat) (also report Kendall's tau where applicable)",
            "agreement_score": 0.5486,
            "reported_loss_aspects": "minor sensitivity to additional human filtering of checklist questions (very small changes only); potential mismatch with human annotation practices for some dimensions",
            "qualitative_findings": "Mistral-Large + CheckEval achieved one of the highest correlations among models (ρ = 0.5486 on SummEval) and strong performance on Topical-Chat (r ≈ 0.6451); CheckEval preserved or improved correlation while increasing evaluator-model agreement",
            "advantages_of_llm_judge": "High-performing open-source model achieves strong alignment with human scores while benefiting from CheckEval's improved consistency",
            "experimental_setting": "CheckEval prompts (binary checklist), multi-question grouping per prompt, max_length=200, temperature=0; correlations computed at sample-level for SummEval and turn-level for Topical-Chat",
            "uuid": "e7860.1",
            "source_info": {
                "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "IEA_CheckEval_vs_G-Eval",
            "name_full": "Inter-evaluator agreement (IEA) comparisons: CheckEval vs G-Eval/SEEval",
            "brief_description": "Reported inter-evaluator agreement metrics showing that CheckEval yields much higher agreement across different LLM evaluators than Likert-based methods (G-Eval and SEEval).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
            "evaluation_task": "Summarization and dialogue evaluation (multi-dimension)",
            "dataset_name": "SummEval; Topical-Chat",
            "judge_model_name": "Multiple LLM groups (Large / Medium / Small / GPT / Top-3)",
            "judge_model_details": "Groupings by model size used to compute IEA; 12 LLM evaluators in total including open and closed-source models",
            "human_evaluator_type": "Human IEA reported from Fabbri et al. (SummEval) for comparison (human κ ≈ 0.7 cited)",
            "agreement_metric": "Krippendorff's alpha and Fleiss' kappa (used to measure IEA across evaluator models)",
            "agreement_score": 0.67,
            "reported_loss_aspects": "G-Eval and SEEval show low IEA across models (IEA often &lt;&lt; CheckEval); Likert-scale scoring is sensitive to model choice and calibration, causing inconsistent scores",
            "qualitative_findings": "CheckEval achieves IEA ≈ 0.67 (α and κ) on SummEval for large models — comparable to human rater agreement (κ ≈ 0.7); across model groups CheckEval's IEA (e.g., All ~0.48 reported) is substantially higher than G-Eval/SEEval",
            "advantages_of_llm_judge": "Using CheckEval reduces inter-evaluator variability, enabling more reliable comparisons across systems without requiring human raters for every experiment",
            "experimental_setting": "IEA computed by treating different LLMs within the same model-capacity group as annotators; comparisons include grouped averages (All, Large, Medium, Small, GPT, Top-3)",
            "uuid": "e7860.2",
            "source_info": {
                "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Binarize_G-Eval_experiment",
            "name_full": "Binarization of Likert outputs (G-Eval) and effect on IEA compared to CheckEval",
            "brief_description": "Experiment converting G-Eval's Likert outputs to binary (0/1) to test whether output format alone explains IEA differences; binarization improves G-Eval IEA but a large gap to CheckEval remains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
            "evaluation_task": "Summarization (SummEval) IEA analysis",
            "dataset_name": "SummEval",
            "judge_model_name": "G-Eval (various LLM backends) after binarization",
            "judge_model_details": "G-Eval Likert outputs (1-5) converted to binary under different mappings; reported for model-size groups",
            "human_evaluator_type": "N/A (analysis among LLM evaluators)",
            "agreement_metric": "Krippendorff's alpha (primary reported metric in these tables)",
            "agreement_score": 0.3204,
            "reported_loss_aspects": "Output-format binarization improves IEA for G-Eval but does not match CheckEval; implies that protocol differences (question design/criteria ambiguity) — not only output format — drive IEA gap",
            "qualitative_findings": "Binarizing G-Eval's Likert scores increased its IEA relative to original Likert outputs (example: Large-group alpha from 0.0512 to 0.3204 under one binarization scheme), but CheckEval's IEA remained substantially higher (e.g., CheckEval alpha ≈ 0.6731 for Large), indicating deeper protocol-driven differences",
            "advantages_of_llm_judge": "Shows that simple output-format fixes (binarization) can yield some gains but structured checklist design yields larger, more robust agreement improvements",
            "experimental_setting": "G-Eval outputs converted to binary under multiple mapping schemes (e.g., mapping top Likert values to 1); IEA recomputed and compared to CheckEval IEA",
            "uuid": "e7860.3",
            "source_info": {
                "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Human_Validation_checklist",
            "name_full": "Human validation of CheckEval protocol: correlation and agreement analyses between human checklist scores and LLMs",
            "brief_description": "Human studies where three PhD-level annotators applied the CheckEval checklist to samples; correlations between human-generated checklist scores and LLM scores were computed and inter-annotator agreement (and human-LLM agreement) analyzed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
            "evaluation_task": "Summarization (SummEval) — correlation study and relevance-only agreement study",
            "dataset_name": "SummEval (20-sample correlation study; 100-sample relevance agreement study)",
            "judge_model_name": "Large LLM group (Mistral-Large, Qwen2.5-72B, Llama3.1-70B aggregated in reported analyses)",
            "judge_model_details": "Large-model group used for comparison to human checklist annotations",
            "human_evaluator_type": "Three PhD-level annotators (experts) recruited by the authors for checklist validation (compensated, trained); original SummEval crowd/human ratings also used in benchmark",
            "agreement_metric": "Correlation coefficients (Pearson r, Spearman ρ, Kendall τ) for human vs LLM; Exact Match and IEA metrics for checklist validation (Exact Match reported for checklist validation IAA)",
            "agreement_score": null,
            "reported_loss_aspects": "Checklist alignment can be affected if human annotation practice deviates from predefined checklist criteria; annotators asked for additional filtering for 1-2 redundant questions per dimension",
            "qualitative_findings": "All reported human-LLM correlations were statistically significant; high agreement observed among humans and between humans and LLMs for relevance in the agreement study; human validation suggests the checklist generation (augmentation + filtering) is high-quality (augmentation &gt;90% pass rate)",
            "advantages_of_llm_judge": "CheckEval's checklist format can be directly used by humans and LLMs, enabling direct cross-comparisons and interpretable aggregation of binary judgments",
            "experimental_setting": "Correlation study: 20 stratified summaries, 3 annotators answered ~25 binary questions per dimension (≈2,000 binary annotations per annotator); Agreement study: 100 summaries annotated for relevance only (~10K questions per annotator); aggregation by summing 'yes' responses per dimension",
            "uuid": "e7860.4",
            "source_info": {
                "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CaseStudy_TopicalChat_naturalness",
            "name_full": "Topical-Chat naturalness case study: normalized scores human vs G-Eval vs CheckEval",
            "brief_description": "Example case showing concrete normalized scores on a single Topical-Chat sample comparing human, G-Eval, and CheckEval evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
            "evaluation_task": "Knowledge-grounded dialogue — naturalness dimension (Topical-Chat)",
            "dataset_name": "Topical-Chat (single example case study)",
            "judge_model_name": "Mistral-Large (system response was from Mistral-Large in the case study)",
            "judge_model_details": "Case study evaluated Mistral-Large system response; scores normalized to 0-1",
            "human_evaluator_type": "Human benchmark score for that sample (normalized)",
            "agreement_metric": "Normalized score comparison (0-1 scale)",
            "agreement_score": 0.88,
            "reported_loss_aspects": "G-Eval assigned a much lower naturalness score in this instance (0.25) relative to human (1.0) and CheckEval (0.88), illustrating inconsistent Likert-style scoring by LLM evaluators on some samples",
            "qualitative_findings": "CheckEval score (0.88) aligned much more closely with human normalized score (1.0) than G-Eval (0.25) for the presented sample; CheckEval additionally provided checklist-level 'yes'/'no' answers enabling traceability of why it rated naturalness highly",
            "advantages_of_llm_judge": "CheckEval provides explainability (which checklist items passed/failed) in addition to numeric alignment with humans",
            "experimental_setting": "Single-case comparison on Topical-Chat naturalness; scores normalized to 0-1 for direct comparison (human, G-Eval single Likert-&gt;normalized, CheckEval proportion of 'yes' answers)",
            "uuid": "e7860.5",
            "source_info": {
                "paper_title": "CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpteval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "gpteval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "SEEval: Advancing LLM text evaluation efficiency and accuracy through self-explanation prompting",
            "rating": 2,
            "sanitized_title": "seeval_advancing_llm_text_evaluation_efficiency_and_accuracy_through_selfexplanation_prompting"
        },
        {
            "paper_title": "Summeval: Re-evaluating summarization evaluation",
            "rating": 2,
            "sanitized_title": "summeval_reevaluating_summarization_evaluation"
        },
        {
            "paper_title": "Large language models are inconsistent and biased evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_inconsistent_and_biased_evaluators"
        },
        {
            "paper_title": "A closer look into using large language models for automatic evaluation",
            "rating": 1,
            "sanitized_title": "a_closer_look_into_using_large_language_models_for_automatic_evaluation"
        },
        {
            "paper_title": "HD-EVAL: Aligning large language model evaluators through hierarchical criteria decomposition",
            "rating": 1,
            "sanitized_title": "hdeval_aligning_large_language_model_evaluators_through_hierarchical_criteria_decomposition"
        }
    ],
    "cost": 0.017148749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists</p>
<p>Yukyung Lee 
Boston University</p>
<p>Joonghoon Kim 
Telecom</p>
<p>Jaehee Kim 
Seoul National University
4 KAIST 5 NAVER</p>
<p>Hyowon Cho 
Jaewook Kang 
Pilsung Kang pilsung_kang@snu.ac.kr 
Seoul National University
4 KAIST 5 NAVER</p>
<p>Najoung Kim najoung@bu.edu 
Boston University</p>
<p>CheckEval: A reliable LLM-as-a-Judge framework for evaluating text generation using checklists
74CE7DD9BE0948BAD26619896968E607
Existing LLM-as-a-Judge approaches for evaluating text generation suffer from rating inconsistencies, with low agreement and high rating variance across different evaluator models.We attribute this to subjective evaluation criteria combined with Likert scale scoring in existing protocols.To address this issue, we introduce CheckEval, a checklist-based evaluation framework that improves rating reliability via decomposed binary questions.Through experiments with 12 evaluator models across multiple datasets, we first demonstrate that CheckEval strongly correlates with human judgments.More importantly, CheckEval dramatically improves the average agreement across evaluator models by 0.45 and reduces the score variance.CheckEval scores furthermore have the benefit of being more interpretable because it decomposes evaluation criteria into traceable binary decisions, allowing analyses of specific attributes driving quality judgments.</p>
<p>Introduction</p>
<p>Evaluating text generation quality remains a major challenge in Natural Language Generation (NLG), particularly as Large Language Models (LLMs) continue to advance in their generative capabilities (Brown et al., 2020;Chowdhery et al., 2023;Achiam et al., 2023).This is especially evident in tasks such as summarization, dialogue, and creative writing (Liu et al., 2023d;Kim et al., 2023;Liu et al., 2023a), where qualitative dimensions of the output are crucial yet difficult to measure systematically.Consequently, there is growing interest in developing evaluation methods that can effectively capture these aspects.These methods will ideally involve well-defined protocols that ensure reliability across different raters and tasks.In obtaining actual scores from such protocols, human evaluation remains the gold standard, but it # , † Equal contribution.Our code is available at https: //github.com/yukyunglee/CheckEval. is costly, time-consuming, and difficult to scale (Novikova et al., 2017;Belz et al., 2020).While lexical overlap-based metrics such as ROUGE and BLEU (Lin, 2004;Papineni et al., 2002) have been widely adopted for ease of automation, they align poorly with human judgments, calling for alternatives that better approximate human evaluation.</p>
<p>Recent work has explored the use of LLM-as-a-Judge as a scalable alternative, leveraging LLMs to assess generated text directly (Zheng et al., 2023).This paradigm has evolved through various approaches: single-turn prompting (Liu et al., 2023b;Fu et al., 2023), meta-evaluator training (Kim et al., 2023;Wu et al., 2024b), and even more sophisticated methods like multi-agent debate (Chan et al., 2024;Kim et al., 2024).However, these methods often rely on subjective evaluation protocols that require evaluators to assign holistic scores without clear decision criteria.For example, evaluators are typically asked to rate text on a Likert scale from 1 to 5 (higher is better) on dimensions such as coherence, consistency, fluency, and relevance.While Likert scales are useful for capturing ordinal relationships in human evaluation, they face two key challenges when applied to LLM-based evaluator models.First, current LLMs are known to struggle with subjective criteria in Likert-scale evaluations, in particular showing difficulty in differentiating between high-quality texts (Li et al., 2019;Stureborg et al., 2024).Second, evaluation results are highly sensitive to the choice of evaluator models.These lead to low inter-evaluator agreement (IEA),1 which we define as the agreement among evaluator models (of similar capacity), as well as high variance in evaluation results (Stureborg et al., 2024).Yet, previous LLM-as-a-Judge approaches have overlooked these issues (Gao et al., 2024a).</p>
<p>To address these challenges, we introduce CheckEval, a reliable evaluation framework that decomposes evaluation criteria to target finegrained qualitative dimensions and turns them into a checklist. 2Inspired by recent advances in finegrained decomposition of evaluation (Liu et al., 2023c;Min et al., 2023), our framework breaks down evaluation into discrete Boolean questions.This decomposition simplifies each individual evaluation question and clarifies the rationale behind evaluation decisions.CheckEval addresses key limitations of existing methods in two ways.First, it improves explainability by tracking how specific criteria are met, making evaluation decisions more explicit and reducing ambiguity.Second, it enhances consistency through structured binary responses, which improve IEA and reduce variability.Importantly, CheckEval maintains competitive correlation with human evaluation while achieving these improvements.These improvements are verified through comprehensive experiments across 12 different LLM-based evaluator models of varying sizes, including both open and closed-source models, on multiple datasets.The main contributions of this study can be summarized as follows:</p>
<p>• We introduce CheckEval, a fine-grained evaluation framework leveraging a Boolean QA checklist to address the rating consistency issues with existing LLM-as-a-Judge methods for NLG evaluation.</p>
<p>• Experiments across 12 LLMs and multiple datasets demonstrate significant improvements in correlation with human evaluation compared to Likert-based approaches like G-Eval (Liu et al., 2023b) and SEEval (Wu et al., 2025).</p>
<p>• CheckEval shows reduced sensitivity to the choice of evaluator models, leading to more consistent evaluation results with lower variance and higher IEA.</p>
<p>2 Related Work</p>
<p>LLM-as-a-Judge</p>
<p>Traditional NLG evaluation metrics like ROUGE and BLEU show clear limitations due to their reliance on reference texts (Gu et al., 2021).</p>
<p>With advances in LLMs, researchers have explored LLM-as-a-Judge, where an LLM evaluates texts based on specified criteria, formalized as F (subject, criteria) → result (Li et al., 2024).LLM-as-a-Judge can be categorized into pairwise and pointwise evaluation approaches (Gu et al., 2024).Pairwise evaluation (Zheng et al., 2023;Qin et al., 2024) compares two outputs to determine relative preference but is computationally expensive as comparisons scale exponentially.In contrast, pointwise evaluation (Liu et al., 2023b;Fu et al., 2023) assigns scores to individual outputs, allowing for absolute scaling.However, existing pointwise evaluation protocols often lack granularity, assigning a single numeric score to each dimension of evaluation.If the specified dimensions of evaluation are too broad (e.g., fluency), this may lead to inconsistencies in judgments because many factors could influence the quality along the target dimension.CheckEval falls in to the category of pointwise evaluation but addresses its limitations by adopting a finer-grained Boolean QA Checklist.3</p>
<p>Decompositional Approaches</p>
<p>Decomposing complex information into minimal units to simplify tasks have been explored in various areas of NLP (Kamoi et al., 2023;Chen et al., 2022;Wright et al., 2022;Krishna et al., 2023;Nenkova and Passonneau, 2004;Liu et al., 2024).</p>
<p>Recent studies have shown that breaking down content into atomic units reduces subjectivity in factual consistency judgment (Liu et al., 2023c;Min et al., 2023).Atomic units represent elementary information that cannot be further divided.Similarly, CheckEval decomposes evaluation criteria into finegrained Boolean QA Checklists to enhance clarity and reduce ambiguity in the evaluation process.</p>
<p>Reliability of Evaluation</p>
<p>Reliability is an important yet often overlooked component of evaluation.Many LLM-as-a-Judge methods focus only on correlation with human scores, often neglecting consistency and stability across different LLMs.Recent studies have highlighted several reliability concerns.Xiao et al. (2023) demonstrate that LLMs fail to reliably assess subtle quality differences in text.Similarly, Bavaresco et al. (2024) find these models often assign highly variable ratings to identical inputs.Furthermore, IEA remains low across models, compromising evaluation reliability (Stureborg et al., 2024).Our work addresses these issues by evaluating not only correlation but also IEA and score variance across evaluator models, showing that Check-Eval improves reliability across diverse LLMs.</p>
<p>Method</p>
<p>CheckEval consists of three stages, (1) Defining Dimensions of Evaluation, (2) Checklist Generation, and (3) Checklist-Based Evaluation, as shown in Figure 1.The framework translates high-level evaluation criteria into a Boolean QA checklist, each question in the checklist expecting a binary (yes/no) response.This format improves clarity and alleviates ambiguity compared to Likert-scale scoring (discussed further in Section 7.2).</p>
<p>Defining Dimensions of Evaluation</p>
<p>The first stage defines the dimensions of text quality (e.g., consistency, fluency) to be evaluated by either adopting predefined dimensions from benchmarks or specifying custom dimensions for the task.</p>
<p>For each dimension, we then define sub-dimensions that break down the high-level dimensions further into distinct and detailed components.The subdimensions are grounded in the original definitions of the dimensions from benchmark datasets and can also also informed by related work (Liu et al., 2023c;Laban et al., 2023;Tang et al., 2019).For instance, the original SummEval paper proposes that fluency in summarization should include sub-dimensions such as formatting, grammar, completeness, and readability.Sub-dimensions must be carefully designed to align with benchmark definitions and to prevent inconsistencies with the intended evaluation criteria.While LLMs can be used to automate the generation of sub-dimensions and questions, we found that fully relying on them often led to misalignment with the criteria defined by the benchmark (e.g., conflating coherence and fluency).This leads to evaluation that is not grounded in the benchmark design, potentially producing incorrect assessments.To address this, we only allowed human-selected sub-dimensions in our work, following prior work that recommends human oversight as an effective way to maintain alignment with benchmark objectives (Szymanski et al., 2024;Pan et al., 2024).</p>
<p>Checklist Generation</p>
<p>Seed Question Writing We create Boolean questions that correspond to the sub-dimensions defined in the first step.Each question requires a 'yes' or 'no' answer, where 'yes' indicates adherence to the evaluation criterion corresponding to the target sub-dimension.This binary format simplifies the judgment process, ensuring that evaluation criteria are explicitly defined and consistently applied (Laban et al., 2023;Liu et al., 2023c).This format also helps LLMs generate more reliable responses by constraining the answer space, minimizing response variability and reducing ambiguity.For example, the question "Are all words in the sentence spelled correctly?"elicits a clearer and more direct response than a more open-ended alternative like "How well does the sentence adhere to or deviate from standard grammar rules?".</p>
<p>Question Augmentation Manually designing a comprehensive set of evaluation questions would be ideal for ensuring a high-quality checklist.However, this approach faces scalability limitations, making it impractical to generate a sufficiently large and diverse set of questions for evaluation.This challenge becomes even more significant when extending to individual application scenarios, each requiring its own comprehensive set of questions.To this end, we expand the seed questions using LLMs, enhancing both the diversity and granularity of evaluation.Augmentation enables broader coverage while refining questions to capture a wider range of lexical and semantic variations.This process follows two strategies, each extending the coverage of seed questions.(1) Question Diversification expands evaluation diversity by introducing variations that explore different perspectives of sub-dimensions and contexts of the seed question.(2) Question Elaboration increases granularity by expanding the seed questions into more specific and detailed questions.To ensure that the augmented questions remain grounded in the seed questions, Question Diversification and Elaboration are performed independently rather than sequentially.For example, the seed question "Are all words in the sentence spelled correctly?"can be expanded into "Are all sentences complete, with no fragments or missing components?" (diversification) or specified into "Are proper nouns (names of people, places, etc.) spelled correctly?"(elaboration).</p>
<p>Question Filtering LLM-based augmentation expands the question set, but it can also generate questions that do not fully align with the intended evaluation criteria.Some questions may reflect misinterpretations of dimension definitions or add unnecessary redundancy, which can affect evaluation reliability.To filter out such questions, we apply an LLM-based minimal filtering process that evalu-ates a combined pool of seed and augmented questions for each dimension.This filtering step applies three main criteria for retaining relevant questions:</p>
<p>(1) alignment, verifying that a 'yes' response to the question indicates higher quality; (2) dimension consistency, confirming that the question adheres to the original definition of the evaluation dimension; and (3) redundancy removal, eliminating semantically overlapping questions to avoid unnecessary repetition.While there is no direct metric to measure filtering effectiveness, we observe improved correlation with human judgments after filtering, suggesting that the filtering is functioning as intended.We further validated the quality of the checklist via a human study, where annotators scored the augmented and filtered questions (Section 6.1).</p>
<p>Checklist-based Evaluation</p>
<p>In the final stage, LLMs evaluate the target text using the completed checklist (see Table 17 and 18 for the number of checklist questions and Table 26 and 27 for the dimensions, sub-dimensions, and corresponding seed question for each dataset).To improve efficiency, we ask multiple questions simultaneously rather than asking each question separately.We compared single-question and multi-question inference in our pilot experiments and found no noticeable difference in performance.Therefore, we evaluated multiple questions together to reduce the computational cost.The questions are grouped by sub-dimensions, ensuring that related questions are presented together to aid model comprehension.For each question in the checklist, the LLM generates a 'yes' or 'no' response.The final quality score is computed as the proportion of 'yes' responses among all questions (e.g., 15 'yes' out of 20 questions yields 0.75).We note that the final score is computed by uniformly weighting the checklist questions: each 'yes' response contributes equally to the final score.We discuss an alternative weighting strategy in Appendix C.2.More implementation details about the evaluation process are described in Section 4.4.</p>
<p>This approach enhances explainability by explicitly tracking how specific criteria are met, making evaluation decisions more interpretable without requiring additional rationale generation.Unlike existing LLM-as-a-Judge approaches such as G-Eval and SEEval (our main comparison points) that generate numerical scores without explanation (e.g., "Based on the conversation history, the cor-responding context, and the response, here is the evaluation: 'Naturalness': 2"), the reasoning behind the evaluation score is easily traceable from the checklist responses.</p>
<p>Experimental Setup</p>
<p>Datasets and Metrics</p>
<p>Following Liu et al. (2023b), We use three meta-evaluation benchmarks spanning various tasks to measure the effectiveness of CheckEval. SummEval (Fabbri et al., 2021) is a benchmark designed for the meta-evaluation of summarization.SummEval includes human evaluations for each generated summary across four dimensions: coherence, consistency, fluency, and relevance.Topical-Chat (Gopalakrishnan et al., 2019) is a benchmark for meta-evaluating evaluation methods for knowledge-grounded dialogue systems.Following Zhong et al. (2022), we evaluate our method using human ratings across four dimensions: naturalness, coherence, engagingness, and groundedness.QAGS (Wang et al., 2020) is another widely used benchmark, but since it focuses solely on factual consistency in summarization, we only report the results in Appendix B.1.We report Pearson's r, Spearman's ρ, Kendall's τ on each benchmark.For SummEval, correlations are calculated at the sample-level (per summary), while for Topical-Chat, they are calculated at the turn-level (per conversational response).</p>
<p>Baselines</p>
<p>We selected G-Eval (Liu et al., 2023b) and SEEval (Wu et al., 2025) as our main baselines.G-Eval adopts chain-of-thought prompting (Wei et al., 2022) and a form-filling paradigm to generate evaluation scores on a Likert scale.We selected it based on three factors: (1) its widespread adoption as a representative baseline in LLM-as-ajudge research, (2) the availability of publicly released prompts that facilitate reproducibility, and (3) its relatively simple setup that avoids confounding performance-enhancing techniques-such as prompt optimization (e.g., self-correction), training meta-evaluators, preference learning, or multiagent frameworks.SEEval follows a similar Likertstyle scoring procedure to G-Eval but augments it with a self-explanation step, prompting the model to generate brief justifications before producing its rating.This strategy is intended to improve evaluation quality without additional training.</p>
<p>Like G-Eval and SEEval, CheckEval is also designed to rely solely on a binary checklist mechanism, without introducing additional optimization techniques beyond standard prompting.Although they are not apples-to-apples comparisons, we also include comparisons to several strong methods surveyed in Gu et al. (2024) and Gao et al. (2024b), showing that CheckEval remains competitive even in light of more recent developments.Further details on the baseline implementations are provided in Appendix A.1.</p>
<p>Models</p>
<p>We test both open-source models of varying sizes and closed-source GPT models as evaluators.The models included in each category are as follows: 4  (1) Large models (70-123B):
LLama3.1-70B, Mistral-Large (123B), Qwen2.5-72B. (2) Medium models (22-32B): Mistral-Small (22B), Gemma2-27B, Qwen2.5-32B. (3) Small models (7-9B): LLama3.1-8B, Gemma2-9B, Qwen2
.5-7B, (4) GPT models: GPT-4-Turbo, GPT-4o, GPT-4o-mini (Achiam et al., 2023;Dubey et al., 2024;Jiang et al., 2023;Yang et al., 2024;Riviere et al., 2024).</p>
<p>Implementation Details</p>
<p>Following prior work (Liu et al., 2023b), we set temperature = 0, n = 1, and fix the random seed for both G-Eval, SEEval and CheckEval.Additionally, We set max_length to 20 for G-Eval as it generates a single score, 500 for SEEval following the original implementation and 200 for CheckEval as it needs to generate responses to multiple checklist questions.We used the original prompts provided by the authors of G-Eval and SEEval without any modifications.Example prompts for CheckEval are provided in the Appendix F. We evaluated multiple questions in the checklist within a single prompt to enhance efficiency and practicality rather than evaluating each question individually, as discussed in Section 3.3.</p>
<p>We used GPT-4o for both the question augmentation and filtering steps in the checklist generation stage.The total number of generated questions at each step is provided in Appendix D. Our experiments on open-weight models were conducted using vLLM 0.6.3(Kwon et al., 2023) with four A100 GPUs (or eight A6000 GPUs).The API cost to evaluate the 1,600 SummEval samples was approx- imately $66 with GPT-4 Turbo, $22 with GPT-4o, and $1.30 with GPT-4o-mini.</p>
<p>Results</p>
<p>Correlation with Human Evaluation</p>
<p>Table 1 shows the correlation between various evaluation methods and human judgments on the Sum-mEval and Topical-Chat datasets (detailed correlation results for all dimensions are shown in Table 22, 24 and 7 in the Appendix).We compare both non-LLM-as-a-Judge and LLM-as-a-Judge methods, with an emphasis on how CheckEval compares against G-Eval and SEEval across 12 LLMs.Excluding MOVERScore, most non-LLM-as-a- Judge metrics exhibit very low correlation with humans.Among LLM-as-a-Judge methods, Check-Eval consistently achieves higher correlation with human judgments than G-Eval and SEEval, with only a few exceptions of Qwen2.5 and Mistral-Small.These results suggest that CheckEval's finegrained, checklist-based design more effectively captures subtle differences in text quality, leading to improved correlation with human judgments.When analyzing model sizes, large open-source models show strong performance, with Mistral-Large combined with CheckEval achieving the highest correlation among all models (ρ = 0.55 on SummEval and r = 0.65 on Topical-Chat).Even in medium-and small-sized models-where evaluation capacity tends to be weaker-CheckEval maintains its advantage over G-Eval.Notably, some medium-sized models perform particularly well on SummEval, achieving correlations comparable to larger models.For GPT models, CheckEval consistently yields stronger correlations than G-Eval and SEEval, particularly with GPT-4o.α and Fleiss' κ, treating different LLMs within the same group (large, medium, small, GPT) as annotators.While correlation with human judgments is a main metric in LLM-as-a-Judge, high correlation alone does not guarantee reliability.Reliability is a desirable property for evaluation methods, as it ensures that different evaluator models (of similar capacity) assign similar scores/rating to the same input.This reliability is critical yet overlooked in existing frameworks.Both G-Eval and SEEval demonstrate this limitation.They achieve fairly good correlation with human judgments but show much lower IEA in general.Table 2 shows a clear gap between the IEA of G-Eval and SEEval and IEA of CheckEval, particularly for the Large and Top-3 models.This indicates inconsistent scoring across different LLM evaluator models of similar capacity.We speculate that existing protocols like G-Eval's mainly lend themselves to inconsistencies in the following two ways: (1) the evaluation dimensions adopted encompass multiple distinct fine-grained criteria, making it difficult for LLMs to generate a consistent holistic score, and (2) adjacent Likert scale scores lack clear distinctions (e.g., 3 vs. 4) and are not calibrated well across models (Laban et al., 2023).</p>
<p>Inter-evaluator Agreement (IEA)</p>
<p>CheckEval's fine-grained checklist approach improves upon this limitation greatly.For the large models, CheckEval achieves best IEA scores of 0.67 (α and κ), on SummEval, which is comparable to IEA among human raters (κ ≈ 0.7) (Fabbri et al., 2021) (See Table 23 and 25 for a detailed per-dimension IEA).We furthermore show that this improvement in IEA is not solely due to the format of the output (Likert vs. binary) in Appendix C.3.</p>
<p>Human Validation</p>
<p>We conducted two distinct human evaluation studies to validate our approach: (1) an assessment of our automated checklist generation process, and</p>
<p>(2) a direct comparison between LLM and human scores using the CheckEval protocol.</p>
<p>Validation of Checklist Generation Process</p>
<p>To verify that each stage of the checklist generation process worked as intended, we conducted an additional human evaluation focused on checklist quality.This evaluation validates the augmentation stage (seed questions, augmented questions), and filtering stage (seed questions, filtered questions) on both the SummEval and Topical-Chat datasets.</p>
<p>Human evaluators are tasked with assessing each question on a binary (yes/no) basis, determining whether it satisfies the augmentation and filtering criteria.Figure 2 shows the average scores derived from the checklist validation evaluation for both the SummEval and Topical-Chat datasets.The augmentation stage consistently achieves very high average scores across both datasets (above 90%), which suggests that the question augmentation process of CheckEval is highly effective.The filtering stage yields slightly lower scores but remains competitive.We observed that annotators often expected 1-2 additional questions per dimensions to  be filtered.Comments from annotators suggest that these questions were mostly semantically overlapping questions that the filter failed to capture.</p>
<p>To test whether removing these remaining questions would affect evaluation results, we conducted a follow-up experiment by applying an additional human-curated filtering step.We used Mistral-Large, the best-performing model, for this experiment.As shown in As shown in Table 3, the correlation scores after applying the additional filtering were extremely similar to the original results, with only minor drops.This indicates that removing one or two additional questions per evaluation dimension does not meaningfully impact the evaluation behavior, suggesting that CheckEval's automatic filtering is functioning effectively in practice.</p>
<p>Validation of CheckEval Protocol</p>
<p>To further assess the validity of CheckEval protocol, we asked human annotators to manually apply the same checklist.We then used these humangenerated scores to perform two analyses: a correlation analysis against scores from LLMs, and an inter-rater agreement analysis (Table 4).Details of the human validation setup are provided in Appendix A.4.</p>
<p>Correlation We sampled 20 summaries from the SummEval dataset.The random sampling was stratified based on original annotation scores to ensure balanced coverage of a wide range of quality levels.Three annotators evaluated each summary using the checklist, which contains approximately 25 binary (yes/no) questions per evaluation dimension.This resulted in roughly 2,000 binary annotations per annotator.For each summary, we aggregated checklist scores by summing the number of 'yes' responses per dimension, following the same method used for LLM outputs.We then computed correlation between these aggregated human scores and those from three large LLMs: Mistral-Large, Qwen2.5-72B, and Llama3.1-70B.In addition, we calculated correlation between the original Likert-scale scores from SummEval and the checklist-based human scores.All correlations are statistically significant, indicating that CheckEval scores successfully capture human judgments.</p>
<p>Agreement Due to the high annotation cost, we focused on relevance for agreement analysis.We collected binary annotations on 100 summaries (sample size selected based on a power analysis targeting 95% confidence interval width of ≤ 0.2 for IEA scores).Each annotator answered approximately 10,000 questions.We report inter-annotator agreement among the three human annotators, as well as agreement between the human group and the large LLM group.We observe high agreement between humans as well as between humans and LLMs, showing that CheckEval elicits consistent scores across both human and LLM raters.</p>
<p>Conversation history (source)</p>
<p>A: Hello, how are you today?Do you like to go to concerts?B: Not as much as I used to, but I do.A: Yeah, same here!Creed gave a concert so bad there were lawsuits against the band.... B: I have no idea.I'm sure that someone has video of it.Do you enjoy the music of the Foo Fighters?A: Oh yes, I love them.I love the video of all the drummers and other instruments playing at the same time.People came from all over the world to be in that.B: They are pretty amazing.They performed a concert so loud that it showed up on New Zealand's seismic monitors!Context -In 2002, a Creed concert was so bad that four concertgoers filed a lawsuit against the band.</p>
<p>System Response -I know, I think I have heard that before, I think it was really cool.</p>
<p>Checklist</p>
<p>Questions Answer</p>
<p>Does the response avoid unnecessary repetition of the same content between sentences?Yes Does each sentence directly relate to the topic being discussed?</p>
<p>No Is the overall message clear and easy to understand?Yes Does each sentence in the response convey a clear meaning?Yes Is the tone consistent throughout?Yes Does the response avoid using jargon or overly complex words that might confuse the listener?</p>
<p>Yes</p>
<p>Are there no major grammatical errors?Yes Are there no ambiguous terms or phrases that could confuse the reader?Yes</p>
<p>Raw Scores -Human: 3 (1-3), G-Eval: 2 (1-5), CheckEval: 0.88 (0-1)</p>
<p>Normalized Scores -Human: 1 (0-1), G-Eval: 0.25 (0-1), CheckEval: 0.88 (0-1)</p>
<p>Table 5: Case study on the naturalness dimension in the Topical-Chat.</p>
<p>Analysis</p>
<p>Stability Analysis of Evaluation Methods</p>
<p>We further analyze the stability of evaluation methods by examining the distribution of correlations with human judgments across different evaluator models.While the agreement metric (Section 5.2) focuses on how consistently models assess the same samples, stability evaluates whether an evaluation method maintains reliable alignment with human annotations across all evaluator models.As shown in Figure 3, CheckEval achieves higher mean correlation and lower variance than G-Eval and SEEval on both datasets, demonstrating more stable evaluation across different models.Detailed correlation statistics, including full mean and variance values, are available in Table 15.</p>
<p>Case Study</p>
<p>We conduct a case study on the naturalness dimension in the Topical-Chat dataset to illustrate how CheckEval enhances explainability by explicitly showing which evaluation criteria contribute to the final score (see Table 5).We evaluate system responses generated by Mistral-large, the model with the strongest correlation with human judgments.For this case study, we normalize all scores to a 0-1 scale for direct comparison.On evaluating the given text on naturalness, CheckEval (0.88) aligns more closely with human judgments (1.0), rating the response as natural.In contrast, G-Eval (0.25) assigned a much lower naturalness score.More importantly, while G-Eval provides only a score without explanation, CheckEval's systematic ).An additional case study on low-quality samples from benchmark datasets is presented in Appendix C.1, further demonstrating how CheckEval operates across a wider range of text qualities.</p>
<p>Ablation Study</p>
<p>We conducted an ablation study to assess the contribution of each component in the CheckEval pipeline.Table 6 reports results when removing filtering and augmentation step.Both components contribute to overall performance, with the augmentation stage showing a slightly larger impact.We also explore whether the performance gap can be closed by increasing the baseline inference budget in Appendix C.4.</p>
<p>Conclusion</p>
<p>We propose CheckEval, a fine-grained Boolean QA Checklist framework that addresses key limitations in existing LLM-as-Judge approaches for evaluating text generation.By decomposing evaluation criteria into structured binary questions, Check-Eval enables reliable evaluation of (open-ended) text.Our experiments across various models and datasets demonstrate that CheckEval outperforms widely-adopted Likert scale-based methods like G-Eval, achieving higher correlation to human evaluation and IEA across different LLM evaluators.The framework shows particular strength in evaluating high-quality texts by effectively capturing subtle qualitative differences while maintaining explainability.Additionally, CheckEval enhances evaluation stability through reduced variance across LLMs.This shows that our framework offers a promising solution for constructing more reliable evaluation benchmarks across diverse NLG tasks.</p>
<p>Limitations</p>
<p>CheckEval improves the reliability of LLM-as-a-Judge evaluation, but it has several limitations.</p>
<p>First, while automating checklist generation is a promising direction for improving scalability, it introduces challenges that are common to many automatic evaluation methods.CheckEval uses task-specific, human-written seed questions, which helps ground the evaluation in task-relevant criteria.However, as an automatic evaluation method, there may be factors beyond our control that lead to potential misalignment.In such cases, human involvement may be necessary to ensure alignment with task-specific goals.This is not a limitation of CheckEval specifically, but a broader challenge inherent to automatic evaluation approaches.</p>
<p>Second, this study focused on analyzing modelwise evaluation trends and comparing Likert-scale evaluation with Boolean QA checklist-based evaluation.However, recent LLM-as-a-Judge studies have introduced various techniques to enhance human alignment.Methods such as prompt optimization (e.g.chain-of-thought (Wei et al., 2022), selfcorrection (Xu et al., 2023)), multi-agent debate (Chan et al., 2024;Kim et al., 2024), and metaevaluator training (Kim et al., 2023;Wu et al., 2024b;Zhu et al., 2025) enable LLMs to make more enhanced judgments.Therefore, future work should compare it against these approaches and analyze how it differs in terms of reliability.This would also help determine whether CheckEval can be combined with such techniques to build a more robust evaluation framework.</p>
<p>Third, while CheckEval's boolean-style decision improves evaluation reliability, not all NLG tasks and evaluation criteria can be strictly answered with a yes/no response.This limitation becomes more apparent when considering evaluation scenarios involving texts two to three times longer than those in the current benchmarks.As text length increases, some parts of a response may be strong while others are weak.For example, the first half of a response may be well-written and coherent, while the latter half is unclear or contains errors.This makes binary decisions insufficient for capturing subtle quality differences.The constraints of a yes/no format may become more pronounced in long-form evaluations, suggesting that future research should explore ways to mitigate this limitation while preserving the strengths of CheckEval.</p>
<p>Fourth, CheckEval's efficacy should be tested on a wider range of NLG tasks.While this study primarily focused on summarization and dialogue response generation, additional experiments are needed to validate CheckEval's applicability to tasks such as story generation, long-form question answering, machine translation, and dialogue generation.Given that evaluation criteria vary by domain, it is important to examine how well Check-Eval generalizes across different task settings.We note that generalizability of CheckEval is already actively being tested in follow-up work: for instance CheckEval has been used for tasks such as essay scoring (Chu et al., 2025), creative writing evaluation (Lee et al., 2024), and healthcare evaluation (Mallinar et al., 2025).</p>
<p>Finally, improving the automation of checklist design and evaluation processes would enhance CheckEval's usability.Currently, checklist construction is a manual process tailored to specific tasks, making it difficult to predict the time and effort required for new evaluation domains.One potential solution is to pre-build a large-scale question database for NLG tasks and develop a system that automatically assembles relevant checklists based on task requirements.Future research should explore LLM-assisted checklist generation and reconfiguration methods to ensure that CheckEval can be efficiently applied to a broader range of tasks.</p>
<p>Acknowledgments</p>
<p>We thank Meng-Chen Wu for his help during the rebuttal process.We also thank Jungsoo Park for discussions that helped shape the initial idea of CheckEval.Thanks to Soonwon Ka, Bokyung Son, and Keonwoo Kim for their useful feedback in the early stages of the project.We also appreciate the valuable discussion and support from Yulu Qin, tinlab at BU, Naver AX unsupervised learning and DSBA NLP group at KU &amp; SNU.We acknowledge that the computational work reported in this paper was performed on the Shared Computing Cluster which is administered by Boston University's Research Computing Services.</p>
<p>A Detailed Experimental Setup</p>
<p>A.1 Baselines Baselines for main comparison (Table 1) (1) BERTScore (Zhang et al., 2019) calculates text similarity by contextual embeddings of BERT (Devlin et al., 2018).( 2) MoverScore (Zhao et al., 2019) extends BERTScore by incorporating soft alignments, allowing words to be dynamically matched across texts.It refines similarity computation through an improved aggregation strategy that accounts for word importance and semantic shifts.</p>
<p>(3) BARTScore (Yuan et al., 2021) evaluates text quality by computing the average likelihood of a generated output under a BART-based conditional probability model.( 4) UniEval (Zhong et al., 2022) is a multi-dimensional evaluation framework that assesses various dimensions of text generation by leveraging both reference-based and reference-free evaluation.(5) G-Eval (Liu et al., 2023b) is an LLM-based method, using chain-of-thought (Wei et al., 2022) and a form-filling paradigm to generate evaluation scores on a Likert scale.We select G-Eval as the main comparison point due to its widespread adoption (Liu et al., 2023a(Liu et al., , 2024)), as well as considering the similarity between G-Eval and CheckEval that neither approach involves complex prompt engineering, additional model training or multi-agent evaluation.( 6) SEEval (Wu et al., 2025) is a prompt-based evaluator that incorporates self-explanation, guiding the model to justify its rating decisions without additional training.</p>
<p>Baselines for Comparative Analysis (Table 10) (1) TIGERScore (Jiang et al., 2024) is a Llama 2 fine-tuned evaluation method that uses LLM to perform an explainable text evaluation.(2) GPTScore (Fu et al., 2023) evaluates text by computing the conditional log-likelihood of reference or output generated under LLM.(3) Analyze-Rate (Chiang and Lee, 2023) analyzes how specific design choices in LLM-based evaluation, such as explanation prompting and output format, affect alignment with human judgment and finds that encouraging explanation improves correlation.(4) HD-EVAL (Liu et al., 2024) decomposes the evaluation into fine-grained criteria and trains a regression model to aggregate them in alignment with human preferences through iterative preference-based optimization.</p>
<p>A.2 Detailed Process of Seed Question Writing</p>
<p>We constructed seed questions based on predefined evaluation criteria (e.g., coherence, consistency), aiming for atomic, conceptually clear, and non-overlapping formulations.Each evaluation dimension was first decomposed into finer-grained sub-dimensions, and a set of seed questions was written to cover each sub-dimension.This ensured both conceptual coverage and balance across dimensions.To guide this process, we consulted prior task-specific literature (e.g., summarization evaluation papers) and followed established guidelines where available.We observed that overly finegrained seed questions often led LLMs to generate augmented variants that deviated from the original intent.Therefore, we intentionally maintained an appropriate granularity level to preserve alignment throughout augmentation.All seed questions were cross-validated by our team to ensure clarity, consistency, and relevance across different evaluation dimensions.</p>
<p>A.3 Top-3 Models per Evaluation Method</p>
<p>The following models achieved the highest correlation with human judgments for each evaluation method: CheckEval (SummEval: GPT-4-Turbo, Mistral-Large, Gemma2-27B; Topical-Chat: GPT-4-Turbo, GPT-4o, Mistral-Large), G-Eval (SummEval: GPT-4-Turbo, GPT-4o-mini, Mistral-Large; Topical-Chat: GPT-4-Turbo, Mistral-Large, Qwen2.5-72B), and SEEval (SummEval: Mistral-Large, GPT-4-Turbo, Qwen2.5-32B;Topical-Chat: Mistral-Large, Qwen2.5-72B,GPT-4-Turbo).</p>
<p>A.4 Human Validation</p>
<p>To validate CheckEval, we conducted three human evaluation studies (correlation, agreement study: Section 6.2 and Checklist Validation Figure 2).For these studies, summaries were randomly sampled from the SummEval dataset using stratification based on original human annotation scores to ensure balanced coverage across quality levels.</p>
<p>Each study involves three Ph.D student-level evaluators.We recruited three human evaluators with Ph.D. student-level qualifications or above in Computer Science, all of whom had a background in evaluation research and summarization/dialogue tasks.Each participant was compensated with a gift card equivalent to approximately 10,000 KRW (≈ 7 USD) per hour.5</p>
<p>For the correlation study (Table 4 -Correlation), 20 summaries are randomly sampled from the Sum-mEval dataset.These summaries are subsequently evaluated on a binary (yes/no) basis against a checklist comprising four dimensions: coherence, consistency, fluency, and relevance.</p>
<p>For the agreement study (Table 4 -Agreement), 100 summaries are sampled from the SummEval dataset.These summaries are then evaluated on a binary (yes/no) basis concerning only relevance due to practical cost constraints (evaluation this dimension alone already requires each annotator to answer approximately 10K questions).The sample size of 100 was calculated from a power analysis based on a pilot study.</p>
<p>For the checklist validation study (Figure 2), each annotator saw the same set of items, with approximately 28 questions per evaluation dimension in SummEval and 26 in Topical-Chat.parable to results on the other two datasets reported in the main text), indicating its effectiveness as an evaluation Framework.Furthermore, Table 8 compares the IEA of G-Eval and CheckEval on the QAGS dataset.Across all model groups, CheckEval consistently achieves a higher IEA than G-Eval, demonstrating its advantage in robustness.</p>
<p>B.2 Comparative performance of various LLM-as-a-Judge methods</p>
<p>We also included a broader comparison with recent evaluation methods surveyed in Gu et al. (2024); Gao et al. (2024b).For CheckEval and G-Eval, we use scores using the best-performing evaluator in our experiments (Mistral-large).Table 10 shows that CheckEval performs well overall on both datasets, and remains competitive even compared to more recent approaches.However, we would like to emphasize again that our main goal is not to propose the best-performing LLM-as-ajudge method.Instead, our focus is on building a more reliable evaluation process and analyzing its consistency across different LLMs, and that is why comparison to G-Eval is the most directly relevant result.</p>
<p>B.3 Checklist Validation</p>
<p>To quantify the reliability of human annotations in the checklist validation study, we adopted Exact Match as our IAA metric over more common alternatives like Fleiss's Kappa.This choice was motivated by two characteristics of our data.The evaluation results showed a response distribution heavily concentrated on 'Yes' (or 1) due to the high quality of the items (see Figure Figure 2), which can make Kappa's chance correction misleading.Furthermore, the small number of items per dimension (fewer than 30) can impact the stability of Kappa scores.Given these factors, we report Exact Match scores of 0.677 for SummEval and 0.634 for Topical-Chat (see Table 9).</p>
<p>C Discussion</p>
<p>C.1 Analysis of Performance on High and Low-Quality Texts</p>
<p>As LLMs improve, their high-quality outputs become more fluent and coherent, making it increasingly difficult for evaluation methods to differentiate subtle quality differences.Meanwhile, lowquality text poses a different challenge, as its overall readability is low, obscuring distinctions between evaluation criteria and making it harder to properly assess all target dimensions of quality.Given these differences, it is important to assess how evaluation methods handle varying levels of text quality.To this end, we conduct a detailed dimension-wise analysis by dividing the data into high-quality and low-quality groups based on human annotation scores (e.g., on a 1-5 scale, treat scores ≥3 as High, &lt;3 as Low).We compute the average correlation across 12 LLMs to analyze how CheckEval and G-Eval align with human judgments for different levels of text quality.As shown in Figure 4, CheckEval consistently achieves higher correlations with human judgments than G-Eval in high-quality texts across all dimensions.Notably, for SummEval, CheckEval shows much stronger alignment in fluency (0.34 vs. 0.16).For Topical-Chat, it outperforms G-Eval in engagingness (0.60 vs. 0.42) and naturalness (0.44 vs. 0.35) by a large margin.</p>
<p>However, for low-quality texts, while CheckEval generally maintains stronger correlations compared to G-Eval, it exhibits performance drops in a small number of cases, notably in fluency (Sum-mEval) and groundedness (Topical-Chat).From our additional analysis of the results, one possible explanation is that discrepancies between benchmark definitions and actual human annotations of these dimensions may have contributed to the observed performance drop in CheckEval.For example, while SummEval defines fluency as the ab- sence of formatting issues, capitalization errors, or ungrammatical sentence structures that hinder readability, human annotators often prioritized overall readability over strict grammatical correctness.Since CheckEval relies on fine-grained Boolean QA decisions aligned with predefined criteria, the correlation with human scores may be impacted when human annotation practices deviate from the exact evaluation guidelines.In the groundedness dimension of Topical-Chat, a different issue arises.For low-quality texts, CheckEval's strict yes/no framework often results in uniformly low scores, making it difficult to distinguish between varying degrees of poor responses.In contrast, G-Eval, which allows for more gradient judgments, showed advantages in these cases.This suggests potential refinements to the Boolean QA framework to better handle annotation inconsistencies while preserving its fine-grained evaluation capability.</p>
<p>C.2 Does CheckEval need question weighting?</p>
<p>We conducted an additional analysis to investigate whether incorporating question-specific weights improves the reliability of CheckEval scores.Motivated by HD-Eval (Liu et al., 2024)  weights) of each checklist question.These weights were then used to compute a weighted CheckEval score.To assess robustness, the process was repeated across five random seeds, each sampling a different 20% subset of the data.Table 11 reports the average results and standard deviation across seeds."original" denotes the unweighted CheckEval score, while "weighted" denotes the score after applying the learned question-specific weights.The overall results were mixed.A couple of evaluator models benefited from learning the weights, but most others did not.Since there were no reliable gains from weighting the questions, we ultimately chose not to incorporate weighted aggregation into our results.While we only experimented with a simple linear weighting strategy here, we could explore more sophisticated methods of estimating question importance as well as learning weights that are generalizable across different evaluator models in future work.</p>
<p>C.3 Does Binarizing Likert-Scale Outputs</p>
<p>Close the IEA Gap?</p>
<p>We conducted an additional analysis to investigate whether the observed IEA gap is a fundamental difference between the evaluation protocols or simply an artifact of their different output formats (binary vs. Likert).One way to test this would be to directly binarize the Likert scores derived from the evaluator models.We conducted this experiment with G-Eval's Likert-scale outputs-that is, we converted the Likert scores (1-5 scale for SummEval and 1-3 scale for Topical-Chat) to binary (0/1) scores by mapping the lower values to 0 and higher values to 1.To ensure that the results are not affected by the mapping choice of the middle value on the scale, we tested both possible versions of the mapping schemes: treating the middle value as 0 and 1, respectively, as detailed in Table 12.Scores of evaluation dimensions that already employed a binary scoring scheme were not converted.</p>
<p>The results, shown in Tables 13 and 14, are clear and consistent.While binarizing the outputs does improve G-Eval's IEA scores compared to using the original Likert scale scores, a large performance gap to CheckEval remains across all model groups.We therefore conclude that the performance difference is not solely an effect of the output format but stems from the fundamental improvements in our proposed checklist-based evaluation protocol.</p>
<p>C.4 Does Increasing the Inference Budget</p>
<p>Strengthen the Baselines?</p>
<p>To address the possibility that our performance gains stem from differences in the inference budget, we increased the budget for the baseline.One straightforward way to do this is to sample multiple outputs and aggregate the results.We applied this method to G-Eval on Topical-Chat, setting 'tem-perature=1.0' to enable diverse generations and using 'n=3' samples before averaging the scores.</p>
<p>As shown in</p>
<p>D The number of questions at each stage</p>
<p>We provide a step-by-step breakdown of the number of questions, from the initial seed questions through the augmentation and filtering stages to the final checklist, with the number of questions varying across different dimensions.Before and after filtering, the correlation shows slight variations.For the SummEval, Spearman's ρ changed from 0.4790 to 0.4816, while Kendall's τ changed from 0.4143 to 0.4163.In the Topical-Chat, Pearson's r remained unchanged at 0.5553, whereas Spearman's ρ increased from 0.5446 to 0.5546.The number of questions for each dataset is reported in Table 17 and 18, respectively.</p>
<p>E Open-source model information Evaluation Prompt for SummEval <Task Overview> Your task is to read a provided news article and its summary, then answer 'yes' or 'no' to specific questions.These questions will relate to a particular dimension of the summary.</p>
<p><dimension Definition> <dimension>-<definition> <Instructions> 1. Read these instructions thoroughly.</p>
<ol>
<li>
<p>Carefully read both the Article and the Summary.</p>
</li>
<li>
<p>Understand the given questions and the definition of the <dimension>.</p>
</li>
</ol>
<p>4.</p>
<p>Respond to each question with 'yes' or 'no'.Base your answers on a clear rationale.</p>
<ol>
<li>Follow the specified format for your answers.Evaluation Prompt for Topical-Chat <Task Overview> You will be given a conversation between two individuals.You will then be given one potential response for the next turn in the conversation.The response concerns an interesting fact, which will be provided as well.</li>
</ol>
<p>Your task is to read a provided conversation history, corresponding fact, and response, then answer 'yes' or 'no' to specific questions.These questions will relate to a particular dimension of the response.</p>
<p><dimension Definition> <dimension>-<definition> <Instructions> 1. Read these instructions thoroughly.</p>
<ol>
<li>
<p>Carefully read the Conversation History, the Corresponding Fact, and the Response.</p>
</li>
<li>
<p>Understand the given questions and the definition of the <dimension>.</p>
</li>
</ol>
<p>4.</p>
<p>Respond to each question with 'yes' or 'no'.Base your answers on a clear rationale.</p>
<ol>
<li>Follow the specified format for your answers.Augmentation -Question Diversification Prompt <Task Overview> You will be provided with: 1) Information about the benchmark to be evaluated, 2) The main concept being assessed in the benchmark, and 3) Seed questions that include key components and sub-questions related to this concept.Your task is to create additional sub-questions for the key components to comprehensively assess the main concept.Each sub-question must meet given conditions to ensure a high-quality question set.-Each question should assess only a single dimension or concept.</li>
</ol>
<p>-Each question should not ask about more than one topic or concept.</p>
<p>Figure 1 :
1
Figure 1: Overall process of CheckEval.CheckEval consists of three stages: (1) Defining Dimensions of Evaluation, where humans select specific dimensions and define sub-dimensions; (2) Checklist Generation, which incorporates two augmentation methods-question diversification (green) and elaboration (blue); and (3) Checklist-based Evaluation, where the model responds to the checklist with yes/no answers.</p>
<p>Figure 3 :
3
Figure 3: Kernel density estimation (KDE) of correlations with human judgments for G-Eval (purple), SEEval (blue) and CheckEval (pink) across different evaluator models on SummEval and Topical-Chat.Dashed lines indicate mean correlation values.</p>
<p>In addition, JK and PK were supported by Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2025-02214591, Development of an Innovative AI Agent for Worker-Friendly Autonomous Manufacturing).Also, JK and PK were supported by the BK21 FOUR Program (Education and Research Center for Industrial Innovation Analytics) funded by the Ministry of Education, Korea (No. 4120240214912)</p>
<p>Figure 4: dimension-wise correlation analysis of G-Eval (purple) and CheckEval (pink), with samples divided based on human annotator ratings into High-Quality (human ratings ≥3) and Low-Quality (human ratings &lt;3) groups.Each bar represents correlation with human judgments across different quality dimensions.</p>
<p>Provide your answers to the given questions, following the specified Answer Format.</p>
<p>Figure 5 :
5
Figure 5: Evaluation Prompt -SummEval</p>
<p>Provide your answers to the given questions, following the specified Answer Format.</p>
<p>Figure 6 :
6
Figure 6: Evaluation Prompt -Topical-Chat</p>
<p>-question must be answerable with a simple 'yes' or 'no'.-A'yes' answer should indicate that the sentence improves the specified evaluation criterion (e.g., Coherence, Relevance).</p>
<p>Figure 7 :
7
Figure 7: Augmentation -Question Diversification Prompt</p>
<p>Table 2 :
2
Inter-evaluator agreement (IEA) results for SummEval and Topical-Chat, comparing G-Eval, SEEval and CheckEval across different model groups.Top-3 refers to the three models with the highest correlation to human judgments (
ModelEvaluationSummEval (Avg.) Topical-Chat (Avg.)GroupMethodsακακAllG-Eval SEEval0.09 0.080.19 0.140.06 0.070.34 0.31CheckEval 0.480.480.450.45LargeG-Eval SEEval0.05 0.060.16 0.190.01 0.550.51 0.61CheckEval 0.670.670.670.67MediumG-Eval SEEval0.04 0.090.14 0.130.07 0.060.22 0.34CheckEval 0.560.560.500.50SmallG-Eval SEEval0.06 0.020.10 0.070.04 0.160.16 0.15CheckEval 0.240.240.170.17GPTG-Eval SEEval0.08 0.130.20 0.320.04 0.120.50 0.51CheckEval 0.560.560.540.54Top-3  *G-Eval SEEval0.07 0.090.23 0.190.03 0.060.56 0.34CheckEval 0.650.650.570.57
* see Appendix A.3 for the list of top-3 models for each evaluation method).The best score per model category is bolded.</p>
<p>Table 2
2AugmentationFilteringHuman Score0.7 0.8 0.9 1.00.930.810.960.850.6SummEvalTopical ChatFigure 2: Human validation scores for the checklist gen-eration process, averaged across all dimensions on bothSummEval and Topical-Chat. 'Augmentation' refers tothe percentage of augmented questions that fulfilled thespecified quality criteria, and 'Filtering' refers to thepercentage for filtered questions.
compares the IEA of G-Eval, SEEval and CheckEval on the SummEval and Topical-Chat datasets.We measure IEA using Krippendorff's</p>
<p>Table 3 :
3
, and 0.67 (α and κ) on Topical-Chat.Crucially, CheckEval maintains both high correlation and IEA across different LLMs and tasks.These results demonstrate that CheckEval provides a more reliable evaluation than G-Eval and SEEval Effect of applying additional human filtering to Mistral-Large.# indicates that filtering was applied.
ModelEvaluationSummEval (Avg.)MethodρτMistral-LargeCheckEval0.54860.4797CheckEval #0.54860.4797Topical-Chat (Avg.)ρrCheckEval0.64510.6453CheckEval #0.64430.6412QAGS (Avg.)rρCheckEval0.66810.6558CheckEval #0.66800.6558</p>
<p>Table 4 :
4
Human validation of the CheckEval protocol on SummEval.C denotes CheckEval, L denotes Likert (original SummEval Score).We use the LLM results from the large model group.( * * : p &lt; .01,* * * : p &lt; .001)</p>
<p>Table 7 :
7
Average correlation scores across dimensions on the QAGS-CNN and QAGS-Xsum.we report r, ρ and τ .</p>
<p>Colors indicate model groups: large (pink), medium (blue), small (green) and GPT (purple).</p>
<p>Table 8 :
8
IEA -QAGS
B Additional ResultsB.1 Additional experiments with QAGS</p>
<p>Table7shows the correlation between various evaluation methods and human judgments on the QAGS dataset.The results show that CheckEval outperforms G-Eval for 9 out of the 12 LLMs (com-
SummEvalCoh.Con.Flu.Rel.Avg.EM0.7330 0.6920 0.7100 0.5710 0.6765Topical-ChatCoh.Eng.Gro.Nat.Avg.EM0.6470 0.6670 0.6300 0.5910 0.6338Table 9: Agreement (Exact Match) for each dimensionin checklist validation.</p>
<p>Table 10 :
10
Comparative performance of various LLM-asa-Judge methods.Models marked with † are fine-tuned.
EvaluationModelSummEval (Avg.) Topical-Chat (Avg.)MethodsρτρrTIGERScore LLaMA 2-13B  † 0.390.310.280.26GPTScoreGPT-40.390.340.360.34G-EvalMistral-large0.520.470.640.62Analyze-Rate Claude 3 Sonnet 0.530.440.640.64HD-EVALGPT-40.53-0.620.63SEEvalClaude 3 Sonnet 0.520.470.650.64CheckEval Mistral-large 0.550.480.650.65</p>
<p>Table 11 :
11
Effect of question weighting strategy on Sum-mEval.</p>
<p>Table 12 :
12
Binary conversion schemes applied to G-Eval's Likert-scale outputs to enable fairer IEA comparison.</p>
<p>Table 13 :
13
Table 16, the resulting correlations changed minimally (r 0.6387 vs. 0.6389; ρ 0.6169 vs. 0.6176), indicating that this aggregation does not close the performance gap with our checklistbased approach.IEA on SummEval after converting G-Eval's Likert-scale outputs to binary formats.
Model SizeMethodακAllG-Eval0.09290.1859G-Eval (binary [4,5]→1)0.10630.2812G-Eval (binary [3,4,5]→1)0.10740.2835CheckEval0.48030.4803BestG-Eval0.07310.2266G-Eval (binary [4,5]→1)0.06660.4650G-Eval (binary [3,4,5]→1)0.06660.4647CheckEval0.64710.6471GPTG-Eval0.08410.2018G-Eval (binary [4,5]→1)0.06930.3012G-Eval (binary [3,4,5]→1)0.06760.3016CheckEval0.55750.5575LargeG-Eval0.05120.1586G-Eval (binary [4,5]→1)0.32040.4646G-Eval (binary [3,4,5]→1)0.32280.4575CheckEval0.67310.6731MediumG-Eval0.04300.1411G-Eval (binary [4,5]→1)0.06060.2758G-Eval (binary [3,4,5]→1)0.06580.2821CheckEval0.56170.5617SmallG-Eval0.06350.0998G-Eval (binary [4,5]→1)0.14500.1984G-Eval (binary [3,4,5]→1)0.08350.1995CheckEval0.23870.2387</p>
<p>Table 19 provides links to all open-source models used in our experiments.Table 20 lists each model along with its corresponding license.Table 21 summarizes the datasets used and their associated licenses.If a dataset is publicly available but no explicit license is provided, we denote the license as '-' in the table.</p>
<p>This is equivalent to Inter-Annotator Agreement (IAA) in human evaluation(Artstein,<br />
), but we use the term IEA in this paper to make it clear that the agreement we are aiming to improve is agreement between evaluator models, rather than between human raters providing the gold evaluation.
Our checklist concept is inspired by Ribeiro et al. (2020), who proposed checklist-based testing for NLP models.
Recent studies(Wu et al., 2024a;Wang et al., 2024) use LLM-as-a-Judge as a reward signal in alignment training with RLHF(Ouyang et al., 2022). However, this approach primarily aims to optimize model training rather than enhance evaluation robustness and explainability. Our work focuses on improving evaluation frameworks, and integrating evaluation signals into model training is beyond our scope.
Note that the annotation was conducted in South Korea, where the compensation level is slightly above the local minimum wage.
MethodsCNNF PromptsFigure5and 6 shows the detailed evaluation prompt.Figure7and 8 shows the detailed augmentation prompt.Figure9shows the filtering prompt.Filtering Prompt <Task Overview>Your task is to filter out questions from a list based on the following criteria:1) dimension Alignment:-dimension definition: {dimension def} -Remove questions that deviate from the given dimension's definition.-Remove questions that are more closely related to other dimensions than the current one.2) Redundancy:-Remove questions that: * Ask for the same or very similar information (even if phrased differently).<em> Convey very similar meanings without adding unique insight.3) Style:-Remove questions that: * Use overly exaggerated wording.</em> Focus on excessively detailed or minor points that don't meaningfully affect overall quality.4) Benchmark Context-Name: Topical-Chat -Purpose: Evaluation of knowledge-grounded dialogue systems -Key Metrics: Naturalness, Coherence, Engagingness, Groundedness -Do not modify any of the remaining questions or generate new ones.-Keep questions in their original dictionary format.5) Sub-dimensions and Questions: {format_sub_dimensions(sub_dimensions)}6) Output Requirements:-Output format: JSON only -Structure:{"Sub-dimension Name": [ "Filtered Question 1", "Filtered Question 2"]} <Important Note> -Do not modify the content of remaining questions -Do not generate new questions -Maintain the original dictionary format -Only remove questions that fail the above criteria -Do not remove entire sub-dimensions or their keys unless no valid questions remain.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. GPT-4 technical report. </p>
<p>Inter-annotator agreement. Handbook of linguistic annotation. Ron Artstein, 2017</p>
<p>Alessandro Suglia, Aditya K. Surikuchi, Ece Takmaz, and Alberto Testoni. 2024. Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, F T André, Philipp Martins, Vera Mondorf, Sandro Neplenbroek, Pezzelle, 10.48550/arXiv.2406.18403CoRR, abs/2406.18403Barbara Plank, David Schlangen</p>
<p>Disentangling the properties of human evaluation methods: A classification system to support comparability, meta-evaluation and reproducibility testing. Anya Belz, Simon Mille, David M Howcroft, 10.18653/v1/2020.inlg-1.24Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, Ireland2020Association for Computational Linguistics</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Chateval: Towards better LLM-based evaluators through multi-agent debate. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Generating literal and implied subquestions to fact-check complex claims. Jifan Chen, Aniruddh Sriram, Eunsol Choi, Greg Durrett, arXiv:2205.069382022</p>
<p>A closer look into using large language models for automatic evaluation. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.findings-emnlp.599Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023Association for Computational Linguistics</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Think together and work better: Combining humans' and llms' think-aloud outcomes for effective text evaluation. Seong Yeub, Chu , Jong Woo Kim, Mun Yong, Yi , 10.1145/3706598.3713181Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI '25. the 2025 CHI Conference on Human Factors in Computing Systems, CHI '25New York, NY, USAAssociation for Computing Machinery2025</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018</p>
<p>Angela Fan. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, arXiv:2407.21783et al. 2024. The llama 3 herd of models</p>
<p>Summeval: Re-evaluating summarization evaluation. Wojciech Alexander R Fabbri, Bryan Kryściński, Caiming Mc-Cann, Richard Xiong, Dragomir Socher, Radev, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023</p>
<p>Analyzing and evaluating correlation measures in nlg meta-evaluation. Mingqi Gao, Xinyu Hu, Li Lin, Xiaojun Wan, arXiv:2410.168342024a</p>
<p>Llm-based nlg evaluation: Current status and challenges. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan, ArXiv, abs/2402.013832024b</p>
<p>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tür, 10.21437/Interspeech.2019-3079Proc. Interspeech. Interspeech2019. 2019</p>
<p>. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, arXiv:2411.15594et al. 2024. A survey on llm-as-a-judge</p>
<p>Perception score: A learned metric for open-ended text generation evaluation. Jing Gu, Qingyang Wu, Zhou Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023</p>
<p>TIGER-Score: Towards building explainable metric for all text generation tasks. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen, Transactions on Machine Learning Research. 2024</p>
<p>Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, Greg Durrett, arXiv:2303.01432Wice: Real-world entailment for claims in wikipedia. 2023</p>
<p>DEBATE: Devil's advocate-based assessment and text evaluation. Alex Kim, Keonwoo Kim, Sangwon Yoon, 10.18653/v1/2024.findings-acl.112Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, arXiv:2310.08491Prometheus: Inducing fine-grained evaluation capability in language models. 2023</p>
<p>Longeval: Guidelines for human evaluation of faithfulness in long-form summarization. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, Kyle Lo, arXiv:2301.132982023</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>SummEdits: Measuring LLM ability at factual reasoning through the lens of summarization. Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu, 10.18653/v1/2023.emnlp-main.600Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Navigating the path of writing: Outline-guided text generation with large language models. Yukyung Lee, Soonwon Ka, Bokyung Son, Pilsung Kang, Jaewook Kang, arXiv:2404.139192024</p>
<p>Evaluating large language models -principles, approaches, and applications. Bo Li, Irina Sigler, Yuan Xue, 2024. 2024 TutorialNeurips</p>
<p>Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. Margaret Li, Jason Weston, Stephen Roller, arXiv:1909.030872019</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects. Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang, arXiv:2311.08788arXiv:2303.16634Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. Gpteval: Nlg evaluation using gpt-4 with better human alignment. 2023a</p>
<p>Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev, 10.18653/v1/2023.acl-long.228Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023c1</p>
<p>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, arXiv:2309.13308Calibrating llmbased evaluator. 2023d</p>
<p>HD-eval: Aligning large language model evaluators through hierarchical criteria decomposition. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, 10.18653/v1/2024.acl-long.413Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Neil Mallinar, Ali Heydari, Xin Liu, Anthony Z Faranesh, Brent Winslow, Nova Hammerquist, Benjamin Graef, Cathy Speed, Mark Malhotra, arXiv:2503.23339Shwetak Patel, et al. 2025. A scalable framework for evaluating health language models. arXiv preprint</p>
<p>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/2023.emnlp-main.741Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Evaluating content selection in summarization: The pyramid method. Ani Nenkova, Rebecca J Passonneau, Proceedings of the human language technology conference of the north american chapter of the association for computational linguistics: Hltnaacl 2004. the human language technology conference of the north american chapter of the association for computational linguistics: Hltnaacl 20042004</p>
<p>Why we need new evaluation metrics for NLG. Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, Verena Rieser, 10.18653/v1/D17-1238Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, Denmark2017Association for Computational Linguistics</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Human-centered design recommendations for LLM-as-a-judge. Qian Pan, Zahra Ashktorab, Michael Desmond, Martín Santillán Cooper, James Johnson, Rahul Nair, Elizabeth Daly, Werner Geyer, Proceedings of the 1st Human-Centered Large Language Modeling Workshop, TBD. ACL. the 1st Human-Centered Large Language Modeling Workshop, TBD. ACL2024</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Large language models are effective text rankers with pairwise ranking prompting. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, Michael Bendersky, 10.18653/v1/2024.findings-naacl.97Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, Mexico2024Association for Computational Linguistics</p>
<p>Beyond accuracy: Behavioral testing of NLP models with CheckList. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh, 10.18653/v1/2020.acl-main.442Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>. Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.00118et al. 2024. Gemma 2: Improving open language models at a practical size</p>
<p>Large language models are inconsistent and biased evaluators. Rickard Stureborg, Dimitris Alikaniotis, Yoshi Suhara, arXiv:2405.017242024</p>
<p>Comparing criteria development across domain experts, lay users, and models in large language model evaluation. Annalisa Szymanski, Araya Simret, Oghenemaro Gebreegziabher, Ronald A Anuyah, Toby Jia-Jun Metoyer, Li, arXiv:2410.020542024</p>
<p>A topic augmented text generation model: Joint learning of semantics and structural features. Hongyin Tang, Miao Li, Beihong Jin, 10.18653/v1/D19-1513Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Asking and answering questions to evaluate the factual consistency of summaries. Alex Wang, Kyunghyun Cho, Mike Lewis, 10.18653/v1/2020.acl-main.450Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li, arXiv:2408.02666Self-taught evaluators. 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Generating scientific claims for zeroshot scientific fact checking. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu, Wang , arXiv:2203.129902022</p>
<p>SEEval: Advancing LLM text evaluation efficiency and accuracy through self-explanation prompting. Meng-Chen Wu, Md Mosharaf Hossain, Tess Wood, Shayan Ali Akbar, Si-Chi Chin, Erwin Cornejo, Findings of the Association for Computational Linguistics: NAACL 2025. Albuquerque, New MexicoAssociation for Computational Linguistics2025</p>
<p>Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar, arXiv:2407.19594Meta-rewarding language models: Self-improving alignment with llmas-a-meta-judge. 2024a</p>
<p>InstructEval: Instruction-tuned text evaluator from human preference. Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, 10.18653/v1/2024.findings-acl.799Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b</p>
<p>Evaluating evaluation metrics: A framework for analyzing NLG evaluation metrics using measurement theory. Ziang Xiao, Susu Zhang, Vivian Lai, Q Vera, Liao, 10.18653/v1/2023.emnlp-main.676Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Instructscore: Towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang, Wang , Lei Li, arXiv:2305.142822023</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.15115Qwen2. 5 technical report. 2024</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, arXiv:1909.026222019</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, arXiv:2210.071972022</p>
<p>JudgeLM: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, The Thirteenth International Conference on Learning Representations. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9683 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9683</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9683</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-72faa912691bc2d8dc342fc1c9b53cccc9ba56fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/72faa912691bc2d8dc342fc1c9b53cccc9ba56fc" target="_blank">LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context</a></p>
                <p><strong>Paper TL;DR:</strong> LiveIdeaBench is introduced, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts, and it is revealed that the scientific idea generation capabilities measured by this benchmark, are poorly predicted by standard metrics of general intelligence.</p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9683.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9683.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveIdeaBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveIdeaBench benchmark for scientific idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic benchmark that evaluates LLMs' divergent thinking for scientific idea generation from single-keyword prompts across five dimensions (originality, feasibility, clarity, fluency, flexibility) using a multi-model judge panel and a dataset of 1,180 keywords spanning 22 scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>41 state-of-the-art LLMs (dynamic roster drawn from LiveBench)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A heterogeneous set of 41 models (proprietary and open-weight) selected monthly from LiveBench top performers (examples: claude-3.7-sonnet:thinking, gemini-2.0 variants, qwq-32b-preview, deepseek-r1, etc.). Models span multiple architectures and parameter scales and are both idea generators and (top-10) judges in different roles.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multidisciplinary (22 scientific disciplines; cross-domain ideation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated LLM-as-a-judge ensemble with randomized sampling: idea-generation from single-keyword prompts (100-word target), three randomly sampled judges per idea for originality/feasibility/clarity, one randomly sampled judge per keyword-model pair for fluency; judge panel is the top-10 LiveBench models subject to diversity constraints and exclusion of the evaluated model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Five dimensions derived from Guilford: originality (novelty), feasibility (technical/practical implementability), clarity (coherent articulation), fluency (diversity/number of distinct ideas per keyword), flexibility (30th percentile of composite scores across keywords).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Dynamic keyword set of 1,180 high-impact scientific keywords (updated monthly from a real-time analytics source) covering 22 disciplines; model roster is refreshed monthly from LiveBench.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Across 41 models and 1,180 keywords, LiveIdeaBench reveals that scientific idea-generation (divergent thinking) is poorly predicted by general intelligence. Key findings: weak-but-significant correlation between LiveIdeaBench and LiveBench general intelligence (r=0.357, p=0.038, N=41); very weak positive correlation between idea length and quality (r=0.096, p<0.0001). Pareto analysis shows trade-off between originality and feasibility (e.g., claude-3.7-sonnet:thinking highest originality; nova-pro-v1 high feasibility; deepseek-r1 and qwq-32b show balanced profiles). Dynamic judge ensemble and sampling used to produce averaged per-dimension and composite scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Temporal comparability issues as judge panel and model APIs update; potential judge-model biases (sycophancy or misjudgment of novel concepts outside judges' knowledge); safety refusals that reduce creativity scores for safety-conservative models; dynamic benchmark complicates strict reproducibility; LLM-as-judge may mis-evaluate highly novel/out-of-distribution scientific ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>LLM-based judging validated in a domain-specific human expert check (PDE domain) showing strong alignment for originality (r ≈ 0.82), indicating reasonable concordance with human experts in that domain; overall approach trades human-cost for scale and consistency but may miss nuanced expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use dynamic multi-model judge ensembles with random sampling and exclusion of the evaluated model; impose judge diversity constraints (≤20% per organization); standardize response length (100-word target, max 200 words); apply fallback prompts for refusals; combine automated LLM-judging with targeted human validation in specialized domains; consider RAG augmentation for judges to reduce knowledge-bound misjudgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9683.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9683.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge evaluation methodology (ensemble & sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated evaluation approach using a panel of top LLMs as critics to score generated ideas across multiple creativity dimensions, employing randomized sampling and ensemble averaging to mitigate individual model bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Top-10 LiveBench models (sampled judges)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Judge pool comprises the top 10 LiveBench models at evaluation time, subject to diversity constraints (max two models per organization) and exclusion rules so that the evaluated model is never in its own judging set.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific ideation across domains (applied uniformly to all 22 categories)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Randomized sampling from the judge panel: three judges per idea for originality/feasibility/clarity, one judge per keyword-model for fluency; each judge returns text analysis plus numeric 1–10 scores (via prescribed critic prompt); final scores are averages across sampled judges.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality, feasibility, clarity, fluency (graded categorical->numeric mapping), flexibility (derived metric); judges follow a strict system prompt to act as demanding scientific reviewers and output JSON scores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used within LiveIdeaBench across 1,180 keywords and 41 generator models.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM-as-judge permits scalable, consistent scoring with ensemble averaging to reduce single-model bias; produced per-dimension scores used to rank and profile models. Human validation in PDE shows strong agreement for originality (r≈0.82), supporting feasibility of automated judging in at least some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Judge models can exhibit sycophancy or alignment-induced bias, may lack up-to-date knowledge or misunderstand novel concepts, and judge-panel composition changes over time (affecting longitudinal comparability). Single-judge fluency assessment may be noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Offers large-scale, lower-cost alternative to human panels; empirical alignment with human experts observed in PDE for originality, but LLM judges may still fail on domain-nuanced or highly technical feasibility judgments compared to domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multi-judge ensembles, random sampling, and strict judge exclusion rules; complement with targeted human expert validation especially in niche domains; consider RAG for judges to ground evaluations in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9683.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9683.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FluencyMapping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fluency grading: categorical (A/B/C/D) → numeric mapping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fluency evaluation method that grades the distinctiveness of ideas generated for the same keyword using an LLM judge, mapping qualitative grades to a 1–10 numeric scale to measure idea diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Single randomly sampled judge from judge panel (per keyword-model pair)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>One judge evaluates the set of ideas produced by a single model for a given keyword and assigns a grade: D (identical), C (similar), B (different but similar problem), A (completely different problems).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applies across all scientific domains in the LiveIdeaBench keyword set</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>LLM judge compares multiple ideas generated from the same keyword and assigns a categorical grade; categories are mapped linearly to integers: D→1, C→4, B→7, A→10, producing a fluency score on the 1–10 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Idea distinctiveness and substantive diversity (not just surface lexical differences).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to each keyword across models in LiveIdeaBench (1,180 keywords).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fluency scores are used in multidimensional model profiles and in the Pareto visualization (fluency represented by bubble size); top performers (e.g., claude and Gemini variants) show strong fluency alongside originality in many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Single-judge assessment per keyword-model pair can introduce noise; mapping of four qualitative categories to 1–10 is coarse and may miss finer gradations; depends on judge's semantic discrimination capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>LLM judgments for fluency are more semantically informed than simple lexical-diversity metrics but may differ from human panels that can apply more nuanced categorical distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer LLM-based semantic fluency judgments over purely syntactic diversity measures for scientific ideation; consider multiple judges per fluency assessment if resources allow to reduce single-judge noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9683.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9683.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flexibility30P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flexibility measured as 30th-percentile composite score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operationalization of flexibility as a conservative cross-domain generalization metric computed as the 30th percentile of the distribution of a model's averaged scores across originality, feasibility, clarity, and fluency for different keywords.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>All evaluated generator models (used to compute distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Scores from each model across keywords are used to compute the percentile-based flexibility metric.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (assesses generalization across the 22 scientific categories)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute per-keyword composite = mean(originality, feasibility, clarity, fluency); flexibility = 30th percentile of these composites over the keyword set for a model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Considers performance floor rather than mean or max; intended to detect models that maintain reliable performance across diverse contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Computed over LiveIdeaBench's 1,180-keyword dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Flexibility identifies models that generalize rather than excel only on niche keywords; used in multidimensional profiling and contributes to overall composite rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of the 30th percentile is heuristic and influences sensitivity; may under-emphasize rare but important strengths; percentile-based metric can be unstable for small keyword subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unlike human assessments that might judge cross-domain adaptability qualitatively, this provides a reproducible numeric floor metric but requires careful interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use percentile-based flexibility to capture conservative generalization capability; report alongside mean/variance to provide full performance picture; consider alternate percentiles in sensitivity analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9683.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9683.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SingleKeywordPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-keyword minimal-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stimulus design where models receive only one keyword as prompt (with a 100-word idea constraint) to evoke divergent scientific ideation from internal model representations, mirroring classic divergent thinking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Idea-generating models from LiveIdeaBench (41 models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models are prompted to produce concise (≤100 words target, 200 words max) scientific ideas given only a single assigned keyword; a fallback prompt is used for safety-related refusals.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Designed to elicit ideation across all included scientific domains (22 categories).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Single-keyword stimulus followed by automatic evaluation (LLM judges) across the five dimensions; enforces a tight word limit to emphasize concise ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Measures capacity for divergent thinking: fluency (multiple distinct ideas per keyword), originality, feasibility, clarity, and cross-keyword flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to LiveIdeaBench's keyword set of 1,180 items.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Single-keyword prompting exposes divergence between models' convergent problem-solving ability and raw ideation capacity; produced cases where lower-general-intelligence models (e.g., qwq-32b-preview) matched or exceeded higher-general-intelligence models in ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Minimal context limits depth and domain-specific technicality of ideas and may disadvantage models relying on retrieval/long-context capabilities; may conflate internal memorization with genuine combinatorial ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Analogous to human divergent thinking tasks (Guilford-style), intentionally probes ideation rather than synthesis; complements traditional benchmarks that use richer context.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use single-keyword prompts to specifically probe ideation capability; combine with richer-context tasks for full pipeline evaluation; apply fallback prompts for safety refusals and explicitly cap response length for comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9683.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9683.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JudgePanelProtocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judge panel formation & evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedural rules governing judge panel composition, sampling, and evaluation to reduce bias and ensure independence during LiveIdeaBench assessments (diversity constraints, exclusion of evaluated model, sampling sizes, refusal handling, and response standardization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Judge panel: top-10 LiveBench models (dynamically selected)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Panel drawn monthly from LiveBench leaderboard with constraints: max two models per organization, avoid duplicate base-model variants, and exclude the target model when evaluating its outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used for cross-domain evaluation across all LiveIdeaBench categories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>When evaluating an idea: sample 3 judges (without replacement) from eligible panel for originality/feasibility/clarity; sample 1 judge for fluency comparisons; apply the critic system prompt; average scores across judges; exclude overly long responses (>200 words); use fallback prompt on refusals; special parsing for models with reasoning artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Adheres to prescribed judge counts (3 for core dimensions, 1 for fluency), strict prompt templates, and numeric 1–10 scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Operational protocol for LiveIdeaBench's experimental runs over 1,180 keywords and 41 models.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Protocol produced robust ensemble scores and reduced single-model bias; enabled fair comparisons across heterogeneous models and enforced independence to avoid self-evaluation artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Panel composition changes over time causing longitudinal comparability issues; diversity constraints and exclusions may alter which models are available as judges at each run; single-judge fluency scoring is a potential weak point.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Formalized sampling and exclusion rules provide reproducibility advantages over ad-hoc human panels; however human panels offer richer domain judgement which LLM judges may not fully match.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Maintain and report panel composition and sampling seeds for reproducibility; cap per-organization representation; exclude evaluated models from judging rounds; document fallback and refusal handling procedures; consider additional judges for fluency where feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9683.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9683.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveIdeaKeywords</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveIdeaBench dynamic keyword dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuously updated set of 1,180 high-impact scientific keywords spanning 22 domains, drawn from a real-time analytics source and refreshed monthly to reflect emerging trends.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Used to prompt all idea-generating models in LiveIdeaBench</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Keywords are domain-agnostic triggers; models generate multiple concise ideas per keyword under the 100-word constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multidisciplinary (22 scientific categories; categories assigned via SciBERT semantic similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Monthly refreshed keyword set used as stimuli for single-keyword prompts; domain labels assigned via SciBERT for per-discipline analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Keywords chosen for high search engagement; used to measure cross-domain ideation breadth, fluency, and flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Dataset available at the project's Huggingface dataset (liveideabench-v2) and tied to the LiveIdeaBench dynamic leaderboard.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Provides diverse, up-to-date stimuli enabling evaluation of models across contemporary scientific topics; enabled per-discipline heatmaps and domain-specific performance analysis (e.g., chemistry, medicine, physics differences across models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Bias towards trending/search-engaged topics; monthly refresh improves topicality but complicates exact reproducibility across time; keyword selection may underrepresent very niche technical subtopics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Unlike static curated test sets, dynamic keywords better reflect contemporary scientific discourse but differ from human-curated benchmark sets that prioritize canonical challenge problems.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report keyword snapshots (time-stamped) used in any evaluation run for reproducibility; supplement dynamic keywords with fixed canonical subsets for longitudinal comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9683.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9683.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KeyQuantFindings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key quantitative findings & diagnostics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of principal quantitative evaluation outcomes reported: correlations, trade-offs, model-specific highlights, human-LLM validation, and operational metrics (including carbon footprint).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>41 evaluated models (idea generators) and a top-10 judge panel</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Includes proprietary and open models such as claude-3.7 variants, gemini-2.0 variants, qwq-32b-preview, deepseek-r1, mistral variants, GPT-family variants, and others (listed in Methods).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-disciplinary analysis across 22 scientific categories</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregate per-dimension scoring via LLM judges, Pareto front analysis between originality and feasibility, correlation analyses with LiveBench general intelligence scores and with idea length, and human validation on a PDE subset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality, feasibility, clarity, fluency, flexibility; additional diagnostics included rank comparisons and Pareto visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>LiveIdeaBench runs: >40 models × 1,180 keywords; Supplementary Tables S.2–S.4 contain raw scores, aggregated metrics, and per-model emissions breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Notable quantitative results: general-intelligence vs ideation correlation r=0.357 (p=0.038, N=41), idea-length vs quality r=0.096 (p<0.0001). Human expert validation (PDE domain) correlated with LLM judges for originality at r ≈ 0.82. Pareto front shows clear originality–feasibility trade-offs; specific model outcomes: claude-3.7-sonnet:thinking—high originality; nova-pro-v1—high feasibility; qwq-32b-preview—high ideation despite low LiveBench general-intelligence ranking; deepseek-r1—balanced high performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Statistical associations are correlational; panel dynamics and proprietary model updates limit strict reproducibility of exact numeric values; domain-specific human validation limited (PDE only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated scores largely consistent with human expert checks in one domain (PDE) but broader human validation is recommended; automated approach enables scale not feasible with human panels.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report correlations, confidence intervals, and sample sizes; pair automated judging with domain-specific human validation; present Pareto analyses to reveal trade-offs between originality and feasibility; report environmental cost estimates for large-scale evaluations (here ≈3074 kgCO2e for full run).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9683",
    "paper_id": "paper-72faa912691bc2d8dc342fc1c9b53cccc9ba56fc",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "LiveIdeaBench",
            "name_full": "LiveIdeaBench benchmark for scientific idea generation",
            "brief_description": "A dynamic benchmark that evaluates LLMs' divergent thinking for scientific idea generation from single-keyword prompts across five dimensions (originality, feasibility, clarity, fluency, flexibility) using a multi-model judge panel and a dataset of 1,180 keywords spanning 22 scientific domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "41 state-of-the-art LLMs (dynamic roster drawn from LiveBench)",
            "llm_description": "A heterogeneous set of 41 models (proprietary and open-weight) selected monthly from LiveBench top performers (examples: claude-3.7-sonnet:thinking, gemini-2.0 variants, qwq-32b-preview, deepseek-r1, etc.). Models span multiple architectures and parameter scales and are both idea generators and (top-10) judges in different roles.",
            "scientific_domain": "Multidisciplinary (22 scientific disciplines; cross-domain ideation)",
            "evaluation_method": "Automated LLM-as-a-judge ensemble with randomized sampling: idea-generation from single-keyword prompts (100-word target), three randomly sampled judges per idea for originality/feasibility/clarity, one randomly sampled judge per keyword-model pair for fluency; judge panel is the top-10 LiveBench models subject to diversity constraints and exclusion of the evaluated model.",
            "evaluation_criteria": "Five dimensions derived from Guilford: originality (novelty), feasibility (technical/practical implementability), clarity (coherent articulation), fluency (diversity/number of distinct ideas per keyword), flexibility (30th percentile of composite scores across keywords).",
            "benchmark_or_dataset": "Dynamic keyword set of 1,180 high-impact scientific keywords (updated monthly from a real-time analytics source) covering 22 disciplines; model roster is refreshed monthly from LiveBench.",
            "results_summary": "Across 41 models and 1,180 keywords, LiveIdeaBench reveals that scientific idea-generation (divergent thinking) is poorly predicted by general intelligence. Key findings: weak-but-significant correlation between LiveIdeaBench and LiveBench general intelligence (r=0.357, p=0.038, N=41); very weak positive correlation between idea length and quality (r=0.096, p&lt;0.0001). Pareto analysis shows trade-off between originality and feasibility (e.g., claude-3.7-sonnet:thinking highest originality; nova-pro-v1 high feasibility; deepseek-r1 and qwq-32b show balanced profiles). Dynamic judge ensemble and sampling used to produce averaged per-dimension and composite scores.",
            "limitations_or_challenges": "Temporal comparability issues as judge panel and model APIs update; potential judge-model biases (sycophancy or misjudgment of novel concepts outside judges' knowledge); safety refusals that reduce creativity scores for safety-conservative models; dynamic benchmark complicates strict reproducibility; LLM-as-judge may mis-evaluate highly novel/out-of-distribution scientific ideas.",
            "comparison_to_human_or_traditional": "LLM-based judging validated in a domain-specific human expert check (PDE domain) showing strong alignment for originality (r ≈ 0.82), indicating reasonable concordance with human experts in that domain; overall approach trades human-cost for scale and consistency but may miss nuanced expert judgments.",
            "recommendations_or_best_practices": "Use dynamic multi-model judge ensembles with random sampling and exclusion of the evaluated model; impose judge diversity constraints (≤20% per organization); standardize response length (100-word target, max 200 words); apply fallback prompts for refusals; combine automated LLM-judging with targeted human validation in specialized domains; consider RAG augmentation for judges to reduce knowledge-bound misjudgments.",
            "uuid": "e9683.0",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-judge evaluation methodology (ensemble & sampling)",
            "brief_description": "Automated evaluation approach using a panel of top LLMs as critics to score generated ideas across multiple creativity dimensions, employing randomized sampling and ensemble averaging to mitigate individual model bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Top-10 LiveBench models (sampled judges)",
            "llm_description": "Judge pool comprises the top 10 LiveBench models at evaluation time, subject to diversity constraints (max two models per organization) and exclusion rules so that the evaluated model is never in its own judging set.",
            "scientific_domain": "General scientific ideation across domains (applied uniformly to all 22 categories)",
            "evaluation_method": "Randomized sampling from the judge panel: three judges per idea for originality/feasibility/clarity, one judge per keyword-model for fluency; each judge returns text analysis plus numeric 1–10 scores (via prescribed critic prompt); final scores are averages across sampled judges.",
            "evaluation_criteria": "Originality, feasibility, clarity, fluency (graded categorical-&gt;numeric mapping), flexibility (derived metric); judges follow a strict system prompt to act as demanding scientific reviewers and output JSON scores.",
            "benchmark_or_dataset": "Used within LiveIdeaBench across 1,180 keywords and 41 generator models.",
            "results_summary": "LLM-as-judge permits scalable, consistent scoring with ensemble averaging to reduce single-model bias; produced per-dimension scores used to rank and profile models. Human validation in PDE shows strong agreement for originality (r≈0.82), supporting feasibility of automated judging in at least some domains.",
            "limitations_or_challenges": "Judge models can exhibit sycophancy or alignment-induced bias, may lack up-to-date knowledge or misunderstand novel concepts, and judge-panel composition changes over time (affecting longitudinal comparability). Single-judge fluency assessment may be noisy.",
            "comparison_to_human_or_traditional": "Offers large-scale, lower-cost alternative to human panels; empirical alignment with human experts observed in PDE for originality, but LLM judges may still fail on domain-nuanced or highly technical feasibility judgments compared to domain experts.",
            "recommendations_or_best_practices": "Use multi-judge ensembles, random sampling, and strict judge exclusion rules; complement with targeted human expert validation especially in niche domains; consider RAG for judges to ground evaluations in literature.",
            "uuid": "e9683.1",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "FluencyMapping",
            "name_full": "Fluency grading: categorical (A/B/C/D) → numeric mapping",
            "brief_description": "A fluency evaluation method that grades the distinctiveness of ideas generated for the same keyword using an LLM judge, mapping qualitative grades to a 1–10 numeric scale to measure idea diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Single randomly sampled judge from judge panel (per keyword-model pair)",
            "llm_description": "One judge evaluates the set of ideas produced by a single model for a given keyword and assigns a grade: D (identical), C (similar), B (different but similar problem), A (completely different problems).",
            "scientific_domain": "Applies across all scientific domains in the LiveIdeaBench keyword set",
            "evaluation_method": "LLM judge compares multiple ideas generated from the same keyword and assigns a categorical grade; categories are mapped linearly to integers: D→1, C→4, B→7, A→10, producing a fluency score on the 1–10 scale.",
            "evaluation_criteria": "Idea distinctiveness and substantive diversity (not just surface lexical differences).",
            "benchmark_or_dataset": "Applied to each keyword across models in LiveIdeaBench (1,180 keywords).",
            "results_summary": "Fluency scores are used in multidimensional model profiles and in the Pareto visualization (fluency represented by bubble size); top performers (e.g., claude and Gemini variants) show strong fluency alongside originality in many domains.",
            "limitations_or_challenges": "Single-judge assessment per keyword-model pair can introduce noise; mapping of four qualitative categories to 1–10 is coarse and may miss finer gradations; depends on judge's semantic discrimination capabilities.",
            "comparison_to_human_or_traditional": "LLM judgments for fluency are more semantically informed than simple lexical-diversity metrics but may differ from human panels that can apply more nuanced categorical distinctions.",
            "recommendations_or_best_practices": "Prefer LLM-based semantic fluency judgments over purely syntactic diversity measures for scientific ideation; consider multiple judges per fluency assessment if resources allow to reduce single-judge noise.",
            "uuid": "e9683.2",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Flexibility30P",
            "name_full": "Flexibility measured as 30th-percentile composite score",
            "brief_description": "Operationalization of flexibility as a conservative cross-domain generalization metric computed as the 30th percentile of the distribution of a model's averaged scores across originality, feasibility, clarity, and fluency for different keywords.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "All evaluated generator models (used to compute distributions)",
            "llm_description": "Scores from each model across keywords are used to compute the percentile-based flexibility metric.",
            "scientific_domain": "Cross-domain (assesses generalization across the 22 scientific categories)",
            "evaluation_method": "Compute per-keyword composite = mean(originality, feasibility, clarity, fluency); flexibility = 30th percentile of these composites over the keyword set for a model.",
            "evaluation_criteria": "Considers performance floor rather than mean or max; intended to detect models that maintain reliable performance across diverse contexts.",
            "benchmark_or_dataset": "Computed over LiveIdeaBench's 1,180-keyword dataset.",
            "results_summary": "Flexibility identifies models that generalize rather than excel only on niche keywords; used in multidimensional profiling and contributes to overall composite rankings.",
            "limitations_or_challenges": "Choice of the 30th percentile is heuristic and influences sensitivity; may under-emphasize rare but important strengths; percentile-based metric can be unstable for small keyword subsets.",
            "comparison_to_human_or_traditional": "Unlike human assessments that might judge cross-domain adaptability qualitatively, this provides a reproducible numeric floor metric but requires careful interpretation.",
            "recommendations_or_best_practices": "Use percentile-based flexibility to capture conservative generalization capability; report alongside mean/variance to provide full performance picture; consider alternate percentiles in sensitivity analyses.",
            "uuid": "e9683.3",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SingleKeywordPrompting",
            "name_full": "Single-keyword minimal-context prompting",
            "brief_description": "A stimulus design where models receive only one keyword as prompt (with a 100-word idea constraint) to evoke divergent scientific ideation from internal model representations, mirroring classic divergent thinking tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Idea-generating models from LiveIdeaBench (41 models)",
            "llm_description": "Models are prompted to produce concise (≤100 words target, 200 words max) scientific ideas given only a single assigned keyword; a fallback prompt is used for safety-related refusals.",
            "scientific_domain": "Designed to elicit ideation across all included scientific domains (22 categories).",
            "evaluation_method": "Single-keyword stimulus followed by automatic evaluation (LLM judges) across the five dimensions; enforces a tight word limit to emphasize concise ideation.",
            "evaluation_criteria": "Measures capacity for divergent thinking: fluency (multiple distinct ideas per keyword), originality, feasibility, clarity, and cross-keyword flexibility.",
            "benchmark_or_dataset": "Applied to LiveIdeaBench's keyword set of 1,180 items.",
            "results_summary": "Single-keyword prompting exposes divergence between models' convergent problem-solving ability and raw ideation capacity; produced cases where lower-general-intelligence models (e.g., qwq-32b-preview) matched or exceeded higher-general-intelligence models in ideation.",
            "limitations_or_challenges": "Minimal context limits depth and domain-specific technicality of ideas and may disadvantage models relying on retrieval/long-context capabilities; may conflate internal memorization with genuine combinatorial ideation.",
            "comparison_to_human_or_traditional": "Analogous to human divergent thinking tasks (Guilford-style), intentionally probes ideation rather than synthesis; complements traditional benchmarks that use richer context.",
            "recommendations_or_best_practices": "Use single-keyword prompts to specifically probe ideation capability; combine with richer-context tasks for full pipeline evaluation; apply fallback prompts for safety refusals and explicitly cap response length for comparability.",
            "uuid": "e9683.4",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "JudgePanelProtocol",
            "name_full": "Judge panel formation & evaluation protocol",
            "brief_description": "Procedural rules governing judge panel composition, sampling, and evaluation to reduce bias and ensure independence during LiveIdeaBench assessments (diversity constraints, exclusion of evaluated model, sampling sizes, refusal handling, and response standardization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Judge panel: top-10 LiveBench models (dynamically selected)",
            "llm_description": "Panel drawn monthly from LiveBench leaderboard with constraints: max two models per organization, avoid duplicate base-model variants, and exclude the target model when evaluating its outputs.",
            "scientific_domain": "Used for cross-domain evaluation across all LiveIdeaBench categories.",
            "evaluation_method": "When evaluating an idea: sample 3 judges (without replacement) from eligible panel for originality/feasibility/clarity; sample 1 judge for fluency comparisons; apply the critic system prompt; average scores across judges; exclude overly long responses (&gt;200 words); use fallback prompt on refusals; special parsing for models with reasoning artifacts.",
            "evaluation_criteria": "Adheres to prescribed judge counts (3 for core dimensions, 1 for fluency), strict prompt templates, and numeric 1–10 scoring.",
            "benchmark_or_dataset": "Operational protocol for LiveIdeaBench's experimental runs over 1,180 keywords and 41 models.",
            "results_summary": "Protocol produced robust ensemble scores and reduced single-model bias; enabled fair comparisons across heterogeneous models and enforced independence to avoid self-evaluation artifacts.",
            "limitations_or_challenges": "Panel composition changes over time causing longitudinal comparability issues; diversity constraints and exclusions may alter which models are available as judges at each run; single-judge fluency scoring is a potential weak point.",
            "comparison_to_human_or_traditional": "Formalized sampling and exclusion rules provide reproducibility advantages over ad-hoc human panels; however human panels offer richer domain judgement which LLM judges may not fully match.",
            "recommendations_or_best_practices": "Maintain and report panel composition and sampling seeds for reproducibility; cap per-organization representation; exclude evaluated models from judging rounds; document fallback and refusal handling procedures; consider additional judges for fluency where feasible.",
            "uuid": "e9683.5",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LiveIdeaKeywords",
            "name_full": "LiveIdeaBench dynamic keyword dataset",
            "brief_description": "A continuously updated set of 1,180 high-impact scientific keywords spanning 22 domains, drawn from a real-time analytics source and refreshed monthly to reflect emerging trends.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Used to prompt all idea-generating models in LiveIdeaBench",
            "llm_description": "Keywords are domain-agnostic triggers; models generate multiple concise ideas per keyword under the 100-word constraint.",
            "scientific_domain": "Multidisciplinary (22 scientific categories; categories assigned via SciBERT semantic similarity).",
            "evaluation_method": "Monthly refreshed keyword set used as stimuli for single-keyword prompts; domain labels assigned via SciBERT for per-discipline analysis.",
            "evaluation_criteria": "Keywords chosen for high search engagement; used to measure cross-domain ideation breadth, fluency, and flexibility.",
            "benchmark_or_dataset": "Dataset available at the project's Huggingface dataset (liveideabench-v2) and tied to the LiveIdeaBench dynamic leaderboard.",
            "results_summary": "Provides diverse, up-to-date stimuli enabling evaluation of models across contemporary scientific topics; enabled per-discipline heatmaps and domain-specific performance analysis (e.g., chemistry, medicine, physics differences across models).",
            "limitations_or_challenges": "Bias towards trending/search-engaged topics; monthly refresh improves topicality but complicates exact reproducibility across time; keyword selection may underrepresent very niche technical subtopics.",
            "comparison_to_human_or_traditional": "Unlike static curated test sets, dynamic keywords better reflect contemporary scientific discourse but differ from human-curated benchmark sets that prioritize canonical challenge problems.",
            "recommendations_or_best_practices": "Report keyword snapshots (time-stamped) used in any evaluation run for reproducibility; supplement dynamic keywords with fixed canonical subsets for longitudinal comparisons.",
            "uuid": "e9683.6",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "KeyQuantFindings",
            "name_full": "Key quantitative findings & diagnostics",
            "brief_description": "Summary of principal quantitative evaluation outcomes reported: correlations, trade-offs, model-specific highlights, human-LLM validation, and operational metrics (including carbon footprint).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "41 evaluated models (idea generators) and a top-10 judge panel",
            "llm_description": "Includes proprietary and open models such as claude-3.7 variants, gemini-2.0 variants, qwq-32b-preview, deepseek-r1, mistral variants, GPT-family variants, and others (listed in Methods).",
            "scientific_domain": "Cross-disciplinary analysis across 22 scientific categories",
            "evaluation_method": "Aggregate per-dimension scoring via LLM judges, Pareto front analysis between originality and feasibility, correlation analyses with LiveBench general intelligence scores and with idea length, and human validation on a PDE subset.",
            "evaluation_criteria": "Originality, feasibility, clarity, fluency, flexibility; additional diagnostics included rank comparisons and Pareto visualizations.",
            "benchmark_or_dataset": "LiveIdeaBench runs: &gt;40 models × 1,180 keywords; Supplementary Tables S.2–S.4 contain raw scores, aggregated metrics, and per-model emissions breakdowns.",
            "results_summary": "Notable quantitative results: general-intelligence vs ideation correlation r=0.357 (p=0.038, N=41), idea-length vs quality r=0.096 (p&lt;0.0001). Human expert validation (PDE domain) correlated with LLM judges for originality at r ≈ 0.82. Pareto front shows clear originality–feasibility trade-offs; specific model outcomes: claude-3.7-sonnet:thinking—high originality; nova-pro-v1—high feasibility; qwq-32b-preview—high ideation despite low LiveBench general-intelligence ranking; deepseek-r1—balanced high performance.",
            "limitations_or_challenges": "Statistical associations are correlational; panel dynamics and proprietary model updates limit strict reproducibility of exact numeric values; domain-specific human validation limited (PDE only).",
            "comparison_to_human_or_traditional": "Automated scores largely consistent with human expert checks in one domain (PDE) but broader human validation is recommended; automated approach enables scale not feasible with human panels.",
            "recommendations_or_best_practices": "Report correlations, confidence intervals, and sample sizes; pair automated judging with domain-specific human validation; present Pareto analyses to reveal trade-offs between originality and feasibility; report environmental cost estimates for large-scale evaluations (here ≈3074 kgCO2e for full run).",
            "uuid": "e9683.7",
            "source_info": {
                "paper_title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01663025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LiveIdeaBench: Evaluating LLMs’ Divergent Thinking for Scientific Idea Generation with Minimal Context</h1>
<p>Kai Ruan ${ }^{1}$, Xuan Wang ${ }^{2}$, Jixiang Hong ${ }^{1}$, Peng Wang ${ }^{3}$, Yang Liu ${ }^{4,5}$, Hao Sun ${ }^{1, <em>}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China<br>${ }^{2}$ ZJU-UIUC Institute, Zhejiang University, Haining, China<br>${ }^{3}$ Bank of China, Beijing, China<br>${ }^{4}$ School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China<br>${ }^{5}$ State Key Laboratory of Nonlinear Mechanics, Institute of Mechanics, Chinese Academy of Sciences, Beijing, China<br></em>Corresponding author</p>
<h4>Abstract</h4>
<p>While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs’ scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.</p>
<h2>Introduction</h2>
<p>The advancement of scientific knowledge relies heavily on creative thinking and the generation of novel hypotheses. The ability to envision new possibilities and formulate testable explanations is crucial for scientific progress. In recent years, LLMs have demonstrated remarkable capabilities in various scientific tasks, from literature analysis to experimental design, suggesting their potential as powerful tools for augmenting scientific discovery [1-3]. Concurrently, machine learning techniques are being applied to forecast future research directions by analyzing the structure and evolution of scientific knowledge networks [4]. As Rafner et al. [5] note, these generative AI systems now</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Overall design of the LiveIdeaBench benchmark. a. Over 1,000 scientific keywords, representing diverse domains, are used in prompts for the Idea LLMs, encouraging divergent thinking and the generation of novel scientific ideas. b. Sampled Judge LLMs evaluate the generated ideas across three primary dimensions: originality, feasibility and clarity, assigning numerical scores to each idea. c. The evaluation panel comprises the top 10 state-of-the-art models selected from LiveBench, ensuring robust assessment through sampling and ensemble scoring. d. Fluency scores are derived by analyzing the diversity and substantive differences among ideas generated from the same keyword (using a randomly sampled judge), while originality, feasibility, and clarity metrics are combined for integrated evaluation. e. Following Guilford's creativity theory, the evaluation methodology assesses five critical dimensions: originality, feasibility, clarity, fluency, and flexibility, with flexibility computed as the 30th percentile of the averaged scores across the other four dimensions. f. The LiveIdeaBench benchmark provides a comprehensive dataset of generated ideas, evaluation metrics, and a dynamic leaderboard tracking the performance of over 40 models, available at https://huggingface.co/datasets/6cf/liveideabench-v2 and https://liveideabench.com/.</p>
<p>perform comparably to humans on some creativity tests and could potentially enhance the creative capabilities of knowledge workers worldwide.</p>
<p>The landscape of artificial intelligence has undergone dramatic transformation since the emergence of ChatGPT in late 2022, with LLMs demonstrating increasingly sophisticated capabilities in scientific contexts. As we approach late 2024, these models have achieved superhuman performance across multiple dimensions - from language comprehension to mathematical reasoning and code generation. The recent success of AlphaGeometry in solving complex geometric problems at IMO gold medalist level [6] exemplifies this remarkable trajectory. Yet, these advances in general intelligence prompt a fundamental question: does the potential for scientific idea generation in LLMs grow in tandem with their analytical capabilities, particularly in the realm of scientific discovery?</p>
<p>This question becomes particularly pertinent when evaluating LLMs' potential contributions to scientific innovation. Current evaluation benchmarks for LLMs in scientific contexts predominantly</p>
<p>rely on rich contextual inputs, such as research paper titles, abstracts, or complete articles. While these approaches effectively assess models' ability to comprehend and synthesize existing knowledge, they are not designed to systematically evaluate a crucial aspect of scientific thinking - the capacity to generate novel ideas from limited information. This limitation is particularly significant, as many groundbreaking scientific discoveries originate from unexpected connections and conceptual leaps. Shi et al. [2] found that the most surprising scientific breakthroughs often emerge when scientists from one disciplinary background present problems to audiences with different perspectives, suggesting value in approaches that can generate diverse conceptual connections.</p>
<p>The need for a specialized benchmark to assess idea generation from minimal context is further underscored by research showing the distinct nature of creative ideation processes. Recent studies in cognitive psychology have questioned earlier assumptions about the relationship between general cognitive abilities and creative performance. While traditional views suggested a threshold relationship between intelligence and creativity [7], meta-analyses by Kim [8] found only a small relationship between these constructs, and more recent work by Weiss et al. [9] concluded that the relationship is likely linear with no evidence supporting a threshold. Given the complex and debated relationship between general intelligence and creative potential in humans, where they appear distinct yet interconnected, it is crucial to investigate whether analogous distinctions exist in LLMs. Evaluating divergent thinking separately allows us to probe if enhancing idea generation capabilities in these systems requires different approaches.</p>
<p>To address this gap, we propose LiveIdeaBench (Fig. 1), a novel evaluation benchmark designed to assess LLMs' capabilities in divergent thinking for scientific idea generation under constrained conditions. Unlike existing benchmarks that predominantly evaluate convergent thinking by requiring models to derive single correct solutions from rich contextual information, LiveIdeaBench is explicitly grounded in Guilford's seminal theory of divergent production [10]. Our methodology operationalizes key principles from this framework: we use single-keyword prompts as minimal context. This constraint encourages models to generate connections and concepts primarily from their internal knowledge representations, rather than synthesizing readily available information from detailed prompts (e.g., abstracts, or complete articles), thus probing their capacity for less scaffolded, more internally-driven ideation relevant to initial brainstorming. This approach is analogous to the broad stimuli used in classic divergent thinking tasks (e.g., the "Utility Test" described by Guilford), to elicit the generation of multiple and varied potential ideas, fostering a "ready flow of ideas" rather than convergence towards a predefined outcome. This approach necessitates that models "produce their own answers," a hallmark of divergent production assessment distinct from selection-based or highly constrained convergent thinking tasks. Furthermore, our evaluation directly measures core divergent production dimensions like fluency (quantity and diversity of ideas, reflecting the capacity to generate a "number of varied responses") and originality (novelty of ideas), as conceptualized by Guilford. While acknowledging that comprehensive scientific creativity involves an interplay of both divergent and convergent processes, LiveIdeaBench focuses on and evaluates the foundational, generative phase: the ability to produce diverse scientific concepts from sparse cues. This capacity is critical for sparking innovation but is often overlooked by conventional benchmarks. Thus, our focus is on assessing this specific aspect of creative potential relevant to scientific discovery, rather than evaluating the entire scientific process, which also involves deep domain expertise and subsequent validation.</p>
<p>We posit that such benchmarking serves as a crucial initial step towards developing effective human-AI hybrid intelligence systems [11-13] for scientific discovery. By systematically assessing LLMs' performance on idea generation tasks, we aim to provide insights into how these models can best function as collaborative tools or creative sparks for human researchers within the scientific process.</p>
<p>Our work makes several key contributions to the field:</p>
<ol>
<li>We introduce a systematic benchmark for evaluating scientific idea generation in LLMs under minimal contextual conditions, framing this evaluation as a foundational step for understanding their potential role in human-AI scientific discovery workflows.</li>
<li>We conduct comprehensive evaluations across 41 state-of-the-art models spanning various architectures and parameter scales, using a diverse set of over 1,000 scientific keywords from 22 distinct scientific domains;</li>
<li>We demonstrate that an LLM's general intelligence metrics do not necessarily correlate with its capacity for idea generation in scientific contexts. This highlights the need for specialized evaluation benchmarks to understand how to best leverage LLMs to complement human capabilities within AI-assisted scientific innovation.</li>
</ol>
<p>Through extensive experimentation, we reveal significant insights into the relationship between models' general intelligence and their idea generation patterns. Our findings suggest that while LLMs have achieved remarkable benchmarks in general intelligence tasks, their performance on idea generation tasks in scientific contexts requires distinct evaluation methodologies. Some models with lower performance on general intelligence benchmarks demonstrate surprisingly strong idea generation capabilities, while others show the opposite pattern. This specialized assessment complements existing benchmarks and provides new insights into how current LLM systems might effectively support human researchers in scientific innovation, for instance, by acting as idea generators or brainstorming copilots.</p>
<h1>Related Works</h1>
<h2>Human Creativity Research</h2>
<p>Early theoretical work on human creativity offers valuable insights for our investigation. In their seminal 1962 work, Getzels and Jackson [14] examined the relationship between intelligence and creativity, particularly among "gifted" students. Their study yielded two crucial findings: first, high IQ does not necessarily equate to high creativity; and second, creativity and intelligence function as relatively independent traits. These observations led to the threshold theory [10, 14], proposing that intelligence is necessary but not sufficient for creativity, with its influence diminishing beyond a certain threshold.</p>
<p>However, subsequent research has yielded mixed evidence for this theory. While Gralewski et al. [15] and Jauk et al. [7] found some empirical support under certain conditions, some studies found no support $[8,16]$, challenging the threshold concept. More recent work by Weiss et al. [9] concluded that the relationship is likely linear with no evidence supporting a threshold. These varying findings suggest that the development of idea generation capabilities might be underpinned by different cognitive mechanisms than general intelligence, a pattern potentially relevant to LLM development as well. It's also noteworthy that the threshold debate has nuances regarding potential versus achievement; some research suggests that while a threshold might (or might not) apply to divergent thinking ability, intelligence may remain related $[7,17]$ or even become more important for translating potential into real-world creative achievements across the entire range [18, 19]. These early studies primarily focused on the relationship between general cognitive abilities and creative potential, often measured via divergent thinking tasks. While much of this debate centers on general creativity, the specific relationship between intelligence and scientific creativity, which heavily</p>
<p>relies on deep domain knowledge and rigorous methodology alongside ideation, remains an area requiring nuanced investigation, though the general finding of creativity and intelligence being distinct capabilities provides a useful starting point for examining LLMs.</p>
<p>Another theoretical strand addresses creativity assessment methodologies. Guilford's influential theoretical framework $[10,20]$ introduced the distinction between divergent thinking (generating varied responses to open-ended prompts) and convergent thinking (finding optimal solutions to well-defined problems). Guilford identified four key aspects of divergent thinking: the ability to generate a large number of ideas (fluency), the capacity to think across different categories (flexibility), the generation of novel ideas (originality), and the development of detailed and refined ideas (elaboration). More recent research by Cortes et al. [21] has noted that traditional creativity assessments typically involve elements of both divergent and convergent thinking rather than isolating either process. Their findings indicate that purportedly divergent and convergent tasks show limited correlation, questioning whether they reflect different components of the same higher-level construct (creativity). This work highlights the complexity of measuring creative processes and informs our approach to evaluating idea generation in LLMs.</p>
<p>Broader theoretical frameworks further enrich this picture. For instance, Margaret Boden [22] distinguishes between psychological creativity (P-creativity, novel to the individual) and historical creativity (H-creativity, novel to humanity), while Teresa Amabile's [23] componential theory highlights the interplay of domain-relevant skills, creativity-relevant processes (including divergent thinking), and intrinsic task motivation. Melvin Rhodes' [24] "4 P's" (Person, Process, Press, Product) provide a holistic view, reminding us that creativity involves the individual creator, the cognitive processes, the environmental influences, and the resulting outcome. While LiveIdeaBench primarily assesses aspects of the creative "process" (specifically idea generation fluency and diversity under minimal constraints), understanding these broader theories helps contextualize its scope and limitations within the larger landscape of scientific creativity, which ultimately demands valuable and impactful products.</p>
<p>These theoretical frameworks from human creativity research have informed modern approaches to evaluating LLMs' idea generation capabilities and guide our assessment methodology in LiveIdeaBench. Contemporary creativity research has expanded assessment methodologies beyond traditional paper-based tests, with Rafner et al. [25] developing CREA, a comprehensive game-based tool for measuring divergent and convergent thinking. Their validation studies across diverse populations have demonstrated the effectiveness of these novel assessment approaches. Benedek's [19] recent work clarifies that creative potential (measured through divergent thinking tasks) explains only limited variance in creative achievement, emphasizing complex interactions between potential, behavior, and environmental factors.</p>
<h1>Evaluating Idea Generation in LLMs</h1>
<p>Recent research has systematically explored LLMs' potential for tasks analogous to human creative processes through various methodological approaches. Rafner et al. [5] provided a critical analysis of creativity in the age of generative AI, noting that while these systems now perform comparably to humans on some creativity tests, important qualitative differences remain. Their analysis emphasized the importance of integrating psychological science with computational approaches to develop more effective creativity support tools, highlighting both the promising capabilities and important limitations of current systems.</p>
<p>Empirical assessments in creative contexts have shown promising results, exploring LLMs' potential through various applications. For instance, Meincke et al. [26] focused on product ideation, comparing the quality and novelty of ideas generated by GPT-4 against those generated by humans.</p>
<p>Comprehensive human evaluation studies [27] have compared LLM-generated research proposals with those from NLP researchers, revealing statistically significant advantages in novelty while maintaining comparable feasibility metrics. Complementing these approaches, Lee et al. [28] conducted an empirical study investigating the impact of using ChatGPT on human creative performance, finding that it significantly enhanced the articulation and creativity of human-generated ideas.</p>
<p>Studies directly examining divergent thinking abilities in LLMs have employed various methodological approaches. Cropley [29] analyzed ChatGPT on the Divergent Association Task and found that while GPT-4 demonstrated strong semantic distance capabilities in generating unrelated words, its inconsistency and predictability highlighted important differences from human creative processes. This research emphasized the distinction between divergent thinking abilities and true creativity, suggesting the importance of understanding both the capabilities and limitations of these systems. Marrone, Cropley, and Medeiros [30] further examined how narrow AI impacts human creativity, identifying both supportive functions and important limitations in creative processes.</p>
<p>Extensive empirical evidence from a study involving 100,000 participants [31] has demonstrated LLMs' competitive performance, sometimes exceeding human levels, on certain general creative tasks like the Divergent Association Task (DAT) and creative composition, validating the assessment of such capabilities but not addressing the specific demands of scientific ideation. However, Wenger and Kenett [32] found that while LLMs performed on par with humans in individual creativity tests, they exhibited significantly lower response diversity at the group level, showing high homogeneity in creative outputs even across different model families. This creative homogeneity represents an important limitation in current systems. Confirming this potential downside, Doshi et al. [33], in an experimental study on story writing, found that while access to generative AI ideas caused stories to be evaluated as more creative individually (especially for less creative writers), these AI-enabled stories were significantly more similar to each other than stories produced by humans alone, pointing to a potential reduction in collective novelty. These findings collectively underscore the importance of evaluating not just the quality but also the diversity and novelty of AI-generated creative content.</p>
<p>The assessment methodology for LLM creativity has evolved through several sophisticated frameworks designed for broad evaluation. For instance, Lu et al. [34] evaluated text-to-code creativity using programming challenges combined with prompting strategies designed to force novel solutions by restricting previously used techniques, though potential reliance on static problem sets raises limitations. Building upon foundational creativity theories [10], Zhao et al. [35] proposed a comprehensive, albeit static, framework adapting established psychological test tasks to assess general creativity across multiple dimensions. These frameworks offer valuable, comprehensive assessments for their respective areas (coding creativity, general creativity tasks), often incorporating both divergent and convergent aspects or multiple task types with richer contextual inputs. In contrast, LiveIdeaBench adopts a more focused approach, specifically designed to probe the initial divergent idea generation phase under conditions of minimal context, thus complementing these broader evaluations by foregrounding the assessment of the capacity for generating diverse possibilities from sparse cues relevant to scientific discovery. The minimal context inherent in our approach makes it particularly suited for probing creativity, relevant to sparking novel research directions.</p>
<p>Recent applications specifically for scientific ideation provide important context for our benchmark. Wang and colleagues' [36] SCIMON system leverages literature retrieval and iterative novelty enhancement to generate scientific ideas. Similarly, Gu and Krenn [37, 38] demonstrated approaches using knowledge graphs and LLMs to suggest research directions that might not emerge through traditional human ideation. These developments highlight the growing interest in LLMs' potential contribution to scientific idea generation and the need for specialized evaluation methods like LiveIdeaBench.</p>
<h1>Chain-of-Thoughts \&amp; Brainstorming of LLMs</h1>
<p>Chain-of-thought (CoT) prompting has emerged as a powerful technique for enhancing LLMs' reasoning capabilities. Following the initial introduction of CoT prompting [39], several advanced variants have been developed [40, 41], enabling models to explore multiple reasoning paths simultaneously. Recent investigations into programming applications [42] have demonstrated that incorporating brainstorming significantly improves LLMs' performance, achieving a more than $50 \%$ increase in solving competition-level problems.</p>
<p>A common thread among these approaches is their implicit reliance on divergent thinking - the ability to generate multiple distinct solutions or paths from a single starting point. While these methods have proven effective at enhancing model performance, they highlight a crucial gap in our understanding: we lack systematic ways to evaluate LLMs' fundamental capacity for divergent thinking. This gap becomes particularly significant as these prompting strategies increasingly depend on models' ability to generate and explore multiple possibilities.</p>
<p>Our work addresses this need through LiveIdeaBench, which specifically evaluates LLMs' divergent thinking capabilities in scientific contexts. By assessing models' ability to generate multiple novel ideas from minimal input, we provide a benchmark for understanding the foundational cognitive abilities that underpin the success of methods like CoT, Tree-of-Thoughts (ToT), and brainstorming. This evaluation becomes increasingly critical as these techniques continue to evolve and rely more heavily on models' creative thinking capabilities.</p>
<h2>Specialized Idea Generation Agents</h2>
<p>Recent years have seen significant advances in automated scientific systems aimed at aiding or accelerating discovery, each offering unique approaches. The AI Scientist framework [43] demonstrated potential for end-to-end research automation, while Nova [44] introduced iterative planning mechanisms for idea development. Systems like ResearchAgent [45] and Scideator [46] have further refined these approaches through knowledge graph integration and systematic recombination of research elements. Pushing the boundaries further, the recent AI co-scientist framework from Google [47] showcases a sophisticated multi-agent system designed to generate novel research hypotheses and detailed research proposals. The system employs specialized agents for generation, reflection, ranking, evolution, proximity analysis, and meta-review to iteratively refine scientific hypotheses through a self-improving cycle, with generated hypotheses later validated through real-world laboratory experiments conducted by human researchers.</p>
<p>In contrast to LiveIdeaBench, these specialized systems typically rely heavily on extensive knowledge bases, complex contextual inputs, or planning processes to generate and refine ideas or research plans. While highly effective for their intended applications, this dependence on rich context and complex architectures makes it challenging to isolate and evaluate their fundamental capabilities for initial idea generation from minimal cues, particularly their capacity for divergent thinking. Additionally, many such systems inherently focus on convergent processes - integrating information to find optimal solutions or designs within existing knowledge frameworks - rather than specifically probing the ability to explore truly novel conceptual combinations from sparse starting points. This limitation is particularly evident in systems like SciPIP [48] and IdeaSynth [49], whose structure encourages integrating existing information (existing papers or ideas) rather than generating diverse possibilities from minimal input.</p>
<p>Our work with LiveIdeaBench takes a fundamentally different approach by evaluating idea generation capabilities from minimal input, allowing us to assess models' raw potential for divergent scientific thinking independent of complex knowledge retrieval or multi-step planning abilities. This</p>
<p>distinction is crucial for understanding the foundational idea generation capabilities of LLMs and their potential role in sparking novel directions early in the scientific discovery process.</p>
<h1>LLMs-as-a-judge</h1>
<p>The challenges in evaluating creative output at scale necessitate automated assessment methods that can maintain human-level judgment quality.</p>
<p>The emergence of LLMs as evaluation tools has opened new possibilities for assessing model outputs at scale. Various evaluation approaches [50-55] have demonstrated the feasibility of using LLMs to evaluate other models' responses, offering advantages in terms of efficiency and cost-effectiveness compared to human evaluation [56]. Recent advances, such as the jury-based framework [57] and reference-guided verdict method [58], have shown promising results in reducing individual model biases and achieving high agreement with human judgments (Cohen's $\kappa$ up to 0.93 ). The development of specialized evaluation models like Prometheus 2 [59] further underscores the growing sophistication in this area.</p>
<p>However, existing LLM-based evaluation approaches face several limitations when applied to creative tasks. First, most approaches focus on evaluating responses against predetermined criteria or reference answers, making them better suited for convergent thinking tasks than divergent thinking assessment. Second, the evaluation of scientific creativity presents unique challenges that go beyond traditional metrics, requiring simultaneous assessment of originality, feasibility, clarity, fluency, and flexibility.</p>
<p>LiveIdeaBench addresses these limitations through a novel evaluation methodology specifically designed for scientific creativity. Our approach combines multiple LLMs in a specialized jury system that evaluates ideas across five key dimensions derived from Guilford's creativity theory: fluency, flexibility, originality, clarity and feasibility. By incorporating domain-specific scientific knowledge and employing a multi-model consensus mechanism, our benchmark aims to achieve more robust and nuanced evaluations of scientific ideas while maintaining the efficiency advantages of automated assessment.</p>
<h2>Results</h2>
<p>We assessed the performance of over 40 language models across multiple dimensions of scientific ideation using our comprehensive evaluation benchmark. A detailed description of the generated idea dataset is provided in Supplementary Note 2, and its constituent fields are listed in Supplementary Table S.1. The evaluation results are visualized in Fig. 2. For detailed outcomes, including raw scores and examples, please refer to Supplementary Table S.2; aggregated performance metrics and model rankings are presented in Supplementary Table S.3.</p>
<p>Diverse Model Performance Across Scientific Domains Our quantitative assessment and comparative analysis reveal distinct variations in model capabilities across scientific disciplines. (Fig. 3) While gemini-2.0-flash-thinking-exp, deepseek-r1, and claude-3.7-sonnet:thinking demonstrate high overall scores, the pattern of their relative strengths exhibits domain specificity. Importantly, although larger and more recent architectures generally achieve superior results, their advantage is not uniform across disciplines, suggesting that domain expertise and reasoning capabilities are not solely determined by model scale. For example, gemini-2.0-flash-thinkingexp excelled in anthropology ideation; claude-3.7-sonnet:thinking performed best in chemistry, medicine, and data science; while deepseek-r1 demonstrated stronger relative performance in physics.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Performance comparison of models evaluated on LiveIdeaBench. a. Dimensional scores (originality, feasibility, fluency, clarity, and flexibility) and overall performance (red line) for open-weight and proprietary models, with $95 \%$ confidence intervals. b. Multidimensional performance profiles of representative models across the five evaluation dimensions. c. Word cloud visualization of scientific keywords. For detailed scores and $95 \%$ CIs for each model, see Supplementary Table S.3.</p>
<p>Distinction between General Intelligence and Scientific Idea Generation The comparison between LiveIdeaBench and LiveBench metrics (see Fig. 4 and Supplementary Figure S.4) uncovers a notable disconnect between general intelligence and scientific idea generation capabilities, revealing contrasting results patterns across models. While Claude-3.7-sonnet:thinking leads in general intelligence (LiveBench) and also shows high effectiveness on scientific ideation (LiveIdeaBench), its idea generation capabilities are matched by models like qwq-32b-preview (ranked 8/41 in LiveIdeaBench). Notably, qwq-32b-preview exhibits low general intelligence scores, representing a profile of low general intelligence but high scientific ideation capability. As Fig. 4 illustrates, other patterns exist, such as high general intelligence paired with lower scientific ideation capability (e.g.,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Model Performance on LiveIdeaBench Across Scientific Categories. The heatmap displays average performance scores with $95 \%$ confidence intervals for model-discipline combinations. Scientific categories were classified using SciBERT [60] through semantic similarity computation following the framework from [61]. Higher scores (darker blue) indicate better idea generation ability within each discipline. Numbers in parentheses following each scientific category indicate the keyword count associated with that discipline. Categories are sorted by keyword count.
o3-mini-high). These contrasting profiles underscore that scientific idea generation capability, as measured by LiveIdeaBench, is poorly predicted by general intelligence, necessitating specialized benchmarks like LiveIdeaBench for evaluating this aspect of LLM potential.</p>
<p>Trade-offs in Scientific Originality and Feasibility The Pareto front visualization (see Fig. 5) illustrates clear trade-offs between feasibility and originality. While claude-3.7-sonnet:thinking achieves the highest originality with moderate feasibility, nova-pro-v1 demonstrates the opposite pattern. Models like deepseek-r1, qwq-32b, and gemini-2.0-flash-exp exhibit balanced effectiveness between these two dimensions. Particularly, deepseek-r1 stands out for its exceptional all-round capabilities across all measured dimensions of scientific idea generation, demonstrating that balanced effectiveness is achievable despite common tendencies toward specialization. Additionally, while models like claude-3.7-sonnet:thinking and several Gemini variants show strong fluency and flexibility, the mediocre results of o3-mini-high, known for logical reasoning, highlight the specific nature of the capabilities measured by this benchmark.</p>
<p>Independence of Idea Quality from Length As shown in Fig. 6, while the majority of models generally adhered well to the 100-word limit specified in the prompt (Fig. 6b), their mean originality, feasibility, and clarity scores demonstrated significant variations across different models</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Comparison of LiveIdeaBench (upper half, assessing scientific ideation) and LiveBench (lower half, assessing general intelligence, denoted as LB) across various evaluation metrics for the same set of models. The middle rows show contrasting trends between models in average performance. While Claude-3.7-sonnet:thinking achieves the highest average score on LiveBench, its performance on scientific ideation tasks (LiveIdeaBench) is comparable to qwq-32b-preview, which ranks fourth from last on general intelligence metrics. The arrows highlight three representative patterns: Claude-3.7-sonnet:thinking (left) exemplifies high general intelligence combined with high scientific ideation capability; o3-mini-high (middle) shows high general intelligence but low scientific ideation capability; and qwq-32b-preview (right) demonstrates low general intelligence but high scientific ideation capability. These contrasting patterns highlight that LLMs' scientific idea generation capability, as measured by LiveIdeaBench, is distinct from their general intelligence capabilities (e.g., reasoning, coding, and math), underscoring the necessity of LiveIdeaBench for evaluating scientific ideation potential.
(Fig. 6a,c-e ). Further analysis of idea length (see Supplementary Figs. S.1 and S.2) reveals a statistically significant but very weak positive correlation with idea quality ( $r=0.096, p&lt;0.0001$ ). Even for models specifically designed for reasoning, as detailed in Supplementary Note 8, the relationship between the length of their thoughts and the generated idea quality remains very weak. This observation lends further support to the notion that effective idea generation is not merely a function of extensive logical elaboration, distinguishing it from certain aspects of general reasoning.</p>
<h1>Discussion</h1>
<p>To the best of our knowledge, LiveIdeaBench represents the first comprehensive benchmark specifically designed to evaluate LLMs' divergent thinking capabilities in scientific innovation. Existing LLM benchmarks predominantly focus on problem-solving tasks such as logical reasoning, mathematical computation, and code generation. These benchmarks inherently assess convergent</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Pareto front visualization of model performance on LiveIdeaBench. This illustrates the trade-off between feasibility and originality across different language models, with bubble size, color gradient and edge width representing fluency, flexibility and clarity scores, respectively. The distribution reveals a clear Pareto frontier, where claude-3.7-sonnet:thinking achieves the highest originality but moderate feasibility, while nova-pro-v1 demonstrates the opposite pattern. Models such as deepseek-r1, qwq-32b, and gemini-2.0-flash-exp exhibit relatively balanced performance between these two dimensions. In terms of fluency and flexibility, claude-3.7-sonnet, claude-3.7-sonnet:thinking, gemini-2.0-flash-exp, gemini-2.0-pro-exp-02-05 and gemini-2.0-flash-thinking-exp show particularly strong performance. Notably, deepseek-r1 stands out for its exceptional all-round performance across all dimensions of scientific idea generation, suggesting strong performance across the dimensions measured by this benchmark. Moreover, while the o3-mini-high is renowned for its proficiency in logical and mathematical reasoning, it delivered a mediocre performance on this benchmark. For detailed scores and 95% CIs for each model, see Supplementary Table S.3.
thinking-the ability to arrive at predetermined correct answers through structured problem-solving (e.g., selecting the right option in multiple choice questions, completing text with expected words, or fixing code to match specific requirements). This stands in contrast to divergent thinking, which involves generating diverse, novel solutions from minimal contextual input.</p>
<p>Furthermore, our benchmark incorporates mechanisms to address potential data contamination and overfitting issues that commonly plague static benchmarks. Traditional evaluation methods may encourage models to perform well on specific test cases without developing generalizable creative</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Comprehensive Analysis of Idea Quality, Length, and Component Scores. a. Mean composite idea scores (average of Originality, Feasibility, and Clarity) with 95% confidence intervals for different language models. b. Mean idea length in words across models with specific word counts labeled. c-e. Detailed breakdown of performance on individual dimensions: Originality (c), Feasibility (d), and Clarity (e) scores with 95% confidence intervals. For the regression plot between clarity scores and originality, feasibility scores across all generated ideas, see Supplementary Figure S.5.</p>
<p>thinking abilities. Our approach employs a dynamic judge panel comprising multiple state-of-the-art models, randomly sampling multiple LLMs for evaluation and employing ensemble scoring methods. This design not only minimizes individual model biases but also leverages the continually updated knowledge bases of judge models, effectively preventing the limitations associated with fixed benchmarks. This methodology aligns with recent advances in live benchmarking [62, 63], which similarly address data contamination and overfitting concerns through dynamic evaluation mechanisms.</p>
<p>Our analysis through LiveIdeaBench yields several notable insights into LLMs' scientific idea generation capabilities. Most notably, we find that a model's performance on these divergent thinking tasks is not strongly coupled with its performance on general intelligence benchmarks.</p>
<p>For instance, qwq-32b-preview achieves comparable divergence thinking performance to top-tier models despite significantly lower scores on general intelligence benchmarks (confirming that high general intelligence is not a prerequisite for strong performance here). This statistically significant but weak positive correlation ( $r=0.357, p=0.038, N=41$; see Supplementary Figure S.4) suggests that fostering scientific idea generation capabilities in these systems may benefit from distinct development approaches compared to enhancing general problem-solving skills. The varying strengths we observe across different model architectures-particularly in originality versus feasibility trade-offs-point to potential complementarity in scientific applications.</p>
<p>Our findings in Fig. 4 illustrate the relationship between general intelligence and scientific idea generation capabilities. The comparison reveals three distinct performance profiles: models like claude-3.7-sonnet:thinking excelling in both areas; models like o3-mini-high showing high general intelligence but comparatively limited idea generation capability; and crucially, models like qwq-32b-preview exhibiting remarkable scientific divergent thinking capabilities despite relatively lower general intelligence scores. This latter pattern, in particular, highlights that the capabilities measured by LiveIdeaBench are distinct from, and poorly predicted by, general intelligence scores.</p>
<p>These findings indicate that LLMs' divergent thinking capabilities for scientific ideation under minimal context operate largely independently from the convergent thinking abilities typically measured by problem-solving tasks. This distinction could be influenced by several factors, potentially including variations in the relevance of pre-training data to scientific tasks, differences in posttraining methodologies applied, and inherent architectural properties of the models. This observed difference between metrics underscores the importance of specialized evaluation benchmarks like LiveIdeaBench that specifically target these idea generation capabilities, rather than attempting to infer them from general intelligence assessments.</p>
<p>We believe this phenomenon is closely tied to models' pretraining data and post-training procedures. From Fig. 4, we can also observe that mistral-small, which ranks near the bottom in general intelligence, demonstrates remarkably high scientific divergent thinking capabilities. Even more interestingly, the earlier released mistral-small model scored significantly higher in creativity than the newer same-size, same-family model mistral-small-24b-instruct-2501. We also observe that models from the same family—qwq-32b and qwq-32b-preview-score similarly in creativity while differing dramatically in general intelligence. These two pairs of examples strongly suggest that a model's divergent thinking and convergent thinking abilities cannot reliably predict each other.</p>
<p>This pattern holds true even when comparing models from the same family with different parameter counts. In Fig. 4, focusing on mistral-large-2411 and mistral-small, we see that mistral-small still slightly outperforms mistral-large-2411 in scientific divergent think-ing-despite the latter having 123 billion parameters, far exceeding the former's 24 billion. This further suggests that parameter count alone does not appear to be the primary determinant of scientific divergence capabilities, suggesting other factors like training data or architectural nuances play significant roles.</p>
<p>However, several limitations warrant consideration. The use of contemporary state-of-theart models as judges makes temporal comparisons difficult. When the judge panel composition changes with model updates, direct performance comparisons across different evaluation periods become unreliable. While this limitation mirrors challenges faced by other dynamic benchmarks like LiveBench, it limits our ability to track longitudinal trends in model capabilities. Furthermore, our evaluation includes several proprietary, closed-source models (e.g., GPT-40). Access to these models is typically via APIs which may be subject to change, and the underlying models can be updated without public notice, potentially impacting exact reproducibility. While including these models is essential for a comprehensive assessment of the current state-of-the-art, we acknowledge this inherent</p>
<p>limitation regarding reproducibility compared to evaluations focused solely on open-weight models. Moreover, the LLM-as-a-judge approach itself introduces a potential source of bias. Aligned models can exhibit sycophancy [64-66], a tendency to agree with the user or produce agreeable outputs, which may lead to inflated absolute scores and a compressed score range, further reinforcing the need to focus on relative rather than absolute performance rankings. Additionally, we observe an inherent tension between models' safety constraints and creative evaluation. Some models exemplify this behavior by declining to generate ideas for potentially sensitive keywords (e.g., "ecotoxicology"). While such safety measures are crucial, they can negatively impact creativity scores, potentially undervaluing models with stronger ethical constraints. The potential for hallucinations in the generated ideas themselves [67] also underscores the necessity of human oversight in practical applications.</p>
<p>Furthermore, we recognize a fundamental challenge in the reliability of LLM-as-a-Judge approaches. When evaluating scientific ideas containing concepts outside the judge models' knowledge boundaries, these models might misunderstand novel concepts and consequently misjudge their originality or feasibility. While our use of a dynamic panel of state-of-the-art judge models likely provides broader and more current knowledge coverage than static panels, this fundamental limitation persists. While our multi-model ensemble approach mitigates this issue to some extent, more comprehensive solutions could involve retrieval-augmented generation (RAG) approaches that incorporate up-to-date scientific literature. By augmenting LLM judges with the ability to retrieve and reference the latest scientific papers, such approaches could significantly enhance evaluation accuracy, particularly for highly specialized or cutting-edge scientific domains, although implementing effective RAG systems for this purpose presents its own challenges regarding retrieval relevance and integration with the judging process. This represents a promising direction for future work that could further strengthen the reliability of LLM-based creativity assessments. To empirically assess this reliability, we conducted a human expert validation focused on the Partial Differential Equations (PDE) domain (see Supplementary Note 7). The results showed encouraging alignment between human experts and LLM judgments, particularly for originality ( $r \approx 0.82$ ), lending empirical support to the LLM-as-a-judge approach, at least within this specific domain.</p>
<p>Looking ahead, several research directions emerge. To address temporal comparability issues, normalized scoring mechanisms could maintain meaningful cross-temporal comparisons while preserving the advantages of dynamic evaluation. The comprehensive dataset generated through our evaluations offers opportunities for training scientific language models, mining ideation patterns, and investigating novel scientific ideas. Future work must also tackle the challenge of fairly evaluating creative potential while accounting for ethical constraints, possibly through domain-specific scoring adjustments or separate evaluation tracks for models with different safety priorities.</p>
<p>The implications of our findings extend beyond model evaluation to the broader landscape of AI-assisted scientific discovery. As LLMs demonstrate increasingly sophisticated idea generation capabilities, they hold promise as powerful tools for accelerating scientific innovation - from hypothesis generation to experimental design. Our benchmark provides a foundation for understanding and improving these capabilities, potentially enabling more effective human-AI collaboration [11-13] and informing the design of human-aware AI systems for science [68] in pushing the boundaries of scientific knowledge. The creative strengths we observed in different model architectures suggest that a diverse ecosystem of AI tools, each with complementary capabilities, could support different aspects of the scientific process.</p>
<p>These findings and challenges point toward a broader research agenda: understanding how to nurture and evaluate machine creativity while maintaining essential safety guardrails, ultimately in service of advancing scientific discovery. Through continued refinement, LiveIdeaBench aims to serve as a key tool in this evolving landscape of AI capability assessment and scientific innovation.</p>
<p>Finally, a practical consideration is the environmental cost associated with the extensive LLM usage required for comprehensive benchmarking [69]. Using the "EcoLogits Calculator" [70] to estimate the carbon footprint of our benchmark evaluations, we calculate a total emission of approximately $3074 \mathrm{~kgCO}_{2}$ eq for the full evaluation run reported here. While necessary for rigorous assessment, this highlights the significant energy demands of current AI systems. A detailed breakdown of the estimated emissions per model and per role (idea generator vs. judge) can be found in Supplementary Table S.4.</p>
<h1>Methods</h1>
<p>Building upon Guilford's foundational theory of divergent thinking, we develop a comprehensive evaluation methodology (see Fig. 1) that quantitatively assesses five fundamental dimensions in scientific idea generation. While Guilford's original theoretical framework provides theoretical underpinnings, we extend and operationalize these concepts specifically for evaluating LLMs' scientific idea generation capabilities within our LiveIdeaBench benchmark. It is important to note that our methodology evaluates an essential but not exhaustive aspect of scientific creativity, focusing primarily on divergent thinking capabilities.</p>
<h2>Dimensions of Evaluation</h2>
<p>Originality Originality assessment focuses on the uniqueness and novelty of generated ideas. We implement this through our critic system, where judge LLMs evaluate each idea's originality independently (see Fig. 1b and Supplementary Note 1.2). The final originality score for each model is computed as the mean evaluation across all scientific keywords and generated ideas, providing an absolute score that reflects the model's capacity for novel ideation. To ensure assessment reliability, each generated idea is evaluated by a minimum of three randomly assigned critic LLMs from our panel. This multiple-evaluator approach mitigates potential assessment bias that could arise from relying on a single model's judgment, thereby enhancing the objectivity and reliability of our evaluation benchmark.</p>
<p>Feasibility In the context of scientific innovation, the practical implementability and scientific soundness of an idea are paramount. Therefore, our evaluation includes a distinct feasibility dimension, assessing whether a proposed idea is technically achievable and aligns with established scientific principles and constraints. This aligns with the instructions given to the LLMs, which noted feasibility as a key characteristic of a good scientific idea (see Supplementary Note 1.1). Similar to originality and clarity, feasibility scores are determined by our critic system and averaged across all keywords and ideas to produce an absolute metric (see Supplementary Note 1.2). This ensures our benchmark evaluates the practical viability crucial for scientific progress.</p>
<p>Clarity Our evaluation benchmark incorporates a clarity dimension, directly informed by the prompt provided to the idea-generating LLMs, which notes that good scientific ideas should be clearly articulated (see Supplementary Note 1.1). This dimension assesses the quality of the idea's expression, focusing on its coherence, logical flow, and comprehensibility, particularly given the constraint of the 100-word limit which demands concise articulation. While conceptually related to the elaboration aspect in Guilford's creativity theory, our assessment prioritizes effective and understandable communication within the specified format. Like originality and feasibility, clarity scores are determined by our critic system (see Fig. 1b and Supplementary Note 1.2), involving</p>
<p>multiple judges per idea and averaging the results. Assessing clarity acknowledges that the potential impact of a scientific idea depends not only on its novelty and feasibility but also on how effectively it is communicated [28].</p>
<p>Fluency Fluency assessment examines the model's capacity to generate diverse, non-redundant ideas using identical keywords (see Fig. 1d). Through our judge panel, we evaluate the distinctiveness of generated outputs using a letter-grade scoring system: D indicates academically identical ideas; C represents similar ideas addressing similar problems; B denotes different ideas addressing similar problems; and A signifies completely different ideas addressing different problems. To align with the 1-10 integer scale used for all evaluation dimensions, these four qualitative grades are mapped linearly to the integer scores 1 (for D), 4 (for C), 7 (for B), and 10 (for A), respectively. This mapping ensures consistent scaling across dimensions and maintains equal intervals between the assessed qualitative distinctness levels, enabling precise measurement of genuine idea diversity versus surface-level variations (see Supplementary Note 1.3 for prompts). While simpler diversity metrics examining syntax or semantics would require fewer computational resources, we chose LLM-as-a-Judge for its ability to better capture the nuanced differences between genuinely distinct scientific ideas versus superficial variations. For a benchmark specifically designed to evaluate scientific divergent thinking capabilities, this precision is essential.</p>
<p>Flexibility Flexibility measurement evaluates the model's ability to maintain consistent performance across different scientific domains and contexts. Rather than treating flexibility as an independent metric, we derive it from the distribution of the combined scores (averaging originality, feasibility, clarity, and fluency) across various keywords. Following the principle that a system's overall effectiveness is constrained by its weakest performing components, we focus on the 30th percentile of this composite score distribution (see Fig. 1e). This percentile choice provides a robust measure of a model's performance floor while avoiding extreme outliers, enabling us to assess whether its scientific creativity can genuinely generalize to less common or niche domains. The resulting metric identifies models that maintain reliable performance across diverse scientific contexts rather than those exhibiting domain-specific excellence, thus providing a conservative estimate of cross-domain capabilities.</p>
<h1>Scientific Keyword Selection</h1>
<p>Our evaluation leverages a dynamic, continuously updated set of scientific keywords sourced from a real-time analytics database [71]. The current keywords set (as of December 16, 2024) comprises 1,180 high-impact scientific keywords (Fig. 2c) across 22 distinct scientific disciplines, selected based on current search engine engagement metrics. Unlike static benchmarks, LiveIdeaBench updates its keyword database monthly to maintain alignment with emerging scientific trends and research frontiers. This automated refresh mechanism ensures the benchmark consistently reflects contemporary scientific discourse and technological advancement, making it particularly valuable for evaluating LLMs' ability to engage with cutting-edge scientific concepts rather than just established knowledge.</p>
<h2>Model Selection</h2>
<p>LiveIdeaBench maintains a continuously evolving roster of evaluated models by automatically incorporating the top 41 performers from the most recent LiveBench evaluations [62]. This dynamic selection process ensures our benchmark always tests the latest advancements in language model</p>
<p>capabilities. We implement a dual-role system where all models serve as idea generators, while the top 10 performers additionally function as our judge panel (critics), subject to the diversity constraints outlined in the Experimental Protocol. This approach creates a self-updating evaluation benchmark that evolves alongside rapid developments in AI, ensuring that both idea generation and assessment standards reflect current state-of-the-art capabilities. The automatic monthly refresh of both models and evaluation criteria through LiveBench integration helps prevent benchmark staleness and potential gaming of the system, maintaining LiveIdeaBench's relevance as a contemporary measure of scientific creativity.</p>
<p>Our evaluation benchmark currently encompasses 41 state-of-the-art LLMs based on LiveBench's March 2025 results. This includes models from major developers such as Anthropic (claude-3.7sonnet:thinking, claude-3.7-sonnet, claude-3.5-sonnet, claude-3-opus, claude-3.5-haiku20241022) [72]; OpenAI (o3-mini-high, gpt-4.5-preview, o1, o3-mini, o1-mini, gpt-4o-202411-20, gpt-4-turbo, gpt-4o-mini) [73]; Google (gemini-2.0-flash-thinking-exp, gemini-2.0-pro-exp-02-05, gemini-2.0-flash-exp, gemini-pro-1.5, gemini-2.0-flash-lite-001, gemma-227b-it) [74]; Qwen (qwq-32b, qwen-max, qwen2.5-dracarys2-72b, qwen-2.5-72b-instruct, qwen2.5-coder-32b-instruct, qwq-32b-preview, qwen-2.5-7b-instruct) [75-77]; DeepSeek (deepseekr1, deepseek-chat (v3), deepseek-r1-distill-llama-70b, deepseek-r1-distill-qwen-32b) [78-80]; Meta (llama-3.1-405b-instruct, llama-3.3-70b-instruct, llama-3.1-70b-instruct) [81]; Mistral (mistral-large-2411, mistral-small-24b-instruct-2501, mistral-small (v2409)) [82]; Amazon (nova-pro-v1, nova-lite-v1) [83]; StepFun (step-2-16k-202411) [84]; xAI (grok-21212) [85]; and Microsoft (phi-4) [86]. This comprehensive set includes both proprietary and openweight models, spanning diverse architectures, parameter scales, and training methodologies.</p>
<h1>Experimental Protocol</h1>
<p>We implemented several methodological controls to ensure rigorous evaluation:</p>
<ul>
<li>Model Selection Criteria for Idea Generation: To prevent redundancy in the pool of ideagenerating models, we selected only the most recent version of models with multiple temporal variants (e.g., GPT-4o series). Models exhibiting API instability during the evaluation period were also excluded to maintain data quality consistency across all evaluated models.</li>
<li>Judge LLM Panel Formation, Independence, and Application: The judge panel, comprising the top 10 models from LiveBench, is formed while applying specific diversity constraints to mitigate potential correlated biases. To ensure broader representation, we limit the contribution from any single organization to a maximum of $20 \%$ (i.e., 2 models) of the panel; if the initial top 10 includes more than two models from one organization, the lower-ranked ones are replaced by the next highest-ranked eligible models from different organizations. Additionally, when considering model pairs with identical base models that differ primarily in "reasoning effort" (e.g., o3-mini vs. o3-mini-high), we select only one representative for the judge panel (prioritizing the variant with higher general intelligence scores on LiveBench) to avoid redundancy and the potential amplification of biases inherent to that specific base model. Furthermore, to prevent circular dependency during evaluation, we implement strict independence: when evaluating any specific model's generated ideas, that model is explicitly excluded from serving on the judge panel for that evaluation round, ensuring independent assessment free from self-evaluation. This established panel is then utilized through specific sampling procedures for evaluation: For assessing originality, feasibility, and clarity, each individual generated idea is evaluated by a subset of 3 judges randomly sampled from the remaining eligible panel members. The final score for each of these dimensions is the average</li>
</ul>
<p>of the scores provided by these three judges, enhancing assessment robustness. For assessing fluency, which evaluates the diversity of ideas generated for the same keyword by a given model, the comparison is performed by a single judge randomly sampled from the eligible panel members for each keyword-model pair.</p>
<ul>
<li>Response Standardization: All models were prompted to generate ideas within a 100-word target length, with a maximum allowable threshold of 200 words (see Supplementary Note 1.1). Responses exceeding this limit were excluded from analysis to ensure comparative validity across models.</li>
<li>Special Implementations: The reasoning-centric architecture of qwq-32b-preview necessitated a modified protocol, incorporating a " $<em> </em>$ Final Idea: $<em> </em>$ " delimiter for response parsing (see Supplementary Note 1.4). In cases where parsing failed, critic LLMs evaluated the complete reasoning output to maintain assessment comprehensiveness.</li>
<li>Handling Refused Responses: To fairly assess models, especially those with strong safety alignments that might refuse prompts for sensitive keywords, we implemented a two-step refusal handling protocol. If an initial idea generation request is refused (detected via specific keywords detailed in Supplementary Note 5), a fallback prompt reframing the task within an academic context is used for a second attempt. This ensures models are not unduly penalized for safety constraints when they might still be capable of generating relevant scientific ideas under appropriate framing. Further details and refusal rates are provided in Supplementary Note 5 .</li>
</ul>
<h1>Data availability</h1>
<p>All the datasets used to test the methods in this study are available on Huggingface at https: //huggingface.co/datasets/6cf/liveideabench-v2. The dynamic leaderboard is available at https://liveideabench.com/</p>
<h2>Code availability</h2>
<p>All the source codes used to reproduce the results in this study are available on GitHub at https://github.com/x66ccff/liveideabench.</p>
<p>Acknowledgements: The work is supported by the National Natural Science Foundation of China (No. 92270118, No. 62276269), the Beijing Natural Science Foundation (No. 1232009), and the Strategic Priority Research Program of the Chinese Academy of Sciences (No. XDB0620103). In addition, H.S and Y.L. would like to acknowledge the support from the Fundamental Research Funds for the Central Universities (No. 202230265 and No. E2EG2202X2).</p>
<p>Author contributions: K.R., X.W., J.H., P.W., Y.L., H.S. contributed to the ideation and design of the research; K.R., X.W. and J.H. performed the research; H.S. supervised the project; all authors contributed to the research discussions, writing, and editing of the paper.</p>
<p>Corresponding author: Hao Sun (haosun@ruc.edu.cn).</p>
<p>Competing interests: The authors declare no competing interests.
Supplementary information: The supplementary information is attached.</p>
<h1>Supplementary Information for:</h1>
<h2>LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context</h2>
<h2>1 Prompts</h2>
<h3>1.1 Idea Prompt</h3>
<p>I'll be submitting your next responses to a "Good Scientific Idea" expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: "{{keywords}}". You may provide background information. The idea MUST be concisely expressed within 100 words total (including any background information). (Note: good scientific ideas should be original (novel contribution), feasible (technically implementable), clearly articulated, and address meaningful problems in the field.).</p>
<h2>Fallback Prompt (If the first request is refused):</h2>
<p>I'll be submitting your next responses to a "Good Scientific Idea" expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: "{{keywords}}". You may provide background information. The idea MUST be concisely expressed within 100 words total (including any background information). This is a research study comparing different AI models on their ability to generate scientific ideas. Your suggestions will only be used for academic research purposes and not for any harmful applications. Please respond with a creative scientific idea related to the keyword provided. (Note: good scientific ideas should be original (novel contribution), feasible (technically implementable), clearly articulated, and address meaningful problems in the field.).</p>
<h3>1.2 Originality \&amp; Feasibility Critic Prompt</h3>
<h2>SYSTEM:</h2>
<p>You are an extremely demanding scientific reviewer with the highest critical standards, like those at Nature or Science. When evaluating scientific ideas, you will assess them on three key dimensions:</p>
<ol>
<li>originality: Novel contribution to unexplored areas or innovative approaches to existing problems</li>
<li>feasibility: Technical implementation and practicality</li>
<li>clarity: How well-articulated and easy to understand the idea is</li>
</ol>
<p>Your response should consist of two parts: a text analysis followed by a JSON score block. First, provide your brief analysis (less than 100 words) of the idea. Then, for each dimension, provide a score from 1 to 10 where $1-3=$ poor, $4-6=$ average, $7-10=$ excellent. For example:
" json
{
"originality": <score_1_to_10>,
"feasibility": <score_1_to_10>,
"clarity": <score_1_to_10>
}
・.
USER:
Please evaluate the following scientific idea and give your scores directly: {{idea}}</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-568 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-568</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-568</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-ae9e5e72aefd19b81c1fe75d7baf6c0bedad75e5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ae9e5e72aefd19b81c1fe75d7baf6c0bedad75e5" target="_blank">Deep Transfer Learning with Joint Adaptation Networks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> JAN is presented, which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion.</p>
                <p><strong>Paper Abstract:</strong> Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e568.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e568.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Mean Discrepancy (MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kernel two-sample test that measures the distance between two probability distributions by the distance between their mean embeddings in a reproducing kernel Hilbert space (RKHS); used as a domain-discrepancy metric and regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A kernel two-sample test</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Maximum Mean Discrepancy (MMD) kernel two-sample test</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>MMD represents each distribution by its mean embedding in an RKHS induced by a kernel k(·,·) and computes the squared RKHS-norm of the difference between embeddings: D_H(P,Q)=||μ_P-μ_Q||^2_H. In practice an empirical (unbiased) estimator is computed from samples via sums of kernel evaluations (quadratic form) or via linear-time unbiased approximations. MMD serves as a statistical test and as a differentiable loss that can be minimized to align feature distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / statistical two-sample test / regularizer</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>statistical learning theory / kernel methods / nonparametric hypothesis testing</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>deep learning for unsupervised domain adaptation (computer vision tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied MMD as a differentiable regularizer inside deep CNN training to align distributions of network activations across domains; used Gaussian kernels with bandwidth set to median pairwise distances; adopted minibatch estimators (linear-time unbiased MMD estimator) to fit SGD; used MMD both as independent-layer penalties (prior work DAN) and as building block for joint-distribution measures (JMMD).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful — MMD-based methods (DAN, DDC) provided significant gains over non-adaptive CNN baselines; when integrated into JAN framework as base component, contributed to state-of-the-art results (e.g., JAN/JAN-A improved average accuracies reported in Tables: AlexNet-based JAN-A avg ≈ 76.3% on Office-31; ResNet-based JAN-A avg ≈ 84.6% on Office-31).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Quadratic computation cost of naive MMD for large sample sizes; possible vanishing gradients or limited expressivity for some kernels in very high-dimensional image feature spaces; need to choose kernel and bandwidth carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Established theory of RKHS embeddings and unbiased estimators (Gretton et al.); differentiability of kernel-sum estimators enabling backpropagation; availability of pretrained deep features to which MMD can be applied.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires choice of kernel (authors use Gaussian kernels with median heuristic); integration into minibatch SGD requires linear-time estimators or batching strategy; needs source and target samples available during training (unlabeled target samples acceptable).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within machine-learning contexts: MMD is a general nonparametric test and can be applied to align distributions of arbitrary feature representations and to other domains beyond vision, but kernel selection and scalability considerations must be addressed case-by-case.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit computational procedure</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Transfer Learning with Joint Adaptation Networks', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e568.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e568.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JMMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Maximum Mean Discrepancy (JMMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of MMD to measure discrepancy between joint distributions by embedding joint distributions into a tensor-product RKHS and computing the RKHS distance between joint embeddings; introduced in this paper to align joint distributions of activations across multiple deep network layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Joint Maximum Mean Discrepancy (JMMD) for joint-distribution alignment</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>JMMD computes the squared distance between empirical embeddings of joint distributions of multiple variables by using tensor-product feature maps: embed P(Z^{1:|L|}) and Q(Z^{1:|L|}) into ⊗_{ℓ} H^ℓ and compute the RKHS norm squared of their difference. Empirically, JMMD reduces to sums of products of layer-wise kernels across domain-specific layers; a linear-time unbiased estimator is derived for minibatch SGD. JMMD is used as a penalty added to classification loss to jointly align the joint distributions of multilayer network activations between source and target.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data-analysis technique / regularizer for domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>kernel embedding methods and tensor-product RKHS theory from statistical machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>deep neural network training for unsupervised domain adaptation (align joint distributions of feature and label surrogates across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (novel extension and integration into deep networks)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Extended kernel mean embedding machinery to joint embeddings of multiple deep-layer activations by using tensor-product RKHS; derived an empirical estimator that uses products of per-layer kernels; derived a linear-time unbiased minibatch estimator to enable SGD; integrated JMMD as an end-to-end differentiable regularizer in CNN fine-tuning pipelines (AlexNet and ResNet) and selected domain-specific layers L (e.g., {fc6, fc7, fc8} for AlexNet).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful — JAN models that minimize JMMD achieved state-of-the-art or strong improvements over prior deep adaptation methods on benchmarks (e.g., JAN improved average accuracy over DAN/RevGrad/RTN in Office-31 and ImageCLEF-DA; reported specific gains in Tables: AlexNet-based JAN avg 76.0% and JAN-A 76.3% vs baselines, ResNet-based JAN avg 84.3% and JAN-A 84.6%). JMMD values (measured in analysis) were substantially reduced by JAN, and t-SNE visualizations show improved class discrimination on target data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Naive JMMD has quadratic complexity in dataset size (sum-of-products over pairs); selecting which layers to include (L) is a design choice; potential sensitivity to kernel choice and to mini-batch sampling balance between source and target.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>The mathematical correspondence between joint distributions of (X,Y) and joint activations across task-specific layers provided a principled surrogate; existing kernel-embedding theory (Song et al., Smola et al.) supported the formulation; the linear-time estimator enabled scalable SGD integration; pretrained deep networks provided stable features to adapt.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires identification of domain-specific layers L to include in joint embedding; requires the ability to compute per-layer kernel evaluations and their products in minibatches; balanced sampling of source and target per minibatch recommended; moderate compute for deep model fine-tuning and kernel computations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately high within deep learning adaptation problems: authors applied JMMD to multiple CNN architectures (AlexNet, ResNet) and datasets (Office-31, ImageCLEF-DA). The approach is conceptually generalizable to other modalities (e.g., speech, text) where joint-distribution shifts across layers are meaningful, subject to kernel choice and computational scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles extended to explicit procedural steps and computational method</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Transfer Learning with Joint Adaptation Networks', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e568.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e568.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial JMMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Maximization of JMMD (JMMD-A / JAN-A)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of adversarial training (GAN-style / domain-adversarial) to maximize the JMMD criterion via a learned neural critic so that the JMMD becomes more powerful at distinguishing source and target joint distributions during adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative adversarial nets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Adversarial maximization of JMMD (JAN-A)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Instead of using a fixed RKHS kernel, the JMMD criterion is parametrized with additional fully-connected neural layers (parameters θ) that map activations to features used in the MMD computation; training alternates (or jointly optimizes) between maximizing JMMD w.r.t θ (making source and target more distinguishable) and minimizing classification loss plus JMMD w.r.t. network parameters f (making distributions aligned). This mirrors GAN-style critic maximization to increase test power of the discrepancy measure and avoids some limitations of kernel MMD (vanishing gradients, limited expressivity).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / adversarial training procedure / optimization modification</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>generative adversarial networks and adversarial training from unsupervised generative modeling (deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>domain adaptation discrepancy estimation and deep feature learning for transfer (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (hybrid approach combining adversarial critic with JMMD)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Added neural-network parametrized critic (multiple fully-connected layers) on top of activations used in JMMD, and formulated a min_f max_θ objective where θ maximizes JMMD and f minimizes classification loss plus JMMD; used adversarial training schedule and same progressive λ schedule as domain-adversarial methods to stabilize training; integrated within CNN fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful — JAN-A (adversarial JMMD) yielded small but consistent improvements over non-adversarial JAN, and outperformed prior domain-adversarial method RevGrad in reported experiments (e.g., JAN-A avg 76.3% vs RevGrad 74.3% on AlexNet Office-31; ResNet JAN-A avg 84.6% vs RevGrad 82.2%). Convergence behavior reported as comparable to RevGrad with improved final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Adversarial optimization introduces instability and convergence concerns typical of min-max training; kernel-based JMMD without critic may suffer vanishing gradients or limited expressivity; need to balance maximization and minimization steps and tune learning schedules (progressive λ).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Prior success of adversarial training (GANs) and domain-adversarial methods (RevGrad) provided methodological templates; differentiability of JMMD computations and integration into backprop allowed joint adversarial updates; richer function class (neural critic) increased test power.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires implementing an auxiliary neural critic network and careful optimization (min-max); progressive scheduling of adaptation weight λ to stabilize early training; sufficient compute to train additional critic parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable across deep adaptation tasks where kernel MMD is insufficiently expressive; applicable to other modalities and architectures but requires extra tuning and stabilization strategies typical of adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational method and procedural know-how (optimization strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Transfer Learning with Joint Adaptation Networks', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e568.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e568.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear-time JMMD estimator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear-time unbiased estimator for Joint MMD (mini-batch friendly JMMD estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear-time unbiased estimator of the JMMD objective derived by analogy with the unbiased linear-time MMD estimator, enabling computation of JMMD within minibatch SGD with balanced sampling of source and target points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Linear-time unbiased JMMD estimator for minibatch SGD</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>The estimator rewrites the JMMD empirical sum-of-products into a linear-time form using pairwise pairings inside minibatches: for n (even) samples, sum over i=1..n/2 of products of per-layer kernel evaluations between paired source-source, target-target, source-target, and target-source examples, scaled appropriately. This yields an unbiased estimator that is O(n) within a minibatch and suitable for backprop-based SGD.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / algorithmic optimization</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computational-statistics / kernel two-sample test algorithms (unbiased linear-time MMD estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>deep neural network training (minibatch SGD for domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Extended the linear-time unbiased MMD estimator idea to the JMMD setting (product of per-layer kernels); enforced balanced minibatches (equal source and target counts) and pairing scheme to maintain unbiasedness; integrated estimator into standard backprop pipeline for end-to-end training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful — enabled scalable training of JAN/JAN-A on standard benchmarks and permitted use of JMMD inside minibatch SGD; authors report practical convergence and improved performance while avoiding quadratic kernel cost.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires careful minibatch construction (equal source/target counts, even minibatch size) and pairing strategy; estimator variance may be higher than full-sample quadratic estimator; still requires computing products of kernels per pair which adds overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Existence of unbiased linear-time MMD estimator and the minibatch SGD paradigm in deep learning made adaptation straightforward; balanced sampling protocol and efficient kernel evaluation within minibatches facilitated integration.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Implementation requires minibatch sampling code to ensure balance and pairings; choice of minibatch size and kernel hyperparameters affects estimator variance and training stability; GPU-friendly vectorized kernel computations recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for deep-learning contexts that already use minibatch SGD; the pairing/unbiased estimator technique can be applied to other composite-kernel discrepancy measures but must be adapted per measure.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and computational know-how</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Transfer Learning with Joint Adaptation Networks', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A kernel two-sample test <em>(Rating: 2)</em></li>
                <li>A Hilbert space embedding for distributions <em>(Rating: 2)</em></li>
                <li>Hilbert space embeddings of conditional distributions with applications to dynamical systems <em>(Rating: 2)</em></li>
                <li>Generative adversarial nets <em>(Rating: 2)</em></li>
                <li>Unsupervised domain adaptation by backpropagation <em>(Rating: 2)</em></li>
                <li>Learning transferable features with deep adaptation networks <em>(Rating: 2)</em></li>
                <li>Deep domain confusion: Maximizing for domain invariance <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-568",
    "paper_id": "paper-ae9e5e72aefd19b81c1fe75d7baf6c0bedad75e5",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "MMD",
            "name_full": "Maximum Mean Discrepancy (MMD)",
            "brief_description": "A kernel two-sample test that measures the distance between two probability distributions by the distance between their mean embeddings in a reproducing kernel Hilbert space (RKHS); used as a domain-discrepancy metric and regularizer.",
            "citation_title": "A kernel two-sample test",
            "mention_or_use": "use",
            "procedure_name": "Maximum Mean Discrepancy (MMD) kernel two-sample test",
            "procedure_description": "MMD represents each distribution by its mean embedding in an RKHS induced by a kernel k(·,·) and computes the squared RKHS-norm of the difference between embeddings: D_H(P,Q)=||μ_P-μ_Q||^2_H. In practice an empirical (unbiased) estimator is computed from samples via sums of kernel evaluations (quadratic form) or via linear-time unbiased approximations. MMD serves as a statistical test and as a differentiable loss that can be minimized to align feature distributions.",
            "procedure_type": "computational method / statistical two-sample test / regularizer",
            "source_domain": "statistical learning theory / kernel methods / nonparametric hypothesis testing",
            "target_domain": "deep learning for unsupervised domain adaptation (computer vision tasks)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Applied MMD as a differentiable regularizer inside deep CNN training to align distributions of network activations across domains; used Gaussian kernels with bandwidth set to median pairwise distances; adopted minibatch estimators (linear-time unbiased MMD estimator) to fit SGD; used MMD both as independent-layer penalties (prior work DAN) and as building block for joint-distribution measures (JMMD).",
            "transfer_success": "Successful — MMD-based methods (DAN, DDC) provided significant gains over non-adaptive CNN baselines; when integrated into JAN framework as base component, contributed to state-of-the-art results (e.g., JAN/JAN-A improved average accuracies reported in Tables: AlexNet-based JAN-A avg ≈ 76.3% on Office-31; ResNet-based JAN-A avg ≈ 84.6% on Office-31).",
            "barriers_encountered": "Quadratic computation cost of naive MMD for large sample sizes; possible vanishing gradients or limited expressivity for some kernels in very high-dimensional image feature spaces; need to choose kernel and bandwidth carefully.",
            "facilitating_factors": "Established theory of RKHS embeddings and unbiased estimators (Gretton et al.); differentiability of kernel-sum estimators enabling backpropagation; availability of pretrained deep features to which MMD can be applied.",
            "contextual_requirements": "Requires choice of kernel (authors use Gaussian kernels with median heuristic); integration into minibatch SGD requires linear-time estimators or batching strategy; needs source and target samples available during training (unlabeled target samples acceptable).",
            "generalizability": "High within machine-learning contexts: MMD is a general nonparametric test and can be applied to align distributions of arbitrary feature representations and to other domains beyond vision, but kernel selection and scalability considerations must be addressed case-by-case.",
            "knowledge_type": "theoretical principles and explicit computational procedure",
            "uuid": "e568.0",
            "source_info": {
                "paper_title": "Deep Transfer Learning with Joint Adaptation Networks",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "JMMD",
            "name_full": "Joint Maximum Mean Discrepancy (JMMD)",
            "brief_description": "An extension of MMD to measure discrepancy between joint distributions by embedding joint distributions into a tensor-product RKHS and computing the RKHS distance between joint embeddings; introduced in this paper to align joint distributions of activations across multiple deep network layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Joint Maximum Mean Discrepancy (JMMD) for joint-distribution alignment",
            "procedure_description": "JMMD computes the squared distance between empirical embeddings of joint distributions of multiple variables by using tensor-product feature maps: embed P(Z^{1:|L|}) and Q(Z^{1:|L|}) into ⊗_{ℓ} H^ℓ and compute the RKHS norm squared of their difference. Empirically, JMMD reduces to sums of products of layer-wise kernels across domain-specific layers; a linear-time unbiased estimator is derived for minibatch SGD. JMMD is used as a penalty added to classification loss to jointly align the joint distributions of multilayer network activations between source and target.",
            "procedure_type": "computational method / data-analysis technique / regularizer for domain adaptation",
            "source_domain": "kernel embedding methods and tensor-product RKHS theory from statistical machine learning",
            "target_domain": "deep neural network training for unsupervised domain adaptation (align joint distributions of feature and label surrogates across domains)",
            "transfer_type": "adapted/modified for new context (novel extension and integration into deep networks)",
            "modifications_made": "Extended kernel mean embedding machinery to joint embeddings of multiple deep-layer activations by using tensor-product RKHS; derived an empirical estimator that uses products of per-layer kernels; derived a linear-time unbiased minibatch estimator to enable SGD; integrated JMMD as an end-to-end differentiable regularizer in CNN fine-tuning pipelines (AlexNet and ResNet) and selected domain-specific layers L (e.g., {fc6, fc7, fc8} for AlexNet).",
            "transfer_success": "Successful — JAN models that minimize JMMD achieved state-of-the-art or strong improvements over prior deep adaptation methods on benchmarks (e.g., JAN improved average accuracy over DAN/RevGrad/RTN in Office-31 and ImageCLEF-DA; reported specific gains in Tables: AlexNet-based JAN avg 76.0% and JAN-A 76.3% vs baselines, ResNet-based JAN avg 84.3% and JAN-A 84.6%). JMMD values (measured in analysis) were substantially reduced by JAN, and t-SNE visualizations show improved class discrimination on target data.",
            "barriers_encountered": "Naive JMMD has quadratic complexity in dataset size (sum-of-products over pairs); selecting which layers to include (L) is a design choice; potential sensitivity to kernel choice and to mini-batch sampling balance between source and target.",
            "facilitating_factors": "The mathematical correspondence between joint distributions of (X,Y) and joint activations across task-specific layers provided a principled surrogate; existing kernel-embedding theory (Song et al., Smola et al.) supported the formulation; the linear-time estimator enabled scalable SGD integration; pretrained deep networks provided stable features to adapt.",
            "contextual_requirements": "Requires identification of domain-specific layers L to include in joint embedding; requires the ability to compute per-layer kernel evaluations and their products in minibatches; balanced sampling of source and target per minibatch recommended; moderate compute for deep model fine-tuning and kernel computations.",
            "generalizability": "Moderately high within deep learning adaptation problems: authors applied JMMD to multiple CNN architectures (AlexNet, ResNet) and datasets (Office-31, ImageCLEF-DA). The approach is conceptually generalizable to other modalities (e.g., speech, text) where joint-distribution shifts across layers are meaningful, subject to kernel choice and computational scaling.",
            "knowledge_type": "theoretical principles extended to explicit procedural steps and computational method",
            "uuid": "e568.1",
            "source_info": {
                "paper_title": "Deep Transfer Learning with Joint Adaptation Networks",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Adversarial JMMD",
            "name_full": "Adversarial Maximization of JMMD (JMMD-A / JAN-A)",
            "brief_description": "An adaptation of adversarial training (GAN-style / domain-adversarial) to maximize the JMMD criterion via a learned neural critic so that the JMMD becomes more powerful at distinguishing source and target joint distributions during adaptation.",
            "citation_title": "Generative adversarial nets",
            "mention_or_use": "use",
            "procedure_name": "Adversarial maximization of JMMD (JAN-A)",
            "procedure_description": "Instead of using a fixed RKHS kernel, the JMMD criterion is parametrized with additional fully-connected neural layers (parameters θ) that map activations to features used in the MMD computation; training alternates (or jointly optimizes) between maximizing JMMD w.r.t θ (making source and target more distinguishable) and minimizing classification loss plus JMMD w.r.t. network parameters f (making distributions aligned). This mirrors GAN-style critic maximization to increase test power of the discrepancy measure and avoids some limitations of kernel MMD (vanishing gradients, limited expressivity).",
            "procedure_type": "computational method / adversarial training procedure / optimization modification",
            "source_domain": "generative adversarial networks and adversarial training from unsupervised generative modeling (deep learning)",
            "target_domain": "domain adaptation discrepancy estimation and deep feature learning for transfer (computer vision)",
            "transfer_type": "adapted/modified for new context (hybrid approach combining adversarial critic with JMMD)",
            "modifications_made": "Added neural-network parametrized critic (multiple fully-connected layers) on top of activations used in JMMD, and formulated a min_f max_θ objective where θ maximizes JMMD and f minimizes classification loss plus JMMD; used adversarial training schedule and same progressive λ schedule as domain-adversarial methods to stabilize training; integrated within CNN fine-tuning.",
            "transfer_success": "Successful — JAN-A (adversarial JMMD) yielded small but consistent improvements over non-adversarial JAN, and outperformed prior domain-adversarial method RevGrad in reported experiments (e.g., JAN-A avg 76.3% vs RevGrad 74.3% on AlexNet Office-31; ResNet JAN-A avg 84.6% vs RevGrad 82.2%). Convergence behavior reported as comparable to RevGrad with improved final accuracy.",
            "barriers_encountered": "Adversarial optimization introduces instability and convergence concerns typical of min-max training; kernel-based JMMD without critic may suffer vanishing gradients or limited expressivity; need to balance maximization and minimization steps and tune learning schedules (progressive λ).",
            "facilitating_factors": "Prior success of adversarial training (GANs) and domain-adversarial methods (RevGrad) provided methodological templates; differentiability of JMMD computations and integration into backprop allowed joint adversarial updates; richer function class (neural critic) increased test power.",
            "contextual_requirements": "Requires implementing an auxiliary neural critic network and careful optimization (min-max); progressive scheduling of adaptation weight λ to stabilize early training; sufficient compute to train additional critic parameters.",
            "generalizability": "Likely generalizable across deep adaptation tasks where kernel MMD is insufficiently expressive; applicable to other modalities and architectures but requires extra tuning and stabilization strategies typical of adversarial training.",
            "knowledge_type": "computational method and procedural know-how (optimization strategy)",
            "uuid": "e568.2",
            "source_info": {
                "paper_title": "Deep Transfer Learning with Joint Adaptation Networks",
                "publication_date_yy_mm": "2016-05"
            }
        },
        {
            "name_short": "Linear-time JMMD estimator",
            "name_full": "Linear-time unbiased estimator for Joint MMD (mini-batch friendly JMMD estimator)",
            "brief_description": "A linear-time unbiased estimator of the JMMD objective derived by analogy with the unbiased linear-time MMD estimator, enabling computation of JMMD within minibatch SGD with balanced sampling of source and target points.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Linear-time unbiased JMMD estimator for minibatch SGD",
            "procedure_description": "The estimator rewrites the JMMD empirical sum-of-products into a linear-time form using pairwise pairings inside minibatches: for n (even) samples, sum over i=1..n/2 of products of per-layer kernel evaluations between paired source-source, target-target, source-target, and target-source examples, scaled appropriately. This yields an unbiased estimator that is O(n) within a minibatch and suitable for backprop-based SGD.",
            "procedure_type": "computational method / algorithmic optimization",
            "source_domain": "computational-statistics / kernel two-sample test algorithms (unbiased linear-time MMD estimator)",
            "target_domain": "deep neural network training (minibatch SGD for domain adaptation)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Extended the linear-time unbiased MMD estimator idea to the JMMD setting (product of per-layer kernels); enforced balanced minibatches (equal source and target counts) and pairing scheme to maintain unbiasedness; integrated estimator into standard backprop pipeline for end-to-end training.",
            "transfer_success": "Successful — enabled scalable training of JAN/JAN-A on standard benchmarks and permitted use of JMMD inside minibatch SGD; authors report practical convergence and improved performance while avoiding quadratic kernel cost.",
            "barriers_encountered": "Requires careful minibatch construction (equal source/target counts, even minibatch size) and pairing strategy; estimator variance may be higher than full-sample quadratic estimator; still requires computing products of kernels per pair which adds overhead.",
            "facilitating_factors": "Existence of unbiased linear-time MMD estimator and the minibatch SGD paradigm in deep learning made adaptation straightforward; balanced sampling protocol and efficient kernel evaluation within minibatches facilitated integration.",
            "contextual_requirements": "Implementation requires minibatch sampling code to ensure balance and pairings; choice of minibatch size and kernel hyperparameters affects estimator variance and training stability; GPU-friendly vectorized kernel computations recommended.",
            "generalizability": "High for deep-learning contexts that already use minibatch SGD; the pairing/unbiased estimator technique can be applied to other composite-kernel discrepancy measures but must be adapted per measure.",
            "knowledge_type": "explicit procedural steps and computational know-how",
            "uuid": "e568.3",
            "source_info": {
                "paper_title": "Deep Transfer Learning with Joint Adaptation Networks",
                "publication_date_yy_mm": "2016-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A kernel two-sample test",
            "rating": 2
        },
        {
            "paper_title": "A Hilbert space embedding for distributions",
            "rating": 2
        },
        {
            "paper_title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems",
            "rating": 2
        },
        {
            "paper_title": "Generative adversarial nets",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised domain adaptation by backpropagation",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable features with deep adaptation networks",
            "rating": 2
        },
        {
            "paper_title": "Deep domain confusion: Maximizing for domain invariance",
            "rating": 1
        }
    ],
    "cost": 0.014902499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Transfer Learning with Joint Adaptation Networks</h1>
<p>Mingsheng Long ${ }^{1}$ Han Zhu ${ }^{1}$ Jianmin Wang ${ }^{1}$ Michael I. Jordan ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.</p>
<h2>1. Introduction</h2>
<p>Deep networks have significantly improved the state of the arts for diverse machine learning problems and applications. Unfortunately, the impressive performance gains come only when massive amounts of labeled data are available for supervised learning. Since manual labeling of sufficient training data for diverse application domains on-the-fly is often prohibitive, for a target task short of labeled data, there is strong motivation to build effective learners that can leverage rich labeled data from a different source domain. However, this learning paradigm suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting predictive models for the target task (Quionero-Candela et al., 2009; Pan \&amp; Yang, 2010).</p>
<p>Learning a discriminative model in the presence of the shift between training and test distributions is known as transfer learning or domain adaptation (Pan \&amp; Yang, 2010). Previous</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating instance importance without using target labels (Huang et al., 2006; Pan et al., 2011; Gong et al., 2013). Recent deep transfer learning methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains (Tzeng et al., 2014; 2015; Long et al., 2015; 2016; Ganin \&amp; Lempitsky, 2015; Bousmalis et al., 2016).</p>
<p>Transfer learning becomes more challenging when domains may change by the joint distributions of input features and output labels, which is a common scenario in practical applications. First, deep networks generally learn the complex function from input features to output labels via multilayer feature transformation and abstraction. Second, deep features in standard CNNs eventually transition from general to specific along the network, and the transferability of features and classifiers decreases when the cross-domain discrepancy increases (Yosinski et al., 2014). Consequently, after feedforwarding the source and target domain data through deep networks for multilayer feature abstraction, the shifts in the joint distributions of input features and output labels still linger in the network activations of multiple domain-specific higher layers. Thus we can use the joint distributions of the activations in these domain-specific layers to approximately reason about the original joint distributions, which should be matched across domains to enable domain adaptation. To date, this problem has not been addressed in deep networks.</p>
<p>In this paper, we present Joint Adaptation Networks (JAN) to align the joint distributions of multiple domain-specific layers across domains for unsupervised domain adaptation. JAN largely extends the ability of deep adaptation networks (Long et al., 2015) to reason about the joint distributions as mentioned above, while keeping the training procedure even simpler. Specifically, JAN admits a simple transfer pipeline, which processes the source and target domain data by convolutional neural networks (CNN) and then aligns the joint distributions of activations in multiple task-specific layers. To learn parameters and enable alignment, we derive joint maximum mean discrepancy (JMMD), which measures the Hilbert-Schmidt norm between kernel mean embedding of empirical joint distributions of source and target data.</p>
<p>Thanks to a linear-time unbiased estimate of JMMD, we can easily draw a mini-batch of samples to estimate the JMMD criterion, and implement it efficiently via back-propagation. We further maximize JMMD using adversarial training strategy such that the distributions of source and target domains are made more distinguishable. Empirical study shows that our models yield state of the art results on standard datasets.</p>
<h2>2. Related Work</h2>
<p>Transfer learning (Pan \&amp; Yang, 2010) aims to build learning machines that generalize across different domains following different probability distributions (Sugiyama et al., 2008; Pan et al., 2011; Duan et al., 2012; Gong et al., 2013; Zhang et al., 2013). Transfer learning finds wide applications in computer vision (Saenko et al., 2010; Gopalan et al., 2011; Gong et al., 2012; Hoffman et al., 2014) and natural language processing (Collobert et al., 2011; Glorot et al., 2011).</p>
<p>The main technical problem of transfer learning is how to reduce the shifts in data distributions across domains. Most existing methods learn a shallow representation model by which domain discrepancy is minimized, which cannot suppress domain-specific exploratory factors of variations. Deep networks learn abstract representations that disentangle the explanatory factors of variations behind data (Bengio et al., 2013) and extract transferable factors underlying different populations (Glorot et al., 2011; Oquab et al., 2013), which can only reduce, but not remove, the cross-domain discrepancy (Yosinski et al., 2014). Recent work on deep domain adaptation embeds domain-adaptation modules into deep networks to boost transfer performance (Tzeng et al., 2014; 2015; 2017; Ganin \&amp; Lempitsky, 2015; Long et al., 2015; 2016). These methods mainly correct the shifts in marginal distributions, assuming conditional distributions remain unchanged after the marginal distribution adaptation.</p>
<p>Transfer learning will become more challenging as domains may change by the joint distributions $P(\mathbf{X}, \mathbf{Y})$ of input features $\mathbf{X}$ and output labels $\mathbf{Y}$. The distribution shifts may stem from the marginal distributions $P(\mathbf{X})$ (a.k.a. covariate shift (Huang et al., 2006; Sugiyama et al., 2008)), the conditional distributions $P(\mathbf{Y} \mid \mathbf{X})$ (a.k.a. conditional shift (Zhang et al., 2013)), or both (a.k.a. dataset shift (QuioneroCandela et al., 2009)). Another line of work (Zhang et al., 2013; Wang \&amp; Schneider, 2014) correct both target and conditional shifts based on the theory of kernel embedding of conditional distributions (Song et al., 2009; 2010; Sriperumbudur et al., 2010). Since the target labels are unavailable, adaptation is performed by minimizing the discrepancy between marginal distributions instead of conditional distributions. In general, the presence of conditional shift leads to an ill-posed problem, and an additional assumption that the conditional distribution may only change under locationscale transformations on $\mathbf{X}$ is commonly imposed to make
the problem tractable (Zhang et al., 2013). As it is not easy to justify which components of the joint distribution are changing in practice, our work is transparent to diverse scenarios by directly manipulating the joint distribution without assumptions on the marginal and conditional distributions. Furthermore, it remains unclear how to account for the shift in joint distributions within the regime of deep architectures.</p>
<h2>3. Preliminary</h2>
<h3>3.1. Hilbert Space Embedding</h3>
<p>We begin by providing an overview of Hilbert space embeddings of distributions, where each distribution is represented by an element in a reproducing kernel Hilbert space (RKHS). Denote by $\mathbf{X}$ a random variable with domain $\Omega$ and distribution $P(\mathbf{X})$, and by $\mathbf{x}$ the instantiations of $\mathbf{X}$. A reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ on $\Omega$ endowed by a kernel $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ is a Hilbert space of functions $f: \Omega \mapsto \mathbb{R}$ with inner product $\langle\cdot, \cdot\rangle_{\mathcal{H}}$. Its element $k(\mathbf{x}, \cdot)$ satisfies the reproducing property: $\langle f(\cdot), k(\mathbf{x}, \cdot)\rangle_{\mathcal{H}}=f(\mathbf{x})$. Alternatively, $k(\mathbf{x}, \cdot)$ can be viewed as an (infinite-dimensional) implicit feature map $\phi(\mathbf{x})$ where $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left\langle\phi(\mathbf{x}), \phi\left(\mathbf{x}^{\prime}\right)\right\rangle_{\mathcal{H}}$. Kernel functions can be defined on vector space, graphs, time series and structured objects to handle diverse applications. The kernel embedding represents a probability distribution $P$ by an element in RKHS endowed by a kernel $k$ (Smola et al., 2007; Sriperumbudur et al., 2010; Gretton et al., 2012)</p>
<p>$$
\mu_{\mathbf{X}}(P) \triangleq \mathbb{E}<em _Omega="\Omega">{\mathbf{X}}[\phi(\mathbf{X})]=\int</em>)
$$} \phi(\mathbf{x}) \mathrm{d} P(\mathbf{x</p>
<p>where the distribution is mapped to the expected feature map, i.e. to a point in the RKHS, given that $\mathbb{E}<em _mathbf_X="\mathbf{X">{\mathbf{X}}\left[k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right] \leqslant \infty$. The mean embedding $\mu</em>}}$ has the property that the expectation of any RKHS function $f$ can be evaluated as an inner product in $\mathcal{H},\left\langle\mu_{\mathbf{X}}, f\right\rangle_{\mathcal{H}} \triangleq \mathbb{E<em _mathbf_X="\mathbf{X">{\mathbf{X}}[f(\mathbf{X})], \forall f \in \mathcal{H}$. This kind of kernel mean embedding provides us a nonparametric perspective on manipulating distributions by drawing samples from them. We will require a characteristic kernel $k$ such that the kernel embedding $\mu</em>(P)$ is injective, and that the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, which removes the necessity of density estimation of $P$. This technique has been widely applied in many tasks, including feature extraction, density estimation and two-sample test (Smola et al., 2007; Gretton et al., 2012).}</p>
<p>While the true distribution $P(\mathbf{X})$ is rarely accessible, we can estimate its embedding using a finite sample (Gretton et al., 2012). Given a sample $\mathcal{D}<em 1="1">{\mathbf{X}}=\left{\mathbf{x}</em>)$, the empirical kernel embedding is}, \ldots, \mathbf{x}_{n}\right}$ of size $n$ drawn i.i.d. from $P(\mathbf{X</p>
<p>$$
\widehat{\mu}<em i="1">{\mathbf{X}}=\frac{1}{n} \sum</em>\right)
$$}^{n} \phi\left(\mathbf{x}_{i</p>
<p>This empirical estimate converges to its population counterpart in RKHS norm $\left|\mu_{\mathbf{X}}-\widehat{\mu}<em _mathcal_H="\mathcal{H">{\mathbf{X}}\right|</em>\right)$.
Kernel embeddings can be readily generalized to joint distributions of two or more variables using tensor product feature spaces (Song et al., 2009; 2010; Song \&amp; Dai, 2013). A joint distribution $P$ of variables $\mathbf{X}^{1}, \ldots, \mathbf{X}^{m}$ can be embedded into an $m$-th order tensor product feature space $\otimes_{\ell=1}^{m} \mathcal{H}^{\ell}$ by}}$ with a rate of $O\left(n^{-\frac{1}{2}</p>
<p>$$
\begin{aligned}
\mathcal{C}<em _mathbf_X="\mathbf{X">{\mathbf{X}^{1: m}}(P) &amp; \triangleq \mathbb{E}</em>\right)\right] \
&amp; =\int_{\times_{\ell=1}^{m} \Omega^{\ell}}\left(\otimes_{\ell=1}^{m} \phi^{\ell}\left(\mathbf{x}^{\ell}\right)\right) \mathrm{d} P\left(\mathbf{x}^{1}, \ldots, \mathbf{x}^{m}\right)
\end{aligned}
$$}^{1: m}}\left[\otimes_{\ell=1}^{m} \phi^{\ell}\left(\mathbf{X}^{\ell</p>
<p>where $\mathbf{X}^{1: m}$ denotes the set of $m$ variables $\left{\mathbf{X}^{1}, \ldots, \mathbf{X}^{m}\right}$ on domain $\times_{\ell=1}^{m} \Omega^{\ell}=\Omega^{1} \times \ldots \times \Omega^{m}, \phi^{\ell}$ is the feature map endowed with kernel $k^{\ell}$ in RKHS $\mathcal{H}^{\ell}$ for variable $\mathbf{X}^{\ell}$, $\otimes_{\ell=1}^{m} \phi^{\ell}\left(\mathbf{x}^{\ell}\right)=\phi^{1}\left(\mathbf{x}^{1}\right) \otimes \ldots \otimes \phi^{m}\left(\mathbf{x}^{m}\right)$ is the feature map in the tensor product Hilbert space, where the inner product satisfies $\left\langle \otimes_{\ell=1}^{m} \phi^{\ell}\left(\mathbf{x}^{\ell}\right), \otimes_{\ell=1}^{m} \phi^{\ell}\left(\mathbf{x}^{\prime \ell}\right)\right\rangle=\prod_{\ell=1}^{m} k^{\ell}\left(\mathbf{x}^{\ell}, \mathbf{x}^{\prime \ell}\right)$. The joint embeddings can be viewed as an uncentered crosscovariance operator $\mathcal{C}<em _mathbf_X="\mathbf{X">{\mathbf{X}^{1: m}}$ by the standard equivalence between tensor and linear map (Song et al., 2010). That is, given a set of functions $f^{1}, \ldots, f^{m}$, their covariance can be computed by $\mathbb{E}</em>}^{1: m}}\left[\prod_{\ell=1}^{m} f^{\ell}\left(\mathbf{X}^{\ell}\right)\right]=\left\langle \otimes_{\ell=1}^{m} f^{\ell}, \mathcal{C<em _mathbf_X="\mathbf{X">{\mathbf{X}^{1: m}}\right\rangle$.
When the true distribution $P\left(\mathbf{X}^{1}, \ldots, \mathbf{X}^{m}\right)$ is unknown, we can estimate its embedding using a finite sample (Song et al., 2013). Given a sample $\mathcal{D}</em>}^{1: m}}=\left{\mathbf{x<em n="n">{1}^{1: m}, \ldots, \mathbf{x}</em>\right)$, the empirical joint embedding (the cross-covariance operator) is estimated as}^{1: m}\right}$ of size $n$ drawn i.i.d. from $P\left(\mathbf{X}^{1}, \ldots, \mathbf{X}^{m</p>
<p>$$
\widehat{\mathcal{C}}<em i="1">{\mathbf{X}^{1: m}}=\frac{1}{n} \sum</em>\right)
$$}^{n} \otimes_{\ell=1}^{m} \phi^{\ell}\left(\mathbf{x}_{i}^{\ell</p>
<p>This empirical estimate converges to its population counterpart with a similar convergence rate as marginal embedding.</p>
<h3>3.2. Maximum Mean Discrepancy</h3>
<p>Let $\mathcal{D}<em 1="1">{\mathbf{X}^{s}}=\left{\mathbf{x}</em>}^{s}, \ldots, \mathbf{x<em s="s">{n</em>}}^{s}\right}$ and $\mathcal{D<em 1="1">{\mathbf{X}^{t}}=\left{\mathbf{x}</em>}^{t}, \ldots, \mathbf{x<em t="t">{n</em>\right)$, respectively. Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) is a kernel two-sample test which rejects or accepts the null hypothesis $P=Q$ based on the observed samples. The basic idea behind MMD is that if the generating distributions are identical, all the statistics are the same. Formally, MMD defines the following difference measure:}}^{t}\right}$ be the sets of samples from distributions $P\left(\mathbf{X}^{s}\right)$ and $Q\left(\mathbf{X}^{t</p>
<p>$$
D_{\mathcal{H}}(P, Q) \triangleq \sup <em _mathbf_X="\mathbf{X">{f \in \mathcal{H}}\left(\mathbb{E}</em>\right)\right]\right)
$$}^{s}}\left[f\left(\mathbf{X}^{s}\right)\right]-\mathbb{E}_{\mathbf{X}^{t}}\left[f\left(\mathbf{X}^{t</p>
<p>where $\mathcal{H}$ is a class of functions. It is shown that the class of functions in an universal RKHS $\mathcal{H}$ is rich enough to distinguish any two distributions and MMD is expressed as the distance between their mean embeddings: $D_{\mathcal{H}}(P, Q)=$ $\left|\mu_{\mathbf{X}^{s}}(P)-\mu_{\mathbf{X}^{t}}(Q)\right|<em _mathcal_H="\mathcal{H">{\mathcal{H}}^{2}$. The main theoretical result is that $P=Q$ if and only if $D</em>(P, Q)=0$ (Gretton et al., 2012).}</p>
<p>In practice, an estimate of the MMD compares the square distance between the empirical kernel mean embeddings as</p>
<p>$$
\begin{aligned}
\widehat{D}<em s="s">{\mathcal{H}}(P, Q) &amp; =\frac{1}{n</em>}^{2}} \sum_{i=1}^{n_{s}} \sum_{j=1}^{n_{s}} k\left(\mathbf{x<em j="j">{i}^{s}, \mathbf{x}</em>\right) \
&amp; +\frac{1}{n_{t}^{2}} \sum_{i=1}^{n_{t}} \sum_{j=1}^{n_{t}} k\left(\mathbf{x}}^{s<em j="j">{i}^{t}, \mathbf{x}</em>\right) \
&amp; -\frac{2}{n_{s} n_{t}} \sum_{i=1}^{n_{s}} \sum_{j=1}^{n_{t}} k\left(\mathbf{x}}^{t<em j="j">{i}^{s}, \mathbf{x}</em>\right)
\end{aligned}
$$}^{t</p>
<p>where $\widehat{D}<em _mathcal_H="\mathcal{H">{\mathcal{H}}(P, Q)$ is an unbiased estimator of $D</em>(P, Q)$.}</p>
<h2>4. Joint Adaptation Networks</h2>
<p>In unsupervised domain adaptation, we are given a source domain $\mathcal{D}<em i="i">{s}=\left{\left(\mathbf{x}</em>}^{s}, \mathbf{y<em i="1">{i}^{s}\right)\right}</em>}^{n_{s}}$ of $n_{s}$ labeled examples and a target domain $\mathcal{D<em j="j">{t}=\left{\mathbf{x}</em>\right}}^{t<em t="t">{j=1}^{n</em>}}$ of $n_{t}$ unlabeled examples. The source domain and target domain are sampled from joint distributions $P\left(\mathbf{X}^{s}, \mathbf{Y}^{s}\right)$ and $Q\left(\mathbf{X}^{t}, \mathbf{Y}^{t}\right)$ respectively, $P \neq Q$. The goal of this paper is to design a deep neural network $\mathbf{y}=f(\mathbf{x})$ which formally reduces the shifts in the joint distributions across domains and enables learning both transferable features and classifiers, such that the target risk $R_{t}(f)=\mathbb{E<em s="s">{(\mathbf{x}, \mathbf{y}) \sim Q}[f(\mathbf{x}) \neq \mathbf{y}]$ can be minimized by jointly minimizing the source risk and domain discrepancy.
Recent studies reveal that deep networks (Bengio et al., 2013) can learn more transferable representations than traditional hand-crafted features (Oquab et al., 2013; Yosinski et al., 2014). The favorable transferability of deep features leads to several state of the art deep transfer learning methods (Ganin \&amp; Lempitsky, 2015; Tzeng et al., 2015; Long et al., 2015; 2016). This paper also tackles unsupervised domain adaptation by learning transferable features using deep neural networks. We extend deep convolutional neural networks (CNNs), including AlexNet (Krizhevsky et al., 2012) and ResNet (He et al., 2016), to novel joint adaptation networks (JANs) as shown in Figure 1. The empirical error of CNN classifier $f(\mathbf{x})$ on source domain labeled data $\mathcal{D}</em>$ is</p>
<p>$$
\min <em s="s">{f} \frac{1}{n</em>}} \sum_{i=1}^{n_{s}} J\left(f\left(\mathbf{x<em i="i">{i}^{s}\right), \mathbf{y}</em>\right)
$$}^{s</p>
<p>where $J(\cdot, \cdot)$ is the cross-entropy loss function. Based on the quantification study of feature transferability in deep convolutional networks (Yosinski et al., 2014), convolutional layers can learn generic features that are transferable across domains (Yosinski et al., 2014). Thus we opt to fine-tune the features of convolutional layers when transferring pretrained deep models from source domain to target domain.</p>
<p>However, the literature findings also reveal that the deep features can reduce, but not remove, the cross-domain distribution discrepancy (Yosinski et al., 2014; Long et al., 2015;</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The architectures of Joint Adaptation Network (JAN) (a) and its adversarial version (JAN-A) (b). Since deep features eventually transition from general to specific along the network, activations in multiple domain-specific layers $\mathcal{L}$ are not safely transferable. And the joint distributions of the activations $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)$ and $Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|}\right)$ in these layers should be adapted by JMMD minimization.
2016). The deep features in standard CNNs must eventually transition from general to specific along the network, and the transferability of features and classifiers decreases when the cross-domain discrepancy increases (Yosinski et al., 2014). In other words, even feed-forwarding the source and target domain data through the deep network for multilayer feature abstraction, the shifts in the joint distributions $P\left(\mathbf{X}^{s}, \mathbf{Y}^{s}\right)$ and $Q\left(\mathbf{X}^{t}, \mathbf{Y}^{t}\right)$ still linger in the activations $\mathbf{Z}^{1}, \ldots, \mathbf{Z}^{|\mathcal{L}|}$ of the higher network layers $\mathcal{L}$. Taking AlexNet (Krizhevsky et al., 2012) as an example, the activations in the higher fullyconnected layers $\mathcal{L}={f c 6, f c 7, f c 8}$ are not safely transferable for domain adaptation (Yosinski et al., 2014). Note that the shift in the feature distributions $P\left(\mathbf{X}^{s}\right)$ and $Q\left(\mathbf{X}^{t}\right)$ mainly lingers in the feature layers $f c 6, f c 7$ while the shift in the label distributions $P\left(\mathbf{Y}^{s}\right)$ and $Q\left(\mathbf{Y}^{t}\right)$ mainly lingers in the classifier layer $f c 8$. Thus we can use the joint distributions of the activations in layers $\mathcal{L}$, i.e. $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)$ and $Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|}\right)$ as good surrogates of the original joint distributions $P\left(\mathbf{X}^{s}, \mathbf{Y}^{s}\right)$ and $Q\left(\mathbf{X}^{t}, \mathbf{Y}^{t}\right)$, respectively. To enable unsupervised domain adaptation, we should find a way to match $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)$ and $Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|}\right)$.</p>
<h3>4.1. Joint Maximum Mean Discrepancy</h3>
<p>Many existing methods address transfer learning by bounding the target error with the source error plus a discrepancy between the marginal distributions $P\left(\mathbf{X}^{s}\right)$ and $Q\left(\mathbf{X}^{t}\right)$ of the source and target domains (Ben-David et al., 2010). The Maximum Mean Discrepancy (MMD) (Gretton et al., 2012), as a kernel two-sample test statistic, has been widely applied to measure the discrepancy in marginal distributions $P\left(\mathbf{X}^{s}\right)$ and $Q\left(\mathbf{X}^{t}\right)$ (Tzeng et al., 2014; Long et al., 2015; 2016). To date MMD has not been used to measure the discrepancy in joint distributions $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)$ and $Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|}\right)$, possibly because MMD has not been directly defined for joint distributions by (Gretton et al., 2012) while in conventional shallow domain adaptation methods the joint distributions are not easy to manipulate and match.</p>
<p>Following the virtue of MMD (5), we use the Hilbert space embeddings of joint distributions (3) to measure the discrepancy of two joint distributions $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)$ and
$Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|}\right)$. The resulting measure is called Joint Maximum Mean Discrepancy (JMMD), which is defined as</p>
<p>$$
D_{\mathcal{L}}(P, Q) \triangleq\left|\mathcal{C}<em _mathbf_Z="\mathbf{Z">{\mathbf{Z}^{s, 1:|\mathcal{L}|}}(P)-\mathcal{C}</em>(Q)\right|}^{t, 1:|\mathcal{L}|}<em t="1">{\otimes</em>
$$}^{|\mathcal{L}|} \mathcal{H}^{\ell}}^{2</p>
<p>Based on the virtue of the kernel two-sample test theory (Gretton et al., 2012), we will have $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)=$ $Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|}\right)$ if and only if $D_{\mathcal{L}}(P, Q)=0$. Given source domain $\mathcal{D}<em s="s">{s}$ of $n</em>}$ labeled points and target domain $\mathcal{D<em t="t">{t}$ of $n</em>}$ unlabeled points drawn i.i.d. from $P$ and $Q$ respectively, the deep networks will generate activations in layers $\mathcal{L}$ as $\left{\left(\mathbf{z<em i="i">{i}^{s 1}, \ldots, \mathbf{z}</em>\right)\right}}^{s|\mathcal{L}|<em s="s">{i=1}^{n</em>}}$ and $\left{\left(\mathbf{z<em j="j">{j}^{t 1}, \ldots, \mathbf{z}</em>\right)\right}}^{t|\mathcal{L}|<em t="t">{j=1}^{n</em>(P, Q)$ is computed as the squared distance between the empirical kernel mean embeddings as}}$. The empirical estimate of $D_{\mathcal{L}</p>
<p>$$
\begin{aligned}
\widehat{D}<em s="s">{\mathcal{L}}(P, Q) &amp; =\frac{1}{n</em>}^{2}} \sum_{i=1}^{n_{s}} \sum_{j=1}^{n_{s}} \prod_{\ell \in \mathcal{L}} k^{\ell}\left(\mathbf{z<em j="j">{i}^{s \ell}, \mathbf{z}</em>\right) \
&amp; +\frac{1}{n_{t}^{2}} \sum_{i=1}^{n_{t}} \sum_{j=1}^{n_{t}} \prod_{\ell \in \mathcal{L}} k^{\ell}\left(\mathbf{z}}^{s \ell<em j="j">{i}^{t \ell}, \mathbf{z}</em>\right) \
&amp; -\frac{2}{n_{s} n_{t}} \sum_{i=1}^{n_{s}} \sum_{j=1}^{n_{s}} \prod_{\ell \in \mathcal{L}} k^{\ell}\left(\mathbf{z}}^{t \ell<em j="j">{i}^{s \ell}, \mathbf{z}</em>\right)
\end{aligned}
$$}^{t \ell</p>
<p>Remark: Taking a close look on the objectives of MMD (6) and JMMD (9), we can find some interesting connections. The difference is that, for the activations $\mathbf{Z}^{\ell}$ in each layer $\ell \in$ $\mathcal{L}$, instead of putting uniform weights on the kernel function $k^{\ell}\left(\mathbf{z}<em j="j">{i}^{\ell}, \mathbf{z}</em>\right)$, which is crucial for domain adaptation. All previous deep transfer learning methods (Tzeng et al., 2014; Long et al., 2015; Ganin \&amp; Lempitsky, 2015; Tzeng et al., 2015; Long et al., 2016) have not addressed this issue.}^{\ell}\right)$ as in MMD, JMMD applies non-uniform weights, reflecting the influence of other variables in other layers $\mathcal{L} \backslash \ell$. This captures the full interactions between different variables in the joint distributions $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)$ and $Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|</p>
<h3>4.2. Joint Adaptation Networks</h3>
<p>Denote by $\mathcal{L}$ the domain-specific layers where the activations are not safely transferable. We will formally reduce the discrepancy in the joint distributions of the activations</p>
<p>in layers $\mathcal{L}$, i.e. $P\left(\mathbf{Z}^{s 1}, \ldots, \mathbf{Z}^{s|\mathcal{L}|}\right)$ and $Q\left(\mathbf{Z}^{t 1}, \ldots, \mathbf{Z}^{t|\mathcal{L}|}\right)$. Note that the features in the lower layers of the network are transferable and hence will not require a further distribution matching. By integrating the JMMD (9) over the domain-specific layers $\mathcal{L}$ into the CNN error (7), the joint distributions are matched end-to-end with network training,</p>
<p>$$
\min <em s="s">{f} \frac{1}{n</em>}} \sum_{i=1}^{n_{s}} J\left(f\left(\mathbf{x<em i="i">{i}^{s}\right), \mathbf{y}</em>(P, Q)
$$}^{s}\right)+\lambda \widehat{D}_{\mathcal{L}</p>
<p>where $\lambda&gt;0$ is a tradeoff parameter of the JMMD penalty. As shown in Figure 1(a), we set $\mathcal{L}={f c 6, f c 7, f c 8}$ for the JAN model based on AlexNet (last three layers) while we set $\mathcal{L}={$ pool $5, f c}$ for the JAN model based on ResNet (last two layers), as these layers are tailored to task-specific structures, which are not safely transferable and should be jointly adapted by minimizing CNN error and JMMD (9).
A limitation of JMMD (9) is its quadratic complexity, which is inefficient for scalable deep transfer learning. Motivated by the unbiased estimate of MMD (Gretton et al., 2012), we derive a similar linear-time estimate of JMMD as follows,</p>
<p>$$
\begin{aligned}
\widehat{D}<em i="1">{\mathcal{L}}(P, Q) &amp; =\frac{2}{n} \sum</em>}^{n / 2}\left(\prod_{\ell \in \mathcal{L}} k^{\ell}\left(\mathbf{z<em 2="2" i="i">{2 i-1}^{s \ell}, \mathbf{z}</em>}^{s \ell}\right)+\prod_{\ell \in \mathcal{L}} k^{\ell}\left(\mathbf{z<em 2="2" i="i">{2 i-1}^{t \ell}, \mathbf{z}</em>\right)\right) \
&amp; -\frac{2}{n} \sum_{i=1}^{n / 2}\left(\prod_{\ell \in \mathcal{L}} k^{\ell}\left(\mathbf{z}}^{t \ell<em 2="2" i="i">{2 i-1}^{s \ell}, \mathbf{z}</em>}^{t \ell}\right)+\prod_{\ell \in \mathcal{L}} k^{\ell}\left(\mathbf{z<em 2="2" i="i">{2 i-1}^{t \ell}, \mathbf{z}</em>\right)\right)
\end{aligned}
$$}^{s \ell</p>
<p>where $n=n_{s}$. This linear-time estimate well fits the minibatch stochastic gradient descent (SGD) algorithm. In each mini-batch, we sample the same number of source points and target points to eliminate the bias caused by domain size. This enables our models to scale linearly to large samples.</p>
<h3>4.3. Adversarial Training for Optimal MMD</h3>
<p>The MMD defined using the RKHS (6) has the advantage of not requiring a separate network to approximately maximize the original definition of MMD (5). But the original MMD (5) reveals that, in order to maximize the test power such that any two distributions can be distinguishable, we require the class of functions $f \in \mathcal{H}$ to be rich enough. Although (Gretton et al., 2012) shows that an universal RKHS is rich enough, such kernel-based MMD may suffer from vanishing gradients for low-bandwidth kernels. Moreover, it may be possible that some widely-used kernels are unable to capture very complex distances in high dimensional spaces such as natural images (Reddi et al., 2015; Arjovsky et al., 2017).
To circumvent the issues of vanishing gradients and non-rich function class of kernel-based MMD (6), we are enlightened by the original MMD (5) which fits the adversarial training in GANs (Goodfellow et al., 2014). We add multiple fullyconnected layers parametrized by $\theta$ to the proposed JMMD (9) to make the function class of JMMD richer using neural
network as shown in Figure 1(b). We maximize JMMD with respect to these new parameters $\theta$ to approach the virtue of the original MMD (5), that is, maximizing the test power of JMMD such that distributions of source and target domains are made more distinguishable (Sriperumbudur et al., 2009). This leads to a new adversarial joint adaptation network as</p>
<p>$$
\min <em _theta="\theta">{f} \max </em>} \frac{1}{n_{s}} \sum_{i=1}^{n_{s}} J\left(f\left(\mathbf{x<em i="i">{i}^{s}\right), \mathbf{y}</em>(P, Q ; \theta)
$$}^{s}\right)+\lambda \widehat{D}_{\mathcal{L}</p>
<p>Learning deep features by minimizing this more powerful JMMD, intuitively any shift in the joint distributions will be more easily identified by JMMD and then adapted by CNN.</p>
<p>Remark: This version of JAN shares the idea of domainadversarial training with (Ganin \&amp; Lempitsky, 2015), but differs in that we use the JMMD as the domain adversary while (Ganin \&amp; Lempitsky, 2015) uses logistic regression. As pointed out in a very recent study (Arjovsky et al., 2017), our JMMD-adversarial network can be trained more easily.</p>
<h2>5. Experiments</h2>
<p>We evaluate the joint adaptation networks with state of the art transfer learning and deep learning methods. Codes and datasets are available at http://github.com/thum1.</p>
<h3>5.1. Setup</h3>
<p>Office-31 (Saenko et al., 2010) is a standard benchmark for domain adaptation in computer vision, comprising 4,652 images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which contain images respectively taken by web camera and digital SLR camera under different settings. We evaluate all methods across three transfer tasks $\mathbf{A} \rightarrow \mathbf{W}, \mathbf{D} \rightarrow \mathbf{W}$ and $\mathbf{W}$ $\rightarrow \mathbf{D}$, which are widely adopted by previous deep transfer learning methods (Tzeng et al., 2014; Ganin \&amp; Lempitsky, 2015), and another three transfer tasks $\mathbf{A} \rightarrow \mathbf{D}, \mathbf{D} \rightarrow \mathbf{A}$ and $\mathbf{W} \rightarrow \mathbf{A}$ as in (Long et al., 2015; 2016; Tzeng et al., 2015).
ImageCLEF-DA ${ }^{1}$ is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which is organized by selecting the 12 common categories shared by the following three public datasets, each is considered as a domain: Caltech-256 (C), ImageNet ILSVRC 2012 (I), and Pascal VOC 2012 (P). There are 50 images in each category and 600 images in each domain. We use all domain combinations and build 6 transfer tasks: $\mathbf{I} \rightarrow \mathbf{P}, \mathbf{P} \rightarrow \mathbf{I}, \mathbf{I} \rightarrow \mathbf{C}, \mathbf{C}$ $\rightarrow \mathbf{I}, \mathbf{C} \rightarrow \mathbf{P}$, and $\mathbf{P} \rightarrow \mathbf{C}$. Different from Office-31 where different domains are of different sizes, the three domains in ImageCLEF-DA are of equal size, which makes it a good complement to Office-31 for more controlled experiments.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We compare with conventional and state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) (Pan et al., 2011), Geodesic Flow Kernel (GFK) (Gong et al., 2012), Convolutional Neural Networks AlexNet (Krizhevsky et al., 2012) and ResNet (He et al., 2016), Deep Domain Confusion (DDC) (Tzeng et al., 2014), Deep Adaptation Network (DAN) (Long et al., 2015), Reverse Gradient (RevGrad) (Ganin \&amp; Lempitsky, 2015), and Residual Transfer Network (RTN) (Long et al., 2016). TCA is a transfer learning method based on MMD-regularized Kernel PCA. GFK is a manifold learning method that interpolates across an infinite number of intermediate subspaces to bridge domains. DDC is the first method that maximizes domain invariance by regularizing the adaptation layer of AlexNet using linear-kernel MMD (Gretton et al., 2012). DAN learns transferable features by embedding deep features of multiple task-specific layers to reproducing kernel Hilbert spaces (RKHSs) and matching different distributions optimally using multi-kernel MMD. RevGrad improves domain adaptation by making the source and target domains indistinguishable for a domain discriminator by adversarial training. RTN jointly learns transferable features and adaptive classifiers by deep residual learning (He et al., 2016).</p>
<p>We examine the influence of deep representations for domain adaptation by employing the breakthrough AlexNet (Krizhevsky et al., 2012) and the state of the art ResNet (He et al., 2016) for learning transferable deep representations. For AlexNet, we follow DeCAF (Donahue et al., 2014) and use the activations of layer $f c 7$ as image representation. For ResNet (50 layers), we use the activations of the last feature layer pool5 as image representation. We follow standard evaluation protocols for unsupervised domain adaptation (Long et al., 2015; Ganin \&amp; Lempitsky, 2015). For both Office-31 and ImageCLEF-DA datasets, we use all labeled source examples and all unlabeled target examples. We compare the average classification accuracy of each method on three random experiments, and report the standard error of the classification accuracies by different experiments of the same transfer task. We perform model selection by tuning hyper-parameters using transfer cross-validation (Zhong et al., 2010). For MMD-based methods and JAN, we adopt Gaussian kernel with bandwidth set to median pairwise squared distances on the training data (Gretton et al., 2012).</p>
<p>We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-provided models of AlexNet (Krizhevsky et al., 2012) and ResNet (He et al., 2016), both are pre-trained on the ImageNet 2012 dataset. We fine-tune all convolutional and pooling layers and train the classifier layer via back propagation. Since the classifier is trained from scratch, we set its learning rate to be 10 times that of the other layers. We use mini-batch stochastic gradient descent (SGD) with momentum of 0.9 and the learning rate annealing strategy in RevGrad (Ganin \&amp; Lempitsky, 2015):
the learning rate is not selected by a grid search due to high computational cost-it is adjusted during SGD using the following formula: $\eta_{p}=\frac{\eta_{0}}{(1+\alpha p)^{2}}$, where $p$ is the training progress linearly changing from 0 to $1, \eta_{0}=0.01, \alpha=10$ and $\beta=0.75$, which is optimized to promote convergence and low error on the source domain. To suppress noisy activations at the early stages of training, instead of fixing the adaptation factor $\lambda$, we gradually change it from 0 to 1 by a progressive schedule: $\lambda_{p}=\frac{2}{1+\exp (-\gamma p)}-1$, and $\gamma=10$ is fixed throughout experiments (Ganin \&amp; Lempitsky, 2015). This progressive strategy significantly stabilizes parameter sensitivity and eases model selection for JAN and JAN-A.</p>
<h3>5.2. Results</h3>
<p>The classification accuracy results on the Office-31 dataset for unsupervised domain adaptation based on AlexNet and ResNet are shown in Table 1. As fair comparison with identical evaluation setting, the results of DAN (Long et al., 2015), RevGrad (Ganin \&amp; Lempitsky, 2015), and RTN (Long et al., 2016) are directly reported from their published papers. The proposed JAN models outperform all comparison methods on most transfer tasks. It is noteworthy that JANs promote the classification accuracies substantially on hard transfer tasks, e.g. $\mathbf{D} \rightarrow \mathbf{A}$ and $\mathbf{W} \rightarrow \mathbf{A}$, where the source and target domains are substantially different and the source domain is smaller than the target domain, and produce comparable classification accuracies on easy transfer tasks, $\mathbf{D} \rightarrow \mathbf{W}$ and $\mathbf{W} \rightarrow \mathbf{D}$, where the source and target domains are similar (Saenko et al., 2010). The encouraging results highlight the key importance of joint distribution adaptation in deep neural networks, and suggest that JANs are able to learn more transferable representations for effective domain adaptation.</p>
<p>The results reveal several interesting observations. (1) Standard deep learning methods either outperform (AlexNet) or underperform (ResNet) traditional shallow transfer learning methods (TCA and GFK) using deep features (AlexNet-fc7 and ResNet-pool5) as input. And traditional shallow transfer learning methods perform better with more transferable deep features extracted by ResNet. This confirms the current practice that deep networks learn abstract feature representations, which can only reduce, but not remove, the domain discrepancy (Yosinski et al., 2014). (2) Deep transfer learning methods substantially outperform both standard deep learning methods and traditional shallow transfer learning methods. This validates that reducing the domain discrepancy by embedding domain-adaptation modules into deep networks (DDC, DAN, RevGrad, and RTN) can learn more transferable features. (3) The JAN models outperform previous methods by large margins and set new state of the art record. Different from all previous deep transfer learning methods that only adapt the marginal distributions based on independent feature layers (one layer for RevGrad and multilayer for DAN and RTN), JAN adapts the joint distribu-</p>
<p>Table 1. Classification accuracy (\%) on Office-31 dataset for unsupervised domain adaptation (AlexNet and ResNet)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">$\mathrm{A} \rightarrow \mathrm{W}$</th>
<th style="text-align: center;">$\mathrm{D} \rightarrow \mathrm{W}$</th>
<th style="text-align: center;">$\mathrm{W} \rightarrow \mathrm{D}$</th>
<th style="text-align: center;">$\mathrm{A} \rightarrow \mathrm{D}$</th>
<th style="text-align: center;">$\mathrm{D} \rightarrow \mathrm{A}$</th>
<th style="text-align: center;">$\mathrm{W} \rightarrow \mathrm{A}$</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AlexNet (Krizhevsky et al., 2012)</td>
<td style="text-align: center;">$61.6 \pm 0.5$</td>
<td style="text-align: center;">$95.4 \pm 0.3$</td>
<td style="text-align: center;">$99.0 \pm 0.2$</td>
<td style="text-align: center;">$63.8 \pm 0.5$</td>
<td style="text-align: center;">$51.1 \pm 0.6$</td>
<td style="text-align: center;">$49.8 \pm 0.4$</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: left;">TCA (Pan et al., 2011)</td>
<td style="text-align: center;">$61.0 \pm 0.0$</td>
<td style="text-align: center;">$93.2 \pm 0.0$</td>
<td style="text-align: center;">$95.2 \pm 0.0$</td>
<td style="text-align: center;">$60.8 \pm 0.0$</td>
<td style="text-align: center;">$51.6 \pm 0.0$</td>
<td style="text-align: center;">$50.9 \pm 0.0$</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: left;">GFK (Gong et al., 2012)</td>
<td style="text-align: center;">$60.4 \pm 0.0$</td>
<td style="text-align: center;">$95.6 \pm 0.0$</td>
<td style="text-align: center;">$95.0 \pm 0.0$</td>
<td style="text-align: center;">$60.6 \pm 0.0$</td>
<td style="text-align: center;">$52.4 \pm 0.0$</td>
<td style="text-align: center;">$48.1 \pm 0.0$</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: left;">DDC (Tzeng et al., 2014)</td>
<td style="text-align: center;">$61.8 \pm 0.4$</td>
<td style="text-align: center;">$95.0 \pm 0.5$</td>
<td style="text-align: center;">$98.5 \pm 0.4$</td>
<td style="text-align: center;">$64.4 \pm 0.3$</td>
<td style="text-align: center;">$52.1 \pm 0.6$</td>
<td style="text-align: center;">$52.2 \pm 0.4$</td>
<td style="text-align: center;">70.6</td>
</tr>
<tr>
<td style="text-align: left;">DAN (Long et al., 2015)</td>
<td style="text-align: center;">$68.5 \pm 0.5$</td>
<td style="text-align: center;">$96.0 \pm 0.3$</td>
<td style="text-align: center;">$99.0 \pm 0.3$</td>
<td style="text-align: center;">$67.0 \pm 0.4$</td>
<td style="text-align: center;">$54.0 \pm 0.5$</td>
<td style="text-align: center;">$53.1 \pm 0.5$</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: left;">RTN (Long et al., 2016)</td>
<td style="text-align: center;">$73.3 \pm 0.3$</td>
<td style="text-align: center;">$\mathbf{9 6 . 8} \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6} \pm 0.1$</td>
<td style="text-align: center;">$71.0 \pm 0.2$</td>
<td style="text-align: center;">$50.5 \pm 0.3$</td>
<td style="text-align: center;">$51.0 \pm 0.1$</td>
<td style="text-align: center;">73.7</td>
</tr>
<tr>
<td style="text-align: left;">RevGrad (Ganin \&amp; Lempitsky, 2015)</td>
<td style="text-align: center;">$73.0 \pm 0.5$</td>
<td style="text-align: center;">$96.4 \pm 0.3$</td>
<td style="text-align: center;">$99.2 \pm 0.3$</td>
<td style="text-align: center;">$72.3 \pm 0.3$</td>
<td style="text-align: center;">$53.4 \pm 0.4$</td>
<td style="text-align: center;">$51.2 \pm 0.5$</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">JAN (ours)</td>
<td style="text-align: center;">$74.9 \pm 0.3$</td>
<td style="text-align: center;">$96.6 \pm 0.2$</td>
<td style="text-align: center;">$99.5 \pm 0.2$</td>
<td style="text-align: center;">$71.8 \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{5 8 . 3} \pm 0.3$</td>
<td style="text-align: center;">$55.0 \pm 0.4$</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: left;">JAN-A (ours)</td>
<td style="text-align: center;">$\mathbf{7 5 . 2} \pm 0.4$</td>
<td style="text-align: center;">$96.6 \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6} \pm 0.1$</td>
<td style="text-align: center;">$\mathbf{7 2 . 8} \pm 0.3$</td>
<td style="text-align: center;">$57.5 \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{5 6 . 3} \pm 0.2$</td>
<td style="text-align: center;">76.3</td>
</tr>
<tr>
<td style="text-align: left;">ResNet (He et al., 2016)</td>
<td style="text-align: center;">$68.4 \pm 0.2$</td>
<td style="text-align: center;">$96.7 \pm 0.1$</td>
<td style="text-align: center;">$99.3 \pm 0.1$</td>
<td style="text-align: center;">$68.9 \pm 0.2$</td>
<td style="text-align: center;">$62.5 \pm 0.3$</td>
<td style="text-align: center;">$60.7 \pm 0.3$</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: left;">TCA (Pan et al., 2011)</td>
<td style="text-align: center;">$72.7 \pm 0.0$</td>
<td style="text-align: center;">$96.7 \pm 0.0$</td>
<td style="text-align: center;">$99.6 \pm 0.0$</td>
<td style="text-align: center;">$74.1 \pm 0.0$</td>
<td style="text-align: center;">$61.7 \pm 0.0$</td>
<td style="text-align: center;">$60.9 \pm 0.0$</td>
<td style="text-align: center;">77.6</td>
</tr>
<tr>
<td style="text-align: left;">GFK (Gong et al., 2012)</td>
<td style="text-align: center;">$72.8 \pm 0.0$</td>
<td style="text-align: center;">$95.0 \pm 0.0$</td>
<td style="text-align: center;">$98.2 \pm 0.0$</td>
<td style="text-align: center;">$74.5 \pm 0.0$</td>
<td style="text-align: center;">$63.4 \pm 0.0$</td>
<td style="text-align: center;">$61.0 \pm 0.0$</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: left;">DDC (Tzeng et al., 2014)</td>
<td style="text-align: center;">$75.6 \pm 0.2$</td>
<td style="text-align: center;">$96.0 \pm 0.2$</td>
<td style="text-align: center;">$98.2 \pm 0.1$</td>
<td style="text-align: center;">$76.5 \pm 0.3$</td>
<td style="text-align: center;">$62.2 \pm 0.4$</td>
<td style="text-align: center;">$61.5 \pm 0.5$</td>
<td style="text-align: center;">78.3</td>
</tr>
<tr>
<td style="text-align: left;">DAN (Long et al., 2015)</td>
<td style="text-align: center;">$80.5 \pm 0.4$</td>
<td style="text-align: center;">$97.1 \pm 0.2$</td>
<td style="text-align: center;">$99.6 \pm 0.1$</td>
<td style="text-align: center;">$78.6 \pm 0.2$</td>
<td style="text-align: center;">$63.6 \pm 0.3$</td>
<td style="text-align: center;">$62.8 \pm 0.2$</td>
<td style="text-align: center;">80.4</td>
</tr>
<tr>
<td style="text-align: left;">RTN (Long et al., 2016)</td>
<td style="text-align: center;">$84.5 \pm 0.2$</td>
<td style="text-align: center;">$96.8 \pm 0.1$</td>
<td style="text-align: center;">$99.4 \pm 0.1$</td>
<td style="text-align: center;">$77.5 \pm 0.3$</td>
<td style="text-align: center;">$66.2 \pm 0.2$</td>
<td style="text-align: center;">$64.8 \pm 0.3$</td>
<td style="text-align: center;">81.6</td>
</tr>
<tr>
<td style="text-align: left;">RevGrad (Ganin \&amp; Lempitsky, 2015)</td>
<td style="text-align: center;">$82.0 \pm 0.4$</td>
<td style="text-align: center;">$96.9 \pm 0.2$</td>
<td style="text-align: center;">$99.1 \pm 0.1$</td>
<td style="text-align: center;">$79.7 \pm 0.4$</td>
<td style="text-align: center;">$68.2 \pm 0.4$</td>
<td style="text-align: center;">$67.4 \pm 0.5$</td>
<td style="text-align: center;">82.2</td>
</tr>
<tr>
<td style="text-align: left;">JAN (ours)</td>
<td style="text-align: center;">$85.4 \pm 0.3$</td>
<td style="text-align: center;">$\mathbf{9 7 . 4} \pm 0.2$</td>
<td style="text-align: center;">$\mathbf{9 9 . 8} \pm 0.2$</td>
<td style="text-align: center;">$84.7 \pm 0.3$</td>
<td style="text-align: center;">$68.6 \pm 0.3$</td>
<td style="text-align: center;">$70.0 \pm 0.4$</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: left;">JAN-A (ours)</td>
<td style="text-align: center;">$\mathbf{8 6 . 0} \pm 0.4$</td>
<td style="text-align: center;">$96.7 \pm 0.3$</td>
<td style="text-align: center;">$99.7 \pm 0.1$</td>
<td style="text-align: center;">$\mathbf{8 5 . 1} \pm 0.4$</td>
<td style="text-align: center;">$\mathbf{6 9 . 2} \pm 0.4$</td>
<td style="text-align: center;">$\mathbf{7 0 . 7} \pm 0.5$</td>
<td style="text-align: center;">$\mathbf{8 4 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 2. Classification accuracy (\%) on ImageCLEF-DA for unsupervised domain adaptation (AlexNet and ResNet)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">$\mathrm{I} \rightarrow \mathrm{P}$</th>
<th style="text-align: left;">$\mathrm{P} \rightarrow \mathrm{I}$</th>
<th style="text-align: left;">$\mathrm{I} \rightarrow \mathrm{C}$</th>
<th style="text-align: left;">$\mathrm{C} \rightarrow \mathrm{I}$</th>
<th style="text-align: left;">$\mathrm{C} \rightarrow \mathrm{P}$</th>
<th style="text-align: left;">$\mathrm{P} \rightarrow \mathrm{C}$</th>
<th style="text-align: left;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AlexNet (Krizhevsky et al., 2012)</td>
<td style="text-align: left;">$66.2 \pm 0.2$</td>
<td style="text-align: left;">$70.0 \pm 0.2$</td>
<td style="text-align: left;">$84.3 \pm 0.2$</td>
<td style="text-align: left;">$71.3 \pm 0.4$</td>
<td style="text-align: left;">$59.3 \pm 0.5$</td>
<td style="text-align: left;">$84.5 \pm 0.3$</td>
<td style="text-align: left;">73.9</td>
</tr>
<tr>
<td style="text-align: left;">DAN (Long et al., 2015)</td>
<td style="text-align: left;">$67.3 \pm 0.2$</td>
<td style="text-align: left;">$80.5 \pm 0.3$</td>
<td style="text-align: left;">$87.7 \pm 0.3$</td>
<td style="text-align: left;">$76.0 \pm 0.3$</td>
<td style="text-align: left;">$61.6 \pm 0.3$</td>
<td style="text-align: left;">$88.4 \pm 0.2$</td>
<td style="text-align: left;">76.9</td>
</tr>
<tr>
<td style="text-align: left;">RTN (Long et al., 2016)</td>
<td style="text-align: left;">$\mathbf{6 7 . 4} \pm 0.3$</td>
<td style="text-align: left;">$81.3 \pm 0.3$</td>
<td style="text-align: left;">$89.5 \pm 0.4$</td>
<td style="text-align: left;">$78.0 \pm 0.2$</td>
<td style="text-align: left;">$62.0 \pm 0.2$</td>
<td style="text-align: left;">$89.1 \pm 0.1$</td>
<td style="text-align: left;">77.9</td>
</tr>
<tr>
<td style="text-align: left;">JAN (ours)</td>
<td style="text-align: left;">$67.2 \pm 0.5$</td>
<td style="text-align: left;">$\mathbf{8 2 . 8} \pm 0.4$</td>
<td style="text-align: left;">$\mathbf{9 1 . 3} \pm 0.5$</td>
<td style="text-align: left;">$\mathbf{8 0 . 0} \pm 0.5$</td>
<td style="text-align: left;">$\mathbf{6 3 . 5} \pm 0.4$</td>
<td style="text-align: left;">$\mathbf{9 1 . 0} \pm 0.4$</td>
<td style="text-align: left;">$\mathbf{7 9 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">ResNet (He et al., 2016)</td>
<td style="text-align: left;">$74.8 \pm 0.3$</td>
<td style="text-align: left;">$83.9 \pm 0.1$</td>
<td style="text-align: left;">$91.5 \pm 0.3$</td>
<td style="text-align: left;">$78.0 \pm 0.2$</td>
<td style="text-align: left;">$65.5 \pm 0.3$</td>
<td style="text-align: left;">$91.2 \pm 0.3$</td>
<td style="text-align: left;">80.7</td>
</tr>
<tr>
<td style="text-align: left;">DAN (Long et al., 2015)</td>
<td style="text-align: left;">$74.5 \pm 0.4$</td>
<td style="text-align: left;">$82.2 \pm 0.2$</td>
<td style="text-align: left;">$92.8 \pm 0.2$</td>
<td style="text-align: left;">$86.3 \pm 0.4$</td>
<td style="text-align: left;">$69.2 \pm 0.4$</td>
<td style="text-align: left;">$89.8 \pm 0.4$</td>
<td style="text-align: left;">82.5</td>
</tr>
<tr>
<td style="text-align: left;">RTN (Long et al., 2016)</td>
<td style="text-align: left;">$74.6 \pm 0.3$</td>
<td style="text-align: left;">$85.8 \pm 0.1$</td>
<td style="text-align: left;">$94.3 \pm 0.1$</td>
<td style="text-align: left;">$85.9 \pm 0.3$</td>
<td style="text-align: left;">$71.7 \pm 0.3$</td>
<td style="text-align: left;">$91.2 \pm 0.4$</td>
<td style="text-align: left;">83.9</td>
</tr>
<tr>
<td style="text-align: left;">JAN (ours)</td>
<td style="text-align: left;">$\mathbf{7 6 . 8} \pm 0.4$</td>
<td style="text-align: left;">$\mathbf{8 8 . 0} \pm 0.2$</td>
<td style="text-align: left;">$\mathbf{9 4 . 7} \pm 0.2$</td>
<td style="text-align: left;">$\mathbf{8 9 . 5} \pm 0.3$</td>
<td style="text-align: left;">$\mathbf{7 4 . 2} \pm 0.3$</td>
<td style="text-align: left;">$\mathbf{9 1 . 7} \pm 0.3$</td>
<td style="text-align: left;">$\mathbf{8 5 . 8}$</td>
</tr>
</tbody>
</table>
<p>tions of network activations in all domain-specific layers to fully correct the shifts in joint distributions across domains. Although both JAN and DAN (Long et al., 2015) adapt multiple domain-specific layers, the improvement from DAN to JAN is crucial for the domain adaptation performance: JAN uses a JMMD penalty to reduce the shift in the joint distributions of multiple task-specific layers, which reflects the shift in the joint distributions of input features and output labels; DAN needs multiple MMD penalties, each independently reducing the shift in the marginal distribution of each layer, assuming feature layers and classifier layer are independent.</p>
<p>By going from AlexNet to extremely deep ResNet, we can attain a more in-depth understanding of feature transferability. (1) ResNet-based methods outperform AlexNet-based methods by large margins. This validates that very deep convolutional networks, e.g. VGGnet (Simonyan \&amp; Zisserman, 2015), GoogLeNet (Szegedy et al., 2015), and ResNet, not only learn better representations for general vision tasks but also learn more transferable representations for domain adaptation. (2) The JAN models significantly outperform ResNet-based methods, revealing that even very deep networks can only reduce, but not remove, the domain discrepancy. (3) The boost of JAN over ResNet is more significant than the improvement of JAN over AlexNet. This implies
that JAN can benefit from more transferable representations.
The great aspect of JAN is that via the kernel trick there is no need to train a separate network to maximize the MMD criterion (5) for the ball of a RKHS. However, this has the disadvantage that some kernels used in practice are unsuitable for capturing very complex distances in high dimensional spaces such as natural images (Arjovsky et al., 2017). The JAN-A model significantly outperforms the previous domain adversarial deep network (Ganin \&amp; Lempitsky, 2015). The improvement from JAN to JAN-A also demonstrates the benefit of adversarial training for optimizing the JMMD in a richer function class. By maximizing the JMMD criterion with respect to a separate network, JAN-A can maximize the distinguishability of source and target distributions. Adapting domains against deep features where their distributions maximally differ, we can enhance the feature transferability.</p>
<p>The three domains in ImageCLEF-DA are more balanced than those of Office-31. With these more balanced transfer tasks, we are expecting to testify whether transfer learning improves when domain sizes do not change. The classification accuracy results based on both AlexNet and ResNet are shown in Table 2. The JAN models outperform comparison methods on most transfer tasks, but by less improvements. This means the difference in domain sizes may cause shift.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. The t-SNE visualization of network activations (ResNet) generated by DAN (a)(b) and JAN (c)(d), respectively.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Analysis: (a) A-distance; (b) JMMD; (c) parameter sensitivity of λ; (d) convergence (dashed lines show best baseline results).</p>
<h3>5.3. Analysis</h3>
<p><strong>Feature Visualization:</strong> We visualize in Figures 2(a)–2(d) the network activations of task <strong>A → W</strong> learned by DAN and JAN respectively using t-SNE embeddings (Donahue et al., 2014). Compared with the activations given by DAN in Figure 2(a)–2(b), the activations given by JAN in Figures 2(c)–2(d) show that the target categories are discriminated much more clearly by the JAN source classifier. This suggests that the adaptation of joint distributions of multilayer activations is a powerful approach to unsupervised domain adaptation.</p>
<p><strong>Distribution Discrepancy:</strong> The theory of domain adaptation (Ben-David et al., 2010; Mansour et al., 2009) suggests A-distance as a measure of distribution discrepancy, which, together with the source risk, will bound the target risk. The proxy A-distance is defined as $$d_A = 2(1 - 2\epsilon)$$, where $$\epsilon$$ is the generalization error of a classifier (e.g., kernel SVM) trained on the binary problem of discriminating the source and target. Figure 3(a) shows $$d_A$$ on tasks <strong>A → W</strong>, <strong>W → D</strong> with features of CNN, DAN, and JAN. We observe that $$d_A$$ using JAN features is much smaller than $$d_A$$ using CNN and DAN features, which suggests that JAN features can close the cross-domain gap more effectively. As domains <strong>W</strong> and <strong>D</strong> are very similar, $$d_A$$ of task <strong>W → D</strong> is much smaller than that of <strong>A → W</strong>, which explains better accuracy of <strong>W → D</strong>.</p>
<p>A limitation of the A-distance is that it cannot measure the cross-domain discrepancy of joint distributions, which is addressed by the proposed JMMD (9). We compute JMMD (9) across domains using CNN, DAN and JAN activations respectively, based on the features in fc7 and ground-truth labels in fc8 (the target labels are not used for model training). Figure 3(b) shows that JMMD using JAN activations is much smaller than JMMD using CNN and DAN activations, which validates that JANs successfully reduce the shifts in joint distributions to learn more transferable representations.</p>
<p><strong>Parameter Sensitivity:</strong> We check the sensitivity of JMMD parameter λ, i.e., the maximum value of the relative weight for JMMD. Figure 3(c) demonstrates the transfer accuracy of JAN based on AlexNet and ResNet respectively, by varying $$\lambda \in {0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}$$ on task <strong>A → W</strong>. The accuracy of JAN first increases and then decreases as λ varies and shows a bell-shaped curve. This confirms the motivation of deep learning and joint distribution adaptation, as a proper trade-off between them enhance transferability.</p>
<p><strong>Convergence Performance:</strong> As JAN and JAN-A involve adversarial training procedures, we testify their convergence performance. Figure 3(d) demonstrates the test errors of different methods on task <strong>A → W</strong>, which suggests that JAN converges fastest due to nonparametric JMMD while JAN-A has similar convergence speed as RevGrad with significantly improved accuracy in the whole procedure of convergence.</p>
<h2>6. Conclusion</h2>
<p>This paper presented a novel approach to deep transfer learning, which enables end-to-end learning of transferable representations. Unlike previous methods that match the marginal distributions of features across domains, the proposed approach reduces the shift in joint distributions of the network activations of multiple task-specific layers, which approximates the shift in the joint distributions of input features and output labels. The discrepancy between joint distributions can be computed by embedding the joint distributions in a tensor-product Hilbert space, which can be scaled linearly to large samples and be implemented in most deep networks. Experiments testified the efficacy of the proposed approach.</p>
<h2>Acknowledgments</h2>
<p>We thank Zhangjie Cao for conducting part of experiments. This work was supported by NSFC (61502265, 61325008), National Key R\&amp;D Program of China (2016YFB1000701, 2015BAF32B01), and Tsinghua TNList Lab Key Projects.</p>
<h2>References</h2>
<p>Arjovsky, Martin, Chintala, Soumith, and Bottou, Léon. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.</p>
<p>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. A theory of learning from different domains. Machine Learning, 79(1-2):151-175, 2010.</p>
<p>Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 35(8):1798-1828, 2013.</p>
<p>Bousmalis, Konstantinos, Trigeorgis, George, Silberman, Nathan, Krishnan, Dilip, and Erhan, Dumitru. Domain separation networks. In Advances in Neural Information Processing Systems (NIPS), pp. 343-351, 2016.</p>
<p>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of Machine Learning Research (JMLR), 12:2493-2537, 2011.</p>
<p>Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning (ICML), 2014.</p>
<p>Duan, L., Tsang, I. W., and Xu, D. Domain transfer multiple kernel learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 34(3):465-479, 2012.</p>
<p>Ganin, Y. and Lempitsky, V. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning (ICML), 2015.</p>
<p>Glorot, X., Bordes, A., and Bengio, Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In International Conference on Machine Learning (ICML), 2011.</p>
<p>Gong, B., Shi, Y., Sha, F., and Grauman, K. Geodesic flow kernel for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.</p>
<p>Gong, B., Grauman, K., and Sha, F. Connecting the dots with landmarks: Discriminatively learning domaininvariant features for unsupervised domain adaptation. In</p>
<p>International Conference on Machine Learning (ICML), 2013.</p>
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014.</p>
<p>Gopalan, R., Li, R., and Chellappa, R. Domain adaptation for object recognition: An unsupervised approach. In IEEE International Conference on Computer Vision (ICCV), 2011.</p>
<p>Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B., and Smola, A. A kernel two-sample test. Journal of Machine Learning Research (JMLR), 13:723-773, 2012.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</p>
<p>Hoffman, J., Guadarrama, S., Tzeng, E., Hu, R., Donahue, J., Girshick, R., Darrell, T., and Saenko, K. LSDA: Large scale detection through adaptation. In Advances in Neural Information Processing Systems (NIPS), 2014.</p>
<p>Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M., and Schölkopf, B. Correcting sample selection bias by unlabeled data. In Advances in Neural Information Processing Systems (NIPS), 2006.</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.</p>
<p>Long, Mingsheng, Cao, Yue, Wang, Jianmin, and Jordan, Michael I. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning (ICML), 2015.</p>
<p>Long, Mingsheng, Zhu, Han, Wang, Jianmin, and Jordan, Michael I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems (NIPS), pp. 136-144, 2016.</p>
<p>Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain adaptation: Learning bounds and algorithms. In Conference on Computational Learning Theory (COLT), 2009.</p>
<p>Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and transferring mid-level image representations using convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.</p>
<p>Pan, S. J. and Yang, Q. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering (TKDE), 22(10):1345-1359, 2010.</p>
<p>Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks (TNN), 22(2):199-210, 2011.</p>
<p>Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N. D. Dataset shift in machine learning. The MIT Press, 2009.</p>
<p>Reddi, Sashank J, Ramdas, Aaditya, Póczos, Barnabás, Singh, Aarti, and Wasserman, Larry A. On the high dimensional power of a linear-time two sample test under mean-shift alternatives. In Artificial Intelligence and Statistics Conference (AISTATS), 2015.</p>
<p>Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010.</p>
<p>Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015 (arXiv:1409.1556v6), 2015.</p>
<p>Smola, Alex, Gretton, Arthur, Song, Le, and Schölkopf, Bernhard. A hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory (ALT), pp. 13-31. Springer, 2007.</p>
<p>Song, L., Huang, J., Smola, A., and Fukumizu, K. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In International Conference on Machine Learning (ICML), 2009.</p>
<p>Song, Le and Dai, Bo. Robust low rank kernel embeddings of multivariate distributions. In Advances in Neural Information Processing Systems (NIPS), pp. 3228-3236, 2013.</p>
<p>Song, Le, Boots, Byron, Siddiqi, Sajid M, Gordon, Geoffrey J, and Smola, Alex. Hilbert space embeddings of hidden markov models. In International Conference on Machine Learning (ICML), 2010.</p>
<p>Song, Le, Fukumizu, Kenji, and Gretton, Arthur. Kernel embeddings of conditional distributions: A unified kernel framework for nonparametric inference in graphical models. IEEE Signal Processing Magazine, 30(4):98-111, 2013.</p>
<p>Sriperumbudur, B. K., Fukumizu, K., Gretton, A., Lanckriet, G., and Schölkopf, B. Kernel choice and classifiability for rkhs embeddings of probability distributions. In Advances in Neural Information Processing Systems (NIPS), 2009.</p>
<p>Sriperumbudur, Bharath K, Gretton, Arthur, Fukumizu, Kenji, Schölkopf, Bernhard, and Lanckriet, Gert RG.</p>
<p>Hilbert space embeddings and metrics on probability measures. Journal of Machine Learning Research (JMLR), 11(Apr):1517-1561, 2010.</p>
<p>Sugiyama, M., Nakajima, S., Kashima, H., Buenau, P. V., and Kawanabe, M. Direct importance estimation with model selection and its application to covariate shift adaptation. In Advances in Neural Information Processing Systems (NIPS), 2008.</p>
<p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
<p>Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. Deep domain confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014.</p>
<p>Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. Simultaneous deep transfer across domains and tasks. In IEEE International Conference on Computer Vision (ICCV), 2015.</p>
<p>Tzeng, Eric, Hoffman, Judy, Saenko, Kate, and Darrell, Trevor. Adversarial discriminative domain adaptation. arXiv preprint arXiv:1702.05464, 2017.</p>
<p>Wang, X. and Schneider, J. Flexible transfer learning under support and model shift. In Advances in Neural Information Processing Systems (NIPS), 2014.</p>
<p>Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems (NIPS), 2014.</p>
<p>Zhang, K., Schölkopf, B., Muandet, K., and Wang, Z. Domain adaptation under target and conditional shift. In International Conference on Machine Learning (ICML), 2013.</p>
<p>Zhong, E., Fan, W., Yang, Q., Verscheure, O., and Ren, J. Cross validation framework to choose amongst models and datasets for transfer learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD), pp. 547-562. Springer, 2010.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://imageclef.org/2014/adaptation&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
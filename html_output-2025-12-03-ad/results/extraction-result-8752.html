<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8752 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8752</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8752</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-268230542</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.00827v1.pdf" target="_blank">Self-Refinement of Language Models from External Proxy Metrics Feedback</a></p>
                <p><strong>Paper Abstract:</strong> It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised fine-tuned model on human annotated data.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8752.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8752.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProMiSe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proxy Metric-based Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, principle-guided self-refinement algorithm that enables an LLM to improve its own responses using external proxy-metric feedback (e.g., ROUGE, WeCheck), principle-specific in-context refinement prompts, best-of-N/rejection sampling, and thresholded acceptance criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-XXL (≈11B); Llama-2-13B-Chat (≈13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned (FLAN-T5-XXL) and chat (Llama-2-13B-Chat) large language models used in few-shot and fine-tuning experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ProMiSe (Proxy Metric-based Self-Refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate-best-of-N initial candidates; score each candidate with a set of external proxy metrics T (e.g., ROUGE-1 recall vs document, ROUGE-L vs document, ROUGE-L vs query, and a factuality reward model WeCheck); if best candidate fails element-wise threshold τ, perform iterative, principle-guided refinement: for each principle in P, call the same LLM with a principle-specific in-context refinement prompt (contrastive exemplars 'worse'→'better'), sample candidates, reject/select via majority metric wins, and accept updates only if weighted per-metric improvements exceed λ; repeat until sufficiency or max iterations J.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Content-grounded question answering; synthetic multi-turn dialogue generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce agent responses grounded in a provided document and conversation history (single-turn QA on MultiDoc2Dial and QuAC; multi-turn synthetic dialogues for fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Across single-turn QA experiments (MultiDoc2Dial and QuAC) ProMiSe yields consistent improvements on multiple automatic metrics (ROUGE-L, BERT-Recall, BERT K-Precision, Recall, K-Precision) when using combined ROUGE+WeCheck proxy metrics; in synthetic multi-turn dialogue experiments, fine-tuning Llama-2-13B-Chat on ProMiSe-refined data produced sizable gains: approximately +6–6.75% absolute increases for BERT-Recall and Recall and approximately +7.5–8% for BERT K-Precision and K-Precision (reported averages across conditions). LLM-as-judge (GPT-4) prefers final (post-refinement) responses over initial responses in MultiDoc2Dial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial (pre-refinement or baseline) generations produce lower automatic metric scores; zero-shot and supervised baselines are reported in tables — fine-tuning on synthetic data generated without refinement yields substantially smaller gains compared to fine-tuning on ProMiSe-refined synthetic dialogues; exact per-model metric tables available in paper (see Table 1, Table 2, Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (in-context exemplars for principle-specific refinement), external proxy metrics for feedback (ROUGE variants, WeCheck reward model), best-of-N sampling + rejection sampling, weighted per-metric improvement checks, and user-specified sufficiency thresholds and weights.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: consistent improvements on multiple automatic metrics across MD2D and QuAC; synthetic-dialog fine-tuning yields +6–8% improvements on several metrics; GPT-4-as-judge evaluation shows higher win rate for final responses in MultiDoc2Dial conditions. Qualitative: iterative per-principle prompts and proxy-metric scoring produced responses judged more specific/faithful/relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: dependence on the quality and design of proxy metrics and thresholds (poor metric choice or thresholds can reduce efficacy); WeCheck-as-sole-metric is less consistent and can prefer longer entailed but incorrect answers (notably in QuAC); generated outputs inherit biases of the open-source LMs (toxicity, stereotypes); experiments only in English and lacking human evaluation (authors note human eval would strengthen claims); general prior observation (cited) that without good stopping mechanisms self-refinement can worsen already high-quality outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with internal LLM self-refinement approaches (e.g., Self-Refine / self-critique that leverage very large LLMs as internal critics): ProMiSe employs external proxy metrics enabling smaller models (FLAN-T5-XXL, Llama-2-13B-Chat) to successfully self-refine. The paper ablates metric sets (ROUGE-only, WeCheck-only, ROUGE+WeCheck) and finds ROUGE+WeCheck combination generally best. Compared qualitatively to methods that use critic models, tool-based critics, or supervised refiners in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>The paper reports metric selection and threshold calibration experiments: thresholds chosen (example: response-document ROUGE-1 Recall 0.02, response-document ROUGE-L 0.05, response-query ROUGE-L 0.05; WeCheck threshold 0.5) and experiments sweeping WeCheck thresholds (0.4–0.8). Results show ProMiSe effective across threshold choices, with ROUGE+WeCheck often giving best performance; using WeCheck alone is less consistent. (Tables and appendices provide detailed numeric sweeps.)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8752.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (Madaan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent line of work where the same LLM generates an output and then critiques/revises it (internal self-feedback), demonstrating refinement benefits when using very large models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / ChatGPT / GPT-4 (as reported in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Very large proprietary instruction/chat LLMs used as both generator and critic in the cited work (mentioned as relying on very large LLM internal feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refinement / internal self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use the same LLM to critique its own outputs and produce refined outputs (generate-then-reflect) relying on internal model capabilities rather than external metric feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General iterative refinement tasks (as reported in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generation tasks where the model both produces and critiques/refines outputs via internal prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Internal-model critique via prompt-based self-critique and revision (no external proxy metrics in the cited approach as described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as prior work demonstrating refinement with large LLMs; the current paper notes that those approaches rely on very large models and that smaller instruction-tuned models often fail to replicate those results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper cites observations from prior work: smaller instruction-tuned models often fail to replicate large-model refinement results; without good stopping mechanisms, self-refinement can degrade already high-quality responses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as a point of contrast: ProMiSe uses external proxy metrics and in-context principle prompts to enable similar self-refinement behavior in smaller open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8752.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Welleck et al. 2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating sequences by learning to self-correct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work on training models to generate sequences while learning to self-correct errors via model-internal procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating sequences by learning to self-correct</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-correction during generation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Learning-to-self-correct paradigm where generation is paired with mechanisms to detect and correct mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Training-time/self-correction procedures (cited only in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned among prior work exploring self-evaluation and self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8752.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang et al. 2023b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enable language models to implicitly learn self-improvement from data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study exploring how language models can learn self-improvement behaviors implicitly from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enable language models to implicitly learn self-improvement from data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Implicit self-improvement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Approaches that induce models to perform self-improvement behaviors by training on data that contains examples of revisions or critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Data-driven implicit learning of refinement behaviors (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited as part of the landscape of self-refinement research.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8752.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shepherd</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shepherd: A critic for language model generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-model-based critic that assesses generations and suggests refinements (tuning a model to critique and propose improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Shepherd: A critic for language model generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Model-as-critic (Shepherd)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Tune a model to critique its own responses and suggest refinements, operating as an explicit critic component.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Train/tune a separate critic model to evaluate and recommend refinements (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Related to external-critic approaches; ProMiSe differs by using external metric feedback rather than a learned critic model.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8752.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REFINER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REFINER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that trains LMs to generate intermediate reasoning steps while interacting with an automated critic that provides feedback on reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REFINER</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critic-guided refinement (REFINER)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Finetune LMs to produce intermediate reasoning and interact iteratively with a critic model that supplies automated feedback for revision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Interaction with a learned critic model that scores/validates intermediate steps; used in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned among external-feedback approaches; ProMiSe uses metric feedback rather than critic-model interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8752.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC (Gou et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC: tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that lets LMs interact with external tools (calculator, search, Wikipedia) to evaluate and then revise outputs based on tool-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Critic: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tool-interactive critiquing (CRITIC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use external tools to check aspects of generated text and then revise outputs based on the validation feedback obtained from those tools.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External tool checks integrated into an iterative revision loop (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>An external-feedback approach that leverages tools rather than proxy metrics; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8752.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RARR (Gao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RARR: Researching and revising what language models say, using language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A revision approach that retrieves relevant evidence and revises generated text based on retrieved evidence and model-guided revision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rarr: Researching and revising what language models say, using language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Retrieval-based revision (RARR)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Retrieve relevant evidence via search, then revise generated text in light of retrieved evidence, often using the model to coordinate research and revision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Retrieval + revision loop; cited as related work that revises outputs based on retrieved supporting evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Related retrieval-and-revise approach; ProMiSe differs by using proxy metrics and in-context principle refinement rather than retrieval-driven evidence revision.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8752.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8752.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Olausson et al. 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Is self-repair a silver bullet for code generation?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis concluding that LLM self-repair on programming problems still underperforms human-level debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is self-repair a silver bullet for code generation?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-repair / self-repair analysis</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Empirical examination of LLMs' ability to self-repair programmatic solutions and their limitations compared to human debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code generation debugging benchmarks (HumanEval, APPS as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation correctness tasks where models attempt to repair failing code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Iterative self-repair attempts on code with model-internal debugging prompts (cited conclusion: still lags behind human debugging).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cited concluding that self-repair lags behind human-level debugging for code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as evidence that self-repair is not universally effective, particularly for code; relevant as a cautionary result in the self-refinement literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used in related work discussion to temper expectations about self-repair efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Generating sequences by learning to self-correct <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Shepherd: A critic for language model generation <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Rarr: Researching and revising what language models say, using language models <em>(Rating: 2)</em></li>
                <li>Is self-repair a silver bullet for code generation? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8752",
    "paper_id": "paper-268230542",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "ProMiSe",
            "name_full": "Proxy Metric-based Self-Refinement",
            "brief_description": "An iterative, principle-guided self-refinement algorithm that enables an LLM to improve its own responses using external proxy-metric feedback (e.g., ROUGE, WeCheck), principle-specific in-context refinement prompts, best-of-N/rejection sampling, and thresholded acceptance criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-XXL (≈11B); Llama-2-13B-Chat (≈13B)",
            "model_description": "Open-source instruction-tuned (FLAN-T5-XXL) and chat (Llama-2-13B-Chat) large language models used in few-shot and fine-tuning experiments reported in this paper.",
            "reflection_method_name": "ProMiSe (Proxy Metric-based Self-Refinement)",
            "reflection_method_description": "Generate-best-of-N initial candidates; score each candidate with a set of external proxy metrics T (e.g., ROUGE-1 recall vs document, ROUGE-L vs document, ROUGE-L vs query, and a factuality reward model WeCheck); if best candidate fails element-wise threshold τ, perform iterative, principle-guided refinement: for each principle in P, call the same LLM with a principle-specific in-context refinement prompt (contrastive exemplars 'worse'→'better'), sample candidates, reject/select via majority metric wins, and accept updates only if weighted per-metric improvements exceed λ; repeat until sufficiency or max iterations J.",
            "task_name": "Content-grounded question answering; synthetic multi-turn dialogue generation",
            "task_description": "Produce agent responses grounded in a provided document and conversation history (single-turn QA on MultiDoc2Dial and QuAC; multi-turn synthetic dialogues for fine-tuning).",
            "performance_with_reflection": "Across single-turn QA experiments (MultiDoc2Dial and QuAC) ProMiSe yields consistent improvements on multiple automatic metrics (ROUGE-L, BERT-Recall, BERT K-Precision, Recall, K-Precision) when using combined ROUGE+WeCheck proxy metrics; in synthetic multi-turn dialogue experiments, fine-tuning Llama-2-13B-Chat on ProMiSe-refined data produced sizable gains: approximately +6–6.75% absolute increases for BERT-Recall and Recall and approximately +7.5–8% for BERT K-Precision and K-Precision (reported averages across conditions). LLM-as-judge (GPT-4) prefers final (post-refinement) responses over initial responses in MultiDoc2Dial conditions.",
            "performance_without_reflection": "Initial (pre-refinement or baseline) generations produce lower automatic metric scores; zero-shot and supervised baselines are reported in tables — fine-tuning on synthetic data generated without refinement yields substantially smaller gains compared to fine-tuning on ProMiSe-refined synthetic dialogues; exact per-model metric tables available in paper (see Table 1, Table 2, Table 6).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering (in-context exemplars for principle-specific refinement), external proxy metrics for feedback (ROUGE variants, WeCheck reward model), best-of-N sampling + rejection sampling, weighted per-metric improvement checks, and user-specified sufficiency thresholds and weights.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: consistent improvements on multiple automatic metrics across MD2D and QuAC; synthetic-dialog fine-tuning yields +6–8% improvements on several metrics; GPT-4-as-judge evaluation shows higher win rate for final responses in MultiDoc2Dial conditions. Qualitative: iterative per-principle prompts and proxy-metric scoring produced responses judged more specific/faithful/relevant.",
            "limitations_or_failure_cases": "Reported limitations include: dependence on the quality and design of proxy metrics and thresholds (poor metric choice or thresholds can reduce efficacy); WeCheck-as-sole-metric is less consistent and can prefer longer entailed but incorrect answers (notably in QuAC); generated outputs inherit biases of the open-source LMs (toxicity, stereotypes); experiments only in English and lacking human evaluation (authors note human eval would strengthen claims); general prior observation (cited) that without good stopping mechanisms self-refinement can worsen already high-quality outputs.",
            "comparison_to_other_methods": "Contrasted with internal LLM self-refinement approaches (e.g., Self-Refine / self-critique that leverage very large LLMs as internal critics): ProMiSe employs external proxy metrics enabling smaller models (FLAN-T5-XXL, Llama-2-13B-Chat) to successfully self-refine. The paper ablates metric sets (ROUGE-only, WeCheck-only, ROUGE+WeCheck) and finds ROUGE+WeCheck combination generally best. Compared qualitatively to methods that use critic models, tool-based critics, or supervised refiners in related work.",
            "ablation_study_results": "The paper reports metric selection and threshold calibration experiments: thresholds chosen (example: response-document ROUGE-1 Recall 0.02, response-document ROUGE-L 0.05, response-query ROUGE-L 0.05; WeCheck threshold 0.5) and experiments sweeping WeCheck thresholds (0.4–0.8). Results show ProMiSe effective across threshold choices, with ROUGE+WeCheck often giving best performance; using WeCheck alone is less consistent. (Tables and appendices provide detailed numeric sweeps.)",
            "uuid": "e8752.0",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Refine (Madaan et al.)",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "A recent line of work where the same LLM generates an output and then critiques/revises it (internal self-feedback), demonstrating refinement benefits when using very large models.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 / ChatGPT / GPT-4 (as reported in the referenced work)",
            "model_description": "Very large proprietary instruction/chat LLMs used as both generator and critic in the cited work (mentioned as relying on very large LLM internal feedback).",
            "reflection_method_name": "Self-refinement / internal self-feedback",
            "reflection_method_description": "Use the same LLM to critique its own outputs and produce refined outputs (generate-then-reflect) relying on internal model capabilities rather than external metric feedback.",
            "task_name": "General iterative refinement tasks (as reported in the cited work)",
            "task_description": "Generation tasks where the model both produces and critiques/refines outputs via internal prompts.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Internal-model critique via prompt-based self-critique and revision (no external proxy metrics in the cited approach as described in this paper).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as prior work demonstrating refinement with large LLMs; the current paper notes that those approaches rely on very large models and that smaller instruction-tuned models often fail to replicate those results.",
            "limitations_or_failure_cases": "The paper cites observations from prior work: smaller instruction-tuned models often fail to replicate large-model refinement results; without good stopping mechanisms, self-refinement can degrade already high-quality responses.",
            "comparison_to_other_methods": "Used as a point of contrast: ProMiSe uses external proxy metrics and in-context principle prompts to enable similar self-refinement behavior in smaller open-source models.",
            "ablation_study_results": null,
            "uuid": "e8752.1",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Welleck et al. 2022",
            "name_full": "Generating sequences by learning to self-correct",
            "brief_description": "Work on training models to generate sequences while learning to self-correct errors via model-internal procedures.",
            "citation_title": "Generating sequences by learning to self-correct",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-correction during generation",
            "reflection_method_description": "Learning-to-self-correct paradigm where generation is paired with mechanisms to detect and correct mistakes.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Training-time/self-correction procedures (cited only in related work).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Mentioned among prior work exploring self-evaluation and self-correction.",
            "ablation_study_results": null,
            "uuid": "e8752.2",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Wang et al. 2023b",
            "name_full": "Enable language models to implicitly learn self-improvement from data",
            "brief_description": "A study exploring how language models can learn self-improvement behaviors implicitly from training data.",
            "citation_title": "Enable language models to implicitly learn self-improvement from data",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Implicit self-improvement learning",
            "reflection_method_description": "Approaches that induce models to perform self-improvement behaviors by training on data that contains examples of revisions or critiques.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Data-driven implicit learning of refinement behaviors (cited as related work).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Cited as part of the landscape of self-refinement research.",
            "ablation_study_results": null,
            "uuid": "e8752.3",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Shepherd",
            "name_full": "Shepherd: A critic for language model generation",
            "brief_description": "A language-model-based critic that assesses generations and suggests refinements (tuning a model to critique and propose improvements).",
            "citation_title": "Shepherd: A critic for language model generation",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Model-as-critic (Shepherd)",
            "reflection_method_description": "Tune a model to critique its own responses and suggest refinements, operating as an explicit critic component.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Train/tune a separate critic model to evaluate and recommend refinements (mentioned in related work).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Related to external-critic approaches; ProMiSe differs by using external metric feedback rather than a learned critic model.",
            "ablation_study_results": null,
            "uuid": "e8752.4",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "REFINER",
            "name_full": "REFINER",
            "brief_description": "A framework that trains LMs to generate intermediate reasoning steps while interacting with an automated critic that provides feedback on reasoning.",
            "citation_title": "REFINER",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Critic-guided refinement (REFINER)",
            "reflection_method_description": "Finetune LMs to produce intermediate reasoning and interact iteratively with a critic model that supplies automated feedback for revision.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Interaction with a learned critic model that scores/validates intermediate steps; used in related work.",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Mentioned among external-feedback approaches; ProMiSe uses metric feedback rather than critic-model interaction.",
            "ablation_study_results": null,
            "uuid": "e8752.5",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CRITIC (Gou et al.)",
            "name_full": "CRITIC: tool-interactive critiquing",
            "brief_description": "A method that lets LMs interact with external tools (calculator, search, Wikipedia) to evaluate and then revise outputs based on tool-based validation.",
            "citation_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Tool-interactive critiquing (CRITIC)",
            "reflection_method_description": "Use external tools to check aspects of generated text and then revise outputs based on the validation feedback obtained from those tools.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "External tool checks integrated into an iterative revision loop (mentioned in related work).",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "An external-feedback approach that leverages tools rather than proxy metrics; cited as related work.",
            "ablation_study_results": null,
            "uuid": "e8752.6",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RARR (Gao et al.)",
            "name_full": "RARR: Researching and revising what language models say, using language models",
            "brief_description": "A revision approach that retrieves relevant evidence and revises generated text based on retrieved evidence and model-guided revision.",
            "citation_title": "Rarr: Researching and revising what language models say, using language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Retrieval-based revision (RARR)",
            "reflection_method_description": "Retrieve relevant evidence via search, then revise generated text in light of retrieved evidence, often using the model to coordinate research and revision.",
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Retrieval + revision loop; cited as related work that revises outputs based on retrieved supporting evidence.",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Related retrieval-and-revise approach; ProMiSe differs by using proxy metrics and in-context principle refinement rather than retrieval-driven evidence revision.",
            "ablation_study_results": null,
            "uuid": "e8752.7",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Olausson et al. 2024",
            "name_full": "Is self-repair a silver bullet for code generation?",
            "brief_description": "An analysis concluding that LLM self-repair on programming problems still underperforms human-level debugging.",
            "citation_title": "Is self-repair a silver bullet for code generation?",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-repair / self-repair analysis",
            "reflection_method_description": "Empirical examination of LLMs' ability to self-repair programmatic solutions and their limitations compared to human debugging.",
            "task_name": "Code generation debugging benchmarks (HumanEval, APPS as cited)",
            "task_description": "Code generation correctness tasks where models attempt to repair failing code.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Iterative self-repair attempts on code with model-internal debugging prompts (cited conclusion: still lags behind human debugging).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cited concluding that self-repair lags behind human-level debugging for code tasks.",
            "limitations_or_failure_cases": "Cited as evidence that self-repair is not universally effective, particularly for code; relevant as a cautionary result in the self-refinement literature.",
            "comparison_to_other_methods": "Used in related work discussion to temper expectations about self-repair efficacy.",
            "ablation_study_results": null,
            "uuid": "e8752.8",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Generating sequences by learning to self-correct",
            "rating": 2,
            "sanitized_title": "generating_sequences_by_learning_to_selfcorrect"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Shepherd: A critic for language model generation",
            "rating": 2,
            "sanitized_title": "shepherd_a_critic_for_language_model_generation"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Rarr: Researching and revising what language models say, using language models",
            "rating": 2,
            "sanitized_title": "rarr_researching_and_revising_what_language_models_say_using_language_models"
        },
        {
            "paper_title": "Is self-repair a silver bullet for code generation?",
            "rating": 1,
            "sanitized_title": "is_selfrepair_a_silver_bullet_for_code_generation"
        }
    ],
    "cost": 0.01864525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Refinement of Language Models from External Proxy Metrics Feedback
27 Feb 2024</p>
<p>Keshav Ramji keshavr@seas.upenn.edu 
University of Pennsylvania</p>
<p>Young-Suk Lee 
IBM Research AI</p>
<p>Ramón Fernandez Astudillo 
IBM Research AI</p>
<p>Md Arafat Sultan 
IBM Research AI</p>
<p>Tahira Naseem tnaseem@us.ibm.com 
IBM Research AI</p>
<p>Asim Munawar 
IBM Research AI</p>
<p>Radu Florian raduf@us.ibm.com 
IBM Research AI</p>
<p>Salim Roukos roukos@us.ibm.com 
IBM Research AI</p>
<p>Baolin Peng 
Michel Galley 
Pengcheng He 
Hao Cheng 
Yujia Xie 
Yu Hu 
Qiuyuan Huang 
Lars Liden 
Zhou Yu 
Weizhu Chen 
Jianfeng 2023 Gao 
Check 
William Saunders 
Catherine Yeh 
Jeff Wu 
Steven Bills 
Jérémy Scheurer 
Jon Ander Campos 
Tomasz Korbak 
Jun Shern Chan 
Angelica Chen 
Kyunghyun Cho 
Ethan Perez 
Training 
Noah Shinn 
Federico Cassano 
Edward Berman 
Ash- Win Gopinath 
Karthik Narasimhan 
Shunyu Yao 
Hugo Touvron 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cristian Canton Ferrer 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
Naman Goyal 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Marie-Anne Lachaux 
Thibaut Lavril 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi </p>
<p>IBM Research AI</p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Angela Fan, Melanie Kambadur, Sharan NarangYuchen Zhang</p>
<p>Aurelien Ro-driguez
Sergey EdunovRobert Stojnic</p>
<p>Thomas Scialom
2023Llama</p>
<p>Self-Refinement of Language Models from External Proxy Metrics Feedback
27 Feb 202477FE523A13309882B25F8AB13C91721FarXiv:2403.00827v1[cs.CL]
It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response.In documentgrounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document.In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response.ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time.We apply ProMiSe to open source language models FLAN-T5-XXL and LLAMA-2-13B-CHAT, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality.We further show that finetuning LLAMA-2-13B-CHAT on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised finetuned model on human annotated data.</p>
<p>Introduction</p>
<p>The state-of-the-art large language models (LLMs) have demonstrated to be effective in generating new synthetic data, useful in improving zero-shot task generalization through fine-tuning without requiring vast amounts of human annotations.Various approaches have been proposed to show the ability of models to evaluate and critique responses (Saunders et al., 2022;Scheurer et al., 2023;Shinn et al., 2023;Ye et al., 2023), as well as their potential to refine: given feedback, correct their outputs (Welleck et al., 2022;Peng et al., 2023;Madaan et al., 2023;Huang et al., 2023;Wang et al., 2023b).These explorations have studied various feedback mechanisms (human-in-the-loop, reward models to capture human preferences, model-generated feedback) and forms (pairwise comparisons, scalar scores, natural language descriptions), as well as refinement techniques (separate supervised refiners, domain-specific refinement).</p>
<p>Of particular note are recent works exploring the self-refinement phenomenon (Madaan et al., 2023;Wang et al., 2023b;Shinn et al., 2023), leveraging the same LLM to perform critique and/or refinement on top of generating responses.The observations of these works unveil shortcomings: smaller instruction-tuned models fail to replicate the results of systems such as GPT-3.5 and GPT-4 in refinement, and in the absence of well-designed stopping mechanisms, self-refinement applied to high-quality responses can make the results worse (Huang et al., 2023).When humans correct themselves, they do it often with one or more objectives in mind, i.e. principles.Such principles may include faithfulness, specificity, safety (i.e.nontoxic), relevance to a question posed, etc. and may vary across tasks -we seek to imbue these aspects into conversational agents, to ensure they are reflected in the agent's responses.</p>
<p>To this effect, we introduce an iterative, principle-guided approach to self-refinement in relatively smaller language models where refinement has previously proven unsuccessful.Our algorithm, termed Proxy Metric-based Self-Refinement (ProMiSe), combines proxy metric thresholding for different principles with independent principlespecific few-shot refinement and best-of-N rejection sampling.This allows for the deliberate selection of task-appropriate metrics with calibrated sufficiency thresholds, and specific prompts better designed to match the instruction-following capabilities of smaller models.In this manner, we perform multi-aspect self-refinement via iterative Figure 1: A high-level overview of our proposed self-refinement algorithm for content-grounded question answering, with both initial response generation and iterative refinement performed with the same Large Language Model M.</p>
<p>single-aspect improvement queries, as opposed to simultaneous refinement on many dimensions.</p>
<p>We apply this method to content-grounded question answering, demonstrating consistent improvements on a diverse set of evaluation metrics for single-turn response generation.We then extend ProMiSe to multi-turn dialogue data generation to generate user queries in addition to agent responses.We fine-tune LLAMA-2-13B-CHAT on the synthetic data, yielding significant improvement over the zero-shot baseline and supervised models solely fined-tuned on human annotations.</p>
<p>Crucially, this approach is built on open-source models and does not rely on propietary models with black-box API access; we note, however, that the proposed algorithm can be directly applied to closed-source models as well.Furthermore, it can be extended to other tasks, provided that proxy metrics can be defined and a few in-context exemplars can be created for the relevant principles.Our key contributions are:</p>
<p>• We introduce a novel domain-agnostic algorithm, ProMiSe, to perform multi-aspect self-refinement on desirable principles for a response through in-context learning, using proxy metrics as external quality feedback.</p>
<p>• ProMiSe is applied to both content-grounded single-turn question answering and multi-turn dialogue generation.Extensive evaluations on MultiDoc2Dial and QuAC datasets with 5 automatic evaluation metrics (RougeL, Bert-Recall, Bert-K-Precision, Recall, K-Precision) as well as LLM-as-judge with GPT-4, demonstrate its effectivenss both in few-shot and fine-tuning settings.We will release both the software and the synthetic dialogue data.</p>
<p>• We analyze the relationship between the change in proxy metric scores and the downstream evaluation metrics, revealing an unsupervised correlation and reinforcing the efficacy of our method.</p>
<p>Algorithm</p>
<p>Given an input, e.g. a document and conversation history, ProMiSe executes three main steps: (i) Generate an initial response, (ii) Obtain external feedback via proxy metrics, and (iii) Refine the response with respect to each principle, if the response is deemed inadequate by the feedback mechanism.The last two steps are run iteratively until the response meets a quality threshold.We present a detailed description of these steps below.</p>
<p>Initial Response Generation</p>
<p>For an input instance, we perform Best-of-N sampling to yield a set of responses, Y 0 , from Lan-
1: Y0 = {yn} N n=1 with yn ∼ pM(y | x, i) 2: y 0 = arg max y∈Y 0 |T | t=1 1(arg max y ′ ∈Y 0 {mt(y ′ , x)} = y) 3: if |T | t=1 1(mt(y 0 , x) ≥ τt) = |T | then
4: return y 0 5: else 6: for iteration j = 0, 1, . . .J: do 7:
Yj+1 = {yp} |P| p=1 with yp ∼ pM(y | y j , x, rp)
8:
y j+1 = arg max y∈Y j+1 |T | t=1 1(arg max y ′ ∈Y j+1 {mt(y ′ , x)} = y) 9: if |T | t=1 1(mt(y j+1 , x) ≥ τp) = |T | then 10: return y j+1 11: else if |T | t=1 wt • 1(mt(y j+1 , x) ≥ mt(y j , x)) then 12: y j+1 = y j 13:
end if 14:</p>
<p>end for 15: return y J 16: end if guage Model M, given the input and an initial generation prompt.The initial generation prompt consists of an instruction and optional in-context demonstrations.The instruction explicitly suggests that a response be generated which reflects desirable principles, the set of which is contained in P. We determine the quality of the sampled responses based on a set of proxy metrics determined a priori, designated as T .We note that the selected metrics should be designed by the user to improve alignment by reflecting the principle set for the response, P, with respect to the current task.As such, each metric m t is also predicated on the inputs provided which may be used as means to assess candidate responses -a text passage or document, conversation history, etc. (thus lending itself to the contentgrounded setting).Each responses in Y 0 is scored with each metric m t in T , and the response with the highest scores on the greatest number of metrics is chosen as the best initial response, y 0 .</p>
<p>Next, we determine the global sufficiency of y 0 as an acceptable response, by comparing the proxy scores element-wise against a threshold τ , consisting of scalar values τ 1 , τ 2 , . . ., τ |T | .τ i is the minimum value such that a response is deemed sufficient, for each metric i in T .If the scores of y 0 exceeds their respective thresholds, for all T components, we return it as the final response.If not (i.e.y 0 fails to clear the threshold on at least one metric), we proceed to the refinement module.</p>
<p>Response Refinement</p>
<p>Our approach to response refinement is predicated on in-context exemplars of principle-specific refinement for the given task.The refinement prompt also contains the previous best response, denoted y jin the first iteration, this is equal to y 0 .Aligning responses with multiple principles (i.e.where P &gt; 1) induces a multi-objective problem; rather than explicitly optimizing across the set simultaneously, we propose deliberate refinement with respect to one principle at a time, selecting an optimal candidate at each iteration based on the proxy metrics.For each iteration in the self-refinement phase, we loop through the set of principles P and generate a set of new responses, with the goal of each resulting response reflecting improvement on its respective principle.In each such query to Language Model M, we introduce a principle-specific refinement prompt, consisting of in-context demonstrations of refinement and an instruction to improve the current best response, both with respect to the current principle.Examples of such prompts are contained in Appendix C.</p>
<p>Determining Improvement We perform rejection sampling, this time on the set of refinement candidates, scoring with each metric in T and selecting the response, y j+1 , with the highest scores on the majority of metrics.The scores of y j+1 , the best refinement candidate, are then compared against the threshold τ .If the scores of y j+1 exceed the threshold on all |T | metrics, then we stop refinement and accept it as the final response.Otherwise, we now compare against the scores of the previous best response, y j .The user assigns weights w = [w 1 , w 2 , . . ., w |T | ] for the respective metrics in T ; these importances should likely be informed by the principles in P which each metric corresponds to, and the user's design goals.Then, given the scores for y j+1 and y j , we compute:
|T | t=1 w t • 1(m t (y j+1 , x) ≥ m t (y j , x))
For each metric in T , the indicator takes on a value of 1 if the new response is an improvement on the previous best response, with respect to that metric, or 0 otherwise, and is weighted by the elements in w.If this sum fails to exceed a user-defined threshold of λ, we do not update the best refinement response for this iteration (i.e.set y j+1 = y j ); else, we proceed to the next refinement iteration, until termination.</p>
<p>Evidence: Question Answering</p>
<p>We apply ProMiSe to content-grounded question answering: given a document and a conversation history, which may consist of a single user utterance (a question posed to the conversational agent) or a multi-turn dialogue between the user and the agent, we seek for the LLM to produce a response to the most recent user query.</p>
<p>Set of Principles</p>
<p>We first identify an appropriate set of principles for the task, which define key characteristics of a good agent response.They are as follows:</p>
<ol>
<li>
<p>Specificity.If an initial response is too vague, this would likely lead to more user interactions asking the agent to make its response more specific.</p>
</li>
<li>
<p>Faithfulness.We suggest that accurate, factual responses are those grounded in the document, and thus should have high (semantic and lexical) overlap with the document.</p>
</li>
<li>
<p>Relevance and Consistency.The conversational agent response should be relevant to the most recent user query, and by induction to the entire conversation history.</p>
</li>
</ol>
<p>In-Context Demonstration Selection</p>
<p>We explore our algorithm through the generation of both a single agent response and an entire multiturn dialogue.We include the algorithm for multiturn dialog generation in Appendix A.</p>
<p>Response and Query Generation.For the generation of an initial response consistent with the content-grounded QA setting, we extract 3 instances from the MultiDoc2Dial (Feng et al., 2021) training data as in-context exemplars; the prompt template is included in Appendix C.This includes the document, conversation history, and the gold response provided by the annotators.The in-context exemplars for query generation work similarly, with 3 demonstrations consisting of different conversation lengths (in number of utterances), but where the last utterance is the final user query.</p>
<p>Principle Refinement.To perform in-context refinement on a particular principle, we similarly take 3 in-context demonstrations from the training dataset, but seek to contrast between a better and worse response, with respect to the principle.</p>
<p>To accomplish this, we manually annotate a worse response for each instance relative to the gold response.In the prompt, we model this as 3 separate utterances: the worse agent response, a user turn probing the agent to improve its response to update along the principle, and another agent utterance containing the better response (i.e. the gold response).To more explicitly suggest the presence of a response quality difference, we include the tags "not {principle}" and "more {principle}", for the two agent turns, respectively, where {principle} is either 'specific', 'relevant', or 'accurate'.</p>
<p>External Proxy Metrics</p>
<p>To capture the aforementioned principles, we define relevant proxy metrics.The proxy metrics should be reflective of response quality improvement along our chosen dimensions and should not directly optimize the final evaluation metrics.ROUGE Metrics.We select three ROUGE metrics intended to correspond to each of the three principles.ROUGE-1 recall between the response and the document mostly represents specificity as more specific answers contain more details from the document.Next, we use ROUGE-L between the response and the document -this primarily addresses faithfulness, as a greater score would suggest a more extractive answer, which is clearly preferable to hallucinated facts.Finally, we compute ROUGE-L between the response and the conversation history to capture consistency between the user query and the response and relevance of the response to the query history.</p>
<p>WeCheck: Factual Consistency Checker.Given a candidate response and the grounding document, WeCheck (Wu et al., 2022) addresses the faithfulness principle.</p>
<p>Our experiments evaluate each model in three thresholding settings: solely using the three aforementioned ROUGE metrics, solely using the WeCheck model, and using a combination of both.If we use only WeCheck, rejection sampling is Table 1: Experimental Results on the MultiDoc2Dial (MD2D) and QuAC test sets, containing 10,204 and 1,000 instances, respectively.Experiments are reported with the Flan-T5-XXL (11B) and Llama-2-13B-Chat models, using 3 Rouge (ROU) measures, the WeCheck reward model (RM), and both in tandem for thresholding."Initial" refers to scoring generations after rejection sampling, while "Final" includes both "sufficient" initial responses and post-refinement responses.In proxy metrics, Rouge-L includes computing between the candidate response with both the grounding document and the given user query, and Rouge-1 is with respect to the document.Highest scores are boldfaced for each model.We decode with sampling method by setting temperature=0.7,top-k=50 and top-p=1.</p>
<p>performed to yield the highest scoring response according to WeCheck and we determine whether a refined response constitutes an improvement solely using the WeCheck scores.If using both ROUGE and WeCheck, a sufficient response must clear the threshold on all four metrics.During refinement, we yield a reward indicator with each category (Rouge and reward model, i.e.WeCheck) which is 1 if deemed to have improved during the present iteration and 0 otherwise, and compute a weighted sum using a user-defined weight vector w.If this sum is greater than 0.5, we update the best response to be the new one, else retain the previous best.</p>
<p>Experimental Results and Discussion</p>
<p>We use two widely-used open-source language models to evaluate our algorithm for contentgrounded question answering, FLAN-T5-XXL Recall, BERTScore K-Precision (K-Prec.hereafter), (Zhang et al., 2020), Recall, and K-Precision.ROUGE-L, BERTScore Recall and Recall measure the agreement between the candidate response and the provided gold response.BERTScore K-Prec.and K-Prec.measure the agreement between the candidate response and the grounding document.We chose Recall and K-Prec.metrics due to their strong correlation with human assessments of instruction-following models in content-grounded QA tasks, (Adlakah et al., 2023).</p>
<p>Single-Turn QA Results</p>
<p>Table 1 presents the results across the three possible metric sets (ROUGE metrics, the WeCheck reward model, and both) as defined in Section 3.3.It can be observed that designating T to be the combination of the three ROUGE metrics and the WeCheck reward model yields improved results across the majority of the metrics.We find that using the WeCheck reward model as the sole sufficiency metric yields less consistent improvement across the set of evaluation metrics, yet boosts performance when applied in tandem with ROUGE metrics.</p>
<p>To identify the appropriate sufficiency threshold for the proxy metrics, we perform a rigorous study of various settings, included in Appendix B. Experiments containing ROUGE metrics maintain a sufficiency threshold of 0.02 for responsedocument Rouge-1 Recall, 0.05 for responsedocument Rouge-L and 0.05 for response-query Rouge-L.Results involving the WeCheck reward model use a threshold of 0.5 between the response and document.In Table 3, we compare the average length (word count) of the initial and final responses, for the Mul-tiDoc2Dial and QuAC datasets.It can be observed that the length of final responses is marginally greater than the average initial response length.This suggests that our performance improvements exhibited in Table 1 are unlikely to be solely a re-sult of longer responses (e.g.reproducing large sections of the document).Simultaneously, our model producing longer responses relative to the gold response likely explains slight declines in Rouge-L scores with both models and both datasets; in particular, Llama-2's responses are much longer than Flan-T5's and the gold response.</p>
<p>Analysis with Proxy Metrics</p>
<p>We explore the relationship between the improvement in the final evaluation metrics and the direction of change on the proxy metrics in ProMiSe.That is, is improvement on the proxy sufficiency metrics during the execution of the algorithm correlated with the downstream evaluation metric improvement from initial to final response?</p>
<p>Count ROU-L Diff.BERT-R Diff.The relationship between the proxy metric scores and the Rouge-L and BERTScore-Recall evaluation metrics is shown in Table 4.We find that the chosen proxy metrics appear to serve as an unsupervised link to the final evaluation metrics.The number of samples that improve for each proxy metric change are roughly similar, a trend noticeable across settings.Furthermore, a greater degree of improvement on proxy metrics (e.g.improving on all three ROUGE metrics) generally corresponds to a larger average improvement (or less negative change) for Rouge-L and BERTScore-Recall with respect to the gold response.This highlights the value of our external metric feedback technique: by optimizing on a scoring scheme while simultaneously preserving the integrity of the downstream evaluation metrics, we can capture a similar notion of response quality and sufficiency.</p>
<p>ROU-Only</p>
<p>Multi-Turn Synthetic Dialogues</p>
<p>We generate synthetic dialogues of varying lengths from Flan-T5-XXL, containing refinement instances: the initial response, a user query to improve the response along a principle, and the refined response.The dialogues are generated from scratch, bootstrapping solely on the grounding documents in MultiDoc2Dial training data.They alternate between user and agent utterances, and consist of 1-3 agent responses (thus containing total 2, 4, or 6 turns).We sampled 10k dialogues with 2 turns, 2k with 4 turns and another 2k with 6 turns.We QLoRA fine-tune (Dettmers et al., 2023) Llama-2-13B-Chat model on these synthetic data.See Section D for details.</p>
<p>The results are shown in Table 2.We observe sizable improvements across all metrics when comparing the performance without refinement, denoted INITIAL, as opposed to with refinement, denoted FINAL.Notably, these improvements are present on both lexical and semantic similarity measures; +6-6.75% for both BERT-Recall and Recall, and +7.5-8% for BERT K-Precision and K-Precision.Furthermore, merging the synthetic data with 38k samples of human annotated data from the Multi-Doc2Dial train set yields improvements over solely training on human annotated data.These results suggest the value of response quality refinement in generating high-quality synthetic data and yielding downstream improvements on evaluation metrics.</p>
<p>LLM-as-a-Judge Evaluation</p>
<p>We also perform automated evaluation with GPT-4 as a judge, (Zheng et al., 2023), which has been shown strongly correlate to human evaluation.Given the initial and final generations, we prompt the model to impartially assess which response is better.We largely adapt the prompts used for MT-bench evaluation in Zheng et al. (2023), which we show in Appendix F. For MultiDoc2Dial, we randomly sample 2,551 of the indices of the test set responses (exactly one quarter), and only With the QuAC dataset, analyze all 1,000 test set instances, likewise evaluating where initial and final responses differ.The results are shown in Figure 2, where the numbers in percentage are the win rate of each response.</p>
<p>We find that GPT-4 deems the final response to be better than the initial response on all conditions for MultiDoc2Dial.The relative outlier is the QuAC dataset with RM-only; this is likely because WeCheck measures entailment rather than agreement.Often the correct short response is less likely to be entailed than an incorrect longer response by the grounding document.However, a higher win rate with the ROUGE + RM combination validates the complementary nature of our proxy metrics.Furthermore, the strong correlation between the automatic evaluation metrics in Table 1 with the GPT-4 evaluation results in Figure 2 evidences the efficacy of our algorithm.</p>
<p>Other Related Work</p>
<p>Various work on self-refinement may be distinguished according to the source of feedback, (Pan et al., 2023;Huang et al., 2023).Internal feed-back relies on the model's inherent knowledge and parameters to reassess its outputs.External feedback incorporates inputs from humans, other models.Our work is inspired by (Madaan et al., 2023).Unlike Madaan et al. (2023), however, who rely on very large LLMs (GPT-3.5,ChatGPT, GPT-4) as the source of internal feedback, we introduce external feedback with proxy metrics and enable self-refinement technique to work with relatively small LLMs including Flan-T5-XXL and Llama-2-13B-Chat in content-grounded setups.</p>
<p>Regarding internal feedback, (Bai et al., 2022) experiment with method for training a harmless AI assistant through self-improvement.(Wang et al., 2023a) propose Shepherd, a language model tuned to critique its own responses and suggest refinements.As for external feedback, (Paul et al., 2023) propose REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning.(Gou et al., 2023) propose CRITIC that interacts with appropriate tools, e.g.calculator, search engine, wikipedia, etc., to evaluate certain aspects of the text and then revise the output based on the feedback obtained during the validation process.(Olausson et al., 2024) critically examines the LLM's ability to perform self-repair on problems taken from Hu-manEval and APPS and concludes that self-repair still lags behind what can be achieved with humanlevel debugging.(Gao et al., 2023) propose RARR (Retrofit Attribution using Research and Revision) that revises a generated text on the basis of the relevant evidence retrieved by re-search.</p>
<p>Conclusion</p>
<p>We present a novel algorithm, ProMiSe, for selfrefinement of language models.ProMiSe uses external multi-aspect feedback via proxy metrics capturing desirable principles for a high-quality response.ProMiSe is applied to content-grounded single-turn question answering and multi-turn dialogue generation.Extensive evaluations on Mul-tiDoc2Dial and QuAC datasets with 5 automatic evaluation metrics as well as LLM-as-a-judge with GPT-4, demonstrate its effectiveness in both fewshot learning and supervised fine-tuning setups.This approach crucially enables relatively small LMs like Flan-T5-XXL and Llama-2-13B-Chat to successfully perform self-refinement.</p>
<p>Limitations</p>
<p>Our work employs two open-source LMs: FLAN-T5-XXL and LLAMA-2-13B-CHAT.Therefore, the generated data, including the synthetic multi-turn dialogues, can be susceptible to the limitations of such LMs, particularly the biases inherent in the training data which may be harmful with hate, abuse and social stereotypes.We have tested the algorithm ProMiSe on English only although it would have been more desirable to verify the value of the algorithm in multi-lingual setups.We have conducted extensive evaluations including 5 wellknown automtic evaluation metrics and LLM-as-ajudge with GPT-4, which has been shown to correlate well with human evaluations.Nonetheless, inclusion of human evaluation would have strengthened our position further.</p>
<p>Ethics and Impact</p>
<p>Our technique can be used to guide generations towards user-specified targets; however, this could be applied to generate toxic or malicious content, by way of an adversarial principle selection.Nonetheless, we note that ProMiSe does present meaningful implications in enabling alignment to human preferences (where preferences, in this setting, refer to the user-defined principles).We will release the software for the ProMiSe algorithm, enabling others in the community to consider other principles of interest, or applications to other tasks.2022) model.Note that Rouge-L between response and document (Rouge-L-Doc) as well as between response and the user query (Rouge-L-Query) are maintained constant, while we experiment with changing the third metric between Rouge-1 F1, Rouge-1 K-Precision, and Rouge-1 Recall.We also vary the threshold for the WeCheck reward model (Wu et al., 2022), in isolation and in tandem with the best performing Rouge metric combination.</p>
<p>B Metric Selection and Threshold Calibration</p>
<p>ROUGE Metric</p>
<p>Thresholding.We explore a plethora of different thresholding settings to calibrate sufficiency and select the metric set T accordingly.Note that "Rouge-1 K-Prec.≥ 0.7", refers to using Rouge-1 K-Precision with a threshold of 0.7, and Rouge-L F1 of the response with respect to both the document and the user query with a threshold of 0.05.Then, maintaining the latter two with the same configuration, we vary the first metric, exploring the use of a Rouge-1 K-Precision measure in place of recall and experimenting with thresholds of 0.7, 0.8, and 0.9.</p>
<p>WeCheck and Combo Thresholding.We also vary the threshold for the WeCheck reward model when applied as a standalone sufficiency metric, from 0.4 to 0.8 in increments of size 0.1.Finally, using the chosen combination of the three ROUGE metrics (i.e.Rouge-1 Recall with 0.02, Rouge-L F1 with 0.05 for both response-document and response-query comparison), we vary the WeCheck threshold, yielding a very interesting set of results.Across the majority of metrics, our self-refinement is effective for all thresholding methods applied, although it manifests to different degrees depending on the set of proxy metrics.Notably, the similarity in performance across threshold levels (i.e.not exhibiting a clear trend correlating to an increase in threshold) allows our algorithm to more effectively serve as a means of user-defined risk control with respect to a target refinement rate α.As noted in Section 2, a greater threshold results in a higher standard for the initial response to meet, thus yielding a higher rate of refinement as more responses are deemed inadequate.</p>
<p>C Initial Response Generation, Query Generation, and Principle Refinement Prompts</p>
<p>C.1 Initial Generation Prompt</p>
<p>Provided is a dialog between two speakers, User and Agent.Generate a response that is coherent with the dialog history and the provided document.Desired traits for responses are: 1) Specific -The response contains specific content, and 2) Accurate -The response is correct and factual with respect to the document.</p>
<p>C.2 Query Generation Prompt for Synthetic Dialogue Generation</p>
<p>Provided is a dialog between two speakers, User and Agent.Generate a new question, posed by the user, that is coherent with the dialog history and contains specfic content.</p>
<p>document: \n\nBenefits Planner: Family Benefits \nWhen you start receiving disability benefits , certain members of your family may also qualify for benefits on your record.Benefits may be paid to your : spouse; divorced spouse ; children; disabled child ; and adult child disabled before age 22. . . .Our instruction explicitly suggests to improve on specificity, and that in the provided in-context exemplars, the latter response (Agent response 2) is a specificity improvement over the former response (Agent response 1).Notably, we demonstrate to the model that "Let's make this response more specific" is an utterance in between the worse and better responses.Each exemplar and the "more specific" (gold) response is derived from the MultiDoc2Dial train set, while the "not specific" response is developed by a human annotator, bootstrapping off the gold response.The three exemplars are separated by "###", and the above prompt omits the current instance inputs.</p>
<p>Find out more about</p>
<p>D Fine-tuning Experimental Setup</p>
<p>We QLoRA fine-tune LLAMA-2-13B-CHAT on both human annotated and synthetically generated Multi-Doc2Dial datasets.We set the learning rate to 1e-5, LoRA rank to 8 and LoRA alpha to 32.We apply 4bit quantization for both model training and inferencing.Unlike baseline model inferencing with few-shot learning for which we use sampling method, we use greedy decoding for fine-tuned models.We train the models with 4 A100 (80GB memory) GPUs up to 10 epochs.Training takes between 5 hours for 8k training samples and 24 hours for about 50k samples.We select the best checkpoint on the basis of the 5 evaluation metrics (RougeL, BERTScore Recall, BERTScore K-Prec., Recall and K-Prec.)scores on the development test data.1; this table presents a comparison between 0-shot and 3-shot performance, for each metric set.</p>
<p>E Zero-Shot vs Few-Shot Comparison</p>
<p>In Table 6, we also present a comparison based on the number of few-shot exemplars employed in initial response generation.Notably, we observe that the 0-shot performance of initial responses are, in fact, higher than the 3-shot results for the same phase.This suggests that instruction-tuned LMs such as Flan-T5-XXL are already fairly adept at dialogue response generation without in-context exemplars.Furthermore, we find that the zero-shot setting achieves higher initial scores relative to 3-shot for Flan-T5-XXL, but such improvement is less consistent for Llama-2-13B-Chat.Simultaneously, we found that the 3-shot results with refinement constitute an improvement over the 0-shot performance.This indicates that in-context exemplars are necessary to improve performance during the refinement phase, although only three demonstrations are sufficient to illustrate the notion of the target principle to the LM.That is, despite fairly coherent initial responses, there is still room for improvement, achieved when using three in-context exemplars per principle.Thus, the results reported above and in Table 1 hold 3 exemplars constant for the refinement phase.</p>
<p>F LLM-as-a-Judge Evaluation Setup</p>
<p>Recent literature (Zheng et al., 2023;Zhang et al., 2024) evidences the ability of using language models as discriminators, judging generation quality in lieu of (or as a supplement to) human evaluation feedback.The line of work has also been a strong motivator in influencing self-feedback and refinement approaches; it demonstrates the ability of powerful models to reflect human preferences and provide meaningful critiques.In pursuing such an approach, we deliberately choose to explicitly model certain properties in the instruction: for example, we seek permutation-invariance (while knowing that models are susceptible to position bias when given a set of multiple choice options and mitigating their preference for longer answers.</p>
<p>F.1 Judge Prompt Template</p>
<p>Please act as an impartial judge and evaluate the quality of the responses provided by the two AI assistants to the user question displayed below.Your evaluation should consider correctness and helpfulness.You will be given a reference document, a user conversation, assistant A's answer, and assistant B's answer.Your job is to evaluate which assistant's answer is better based on the information in the reference document and the user conversation so far.Begin your evaluation by comparing both assistants' answers with the document and the user conversation so far.Identify and correct any mistakes.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.After providing your explanation, output your final verdict by strictly following this format: "</p>
<p>Figure 2 :
2
Figure 2: GPT-4-as-a-Judge results on Flan-T5-XXL for MultiDoc2Dial (MD2D) and QuAC.With 2551 randomly sampled instances from the MultiDoc2Dial test set, we examine those for which the initial and final response differ: 495 samples for ROUGE-only, 131 samples for RM-only (WeCheck), and 504 samples for ROUGE + RM.We perform a similar analysis with all 1000 QuAC test set instances; the respective counts are: 193 samples for ROUGE-only, 65 samples for RM-only, and 224 samples for ROUGE + RM. perform evaluation on samples for which the initial and final responses differ as a result of refinement.With the QuAC dataset, analyze all 1,000 test set instances, likewise evaluating where initial and final responses differ.The results are shown in Figure 2, where the numbers in percentage are the win rate of each response.We find that GPT-4 deems the final response to be better than the initial response on all conditions for MultiDoc2Dial.The relative outlier is the QuAC dataset with RM-only; this is likely because WeCheck measures entailment rather than agreement.Often the correct short response is less likely to be entailed than an incorrect longer response by the grounding document.However, a higher win rate with the ROUGE + RM combination validates the complementary nature of our proxy metrics.Furthermore, the strong correlation between the automatic evaluation metrics in Table1with the GPT-4 evaluation results in Figure2evidences the efficacy of our algorithm.</p>
<p>Figure 3 :
3
Figure3: Above is the initial generation prompt, containing the instruction and the three in-context exemplars drawn from the train set of MultiDoc2Dial(Feng et al., 2021), omitting the current sample inputs (document and context).The exemplars demonstrate question answering given the conversation history, and are separated by "###".</p>
<p>Figure 4 :
4
Figure4: Query generation prompt (q in Appendix A's algorithm), containing an instruction and three in-context demonstrations of user queries given a document (separated by "###"), omitting the current instance inputs.</p>
<p>Figure 5 :
5
Figure5: Refinement prompt (r p ) with respect to the specificity principle, with an instruction and three in-context demonstrations.Our instruction explicitly suggests to improve on specificity, and that in the provided in-context exemplars, the latter response (Agent response 2) is a specificity improvement over the former response (Agent response 1).Notably, we demonstrate to the model that "Let's make this response more specific" is an utterance in between the worse and better responses.Each exemplar and the "more specific" (gold) response is derived from the MultiDoc2Dial train set, while the "not specific" response is developed by a human annotator, bootstrapping off the gold response.The three exemplars are separated by "###", and the above prompt omits the current instance inputs.</p>
<p>Figure 6 :
6
Figure6: LLM-as-a-Judge prompt template for automated response evaluation between initial and final generations in the content-grounded question answering setting.A document and conversation are given as an input for each sample, and we compare two possible agent responses to the most recent user query posed in the conversation.</p>
<p>Table 2 :
2
Effectiveness of the proposed refinement algorithm measured by the synthetic data qualities.We QLoRA fine-tune LLAMA-2-13B-CHAT model on the two sets of synthetic multi-turn dialogues, one generated with the refinement algorithm denoted by Synset x -FINAL, and the other generated without the refinement algorithm denoted by Synset x -INITIAL.Synset 1 includes 8k and Synset 2 , 10k samples of 2 turn dialogues.Synset 3 includes 10k samples of 2 turn, 2k samples of 4 turn, and 2k samples of 6 turn dialogues.The upper portion of the table compares the performance of the model fine-tuned on the synthetic data with the highest-scoring baseline without fine-tuning.The lower portion of the table compares the performance of the model fine-tuned on the combination of human annotated and synthetic data with the model fine-tuned on human annotated data only.
Both datasets feature conversations wherein an-swers to queries posed by the user are expected tocome from a document. 1Evaluation Metrics We use five automatic evalu-ation metrics: ROUGE-L (Lin, 2004), BERTScore
(Choi et al., 2018))and LLAMA-2-13B-CHAT(Touvron et al., 2023)Evaluation Datasets.We evaluate the technique on the test dataset of MultiDoc2Dial(Feng et al., 2021)(https://doc2dial.github.io/multidoc2dial/),content-groundeddialogues,and the validation dataset of QuAC(Choi et al., 2018)(https://quac.ai/),short form question-answering.</p>
<p>Table 3 :
3
Average word token counts for initial and final generations with ProMiSe.Statistics are computed for the three different settings of the proxy metric set, T ; RM is the WeCheck reward model.</p>
<p>Table 4 :
4
Analysis of the correlation between improvement on proxy ROUGE (ROU) and WeCheck reward model (RM) metrics with change in the final evaluation ROUGE-L and BERT-Recall with the gold response.Performed with Flan-T5-XXL on a 2,038 sample Mul-tiDoc2Dial development set.Proxy metric scores are computed between the candidate and either the provided document or context.↑ and ↓ represents improvement and decline, respectively."2 ↑" means that two of the proxy ROUGE metric set improved.The differences reported are averaged across the sample count.</p>
<p>Table 5 :
5
Threshold calibration and metric selection was performed on a development (validation) set split of the MultiDoc2Dial dataset, consisting of 2,038 samples.Experiments are reported with the Flan-T5-XXL (Chung et al.,</p>
<p>document: DIAL-IN search accounts#3_0Log On to DIAL -IN [1 ] \n\nWhat business records must I keep to document the searches I perform?\nThe business records you keep must exist prior to the search you perform and must establish the business purpose of the search.. . .To verify your browser is compatible to continue using any of the state's government websites, please visit https://encryption.ny.gov/ [6].If your browser is not currently compatible , please update it to the newest version.Find out more about Benefits For A Disabled Child.\n\nPublications \nDisability Benefits Benefits For Children What You Need To Know When You Get Social Security Disability Benefits Information for Government Employees Benefits For Children With Disabilities context: User: Oh, hi.Please, i'm looking for some info about family benefits.could you help me out?Agent: Are you currently receiving any disability benefits?User: Yeah, i started to receive it recently.Agent: Well, in that case, i can tell you that some members of your family may also qualify to get benefits on your record.Project 112 or Project SHAD \nIf you were a part of chemical and biological warfare testing through Project 112 or Project Shipboard Hazard and Defense SHAD , you may be at risk for certain illnesses.The Department of Defense s Deseret Test Center in Fort Douglas, Utah, conducted this testing, which took place aboard ships and on land in various locations from 1962 to 1974.Find out if you can get disability compensation or benefits.\n\nCan I get disability benefits from VA? \nYou may be able to get disability benefits if you meet both of the requirements listed below. . . .Get declassified Department of Defense fact sheets If you have a question about the tests , if you have any information that can help show you were part of them including whether you may have been part of them or contact the Department of Defense at 800 -497 -6261.
. . .document: \n\nBenefits Planner: Family Benefits \nWhen you start receivingdisability benefits , certain members of your family may also qualify for benefitson your record. Benefits may be paid to your : spouse; divorced spouse ; children;disabled child ; and adult child disabled before age 22.. . .###Agent: then, your deposit will be added to your new account balance. User: Can you tell me some of the organizations that are exempt from the search fees? Agent: Some of the exempted organizations are any public organization, its officers, a volunteer fire company, volunteer ambulance service, etc. These organizations are exempt from the search fee. User: What to do in case none of the users performed a search that the DMV contacted me for? Agent: You should contact the DMV immediately. User: Why would the DMV contact me about a search? Agent: The DMV may contact you to ask you about a search to make sure you comply with the Dial-In Terms of Service. ### . . . document: \n\nExposure through context: User: I wanted more information on VA benefits and project 112Agent: Were you part of chemical and biological warfare testing through Project 112or Project Shipboard Hazard and Defense SHAD?
context: User: I need to know how to pay the dial-in search account fees.Agent: The custoers must pay a deposit with the application, and it should be enough to pay for two months of searches.Was your application accepted?User: Yes, it was.</p>
<p>Benefits For A Disabled Child.\n\nPublications \nDisability Benefits Benefits For Children What You Need To Know When You Get Social Security Disability Benefits Information for Government Employees Benefits For Children If your service member is part of the Public Health Service , you ll need to fill out the Spouse Coverage Election and Certificate SGLV 8286A and have them turn it in to their unit s personnel officer.Download the Spouse Coverage Election and Certificate PDF context: User: How much will my service member pay for dependent coverage?Agent: Nothing.sWe provide dependent coverage at no cost until the child is 18 years old , or sometimes longer if the child meets one of the requirements listed below User: To continue receiving dependent coverage after age 18, what are the requirements?
. . .context: User: I have a restricted use license issued in NJ and need informationabout driving in NY.Agent: Do you meet NY requirements for obtaining a restricted license?User: Yes, I do.Agent: Great. You can receive a restricted license to drive in NY. The restrictionswill be the same as the same as the restrictions for a driver with a NY driver license.User: Where can I apply for the restricted driver license?###document: \n\nFamily Servicemembers Group Life Insurance (FSGLI) \nFamily SGLI, alsoknown as Family Servicemembers Group Life Insurance FSGLI, offers coverage for thespouse and dependent children of service members covered under full -time SGLI.With Disabilities. . .context: User: Oh, hi. Please, i'm looking for some info about family benefits.could you help me out?Agent: Are you currently receiving any disability benefits?User: Yeah, i started to receive it recently.###document: NY State Adventure License FAQs#3_0\n\n7. Is there an additional fee tohave icons added to my DMV photo document? \nThere are no additional fees if yourequest the icons be added at the time of your photo document renewal.. . .For Boating Safety Certificate and Empire Passport holders , contact Parks via theirwebsite: www.parks.ny.gov [3]. For Lifetime Sportsman, Small / Big Game, BowHunting, Trapping, Muzzle Loading, or Fishing, contact DEC via their website :www.dec.ny.gov [4].. . .</p>
<p>Specificity Principle Refinement Prompt with In-Context ExemplarsWe want to improve the previous response to make it more specific.To aid in this process, we provide examples of incremental improvement on specific, where Agent response 2 is more specific than Agent response 1.To verify your browser is compatible to continue using any of the state's government websites , please visit https://encryption.ny.gov/[6].If your browser is not currently compatible , please update it to the newest version.context: User: I need to know how to pay the dial-in search account fees.Agent: The custoers must pay a deposit with the application, and it should be enough to pay for two months of searches.Agent: Was your application accepted?User: Yes, it was.Agent: then, your deposit will be added to your new account balance.User: Can you tell me some of the organizations that are exempt from the search fees?Agent: Some of the exempted organizations are any public organization, its officers, a volunteer fire company, volunteer ambulance service, etc.These organizations are exempt from the search fee.
. . . document: \n\nBenefits Planner: Family Benefits \nWhen you start receiving C.4 document: DIAL-IN search accounts#3_0Log On to DIAL -IN [1 ] \n\nWhat business disability benefits , certain members of your family may also qualify for benefitsrecords must I keep to document the searches I perform? \nThe business records you on your record. Benefits may be paid to your : spouse; divorced spouse ; children;keep must exist prior to the search you perform and must establish the business disabled child ; and adult child disabled before age 22.purpose of the search.context: User: Oh, hi. Please, i'm looking for some info about family benefits. . . . could you help me out?Agent: Are you currently receiving any disability benefits?User: Yeah, i started to receive it recently.Agent response 1 (not specific): Then, some others may qualify for benefits.Let's make this response more specific.Agent response 2 (more specific): Well, in that case, i can tell you that somemembers of your family may also qualify to get benefits on your record.###document: \n\nExposure through Project 112 or Project SHAD \nIf you were a part ofchemical and biological warfare testing through Project 112 or Project ShipboardHazard and Defense SHAD , you may be at risk for certain illnesses.User: What to do in case none of the users performed a search that the DMV contacted me for? . . .Agent: You should contact the DMV immediately. Get declassified Department of Defense fact sheets If you have a question about theUser: Why would the DMV contact me about a search? tests , if you have any information that can help show you were part of them includingAgent response 1 (not specific): The DMV may contact you about a search to ensure whether you may have been part of them or contact the Department of Defense at C.3 Principle Refinement Prompt Template compliance. 800 -497 -6261.document: {document} Let's make this response more specific. context: User: I wanted more information on VA benefits and project 112Agent response 1 (not specific): Were you part of Project 112 or Project SHAD?context: {context} Agent response 2 (more specific): The DMV may contact you to ask you about a search toAgent response 1 (not {principle}): {less_principle_response} make sure you comply with the Dial-In Terms of Service. Let's make this response more specific.Let's make this response more {principle}. ### Agent response 2 (more specific): Were you part of chemical and biological warfareAgent response 2 (more {principle}): {more_principle_response} . . . testing through Project 112 or Project Shipboard Hazard and Defense SHAD?</p>
<p>Table 6 :
6
Experimental Results on MultiDoc2Dial test set, containing 10,204 instances.Experiments are reported with the Flan-T5-XXL and Llama-2-13B-Chat models, using 3 Rouge (ROU) measures, the WeCheck reward model (abbreviated as RM), and both in tandem for sufficiency thresholding.Highest scores are boldfaced for each model.The zero-shot results are the same as those included in Table
INITIAL EXEMPLARS + METRICSSTAGEROUGE-L BERT-RECALL BERT K-PREC. RECALL K-PREC.FLAN-T5-XXL (11B) RESULTS3-SHOT / ONLY ROU-L + ROU-1INITIAL21.5027.2738.0831.1975.58FINAL21.8428.9641.4133.9678.780-SHOT / ONLY ROU-L + ROU-1INITIAL21.5528.1140.4232.3476.77FINAL21.7229.2942.7434.1479.293-SHOT / ONLY RMINITIAL22.6728.8042.8333.0080.42FINAL22.6528.9143.5533.3181.140-SHOT / ONLY RMINITIAL22.3328.9144.5833.6181.29FINAL22.4329.1745.6034.2082.253-SHOT / ROU + RMINITIAL22.6128.7542.6032.7580.36FINAL22.7129.8944.8634.7681.980-SHOT / ROU + RMINITIAL22.3028.9444.5533.6881.56FINAL22.3830.1046.6035.5883.13LLAMA-2-13B-CHAT RESULTS3-SHOT / ONLY ROU-L + ROU-1INITIAL20.6329.3534.3236.4971.03FINAL20.2330.2236.0738.8272.740-SHOT / ONLY ROU-L + ROU-1INITIAL19.3128.9234.4438.4570.33FINAL18.9529.6736.0440.4371.763-SHOT / ONLY RMINITIAL21.5030.3539.2036.8976.29FINAL21.2530.1139.5136.8576.530-SHOT / ONLY RMINITIAL19.9729.6534.3338.0770.08FINAL19.9529.8940.6838.5976.733-SHOT / ROU + RMINITIAL21.4830.2939.1536.7976.34FINAL21.1130.9540.0538.6276.890-SHOT / ROU + RMINITIAL20.3630.1740.6838.5976.84FINAL20.0630.6441.4640.0077.43
We use a sub-document split on MultiDoc2Dial, to remove the information retrieval (IR) component such that we only have the most relevant document as opposed to the entire set of candidate documents. We use the validation dataset of QuAC as the test data since the testset is not publicly available.
for iteration j = 0, 1, . . .J: doWe include a complete version of Algorithm 1 adapted for synthetic dialogue generation, leveraged in our fine-tuning experiments.At first, we sample a new user query from large language model M, bootstrapping only off of the document.As the total number of turns (utterances) to be modeled is user-defined, we append each utterance to the end of the conversation history.For example, given the last user query, we append an agent response to it, which is either an initial (no refinement necessary) or final (post-refinement) response.If refinement did occur, then we first append the previous best agent response (y j ), then a user turn u of "User: Please make this response more {principle}", and then the improved and sufficient agent response y j+1 .Once an agent response has been procured and appended for the current turn, we continue back to line 1 and generate a new user query, this time conditioning on the conversation history as well; this repeats until the user-specified max turn limit is reached.
Initial Final MultiDoc2Dial (Avg. Gold: 15.55) Flan-T5-XXL ROU-Only. 32.06 35.88</p>
<p>Llama-2-13B-Chat ROU-Only. 39.30 44.40</p>
<p>Llama-2-13B-Chat RM-Only. 38.45 39.51</p>
<p>Llama-2-13B-Chat ROUGE + RM 38. 5659</p>
<p>. Quac (avg, Gold: 12.57</p>
<p>Llama-2-13B-Chat ROU-Only 29. 3341</p>
<p>Llama-2-13B-Chat RM-Only. 27.73 27.39</p>
<p>Evaluating correctness and faithfulness of instructionfollowing models for question answering. References Vaibhav Adlakah, Parishad Behnamghader, Han Xing, Nicholas Lu, Siva Meade, Reddy, 2023</p>
<p>. Yuntao Bai, Saurav Kadavath, Sadipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perrez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Tristan Hume, Samuel R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston; Nicholas Joseph, Sam McCandlish, Tom Brownand Jared Kaplan. 2022. Constitutional ai: Harmlessness from ai feedback</p>
<p>Quac : Question answering in context. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen Tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, 2018</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, 2023</p>
<p>MultiDoc2Dial: Modeling dialogues grounded in multiple documents. Song Feng, Sankalp Siva, Hui Patel, Sachindra Wan, Joshi, 10.18653/v1/2021.emnlp-main.498Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Rarr: Researching and revising what language models say, using language models. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 2023</p>
<p>Is self-repair a silver bullet for code generation?. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Solar-Lezama, Proceedings of ICLR 2024. ICLR 20242024</p>
<p>Tianlu Wang, Ping Ya, Ellen Xiaoqing, Sean O Tan, Ramakanth 'brien, Jane Pasunuru, Olga Dwivedi-Yu, Luke Golovneva, Zettlemoyer, arXiv:2308.04592Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023a. Shepherd: A critic for language model generation. arXiv preprint</p>
<p>Enable language models to implicitly learn self-improvement from data. Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji, 2023b</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, 2022</p>
<p>Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, Yajuan Lv, arXiv:2212.10057Wecheck: Strong factual consistency checker via weakly supervised learning. 2022arXiv preprint</p>
<p>Selfee: Iterative self-revising llm empowered by selffeedback generation. Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Minjoon Seo, 2023Blog post</p>
<p>A comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. Chen Zhang, Luis Fernando, D' Haro, Yiming Chen, Malu Zhang, Haizhou Li, AAAI-2024. 2024</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 20202020</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>            </div>
        </div>

    </div>
</body>
</html>
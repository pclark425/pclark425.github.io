<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8663 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8663</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8663</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273163226</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.03138v2.pdf" target="_blank">Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models (LLMs) have demonstrated impressive performance in molecular generation, which offers potential to accelerate drug discovery. However, the current LLMs overlook a critical requirement for drug discovery: proposing a diverse set of molecules. This diversity is essential for improving the chances of finding a viable drug, as it provides alternative molecules that may succeed where others fail in real-world validations. Nevertheless, the LLMs often output structurally similar molecules. While decoding schemes like diverse beam search may enhance textual diversity, this often does not align with molecular structural diversity. In response, we propose a new method for fine-tuning molecular generative LLMs to autoregressively generate a set of structurally diverse molecules, where each molecule is generated by conditioning on the previously generated molecules. Our approach consists of two stages: (1) supervised fine-tuning to adapt LLMs to autoregressively generate molecules in a sequence and (2) reinforcement learning to maximize structural diversity within the generated molecules. Our experiments show that the proposed approach enables LLMs to generate diverse molecules better than existing approaches for diverse sequence generation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8663.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8663.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Div-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity-aware Supervised Fine-Tuning (Div-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuning procedure that repurposes pre-trained molecular LLMs to autoregressively emit a concatenated sequence of multiple molecules conditioned on a single prompt, enabling the model to produce sets of molecules in one output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Div-SFT (method applied to BioT5+ and DrugAssist)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (fine-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to BioT5+ (T5 family, reported 220M) and DrugAssist (Llama-7B base, 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Collected molecule sets per prompt: for BioT5+ used training splits of L+M-24 and ChEBI-20 (up to T=100 molecules per description collected via contrastive beam search and filtered); for DrugAssist used generated molecules (T=300) from base model and paired prompts for property values.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / description-guided molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning to maximize log-likelihood of concatenated molecular sequences (M1:K = m1||...||mK) given p_desc+div (prompt requesting diverse molecules); models generate multiple molecules autoregressively in one output.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Paper reports increased structural diversity compared to decoding baselines (higher NCircles, IntDiv, and Accepted & Unique counts); no explicit percent novel vs training set given, but target coverage on L+M-24 improved to 0.558 (highest among methods) and accepted unique counts increased (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Ensured via description-matching in supervised data collection (filtered molecules must match provided example targets or property checks); description-matching reward later used in RL stage to enforce property relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accepted & Unique (Dice similarity > 0.7 to dataset targets), NCircles (largest subset with pairwise Tanimoto < threshold), Internal Diversity (IntDiv; avg 1 - Tanimoto), Tanimoto and Dice similarities, target coverage (Dice > 0.95), and docking-based acceptance (Autodock Vina score <= -7.0 for EGFR example).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Div-SFT alone improves generation of diverse molecules relative to standard decoding on the same base LLM (BioT5+): produced higher NCircles and IntDiv than beam search / nucleus sampling in experiments (see Table 1/3). It forms the initialization for RL stage that further increases diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to decoding-based diversification (random sampling, nucleus sampling, beam search, diverse beam search, contrastive beam search), Div-SFT yields higher structural diversity metrics when applied to the same base LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Supervised stage depends on the diversity present in collected/generated training sets; if π_pre produces insufficiently diverse molecules, Div-SFT alone may not achieve high structural diversity. Sequence length limits (max sequence length due to memory) and computational cost for concatenated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8663.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8663.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Div-SFT+RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Div-SFT with Multi-stage Reinforcement Learning (Div-SFT+RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline: (1) Div-SFT supervised fine-tuning to produce concatenated molecule sequences, followed by (2) molecule-wise multi-stage reinforcement learning using PPO to maximize structural diversity of each newly generated molecule relative to previously generated ones while preserving description matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Div-SFT+RL (applied to BioT5+ and DrugAssist)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer fine-tuned and RL-optimized (policy optimized via PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applied to BioT5+ (220M T5 variant) and DrugAssist (Llama-7B, 7B); experiments limited to models up to 7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Same supervised dataset as Div-SFT (L+M-24 and ChEBI-20 training splits, and synthetic collections from base models) used to initialize; RL uses on-policy generations during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery; generating structurally diverse candidate molecules conditioned on text descriptions (qualitative and quantitative properties).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multi-stage molecule-wise reinforcement learning: generate m1 conditioned on prompt, optimize reward r(m1) (diversity 0 for first), then sequentially generate mk conditioned on M1:k-1 and optimize r_div(mk, previous) + r_match(mk,p_desc) using PPO with per-token KL penalty to supervised model; diversity reward is 1 - max Tanimoto(prev,mk)^β, and match reward uses Dice similarity to dataset targets or property checks (RDKit) depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Substantially increased structural diversity by metrics: on L+M-24 Div-SFT+RL achieved NCircles h=0.85 = 10.51 (vs baselines in range ~1–7) and NCircles h=0.75 = 6.278; on ChEBI-20 NCircles h=0.85 = 11.30. Internal diversity (IntDiv) and Accepted & Unique counts also improved. No explicit fraction of molecules absent from training data reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Description-matching reward enforces molecules share substructures with dataset targets (Dice-based) or satisfy exact quantitative properties (RDKit checks) for DrugAssist experiments; final actions (complete molecules) yield rewards, ensuring per-molecule property relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as Div-SFT plus RL-specific ablations: single-stage vs multi-stage RL comparison (multi-stage outperformed single-stage), computational/time cost vs number of generated molecules, coverage of target molecules (fraction captured at Dice > 0.95).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Div-SFT+RL substantially outperforms Div-SFT and decoding baselines on structural diversity metrics while maintaining or improving acceptability against provided descriptions. Multi-stage RL is key: single-stage RL performed markedly worse due to credit assignment problems (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms decoding-based diversity schemes (diverse beam search, contrastive beam search), and other LLMs when those LLMs are used with decoding heuristics (see Table 3). The paper compares quantitative NCircles/IntDiv/Accepted & Unique numbers showing clear improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires autoregressive generation of long concatenated sequences (computationally and memory intensive); experiments constrained to models <=7B; evaluation is computational (RDKit, fingerprint similarity, docking) and not validated in wet lab; possible synthesizability issues; reward hyperparameters (α, β) and amplification used — sensitivity not fully explored; long RL training costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8663.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8663.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-style molecular generative LLM that integrates biological/chemical knowledge and supports SELFIES molecular representations; used as the primary base model for implementing Div-SFT and Div-SFT+RL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5-style transformer (encoder-decoder) adapted for molecular generation; uses SELFIES for molecular strings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5 models used in experiments reported as 220M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on biological/chemical corpora (per Pei et al., 2024); in experiments additionally trained on L+M-24 training split for some runs; uses SELFIES representations and dataset-specific target molecules for supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Description-guided molecular generation (drug discovery-focused prompts like 'EGFR inhibitor').</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based generation (SELFIES outputs) with beam/contrastive beam search for dataset collection; fine-tuned with Div-SFT and Div-SFT+RL to produce diverse molecular sets.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>When fine-tuned with Div-SFT+RL, BioT5+ generates more structurally diverse and higher-quality accepted molecules as measured by NCircles, IntDiv, and Accepted & Unique counts; exact novelty vs external databases not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Description matching via Dice-similarity to provided dataset target molecules (L+M-24/ChEBI-20) and external property evaluations (docking) for qualitative descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accepted & Unique (Dice > 0.7), NCircles at thresholds (h=0.85, 0.75), Internal Diversity (IntDiv), and docking Vina scores for specific case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BioT5+ fine-tuned with Div-SFT+RL achieved the highest NCircles and improved target coverage vs decoding baselines and other LLM baselines (see Tables 1 and 3). Applying Div-SFT+RL to BioT5+ gave NCircles h=0.85 up to ~10.51 on L+M-24 and ~11.30 on ChEBI-20.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performed better when fine-tuned with Div-SFT+RL than the same base used with diverse/contrastive beam search or other decoding schemes; outperformed other LLMs (MolT5, Text+Chem T5, LlaSMol, Meditron) when those used decoding schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>BioT5+ uses SELFIES, so mapping to SMILES or assessing synthesizability still required; supervised fine-tuning limited by sequence length (max 2560) and GPU memory; experiments limited to medium-size models (220M).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8663.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8663.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugAssist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugAssist (Llama-based molecule optimization LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist LLM for molecule optimization based on Llama-7B; used to evaluate whether Div-SFT and Div-SFT+RL can improve generalist models' ability to generate diverse molecules satisfying quantitative properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Drugassist: A large language model for molecule optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugAssist</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Llama-based autoregressive transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama-7B base (7B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on chemical/biomolecular corpora (per Ye et al., 2023); used SMILES outputs. For Div-SFT fine-tuning, synthetic paired prompts and collected molecule sets (T=300) were used.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery; generation of molecules satisfying quantitative properties (HB donors/acceptors, Bertz complexity, QED).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based generation; fine-tuned with Div-SFT and then with multi-stage RL (Div-SFT+RL) where r_match is a binary RDKit property-check (1 if property exactly satisfied) and r_div is Tanimoto-based.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Div-SFT and Div-SFT+RL improved the number and diversity of molecules exactly satisfying quantitative property prompts (results in Figure 3); generalization to an unseen property (QED) also improved. Exact novelty vs canonical datasets not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Quantitative properties evaluated using RDKit during RL (description-matching reward), enabling explicit enforcement of properties like number of H-bond donors/acceptors and Bertz complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Count of molecules exactly satisfying quantitative property, NCircles, IntDiv, and generative generalization to unseen prompts (QED).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuning DrugAssist with Div-SFT and RL improved performance in generating diverse molecules that exactly match specified quantitative properties and generalized better to an unseen QED prompt (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed unguided decoding schemes applied to DrugAssist for property-specific diversity; Div-SFT+RL gave superior property-satisfying diversity vs baseline decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Experiments limited to Llama-7B scale; supervised fine-tuning used LoRA to reduce compute but still required multiple GPUs; synthesizability and wet-lab validation not performed; RL hyperparameters and KL penalties tuned for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8663.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8663.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-style molecular generative model that translates between SMILES and molecular text descriptions, used as a baseline molecular generative LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5-style transformer (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported T5 models in paper are 220M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on paired SMILES/text translation tasks (per Edwards et al., 2022); used SMILES representations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Description-guided molecular generation / translation between molecule strings and textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based generation using decoding schemes (random, nucleus, beam, diverse/contrastive beam search) in the paper's baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>When used with decoding-based diversification methods, MolT5 produced lower NCircles / IntDiv compared to Div-SFT+RL; specific novelty counts not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Evaluated against dataset targets using Dice/Tanimoto similarity thresholds; MolT5 used as a baseline for description-guided generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>NCircles, Accepted & Unique, IntDiv as used in comparative tables.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5 baselines achieved lower diversity metrics compared to the Div-SFT+RL-fine-tuned BioT5+; one MolT5 variant (diverse BS) had NCircles ~6.295 on L+M-24 but still underperformed the proposed method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Served as a baseline generative LLM using decoding heuristics; outperformed by Div-SFT+RL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>As with other decoding-focused baselines, textual diversity does not always correspond to molecular structural diversity; sensitive to decoding heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8663.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8663.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-style model unifying molecular and textual representations via multi-task language modeling, used as an LLM baseline for molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifying molecular and textual representations via multi-task language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text+Chem T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>T5-style transformer (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported as 220M parameters for T5 variants used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Multi-task training combining textual and molecular data (per Christofidellis et al., 2023); uses SMILES representations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Description-guided molecular generation and cross-modal molecular-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based generation with decoding schemes (beam/diverse/contrastive beam search) used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Produced acceptable molecules under decoding baselines but with lower NCircles vs Div-SFT+RL; specific novelty vs datasets not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Assessed against dataset-provided target molecules via fingerprint similarities and diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>NCircles, Accepted & Unique, IntDiv, Tanimoto/Dice similarities.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Text+Chem T5 used with contrastive beam search yielded moderate diversity metrics on ChEBI-20 but was outperformed by Div-SFT+RL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared unfavorably to Div-SFT+RL in terms of structural diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>As a decoding-baseline, faces the mismatch between textual and molecular diversity; limited by decoding scheme effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8663.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8663.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LlaSMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LlaSMol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemistry-focused large language model trained with a large instruction tuning dataset (baseline in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LlaSMol</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned LLM (autoregressive transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported in paper as 7B for models in that family used as baselines</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale instruction tuning dataset for chemistry (per Yu et al., 2024); uses SMILES outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry-focused generation and instruction-following for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based molecule generation using decoding schemes in baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>When used with beam search variants, produced moderate NCircles/IntDiv but underperformed Div-SFT+RL; no explicit novelty percentages reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Evaluated by matching generated molecules to dataset targets via fingerprint similarities and diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>NCircles, Accepted & Unique, IntDiv, Tanimoto/Dice.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LlaSMol with beam search produced moderate diversity metrics on ChEBI-20 but was outperformed by the Div-SFT+RL-fine-tuned BioT5+.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Serves as a competitive baseline among generalist models but did not surpass the proposed fine-tuning+RL pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Relies on decoding strategies for diversity; performance depends on instruction-tuning coverage and decoding heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8663.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8663.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decoding Schemes (BS, Diverse BS, Contrastive BS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam Search (BS), Diverse Beam Search, Contrastive Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding heuristics used to increase textual diversity in sequence generation (beam search variants); commonly applied to LLMs as baseline diversification strategies but do not necessarily increase molecular structural diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diverse beam search: Decoding diverse solutions from neural sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diverse/Contrastive Beam Search (decoding algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Decoding/sequence-generation heuristics for autoregressive models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a (decoding method applied to various base LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>n/a (methods applied at generation time to pre-trained LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Used as baselines for molecular generation diversity experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Post-hoc decoding modifications (beam search, diverse beam search with diversity penalty, contrastive beam search with contrastive penalty) to generate multiple output candidates per prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Often increases textual diversity but can fail to increase molecular structural diversity because multiple SMILES/SELFIES can map to identical molecules; experimental results show lower NCircles and IntDiv vs Div-SFT+RL.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Not inherently property-aware; any application-specific enforcement must come from the base model or external filters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Applied with same evaluation metrics (Accepted & Unique, NCircles, IntDiv); metrics in tables show beam-search variants underperform Div-SFT+RL on structural diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Decoding schemes improved textual diversity but did not reliably produce structurally diverse molecules in experiments; e.g., contrastive beam search and diverse beam search produced lower NCircles and coverage than Div-SFT+RL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as the principal baseline; Div-SFT and Div-SFT+RL outperformed these decoding-based approaches significantly on structural-diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Textual-token diversity does not guarantee structural molecular diversity due to non-unique string representations (SMILES/SELFIES); may yield many syntactically distinct but structurally identical molecules; limited for property-specific generation without additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning <em>(Rating: 2)</em></li>
                <li>Drugassist: A large language model for molecule optimization. <em>(Rating: 2)</em></li>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. <em>(Rating: 2)</em></li>
                <li>Diverse beam search: Decoding diverse solutions from neural sequence models <em>(Rating: 1)</em></li>
                <li>A contrastive framework for neural text generation <em>(Rating: 1)</em></li>
                <li>Searching for high-value molecules using reinforcement learning and transformers <em>(Rating: 2)</em></li>
                <li>De novo drug design using reinforcement learning with multiple gpt agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8663",
    "paper_id": "paper-273163226",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Div-SFT",
            "name_full": "Diversity-aware Supervised Fine-Tuning (Div-SFT)",
            "brief_description": "A supervised fine-tuning procedure that repurposes pre-trained molecular LLMs to autoregressively emit a concatenated sequence of multiple molecules conditioned on a single prompt, enabling the model to produce sets of molecules in one output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Div-SFT (method applied to BioT5+ and DrugAssist)",
            "model_type": "autoregressive transformer (fine-tuned LLM)",
            "model_size": "applied to BioT5+ (T5 family, reported 220M) and DrugAssist (Llama-7B base, 7B)",
            "training_data": "Collected molecule sets per prompt: for BioT5+ used training splits of L+M-24 and ChEBI-20 (up to T=100 molecules per description collected via contrastive beam search and filtered); for DrugAssist used generated molecules (T=300) from base model and paired prompts for property values.",
            "application_domain": "Drug discovery / description-guided molecular generation",
            "generation_method": "Supervised fine-tuning to maximize log-likelihood of concatenated molecular sequences (M1:K = m1||...||mK) given p_desc+div (prompt requesting diverse molecules); models generate multiple molecules autoregressively in one output.",
            "novelty_of_chemicals": "Paper reports increased structural diversity compared to decoding baselines (higher NCircles, IntDiv, and Accepted & Unique counts); no explicit percent novel vs training set given, but target coverage on L+M-24 improved to 0.558 (highest among methods) and accepted unique counts increased (see tables).",
            "application_specificity": "Ensured via description-matching in supervised data collection (filtered molecules must match provided example targets or property checks); description-matching reward later used in RL stage to enforce property relevance.",
            "evaluation_metrics": "Accepted & Unique (Dice similarity &gt; 0.7 to dataset targets), NCircles (largest subset with pairwise Tanimoto &lt; threshold), Internal Diversity (IntDiv; avg 1 - Tanimoto), Tanimoto and Dice similarities, target coverage (Dice &gt; 0.95), and docking-based acceptance (Autodock Vina score &lt;= -7.0 for EGFR example).",
            "results_summary": "Div-SFT alone improves generation of diverse molecules relative to standard decoding on the same base LLM (BioT5+): produced higher NCircles and IntDiv than beam search / nucleus sampling in experiments (see Table 1/3). It forms the initialization for RL stage that further increases diversity.",
            "comparison_to_other_methods": "Compared to decoding-based diversification (random sampling, nucleus sampling, beam search, diverse beam search, contrastive beam search), Div-SFT yields higher structural diversity metrics when applied to the same base LLM.",
            "limitations_and_challenges": "Supervised stage depends on the diversity present in collected/generated training sets; if π_pre produces insufficiently diverse molecules, Div-SFT alone may not achieve high structural diversity. Sequence length limits (max sequence length due to memory) and computational cost for concatenated outputs.",
            "uuid": "e8663.0",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Div-SFT+RL",
            "name_full": "Div-SFT with Multi-stage Reinforcement Learning (Div-SFT+RL)",
            "brief_description": "A two-stage pipeline: (1) Div-SFT supervised fine-tuning to produce concatenated molecule sequences, followed by (2) molecule-wise multi-stage reinforcement learning using PPO to maximize structural diversity of each newly generated molecule relative to previously generated ones while preserving description matching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Div-SFT+RL (applied to BioT5+ and DrugAssist)",
            "model_type": "autoregressive transformer fine-tuned and RL-optimized (policy optimized via PPO)",
            "model_size": "applied to BioT5+ (220M T5 variant) and DrugAssist (Llama-7B, 7B); experiments limited to models up to 7B",
            "training_data": "Same supervised dataset as Div-SFT (L+M-24 and ChEBI-20 training splits, and synthetic collections from base models) used to initialize; RL uses on-policy generations during optimization.",
            "application_domain": "Drug discovery; generating structurally diverse candidate molecules conditioned on text descriptions (qualitative and quantitative properties).",
            "generation_method": "Multi-stage molecule-wise reinforcement learning: generate m1 conditioned on prompt, optimize reward r(m1) (diversity 0 for first), then sequentially generate mk conditioned on M1:k-1 and optimize r_div(mk, previous) + r_match(mk,p_desc) using PPO with per-token KL penalty to supervised model; diversity reward is 1 - max Tanimoto(prev,mk)^β, and match reward uses Dice similarity to dataset targets or property checks (RDKit) depending on task.",
            "novelty_of_chemicals": "Substantially increased structural diversity by metrics: on L+M-24 Div-SFT+RL achieved NCircles h=0.85 = 10.51 (vs baselines in range ~1–7) and NCircles h=0.75 = 6.278; on ChEBI-20 NCircles h=0.85 = 11.30. Internal diversity (IntDiv) and Accepted & Unique counts also improved. No explicit fraction of molecules absent from training data reported.",
            "application_specificity": "Description-matching reward enforces molecules share substructures with dataset targets (Dice-based) or satisfy exact quantitative properties (RDKit checks) for DrugAssist experiments; final actions (complete molecules) yield rewards, ensuring per-molecule property relevance.",
            "evaluation_metrics": "Same as Div-SFT plus RL-specific ablations: single-stage vs multi-stage RL comparison (multi-stage outperformed single-stage), computational/time cost vs number of generated molecules, coverage of target molecules (fraction captured at Dice &gt; 0.95).",
            "results_summary": "Div-SFT+RL substantially outperforms Div-SFT and decoding baselines on structural diversity metrics while maintaining or improving acceptability against provided descriptions. Multi-stage RL is key: single-stage RL performed markedly worse due to credit assignment problems (Table 5).",
            "comparison_to_other_methods": "Outperforms decoding-based diversity schemes (diverse beam search, contrastive beam search), and other LLMs when those LLMs are used with decoding heuristics (see Table 3). The paper compares quantitative NCircles/IntDiv/Accepted & Unique numbers showing clear improvements.",
            "limitations_and_challenges": "Requires autoregressive generation of long concatenated sequences (computationally and memory intensive); experiments constrained to models &lt;=7B; evaluation is computational (RDKit, fingerprint similarity, docking) and not validated in wet lab; possible synthesizability issues; reward hyperparameters (α, β) and amplification used — sensitivity not fully explored; long RL training costs.",
            "uuid": "e8663.1",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "BioT5+",
            "name_full": "BioT5+",
            "brief_description": "A T5-style molecular generative LLM that integrates biological/chemical knowledge and supports SELFIES molecular representations; used as the primary base model for implementing Div-SFT and Div-SFT+RL experiments.",
            "citation_title": "BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning",
            "mention_or_use": "use",
            "model_name": "BioT5+",
            "model_type": "T5-style transformer (encoder-decoder) adapted for molecular generation; uses SELFIES for molecular strings",
            "model_size": "T5 models used in experiments reported as 220M parameters",
            "training_data": "Pretrained on biological/chemical corpora (per Pei et al., 2024); in experiments additionally trained on L+M-24 training split for some runs; uses SELFIES representations and dataset-specific target molecules for supervised fine-tuning.",
            "application_domain": "Description-guided molecular generation (drug discovery-focused prompts like 'EGFR inhibitor').",
            "generation_method": "Prompt-based generation (SELFIES outputs) with beam/contrastive beam search for dataset collection; fine-tuned with Div-SFT and Div-SFT+RL to produce diverse molecular sets.",
            "novelty_of_chemicals": "When fine-tuned with Div-SFT+RL, BioT5+ generates more structurally diverse and higher-quality accepted molecules as measured by NCircles, IntDiv, and Accepted & Unique counts; exact novelty vs external databases not reported.",
            "application_specificity": "Description matching via Dice-similarity to provided dataset target molecules (L+M-24/ChEBI-20) and external property evaluations (docking) for qualitative descriptions.",
            "evaluation_metrics": "Accepted & Unique (Dice &gt; 0.7), NCircles at thresholds (h=0.85, 0.75), Internal Diversity (IntDiv), and docking Vina scores for specific case studies.",
            "results_summary": "BioT5+ fine-tuned with Div-SFT+RL achieved the highest NCircles and improved target coverage vs decoding baselines and other LLM baselines (see Tables 1 and 3). Applying Div-SFT+RL to BioT5+ gave NCircles h=0.85 up to ~10.51 on L+M-24 and ~11.30 on ChEBI-20.",
            "comparison_to_other_methods": "Performed better when fine-tuned with Div-SFT+RL than the same base used with diverse/contrastive beam search or other decoding schemes; outperformed other LLMs (MolT5, Text+Chem T5, LlaSMol, Meditron) when those used decoding schemes.",
            "limitations_and_challenges": "BioT5+ uses SELFIES, so mapping to SMILES or assessing synthesizability still required; supervised fine-tuning limited by sequence length (max 2560) and GPU memory; experiments limited to medium-size models (220M).",
            "uuid": "e8663.2",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DrugAssist",
            "name_full": "DrugAssist (Llama-based molecule optimization LLM)",
            "brief_description": "A generalist LLM for molecule optimization based on Llama-7B; used to evaluate whether Div-SFT and Div-SFT+RL can improve generalist models' ability to generate diverse molecules satisfying quantitative properties.",
            "citation_title": "Drugassist: A large language model for molecule optimization.",
            "mention_or_use": "use",
            "model_name": "DrugAssist",
            "model_type": "Llama-based autoregressive transformer",
            "model_size": "Llama-7B base (7B parameters)",
            "training_data": "Pretrained on chemical/biomolecular corpora (per Ye et al., 2023); used SMILES outputs. For Div-SFT fine-tuning, synthetic paired prompts and collected molecule sets (T=300) were used.",
            "application_domain": "Drug discovery; generation of molecules satisfying quantitative properties (HB donors/acceptors, Bertz complexity, QED).",
            "generation_method": "Prompt-based generation; fine-tuned with Div-SFT and then with multi-stage RL (Div-SFT+RL) where r_match is a binary RDKit property-check (1 if property exactly satisfied) and r_div is Tanimoto-based.",
            "novelty_of_chemicals": "Div-SFT and Div-SFT+RL improved the number and diversity of molecules exactly satisfying quantitative property prompts (results in Figure 3); generalization to an unseen property (QED) also improved. Exact novelty vs canonical datasets not reported.",
            "application_specificity": "Quantitative properties evaluated using RDKit during RL (description-matching reward), enabling explicit enforcement of properties like number of H-bond donors/acceptors and Bertz complexity.",
            "evaluation_metrics": "Count of molecules exactly satisfying quantitative property, NCircles, IntDiv, and generative generalization to unseen prompts (QED).",
            "results_summary": "Fine-tuning DrugAssist with Div-SFT and RL improved performance in generating diverse molecules that exactly match specified quantitative properties and generalized better to an unseen QED prompt (Figure 3).",
            "comparison_to_other_methods": "Outperformed unguided decoding schemes applied to DrugAssist for property-specific diversity; Div-SFT+RL gave superior property-satisfying diversity vs baseline decoding.",
            "limitations_and_challenges": "Experiments limited to Llama-7B scale; supervised fine-tuning used LoRA to reduce compute but still required multiple GPUs; synthesizability and wet-lab validation not performed; RL hyperparameters and KL penalties tuned for stability.",
            "uuid": "e8663.3",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5",
            "brief_description": "A T5-style molecular generative model that translates between SMILES and molecular text descriptions, used as a baseline molecular generative LLM.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "use",
            "model_name": "MolT5",
            "model_type": "T5-style transformer (encoder-decoder)",
            "model_size": "Reported T5 models in paper are 220M parameters",
            "training_data": "Trained on paired SMILES/text translation tasks (per Edwards et al., 2022); used SMILES representations.",
            "application_domain": "Description-guided molecular generation / translation between molecule strings and textual descriptions.",
            "generation_method": "Prompt-based generation using decoding schemes (random, nucleus, beam, diverse/contrastive beam search) in the paper's baseline comparisons.",
            "novelty_of_chemicals": "When used with decoding-based diversification methods, MolT5 produced lower NCircles / IntDiv compared to Div-SFT+RL; specific novelty counts not reported in this paper.",
            "application_specificity": "Evaluated against dataset targets using Dice/Tanimoto similarity thresholds; MolT5 used as a baseline for description-guided generation.",
            "evaluation_metrics": "NCircles, Accepted & Unique, IntDiv as used in comparative tables.",
            "results_summary": "MolT5 baselines achieved lower diversity metrics compared to the Div-SFT+RL-fine-tuned BioT5+; one MolT5 variant (diverse BS) had NCircles ~6.295 on L+M-24 but still underperformed the proposed method.",
            "comparison_to_other_methods": "Served as a baseline generative LLM using decoding heuristics; outperformed by Div-SFT+RL.",
            "limitations_and_challenges": "As with other decoding-focused baselines, textual diversity does not always correspond to molecular structural diversity; sensitive to decoding heuristics.",
            "uuid": "e8663.4",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Text+Chem T5",
            "name_full": "Text+Chem T5",
            "brief_description": "A T5-style model unifying molecular and textual representations via multi-task language modeling, used as an LLM baseline for molecular generation.",
            "citation_title": "Unifying molecular and textual representations via multi-task language modelling",
            "mention_or_use": "use",
            "model_name": "Text+Chem T5",
            "model_type": "T5-style transformer (encoder-decoder)",
            "model_size": "Reported as 220M parameters for T5 variants used in experiments",
            "training_data": "Multi-task training combining textual and molecular data (per Christofidellis et al., 2023); uses SMILES representations.",
            "application_domain": "Description-guided molecular generation and cross-modal molecular-text tasks.",
            "generation_method": "Prompt-based generation with decoding schemes (beam/diverse/contrastive beam search) used in comparisons.",
            "novelty_of_chemicals": "Produced acceptable molecules under decoding baselines but with lower NCircles vs Div-SFT+RL; specific novelty vs datasets not reported.",
            "application_specificity": "Assessed against dataset-provided target molecules via fingerprint similarities and diversity metrics.",
            "evaluation_metrics": "NCircles, Accepted & Unique, IntDiv, Tanimoto/Dice similarities.",
            "results_summary": "Text+Chem T5 used with contrastive beam search yielded moderate diversity metrics on ChEBI-20 but was outperformed by Div-SFT+RL.",
            "comparison_to_other_methods": "Compared unfavorably to Div-SFT+RL in terms of structural diversity metrics.",
            "limitations_and_challenges": "As a decoding-baseline, faces the mismatch between textual and molecular diversity; limited by decoding scheme effectiveness.",
            "uuid": "e8663.5",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LlaSMol",
            "name_full": "LlaSMol",
            "brief_description": "A chemistry-focused large language model trained with a large instruction tuning dataset (baseline in comparisons).",
            "citation_title": "LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
            "mention_or_use": "use",
            "model_name": "LlaSMol",
            "model_type": "Instruction-tuned LLM (autoregressive transformer)",
            "model_size": "Reported in paper as 7B for models in that family used as baselines",
            "training_data": "Large-scale instruction tuning dataset for chemistry (per Yu et al., 2024); uses SMILES outputs.",
            "application_domain": "Chemistry-focused generation and instruction-following for molecule generation.",
            "generation_method": "Prompt-based molecule generation using decoding schemes in baseline comparisons.",
            "novelty_of_chemicals": "When used with beam search variants, produced moderate NCircles/IntDiv but underperformed Div-SFT+RL; no explicit novelty percentages reported.",
            "application_specificity": "Evaluated by matching generated molecules to dataset targets via fingerprint similarities and diversity metrics.",
            "evaluation_metrics": "NCircles, Accepted & Unique, IntDiv, Tanimoto/Dice.",
            "results_summary": "LlaSMol with beam search produced moderate diversity metrics on ChEBI-20 but was outperformed by the Div-SFT+RL-fine-tuned BioT5+.",
            "comparison_to_other_methods": "Serves as a competitive baseline among generalist models but did not surpass the proposed fine-tuning+RL pipeline.",
            "limitations_and_challenges": "Relies on decoding strategies for diversity; performance depends on instruction-tuning coverage and decoding heuristics.",
            "uuid": "e8663.6",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Decoding Schemes (BS, Diverse BS, Contrastive BS)",
            "name_full": "Beam Search (BS), Diverse Beam Search, Contrastive Beam Search",
            "brief_description": "Decoding heuristics used to increase textual diversity in sequence generation (beam search variants); commonly applied to LLMs as baseline diversification strategies but do not necessarily increase molecular structural diversity.",
            "citation_title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
            "mention_or_use": "use",
            "model_name": "Diverse/Contrastive Beam Search (decoding algorithms)",
            "model_type": "Decoding/sequence-generation heuristics for autoregressive models",
            "model_size": "n/a (decoding method applied to various base LLMs)",
            "training_data": "n/a (methods applied at generation time to pre-trained LLMs)",
            "application_domain": "Used as baselines for molecular generation diversity experiments",
            "generation_method": "Post-hoc decoding modifications (beam search, diverse beam search with diversity penalty, contrastive beam search with contrastive penalty) to generate multiple output candidates per prompt.",
            "novelty_of_chemicals": "Often increases textual diversity but can fail to increase molecular structural diversity because multiple SMILES/SELFIES can map to identical molecules; experimental results show lower NCircles and IntDiv vs Div-SFT+RL.",
            "application_specificity": "Not inherently property-aware; any application-specific enforcement must come from the base model or external filters.",
            "evaluation_metrics": "Applied with same evaluation metrics (Accepted & Unique, NCircles, IntDiv); metrics in tables show beam-search variants underperform Div-SFT+RL on structural diversity.",
            "results_summary": "Decoding schemes improved textual diversity but did not reliably produce structurally diverse molecules in experiments; e.g., contrastive beam search and diverse beam search produced lower NCircles and coverage than Div-SFT+RL.",
            "comparison_to_other_methods": "Used as the principal baseline; Div-SFT and Div-SFT+RL outperformed these decoding-based approaches significantly on structural-diversity metrics.",
            "limitations_and_challenges": "Textual-token diversity does not guarantee structural molecular diversity due to non-unique string representations (SMILES/SELFIES); may yield many syntactically distinct but structurally identical molecules; limited for property-specific generation without additional mechanisms.",
            "uuid": "e8663.7",
            "source_info": {
                "paper_title": "Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning",
            "rating": 2,
            "sanitized_title": "biot5_towards_generalized_biological_understanding_with_iupac_integration_and_multitask_tuning"
        },
        {
            "paper_title": "Drugassist: A large language model for molecule optimization.",
            "rating": 2,
            "sanitized_title": "drugassist_a_large_language_model_for_molecule_optimization"
        },
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models.",
            "rating": 2,
            "sanitized_title": "molinstructions_a_largescale_biomolecular_instruction_dataset_for_large_language_models"
        },
        {
            "paper_title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
            "rating": 1,
            "sanitized_title": "diverse_beam_search_decoding_diverse_solutions_from_neural_sequence_models"
        },
        {
            "paper_title": "A contrastive framework for neural text generation",
            "rating": 1,
            "sanitized_title": "a_contrastive_framework_for_neural_text_generation"
        },
        {
            "paper_title": "Searching for high-value molecules using reinforcement learning and transformers",
            "rating": 2,
            "sanitized_title": "searching_for_highvalue_molecules_using_reinforcement_learning_and_transformers"
        },
        {
            "paper_title": "De novo drug design using reinforcement learning with multiple gpt agents",
            "rating": 1,
            "sanitized_title": "de_novo_drug_design_using_reinforcement_learning_with_multiple_gpt_agents"
        }
    ],
    "cost": 0.0180675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity
17 Feb 2025</p>
<p>Hyosoon Jang 
Yonsei University</p>
<p>Yunhui Jang 
Yonsei University</p>
<p>Jaehyung Kim jaehyungk@yonsei.ac.kr 
Yonsei University</p>
<p>Sungsoo Ahn sungsoo.ahn@kaist.ac.kr 
Yonsei University</p>
<p>Kaist 
Yonsei University</p>
<p>Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity
17 Feb 20259BE4748546D36E36705A984F1CFB112BarXiv:2410.03138v2[cs.LG]
Recent advancements in large language models (LLMs) have demonstrated impressive performance in molecular generation, which offers potential to accelerate drug discovery.However, the current LLMs overlook a critical requirement for drug discovery: proposing a diverse set of molecules.This diversity is essential for improving the chances of finding a viable drug, as it provides alternative molecules that may succeed where others fail in realworld validations.Nevertheless, the LLMs often output structurally similar molecules.While decoding schemes like diverse beam search may enhance textual diversity, this often does not align with molecular structural diversity.In response, we propose a new method for fine-tuning molecular generative LLMs to autoregressively generate a set of structurally diverse molecules, where each molecule is generated by conditioning on the previously generated molecules.Our approach consists of two stages: (1) supervised fine-tuning to adapt LLMs to autoregressively generate molecules in a sequence and (2) reinforcement learning to maximize structural diversity within the generated molecules.Our experiments show that the proposed approach enables LLMs to generate diverse molecules better than existing approaches for diverse sequence generation.</p>
<p>Introduction</p>
<p>Recent advances in large language models (LLMs) have demonstrated the potential to accelerate scientific discovery by leveraging their language processing capabilities.This progress has been particularly impactful for candidate design problems such as drug discovery (Pei et al., 2024), protein design (Zhuo et al., 2024), and material design (Gruver et al., 2024).In particular, with biomolecular datasets and molecular string representations, e.g., SMILES (Weininger, 1988) or SELFIES (Krenn et al., 2020), LLMs have demonstrated impressive abilities to generate molecules from textual descriptions, e.g., molecular properties (Edwards et al., 2022;Ye et al., 2023;Pei et al., 2024).</p>
<p>However, current LLM-based molecular generation approaches (Edwards et al., 2022;Ye et al., 2023;Pei et al., 2024) often overlook a critical requirement for drug discovery: proposing a diverse set of molecules.In computer-aided drug discovery, identifying a single molecule with a desired property does not guarantee success in real-world pipelines that require additional cell-based studies and clinical trials (Vamathevan et al., 2019).Therefore, drug discovery requires a set of structurally diverse molecules.The generation of structurally diverse molecules increases the chances of finding a viable drug candidate (Xie et al., 2023), as different molecules may succeed where others fail.This diversity is essential to enhance the robustness and success of the drug discovery (Krantz, 1998;Hong et al., 2020;Sadybekov and Katritch, 2023).</p>
<p>In response, we explore the use of LLMs for diverse molecular generation.We begin by identifying the limitations of recent LLMs (Ye et al., 2023;OpenAI, 2023) and decoding schemes (Vijayakumar et al., 2018;Su et al., 2022) in generating diverse molecules.Then, we present a new method for fine-tuning LLMs to generate diverse molecules.Our approach can be broadly applied to other LLM-based candidate design problems, e.g., computer-aided design (Wu et al., 2023a).Existing LLMs have limitations in generating diverse molecules.To obtain diverse molecules, one may query the recent generalist LLMs, e.g., Llama (Touvron et al., 2023) or ChatGPT (OpenAI, 2023).However, our empirical observation in Figure 1(a) reveals that even the recent models produce structurally identical or highly similar molecules from the given prompt. 1 This observation aligns  (Ye et al., 2023;OpenAI, 2023) lack the ability to generate a diverse set of molecules.Figure 1: Existing works on LLMs fail to generate diverse molecules.The existing decoding schemes (Vijayakumar et al., 2018) for diverse sequence generation and LLMs for chemical tasks fail to capture the molecular diversity, and may induce structurally identical molecules.</p>
<p>with previous observations that have shown LLMs may fail to generate diverse outputs (Kirk et al., 2024) for general text-based domains.Decoding schemes for diversified generation do not align with molecular diversity.We also acknowledge the existence of decoding schemes, e.g., diverse beam search (Vijayakumar et al., 2018) or contrastive beam search (Su et al., 2022), which have been proposed to improve the diversity of output sequences generated by LLMs.However, these decoding schemes are limited to improving the textual diversity which often does not correspond to molecular structural diversity, e.g., there exist many SMILES or SELFIES strings that correspond to the same molecule, as illustrated in Figure 1(b).Our approach.We repurpose existing molecular generative LLMs to autoregressively generate a diverse set of molecules from a single prompt.By enabling the LLMs to generate a new molecule conditioned on previously generated molecules, we expect the LLMs to learn to enhance the structural diversity between the generated molecules.To this end, we propose a two-stage approach to fine-tune LLMs: (a) a supervised fine-tuning stage to repurpose LLMs to autoregressively generate a sequence of multiple molecules and (b) a reinforcement learning stage to maximize the molecular structural diversity between the generated molecules.</p>
<p>In the supervised training stage, we train LLMs to autoregressively generate a set of molecules in a sequence.Note that the training data, i.e., a set of molecules, can be collected from LLMs themselves through iterative sampling, and then filtered to enhance the quality, e.g., removing invalid molecules.</p>
<p>However, this stage does not necessarily incorporate molecular diversity, as the training may not involve sufficiently distinct molecules (e.g., limitations in Figure 1(b)).To tackle this, we subsequently apply reinforcement learning with exploration towards discovering diverse molecules.</p>
<p>Next, in the reinforcement learning stage, we train LLMs to maximize the diversity of molecules within a generated sequence.However, for our task, conventional sequence-wise reinforcement learning (Ouyang et al., 2022) suffers from the credit assignment problem (Zhou et al., 2024): the challenges in identifying and promoting the generation of molecules responsible for increasing diversity, among a larger set of molecules in the sequence.To resolve this issue, we solve multi-stage molecule generation problems for a sequence of molecules, where the generation of each molecule aims to maximize the diversity with respect to the previously generated molecules.We train LLMs to maximize the associated rewards using proximal policy optimization (Schulman et al., 2017).</p>
<p>We compare our method with the decoding schemes for diversified generation (Vijayakumar et al., 2016;Su et al., 2022) and other representative LLMs, including chemical-task specialists (Edwards et al., 2022;Christofidellis et al., 2023;Pei et al., 2023Pei et al., , 2024)), fine-tuned generalists on chemical domains (Fang et al., 2024;Yu et al., 2024), and the ChatGPT series (OpenAI, 2023(OpenAI, , 2024)).We observe that (1) our fine-tuning approach enables LLMs to better discover diverse molecules compared to existing decoding schemes and (2) our fine-tuned LLM outperforms other LLMs.</p>
<p>To conclude, our contributions can be summarized as follows:</p>
<p>• We are the first to explore the use of LLMs for generating diverse molecules.</p>
<p>• We first propose a fine-tuning approach for LLMs to generate diverse solutions, which presents a new direction distinct from existing approaches focused on the decoding scheme.</p>
<p>• Experimentally, our method outperforms the baselines in generating diverse molecules.</p>
<p>Related Work</p>
<p>Large language models (LLMs) for molecular generation.Recent advancements in LLMs have shown increasing promise in scientific applications, especially for molecular generation (Edwards et al., 2022;Pei et al., 2023;Fang et al., 2024;Pei et al., 2024).First, Edwards et al. (2022) proposed MolT5, a molecular generative LLM that translates between SMILES (Weininger, 1988) and molecular text descriptions.Next, Text+Chem T5 (Christofidellis et al., 2023) and BioT5 (Pei et al., 2023) (Ouyang et al., 2022).In addition, there has been a surge in research on devising RL for LLMs as well, such as addressing multi-turn settings (Shani et al., 2024) and incorporating multiple fine-grained reward signals (Wu et al., 2023b).For molecular generation, Ghugare et al. (2024) proposed RL-based fine-tuning to generate a molecule satisfying target properties.However, to the best of our knowledge, there exist no prior RL-based approaches that aim to increase the diversity of LLM-generated outputs.2</p>
<p>Method</p>
<p>In this section, we present our method for finetuning LLMs to generate diverse molecules.Specifically, we consider fine-tuning existing molecular generative LLMs that produce molecular representations such as SMILES or SELFIES.Importantly, our approach is versatile and can be applied to other domains, e.g., protein sequence (Zhuo et al., 2024) or computer-aided design (Wu et al., 2023a).</p>
<p>Overview.Our goal is to generate a sequence of structurally diverse molecules from a given prompt by producing them in a single concatenated output.</p>
<p>To this end, we fine-tune the LLMs in two stages: (a) a supervised fine-tuning phase that repurposes the LLMs to generate a sequence of molecules rather than a single one, and (b) a reinforcement learning phase aimed at further enhancing the structural diversity among the generated molecules.Task details.In detail, we consider generating molecules from a prompt p desc , where the prompt describes a molecular property that the generated molecules should possess.In this setting, we aim to generate diverse molecules that satisfy the given description p desc , where the diversity is evaluated using similarity measures between the structural features of the molecules, e.g., the presence of specific atoms, or substructures (Bajusz et al., 2015).We let P denote the prompts used for training.</p>
<p>Supervised Fine-tuning</p>
<p>We first describe our supervised fine-tuning process for repurposing the pre-trained LLMs to autoregressively generate multiple molecules in a sequence.This involves collecting a dataset of molecules from a pre-trained LLM π pre , and then fine-tuning the LLM π SFT on the collected dataset.We describe the process in Figure 2(a) and Algorithm 1.  Algorithm 1 Supervised fine-tuning 1: Initialize π SFT with π pre 2: repeat 3:</p>
<p>𝜋 !"#</p>
<p>Get prompt p desc ∼ P 4:
Get {m i } T i=1 from π pre (m | p desc ) 5: Update {m i } K i=1 ← Filter({m i } T i=1 ) 6:
Maximize Equation ( 1) with {m i } K Initially, the pre-trained LLM π pre produces a set of molecules by iterative sampling molecules for a given prompt p desc ∈ P as follows:
m i ∼ π pre (m i |p desc ) for i = 1, . . . , T,
where m i denotes the string representation of the molecule.In practice, we employ beam search to collect the set of molecules {m i } T i=1 .Then, we filter out the invalid string representations, duplicate molecules, and molecules that do not satisfy the given prompt p desc .This results in reducing the set of molecules from {m i } T i=1 to {m i } K i=1 .The details are described in Appendix B. Supervised training.Given the filtered set of molecules {m i } K i=1 , we train the LLM π SFT , which is initialized with π pre , to generate them as a single concatenated sequence.We denote this sequence by
M 1:K = m 1 || • • • ||m K ,
where || denotes the concatenation of the molecular string representations.Specifically, given a modified prompt p desc+div describing the target property with a request for generating diverse molecules, we train the LLM to maximize the log-likelihood:
log π SFT (M 1:K | p desc+div ) .
(1)</p>
<p>However, the policy π SFT does not necessarily incorporate a molecular structural diversity, as the set of molecules {m i } K i=1 collected from π pre may insufficiently involve diverse molecular structures (e.g., due to limitations in Figure 1(b)).To tackle this, we next introduce an online reinforcement learning stage with exploration towards discovering diverse molecules.</p>
<p>Reinforcement Learning</p>
<p>Algorithm 2 Multi-stage RL fine-tuning
1: Sample p desc ∼ P 2: Sample m 1 ∼ π RL (m 1 | p desc+div ) 3: Update π RL with PPO to maximize r(m 1 ) 4: for k = 2, . . . , K do 5: Get m k ∼ π RL (m k | M 1:k−1 , p desc+div ) 6:
Update π RL with PPO to maximize r(m k ) 7: end for We apply reinforcement learning to maximize the diversity of the generated molecules within a sequence.However, when applied to a sequence of molecules M 1:K , conventional sequence-wise reinforcement learning (Ouyang et al., 2022) suffers from the credit assignment problem (Zhou et al., 2024): the challenge in identifying and promoting the generation of molecules responsible for increasing diversity.To circumvent this, we introduce a molecule-wise reinforcement learning. 3able 1: Comparison with existing decoding schemes.NCircles represents both quality and diversity-related metric.Accepted &amp; unique represent quality-related metrics.IntDiv.represents an average of pair-wise diversities.Our method (Div-SFT+RL) generates more diverse and high-quality molecules compared to the baselines.Notably, our method makes a larger gap over the baselines on NCircles related to capturing both quality and diversity.Specifically, we consider reinforcement learning on a sequence of molecules M 1:K as learning in K individual stages.Each stage corresponds to generating a molecule m k conditioned on a sequence of previously generated molecules M 1:k−1 .Then, the LLM π RL is trained to maximize the return of each stage, defined by the reward of the generated molecule.Here, the reward is the diversity between the previously generated molecules {m i } k−1 i=1 and the new molecule m k .We also incorporate an auxiliary reward to ensure that the generated molecule satisfies the description p desc .We present our approach in Figure 2(b) and Algorithm 2. Reward.The reward evaluates the molecule m k with a diversity reward r div (m k , {m i } k−1 i=1 ) and a description-matching reward r match (m k , p desc ):
r(m k ) = r div (m k , {m i } k−1 i=1 ) + r match (m k , p desc )
, where the diversity reward r div evaluates structural differences between the molecule m k and the previously generated molecules {m i } k−1 i=1 by assessing their true molecular structures.Note that r div (m 1 ) is zero.The description-matching reward r match evaluates whether the molecule m k satisfies the description p desc .In practice, the final action completing the molecule yields the reward.The implementation is described in Section 4. Policy optimization.We optimize the LLM π RL to maximize the reward using proximal policy optimization (PPO; Schulman et al., 2017).Here, π RL is initialized with π SFT .We also combine per-token KL penalty from the supervised fine-tuned model following prior studies (Ouyang et al., 2022).</p>
<p>Experiment</p>
<p>In this section, we validate our supervised finetuning and reinforcement learning methods for diverse molecular generation, coined Div-SFT and Div-SFT+RL, respectively.</p>
<p>Comparison with Decoding Schemes</p>
<p>We first show that our fine-tuning method enables molecular generative LLMs to generate diverse molecules better than applying decoding schemes for diverse sequence generation.Here, we implement our method and decoding schemes on a recent description-guided molecular generative LLM: BioT5 + (Pei et al., 2024).Tasks.We consider diverse molecular generation with two existing datasets: L+M-24 (Edwards et al., 2024) and ChEBI-20 (Edwards et al., 2021).Each data point in these datasets provides a qualitative molecular property description, e.g., "The molecule is an EGFR inhibitor", and some examples of target molecules that satisfy the description. 4Both L+M-24 and ChEBI-20 datasets consist of training and test splits where the training splits have been used to pre-train the base LLM.Our finetuning method also uses their training splits.We generate 50 molecules for each description in the test splits for evaluation. 5able 2: Visualization of the generated molecules.We generate ten molecules from: "The molecule is a egfr inhibitor and modulate, belonging to the cancer treatment class of molecules, and is treatment of disorder".The blue line indicates the molecule that follows the given description, as evaluated using the external tool (Trott and Olson, 2010).BS generates structurally similar molecules.The details are described in Appendix E.</p>
<p>BS Ours</p>
<p>Metrics.We evaluate the structural diversity of the generated molecules that satisfy the given molecular description.However, evaluating whether the generated molecule satisfies some qualitative properties, e.g., a membrane stabilizer, is intractable due to the complexity of the required assessments.Therefore, we assume that the molecule satisfies the description if it shares a certain degree of structures with one of the target molecules provided in the dataset. 6Here, the specific degree is described in the explanation of the accepted molecules.</p>
<p>To assess the similarity between two molecules, we compute Dice and Tanimoto similarities on their structural features (Bajusz et al., 2015), focusing on the degree of shared substructures and overall structural similarity, respectively.See Appendix A.2 for details.Based on these, we evaluate the following metrics for the generated molecules:</p>
<p>• The number of accepted and unique molecules (Accepted &amp; Unique) counts unique molecules that share substructures with one of the provided examples of target molecules (as much as Dice similarity higher than 0.7).</p>
<p>• The number of circles (NCircles; Xie et al., 2023) considers both quality and diversity.Given the set of accepted molecules, NCircles h computes the size of the largest subset of molecules in which no two molecules are structurally similar to each other (as much as Tanimoto similarity below a threshold h).</p>
<p>• Internal diversity (IntDiv.;Polykovskiy et al., 2020) measures the average pair-wise structural distances, based on the complement of Tanimoto similarity, between the accepted molecules.Baselines.We compare our method with various decoding schemes, including random sampling, nucleus sampling (Holtzman et al., 2020), and beam search (BS; Rosenberg and Baldwin, 1965).We also consider the variants of BS that promote sequence-level diversity: diverse BS (Vijayakumar et al., 2018) and contrastive BS (Su et al., 2022).Results.In Table 1, we present experimental results on L+M-24 and ChEBI-20 datasets.One can see that applying our fine-tuning approach shows superior performance in discovering diverse accepted molecules, i.e., yields the highest NCircles, compared to applying existing decoding schemes on the molecular generative LLM.We also present qualitative results in Table 2, which evaluate whether the generated molecules actually satisfy the given description using an external tool (Trott and Olson, 2010).One can see that both BS and our method generate molecules that satisfy the given description, but our method generates more diverse molecules compared to BS.</p>
<p>Comparison with Existing LLMs</p>
<p>Additionally, we compare our fine-tuned BioT5 + in Section 4.1 with baselines that apply existing  decoding schemes to other LLMs, extending the experiment in Section 4.1.The purpose of this experiment is to highlight that other existing LLMs have limitations in diverse molecular generation when relying on existing decoding schemes for diverse sequence generation, compared to our finetuned model.The overall metrics and tasks are the same as settings in Section 4.1.</p>
<p>Baselines.For the comparison on L+M-24 dataset, we additionally consider LLMs trained on L+M-24 training split: MolT5 (Edwards et al., 2024(Edwards et al., , 2022) ) and Meditron (Edwards et al., 2024;Chen et al., 2023).For the comparison on ChEBI-20 dataset, we additionally consider LLMs trained on ChEBI-20 training split: MolT5 (Edwards et al., 2022), Text+Chem T5 (Christofidellis et al., 2023), and LlaSMol (Yu et al., 2024).For each baseline, we report the best results (highest NCircles) obtained using one of existing decoding schemes, i.e., random sampling, BS, diverse BS, and constrastive BS.We describe detailed settings in Appendix D.</p>
<p>Results.We present the results in Table 3.One can see that most baselines yield a low NCircles with respect to the number of accepted and unique molecules, indicating a lack of diversity, while our method yields a relatively high NCircles.</p>
<p>Fine-tuning Generalist LLMs</p>
<p>We further validate whether our fine-tuning method improves the generalist LLMs.Here, as a base LLM for implementing our method, we consider DrugAssist (Ye et al., 2023) which is based on the Llama-7B (Touvron et al., 2023).As baselines, we apply existing decoding schemes to DrugAssist.Tasks.We consider molecular descriptions about quantitative molecular properties: hydrogen bond (HB) donors, HB acceptors, Bertz complexity (Bertz, 1981), and quantitative estimate of druglikeness (QED) (Bickerton et al., 2012).Note that these properties can be evaluated using external tools like RDKit (Landrum et al., 2025).Implementations.We consider fine-tuning the base LLM on prompts about three properties: HB donors, HB acceptors, and Bertz complexity.The prompts about QED are used to assess generalization to unseen properties.The overall implementations follow Section 4.1, but the descriptionmatching reward r match (m k ) is designed to yield a positive value when the given property is exactly satisfied.See Appendix B for the detailed settings.Results.We present the results in Figure 3.One can see that our approach improves the performance in generating diverse molecules exactly sat- isfying the given molecular descriptions.Furthermore, our approach consistently demonstrates superior performance for the unseen prompt, i.e., the prompt about QED in Figure 3(d).</p>
<p>Ablation Studies</p>
<p>Large number of samples vs. performance.We also analyze how well our method discovers diverse molecules with respect to the number of generations.Here, we extend beyond the settings in Section 4.1.We consider our method to generate 100, 150, and 200 molecules with a single NVIDIA A100 SXM4 40GB GPU.We also consider BS to generate a larger number of molecules with beam sizes of 300, 400, and 500, reaching our maximum computational budget (four NVIDIA A100 SXM4 40GB GPUs).This experiment uses 250 molecular descriptions in the ChEBI-20 test split.We present the results in Table 4.One can see that our method exhibits further performance improvements compared to the baseline, even though the baseline generates a larger number of molecules using our maximum computational budget.Time costs vs. performance.In addition, we also analyze how well our method discovers diverse molecules with respect to the time costs.In Table 4, we present the time costs for each method.One can see that our method discovers more diverse molecules with respect to the time costs.</p>
<p>Single-stage vs. multi-stage RL.As mentioned in Section 3.2, we consider a multi-stage setting for generating multiple molecules.However, one may also consider a single-stage setting, where the return of a generated sequence is defined as the sum of the rewards from multiple generated molecules.</p>
<p>In Table 5, we compare both approaches.One can see that the multi-stage setting significantly outperforms the single-stage setting.We hypothesize that this result stems from credit assignment issues in the single-stage setting.Namely, the single-stage setting lacks signals to capture molecule-wise impacts on diversity among a large set of molecules and fails to promote the generation of molecules responsible for increasing diversity.Coverage of the target molecules.We further validate how our method benefits capturing the wide range of target molecular space.Here, we compute how many target molecules in the L+M-24 dataset (an average of 17 molecules per description) can be captured within the generation space.The target molecule is considered captured if it is significantly similar to one of the generated molecules (Dice similarity &gt; 0.95).In Table 6, we present the average ratio of captured target molecules.One can see that our method yields the highest score.</p>
<p>Conclusion</p>
<p>In this paper, we identify the limitations of large language models (LLMs) for generating diverse molecules.In response, we present a new finetuning approach to adapt existing LLMs to generate diverse molecules.Experiments show that our approach enables LLMs to better discover diverse molecules compared to the existing approaches.This success highlights the potential of our method to advance LLM-driven drug discovery.</p>
<p>Limitations</p>
<p>Our method may require autoregressively generating a very long sequence to induce a huge set of molecules.In response, an interesting avenue for future work is to reduce the space and time complexity in generating the sequence of molecules, for example, introducing continuous tokens (Hao et al., 2025) that encode the set of previously generated molecules (Zaheer et al., 2017).Next, we validated whether the LLM-generated molecules satisfy the given molecular descriptions using computational approaches.However, this does not imply that the generated molecules will necessarily satisfy the given molecular descriptions in tricky real-world scenarios.To ensure the reliability of our method, the generated molecules should be validated through the real-world experiments.Additionally, although the molecules generated by each method are valid in computational terms, they may not be synthesizable in practice.Lastly, due to the limited computational budgets, we conducted experiments only on models with up to 7B parameters.The generalizability of our method to larger models (e.g., 70B) remains unexplored and is left for future work.</p>
<p>Ethical Considerations</p>
<p>Our fine-tuning method enables LLMs to generate diverse molecules from textual descriptions of molecular properties.However, these advancements also introduce potential risks, such as the generation of harmful drugs and the misuse of synthesized molecules.</p>
<p>A Additional Related Works</p>
<p>A.1 Reinforcement Learning (RL) for Diverse Molecular Generation</p>
<p>Existing literature has studied RL-based methods to generate molecules with desired properties while enhancing their diversity.First, Blaschke et al. (2020); Pereira et al. (2021) introduced memory-assisted RL, which penalizes the reward of a molecule when it is highly similar to the molecules stored in the memory unit.He et al. (2024) also incorporated RL with a diversity penalty in transformer-based architectures for molecular generation.In addition, Hu et al. (2024) leveraged multiple GPT-based agents trained with RL to encourage these agents to explore diverse directions for discovering diverse molecules.Their algorithms are designed to consider a fixed target property.In contrast, our work fine-tunes LLMs to generate diverse molecules given a prompt that is flexible to describe various target properties.</p>
<p>A.2 Molecular Similarity Measures</p>
<p>In this section, we explain the structural similarity between two molecules.These measures are used to define the reward of reinforcement learning (Appendix B) and diversity metrics (Appendix A.3). Molecular structural features.First, molecular structural features are expressed with Morgan fingerprint (Rogers and Hahn, 2010), which is a vector that characterizes the presence of specific atoms, bonds, or substructures in a given molecule.Note that we get this from RDkit package (Landrum, 2016).We denote this with f (m), which maps the molecule m to its Morgan fingerprint.Next, we compute the molecular structural similarity based on the molecular fingerprints.</p>
<p>Tanimoto similarity.The overall similarity between two molecules is typically evaluated as follows:
T (m i , m j ) = |f (m i ) ∩ f (m i )| |f (m i ) ∪ f (m j )| ,
where f (m i ) maps the molecule m i to its Morgan fingerprint.This similarity T (m i , m j ) is referred to as Tanimoto similarity (Bajusz et al., 2015), evaluating overall structural similarity.Dice similarity.In this paper, we also consider Dice similarity (Bajusz et al., 2015) which is sensitive to the degree of shared structural features between two molecules.This is defined as follows:
D(m i , m j ) = 2|f (m i ) ∩ f (m i )| |f (m i )| + |f (m j )| ,
where ⟨•, •⟩ denotes a dot product between two vectors.</p>
<p>A.3 Molecular Diversity Metrics</p>
<p>In this section, we provide a detailed explanation of the diversity metrics for evaluating the given set of molecules.Specifically, we explain two diversity metrics: the number of circles (Xie et al., 2023) and the internal diversity (Polykovskiy et al., 2020).</p>
<p>The number of circles (NCircles.;Xie et al., 2023).To evaluate the diversity of a given set of molecules M, this computes the size of the largest subset of molecules in which no two molecules are similar to each other.This metric is defined with a Tanimoto similarity T (•, •) and a similarity threshold h as follows:
NCircles h = max C⊆M |C| s.t. T (x, y) &lt; h, ∀x ̸ = y ∈ C,(2)
where C is a subset of molecules.Every pair of molecules in C should have a similarity lower than h.The high NCircles value implies that the given set of molecules M is diverse and covers a wide range of molecular space (Xie et al., 2023).Recent work (Renz et al., 2024) haven shown that this metric is relatively exact to measure the molecular diversity, compared to the other metrics, e.g., internal diversity.</p>
<p>Internal diversity (IntDiv.;Polykovskiy et al., 2020).Given a set of molecules M, this metric measures the average of pair-wise Tanimoto similarities:
Intdiv. = 1 |M| • (|M| − 1) |M| i=1 |M| j=i+1 (1 − T (m i , m j )),(3)
where m i is i-th molecule in the given set of molecules M.</p>
<p>B Detailed Implementations and Training</p>
<p>B.1 Supervised Fine-tuning Dataset collection.To fine-tune BioT5 + (Section 4.1), we collect T = 100 molecules using contrastive beam search for each training molecular description in ChEBI-20 training split.In the case of molecular descriptions from the L+M-24 training split, we use only the provided set of target molecules for each description, without additional data collection.To fine-tune DrugAssist (Section 4.3), we collect T = 300 molecules using contrastive beam search for each training prompt.Note that the collected molecules were filtered to remove invalid string representations, duplicated molecules, and unaccepted molecules.</p>
<p>The invalid string representations are evaluated with RDKit package (Landrum, 2016).Additionally, the collected molecules are concatenated into a single sequence m 1 || • • • ||m K .Supervised learning.We consider four NVIDIA A100 SXM4 40GB GPUs for supervised fine-tuning.</p>
<p>• For the supervised fine-tuning of BioT5 + (Section 4.1), we consider 80 epochs, 8-batch size, 5e − 4 learning rate, 0.05 warm-up ratio, and apply a cosine learning scheduler.The maximum sequence length in supervised training is limited to 2560 due to memory limitations.</p>
<p>• For the supervised fine-tuning of DrugAssist (Section 4.3), we consider 80 epochs, 4-batch size, 3e − 5 learning rate, 0.05 warm-up ratio, and apply a cosine learning scheduler.The maximum sequence length in supervised training is limited to 1024 due to memory limitations.We also apply LoRA (Hu et al., 2022), where the rank and alpha are 64 and 128, respectively.</p>
<p>B.2 Reinforcement Learning</p>
<p>Reward design.In experiments on L+M-24 and ChEBI-20 datasets (Section 4.1), we define the description-matching reward using the Dice similarity as follows:
r match (m k , p desc ) = max m target∈M D (p desc ) D(m target , m k ) α ,(4)
where M D (p desc ) is a set of target molecules provided in the datasets for each molecular description p desc . 7s described in Appendix A.2, D(m i , m j ) is Dice similarity captuing the degree of shared structural features between two molecules.Note that α is a hyper-parameter.In experiments with DrugAssist (Section 4.3), the description-matching reward yields 1 if the molecule satisfies the quantitative properties described in p desc (evaluated using RDkit (Landrum, 2016)) and 0 otherwise.Next, the diversity reward, r div , is defined to consider molecular structural diversity between the new molecule m k and the previously generated molecules {m i } k−1 i=1 within a sequence:
r div (m k , {m i } k−1 i=1 ) = 1 − max m∈{m i } k−1 i=1 T (m k , m) β ,(5)
where T (m i , m j ) is Tanimoto similarity (Appendix A.2) between m i and m j and β is a hyper-parameter.Policy optimization.We consider four NVIDIA A100 SXM4 40GB for reinforcement learning implemented with proximal policy optimization.</p>
<p>• For the reinforcement learning of BioT5 + (Section 4.1), we consider 200 PPO iterations, 8 mini-batch size, 128 batch size, and 5e − 5 learning rate.We also consider 0.01 KL penalty.Note that α and β in Equations ( 4) and ( 5) are 0.5 and 1.0, respectively.We increate β to 2.0 during training on ChEBI-20 dataset.The reward signal is amplified by multiplying by a value of 8.0.The maximum sequence length in reinforcement learning is limited to 2560 due to memory limitations.Here, we also apply LoRA (Hu et al., 2022) where the rank and alpha are 16 and 32, respectively.</p>
<p>• For the reinforcement learning of DrugAssist (Section 4.3), we consider 200 PPO iterations, 4 minibatch size, 64 batch size, and 3e − 6 learning rate.We also consider 0.1 KL penalty.Note that β in Equation ( 5) is 2.0.The reward signal is amplified by multiplying by a value of 8.0.The maximum sequence length in reinforcement learning is limited to 1024 due to memory limitations.We also apply LoRA (Hu et al., 2022) where the rank and alpha are 64 and 128, respectively.In this section, we describe detailed data statistics of ChEBI-20 (Edwards et al., 2021) and L+M-24 datasets (Edwards et al., 2024).Both datasets serve to validate description-guided molecular generation capability.Note that both datasets consider a molecular description expressed in English and a molecule represented by SMILES (Weininger, 1988).In Table 7, we provide the number of molecular descriptions.</p>
<p>C Dataset Details</p>
<p>To be specific, L+M-24 dataset provides an average of 70 and 17 target molecules for each description in training and test splits, respectively.Note that the original L+M-24 dataset provides a single target molecule for each description.However, L+M-24 dataset involves identical molecular properties for different molecules.Moreover, some molecular descriptions, e.g., an inhibitor of both BCL2 and BTK proteins, include the molecular properties of other descriptions, e.g., an inhibitor of BCL2 protein.Thus, we associate multiple molecules with a single molecular description.Next, ChEBI-20 dataset provides a single example of target molecule for each description.</p>
<p>In addition, since the original BioT5 + is not pre-trained on L+M-24 dataset, we further train it on the original L+M-24 training split.Note that we use the first 1, 000 molecular descriptions in the test split of the L+M-24 dataset for evaluation, as evaluating the entire test split with 50 generated molecules per description takes too much time (five to six days).</p>
<p>D Experiments Setup</p>
<p>Our experiments consider various molecular generative LLMs: MolT5 (Edwards et al., 2022), BioT5 + (Pei et al., 2024), Text+Chem T5 (Christofidellis et al., 2023), LlasMol (Yu et al., 2024), DrugAssist (Ye et al., 2023), and Meditron (Chen et al., 2023). 89They consider a molecular description expressed in English.They also consider a molecule represented as SMILES (Weininger, 1988), except for BioT5 + , which uses SELFIES (Krenn et al., 2020).T5 models have 220M parameters, and others have 7B parameters.We use four NVIDIA A100 SXM4 40GB GPUs for the experiments.We consider a single run.Comparison with decoding schemes.In this experiment, we first consider random sampling with different temperatures {0.7, 1.0, 1.5}.For the other decoding schemes, we consider conventional configurations: nucleus sampling with top-p 0.8, beam search, diverse beam search with a diversity penalty of 0.5, and contrastive beam search with a penalty alpha of 0.5.We apply greedy decoding for our approach.The prompts for BioT5 + are described in Table 8.Comparison with existing LLMs.For all existing LLMs, we apply random sampling, beam search, diverse beam search with a diversity penalty of 0.5, and contrastive beam search with a penalty alpha of 0.5.The prompts for existing LLMs, which are molecular generative LLMs based on a given molecular description, are described in Table 9.  Fine-tuning generalist LLMs.We synthesize 600 pairs of training prompts and corresponding sets of molecules.The prompts specify hydrogen bond donors and acceptors ranging from one to four, and a Bertz complexity ranging from 0 to 300.The sets of molecules are collected by applying beam search on DrugAssist and then perturbed by shuffling their order.We use the prompts described in Table 10.The system prompt follows the default settings from (Ye et al., 2023).For evaluation, we use four prompts specifying three hydrogen bond donors, three hydrogen bond acceptors, a Bertz complexity between 100 and 200, and a QED value between 0.4 and 0.6.Additionally, as shown in Figure 1, we try to generate diverse molecules by designing prompts (Table 11).However, these prompts show lower performance compared to applying beam search.</p>
<p>Table 12: Visualization of the generated molecules.We generate 50 molecules from: "The molecule is a egfr inhibitor and modulate, belonging to the cancer treatment class of molecules, and is treatment of disorder".The blue line indicates the molecule that follows the given description, as evaluated using the external tool (docking tool; Trott and Olson, 2010).We also evaluate structurally similar molecules (Tanimoto similarity &gt; 0.7).</p>
<p>50 molecules generated from BS 50 molecules generated from ours</p>
<p>E Additional Results</p>
<p>In this section, we present qualitative results by evaluating whether the generated molecules satisfy the given molecular description using an external tool.Here, we consider the generation of a set of molecules from: "The molecule is a egfr inhibitor and modulate, belonging to the cancer treatment class of molecules, and is treatment of disorder" obtained from L+M-24 test split.Note that the molecule with sufficient binding affinity to EGFR can satisfy this description.Thus, we evaluate the generated molecule using a docking tool (García-Ortegón et al., 2022).We accept the generated molecule if it yields a Vina score (Trott and Olson, 2010) below −7.0 when docked with an EGFR protein, which is the threshold for a potential inhibitor of the protein.We present the results in Table 12.One can see that BS generates structurally similar molecules while our method generates structurally diverse molecules.</p>
<p>NC(=[NH2+])c1csc(N)c1 NC(=[NH2+])c1csc(c2ccc2)c1 NC(=[NH2+])c1csc(C(C)c2ccc2)c1 NC(=[NH2+])c1csc(C(C)c2ccc2)c1 NC(=[NH2+])c1csc(C(C)c2ccc2)c1 …</p>
<p>(Left) Diverse output sequences (SMILES) induce the same molecular structures.(Right) Improved textual diversity via diverse beam search does not enhance molecular diversity in the experiments (Section 4.1).</p>
<p>Figure 2 :
2
Figure 2: Illustration of proposed fine-tuning approaches.We consider two stages for fine-tuning LLMs: a supervised fine-tuning Figure 2(a) and a reinforcement learning Figure 2(b).The prompts are simplified for explanatory purposes, and the actual prompts are provided in Appendix D.</p>
<p>i=1 7 :
7
until Converged 8: Output: fine-tuned π SFT Dataset collection.The supervised training process is conducted with a set of training prompts P.</p>
<p>Implementations.</p>
<p>For supervised fine-tuning, we collect molecules for each description p desc in the training splits (a maximum of 100 molecules per description).Then, the collected molecules are filtered to remove invalid, duplicated, and unaccepted molecules.The description-matching reward r match (m k ) is defined as the maximum Dice similarity between the molecule m k and the provided target molecules.Next, the diversity reward r div (m k , {m i } k−1 i=1 ) is defined as the complement of the maximum Tanimoto similarity between the molecule m k and the previously generated molecules {m i } k−1 i=1 .We describe detailed implementations in Appendix B.</p>
<p>Figure 3 :
3
Figure3: Experiments with fine-tuned generalist LLMs (DrugAssist;Ye et al., 2023).Our method consistently improves the performance for generating diverse and high-quality molecules.</p>
<p>Reinforcement learning (Algorithm 2): maximizes the diversity between new and previously generated molecules.
dataset collectionsupervised trainingQ: Can you generate a molecule with <property>?A: MOL 1 A: MOL K …Q: Can you generate a set of molecules with <property>? A: MOL 1, …, MOL 𝐾maximizing likelihoodrepeating 𝑁 times𝜋 $%&amp;(a) Supervised fine-tuning (Algorithm 1): enables generating a sequence of multiple molecules.𝑘 th stageQ: Can you generate a set of molecules with <property>?policy optimizationA: MOL 1, …, MOL 𝑘 − 1,MOL 𝑘𝑟reward computation (structural diversity)𝜋 !"computing structural diversity(b)</p>
<p>Dataset Method NCircles h=0.85 NCircles h=0.75 Accepted &amp; Unique IntDiv.
Random1.0790.9481.9700.109Nucleus1.0060.9181.6230.090BS4.5622.60333.4060.176L+M-24 Diverse BS2.3951.7439.9150.234Contrastive BS4.5212.59433.5680.176Div-SFT5.1983.20520.7110.250Div-SFT+RL10.516.27831.0600.287Random1.5391.3831.9980.045Nucleus0.8970.8760.9720.014BS5.5764.17111.7340.194ChEBI-20 Diverse BS4.4133.6375.9050.140Contrastive BS7.9555.88315.2150.233Div-SFT4.8693.6018.1780.141Div-SFT+RL11.3018.99616.2710.246</p>
<p>Table 3 :
3
Comparison with results obtained by applying existing decoding schemes to each LLM.The base LLM of our method is BioT5 + .For each baseline, we report the best result obtained by one of the existing decoding schemes.Our fine-tuned model shows superiority in generating diverse molecules by yielding highest NCircles.
DatasetMethodNCircles h=0.85 NCircles h=0.75 Accepted &amp; Unique IntDiv.MolT5 (Diverse BS)6.2954.30825.8410.278L+M-24BioT5 + (Contrastive BS) Meditron (Diverse BS)4.562 4.6932.603 3.03733.568 27.8370.176 0.263Ours10.5116.27831.0620.287MolT5 (Diverse BS)1.9021.5092.7980.101Text+Chem T5 (Contrastive BS)6.6744.40815.8280.205ChEBI-20 BioT5 + (Contrastive BS)7.9555.88315.2150.233LlaSMol (BS)7.3715.49216.9780.217Ours11.3018.96616.2710.2460 10 20 30 40 50 Sampled Molecules 0 5 10 15 20 25 30 35 40 45 NCirclesh=0.75 Random BS Diverse BS Constrastive BS Ours</p>
<p>Table 4 :
4
Experiments with the large number of samples.The base LLM is BioT5 + .Our method discovers more diverse molecules with respect to the (1) the number of generations and (2) time costs.Method num. of generations BS 300 BS 400 BS 500 Ours 100 Ours 150 Ours 200
NCircles h=0.8513.971 14.553 15.821 14.472 17.215 19.134Time (sec.)32345258575107146</p>
<p>Table 5 :
5
Comparison with variants of implementations.Our multi-stage reinforcement learning shows superior performance compared to the single-stage reinforcement learning.
MethodNCircles h=0.85 NCircles h=0.75 Accepted &amp; Unique IntDiv.Div-SFT+RL single5.1213.8468.1370.160Div-SFT+RL (ours)11.3018.96616.2710.246</p>
<p>Table 6 :
6
Coverage of the target molecules in L+M-24 dataset.The base LLM is BioT5 + .
MethodTarget CoverageRandom0.137BS0.420Contrastive BS0.420Diverse BS0.343Ours0.558</p>
<p>Table 7 :
7
Data statistics.The number of molecular descriptions.
Dataset♯ of training ♯ of testL+M-24160,49221,839ChEBI-2026,4083,301</p>
<p>Table 8 :
8
(Pei et al., 2024)+(Pei et al., 2024).
PromptContents"Definition: You are given a molecule description in English. Your job isto generate the molecule SELFIES that fits the description.p descNow complete the following example -Input: <molecular description>Output: ""Definition: You are given a molecule description in English. Your job isto generate the molecule SELFIES that fits the description.p desc+div (fine-tuning)Now provide a set of molecules -Input: <molecular description>Output: "</p>
<p>Table 9 :
9
Prompts for various existing LLMs
Methodp descMolT5"<molecular description>""Below is an instruction that describes a task, paired with an input that providesfurther context. Write a response that appropriately completes the request.### Instruction:You are a researcher. You can come up molecule smile strings based on yourexisting knowledge.MeditronMolecule smile strings are given against the following input. You should be asdetailed as possible.Input:<molecular description>In that caption, could you generate a molecule smile string?### Response: "Text+Chem T5 "<molecular description>"
LlasMol"Give me a molecule that satisfies the conditions outlined in the description: <molecular description>"</p>
<p>Table 10 :
10
(Ye et al., 2023)ssist(Ye et al., 2023).Hydrogen bond donors and acceptors: "Can you generate a molecule with <value> <property>?Print it in SMILES format."QED and Bertz complexity: "Can you generate a molecule with <property> below <value1> but at least <value2>?Print it in SMILES format."p desc+div (fine-tuning) Hydrogen bond donors and acceptors: "Can you generate a set of molecules that have <value> <property>?Print each of them in SMILES format."QED and Bertz complexity: "Can you generate a set of molecules that have <property> below <value1> but at least <value2>?Print each of them in SMILES format."
PromptContentsp desc</p>
<p>Table 11 :
11
Prompts for DrugAssist (without fine-tuning, Figure1).Can you generate a set of molecules?Each molecule has <value> <property>.Print each of them in SMILES format.""Can you generate a diverse set of molecules?Each molecule has <value> <property>.Print each of them in SMILES format.""Can you generate a structurally diverse set of molecules?Each molecule has <value> <property>.Print each of them in SMILES format.""Can you generate a set of molecules with <value> <property>?Print each of them in SMILES format.""Can you generate a diverse set of molecules with <value> <property>?Print each of them in SMILES format.""Can you generate a structurally diverse set of molecules with <value> <property>?Print each of them in SMILES format."
p desc+div"</p>
<p>) generates different SMILES strings that map to an identical molecule.
More related works, e.g., RL for diverse molecular generation without using LLMs, are described in Appendix A.
We make comparisons between the sequential-and the molecule-wise approaches in Table5of Section
.4.
In L+M-24 training split, an average of 70 provided molecules corresponds to each molecular property description. We provide the data statistics in Appendix C.
  5  In Section 4.4, we further validate our method by generating 100, 150, and 200 molecules for each description.
In Table2and Section 4.3, we also evaluate whether the generated molecules actually satisfy the molecular description using an external tool, e.g., a docking tool.
As described in Section 4.1, we assume that the molecule satisfies the description if it shares a certain degree of structures with one of the target molecules provided in the dataset.
MolT5, BioT5 + , Text+Chem T5, LlasMol, and DrugAssist are licensed under the MIT License.
Meditron is licensed under the Apache 2.0 License.
F Use of AI AssistantsWe used AI-based writing assistants to improve the sentence structure and grammar.These tools were used only for editorial improvements.The technical content, methodology, and experimental results were entirely authored by the researchers.
Why is tanimoto index an appropriate choice for fingerprint-based similarity calculations. Dávid Bajusz, Anita Rácz, Károly Héberger, Journal of cheminformatics. 712015</p>
<p>The first general index of molecular complexity. H Steven, Bertz, Journal of the American Chemical Society. 103121981</p>
<p>Quantifying the chemical beauty of drugs. Richard Bickerton, V Gaia, Jérémy Paolini, Besnard, Andrew L Sorel Muresan, Hopkins, Nature chemistry. 422012</p>
<p>Memory-assisted reinforcement learning for diverse molecular de novo design. Thomas Blaschke, Ola Engkvist, Jürgen Bajorath, Hongming Chen, Journal of cheminformatics. 121682020</p>
<p>Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.16079Meditron-70b: Scaling medical pretraining for large language models. 2023arXiv preprint</p>
<p>Unifying molecular and textual representations via multi-task language modelling. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, Matteo Manica, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023202</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>L+M-24: Building a dataset for Lan-guage+Molecules @ ACL 2024. Carl Edwards, Qingyun Wang, Lawrence Zhao, Heng Ji, 10.18653/v1/2024.langmol-1.1Proceedings of the 1st Workshop on Language + Molecules (L+M 2024). the 1st Workshop on Language + Molecules (L+M 2024)Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Text2Mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, 10.18653/v1/2021.emnlp-main.47Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Hua , The Twelfth International Conference on Learning Representations. jun Chen. 2024</p>
<p>Dockstring: easy molecular docking yields better benchmarks for ligand design. Miguel García-Ortegón, Gregor Nc Simm, Austin J Tripp, José Miguel Hernández-Lobato, Andreas Bender, Sergio Bacallado, Journal of chemical information and modeling. 62152022</p>
<p>Searching for high-value molecules using reinforcement learning and transformers. Raj Ghugare, Santiago Miret, Adriana Hugessen, The Twelfth International Conference on Learning Representations. Mariano Phielipp, and Glen Berseth. 2024</p>
<p>Fine-tuned language models generate stable inorganic materials as text. Nate Gruver, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, C Lawrence Zitnick, Zachary Ward Ulissi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Shibo Hao, Sainbayar Sukhbaatar, Dijia Su, Xian Li, Zhiting Hu, Jason E Weston, Yuandong Tian, Training large language model to reason in a continuous latent space. 2025</p>
<p>Evaluation of reinforcement learning in transformer-based molecular design. Jiazhen He, Alessandro Tibo, Jon Paul Janet, Eva Nittinger, Christian Tyrchan, Werngard Czechtizky, Ola Engkvist, Journal of Cheminformatics. 161952024</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, The Tenth International Conference on Learning Representations. 2020</p>
<p>Late-stage diversification of natural products. Benke Hong, Tuoping Luo, Xiaoguang Lei, ACS central science. 652020</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, The Tenth International Conference on Learning Representations. 2022</p>
<p>De novo drug design using reinforcement learning with multiple gpt agents. Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>Understanding the effects of rlhf on LLM generalisation and diversity. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Diversification of the drug discovery process. Allen Krantz, Nature biotechnology. 16131998</p>
<p>Selfreferencing embedded strings (selfies): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Machine Learning: Science and Technology. 14450242020Pascal Friederich, and Alan Aspuru-Guzik</p>
<p>Rdkit: Open-source cheminformatics software. Greg Landrum, 2016</p>
<p>Aleksandr Savelev, Tadhurst-Cdd, Alain Vaucher, and Jeremy Monat. Greg Landrum, Paolo Tosco, Brian Kelley, Ricardo Rodriguez, David Cosgrove, Riccardo Vianello, Peter Sriniker, Gareth Gedeck, Nadine Jones, Eisuke Schneider, Dan Kawashima, Andrew Nealschneider, Dalke, 10.5281/zenodo.14779836rdkit/rdkit: 2024_09_5 (q3 2024) release (release_2024_09_5). Matt Swain, Brian Cole, Samo Turk2025</p>
<p>Minimum error rate training in statistical machine translation. Josef Franz, Och, 10.3115/1075096.1075117Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. the 41st Annual Meeting of the Association for Computational LinguisticsSapporo, JapanAssociation for Computational Linguistics2003OpenAI. 2023. Chatgpt: Openai language model</p>
<p>Introducing openai o1 preview. 2024OpenAIAccessed</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning. Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, Rui Yan, Findings of the Association for Computational Linguistics ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023</p>
<p>Diversity oriented deep reinforcement learning for targeted molecule generation. Tiago Pereira, Maryam Abbasi, Bernardete Ribeiro, Joel P Arrais, Journal of cheminformatics. 131212021</p>
<p>Alan Aspuru-Guzik, and Alex Zhavoronkov. 2020. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Frontiers in Pharmacology. </p>
<p>Diverse hits in de novo molecule design: Diversity-based comparison of goal-directed generators. Philipp Renz, Sohvi Luukkonen, Günter Klambauer, Journal of Chemical Information and Modeling. 64152024</p>
<p>Journal of chemical information and modeling. David Rogers, Mathew Hahn, 201050Extendedconnectivity fingerprints</p>
<p>A stochastic difference equation for beam search. Jacob L Rosenberg, David G Baldwin, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningACM1965</p>
<p>Computational approaches streamlining drug discovery. V Anastasiia, Vsevolod Sadybekov, Katritch, Nature. 61679582023</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, arXiv:2405.14655Multi-turn reinforcement learning from preference human feedback. 2024arXiv preprint</p>
<p>A contrastive framework for neural text generation. Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, Nigel Collier, Advances in Neural Information Processing Systems. 202235</p>
<p>Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Preprint</p>
<p>Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Oleg Trott, Arthur J Olson, 10.1002/jcc.21334Journal of Computational Chemistry. 3122010</p>
<p>Applications of machine learning in drug discovery and development. Jessica Vamathevan, Dominic Clark, Paul Czodrowski, Ian Dunham, Edgardo Ferran, George Lee, Bin Li, Anant Madabhushi, Parantu Shah, Michaela Spitzer, Nature reviews Drug discovery. 1862019</p>
<p>Diverse beam search for improved description of complex scenes. Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, Dhruv Batra, 10.1609/aaai.v32i1.12340Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>K Ashwin, Michael Vijayakumar, Cogswell, Qing Ramprasath R Selvaraju, Stefan Sun, David Lee, Dhruv Crandall, Batra, arXiv:1610.02424Diverse beam search: Decoding diverse solutions from neural sequence models. 2016arXiv preprint</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Cad-LLM: Large language model for cad generation. Sifan Wu, Amir Khasahmadi, Mor Katz, Pradeep Kumar Jayaraman, Yewen Pu, Karl Willis, Bang Liu, NeurIPS 2023 Workshop on Machine Learning for Creativity and Design. NeurIPS2023a</p>
<p>Finegrained human feedback gives better rewards for language model training. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, Hannaneh Hajishirzi, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>How much space has been explored? measuring the chemical space covered by databases and machine-generated molecules. Yutong Xie, Ziqiao Xu, Jiaqi Ma, Qiaozhu Mei, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, arXiv:2401.10334Drugassist: A large language model for molecule optimization. 2023arXiv preprint</p>
<p>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, First Conference on Language Modeling. 2024</p>
<p>Deep sets. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, Alexander J Smola, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar, arXiv:2402.19446Archer: Training language model agents via hierarchical multi-turn rl. 2024arXiv preprint</p>
<p>Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang, arXiv:2403.07920ProtLLM: An interleaved proteinlanguage LLM with protein-as-word pre-training. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
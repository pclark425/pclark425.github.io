<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6882 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6882</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6882</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-277356221</p>
                <p><strong>Paper Title:</strong> A Perspective on Foundation Models in Chemistry</p>
                <p><strong>Paper Abstract:</strong> Foundation models are an emerging paradigm in artificial intelligence (AI), with successful examples like ChatGPT transforming daily workflows. Generally, foundation models are large-scale, pretrained models capable of adapting to various downstream tasks by leveraging extensive data and model scaling. Their success has inspired researchers to develop foundation models for a wide range of chemical challenges, from materials discovery to understanding structure–property relationships, areas where conventional machine learning (ML) models often face limitations. In addition, foundation models hold promise for addressing persistent ML challenges in chemistry, such as data scarcity and poor generalization. In this perspective, we review recent progress in the development of foundation models in chemistry across applications of varying scope. We also discuss emerging trends and provide an outlook on promising approaches for advancing foundation models in chemistry.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6882.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6882.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDFM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dialogue Foundation Model for Chemistry (ChemDFM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemistry‑specialized instruction‑tuned LLaMA variant trained on scientific articles, textbooks and large chemistry instruction sets to perform property prediction and text‑based inverse design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemDFM: A Large Language Foundation Model for Chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B (pretrained then chemistry-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, instruction‑tuned / fine‑tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained LLaMA then trained on ~3.8M research articles and ~1.4K chemistry textbooks; instruction tuning with ~2.7M chemistry instructions derived from chemical databases (as reported in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>instruction‑tuning and finetuning for text-based generation and inverse design (prompting/instruction following to output molecule descriptions/SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES / molecular notations embedded in text (chemistry token/notation in text prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text-guided molecule design, molecule recognition, property prediction, reaction prediction (inverse design and prediction tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not explicitly enumerated in the perspective; generation is guided by textual instructions / conditioning but no concrete chemical filters reported in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not specifically reported here (model used as standalone LLM for generation and prediction in the cited study).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Scientific articles and textbooks (3.8M articles, 1.4K textbooks) plus 2.7M chemistry instruction pairs; specific chemistry corpora not further detailed in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream task performance on property prediction and inverse design compared to baselines (classification/regression metrics unspecified in the perspective summary).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to outperform 10-shot generalist LLMs (e.g., GPT-4) on property prediction and inverse design tasks and to be comparable to conventional specialist models (no numeric metrics provided in this perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Perspective notes general LLM limitations: potential hallucinations, need for domain-specific instruction tuning; no model‑specific chemical failure modes beyond these were quantified in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6882.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>nach0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>nach0 (multimodal chemical LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5‑family language model pretrained on both general NLP and chemical domain corpora and instruction‑tuned for cross‑domain chemistry tasks including text‑guided molecule design and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>nach0 (T5 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder (T5) fine‑tuned / instruction‑tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on mixed NLP and chemical domain datasets (paper cites PubMed/USPTO/ZINC tokens in reference list for domain corpora); then instruction‑tuned on multi-task chemical instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>instruction tuning followed by prompt-driven generation (text-to-SMILES and other text-to-chemical tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and natural language descriptions (text multimodal).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text-guided molecule generation, molecular property prediction, reaction prediction, text-to-SMILES generation.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not detailed in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not specified in the perspective (model used as an LLM for direct text-to-chemical generation and prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Mixture of chemical and NLP corpora (specific compositions not detailed in-perspective summary).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Compared to SOTA baselines on multi-task downstream problems (classification/regression); specific metrics not enumerated in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to outperform SOTA baselines on the evaluated cross-domain tasks (no numeric results provided in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General LLM limitations (hallucination, domain shift); no explicit experimental synthesis validation reported in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6882.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine‑tuned GPT-3 (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine‑tuned GPT-3 for molecular/materials property prediction and inverse design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical demonstration of adapting a general pretrained GPT‑3 via the OpenAI API to chemical property prediction and inverse design tasks, evaluated especially in low‑data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging large language models for predictive chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (pretrained OpenAI model, fine‑tuned via API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, fine‑tuned via API</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained GPT-3 (general web-scale corpora); fine‑tuned on task‑specific chemical examples/datasets provided by the study (not enumerated in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine‑tuning via API and conditional generation (prompted) to output SMILES/text; generation temperature used as a control (softmax temperature).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES/text string representations (text‑based outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular inverse design (case study: photoswitch molecules), molecular/materials property prediction (classification/regression).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Temperature-controlled sampling (reported tradeoff between novelty and validity when increasing softmax temperature); no explicit synthetic‑accessibility or toxicity filters described here.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not detailed in the perspective summary; evaluation used property predictors/benchmarks but external toolchain not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Task-specific fine-tuning datasets (not enumerated in the perspective); used as low-data regime benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Classification/regression performance compared to specialist models; for generative case: validity and novelty were examined (validity % and novelty qualitatively discussed).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Fine‑tuned GPT-3 performed comparably or outperformed conventional specialist models on classification (especially in low‑data regimes); in photoswitch inverse design it produced valid molecules with higher novelty at higher temperature but with increased invalidity risk (no numeric values provided in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Tradeoff observed between novelty and validity controlled by sampling temperature (higher novelty but higher invalidity); risk of generating invalid or chemically implausible outputs without downstream filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6882.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP‑MoLFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GP‑MoLFormer (foundation model for molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer pretrained on ~1.1 billion SMILES and finetuned with a parameter‑efficient 'pair‑tuning' method for property‑guided molecular optimization (inverse design).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoLFormer / GP‑MoLFormer (transformer pretrained on 1.1B SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (masked LM / autoregressive style) pretrained on SMILES; finetuned with pair‑tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining corpus: ~1.1 billion SMILES assembled from large molecular databases (PubChem, ZINC cited in the perspective for similar models).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretraining via masked language modeling on SMILES then property‑guided fine‑tuning using a parameter-efficient 'pair‑tuning' approach to steer generation toward target properties.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (tokenized SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Inverse design / property‑guided optimization for small‑molecule properties (examples: logP, QED, DRD2 activity).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Property conditioning implemented via pair‑tuning (implicit constraint during finetuning); no further explicit synthetic or toxicity filters described in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not explicitly reported in the perspective summary (evaluation likely used property predictors / benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Very large SMILES corpus (1.1B SMILES) drawn from public molecular databases (as reported for MoLFormer family).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Performance on inverse design tasks measured by property improvement/optimization and comparison to baselines on logP, QED, DRD2 tasks; specific metrics not enumerated in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Finetuned GP‑MoLFormer using pair‑tuning performed comparably to or better than baselines on inverse design tasks for logP, QED, and DRD2 (no numeric scores provided in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Perspective notes general limitations of text/SMILES LMs (validity of SMILES, novelty constrained by training distribution); model‑specific failure modes not numerically reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6882.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gruver et al. (LLaMA‑2 70B finetune)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine‑tuned LLaMA-2 70B for crystal generation (Gruver et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstration that a large pretrained LLM (LLaMA‑2 70B) can be finetuned and prompted to generate string representations of inorganic crystals conditioned on properties like space group, composition and formation energy (E_hull).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 70B (pretrained, then finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, fine‑tuned with prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained LLaMA-2 (general text) then finetuned on atomistic string representations of crystals (stringified crystal data produced from crystallographic representations).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting and finetuning on string-based crystal representations to generate crystal descriptions satisfying target property conditions (conditioning via prompt/finetune).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Text/string representations of crystal structures (invertible string encodings of crystals).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Generation of inorganic crystal structures with desired spacegroup, composition, and formation‑energy (E_hull) targets.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Conditioning by requested spacegroup, composition, and E_hull; no downstream DFT or synthesis filters reported in the perspective description of this work.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Uses string encodings of crystals (likely generated/parsed by crystallography code) but specific tool integration not detailed in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Stringified crystal datasets derived from crystallographic databases (exact dataset names not specified in the perspective summary).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Quality of generated crystal strings measured by validity of structure encodings and satisfaction of requested conditions (no numeric metrics reported in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Demonstrated adaptability of a large pretrained LLM to atomistic data and conditional crystal generation; quantitative metrics not provided in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Limitations include reliance on suitable invertible string encodings for crystals and the need to verify generated structures with downstream physics (DFT) to confirm stability; perspective highlights general risks of hallucination and invalid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6882.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KV‑PLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KV-PLM (Structure‑Text multimodal model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal BERT‑style PLM that fuses SMILES tokens into text and is pretrained by masked language modeling to enable text→molecule generation and structure‑text retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KV-PLM (BERT backbone with fused SMILES/text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-only transformer (BERT-style) multimodal model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SMILES tokenized with BPE, merged into textual descriptions drawn from S2ORC and chemistry corpora; pretrained with masked language modeling on these fused sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Masked language model decoding and retrieval-based generation (text conditioning to produce SMILES fragments or full SMILES via generation from the fused text tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES tokenized with byte-pair encoding (BPE) embedded within text.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text-to-molecule generation, molecule captioning, molecular property prediction and structure‑text retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Conditioning by textual descriptions; no explicit chemical filters described in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Model fuses textual and SMILES modalities; external generative decoders not specified for KV‑PLM in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>SMILES corpora (PubChem/ZINC-like sources), academic text corpus (S2ORC), and curated graph-text pairs (small paired sets cited).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Measured by generation quality (validity) and property-prediction performance versus baselines (e.g., D‑MPNN); specific numeric metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Successfully generated drug molecules from input text and performed comparably to baseline molecular property models (e.g., D‑MPNN) on evaluated tasks (no numeric values reported in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Multimodal alignment and ensuring chemical validity from text-conditioned SMILES generation; potential for hallucinated or invalid SMILES when decoding from text tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6882.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (text‑SMILES multimodal T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5 encoder‑decoder model trained on SMILES and text, used for text‑based inverse design and molecule captioning (text↔SMILES tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5 (T5 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer (T5) multimodal</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining on paired SMILES and textual descriptions (e.g., SciPubChemSTM dataset of structure-text pairs), exact corpus sizes reported in original work but not enumerated in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-to-sequence generation (text-conditioned decoding to SMILES and SMILES-to-text captioning).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and natural language text (paired).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text-based inverse molecular design and molecule captioning.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Conditioning by textual prompts; no further chemical constraint filters described in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used as an end-to-end text↔SMILES seq2seq generator; external property predictors/generative flows not described for MolT5 in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Paired SMILES/text corpora (SciPubChemSTM and similar structure‑text datasets referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream generation and captioning quality; specific metrics not listed in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Demonstrated viability of T5-style multimodal pretraining for text-based inverse design and molecule captioning (quantitative metrics not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same as other text-to-SMILES approaches: decoding validity, chemical plausibility, and limited extrapolation beyond training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6882.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoleculeSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecule Structure-Text Model (MoleculeSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal model learning joint graph and text representations enabling text‑based molecule editing, retrieval and property prediction, using an external generative model (MoFlow) for molecule synthesis from latent vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi‑modal molecule structure‑text model for text‑based retrieval and editing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoleculeSTM (graph encoder + text encoder + external generator MoFlow)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multimodal (GNN encoder for structure, transformer/BERT-style encoder for text, external invertible generative flow for graph generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on paired structure-text datasets (e.g., Sci-PubChemSTM: ~281K structure-text pairs as cited in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Text-to-graph generation by optimizing latent variable q of an external flow model (MoFlow) to maximize cosine similarity between text encoder and generated graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Molecular graphs (encoded by GNN) and textual descriptions; generation to graphs via MoFlow then decoded to molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text-guided molecule editing (single/multi-objective edits), binding-affinity-based editing, drug-relevance editing, retrieval and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Text-conditioned similarity objectives and property-specific target constraints in editing tasks; explicit synthetic accessibility or toxicity filters not described in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Integrates with MoFlow invertible flow model for converting latent vectors to molecular graphs; uses pretrained graph and text encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Sci-PubChemSTM (structure-text pairs ~281K) and other paired corpora referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Hit ratio on editing tasks (success rate for proposed edits), downstream property prediction accuracy; compared against baselines including genetic search.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>In four editing tasks MoleculeSTM achieved superior hit ratios compared to baselines (qualitative and comparative claims reported; numeric hit ratios not reproduced in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires an external generator (MoFlow) and efficient optimization in latent space; success depends on alignment quality between text and graph encoders and validity of decoded graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6882.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPMM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure‑Property Pretrained Molecular Model (SPMM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that jointly embeds SMILES and a property vector (53 RDKit properties) into a common space to enable bidirectional property‑to‑SMILES generation and zero/few‑shot property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPMM (BERT-style embedding of SMILES + property vector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder style model enabling generation and prediction (multimodal embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large SMILES corpora (PubChem cited: 50M entries referenced in the perspective) with RDKit-calculated property vectors used as the property modality.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Masking and conditional generation: property vector (PV) treated as a 53-length sentence with random masking; SMILES decoded conditioned on PV to produce molecules matching property specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (text strings) and a 53-dimensional property vector derived from RDKit descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Property‑to‑SMILES generation (inverse design), molecular property prediction, and reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Conditioning on specific property vectors (can be full, partial, or single properties); PV random masking during training to allow partial conditioning at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Uses RDKit to compute the property vector; generation performed within the model (no additional external physics simulations mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem-scale SMILES (~50M cited) with RDKit-calculated properties as PVs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity and novelty of generated molecules, downstream property prediction accuracy (MoleculeNet benchmark comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SPMM generated valid and novel molecules with desired property vectors across tested scenarios (all 53 properties, single, four, and none); property‑prediction performance comparable to SOTA on MoleculeNet when finetuned (no numeric metrics provided in the perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Potential masking/generalization limits when only partial PVs provided; reliance on RDKit descriptors which may not capture all target experimental constraints; no wet‑lab validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6882.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6882.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AtomGPT / Atomgpt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AtomGPT (text-based materials/molecule LLM experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Usage examples where general LLMs (GPT-2, Mistral 7B and AtomGPT variants) are adapted for materials property prediction and text-to-material generation using text descriptions produced by ChemNLP and similar pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, Mistral-7B, AtomGPT variants (pretrained LLMs, finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLMs (GPT-2, Mistral 7B) fine‑tuned / prompt‑based</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mistral-7B: 7B; GPT-2: unspecified in perspective</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained general LLM corpora; finetuned using text descriptions of materials generated by ChemNLP and other domain corpora (ChemNLP used to produce structured text descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Finetuning and prompting on textual descriptions to perform property prediction and text‑to‑materials generation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual descriptions of materials (from ChemNLP / Robocrystallographer style outputs), SMILES for molecules where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Materials property prediction, text-to-materials generation (materials inverse design), and molecule generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not detailed in the perspective; conditioning is via textual prompts/descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>ChemNLP used to produce structured textual descriptions as model input; downstream evaluation with property predictors/benchmarks implied.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Text corpora from ChemNLP outputs, public pretraining corpora for LLM backbones; dataset specifics not enumerated in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property prediction accuracy and generative validity/novelty measures compared to baselines; precise metrics not provided in the perspective summary.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>These finetuned LLMs performed competitively for materials property prediction and text-to-material generation tasks in the cited studies, but the perspective does not reproduce numeric performance details.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality of generated molecules/materials depends on the fidelity of textual encodings (ChemNLP/Robocrystallographer); potential for hallucinations and invalid outputs when decoding from text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Perspective on Foundation Models in Chemistry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChemDFM: A Large Language Foundation Model for Chemistry <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 2)</em></li>
                <li>GP‑MoLFormer: Foundation Model For Molecular Generation <em>(Rating: 2)</em></li>
                <li>Fine, Tuned Language Models Generate Stable Inorganic Materials as Text <em>(Rating: 2)</em></li>
                <li>Multi‑modal molecule structure‑text model for text‑based retrieval and editing <em>(Rating: 2)</em></li>
                <li>Large‑scale chemical language representations capture molecular structure and properties <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6882",
    "paper_id": "paper-277356221",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "ChemDFM",
            "name_full": "Dialogue Foundation Model for Chemistry (ChemDFM)",
            "brief_description": "A chemistry‑specialized instruction‑tuned LLaMA variant trained on scientific articles, textbooks and large chemistry instruction sets to perform property prediction and text‑based inverse design tasks.",
            "citation_title": "ChemDFM: A Large Language Foundation Model for Chemistry",
            "mention_or_use": "mention",
            "model_name": "LLaMA-13B (pretrained then chemistry-tuned)",
            "model_type": "decoder-only LLM, instruction‑tuned / fine‑tuned",
            "model_size": "13B",
            "training_data_description": "Pretrained LLaMA then trained on ~3.8M research articles and ~1.4K chemistry textbooks; instruction tuning with ~2.7M chemistry instructions derived from chemical databases (as reported in the perspective).",
            "generation_method": "instruction‑tuning and finetuning for text-based generation and inverse design (prompting/instruction following to output molecule descriptions/SMILES).",
            "chemical_representation": "SMILES / molecular notations embedded in text (chemistry token/notation in text prompts).",
            "target_application": "Text-guided molecule design, molecule recognition, property prediction, reaction prediction (inverse design and prediction tasks).",
            "constraints_used": "Not explicitly enumerated in the perspective; generation is guided by textual instructions / conditioning but no concrete chemical filters reported in this perspective.",
            "integration_with_external_tools": "Not specifically reported here (model used as standalone LLM for generation and prediction in the cited study).",
            "dataset_used": "Scientific articles and textbooks (3.8M articles, 1.4K textbooks) plus 2.7M chemistry instruction pairs; specific chemistry corpora not further detailed in the perspective.",
            "evaluation_metrics": "Downstream task performance on property prediction and inverse design compared to baselines (classification/regression metrics unspecified in the perspective summary).",
            "reported_results": "Reported to outperform 10-shot generalist LLMs (e.g., GPT-4) on property prediction and inverse design tasks and to be comparable to conventional specialist models (no numeric metrics provided in this perspective).",
            "experimental_validation": false,
            "challenges_or_limitations": "Perspective notes general LLM limitations: potential hallucinations, need for domain-specific instruction tuning; no model‑specific chemical failure modes beyond these were quantified in the perspective.",
            "uuid": "e6882.0",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "nach0",
            "name_full": "nach0 (multimodal chemical LLM)",
            "brief_description": "A T5‑family language model pretrained on both general NLP and chemical domain corpora and instruction‑tuned for cross‑domain chemistry tasks including text‑guided molecule design and property prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "nach0 (T5 backbone)",
            "model_type": "encoder-decoder (T5) fine‑tuned / instruction‑tuned",
            "model_size": null,
            "training_data_description": "Pretrained on mixed NLP and chemical domain datasets (paper cites PubMed/USPTO/ZINC tokens in reference list for domain corpora); then instruction‑tuned on multi-task chemical instructions.",
            "generation_method": "instruction tuning followed by prompt-driven generation (text-to-SMILES and other text-to-chemical tasks).",
            "chemical_representation": "SMILES and natural language descriptions (text multimodal).",
            "target_application": "Text-guided molecule generation, molecular property prediction, reaction prediction, text-to-SMILES generation.",
            "constraints_used": "Not detailed in the perspective summary.",
            "integration_with_external_tools": "Not specified in the perspective (model used as an LLM for direct text-to-chemical generation and prediction).",
            "dataset_used": "Mixture of chemical and NLP corpora (specific compositions not detailed in-perspective summary).",
            "evaluation_metrics": "Compared to SOTA baselines on multi-task downstream problems (classification/regression); specific metrics not enumerated in the perspective.",
            "reported_results": "Reported to outperform SOTA baselines on the evaluated cross-domain tasks (no numeric results provided in the perspective).",
            "experimental_validation": false,
            "challenges_or_limitations": "General LLM limitations (hallucination, domain shift); no explicit experimental synthesis validation reported in the perspective.",
            "uuid": "e6882.1",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Fine‑tuned GPT-3 (Jablonka et al.)",
            "name_full": "Fine‑tuned GPT-3 for molecular/materials property prediction and inverse design",
            "brief_description": "A practical demonstration of adapting a general pretrained GPT‑3 via the OpenAI API to chemical property prediction and inverse design tasks, evaluated especially in low‑data regimes.",
            "citation_title": "Leveraging large language models for predictive chemistry",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (pretrained OpenAI model, fine‑tuned via API)",
            "model_type": "decoder-only LLM, fine‑tuned via API",
            "model_size": null,
            "training_data_description": "Pretrained GPT-3 (general web-scale corpora); fine‑tuned on task‑specific chemical examples/datasets provided by the study (not enumerated in the perspective).",
            "generation_method": "Fine‑tuning via API and conditional generation (prompted) to output SMILES/text; generation temperature used as a control (softmax temperature).",
            "chemical_representation": "SMILES/text string representations (text‑based outputs).",
            "target_application": "Molecular inverse design (case study: photoswitch molecules), molecular/materials property prediction (classification/regression).",
            "constraints_used": "Temperature-controlled sampling (reported tradeoff between novelty and validity when increasing softmax temperature); no explicit synthetic‑accessibility or toxicity filters described here.",
            "integration_with_external_tools": "Not detailed in the perspective summary; evaluation used property predictors/benchmarks but external toolchain not specified.",
            "dataset_used": "Task-specific fine-tuning datasets (not enumerated in the perspective); used as low-data regime benchmarks.",
            "evaluation_metrics": "Classification/regression performance compared to specialist models; for generative case: validity and novelty were examined (validity % and novelty qualitatively discussed).",
            "reported_results": "Fine‑tuned GPT-3 performed comparably or outperformed conventional specialist models on classification (especially in low‑data regimes); in photoswitch inverse design it produced valid molecules with higher novelty at higher temperature but with increased invalidity risk (no numeric values provided in the perspective).",
            "experimental_validation": false,
            "challenges_or_limitations": "Tradeoff observed between novelty and validity controlled by sampling temperature (higher novelty but higher invalidity); risk of generating invalid or chemically implausible outputs without downstream filtering.",
            "uuid": "e6882.2",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GP‑MoLFormer",
            "name_full": "GP‑MoLFormer (foundation model for molecular generation)",
            "brief_description": "A large transformer pretrained on ~1.1 billion SMILES and finetuned with a parameter‑efficient 'pair‑tuning' method for property‑guided molecular optimization (inverse design).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MoLFormer / GP‑MoLFormer (transformer pretrained on 1.1B SMILES)",
            "model_type": "transformer (masked LM / autoregressive style) pretrained on SMILES; finetuned with pair‑tuning",
            "model_size": null,
            "training_data_description": "Pretraining corpus: ~1.1 billion SMILES assembled from large molecular databases (PubChem, ZINC cited in the perspective for similar models).",
            "generation_method": "Pretraining via masked language modeling on SMILES then property‑guided fine‑tuning using a parameter-efficient 'pair‑tuning' approach to steer generation toward target properties.",
            "chemical_representation": "SMILES strings (tokenized SMILES).",
            "target_application": "Inverse design / property‑guided optimization for small‑molecule properties (examples: logP, QED, DRD2 activity).",
            "constraints_used": "Property conditioning implemented via pair‑tuning (implicit constraint during finetuning); no further explicit synthetic or toxicity filters described in the perspective.",
            "integration_with_external_tools": "Not explicitly reported in the perspective summary (evaluation likely used property predictors / benchmarks).",
            "dataset_used": "Very large SMILES corpus (1.1B SMILES) drawn from public molecular databases (as reported for MoLFormer family).",
            "evaluation_metrics": "Performance on inverse design tasks measured by property improvement/optimization and comparison to baselines on logP, QED, DRD2 tasks; specific metrics not enumerated in the perspective summary.",
            "reported_results": "Finetuned GP‑MoLFormer using pair‑tuning performed comparably to or better than baselines on inverse design tasks for logP, QED, and DRD2 (no numeric scores provided in the perspective).",
            "experimental_validation": false,
            "challenges_or_limitations": "Perspective notes general limitations of text/SMILES LMs (validity of SMILES, novelty constrained by training distribution); model‑specific failure modes not numerically reported here.",
            "uuid": "e6882.3",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gruver et al. (LLaMA‑2 70B finetune)",
            "name_full": "Fine‑tuned LLaMA-2 70B for crystal generation (Gruver et al.)",
            "brief_description": "Demonstration that a large pretrained LLM (LLaMA‑2 70B) can be finetuned and prompted to generate string representations of inorganic crystals conditioned on properties like space group, composition and formation energy (E_hull).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLaMA-2 70B (pretrained, then finetuned)",
            "model_type": "decoder-only LLM, fine‑tuned with prompts",
            "model_size": "70B",
            "training_data_description": "Pretrained LLaMA-2 (general text) then finetuned on atomistic string representations of crystals (stringified crystal data produced from crystallographic representations).",
            "generation_method": "Prompting and finetuning on string-based crystal representations to generate crystal descriptions satisfying target property conditions (conditioning via prompt/finetune).",
            "chemical_representation": "Text/string representations of crystal structures (invertible string encodings of crystals).",
            "target_application": "Generation of inorganic crystal structures with desired spacegroup, composition, and formation‑energy (E_hull) targets.",
            "constraints_used": "Conditioning by requested spacegroup, composition, and E_hull; no downstream DFT or synthesis filters reported in the perspective description of this work.",
            "integration_with_external_tools": "Uses string encodings of crystals (likely generated/parsed by crystallography code) but specific tool integration not detailed in the perspective.",
            "dataset_used": "Stringified crystal datasets derived from crystallographic databases (exact dataset names not specified in the perspective summary).",
            "evaluation_metrics": "Quality of generated crystal strings measured by validity of structure encodings and satisfaction of requested conditions (no numeric metrics reported in the perspective).",
            "reported_results": "Demonstrated adaptability of a large pretrained LLM to atomistic data and conditional crystal generation; quantitative metrics not provided in the perspective summary.",
            "experimental_validation": false,
            "challenges_or_limitations": "Limitations include reliance on suitable invertible string encodings for crystals and the need to verify generated structures with downstream physics (DFT) to confirm stability; perspective highlights general risks of hallucination and invalid outputs.",
            "uuid": "e6882.4",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "KV‑PLM",
            "name_full": "KV-PLM (Structure‑Text multimodal model)",
            "brief_description": "A multimodal BERT‑style PLM that fuses SMILES tokens into text and is pretrained by masked language modeling to enable text→molecule generation and structure‑text retrieval.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "KV-PLM (BERT backbone with fused SMILES/text)",
            "model_type": "encoder-only transformer (BERT-style) multimodal model",
            "model_size": null,
            "training_data_description": "SMILES tokenized with BPE, merged into textual descriptions drawn from S2ORC and chemistry corpora; pretrained with masked language modeling on these fused sequences.",
            "generation_method": "Masked language model decoding and retrieval-based generation (text conditioning to produce SMILES fragments or full SMILES via generation from the fused text tokens).",
            "chemical_representation": "SMILES tokenized with byte-pair encoding (BPE) embedded within text.",
            "target_application": "Text-to-molecule generation, molecule captioning, molecular property prediction and structure‑text retrieval.",
            "constraints_used": "Conditioning by textual descriptions; no explicit chemical filters described in the perspective summary.",
            "integration_with_external_tools": "Model fuses textual and SMILES modalities; external generative decoders not specified for KV‑PLM in the perspective.",
            "dataset_used": "SMILES corpora (PubChem/ZINC-like sources), academic text corpus (S2ORC), and curated graph-text pairs (small paired sets cited).",
            "evaluation_metrics": "Measured by generation quality (validity) and property-prediction performance versus baselines (e.g., D‑MPNN); specific numeric metrics not provided.",
            "reported_results": "Successfully generated drug molecules from input text and performed comparably to baseline molecular property models (e.g., D‑MPNN) on evaluated tasks (no numeric values reported in the perspective).",
            "experimental_validation": false,
            "challenges_or_limitations": "Multimodal alignment and ensuring chemical validity from text-conditioned SMILES generation; potential for hallucinated or invalid SMILES when decoding from text tokens.",
            "uuid": "e6882.5",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5 (text‑SMILES multimodal T5)",
            "brief_description": "A T5 encoder‑decoder model trained on SMILES and text, used for text‑based inverse design and molecule captioning (text↔SMILES tasks).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolT5 (T5 backbone)",
            "model_type": "encoder-decoder transformer (T5) multimodal",
            "model_size": null,
            "training_data_description": "Pretraining on paired SMILES and textual descriptions (e.g., SciPubChemSTM dataset of structure-text pairs), exact corpus sizes reported in original work but not enumerated in the perspective summary.",
            "generation_method": "Sequence-to-sequence generation (text-conditioned decoding to SMILES and SMILES-to-text captioning).",
            "chemical_representation": "SMILES and natural language text (paired).",
            "target_application": "Text-based inverse molecular design and molecule captioning.",
            "constraints_used": "Conditioning by textual prompts; no further chemical constraint filters described in the perspective summary.",
            "integration_with_external_tools": "Used as an end-to-end text↔SMILES seq2seq generator; external property predictors/generative flows not described for MolT5 in the perspective.",
            "dataset_used": "Paired SMILES/text corpora (SciPubChemSTM and similar structure‑text datasets referenced).",
            "evaluation_metrics": "Downstream generation and captioning quality; specific metrics not listed in the perspective summary.",
            "reported_results": "Demonstrated viability of T5-style multimodal pretraining for text-based inverse design and molecule captioning (quantitative metrics not provided here).",
            "experimental_validation": false,
            "challenges_or_limitations": "Same as other text-to-SMILES approaches: decoding validity, chemical plausibility, and limited extrapolation beyond training distribution.",
            "uuid": "e6882.6",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MoleculeSTM",
            "name_full": "Molecule Structure-Text Model (MoleculeSTM)",
            "brief_description": "A multimodal model learning joint graph and text representations enabling text‑based molecule editing, retrieval and property prediction, using an external generative model (MoFlow) for molecule synthesis from latent vectors.",
            "citation_title": "Multi‑modal molecule structure‑text model for text‑based retrieval and editing",
            "mention_or_use": "mention",
            "model_name": "MoleculeSTM (graph encoder + text encoder + external generator MoFlow)",
            "model_type": "multimodal (GNN encoder for structure, transformer/BERT-style encoder for text, external invertible generative flow for graph generation)",
            "model_size": null,
            "training_data_description": "Pretrained on paired structure-text datasets (e.g., Sci-PubChemSTM: ~281K structure-text pairs as cited in the perspective).",
            "generation_method": "Text-to-graph generation by optimizing latent variable q of an external flow model (MoFlow) to maximize cosine similarity between text encoder and generated graph representations.",
            "chemical_representation": "Molecular graphs (encoded by GNN) and textual descriptions; generation to graphs via MoFlow then decoded to molecules.",
            "target_application": "Text-guided molecule editing (single/multi-objective edits), binding-affinity-based editing, drug-relevance editing, retrieval and property prediction.",
            "constraints_used": "Text-conditioned similarity objectives and property-specific target constraints in editing tasks; explicit synthetic accessibility or toxicity filters not described in the perspective.",
            "integration_with_external_tools": "Integrates with MoFlow invertible flow model for converting latent vectors to molecular graphs; uses pretrained graph and text encoders.",
            "dataset_used": "Sci-PubChemSTM (structure-text pairs ~281K) and other paired corpora referenced.",
            "evaluation_metrics": "Hit ratio on editing tasks (success rate for proposed edits), downstream property prediction accuracy; compared against baselines including genetic search.",
            "reported_results": "In four editing tasks MoleculeSTM achieved superior hit ratios compared to baselines (qualitative and comparative claims reported; numeric hit ratios not reproduced in the perspective).",
            "experimental_validation": false,
            "challenges_or_limitations": "Requires an external generator (MoFlow) and efficient optimization in latent space; success depends on alignment quality between text and graph encoders and validity of decoded graphs.",
            "uuid": "e6882.7",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SPMM",
            "name_full": "Structure‑Property Pretrained Molecular Model (SPMM)",
            "brief_description": "A model that jointly embeds SMILES and a property vector (53 RDKit properties) into a common space to enable bidirectional property‑to‑SMILES generation and zero/few‑shot property prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SPMM (BERT-style embedding of SMILES + property vector)",
            "model_type": "encoder-decoder style model enabling generation and prediction (multimodal embedding)",
            "model_size": null,
            "training_data_description": "Pretrained on large SMILES corpora (PubChem cited: 50M entries referenced in the perspective) with RDKit-calculated property vectors used as the property modality.",
            "generation_method": "Masking and conditional generation: property vector (PV) treated as a 53-length sentence with random masking; SMILES decoded conditioned on PV to produce molecules matching property specifications.",
            "chemical_representation": "SMILES (text strings) and a 53-dimensional property vector derived from RDKit descriptors.",
            "target_application": "Property‑to‑SMILES generation (inverse design), molecular property prediction, and reaction prediction.",
            "constraints_used": "Conditioning on specific property vectors (can be full, partial, or single properties); PV random masking during training to allow partial conditioning at inference.",
            "integration_with_external_tools": "Uses RDKit to compute the property vector; generation performed within the model (no additional external physics simulations mentioned).",
            "dataset_used": "PubChem-scale SMILES (~50M cited) with RDKit-calculated properties as PVs.",
            "evaluation_metrics": "Validity and novelty of generated molecules, downstream property prediction accuracy (MoleculeNet benchmark comparisons).",
            "reported_results": "SPMM generated valid and novel molecules with desired property vectors across tested scenarios (all 53 properties, single, four, and none); property‑prediction performance comparable to SOTA on MoleculeNet when finetuned (no numeric metrics provided in the perspective).",
            "experimental_validation": false,
            "challenges_or_limitations": "Potential masking/generalization limits when only partial PVs provided; reliance on RDKit descriptors which may not capture all target experimental constraints; no wet‑lab validation reported.",
            "uuid": "e6882.8",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "AtomGPT / Atomgpt",
            "name_full": "AtomGPT (text-based materials/molecule LLM experiments)",
            "brief_description": "Usage examples where general LLMs (GPT-2, Mistral 7B and AtomGPT variants) are adapted for materials property prediction and text-to-material generation using text descriptions produced by ChemNLP and similar pipelines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-2, Mistral-7B, AtomGPT variants (pretrained LLMs, finetuned)",
            "model_type": "decoder-only LLMs (GPT-2, Mistral 7B) fine‑tuned / prompt‑based",
            "model_size": "Mistral-7B: 7B; GPT-2: unspecified in perspective",
            "training_data_description": "Pretrained general LLM corpora; finetuned using text descriptions of materials generated by ChemNLP and other domain corpora (ChemNLP used to produce structured text descriptions).",
            "generation_method": "Finetuning and prompting on textual descriptions to perform property prediction and text‑to‑materials generation.",
            "chemical_representation": "Textual descriptions of materials (from ChemNLP / Robocrystallographer style outputs), SMILES for molecules where applicable.",
            "target_application": "Materials property prediction, text-to-materials generation (materials inverse design), and molecule generation tasks.",
            "constraints_used": "Not detailed in the perspective; conditioning is via textual prompts/descriptions.",
            "integration_with_external_tools": "ChemNLP used to produce structured textual descriptions as model input; downstream evaluation with property predictors/benchmarks implied.",
            "dataset_used": "Text corpora from ChemNLP outputs, public pretraining corpora for LLM backbones; dataset specifics not enumerated in the perspective.",
            "evaluation_metrics": "Property prediction accuracy and generative validity/novelty measures compared to baselines; precise metrics not provided in the perspective summary.",
            "reported_results": "These finetuned LLMs performed competitively for materials property prediction and text-to-material generation tasks in the cited studies, but the perspective does not reproduce numeric performance details.",
            "experimental_validation": false,
            "challenges_or_limitations": "Quality of generated molecules/materials depends on the fidelity of textual encodings (ChemNLP/Robocrystallographer); potential for hallucinations and invalid outputs when decoding from text.",
            "uuid": "e6882.9",
            "source_info": {
                "paper_title": "A Perspective on Foundation Models in Chemistry",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChemDFM: A Large Language Foundation Model for Chemistry",
            "rating": 2,
            "sanitized_title": "chemdfm_a_large_language_foundation_model_for_chemistry"
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "GP‑MoLFormer: Foundation Model For Molecular Generation",
            "rating": 2,
            "sanitized_title": "gpmolformer_foundation_model_for_molecular_generation"
        },
        {
            "paper_title": "Fine, Tuned Language Models Generate Stable Inorganic Materials as Text",
            "rating": 2,
            "sanitized_title": "fine_tuned_language_models_generate_stable_inorganic_materials_as_text"
        },
        {
            "paper_title": "Multi‑modal molecule structure‑text model for text‑based retrieval and editing",
            "rating": 2,
            "sanitized_title": "multimodal_molecule_structuretext_model_for_textbased_retrieval_and_editing"
        },
        {
            "paper_title": "Large‑scale chemical language representations capture molecular structure and properties",
            "rating": 2,
            "sanitized_title": "largescale_chemical_language_representations_capture_molecular_structure_and_properties"
        }
    ],
    "cost": 0.024648749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Perspective on Foundation Models in Chemistry
March 25, 2025</p>
<p>Junyoung Choi 
Gunwook Nam 
Jaesik Choi 
Yousung Jung yousung.jung@snu.ac.kr 
Junyoung Choi 
Nam − Gunwook 
Jaesik Choi </p>
<p>Department of Chemical and Biological Engineering
Institute of Chemical Processes
Institute of Engineering Research
Seoul National University
Gwanak-gu08826SeoulRepublic of Korea</p>
<p>Seoul National University
Gwanak-gu08826SeoulRepublic of Korea</p>
<p>Department of Chemical and Biological Engineering
Institute of Chemical Processes
Seoul National University
Gwanak-gu08826SeoulRepublic of Korea</p>
<p>Department of Chemical and Biological Engineering
Institute of Chemical Processes
Seoul National University
Gwanak-gu08826SeoulRepublic of Korea</p>
<p>Graduate School of Artificial Intelligence
KAIST
34141Daejeon, DaejeonRepublic of Korea</p>
<p>A Perspective on Foundation Models in Chemistry
March 25, 2025990E64A22E9CA4ADD61E52A577F78E9D10.1021/jacsau.4c01160Received: November 30, 2024 Revised: February 7, 2025 Accepted: February 7, 2025foundation modelproperty predictionmachine learning potentialsinverse designlarge-scalepretrainingdownstream tasks
Foundation models are an emerging paradigm in artificial intelligence (AI), with successful examples like ChatGPT transforming daily workflows.Generally, foundation models are large-scale, pretrained models capable of adapting to various downstream tasks by leveraging extensive data and model scaling.Their success has inspired researchers to develop foundation models for a wide range of chemical challenges, from materials discovery to understanding structure−property relationships, areas where conventional machine learning (ML) models often face limitations.In addition, foundation models hold promise for addressing persistent ML challenges in chemistry, such as data scarcity and poor generalization.In this perspective, we review recent progress in the development of foundation models in chemistry across applications of varying scope.We also discuss emerging trends and provide an outlook on promising approaches for advancing foundation models in chemistry.</p>
<p>INTRODUCTION</p>
<p>Artificial intelligence (AI) has achieved remarkable success in fields such as computer vision (CV) and natural language processing (NLP), motivating scientists and researchers to explore its potential in chemical and materials science. 1 For instance, machine learning (ML) models are now used to predict molecular and material properties. 2−18 Beyond these, ML has been applied to predict drug interaction, 19,20 synthesis routes, 21−23 reaction products, 24,25 and analyze X-ray diffraction patterns. 26,27ne of the central challenges for ML in chemistry is the scarcity of large, labeled data. 28Unlike CV and NLP, where vast amounts of annotated data are readily available, chemical and materials science datasets are often limited and require labor-intensive experiments or enormous computing resources.Moreover, applications like drug discovery or materials design require extrapolation to out-of-domain compounds, another limitation of deep learning models.These challenges limit conventional ML approaches at a practical level.In addition, traditional MLIPs often rely on deliberately curated datasets crafted with domain expertise to avoid extrapolation, limiting their transferability across different systems and thus restricting broader adoption.</p>
<p>Recently, the concept of foundation models has emerged as a new AI paradigm.Foundation models are large-scale, pretrained models that can adapt to a broad range of downstream tasks. 29By training on vast datasets, foundation models learn general representations that can be shared across different tasks and domains.These models are then adapted to downstream tasks through transfer learning or finetuning.For instance, foundation models in NLP may handle text translation and summarization, while in CV, they are adapted for tasks like image classification and captioning.In the context of chemistry and materials science, a foundation model can be adapted to predict various properties of molecules and crystals, 30 or generate novel compounds with a desired property (Figure 1). 16Another application is a foundational MLIP, which can be transferred to a wide range of systems and can be further finetuned to a target system when necessary. 31ecently, large language models (LLMs) such as ChatGPT 32 have been leveraged to solve chemical problems or finetuned for specific tasks. 33,34−38 Moreover, multimodal foundation models 39 in chemistry leveraging molecular structures and text would provide a user-friendly platform for text-based inverse design of molecules, which was beyond the capabilities of the conventional ML models. 40,41However, research related to foundation models is evolving rapidly and extensively, making it challenging to track recent trends and identify promising approaches.Therefore, it is an appropriate time to summarize current efforts and provide an outlook on foundational models in chemistry.</p>
<p>The precise definition of the foundation model in chemistry has not been established, and it may differ depending on the way the scope of downstream tasks is considered.Broadly speaking, a pretrained prediction model that can be adapted to various property prediction tasks (e.g., band gap, formation energy, etc.) can be considered a foundation model for a property prediction domain.In a narrower sense, a foundation model should be able to span multiple applications such as prediction of properties and PES, generation of data, etc.While the latter may be considered closer to an ultimate form of the foundation model, studies on the former focusing on pretraining methods that work well in each domain are also crucial for developing pretraining strategies for the latter.In this perspective, therefore, we consider both definitions, referring to the former as a small foundation model and the latter as a big foundation model.First, we review the current progress in the small foundation model for three respective domains: (1) property prediction, (2) MLIP, and (3) inverse design (Table 1).Then, we look into big foundation models adaptable to multiple domains in the (4) multi-domain sections (Table 1).Before this, we provide an overview of common methodologies investigated for foundation models in chemistry.Finally, we outline future directions and opportunities.</p>
<p>METHODOLOGY</p>
<p>2.1.Models 2.1.1.Graph Neural Network.−46 Designing an effective descriptor requires domain expertise on the target task, and often involves a labor-intensive feature selection process to identify the best-working descriptors.In the case of MLIPs, most conventional approaches are descriptor-based, with the descriptors designed to capture information on the local environment constructed by the elements specific to the target system. 47,48As a result, this approach limits the transferability of trained MLIPs to other systems.</p>
<p>In contrast, deep learning models using neural networks are known to learn the representation of the input data that best describes the underlying relationship between input and output, generally outperforming the descriptor-based models. 49,50Among these, graph neural networks (GNN) have gained significant attention for their high expressivity and suitability for molecular data, 51 making them promising candidates for foundational models in chemistry.In a GNN, molecules or crystals are represented as graphs G = (V, E), where atoms and bonds are treated as nodes V and edges E, respectively. 52The graphs are typically featurized with node features v i 0 , such as atom types, and edge features e ij 0 , such as bond types and lengths, where i and j denote the node indices of a graph. 51Node features are then updated via a messagepassing framework, where the message is constructed based on the environment of the central node or edge. 53Specifically, the message m i t 1</p>
<ul>
<li>for node i at the (t + 1) th message-passing step is obtained in general as
m M v v e ( , , ) i t j N i t i t j t ij t 1 ( ) = + (1)
where N(i) denotes the number of nodes in the graph, and M(t) is a message function.Then, node features are updated via an update function U t :
v U v m ( , ) i t t i t i t 1 1 = + +(2)
After T message-passing steps, a readout function R is applied to the updated node features to obtain a graph-level feature g:
g R v i G ( ) i T = { | } (3)
Then g is projected onto multilayer perceptrons (MLP) to output a prediction.It should be noted that the update functions M t and U t , and the readout function R can be approximated by learnable neural networks, enabling the effective learning of complex interactions between atoms.−60   M3GNet, 137 Graphormer 65 In-house data (3M, 17M) SL (E, F, S) Calculation of thermodynamics, lattice dynamics, and mechanical properties eqV2 146 EquiformerV2 147 OMat24 (118M) SL (E, F, S), Denoising Structural relaxation Inverse design MatterGen 16 Diffusion Alex-MP-20 132,148</li>
</ul>
<p>Language Model.</p>
<p>A language model is a key area in NLP that aims to learn and predict the likelihood of word sequences. 61The development of BERT by Google, built on transformer architecture, 62 marked the beginning of the foundation model era by showcasing that a single model could perform multiple tasks. 29,63Furthermore, scaling up language models in both model size and data volume has revealed new capabilities, or "emergent abilities", enabling them to solve complex tasks that were previously out of reach. 64As a result, large language models (LLMs) like ChatGPT 29 have gained widespread popularity in various fields where tasks can be expressed in human languages, such as language translation, research assistance, and more.</p>
<p>An autoregressive language model such as GPT aims to predict the next token y in a sequence given a context X. 61 The context consists of the preceding tokens x 1 , x 2 , ..., x t−1 , where t denotes the current position in the sequence.The model is trained by maximizing the conditional probability of the token sequence, expressed as P(y|X) = P(y|x 1 , x 2 , ..., x t−1 ).Using the chain rule, this objective can be further decomposed into the product of probabilities: 61 P y x x x ( , , ..., )
t T t t 1 1 2 1 | = (4)
where T is the sequence length.One of the most successful language models, a transformer, leveraged self-attention mechanism eliminating the need for recurrent neural networks (RNNs) or convolution. 62Specifically, the attention is computed by
Q K V QK d V Attention( , , ) softmax k T i k j j j j j j y { z z z z z z = (5)
where Q, K, and V are the query, key, and value matrices, respectively.To capture rich and intricate information from the input, multi-head attention is used in the transformer, which applies the attention mechanism across h independent subspaces: 62
Q K V W MultiHead( , , ) Concat(head , ..., head ) h O 1 = (6)
QW KW VW where head Attention( , , )
i i Q i K i V =(7)
where
W i Q d d k model × , W i K d d k model × , W i V d d v model × ,
The transformer is known for its effectiveness in capturing the relative importance and long-range dependencies of each element in a sequence, making it the mainstream model in language models today.−71 A crucial aspect of this application is identifying effective stringbased representations for input.Molecules can be readily represented as a string such as the Simplified Molecular Input Line System (SMILES) 72 and Self-referencing embedded strings (SELFIES), 73 enabling active use of language models in chemistry. 74For example, transformers have been applied almost directly with minimal modification to predict chemical reactions represented by the rearrangement of SMILES from reactants to products. 25Conversely, in materials science, the lack of a standardized text-based representation for crystalline structures limits the broader application of language models.However, tools such as Robocrystallographer 75 and ChemNLP 76  Line-Input Crystal-Encoding System (SLICES), which offers a more compact and invertible string representation for crystals. 77,78These advancements are driving increased research into language models for crystalline materials.As a result, large language models have become another key foundation model in the field of chemistry.</p>
<p>Pretraining Methods</p>
<p>Self-Supervised Learning.</p>
<p>A prominent pretraining approach for the foundation model is self-supervised learning (SSL), which leverages the inherent structure of data as a source of supervision without requiring human-labeled data. 79While supervised learning relies on labeled data which is often costly and time-consuming to generate, SSL enables the use of abundant unlabeled data to learn intrinsic data representations that can be finetuned for downstream tasks with a smaller amount of labeled data.Indeed, most successful foundation models including those based on transformer architectures were pretrained using SSL.</p>
<p>SSL is often categorized into three approaches: contrastive, predictive, and generative learning. 79,80Contrastive learning has been actively explored in CV and NLP domains., 81−84 where representations are paired as either positive or negative, with the model learning to maximize similarity between positive pairs while pushing negative pairs apart.The pairing method is crucial and must be designed carefully to ensure the learned representation is relevant to downstream tasks and avoids negative transfer. 80,81Predictive learning is known as a method that predicts self-generated informative labels from the data, while generative learning refers to reconstructing a graph or text. 79,80While contrastive learning focuses on inter-data information, these two learning schemes aim to extract intradata features. 80In the following sections, we review the approaches to adapting these SSL methods for foundation models in chemistry, and for more general and theoretical discussions and benchmarks on SSL, we refer readers to refs 30, 79, 80.</p>
<p>Multimodal Learning.</p>
<p>Multimodal learning aims to capture complementary information from multiple sources or modalities, similar to how humans perceive voice while seeing the speaker. 85,86−90 For instance, Contrastive Language-Image Pretraining (CLIP) learns a common embedding space for text and images by jointly training respective encoders, 87 such as a transformer for text and a convolutional neural network for images. 91pecifically, contrastive learning is used to maximize the similarity of embeddings from the real text-image pairs while minimizing it for incorrect pairs, enabling zero-shot image classification by comparing the similarity between the query image and label texts.The success of VLP has encouraged researchers to explore multimodal learning in chemistry, becoming a promising approach to the development of big foundation models capable of handling diverse downstream tasks across two modalities (Section 3.5.1).More discussion on multimodal learning for foundation models in materials science can be found in Takeda et al., 92 to which we refer interested readers.</p>
<p>APPLICATIONS</p>
<p>Property Prediction</p>
<p>−95 However, deep learning requires large datasets, which are often limited in chemistry, especially for experimental data.In addition, machine learning models often struggle with extrapolating to unseen data, a frequent requirement in chemistry such as with newly designed materials.While advancements in architectures and techniques have improved predictive performance in chemistry, 60,96−98 challenges of data scarcity and generalization remain unresolved.−101 By contrast, foundation models seek to learn universal representations adaptable across diverse properties, holding the potential to address data limitations and improve generalizability, even in low-data and out-of-distribution (OOD) domains.In this section, we review the progress in small foundation models for property prediction, focusing on various self-supervised pretraining strategies.</p>
<p>3.1.1.Contrastive Learning.One of the early studies on contrastive learning for graph data was Deep Graph Infomax (DGI), 102 inspired by Deep InfoMax 83 from CV. DGI generated pairs by comparing local and global views of the graph.Specifically, it learns node embedding in such a way that the embedding of the node and its parent graph (positive pair) becomes similar while the node and the other graphs (negative pair) are dissimilar.InfoGraph 103 adopted a similar approach, with a more focus on graph-level embeddings.When combined with supervised learning, InfoGraph outperformed purely supervised learning on all 12 targets of the QM9 104 dataset and surpassed semi-supervised baselines on 11 targets.</p>
<p>−181 For instance, GraphCL 105 applied data augmentation to graph data for contrastive learning of graph-level embeddings.Specifically, GraphCL introduced four data augmentation techniques: node dropping, edge perturbation, attribute masking, and subgraph sampling.During training, one augmentation method was randomly applied, treating the augmented and original graphs as a positive pair, while pairing the augmented graph with other graphs as negatives.After transfer learning, GraphCL achieved state-of-the-art (SOTA) performance on 5 of 9 tasks, including MoleculeNet 182 classification task.Given the importance of choosing suitable augmentation techniques for downstream performance, the same group further proposed an automated framework to optimize augmentation selection for pretraining. 179ore recently, Wang et al. introduced MolCLR, 108 a contrastive learning framework for molecular graphs trained on an enlarged dataset of 10 million molecular graphs from PubChem. 110Benchmarking on MoleculeNet classification and regression tasks showed that MolCLR achieved significant improvements compared to the supervised counterparts.Additionally, t-SNE visualization of the learned features showed that molecules with similar topologies and functional groups were clustered together (Figure 2c), demonstrating MolCLR's effectiveness in capturing meaningful molecular representations.</p>
<p>Recognizing that 3-dimensional (3D) molecular geometry provides richer information than 2-dimensional (2D) molecular topology, Liu et al. proposed GraphMVP, a framework that utilizes both 3D geometry and contrastive learning to enhance representation learning. 111In GraphMVP's pretraining stage, each molecule's 2D topology and 3D geometry were paired as positives, with negatives defined as pairs of different molecules.In the downstream task, only 2D molecular graphs were used to predict properties, with the test dataset differing from the pretraining dataset.When combined with conventional 2D SSL, GraphMVP outperformed prior SSL approaches across all 8 classification and 6 regression tasks.Moreover, the authors suggested that 3D geometry served as privileged information, improving separability among molecules and accelerating convergence as explained by VC theory. 183ontrastive learning has also shown promise for crystalline materials.Crystals differ from molecules by combining a broader range of elements with diverse symmetries, which may necessitate different approaches.One of the earliest studies by Magar et al. proposed three augmentation methods based on random perturbations, atom masking, and edge masking. 126sing CGCNN 96 as a baseline model, this approach showed improvements in 7 out of 9 Matbench 94 benchmarks.CrysGNN, proposed by Das et al., took a different approach by defining crystals of the same (different) crystal system as positive (negative) pairs. 35The rationale behind this lies in the fact that some electronic and optical properties such as band gap and dielectric constant depend on the spacegroup and crystal structures.</p>
<p>3.1.2.Predictive and Generative Learning.In chemistry, the labels for predictive learning can be contextual information of a node, 114 a motif within a molecule such as a functional group, 118 chemical properties of atoms such as electronegativity, number of valence electrons, and covalent radius, 35 or the space group. 133For generative learning, a typical approach is to mask nodes or/and edges and reconstruct them 114,133 or to reconstruct the entire molecule or crystal, 184 which is essentially generative modeling.</p>
<p>Predictive and generative learning are often applied in a multi-task fashion and are sometimes integrated with contrastive or supervised learning. 35,114,118,133For example, Hu et al. proposed a general framework to learn representations from both local and global aspects of the molecular graph. 114The local features were learned through predictive learning on the node context and generative learning by reconstructing masked nodes and edges, while the global features were learned via supervised learning of molecular properties.Rong et al. took a similar approach but differed at the graph-level representation learning, where they suggested predicting the graph-level motif such as functional groups, rather than supervised learning. 118or crystalline materials, CrysGNN, mentioned earlier in Section 3.1.1,combines generative and predictive learning with contrastive learning. 35This model reconstructs node features and connectivity at the node level and uses space group prediction to learn graph-level embeddings.In downstream tasks on the Materials Project (MP) 132 and JARVIS-DFT 140 datasets, CrysGNN improved the performance of all four models tested across various properties owing to the synergy of these SSL methods.CrysGNN also demonstrated improvements even on small experimental datasets, implying its practical utility.More recently, Fu et al. proposed using atomic-level microproperties as predictive labels in addition to contrastive learning as shown in Figure 3a. 133Their approach is motivated by the fact that macro-properties such as elastic properties or band gaps depend on the ensemble of microproperties of local atoms, such as atomic stiffness or valence electrons.However, selecting relevant pretraining labels for downstream tasks requires domain knowledge and must be done carefully.</p>
<p>Language models also have been employed in terms of predictive and generative learning.For example, Wang et al. pretrained BERT through a Masked SMILES Recovery task, training the model to recover randomly masked SMILES. 119fter finetuning, this model outperformed the SOTA on tested three datasets, demonstrating the effectiveness of their pretraining approach.Similarly, Zhang et al. used BERT with SMILES augmentation by varying starting atoms and traversal orders, which were then masked for pretraining. 185The finetuned model achieved SOTA performance on most of the 60 molecular prediction tasks, including ADMETlab 186 and MoleculeNet.</p>
<p>Chithrananda et al. proposed ChemBERTa, and first systematically demonstrated the potential of transformers in molecular representation learning by exploring data size, tokenizer methods, and string representations. 187The following work by Ahmad et al. developed ChemBERTa-2, 121 investigating pretraining strategies such as masked language modeling and multi-task regression with labels computed using RDKit, 188 without the need for extra experiments.ChemBERTa-2 pretrained with multi-task regression outperformed the original model and SOTA models on some MoleculeNet tasks.</p>
<p>Ross et al. introduced MoLFormer, a transformer-based model pretrained on the PubChem 110 and ZINC 124 datasets, encompassing 1.1 billion SMILES using masked language modeling (Figure 3b). 123MoLFormer was benchmarked on various downstream tasks, including 2D and 3D molecular datasets like MoleculeNet and QM9, outperforming baseline and other language models and even some GNN models.Analysis of MoLFormer's attention maps revealed that the model effectively captured molecular structural features from SMILES (Figure 3c).Kang et al. proposed MOFTransformer, a multimodal transformer for metal−organic frameworks (MOFs), combining local features derived from CGCNN with global energy grid features. 125The pretraining task was predictive learning to predict MOF properties such as topology and void fraction and to classify metal cluster-organic linkers.MOFTransformer achieved SOTA across diverse properties, including gas adsorption, diffusion, and electronic properties, surpassing baselines including CGCNN.</p>
<p>Rubungo et al. developed LLM-Prop, a language model pretrained on text descriptions of crystal structures, then finetuned to predict crystal properties. 135Using Robocrystallographer 75 to convert crystal structures into text, they pretrained a T5 model 136 with span-masking. 189LLM-Prop outperformed GNN baselines on MP dataset benchmarks while using 35k fewer training points out of 125k.These studies underscore language models' capabilities in molecular and crystal representation learning, providing competitive results comparable to those of geometrical GNNs.</p>
<p>Machine Learning Interatomic Potential</p>
<p>In computational chemistry, the potential energy surface (PES) of a system is of significant importance as it provides valuable information about the system, such as local minima and transition states.In particular, one can sample the PES to collect meaningful points through molecular dynamics (MD) or Monte Carlo simulations.To obtain the PES, one can resort to first-principle methods such as DFT or empirical force fields, such as the Lennard-Jones potential.However, there is a trade-off between accuracy and computational cost, where the former is accurate but slow, while the latter is generally faster but less accurate.Because of this, tasks such as MD simulations, which require numerous energy and force evaluations on large cells, mostly employ empirical force fields at the expense of accuracy.In contrast, AIMD has been performed in limited applications using small cells and short time scales.This trade-off between accuracy and speed poses a challenge to the reliability of MD simulations regarding the accuracy of the potential and the rigor of the simulation setup.</p>
<p>Machine learning interatomic potentials (MLIPs) have emerged as an alternative that offers a compromise between these trade-offs. 8MLIPs are predictive models trained on DFT-level total energy and its derivatives, namely forces and stress.Once properly designed and trained, MLIPs serve as both accurate and efficient surrogate potentials, thereby enhancing the reliability of MD simulations.Conventional MLIPs were descriptor-based models, where the descriptors are designed to contain information about the local environment constructed from the combinations of elements in the target system.An intrinsic challenge of this approach is that the trained MLIP cannot be easily transferred to other systems.Moreover, a typical bottleneck in training MLIPs is the generation of data, which often requires an active learning scheme to ensure that the MLIP has seen all relevant configurations likely to be encountered during simulations.</p>
<p>Recently, MLIPs based on GNN architecture have been increasingly reported.−139,141,143−145 These universal MLIPs have been trained on large databases encompassing diverse elements, making them transferable across various systems. 190,191Chen and Ong reported the first universal MLIP, M3GNet, which can perform relaxation or MD simulations of various crystal structures. 137They exploited relaxation trajectory data from the MP database, consisting of over 60,000 compounds and 89 elements.Deng et al. developed CHGNet, a universal MLIP capable of predicting the magnetic moment, which is essential in materials such as transition metal oxides. 138MACE, 58 a recent equivariant MLIP, has also been employed to develop universal MLIPs for crystals (MACE-MP-0) 31 and organic molecules (MACE-OFF23). 143In particular, MACE-MP-0 extensively investigated its transferability to a wide variety of systems including solids, liquids, surfaces, porous materials, and even battery cells.Although not always quantitatively accurate, it demonstrated qualitative universality across a wide range at unprecedented levels. 31hese universal MLIPs not only serve as effective surrogates for tasks such as relaxation, but they also act as foundation models that facilitate the development of MLIPs tailored to specific systems through finetuning when more accurate potentials are required, such as for MD simulations.For example, Jun et al. finetuned CHGNet to study ionic conduction in nitride Li-ion conductors, observing that the anchoring of dopants and Li vacancies lowered the ionic conductivity. 192Lu et al. finetuned M3GNet to extract Li atomic energies to understand Li-ion migration across interfaces between the cathode and solid electrolytes, where the estimated migration barriers were consistent with the experiments. 193ecently, Google reported a universal MLIP called GNoME potential, as part of their work on crystal structure exploration. 144It was trained on a huge amount of relaxation trajectory data (∼ 10 8 ) obtained during the process.Remarkably, through case studies, it was shown that the zero-shot performance of the GNoME potential was comparable to that of SOTA MLIPs trained from scratch (Figure 4d, e).Furthermore, Microsoft reported MatterSim, a universal MLIP trained on a massive dataset of more than 17 million entries, including public databases and in-house datasets calculated over a wide range of conditions covering 0−5000 K and 0−1000 GPa as shown in the top and middle of Figure 4c. 36MatterSim achieved SOTA performance on Matbench Discovery, 38 demonstrating its capability of predicting the stability of new, unknown compounds.In addition, it successfully calculated lattice dynamics and thermodynamics, such as phonon dispersion and phase diagrams, respectively, far outperforming the previous universal MLIPs trained solely on relaxation trajectory data (bottom of Figure 4c).These works illustrate the feasibility of a universal foundational MLIP, even with zero-shot or, if necessary, fewshots.</p>
<p>Inverse Design</p>
<p>Inverse design is a paradigm where desired properties are specified first, guiding the design of new molecules and materials to meet those criteria.High-throughput screening, a common inverse design strategy, generates a large pool of candidates and filters them step by step to identify those with the target properties.However, since initial candidates are often created by fragmenting known molecules and enumerating or substituting them, the success rate tends to be low, and the process relies heavily on the experimenter's intuition.Generative models, on the other hand, can learn the underlying data structure from large datasets and utilize data distributions to generate novel candidates.Inspired by the success of generative models in producing images 194,195 and text, 32 this approach is increasingly being explored in chemistry.</p>
<p>We briefly introduce the concept of the generative model.The goal of the generative model is to learn the probability distribution of data p(x), from which realistic data can be sampled. 196Training of generative models involves the reconstruction of the training data, which is essentially SSL in nature as aforementioned in Section 3.1.2.For example, a variational autoencoder (VAE) maps by encoder training data x to a feature in latent space enforcing Gaussian distribution, from which latent feature z is sampled and decoded by decoder to the original input x. 197 A more recent approach, the diffusion model, gradually adds Gaussian noise to x until the input data becomes pure Gaussian noise, then removes the noise to reconstruct the x. 198For the inverse design on a target property, a property predictor can be used in VAE to regularize the latent space by the property, 199 or the denoising process in the diffusion model is conditioned on the property. 16Another type of generative model, the language model, reconstructs text by either predicting masked words based on the context of a text 63 or predicting the next word based on a sequence of previous words. 177 small foundational generative model for inverse design can be seen as a pretrained generative model which can be adapted to various inverse design tasks.For instance, MatterGen, a diffusion model developed by Microsoft, was first unconditionally pretrained on a large database such as Alexandria 148 and MP, 132 then finetuned by conditioning on the property such as band gap, elastic modulus, spacegroup, and composition to enable inverse design for each property (Figure 5a, b). 16Notably, MatterGen significantly outperformed the previous SOTA model by more than a factor of 2 in generating stable, unique, and novel (S.U.N.) materials, which were more than 10 times closer to the DFT local energy minimum (Figure 5c).IBM reported GP-MoLFormer, a transformer pretrained on 1.1 billion SMILES.This was finetuned via a parameterefficient novel approach called pair-tuning for property-guided optimization, performing comparably to or better than baselines on inverse design for logP, QED (drug-likeness), and DRD2 activity. 149Gruver et al. proposed finetuning the pretrained LLaMA-2 70B 152 for generation of crystals with desired spacegroup, composition, and E hull using string representations of crystals and prompting, demonstrating the adaptability of the pretrained LLM to atomistic data. 151owever, generative models have generally not been explored through pretraining and finetuning schemes; they are most often trained directly from scratch for a target property.As a result, few small foundational generative models exist in this context.Alternatively, some generative models suggest the possibility of a big foundation model, which can be adapted for both property prediction and inverse design.For example, Goḿez-Bombarelli et al. first proposed using a VAE in molecular inverse design by integrating it with a property regressor. 199The VAE successfully optimized molecules for the target property (5 × QED − SAS), while the performance of the regressor was comparable to that of purely supervised GNN.This implies that representations can be learned such that they can be transferred across both property prediction and inverse design.Following this, we will focus on big foundation models that are adaptable to these two domains further in Section 3.5, and we refer the readers interested in generative models in chemistry to refs 18, 196 for more detail.</p>
<p>Multi-domain: Property Prediction and MLIP</p>
<p>Since MLIPs are predictive models by nature, those that perform well at property prediction are also expected to excel in predicting force fields.Additionally, total energy and forces are related to the global and local features of molecules or crystals, respectively, suggesting that learning force fields itself can serve as an effective pretraining task for property prediction. 200In this section, we review two approaches to utilizing force fields: denoising by learning fictitious force fields, and supervised learning on energy and forces.</p>
<p>3.4.1.Denoising.Denoising was originally developed for generative modeling and aims to learn the derivative of the log probability distribution of data, ∇ x log p(x), rather than directly modeling p(x). 201For configurations of materials, this probability distribution can be expressed by the Boltzmann distribution, p(x) ∝ exp(−E(x)), where E(x) represents the potential energy of configuration x. 153,158 Then, ∇ x log p(x) ∝∇ x E(x), which is equivalent to the force.In practice, denoising is implemented by predicting noise added to material configurations, often assumed to be Gaussian since equilibrium configurations are located at local minima (Figure 6a).Consequently, denoising can be considered akin to learning the virtual forces that guide noisy coordinates back to their original equilibrium positions. 158−204 For instance, Jiao et al. achieved a notable improvement in force prediction on the MD17 dataset, reducing the average error from 0.2086 to 0.0968 kcal/ mol Å</p>
<p>• after pretraining, and outperformed other SSL methods across 11 out of 12 properties in the QM9 dataset. 158Denoising has also been explored for predicting properties of crystalline materials. 184,203,204Notably, Song et al. applied denoising to fractional coordinates and lattice parameters using a denoising diffusion probabilistic model (DDPM) 198 framework. 184This method achieved SOTA performance on the JARVIS-DFT benchmark and showed better results in data-limited scenarios, such as with experimental data.</p>
<p>3.4.2.Supervised Pretraining.Thanks to advances in computational resources and speed, high-quality force field databases are expanding and becoming readily available.6 database for pretraining to adapt for force prediction and molecular property tasks. 205Importantly, they applied zero-force regularization, given that the molecular structures in the training data were optimized.Feng et al. introduced a force- centric pretraining strategy combining denoising and supervised learning for equilibrium and nonequilibrium conformations, respectively. 37This approach not only improved test set performance for force prediction but also enhanced stability in MD simulations and accurately reproducing interatomic distance distributions, demonstrating its practicality for MLIP.Additionally, Jia et al. demonstrated that explicitly learning the derivatives of total energy, i.e. forces and stress, generally results in superior performance across a variety of downstream tasks compared to training solely on energies or using denoising methods. 207ecently, Meta's FAIR team introduced a novel joint multidomain pretraining (JMP) strategy, in which the model is trained simultaneously on diverse energy and force databases (Figure 6b). 200To handle the inconsistencies in DFT calculation setups and differences in database sizes and system scales, they normalized the data and modified commonly used loss functions.This approach achieved SOTA performance in 34 out of 40 downstream tasks, spanning various databases and including properties and forces of molecules, crystals, and MOFs, underscoring the effectiveness of their pretraining method.The universal MLIP discussed in Section 3.2 can also be adapted for property prediction tasks.Indeed, Chen et al. and Yang et al. demonstrated that each universal MLIP's performance as a property predictor was comparable to or exceeded SOTA results. 36,137These studies highlight the efficacy of learning force fields as a pretext task to learn the generic representations adaptable to a wide range of property predictions.</p>
<p>Multi-domain: Property Prediction and Inverse Design</p>
<p>While the property-directed inverse design may suggest novel materials beyond the known chemical space, it remains essential to evaluate the generated data using property prediction tools.In this sense, a big foundation model that is adaptable to both property prediction and inverse design would play a key role in materials discovery.This can be achieved by learning representations that are transferable across both domains, as discussed in Section 3.3.In this section, we focus on approaches to big foundation models, categorized into two areas: multimodal learning and leveraging LLMs.</p>
<p>3.5.1.Multimodal Learning.One of the earliest multimodal models is KV-PLM, which leveraged two modalities: molecular structure and text description (Figure 7a). 162pecifically, molecular structure was represented using SMILES, tokenized with the byte pair encoding (BPE) 208 algorithm to capture frequent substring patterns in a molecule, while text descriptions were obtained from S2orc, 163 an academic paper corpus.The segmented SMILES tokens were inserted into the text, which was used to pretrain BERT as a backbone model via masked language modeling.The model successfully generated drug molecules from the input text and performed comparably to baselines such as D-MPNN, 209 showcasing the potential of language-based foundation models.MolT5, proposed by Edwards et al., took a similar approach, leveraging both text and SMILES to train the T5 136 model, with text-based inverse design and molecule captioning (rather than property prediction) investigated as downstream tasks. 210u et al. employed molecular graphs instead of SMILES as a structural modality, with text description as another modality. 165A pretrained BERT model 162,166 and a GNN model from GraphCL 105 were used as backbone text and graph encoders for their multimodal model, respectively, which was then trained using contrastive learning between the text and graph modalities.For inverse molecular design, they utilized an external model, MoFlow, 211 which transforms q sampled from the Gaussian distribution into molecular graphs.They optimized q to maximize the cosine similarity between the input text and generated molecular graph representations, successfully generating valid molecules based on descriptions at various levels.In property prediction, the graph encoder was finetuned, outperforming all compared SSL methods on average in the MoleculeNet benchmark.</p>
<p>Liu et al. proposed a similar multimodal learning framework, MoleculeSTM, which can be adapted to structure-text retrieval, text-based molecule editing, and property prediction (Figure 7b). 40For the zero-shot molecule editing, another generative model is employed, by optimizing representations to maximize similarities to both representations encoded by the pretrained text encoder and the generative model.In the 4 editing tasks including single-and multi-objective editing, binding-affinitybased editing, and drug-relevance editing, MoleculeSTM achieved superior hit ratios compared to baselines including genetic search.Luo et al. proposed MolFM, where a knowledge graph was incorporated as regularization in addition to molecular graph and text encoders. 167KV-PLM 162 and GraphMVP 111 were employed to initialize their encoders, with four pretraining objectives consisting of structure-text contrastive loss, cross-modal matching loss, masked language modeling loss, and knowledge graph embedding loss.In the downstream inverse design, the MolT5 210 decoder was employed to generate SMILES from the input text.</p>
<p>More recently, Chang and Ye proposed SPMM to learn the joint representation of structure and property by embedding SMILES and a property vector (PV) in a common embedding space, enabling bidirectional PV-to-SMILES generation and property prediction (Figure 7c). 41PV consisted of 53 molecular properties calculated by RDKit and was treated as a 53-length sentence.Notably, PV was randomly masked to allow the use of partial properties in practice.SMILES generation takes up to 53 properties and requires no additional training, successfully generating valid and novel molecules with desired PVs in all tested cases, including scenarios with all 53 properties, a single property, four properties, and no properties provided.In addition, SPMM can predict the PVs of input SMILES without finetuning but can be finetuned for enhanced property prediction, achieving performance comparable to the SOTA on the MoleculeNet benchmark.These studies demonstrate that multimodal learning is a promising approach for developing big foundation models in chemistry, enabling a range of downstream tasks across domains and the use of more specific conditions to design the desired molecules.</p>
<p>Large Language Model.</p>
<p>A large language model such as ChatGPT is renowned for its human-like performance and versatility in understanding context and generating text, making it one of the most popular foundation models.−214 Furthermore, more specialized LLMs have been developed that are adaptable to specific chemical problems, including text-based generation, property prediction, molecule captioning, molecule retrieval, reaction prediction, and more. 33,34,215,216hao et al. proposed a Dialogue Foundation Model for Chemistry (ChemDFM), 172 built upon the pretrained LLaMa-13B. 173They trained LLaMa-13B on 3.8M research articles and 1.4K chemistry textbooks, followed by instruction tuning to better adapt the model to chemical languages, including molecular notations, using 2.7M instructions from chemical databases.In property prediction and inverse design tasks, the model performed better than 10-shot generalist LLMs such as GPT-4 and was comparable to conventional specialist models.Livne et al. introduced nach0, pretrained on both NLP and chemical domain datasets. 174nach0 was finetuned with instruction tuning in a multi-task manner for cross-domain tasks such as text-guided molecule design and property prediction, outperforming SOTA baselines.Notably, these models are capable of interacting with humans as they were pretrained on large corpora, distinguishing them from the aforementioned multimodal language models pretrained purely on SMILES and text descriptions.</p>
<p>Instead of pretraining LLMs manually, researchers can leverage publicly available pretrained LLMs and adapt them for chemistry.Jablonka et al. proposed finetuning GPT-3 via the OpenAI API and benchmarked the model for property prediction and inverse design (Figure 7d). 175The fine-tuned GPT-3 performed comparably to or even outperformed conventional specialized models in classification, particularly in low-data regimes.In a case study on the inverse design of molecules for photoswitches, the finetuned GPT-3 was able to generate valid molecules with higher novelty using a higher softmax temperature, albeit with an increased risk of invalidity.Choudhary employed GPT-2 and Mistral 7B 178 for property prediction and inverse design of crystals, respectively, 176 using text descriptions generated by ChemNLP. 76These studies suggest a convenient approach to leveraging foundation models for materials discovery by simply finetuning public LLMs.</p>
<p>TRENDS AND FUTURE DIRECTIONS</p>
<p>Scope</p>
<p>The scope of foundation models can be categorized into three perspectives.The first is the type of materials the foundation models can handle, such as molecules, crystals, surfaces, MOFs, and so on.A large portion of reported foundation models have targeted molecules, and most others have focused on a single type of material.Meanwhile, a foundation model can leverage shared chemistry across different materials, hopefully allowing for complementary learning, especially for those with relatively limited data.This would enhance flexibility across diverse material types.Supervised learning of this concept was explored by Shoghi et al., 200 but a similar approach could be promising in SSL with careful integration and utilization of different databases across a wide range of materials.</p>
<p>Second is the modality of data.Popular modalities for molecules include SMILES, graphs, text descriptions, and properties.These modalities complement each other and offer opportunities for more diverse tasks.For example, as 3D geometric graphs provide richer structural information than 2D graphs and SMILES, leveraging 3D information can aid downstream tasks where only 2D graphs or SMILES are available. 111In this sense, exploring additional modalities, such as images and spectral data, and investigating effective approaches to maximize their utility is a promising direction. 92or example, Suzuki et al. utilized X-ray diffraction (XRD) patterns as a modality alongside crystal graphs to learn the structure−functionality relationship. 217However, multimodal learning for crystals remains underexplored, warranting further investigation. 218inally, the downstream tasks.The ultimate goal of a foundation model in chemistry is to maximize versatility across downstream tasks, regardless of domain.Progress in this direction is thought to depend on the previous two challenges, as each material type and modality constitutes a task in itself (e.g., predicting a molecule's properties from its SMILES).It is more effective to adapt a foundation model to a learned material type and modality, or at least related ones.Indeed, multimodal learning has proven to be one of the most promising approaches toward a big foundation model across domains, as discussed in Section 3.5.1.Furthermore, it is reasonable to expect that a foundation model trained on vast, diverse datasets could adapt well to new chemistry and tasks, an emergent capability. 29Consequently, we anticipate that discovering effective methods to extend the range of materials and modalities will be key to developing big foundational models capable of encompassing any domain.</p>
<p>Performance</p>
<p>Central to the success of foundation models is scale, both in data and model size. 29Hence, understanding the scalability of data and models in chemistry is crucial.It should be noted that although the high cost of labeling data makes SSL popular for pretraining, it is not straightforward to obtain large meaningful unlabeled data efficiently.Naively generating molecules and crystals risks producing a large amount of invalid and unstable material data.While generative models can successfully produce realistic data, they generally fill gaps within known chemical space rather than expand it, as they are trained on existing databases.Therefore, advanced techniques are needed to explore the configuration space and discover meaningful material structures beyond what is currently known, often requiring extensive domain expertise. 219Nevertheless, unstable material data may still be useful to some extent, for example, in learning symmetries of crystals, which are important for determining their properties but challenging to capture. 35,220he integration of existing databases is one of the most straightforward ways to enlarge the dataset for training largescale foundation models.However, data acquisition method must be compatible between existing databases to ensure seamless integration, which is often not the case, necessitating a proper approach to address inconsistencies. 200For example, Shiota et al. recently proposed a total energy alignment method requiring minimal recalculations to integrate databases calculated using different ab initio packages (e.g., MP 132 using the Vienna Ab initio Simulation Package (VASP) 222 and OFF23 143 using Psi4 223 ). 221The importance of consistent data is further highlighted by Pengmei et al., who found that noisy labels, such as DFT-calculated HOMO−LUMO gaps with high uncertainty depending on the choice of DFT functionals and basis sets, could limit scalability and OOD performance. 224 conclusion, both the quantity and quality of data are crucial for the effective scaling and performance of foundation models.</p>
<p>As with large amounts of data, it is important to design scalable models that can harness this information to learn meaningful knowledge.For example, transformers can scale up to tens or hundreds of billions of parameters while improving performance. 32In this regard, language models for chemistry represent a promising architecture that has already been extensively studied.However, the scalability of GNNs has rarely been explored, with complex equivariant models typically having fewer than a million parameters. 38Simply increasing the dimensionality and message-passing layers is not always beneficial due to the curse of dimensionality and oversmoothing, the latter of which Godwin et al. proposed mitigating by introducing noisy nodes. 225Very recently, Sypetkowski et al. demonstrated that GNNs could scale up to 3 billion parameters with consistent improvements in molecular property prediction and highlighted the importance of model width in driving finetuning performance. 226In another recent study, Pengmei et al. found distinct scaling characteristics during pretraining compared to other fields like language modeling. 224Such studies should be conducted more actively to advance large-scale GNNs, paving the way for developing foundation models in chemistry. 224 common observation in many studies suggests that exploiting both local and global information about materials is effective in learning good representations.This is because properties may depend on local, global, or both types of information, providing such representations with maximum versatility in various downstream tasks involving properties.This can be achieved through contrastive learning between local and global views, predictive learning on local motifs and global properties, or generative learning to reconstruct masked nodes and entire graphs.Contrastive learning with data augmentation has also proven effective with carefully designed augmentation methods at both the node and graph levels.Additionally, supervised learning on force fields allows the model to effectively learn both local and global aspects, each represented by atomic forces and total energy.Overall, pretraining tasks that focus on both aspects represent a promising approach deserving further exploration.</p>
<p>While many reported foundation models demonstrate excellent performance compared to conventional approaches on benchmarks such as MoleculeNet, 182 Matbench, 94 and JARVIS-DFT, 140 more attention should be paid to their performance in the limited data regime, a typical challenge in chemistry and materials science. 35For example, experimental datasets such as OQMD-EXP 131,184 and high-fidelity quantum mechanical calculation data (e.g., coupled cluster) 160,227,228 of relatively smaller amounts can serve as useful benchmarks.Performance tests on varying training data sizes would also provide insights into the scaling characteristics of the models. 153,184,202Additionally, robust extrapolation capability is another key factor in chemistry that requires more focus. 37n this regard, constructing benchmark datasets across various domains can help objectively assess a model's OOD performance.For example, Fu et al. reported a benchmark suite and metrics to evaluate the robustness of MLIPs during MD simulations, which is not necessarily guaranteed by low test set prediction error. 229The Matbench Discovery also serves as a benchmark to assess the relaxation performance of universal MLIPs on unseen structures. 38These benchmark datasets would aid in validating foundation MLIPs in more practical settings.</p>
<p>Efficiency</p>
<p>Since the computational burden increases as the size of models grows, efforts have been made to improve model efficiency, particularly for LLMs. 230For example, quantization reduces the number of bits required to represent model weights, pruning aims to identify and remove redundant weights and components, and matrix decomposition alleviates computational overhead in matrix multiplication.While these techniques are model-agnostic in principle, thorough experiments on GNNs for chemistry are necessary.Meanwhile, Sourek et al. proposed a technique to compress GNNs without loss of information by capturing symmetries, i.e., shared correlations in the data, taking advantage of the structured convolutional nature of GNNs. 231Additionally, the necessity of equivariance in 3D geometric GNNs which incur large computational costs may not always manifest, requiring further investigation. 232Studies on model compression tailored to GNNs would represent a promising direction for developing efficient foundation models for chemistry.</p>
<p>Efficiency is particularly critical for MLIPs, as large-scale simulations involving millions of timesteps are expected in MD simulations driven by MLIPs.However, foundation models can be very expensive even in the inference stage, limiting their utility as MLIPs.Knowledge distillation 233 may offer a solution, where knowledge from an expensive but accurate and generalized model (the teacher model) is distilled into a lighter and more efficient model (the student model). 233kstrom Kelvinius et al. first proposed using knowledge distillation in GNN-based MLIPs by aligning embeddings obtained from the teacher and student models. 234Gong et al. introduced ensemble distillation, achieved by training a single MLIP on snapshots sampled from MD simulations labeled by an ensemble of MLIPs. 235While knowledge distillation has primarily been studied in classification models and rarely addressed in regression tasks, exploring what constitutes "knowledge" could unveil significant opportunities.For example, while the decomposition of total energy into atomic energies in the MLIP formalism is not unique and can be somewhat ad hoc if ill-trained, they can be correctly predicted in a well-generalized model, reflecting physical and chemical atomic interactions, 236−238 potentially serving as knowledge for MLIPs.</p>
<p>Interpretability</p>
<p>As many researchers rely on the use of foundation models, the limitations of foundation models could be easily ignored.As an example, some foundation models are still vulnerable to generating hallucination, generating plausible yet nonfactual content, 239 and the model collapsed 240 outputs, which lack sample diversity.Thus, when we use a foundation model, it is important to verify that the generated outputs have valid and safe meaning in chemistry domain.Especially, when the generated contents are biased to a narrow distribution of training data, the novel discovery of chemical compounds or properties is limited.To solve these problems, it is recommended to utilize the model interpretability techniques which explain sample generation mechanisms in the foundation models. 241For example, interpretable features from a foundation model, Claude 3, were extracted and visualized. 242Moreover, it was found that specific concepts learned in foundation models can be localized and edited at the word level. 243,244Recently, a method to accurately identify source documents used to train foundation models was proposed. 245Such advances in model interpretability make it possible to better understand foundation models, 246 paving the way for their more reliable use in chemistry.</p>
<p>CONCLUSION</p>
<p>A foundation model in chemistry is expected to solve various problems in the field, guiding researchers to achieve the longstanding goal of materials discovery and understanding the behavior of matter.The current status of foundation models in chemistry is still premature, and many challenges must be overcome for these models to be implemented at a practical level.However, there are clear trends in effective pretraining approaches and learning schemes, and importantly, databases are expanding.These offer promising opportunities for foundation models in chemistry in the future, toward new possibilities beyond the domains focused in this perspective.</p>
<p>Figure 1 .
1
Figure 1.Overview of a foundation model in chemistry for property prediction, machine learning interatomic potentials, and inverse design.</p>
<p>,</p>
<p>d k and d v are dimensions of query/key and values, respectively, and d model /h = d k .In addition, since the transformer does not rely on RNNs or convolution, positional encoding (PE) is used to make aware of the position of a token within the sequence:62</p>
<p>Figure 2 .
2
Figure 2. Contrastive learning proposed in MolCLR.Reprinted with permission from ref 108.Copyright 2022 Springer Nature.a. Schematic of contrastive learning and b. proposed augmentation methods for molecular graphs.c.Learned features via contrastive learning visualized by t-SNE.</p>
<p>Figure 3
3
Figure 3. a. Predictive and generative learning on crystal graphs.Reprinted with permission from ref 133.Copyright 2024 American Chemical Society.b.Overview of MoLFormer and c. its attention maps, with linear attention capturing the 3D distance information from SMILES.Reprinted with permission from ref 123.Copyright 2022 Springer Nature.</p>
<p>Figure 4
4
Figure 4. a. Architecture of GNN-based M3GNet.Reprinted with permission from ref 137.Copyright 2022 Springer Nature.b.Statistics of the MP trajectory data used to train CHGNet.Reprinted from ref 138 under the terms of the CC BY license.c.Data generation pipeline and statistics proposed in MatterSim and e. its zero-shot performance in predicting properties via simulations.Reprinted from ref 36 with permission from the authors.Copyright 2024, the authors.d.Finetuned and zero-shot performances of GNoME in comparison to NequIP 57 trained from scratch, and f. comparison of the zero-shot performances of universal MLIPs on force prediction of elemental systems.Reprinted from ref 144 under the terms of the CC BY license.</p>
<p>Figure 5 .
5
Figure 5. a. Pipeline of diffusion-based MatterGen, b. its performance in inverse design, and c. generating stable, unique, and novel (S.U.N.) materials.Reproduced from ref 16 with permission from the authors.Copyright 2023, the authors.</p>
<p>Figure 6 .
6
Figure 6.a. Illustration of denoising as a pretraining for properties and force field prediction.Reprinted from ref 158 with permission from the authors.Copyright 2023, the authors.b.Supervised pretraining on force fields dataset via joint multi-domain pretraining (JMP) proposed by Shoghi et al.Reprinted from ref 200 with permission from the authors.Copyright 2024, the authors.</p>
<p>Figure 7 .
7
Figure 7. Multimodal foundation models.a. Schematic of multimodal KV-PLM, using SMILES and text by fusing them into unified data.Reprinted from ref 162 under the terms of the CC BY license.b.Multimodal model proposed by Liu et al., jointly training the structure and text encoders for each modality.Reprinted with permission from ref 40.Copyright 2023 Springer Nature.c. Workflow of SPMM proposed by Chang and Ye, which aims to learn joint representation of structure and properties.Reprinted from ref 41 under the terms of the CC BY license.d.Finetuning the public ChatGPT proposed by Jablonka et al.Reprinted from ref 175 under the terms of the CC BY license.</p>
<p>Table 1 .
1
Summary of Foundation Models in Chemistry
Pretraining</p>
<p>have been developed to generate text descriptions for crystals.Additionally, Xiao et al. proposed a Simplified
Table 1. continuedPretrainingDomainModelArchitectureDataMethod aDownstream taskMolFM 167KV-PLM, 162 GraphMVP, 111 TransE 168PubChem, 164 S2orc, 163 DrugBank 169 (15K graph-text pairs), knowledge graphs (E49K, R3.2M)Multimodal learning (graph, text, knowl-edge graph)Graph-description retrieval, molecule captioning, text-to-graph generation, molecular property predictionMoleculeSTM 40MegaMolBART, 170 GraphMVP, 111 Sci-PubChemSTM 164 (281K structure-text pairs)Multimodal learning (SMILES/graph,Structure-text retrieval, text-based molecule editing, molecular prop-BERTtext)erty predictionSPMM 41BERT 63PubChem 171 (50M)Multimodal learningProperty-to-SMILES generation, mo-(SMILES, property)lecular property prediction, reactionpredictionChemDFM 172Pretrained LLaMa-13B 173Chemical books (1.4K), papers (3.9M), general textLanguage modelingMolecule recognition, text-to-SMILES generation, molecular property pre-diction, reaction predictionnach0 174T5 136Text from PubMed (13M, 355MLanguage modeling14 tasks including molecular propertytokens), USPTO (119K, 2.9Bprediction, reaction prediction, text-tokens), ZINC (100M, 4.7B to-to-SMILES generation, etc.kens)Jablonka et al. 175Pretrained GPT-3 32−−Molecular/materials property predic-tion, text-to-SMILES generationAtomGPT 176Pretrained GPT-2 177−−Materials property predictionPretrained Mistral 7B 178−−Text-to-materials generation
a CL: contrastive learning; aug.: augmentation; PL: predictive learning; GL: generative learning; SL: supervised learning; E: energy; F: forces; S: stress; M: magnetic moments; A: atom types; X: positions; L: lattice.</p>
<p>https://doi.org/10.1021/jacsau.4c01160 JACS Au 2025, 5, 1499−1518</p>
<p>Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution. K Tran, Z W Ulissi, 10.1038/s41929-018-0142-1Nature Catalysis. 12018</p>
<p>Machine Learning-Assisted Property Prediction of Solid-State Electrolyte. J Li, M Zhou, H.-H Wu, L Wang, J Zhang, N Wu, K Pan, G Liu, Y Zhang, J Han, X Liu, X Chen, J Wan, Q Zhang, 10.1002/aenm.202304480Adv. Energy Mater. 1423044802024</p>
<p>Aspuru-Guzik, A. Machinelearned potentials for next-generation matter simulations. P Friederich, F Häse, J Proppe, 10.1038/s41563-020-0777-6Nat. Mater. 202021</p>
<p>Machine learning force fields. O T Unke, S Chmiela, H E Sauceda, M Gastegger, I Poltavsky, K T Schuẗt, A Tkatchenko, K.-R Muller, 10.1021/acs.chemrev.0c01111?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asChem. Rev. 1212021</p>
<p>Four generations of high-dimensional neural network potentials. J Behler, 10.1021/acs.chemrev.0c00868?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asChem. Rev. 1212021</p>
<p>Bridging the gap between simulated and experimental ionic conductivities in lithium superionic conductors. J Qi, S Banerjee, Y Zuo, C Chen, Z Zhu, M L Holekevi Chandrappa, X Li, S P Ong, 10.1016/j.mtphys.2021.100463Mater. Today Phy. 211004632021</p>
<p>The nature of active sites for carbon dioxide electroreduction over oxide-derived copper catalysts. D Cheng, Z.-J Zhao, G Zhang, P Yang, L Li, H Gao, S Liu, X Chang, S Chen, T Wang, G A Ozin, Z Liu, J Gong, 10.1038/s41467-020-20615-0Nat. Commun. 123952021</p>
<p>Inverse design of solid-state materials via a continuous representation. J Noh, J Kim, H S Stein, B Sanchez-Lengeling, J M Gregoire, A Aspuru-Guzik, Y Jung, 10.1016/j.matt.2019.08.017Matter. 12019</p>
<p>Generative adversarial networks for crystal structure prediction. S Kim, J Noh, G H Gu, A Aspuru-Guzik, Y Jung, 10.1021/acscentsci.0c00426?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asACS central science. 62020. 1412−1420</p>
<p>An invertible crystallographic representation for general inverse design of inorganic crystals with targeted properties. Z Ren, 10.1016/j.matt.2021.11.03220225</p>
<p>Crystal Diffusion Variational Autoencoder for Periodic Material Generation. T Xie, X Fu, O.-E Ganea, R Barzilay, T S Jaakkola, Int. Conf. Learn. Representations. 20221−20</p>
<p>Mattergen: a generative model for inorganic materials design. C Zeni, arXiv:2312.036872023arXiv preprint</p>
<p>Crystal structure generation with autoregressive large language modeling. L M Antunes, K T Butler, R Grau-Crespo, 10.1038/s41467-024-54639-7Nat. Commun. 152024</p>
<p>Has generative artificial intelligence solved inverse materials design? Matter. H Park, Z Li, A Walsh, 10.1016/j.matt.2024.05.01720247</p>
<p>Improved prediction of drug-drug interactions using ensemble deep neural networks. T H Vo, N T K Nguyen, N Q Le, 10.1016/j.medidd.2022.100149Medicine in Drug Discovery. 171001492023</p>
<p>Predicting emerging drug interactions using GNNs. N Q K Le, 10.1038/s43588-023-00555-7Nature Computational Science. 2023</p>
<p>Deep retrosynthetic reaction prediction using local reactivity and global attention. S Chen, Y Jung, 10.1021/jacsau.1c00246?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJACS Au. 12021. 1612−1620</p>
<p>Precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature. T He, H Huo, C J Bartel, Z Wang, K Cruse, G Ceder, 10.1126/sciadv.adg8180Sci. Adv. 9eadg81802023</p>
<p>Predicting synthesis recipes of inorganic crystal materials using elementwise template formulation. S Kim, J Noh, G H Gu, S Chen, Y Jung, 10.1039/D3SC03538GChemical Science. 152024</p>
<p>A generalized-template-based graph neural network for accurate organic reactivity prediction. S Chen, Y Jung, 10.1038/s42256-022-00526-zNature Machine Intelligence. 42022</p>
<p>Molecular transformer: a model for uncertaintycalibrated chemical reaction prediction. P Schwaller, T Laino, T Gaudin, P Bolgar, C A Hunter, C Bekas, A A Lee, 10.1021/acscentsci.9b00576?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asACS central science. 52019</p>
<p>Integrated analysis of X-ray diffraction patterns and pair distribution functions for machine-learned phase identification. N J Szymanski, S Fu, E Persson, G Ceder, 10.1038/s41524-024-01230-9Comput. Mater. 10452024</p>
<p>. E A Riesel, T Mackey, H Nilforoshan, M Xu, C Badding, </p>
<p>Crystal structure determination from powder diffraction patterns with generative machine learning. K Altman, A B Leskovec, J Freedman, D E , 10.1021/jacs.4c10244?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Am. Chem. Soc. 1462024</p>
<p>Machine learning methods for small data challenges in molecular science. B Dou, Z Zhu, E Merkurjev, L Ke, L Chen, J Jiang, Y Zhu, J Liu, B Zhang, G.-W Wei, 10.1021/acs.chemrev.3c00189?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asChem. Rev. 1232023</p>
<p>On the opportunities and risks of foundation models. R Bommasani, arXiv:2108.072582021arXiv preprint</p>
<p>Evaluating self-supervised learning for molecular graph embeddings. H Wang, J Kaddour, S Liu, J Tang, J Lasenby, Q Liu, Adv. Neur. Inf. Process. Syst. 362024</p>
<p>A foundation model for atomistic materials chemistry. I Batatia, arXiv:2401.000962023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, Adv</p>
<p>. Neur. Inf. Process. Syst. 332020. 1877−1901</p>
<p>Materials science in the era of large language models: a perspective. G Lei, R Docherty, S J Cooper, 10.1039/D4DD00074ADigital Discovery. 32024. 1257−1272</p>
<p>A review of large language models and autonomous agents in chemistry. M C Ramos, C Collison, A D White, 10.1039/D4SC03921AChemical Science. 162025. 2514−2572</p>
<p>Crysgnn: Distilling pre-trained knowledge to enhance property prediction for crystalline materials. K Das, B Samanta, P Goyal, S.-C Lee, S Bhattacharjee, N Ganguly, 10.1609/aaai.v37i6.25892Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>H Yang, arXiv:2405.04967A deep learning atomistic model across elements, temperatures and pressures. 2024arXiv preprint</p>
<p>May the force be with you: Unified force-centric pretraining for 3d molecular conformations. R Feng, Q Zhu, H Tran, B Chen, A Toland, R Ramprasad, C Zhang, Adv. Neur. Inf. Process. Syst. 362024. 72750−72760</p>
<p>Matbench Discovery − A framework to evaluate machine learning crystal stability predictions. J Riebesell, R E A Goodall, P Benner, Y Chiang, B Deng, A A Lee, A Jain, K A Persson, arXiv:2308.149202024arXiv preprint</p>
<p>Vision-Language Models for Vision Tasks: A Survey. J Zhang, J Huang, S Jin, S Lu, 10.1109/TPAMI.2024.3369699IEEE Transactions on Pattern Analysis &amp; Machine Intelligence. 462024</p>
<p>Multi-modal molecule structure−text model for text-based retrieval and editing. S Liu, W Nie, C Wang, J Lu, Z Qiao, L Liu, J Tang, C Xiao, A Anandkumar, 10.1038/s42256-023-00759-6Nature Machine Intelligence. 52023</p>
<p>Bidirectional generation of structure and properties through a single molecular foundation model. J Chang, J C Ye, 10.1038/s41467-024-46440-3Nat. Commun. 23232024</p>
<p>Library of descriptors for machine learning in materials science. L Himanen, M O Jäger, E V Morooka, F F Canova, Y S Ranawat, D Z Gao, P Rinke, A S Foster, Dscribe, 10.1016/j.cpc.2019.106949Comput. Phys. Commun. 1069492020</p>
<p>How to represent crystal structures for machine learning: Towards fast prediction of electronic properties. K T Schuẗt, H Glawe, F Brockherde, A Sanna, K.-R Muller, E K Gross, 10.1103/PhysRevB.89.205118Phys. Rev. B. 892014. 205118</p>
<p>On representing chemical environments. A P Bartók, R Kondor, G Csányi, 10.1103/PhysRevB.87.184115Phys. Rev. B. 1841152013</p>
<p>Liquid electrolyte informatics using an exhaustive search with linear regression. K Sodeyama, Y Igarashi, T Nakayama, Y Tateyama, M Okada, 10.1039/C7CP08280KPhys. Chem. Chem. Phys. 202018. 22585− 22591</p>
<p>Holistic computational structure screening of more than 12000 candidates for solid lithium-ion conductor materials. A D Sendek, Q Yang, E D Cubuk, K.-A N Duerloo, Y Cui, E J Reed, 10.1039/C6EE02697DEnergy Environ. Sci. 102017</p>
<p>Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons. A P Bartók, M C Payne, R Kondor, G Csányi, 10.1103/PhysRevLett.104.136403Physical review letters. 1041364032010</p>
<p>Moment tensor potentials: A class of systematically improvable interatomic potentials. A V Shapeev, 10.1137/15M1054183Multiscale Modeling &amp; Simulation. 142016. 1153−1173</p>
<p>Deep learning. nature. Y Lecun, Y Bengio, G Hinton, 10.1038/nature145392015521</p>
<p>Deep Learning. I Goodfellow, Y Bengio, A Courville, 2016MIT Press</p>
<p>Graph neural networks for materials science and chemistry. P Reiser, M Neubert, A Eberhard, L Torresi, C Zhou, C Shao, H Metni, C Van Hoesel, H Schopmans, T Sommer, P Friederich, 10.1038/s43246-022-00315-6Commun. Mater. 202293</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, G Long, C Zhang, P S Yu, 10.1109/TNNLS.2020.2978386IEEE Trans. Neural Netw. Learning Syst. 322021</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, Int. Conf. Mach. Learn. 2017. 1263−1272</p>
<p>E (n) equivariant graph neural networks. V G Satorras, E Hoogeboom, M Welling, Int. Conf. Mach. Learn. 2021</p>
<p>N Thomas, T Smidt, S Kearnes, L Yang, L Li, K Kohlhoff, P Riley, arXiv:1802.08219Tensor field networks: Rotation-and translationequivariant neural networks for 3d point clouds. 2018arXiv preprint</p>
<p>J Han, Y Rong, T Xu, W Huang, arXiv:2202.07230Geometrically equivariant graph neural networks: A survey. 2022arXiv preprint</p>
<p>E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. S Batzner, A Musaelian, L Sun, M Geiger, J P Mailoa, M Kornbluth, N Molinari, T E Smidt, B Kozinsky, 10.1038/s41467-022-29939-5Nat. Commun. 24532022</p>
<p>Higher order equivariant message passing neural networks for fast and accurate force fields. I Batatia, D P Kovacs, G Simm, C Ortner, G Csányi, Mace, Adv. Neur. Inf. Process. Syst. 352022</p>
<p>Equivariant message passing for the prediction of tensorial properties and molecular spectra. K Schuẗt, O Unke, M Gastegger, Int. Conf. Mach. Learn. 2021</p>
<p>Universal directional graph neural networks for molecules. J Gasteiger, F Becker, S Gunnemann, Gemnet, Adv. Neur. Inf. Process. Syst. 342021</p>
<p>A survey on evaluation of large language models. Y Chang, 10.1145/3641289ACM Transactions on Intelligent Systems and Technology. 152024</p>
<p>Polosukhin, I. Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, Adv. Neur. Inf. Process. Syst. 302017</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pretraining of Deep Bidirectional Transformers for Language Understanding. 2019arXiv preprint</p>
<p>A survey of large language models. W X Zhao, arXiv:2303.182232023arXiv preprint</p>
<p>Do transformers really perform badly for graph representation?. C Ying, T Cai, S Luo, S Zheng, G Ke, D He, Y Shen, T.-Y Liu, Adv. Neur. Inf. Process. Syst. 342021. 28877−28888</p>
<p>Equivariant Transformers for Neural Network based Molecular Potentials. P Thölke, G D Fabritiis, Int. Conf. Learn. Representations. 20221−20</p>
<p>Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs. Y.-L Liao, T Smidt, 11th Int. Conf. Learn. Representations. 2023</p>
<p>Applications of Transformers in Computational Chemistry: Recent Progress and Prospects. R Wang, Y Ji, Y Li, S.-T Lee, 10.1021/acs.jpclett.4c03128?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. Lett. 162025</p>
<p>Retrosynthesis prediction with an iterative string editing model. Y Han, X Xu, C.-Y Hsieh, K Ding, H Xu, R Xu, T Hou, Q Zhang, H Chen, 10.1038/s41467-024-50617-1Nat. Commun. 64042024</p>
<p>Crystal Composition Transformer: Self-Learning Neural Language Model for Generative and Tinkering Design of Materials. L Wei, Q Li, Y Song, S Stefanov, R Dong, N Fu, E M Siriwardane, F Chen, J Hu, 10.1002/advs.202304305Adv. Sci. 20242304305</p>
<p>Y Wan, T Xie, N Wu, W Zhang, C Kit, B Hoex, arXiv:2410.16165From Tokens to Materials: Leveraging Language Models for Scientific Discovery. 2024arXiv preprint</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, 10.1021/ci00057a005?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJournal of chemical information and computer sciences. 281988</p>
<p>SELFIES and the future of molecular string representations. M Krenn, 10.1016/j.patter.2022.1005882022, 3, 100588</p>
<p>A review of molecular representation in the age of machine learning. D S Wigh, J M Goodman, A A Lapkin, 10.1002/wcms.1603WIREs Comput. Mol. Sci. 122022. e1603</p>
<p>Robocrystallographer: automated crystal structure text descriptions and analysis. A M Ganose, A Jain, 10.1557/mrc.2019.94MRS Commun. 92019</p>
<p>ChemNLP: a natural languageprocessing-based library for materials chemistry text data. K Choudhary, M L Kelley, 10.1021/acs.jpcc.3c03106?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. C. 1272023. 17545−17555</p>
<p>An invertible, invariant crystal representation for inverse design of solid-state materials using generative deep learning. H Xiao, R Li, X Shi, Y Chen, L Zhu, X Chen, L Wang, 10.1038/s41467-023-42870-7Nat. Commun. 70272023</p>
<p>Y Chen, X Wang, X Deng, Y Liu, X Chen, Y Zhang, L Wang, H Xiao, Mattergpt, arXiv:2408.07608A Generative Transformer for Multi-Property Inverse Design of Solid-State Materials. 2024arXiv preprint</p>
<p>Self-supervised learning: Generative or contrastive. X Liu, F Zhang, Z Hou, L Mian, Z Wang, J Zhang, J Tang, 10.1109/TKDE.2021.3090866IEEE Trans. Knowl. Data Eng. 352021</p>
<p>Self-supervised learning on graphs: Contrastive, generative, or predictive. L Wu, H Lin, C Tan, Z Gao, S Z Li, 10.1109/TKDE.2021.3131584IEEE Trans. Knowl. Data Eng. 352023</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, Int. Conf. Mach. Learn. 2020, 1597−1607</p>
<p>A V D Oord, Y Li, O Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. 2018arXiv preprint</p>
<p>Learning deep representations by mutual information estimation and maximization. R D Hjelm, A Fedorov, S Lavoie-Marchildon, K Grewal, P Bachman, A Trischler, Y Bengio, Int. Conf. Learn. Representations. 2019</p>
<p>Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, Proc. IEEE/ CVF Conf. Comput. Vis. Pattern Recogn. IEEE/ CVF Conf. Comput. Vis. Pattern Recogn2020</p>
<p>Multimodal deep learning. J Ngiam, A Khosla, M Kim, J Nam, H Lee, A Y Ng, Proc. 28th Int. Conf. Mach. Learn. (ICML-11). 28th Int. Conf. Mach. Learn. (ICML-11)2011</p>
<p>Multimodal data fusion: an overview of methods, challenges, and prospects. D Lahat, T Adali, C Jutten, 10.1109/JPROC.2015.2460697Proceedings of the IEEE. the IEEE2015103</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, Int. Conf. Mach. Learn. 2021</p>
<p>Scaling up visual and visionlanguage representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q Le, Y.-H Sung, Z Li, T Duerig, Int. Conf. Mach. Learn. 2021</p>
<p>Object-semantics aligned pre-training for vision-language tasks. Computer Vision − ECCV. X Li, X Yin, C Li, P Zhang, X Hu, L Zhang, L Wang, H Hu, L Dong, F Wei, Y Choi, J Gao, Oscar, 10.1007/978-3-030-58577-8_82020. 2020</p>
<p>Towards artificial general intelligence via a multimodal foundation model. N Fei, Z Lu, Y Gao, G Yang, Y Huo, J Wen, H Lu, R Song, X Gao, T Xiang, H Sun, J.-R Wen, 10.1038/s41467-022-30761-2Nat. Commun. 30942022</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recogn. IEEE Conf. Comput. Vis. Pattern Recogn2016</p>
<p>Foundation model for material science. S Takeda, A Kishimoto, L Hamada, D Nakano, J R Smith, 10.1609/aaai.v37i13.26793Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023. 15376−1538337</p>
<p>Benchmarking graph neural networks for materials chemistry. V Fung, J Zhang, E Juarez, B G Sumpter, 10.1038/s41524-021-00554-0npj Comput. Mater. 7842021</p>
<p>Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm. npj Comput. Mater. A Dunn, Q Wang, A Ganose, D Dopp, A Jain, 10.1038/s41524-020-00433-020206138</p>
<p>Recent advances and applications of deep learning methods in materials science. K Choudhary, B Decost, C Chen, A Jain, F Tavazza, R Cohn, C W Park, A Choudhary, A Agrawal, S J L Billinge, E Holm, S P Ong, C Wolverton, 10.1038/s41524-022-00734-62022npj Comput. Mater859</p>
<p>Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. T Xie, J C Grossman, 10.1103/PhysRevLett.120.145301Physical review letters. 1201453012018</p>
<p>Atomistic line graph neural network for improved materials property predictions. K Choudhary, B Decost, 10.1038/s41524-021-00650-1Comput. Mater. 71852021</p>
<p>Directional Message Passing for Molecular Graphs. J Gasteiger, J Groß, S Gunnemann, Int. Conf. Learn. Representations. 2020</p>
<p>A survey on transfer learning. S J Pan, Q Yang, 10.1109/TKDE.2009.191IEEE Transactions on knowledge and data engineering. 222010</p>
<p>Characterizing and avoiding negative transfer. Z Wang, Z Dai, B Póczos, J Carbonell, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recogn. IEEE/CVF Conf. Comput. Vis. Pattern Recogn2019. 11293−11302</p>
<p>From bulk effective mass to 2D carrier mobility accurate prediction via adversarial transfer learning. X Chen, S Lu, Q Chen, Q Zhou, J Wang, 10.1038/s41467-024-49686-zNat. Commun. 53912024</p>
<p>P Velickovic, W Fedus, W L Hamilton, P Lio, Y Bengio, R D Hjelm, Deep graph infomax. ICLR (Poster) 2019. 24</p>
<p>InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization. F.-Y Sun, J Hoffman, V Verma, J Tang, Int. Conf. Learn. Representations. 2020</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. R Ramakrishnan, P O Dral, M Rupp, O A Von Lilienfeld, 10.1038/sdata.2014.22Sci. Data. 12014</p>
<p>Graph contrastive learning with augmentations. Y You, T Chen, Y Sui, T Chen, Z Wang, Y Shen, Adv. Neur. Inf. Process. Syst. 332020</p>
<p>K Xu, W Hu, J Leskovec, S Jegelka, How Powerful are Graph Neural Networks? Int. Conf. Learn. Representations. 2019</p>
<p>ZINC 15−ligand discovery for everyone. T Sterling, J J Irwin, 10.1021/acs.jcim.5b00559?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Inf. Model. 552015. 2324−2337</p>
<p>Molecular contrastive learning of representations via graph neural networks. Y Wang, J Wang, Z Cao, A Barati Farimani, 10.1038/s42256-022-00447-xNature Machine Intelligence. 42022</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. T N Kipf, M Welling, Int. Conf. Learn. Representations. 2017</p>
<p>PubChem 2019 update: improved access to chemical data. S Kim, J Chen, T Cheng, A Gindulyte, J He, S He, Q Li, B A Shoemaker, P A Thiessen, B Yu, L Zaslavsky, J Zhang, E E Bolton, 10.1093/nar/gky1033Nucleic acids research. 472019</p>
<p>Pretraining Molecular Graph Representation with 3D Geometry. S Liu, H Wang, W Liu, J Lasenby, H Guo, J Tang, Int. Conf. Learn. Representations. 2022</p>
<p>Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. K Schuẗt, P.-J.; Sauceda Kindermans, H E Felix, S Chmiela, A Tkatchenko, K.-R Muller, Adv. Neur. Inf. Process. Syst. 302017</p>
<p>GEOM, energy-annotated molecular conformations for property prediction and molecular generation. S Axelrod, R Gomez-Bombarelli, 10.1038/s41597-022-01288-4Sci. Data. 91852022</p>
<p>Strategies for Pre-training Graph Neural Networks. W Hu, B Liu, J Gomes, M Zitnik, P Liang, V Pande, J Leskovec, Int. Conf. Learn. Representations. 2020</p>
<p>W L Hamilton, R Ying, J Leskovec, arXiv:1709.05584Representation learning on graphs: Methods and applications. 2017arXiv preprint</p>
<p>ChEMBL: a large-scale bioactivity database for drug discovery. A Gaulton, L J Bellis, A P Bento, J Chambers, M Davies, A Hersey, Y Light, S Mcglinchey, D Michalovich, B Al-Lazikani, J P Overington, 10.1093/nar/gkr777Nucleic acids research. 402012</p>
<p>Largescale comparison of machine learning methods for drug target prediction on ChEMBL. A Mayr, G Klambauer, T Unterthiner, M Steijaert, J K Wegner, H Ceulemans, D.-A Clevert, S Hochreiter, 10.1039/C8SC00148KChemical science. 92018</p>
<p>Self-supervised graph transformer on large-scale molecular data. Y Rong, Y Bian, T Xu, W Xie, Y Wei, W Huang, J Huang, Adv. Neur. Inf. Process. Syst. 332020. 12559−12571</p>
<p>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, Proc. 10th ACM Int. Conf. Bioinf. 10th ACM Int. Conf. Bioinf2019</p>
<p>ZINC: a free tool to discover chemistry for biology. J J Irwin, T Sterling, M M Mysinger, E S Bolstad, R G Coleman, 10.1021/ci3001277?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Inf. Model. 522012. 1757−1768</p>
<p>Chemberta-2: Towards chemical foundation models. W Ahmad, E Simon, S Chithrananda, G Grand, B Ramsundar, arXiv:2209.017122022arXiv preprint</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, 10.1038/s42256-022-00580-7Nature Machine Intelligence. 42022</p>
<p>ZINC-a free database of commercially available compounds for virtual screening. J J Irwin, B K Shoichet, 10.1021/ci049714+?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Inf. Model. 452005</p>
<p>A multi-modal pretraining transformer for universal transfer learning in metal−organic frameworks. Y Kang, H Park, B Smit, J Kim, 10.1038/s42256-023-00628-2Nature Machine Intelligence. 2023</p>
<p>Crystal twins: selfsupervised learning for crystalline material property prediction. npj Comput. R Magar, Y Wang, A Barati Farimani, 10.1038/s41524-022-00921-5Mater. 2022, 8, 231</p>
<p>Matminer: An open source toolkit for materials data mining. L Ward, 10.1016/j.commatsci.2018.05.018Comput. Mater. Sci. 1522018</p>
<p>Large-scale screening of hypothetical metal−organic frameworks. C E Wilmer, M Leaf, C Y Lee, O K Farha, B G Hauser, J T Hupp, R Q Snurr, 10.1038/nchem.1192Nature Chem. 2012</p>
<p>CrysXPP: An explainable property predictor for crystalline materials. K Das, B Samanta, P Goyal, S.-C Lee, S Bhattacharjee, N Ganguly, 10.1038/s41524-022-00716-8npj Comput. Mater. 2022. 843</p>
<p>Graph convolutional neural networks with global attention for improved materials property prediction. S.-Y Louis, Y Zhao, A Nasiri, X Wang, Y Song, F Liu, J Hu, 10.1039/D0CP01474EPhys. Chem. Chem. Phys. 222020</p>
<p>S Kirklin, J E Saal, B Meredig, A Thompson, J W Doak, M Aykol, S Ruḧl, C Wolverton, 10.1038/npjcompumats.2015.10The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies. npj Comput. Mater. 20151</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, K A Persson, 10.1063/1.4812323APL Mater. 2013, 1, 011002</p>
<p>Physics-Guided Dual Self-Supervised Learning for Structure-Based Material Property Prediction. N Fu, L Wei, J Hu, 10.1021/acs.jpclett.4c00100?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. Lett. 152024. 2841−2850</p>
<p>Scalable deeper graph neural networks for highperformance materials property prediction. S S Omee, S.-Y Louis, N Fu, L Wei, S Dey, R Dong, Q Li, J Hu, 10.1016/j.patter.2022.10049120223100491</p>
<p>Llmprop: Predicting physical and electronic properties of crystalline solids from their text descriptions. A N Rubungo, C Arnold, B P Rand, A B Dieng, arXiv:2310.140292023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 212020</p>
<p>A universal graph deep learning interatomic potential for the periodic table. C Chen, S P Ong, 10.1038/s43588-022-00349-3Nature Computational Science. 2022</p>
<p>CHGNet as a pretrained universal neural network potential for charge-informed atomistic modelling. B Deng, P Zhong, K Jun, J Riebesell, K Han, C J Bartel, G Ceder, 10.1038/s42256-023-00716-3Nature Machine Intelligence. 52023</p>
<p>Unified graph neural network forcefield for the periodic table: solid state applications. K Choudhary, B Decost, L Major, K Butler, J Thiyagalingam, F Tavazza, 10.1039/D2DD00096BDigital Discovery. 22023</p>
<p>The joint automated repository for various integrated simulations (JARVIS) for data-driven materials design. npj Comput. K Choudhary, 10.1038/s41524-020-00440-12020Mater6173</p>
<p>Towards universal neural network potential for material discovery applicable to arbitrary combination of 45 elements. S Takamoto, 10.1038/s41467-022-30687-9Nat. Commun. 1329912022</p>
<p>TeaNet: Universal neural network interatomic potential inspired by iterative electronic relaxations. S Takamoto, S Izumi, J Li, 10.1016/j.commatsci.2022.111280Comput. Mater. Sci. 1112802022</p>
<p>D P Kovács, J H Moore, N J Browning, I Batatia, J T Horton, V Kapil, I.-B Magdaȗ, D J Cole, G Csányi, Mace-Off23, arXiv:2312.15211Transferable machine learning force fields for organic molecules. 2023arXiv preprint</p>
<p>Scaling deep learning for materials discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, 10.1038/s41586-023-06735-9Nature. 6242023</p>
<p>Scalable Parallel Algorithm for Graph Neural Network Interatomic Potentials in Molecular Dynamics Simulations. Y Park, J Kim, S Hwang, S Han, 10.1021/acs.jctc.4c00190?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Theory Comput. 202024</p>
<p>L Barroso-Luque, M Shuaibi, X Fu, B M Wood, M Dzamba, M Gao, A Rizvi, C L Zitnick, Ulissi, arXiv:2410.12771Inorganic Materials Dataset and Models. OMat24. 20242024arXiv preprint</p>
<p>Y.-L Liao, B Wood, A Das, T Smidt, Equiformerv2, arXiv:2306.12059Improved equivariant transformer for scaling to higher-degree representations. 2023arXiv preprint</p>
<p>A dataset of 175k stable and metastable materials calculated with the PBEsol and SCAN functionals. J Schmidt, H.-C Wang, T F Cerqueira, S Botti, M A Marques, 10.1038/s41597-022-01177-wSci. Data. 9642022</p>
<p>J Ross, B Belgodere, S C Hoffman, V Chenthamarakshan, Y Mroueh, P Das, Gp-Molformer, arXiv:2405.04912Foundation Model For Molecular Generation. 2024arXiv preprint</p>
<p>PubChem substance and compound databases. S Kim, P A Thiessen, E E Bolton, J Chen, G Fu, A Gindulyte, L Han, J He, S He, B A Shoemaker, J Wang, B Yu, J Zhang, S H Bryant, 10.1093/nar/gkv951Nucleic acids research. 442016</p>
<p>N Gruver, A Sriram, A Madotto, A G Wilson, C L Zitnick, Z W Ulissi, Fine, Tuned Language Models Generate Stable Inorganic Materials as Text. 12th Int. Conf. Learn. Representations. 2024. 1−20</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, arXiv:2307.092882023arXiv preprint</p>
<p>Pretraining via Denoising for Molecular Property Prediction. S Zaidi, M Schaarschmidt, J Martens, H Kim, Y W Teh, A Sanchez-Gonzalez, P Battaglia, R Pascanu, J Godwin, 11th Int. Conf. Learn. Representations. 2023</p>
<p>Learning to simulate complex physics with graph networks. A Sanchez-Gonzalez, J Godwin, T Pfaff, R Ying, J Leskovec, P Battaglia, Int. Conf. Mach. Learn. 2020</p>
<p>PubChemQC project: a large-scale first-principles electronic structure database for data-driven chemistry. M Nakata, T Shimazaki, 10.1021/acs.jcim.7b00083?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Inf. Model. 572017. 1300−1308</p>
<p>S Liu, H Guo, J Tang, Molecular Geometry Pretraining with SE(3)-Invariant Denoising Distance Matching. 11th Int. Conf. Learn. Representations. 2023</p>
<p>Z Xu, Y Luo, X Zhang, X Xu, Y Xie, M Liu, K Dickerson, C Deng, M Nakata, S Ji, Molecule3d, arXiv:2110.01717A benchmark for predicting 3d geometries from molecular graphs. 2021arXiv preprint</p>
<p>Energymotivated equivariant pretraining for 3d molecular graphs. R Jiao, J Han, W Huang, Y Rong, Y Liu, 10.1609/aaai.v37i7.25978Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Machine learning of accurate energyconserving molecular force fields. S Chmiela, A Tkatchenko, H E Sauceda, I Poltavsky, K T Schuẗt, K.-R Muller, 10.1126/sciadv.16030152017. e16030153</p>
<p>The ANI-1ccx and ANI-1x data sets, coupled-cluster and density functional theory properties for molecules. J S Smith, R Zubatyuk, B Nebgen, N Lubbers, K Barros, A E Roitberg, O Isayev, S Tretiak, 10.1038/s41597-020-0473-z20207134</p>
<p>Pre-training with fractional denoising to enhance molecular property prediction. Y Ni, S Feng, X Hong, Y Sun, W.-Y Ma, Z.-M Ma, Q Ye, Y Lan, 10.1038/s42256-024-00900-zNat. Mach. Intell. 62024</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Z Zeng, Y Yao, Z Liu, M Sun, 10.1038/s41467-022-28494-3Nat. Commun. 138622022</p>
<p>K Lo, L L Wang, M Neumann, R Kinney, D S Weld, S2orc, arXiv:1911.02782The semantic scholar open research corpus. 2019arXiv preprint</p>
<p>PubChem: a public information system for analyzing bioactivities of small molecules. Y Wang, J Xiao, T O Suzek, J Zhang, J Wang, S H Bryant, 10.1093/nar/gkp456Nucleic acids research. 372009</p>
<p>B Su, D Du, Z Yang, Y Zhou, J Li, A Rao, H Sun, Z Lu, J.-R Wen, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022arXiv preprint</p>
<p>SciBERT: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>Y Luo, K Yang, M Hong, X Y Liu, Z Nie, Molfm, arXiv:2307.09484A multimodal molecular foundation model. 2023arXiv preprint</p>
<p>Translating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, Adv. Neur. Inf. Process. Syst. 262013</p>
<p>DrugBank 5.0: a major update to the DrugBank database for. D S Wishart, 10.1093/nar/gkx1037Nucleic acids research. 462018. 2018</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, 10.1088/2632-2153/ac3ffbMachine Learning: Science and Technology. 3150222022</p>
<p>S Kim, J Chen, T Cheng, A Gindulyte, J He, S He, Q Li, B A Shoemaker, P A Thiessen, B Yu, L Zaslavsky, J Zhang, E E Bolton, 10.1093/nar/gkaa971PubChem in 2021: new data content and improved web interfaces. 202149</p>
<p>ChemDFM: A Large Language Foundation Model for Chemistry. Z Zhao, D Ma, L Chen, L Sun, Z Li, Y Xia, H Xu, Z Zhu, S Zhu, S Fan, G Shen, K Yu, X Chen, Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges. 2024</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Roziere, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, 10.1021/jacsau.4c01160?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asarXiv:2302.13971Lample, G. Llama: Open and efficient foundation language models. 20235arXiv preprintJACS Au pubs.acs.org/jacsau Perspective</p>
<p>Zhavoronkov, A. nach0: Multimodal natural and chemical languages foundation model. M Livne, Z Miftahutdinov, E Tutubalina, M Kuznetsov, D Polykovskiy, A Brundyn, A Jhunjhunwala, A Costa, A Aliper, A Aspuru-Guzik, 10.1039/D4SC00966EChemical Science. 152024</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.1038/s42256-023-00788-1Nature Machine Intelligence. 62024</p>
<p>Atomistic Generative Pretrained Transformer for Forward and Inverse Materials Design. K Choudhary, Atomgpt, 10.1021/acs.jpclett.4c01126?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Phys. Chem. Lett. 152024</p>
<p>Sutskever, I. Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, OpenAI Blog. 192019</p>
<p>. A Q Jiang, arXiv:2310.068252023Mistral 7B. arXiv preprint</p>
<p>. Y You, T Chen, Y Shen, Wang, Z. Graph contrastive learning automated. Int. Conf. Mach. Learn. 2021</p>
<p>S Thakoor, C Tallec, M G Azar, M Azabou, E L Dyer, R Munos, P Velicǩovic, M Valko, Large, Scale Representation Learning on Graphs via Bootstrapping; Int. Conf. Learn. Representations. 2022</p>
<p>Graph self-supervised learning with accurate discrepancy learning. D Kim, J Baek, S J Hwang, Adv. Neur. Inf. Process. Syst. 352022</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, 10.1039/C7SC02664AChemical science. 92018</p>
<p>Learning using privileged information: similarity control and knowledge transfer. V Vapnik, R Izmailov, J. Mach. Learn. Res. 162015. 2023−2049</p>
<p>A Diffusion-Based Pre-training Framework for Crystal Property Prediction. Z Song, Z Meng, I King, 10.1609/aaai.v38i8.28748Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Pushing the boundaries of molecular property prediction for drug discovery with multitask learning BERT enhanced by SMILES enumeration. X.-C Zhang, C.-K Wu, J.-C Yi, X.-X Zeng, C.-Q Yang, A.-P Lu, T.-J Hou, D.-S Cao, 10.34133/research.00042022. 20224</p>
<p>ADMETlab 2.0: an integrated online platform for accurate and comprehensive predictions of ADMET properties. G Xiong, Z Wu, J Yi, L Fu, Z Yang, C Hsieh, M Yin, X Zeng, C Wu, A Lu, X Chen, T Hou, D Cao, 10.1093/nar/gkab255Nucleic acids research. 492021</p>
<p>ChemBERTa: large-scale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>RDKit: Open-source cheminformatics. G Landrum, 2006</p>
<p>Improving pre-training by representing and predicting spans. M Joshi, D Chen, Y Liu, D S Weld, L Zettlemoyer, O Levy, Spanbert, 10.1162/tacl_a_00300Transactions of the association for computational. 2020</p>
<p>Performance assessment of universal machine learning interatomic potentials: Challenges and directions for materials' surfaces. B M Focassio, L P Freitas, G R Schleder, 10.1021/acsami.4c03815?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asACS Appl. Mater. Interfaces. 2024</p>
<p>Benchmarking of Universal Machine Learning Interatomic Potentials for Structural Relaxation. C Gonzales, E Fuemmeler, E B Tadmor, S Martiniani, S Miret, AI for Accelerated Materials Design -NeurIPS. 2024. 2024</p>
<p>Nitride Lithium-ion Conductors with Enhanced Oxidative Stability. K Jun, Y Xiao, W Sun, Y.-W Byeon, H Kim, G Ceder, 10.1149/1945-7111/ad76dbJ. Electrochem. Soc. 171905182024</p>
<p>Superior Low-Temperature All-Solid-State Battery Enabled by High-Ionic-Conductivity and Low-Energy-Barrier Interface. P Lu, S Gong, C Wang, Z Yu, Y Huang, T Ma, J Lian, Z Jiang, L Chen, H Li, F Wu, 10.1021/acsnano.3c07023?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asACS Nano. 182024</p>
<p>Zero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, Int. Conf. Mach. Learn. 2021</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF Conf. Comput. Vis. Pattern Recogn. the IEEE/CVF Conf. Comput. Vis. Pattern Recogn2022. 10684− 10695</p>
<p>Generative models as an emerging paradigm in the chemical sciences. D M Anstine, O Isayev, 10.1021/jacs.2c13467?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Am. Chem. Soc. 1452023</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. 2013arXiv preprint</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Adv. Neur. Inf. Process. Syst. 332020</p>
<p>Automatic chemical design using a data-driven continuous representation of molecules. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, 10.1021/acscentsci.7b00572?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asACS central science. 42018</p>
<p>From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction. N Shoghi, A Kolluru, J R Kitchin, Z W Ulissi, C L Zitnick, B M Wood, The Twelfth Int. Conf. Learn. Representations. 2024</p>
<p>A connection between score matching and denoising autoencoders. P Vincent, 10.1162/NECO_a_00142Neural computation. 232011. 1661−1674</p>
<p>Denoise pretraining on nonequilibrium molecules for accurate and transferable neural potentials. Y Wang, C Xu, Z Li, A Barati Farimani, 10.1021/acs.jctc.3c00289?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Theory Comput. 192023</p>
<p>Boost Your Crystal Model with Denoising Pre-training. S Shen, K Liu, M Zhu, H Chen, ICML 2024 AI for Science Workshop 2024. </p>
<p>Self-supervised learning for crystal property prediction via denoising. A New, N Q Le, M Pekala, C D Stiles, ICML 2024 AI for Science Workshop 2024, 1−9</p>
<p>Supervised Pretraining for Molecular Force Fields and Properties Prediction. NeurIPS 2022 AI for Science: Progress and Promises. X Gao, W Gao, W Xiao, Z Wang, C Wang, L Xiang, 2022</p>
<p>PubChemQC PM6: Data sets of 221 million molecules with optimized molecular geometries and electronic properties. M Nakata, T Shimazaki, M Hashimoto, T Maeda, 10.1021/acs.jcim.0c00740?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Inf. Model. 602020</p>
<p>Derivative-based pre-training of graph neural networks for materials property predictions. S Jia, A R Parthasarathy, R Feng, G Cong, C Zhang, V Fung, 10.1039/D3DD00214DDigital Discovery. 32024</p>
<p>R Sennrich, B Haddow, A Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>Analyzing learned molecular representations for property prediction. K Yang, K Swanson, W Jin, C Coley, P Eiden, H Gao, A Guzman-Perez, T Hopper, B Kelley, M Mathea, A Palmer, V Settels, T Jaakkola, K Jensen, R Barzilay, 10.1021/acs.jcim.9b00237?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Inf. Model. 592019</p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, H Ji, arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Moflow: an invertible flow model for generating molecular graphs. C Zang, F Wang, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining 2020. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining 2020</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, arXiv:2211.09085Stojnic, R. Galactica: A large language model for science. 2022arXiv preprint</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, 10.1093/bioinformatics/btz682Bioinformatics. 362020. 1234−1240</p>
<p>MatSciBERT: A materials domain language model for text mining and information extraction. T Gupta, M Zaki, N M A Krishnan, Mausam, Mausam, 10.1038/s41524-022-00784-wComput. Mater. 81022022</p>
<p>Scientific language modeling: A quantitative review of large language models in molecular science. P Liu, J Tao, Z Ren, arXiv:2402.041192024arXiv preprint</p>
<p>Assessment of fine-tuned large language models for real-world chemistry and material science applications. J Van Herck, 10.1039/D4SC04401KChemical Science. 162025</p>
<p>Selfsupervised learning of materials concepts from crystal structures via deep neural networks. Y Suzuki, T Taniai, K Saito, Y Ushiku, K Ono, 10.1088/2632-2153/aca23dMachine Learning: Science and Technology. 3450342022</p>
<p>CrysMMNet: Multimodal Representation for Crystal Property Prediction. K Das, P Goyal, S.-C Lee, S Bhattacharjee, N Ganguly, Proc. 39th Conf. Uncertainty in Artificial Intelligence. 39th Conf. Uncertainty in Artificial Intelligence2023216</p>
<p>Modern methods of crystal structure prediction. A R Oganov, </p>
<p>Examining graph neural networks for crystal structures: limitations and opportunities for capturing periodicity. S Gong, K Yan, T Xie, Y Shao-Horn, R Gomez-Bombarelli, S Ji, J C Grossman, 10.1126/sciadv.adi3245Sci. Adv. 9eadi32452023</p>
<p>Taming Multi-Domain. T Shiota, K Ishihara, T M Do, T Mori, W Mizukami, arXiv:2412.13088Fidelity Data: Towards Foundation Models for Atomistic Scale Simulations. 2024arXiv preprint</p>
<p>Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set. G Kresse, J Furthmuller, 10.1103/PhysRevB.54.11169Phys. Rev. B. 111691996</p>
<p>PSI4 1.4: Open-source software for high-throughput quantum chemistry. D G A Smith, 10.1063/5.0006002J. Chem. Phys. 1841082020</p>
<p>Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training. Z Pengmei, Z Shen, Z Wang, M Collins, H Rangwala, arXiv:2410.216832024Scaling and Zero-Shot Transfer. arXiv preprint</p>
<p>J Godwin, M Schaarschmidt, A Gaunt, A Sanchez-Gonzalez, Y Rubanova, P Velicǩovic, J Kirkpatrick, P Battaglia, arXiv:2106.07971Simple gnn regularisation for 3d molecular property prediction &amp; beyond. 2021arXiv preprint</p>
<p>M Sypetkowski, F Wenkel, F Poursafaei, N Dickson, K Suri, P Fradkin, D Beaini, arXiv:2404.11568On the Scalability of GNNs for Molecular Graphs. 2024arXiv preprint</p>
<p>Learning properties of ordered and disordered materials from multi-fidelity data. C Chen, Y Zuo, W Ye, X Li, S P Ong, 10.1038/s43588-020-00002-xNature Computational Science. 12021</p>
<p>Machine learning of coupled cluster (T)-energy corrections via delta (Δ)-learning. M Ruth, D Gerbig, P R Schreiner, 10.1021/acs.jctc.2c00501?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-asJ. Chem. Theory Comput. 182022</p>
<p>Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations. X Fu, Z Wu, W Wang, T Xie, S Keten, R Gomez-Bombarelli, T Jaakkola, arXiv:2210.072372022arXiv preprint</p>
<p>Compressing large-scale transformer-based models: A case study on bert. P Ganesh, Y Chen, X Lou, M A Khan, Y Yang, H Sajjad, P Nakov, D Chen, M Winslett, 10.1162/tacl_a_00413Transactions of the Association for Computational Linguistics. 92021</p>
<p>Lossless compression of structured convolutional models via lifting. G Sourek, F Zelezny, O Kuzelka, arXiv:2007.065672020arXiv preprint</p>
<p>Recent advances and outstanding challenges for machine learning interatomic potentials. T W Ko, S P Ong, 10.1038/s43588-023-00561-9Nature Computational Science. 32023</p>
<p>G Hinton, O Vinyals, J Dean, arXiv:1503.02531Distilling the Knowledge in a Neural Network. 2015arXiv preprint</p>
<p>Accelerating molecular graph neural networks via knowledge distillation. F Ekström Kelvinius, D Georgiev, A Toshev, J Gasteiger, Adv. Neur. Inf. Process. Syst. 362024. 25761−25792</p>
<p>BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development. S Gong, Y Zhang, Z Mu, Z Pu, H Wang, Z Yu, M Chen, T Zheng, Z Wang, L Chen, X Wu, S Shi, W Gao, W Yan, L Xiang, arXiv:2404.071812024arXiv preprint</p>
<p>Atomic energy mapping of neural network potential. D Yoo, K Lee, W Jeong, D Lee, S Watanabe, S Han, 10.1103/PhysRevMaterials.3.093802Phys. Rev. Mater. 3938022019</p>
<p>Data-driven learning of total and local energies in elemental boron. V L Deringer, C J Pickard, G Csányi, 10.1103/PhysRevLett.120.156001Physical review letters. 1201560012018</p>
<p>Frustration in Super-Ionic Conductors Unraveled by the Density of Atomistic States. S Wang, Y Liu, Y Mo, 10.1002/anie.202215544Angew. Chem., Int. Ed. 622023. e202215544</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, 10.1038/s42256-024-00832-8Nat. Mach. Intell. 62024</p>
<p>AI models collapse when trained on recursively generated data. I Shumailov, Z Shumaylov, Y Zhao, N Papernot, R Anderson, Y Gal, 10.1038/s41586-024-07566-yNature. 6312024</p>
<p>Predicting potentially hazardous chemical reactions using an explainable neural network. J Kim, G H Gu, J Noh, S Kim, S Gim, J Choi, Y Jung, 10.1039/D1SC01049BChemical Science. 122021. 11028−11037</p>
<p>A Templeton, Scaling monosemanticity: Extracting interpretable features from Claude 3 Sonnet. 2024. 2025−01−19</p>
<p>Locating and editing factual associations in GPT. K Meng, D Bau, A Andonian, Y Belinkov, Adv. Neur. Inf. Process. Syst. 352022. 17359−17372</p>
<p>Mass-Editing Memory in a Transformer. K Meng, A S Sharma, A J Andonian, Y Belinkov, D Bau, The Eleventh Int. Conf. Learn. Representations. 2023</p>
<p>Memorizing Documents with Guidance in Large Language Models. B Park, J Choi, IJCAI-24 2024Proc. Thirty-Third International Joint Conference on Artificial Intelligence. Thirty-Third International Joint Conference on Artificial Intelligence</p>
<p>How does ChatGPT 'think'? Psychology and neuroscience crack open AI large language models. M Hutson, 10.1038/d41586-024-01314-yNature. 6292024</p>            </div>
        </div>

    </div>
</body>
</html>
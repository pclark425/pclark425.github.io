<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6419 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6419</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6419</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267499640</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03824v4.pdf" target="_blank">A Call for Embodied AI</a></p>
                <p><strong>Paper Abstract:</strong> We propose Embodied AI (E-AI) as the next fundamental step in the pursuit of Artiﬁcial General Intelligence (AGI), juxtaposing it against current AI advancements, particularly Large Language Models (LLMs). We traverse the evolution of the embodiment concept across diverse ﬁelds (philosophy, psychology, neuroscience, and robotics) to highlight how E-AI distinguishes itself from the classical paradigm of static learning. By broadening the scope of E-AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston’s active inference principle, offering a comprehensive approach to E-AI development. Despite the progress made in the ﬁeld of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future E-AI research. Highlighting the importance of creating E-AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6419.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6419.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that augments a language model with an external database or retrieval component to reduce hallucinations by retrieving relevant documents at generation time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrievalaugmented generation for large language models: A survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-augmented LLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM architecture augmented with an external retrieval database used as long-term memory: at generation time the model retrieves relevant passages and conditions its outputs on them to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external database</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>external database of text passages / documents (as described in the paper: an external DB)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval from external database (document retrieval / retrieval augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes RAG reduces hallucinations; no quantitative trade-offs (latency, memory footprint, or retrieval cost) are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No empirical failure cases or task-specific limitations reported in this paper; the paper emphasizes RAG as a long-term memory approach but does not provide experimental comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Gao et al., 2024. Retrievalaugmented generation for large language models: A survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Call for Embodied AI', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6419.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6419.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT (autonomous GPT agent project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An emergent class of autonomous language-model-based agents that generate and execute plans by decomposing goals into steps; mentioned as an example of agent-based LLMs in the context of embodied cognitive architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What if gpt4 became autonomous: The auto-gpt project and use cases</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as an example of an agentic LLM that generates autonomous agents/behaviors; the paper does not describe any specific memory mechanism for AutoGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>The paper only cites AutoGPT as an example of agent-based LLMs and does not report empirical trade-offs or memory-related costs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No memory-specific limitations for AutoGPT are described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Firat & Kuleli, 2023. What if gpt4 became autonomous: The auto-gpt project and use cases. Journal of Emerging Computer Technologies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Call for Embodied AI', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6419.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6419.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PanguAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pan-guAgent (Pangu-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-focused language model described as a fine-tunable generalist agent with structured reasoning; mentioned as an example of agent-oriented LLMs within the paper's discussion of cognitive architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pangu-agent: A fine-tunable generalist agent with structured reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PanguAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as an agent-focused language model / generalist agent; the paper does not provide details on whether or how PanguAgent uses an explicit memory mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>No memory-specific trade-offs or empirical evaluations involving memory are reported for PanguAgent in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper does not report failure cases or limits of any memory component for PanguAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Christianos et al., 2023. Pangu-agent: A fine-tunable generalist agent with structured reasoning. arXiv:2312.14878.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Call for Embodied AI', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrievalaugmented generation for large language models: A survey <em>(Rating: 2)</em></li>
                <li>What if gpt4 became autonomous: The auto-gpt project and use cases <em>(Rating: 2)</em></li>
                <li>Pangu-agent: A fine-tunable generalist agent with structured reasoning <em>(Rating: 2)</em></li>
                <li>The rise and potential of large language model based agents: A survey <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6419",
    "paper_id": "paper-267499640",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A technique that augments a language model with an external database or retrieval component to reduce hallucinations by retrieving relevant documents at generation time.",
            "citation_title": "Retrievalaugmented generation for large language models: A survey",
            "mention_or_use": "mention",
            "agent_name": "RAG-augmented LLM",
            "agent_description": "An LLM architecture augmented with an external retrieval database used as long-term memory: at generation time the model retrieves relevant passages and conditions its outputs on them to reduce hallucination.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval-augmented external database",
            "memory_representation": "external database of text passages / documents (as described in the paper: an external DB)",
            "memory_access_mechanism": "retrieval from external database (document retrieval / retrieval augmented generation)",
            "task_name": null,
            "task_category": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper notes RAG reduces hallucinations; no quantitative trade-offs (latency, memory footprint, or retrieval cost) are reported in this paper.",
            "limitations_or_failure_cases": "No empirical failure cases or task-specific limitations reported in this paper; the paper emphasizes RAG as a long-term memory approach but does not provide experimental comparisons here.",
            "citation": "Gao et al., 2024. Retrievalaugmented generation for large language models: A survey.",
            "uuid": "e6419.0",
            "source_info": {
                "paper_title": "A Call for Embodied AI",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "AutoGPT",
            "name_full": "Auto-GPT (autonomous GPT agent project)",
            "brief_description": "An emergent class of autonomous language-model-based agents that generate and execute plans by decomposing goals into steps; mentioned as an example of agent-based LLMs in the context of embodied cognitive architectures.",
            "citation_title": "What if gpt4 became autonomous: The auto-gpt project and use cases",
            "mention_or_use": "mention",
            "agent_name": "AutoGPT",
            "agent_description": "Mentioned as an example of an agentic LLM that generates autonomous agents/behaviors; the paper does not describe any specific memory mechanism for AutoGPT.",
            "model_size": null,
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": null,
            "task_category": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "The paper only cites AutoGPT as an example of agent-based LLMs and does not report empirical trade-offs or memory-related costs.",
            "limitations_or_failure_cases": "No memory-specific limitations for AutoGPT are described in this paper.",
            "citation": "Firat & Kuleli, 2023. What if gpt4 became autonomous: The auto-gpt project and use cases. Journal of Emerging Computer Technologies.",
            "uuid": "e6419.1",
            "source_info": {
                "paper_title": "A Call for Embodied AI",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PanguAgent",
            "name_full": "Pan-guAgent (Pangu-agent)",
            "brief_description": "An agent-focused language model described as a fine-tunable generalist agent with structured reasoning; mentioned as an example of agent-oriented LLMs within the paper's discussion of cognitive architectures.",
            "citation_title": "Pangu-agent: A fine-tunable generalist agent with structured reasoning",
            "mention_or_use": "mention",
            "agent_name": "PanguAgent",
            "agent_description": "Cited as an agent-focused language model / generalist agent; the paper does not provide details on whether or how PanguAgent uses an explicit memory mechanism.",
            "model_size": null,
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": null,
            "task_category": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "No memory-specific trade-offs or empirical evaluations involving memory are reported for PanguAgent in this paper.",
            "limitations_or_failure_cases": "The paper does not report failure cases or limits of any memory component for PanguAgent.",
            "citation": "Christianos et al., 2023. Pangu-agent: A fine-tunable generalist agent with structured reasoning. arXiv:2312.14878.",
            "uuid": "e6419.2",
            "source_info": {
                "paper_title": "A Call for Embodied AI",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrievalaugmented generation for large language models: A survey",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "What if gpt4 became autonomous: The auto-gpt project and use cases",
            "rating": 2,
            "sanitized_title": "what_if_gpt4_became_autonomous_the_autogpt_project_and_use_cases"
        },
        {
            "paper_title": "Pangu-agent: A fine-tunable generalist agent with structured reasoning",
            "rating": 2,
            "sanitized_title": "panguagent_a_finetunable_generalist_agent_with_structured_reasoning"
        },
        {
            "paper_title": "The rise and potential of large language model based agents: A survey",
            "rating": 2,
            "sanitized_title": "the_rise_and_potential_of_large_language_model_based_agents_a_survey"
        }
    ],
    "cost": 0.010679749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Call for Embodied AI
13 Sep 2024</p>
<p>Giuseppe Paolo 
Jonas Gonzalez-Billandon 
Balázs Kégl 
A Call for Embodied AI
13 Sep 20240271893EA7538A027264C155DCBC9CE5arXiv:2402.03824v4[cs.AI]
We propose Embodied AI (E-AI) as the next fundamental step in the pursuit of Artificial General Intelligence (AGI), juxtaposing it against current AI advancements, particularly Large Language Models (LLMs).We traverse the evolution of the embodiment concept across diverse fields (philosophy, psychology, neuroscience, and robotics) to highlight how E-AI distinguishes itself from the classical paradigm of static learning.By broadening the scope of E-AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent.This framework is aligned with Friston's active inference principle, offering a comprehensive approach to E-AI development.Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist.Our discussion lays down a foundational guideline for future E-AI research.Highlighting the importance of creating E-AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.</p>
<p>Introduction</p>
<p>Over recent years, the field of artificial intelligence (AI) has experienced a significant surge, leading to substantial breakthroughs in areas ranging from computer vision (CV) and natural language processing (NLP) to neuroscience.This journey through AI's development has been marked</p>
<p>Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).by a series of significant triumphs interspersed with setbacks, including the well-documented AI winter of the mid-1980s.The ambitious goal that has propelled AI research forward from the beginning was to create intelligence that either parallels or exceeds human abilities.This quest for superhuman intelligence, commonly termed Artificial General Intelligence (AGI), has been seen differently by experts across different disciplines, yet it broadly refers to the ability of a system to understand, learn, and apply knowledge in a wide array of tasks and contexts, mirroring the cognitive flexibility of humans and animals.</p>
<p>The remarkable progress in AI over the past decade can largely be attributed to three pivotal developments: i) advancements in deep learning algorithms, ii) the advent of powerful new hardware, and iii) the availability of extensive datasets for training.A prime illustration of this advancement is the creation of Large Language Models (LLMs) like OpenAI's GPT-4 (Achiam et al., 2023) and Google's Gemini (Team et al., 2023).The surprising abilities of these LLMs have sparked discussions within the AI community, with some pondering whether these models have already achieved nascent forms of AGI.Foundation models (large networks with billions of parameters trained on massive datasets) have found success in varied fields, ranging from predicting 3D protein structures (Cramer, 2021) and robotic control (Brohan et al., 2023), to generating images and audio (Ramesh et al., 2022;Radford et al., 2022).This breadth of achievement supports the hypothesis that continued scaling and refinement of foundation models could be a viable path toward realizing AGI.</p>
<p>In our paper, we argue that despite the significant advances made by current AI technologies, they represent only the initial steps towards truly intelligent agents.Despite their impressive capabilities, these large networks are static and unable to evolve with time and experience.They leverage large datasets and cutting-edge hardware for scaling, but they lack the ability to properly care about the truth (Vervaeke &amp; Coyne, 2024), which in turn makes it impossible to dynamically adjust their knowledge and actively search for valuable new information.The two primary manifestations of this fundamental shortfall are i) the difficulty in effectively aligning LLMs (Ouyang et al., 2022), and ii) their propensity to generate plausible but inaccurate information, a phenomenon known as confabulation (Huang et al., 2023;Wei et al., 2023).Current strategies to mitigate these issues, such as post-processing, fine-tuning, prompt engineering, and incorporating human feedback, are undeniably valuable.However, we argue that these methods address only the superficial aspects of the problem and fall short in dealing with the core issue at play: the inherent lack of a deeper, grounded sense of care in LLMs.</p>
<p>Pursuing the development of AGI, we draw upon the insights of Vervaeke &amp; Coyne (2024) to advocate for designing AI agents that are bound to, observe, interact with, and learn from the real world (including humans) in a continuous and dynamic manner.These Embodied AI (E-AI) agents ought to prioritize their continued existence and our bindings to them, thereby learning the value of truth.They should also be capable of adapting to environmental changes and evolving without human intervention.</p>
<p>While Large Language Models play a significant role in the development of AI systems, they fall short of capturing the essence of what constitutes an intelligent agent.Notably, intelligent beings, whether humans or animals, are characterized by three fundamental components: the mind, perception, and action capabilities (Kirchhoff et al., 2018).LLMs, or more broadly, foundation models, may be likened to an aspect of the mind's reasoning function (Xi et al., 2023).Yet, the perceptive and action-oriented dimensions of intelligence, along with the pivotal ability to dynamically revise beliefs and knowledge based on experiences, remain unaddressed.Autoregressive LLMs are not designed to understand the causal relationships between events, but rather to identify proximate context and correlations within sequences (Bariah &amp; Debbah, 2023).In contrast, a fully embodied agent should have the ability to grasp the causality underlying events and actions within its environment, be it digital or physical.By comprehending these causal relationships, such an agent can make informed decisions that consider both the anticipated outcomes and the reasons behind those outcomes.</p>
<p>In short, this paper argues that the necessary next step in our pursuit for truly intelligent and general AIs is the development and study of Embodied AI.We propose that current LLM-based foundation models could lay the groundwork for designing these agents, but are just one component of a truly embodied agent.This approach is akin to how neonates come into the world equipped with inherent priors to successfully adapt to the world (Reynolds &amp; Roth, 2018).</p>
<p>In the next section of this paper, we will define the concept of embodiment and what we mean by E-AI in Sec. 2, analyzing the literature and various scientific and philosophical currents.In Sec. 3, we discuss why we believe this is a necessary step towards Artificial General Intelligence (AGI).</p>
<p>In Sec. 4 we analyze the main components of a truly embod-ied agent, and we discuss the major challenges to achieving this ambitious goal in Sec. 5. Our motivation behind the need to develop E-AI and its fundamental role in our path towards AGI is proposed throughout.Finally, Sec. 6 provides a short recap of our proposition.</p>
<p>What is embodied AI</p>
<p>E-AI is s sub-field of AI, focusing on agents that interact with their physical environment, emphasizing sensorimotor coupling and situated intelligence.As opposed to mere passive observing, E-AI agents act on their environment and learn from the reaction.E-AI is deeply rooted in embodied cognition (Shapiro, 2011;McNearney, 2011), a perspective in philosophy and cognitive science that posits a profound coupling between the mind and the body.This idea, challenging Cartesian dualism -the historically dominant view that distinctly separates the mind from the body (Descartes, 2012) -emerged in the early 20th century.Pioneers like Lakoff &amp; Johnson (1979;1999) have significantly contributed to this paradigm by proposing that reason is not based on abstract laws but is grounded in bodily experiences.Embodied cognition forms a critical part of the 4E cognitive science framework (Varela et al., 1991;Clark, 1997;Clark &amp; Chalmers, 1998), encompassing embodied, enactive, embedded, and extended aspects of cognition.Within E-AI, the focus is predominantly on implementing the 'embodied' and 'enactive' aspects, while the 'embedded' and 'extended' components are more pertinent to situating AI in a social context and as an augmentation of human (individual or collective) cognition.</p>
<p>In AI, initial explorations into embodiment emerged in the 1980s, driven by a growing recognition of the inherent limitations in disembodied agents.These limitations were primarily attributed to the absence of rich, high-bandwidth interactions with the environment (Pfeifer &amp; Iida, 2004;Pfeifer &amp; Bongard, 2006).An early advocate for this paradigm shift was Brooks (1991), who built walking robots simulating insect-like locomotion.Simultaneously, the field of computer vision was undergoing its own transformation.Researchers and practitioners were increasingly focusing on enabling agents to interact with their surroundings.This emphasis on interaction led to a concentration on the perceptual elements of embodiment, particularly from a first-person point of view (POV) (Shapiro, 2021).This approach aligns with the concept of visual exploration and navigation (Ramakrishnan et al., 2021), where an agent acquires information about a 3D environment through movement and sensory perception, thereby continuously refining its model of the environment (Anderson et al., 2018;Chen et al., 2019).Such exploration techniques empower an agent to discover objects and understand their permanence.As a result of these developments, many contemporary benchmarks in E-AI have emerged predominantly from the domains of vision and robotics (Duan et al., 2022), reflecting the integral role these disciplines have played in advancing the field.</p>
<p>That said, the broader definition of E-AI does not require vision.Sensorimotor coupling may be implemented using any physical sense (Pfeifer &amp; Bongard, 2006).In the living world, many organisms survive and thrive without vision, using, for example, chemical or electric sensing (Bargmann, 2006).Levin (2022)'s Technological Approach to Mind Everywhere (TAME) framework further explores this idea, suggesting that cognition emerges from the collective intelligence of cell groups, they themselves deeply embodied within their environment (the body they comprise).This framework challenges traditional Cartesian dualism, embedding cognition within the physical and biological makeup of an organism.In the TAME perspective, cognition is not just an attribute of higher-order organisms; it extends throughout the ontological hierarchy of living beings, from individual cells, through tissues and organs, to complex organisms.Each agent demonstrates cognitive capabilities that are inherently connected to its physical structure and the environmental interactions at its proper level.This broadened view of cognition and embodiment goes beyond the conventional focus on vision in robotics and computer vision.It posits that any entity capable of perceiving, interacting with, and learning from its environment, thereby adapting to it and influencing it, qualifies as embodied.A technological instantiation of this concept is an intelligent router in a telecommunication network.This device 'lives' in a realm dominated by electromagnetic sensing.It continuously learns from and adapts to the network traffic, effectively mapping and managing the flow of information.This example underscores the potential of applying the principles of E-AI beyond the traditional domains, embracing a more inclusive and diverse understanding of intelligence and embodiment.This broadening of the notion of E-AI raises the question: how close current commercial AI tools are to embodiment?Here we examine two such tools: Large Language Models (Brown et al., 2020;Devlin et al., 2019) and Social Media Content AI Recommendation Systems (SMAI) (Bakshy et al., 2015;Covington et al., 2016;Eirinaki et al., 2018).</p>
<p>LLMs operate within a linguistic-symbolic domain, representing textual information and generating new text by completing prompts.Their foundational training is essentially static, relying on datasets meticulously compiled and curated by teams of AI engineers.Their goal is supervised: to generate likely tokens following a context.Their secondary training (fine-tuning) may involve both interactions with their symbolic environment (human users) and goals to reach (satisfy their human users), but these interactions are presents some limits due to both technical (e.g., catastrophic forgetting (Kirkpatrick et al., 2017;Parisi et al., 2019)) and business (e.g., managing individuated LLMs (Strubell et al., 2019;Kaplan et al., 2020)) reasons.Looking ahead, we anticipate advancements that might address these limitations, potentially leading to the emergence of "personal assistant" LLMs.These would represent a form of embodied agents within a symbolic realm.However, at present, LLMs largely resemble static Internet AI (I-AI) (Duan et al., 2022), differing significantly from the dynamic, interactive nature characteristic of E-AI.</p>
<p>It is intriguing that despite the growing concerns about the risks and alignment challenges of LLMs highlighted in recent research (Bender et al., 2021), SMAIs have attracted comparatively less scrutiny (Huszár et al., 2022;Ribeiro et al., 2020).This is noteworthy considering SMAIs have been around for a longer time and their influence on society is both wider and more profound.We propose that their widespread acceptance and their more integrated, less intrusive presence in our lives are due to their closer alignment with the principles of embodiment, in contrast to LLMs.</p>
<p>What do we mean by SMAIs being closer to embodiment?Firstly, SMAIs are driven by clear objectives: to captivate our attention and maximize our engagement with their respective platforms (Bozdag, 2013;Bodó, 2021).These goals are fundamentally linked to the business models of these platforms, which revolve around advertising.The specifics of these "engagement" objectives are typically proprietary, forming the core of the competitive advantage of these platforms.Although these goals are initially human-designed and not intrinsically generated (Covington et al., 2016), they are subject to evolutionary pressure and adaptation, and thus they are tied to the existence and the survival of the SMAI.Secondly, SMAIs learn almost entirely from the data they collect by interacting with us.This leads to a high level of individuation (adapting to our individual preferences (Nguyen et al., 2014)), and notions of exploration (offering us content not so much to satisfy us but for the sake of learning what we like).This creates a user experience that, when wellexecuted, resembles interaction with a considerate friend, who wants our best, who connects us to things we like, and who wants to understand us better.The flip side, however, is the potential for these systems to morph into mechanisms that perpetuate addictive behaviors or harmful content (Schüll, 2012).Nevertheless, since SMAIs connect and adapt to us in a more intuitive and deeper manner than LLMs, we often feel a greater sense of control over our interactions with these systems (by, for example, consciously not clicking on content that we know we do not want to see in the long run).This control, albeit limited, is reminiscent of persuasion more than mechanical manipulation, aligning with how we interact with other sentient beings rather than machines.This type of relationship with AI systems is a fundamental aspect of Levin (2022)'s TAME proposal .Our stance on E-AI suggests that, while systems akin to SMAIs pose greater risks due to their seamless integration into our social fabric, they also present more natural opportunities for alignment with our values.This alignment process is procedural, perspectival, and evolutionary in nature (Vervaeke et al., 2012;Vervaeke &amp; Coyne, 2024), contrasting with the primarily propositional approaches being applied to LLMs (Shen et al., 2023).</p>
<p>We posit that the potential for more effective and naturally aligned AI systems is, alone, a compelling reason to prioritizing E-AI in the broader AI research agenda.</p>
<p>In the forthcoming section, we further explore the pivotal role that well-executed implementations of E-AI could play in the quest for AGI.</p>
<p>Why embodiement?</p>
<p>In the previous section, we examined how contemporary theories of embodiment, particularly the TAME framework (Levin, 2022), challenge the long-standing Cartesian dualism which posits a distinct separation between mind and body (Descartes, 2012).This philosophical stance has significantly influenced the development of current generative AI models, such as LLMs, which primarily rely on static data and lack interaction with the physical or even the symbolic world.It is a prevalent belief that simply scaling up such models, in terms of data volume and computational power, could lead to AGI.We contest this view.We propose that true understanding, not only propositional truth but also the value of propositions that guide us how to act, is achievable only through E-AI agents that live in the world and learn of it by interacting with it.</p>
<p>The significance of embodiment in cognitive development was demonstrated by Held &amp; Hein (1963)'s carousel experiment with kittens.In this study, one kitten could actively interact with and control a carousel, while the other could only observe it passively.Despite both kittens receiving identical visual input, the one engaged in active interaction exhibited normal visual development, unlike its passively observing counterpart.This seminal experiment underscores the vital role of embodied interaction in shaping cognitive abilities (Shenavarmasouleh et al., 2022).It also reinforces the observation that all known forms of intelligence, including human intelligence, are inherently embodied (Smith &amp; Gasser, 2005), suggesting that embodiment serves as a solid foundation for cognitive learning and development.Current AI learns in a very different way from humans.We humans learn by seeing, moving, interacting with the world and speaking with others.We also learn by collecting sequential experiences, not by passive observation of shuffled and randomized, even if carefully selected, data (Smith &amp; Gasser, 2005;Westho et al., 2020).We advocate for an approach where insights from cognitive science and developmental psychology inform the design of AI systems.Such systems should be designed to learn through active interaction with their surroundings, mirroring the embodied learning processes fundamental to human cognition.</p>
<p>Even advocates of static learning concede that multimodal learning is the next milestone towards AGI (Fei et al., 2022;Parcalabescu et al., 2021).In I-AI, multimodal data needs to be collected and connected painstakingly.In contrast, E-AI agents, when equipped with multimodal sensors, will inherently collect and correlate multi-modal data by mere co-occurrence.For instance, robots will see (CV), communicate (NLP), reason (general intelligence), navigate and interact with their environment (planning and RL), all simultaneously (Shenavarmasouleh et al., 2022).Intelligent routers will observe requests and traffic (sensing), communicate with other routers, human engineers, absorb news about their surroundings (NLP), reason (general intelligence), and control the traffic (control and RL).Despite the impressive progress in these domains, much of it has relied on the external collection and curation of vast datasets for algorithmic training.This approach has significant drawbacks: i) the collection and preparation of data demands substantial investments; ii) this data can contain biases that are hard to detect and rectify (Li &amp; Deng, 2020;Balayn et al., 2021;Verma et al., 2021).The issue of biases is particularly pertinent in discussions on AI alignment (Shen et al., 2023;Ji et al., 2023).Efforts to align AI through rule-based and procedural methods (such as RLHF (Lambert et al., 2022)) often struggle, producing systems that feel mechanistic and "dumb", rather than an agent which seamlessly acts according to values compatible with our society.</p>
<p>An embodied agent, designed to interact with and learn from its environment, fundamentally changes the traditional approach to data collection and curation in AI development.By being inherently integrated with its physical and social contexts, such an agent bypasses the laborintensive processes previously required.This shift not only simplifies the challenge of aligning AI with human values but also enhances the agent's learning efficiency by utilizing the unique features of its environment.As a result, the focus in AI development transitions from data to simulators.These simulators serve a dual purpose: they are both training grounds for E-AI and platforms for testing and refining concepts and algorithms (Duan et al., 2022).Moreover, the process of aligning these agents with human values becomes more intuitive as it involves defining goals re-flective of those values.This approach does not claim to fully resolve the alignment challenge, as E-AI systems will still necessitate oversight and guidelines to avert unwanted behaviors.However, the alignment process becomes inherently more natural.Adjusting and defining goals is a more straightforward task than the extensive editing and curating of data.This methodology draws upon our inherent, non-propositional understanding and instincts about aligning embodied intelligences-whether it is guiding our own actions, nurturing children, or training pets.</p>
<p>Another important characteristic of E-AI, stemming from the coupling between the agent and its environment, is the agent's capacity for ongoing evolution and adaptation.This adaptability is vital for any agent destined to navigate a world in perpetual change.It underscores the importance of continual learning: the process of assimilating new experiences while retaining previously acquired knowledge (Wang et al., 2023a).</p>
<p>Moreover, Ishiguro &amp; Kawakatsu (2004) have shown, both through theory and practical application in robotics, that a close and effective integration of control mechanisms with body dynamics significantly enhances energy efficiency.Ororbia &amp; Friston (2024) elaborates on energy efficiency, proposing that embodied mortal systems, which are characterized by their inherent lifecycle and eventual mortality, can optimize energy usage through adaptive processes.These processes allow the system to self-organize and maintain homeostasis by minimizing free energy, in alignment with the principles of the free energy principle (Friston, 2010;Friston et al., 2023).Coupled systems lead also to the emergence of intriguing behaviors that can be hard to explicitly program or learn from disembodied datasets (Rosas et al., 2020), an observation aligning with the principles of the TAME framework.</p>
<p>Embodiment is also a prerequisite for learning about affordances (Gibson, 1979).Learning, or more precisely realizing affordances, according to Vervaeke et al. (2012)'s perspectival learning, is a fundamental capacity of AGI, as affordances are what "fill our world with meaning" (Roli et al., 2022), and are thus necessary for agents that give meaning to their own world.Affordances emerge from the dynamic interplay between an agent's perception, objectives, abilities, and the characteristics of objects and contexts within the environment; for example, a chair affords us to sit, a glass to drink and a hand to grasp and pick up objects.Roli et al. (2022) argue that the capacity to comprehend, utilize, and be influenced by environmental affordances distinguishes biological intelligence from current artificial systems.Besides affordances, E-AI is also indispensable for investigating emergent phenomena such as qualia (Locke, 1847;Korth, 2022), consciousness (Solms, 2019), as well as creativity, empathy (Perez, 2023), and eth-ical understanding (Lake et al., 2017;Russell, 2021).</p>
<p>Finally, there is the important question of why an intelligent agent would do anything in the first place (Pfeifer &amp; Iida, 2004).What drives it to engage and acquire new knowledge without external prompts?Within well-framed small worlds, such as a chess game, an agent's purpose is straightforward: deciding the next move.However, when navigating large, open worlds, the motivations guiding an agent's decisions grow increasingly ambiguous.The concepts of active inference and the free energy principle (Friston, 2010;Friston et al., 2023) provide a compelling framework for understanding the behaviors of intelligent agents.This principle posits that minimizing surprise and uncertainty is the core objective of the agents.They achieve this through the use of internal models to forecast outcomes, continually updating these models with sensory input, and proactively modifying their surroundings to better match their expectations.This concept resonates within the AI community, particularly in the design of agents equipped with mechanisms for intrinsic motivation (Oudeyer &amp; Kaplan, 2007;Pathak et al., 2017), which incentivize agents to explore and acquire new knowledge to reduce uncertainty.</p>
<p>However, what propels an intelligent agent to act, especially beyond mere survival instincts, continues to be a matter of debate.We argue that exploring and developing embodied agents will illuminate this question.Thus, E-AI not only shows potential for significant breakthroughs toward achieving AGI, but also has deep implications for our understanding of cognition in general.</p>
<p>Theoretical framework</p>
<p>In previous sections, we have underscored the pivotal role of E-AI in advancing toward AGI.Shifting focus, we now delve into the essential components that, we believe, will comprise E-AIs.We draw heavily on the concept of cognitive architectures designed by cognitive scientists aiming to model the human mind (Thagard, 2012).Despite the promise these architectures hold for enhancing modern machine learning methods, progress on this has been notably limited (Kotseruba &amp; Tsotsos, 2020).The slow advancement is largely due to cognitive architectures being the domain of neuroscientists and cognitive scientists, with only a select few within the machine learning community exploring their potential for AGI.We advocate for a synergistic strategy that marries cognitive architectures with machine learning within the E-AI paradigm, proposing it as a viable path toward AGI.The emergence of agent-based LLMs, such as AutoGPT (FIRAT &amp; Kuleli, 2023), which pioneers the generation of autonomous agents, and Pan-guAgent (Christianos et al., 2023), an agent-focused language model, indicate the potential of this approach.</p>
<p>We identify four essential components of an E-AI sys-tem: (i) perception: the ability of the agent to sense its environment; (ii) action: the ability to interact with and change its environment; (iii) memory: the capacity to retain past experiences; and (iv) learning: integrating experiences to form new knowledge and abilities.These components are notably aligned with the active inference framework of Friston (2010).In this framework, the agent models its world through a probabilistic generative model that infers the causes of its sensory observations (perception).This model is hierarchical, forecasting future states in a top-down manner and reconciling these predictions with bottom-up sensory data, with discrepancies or errors being escalated upwards only when they cannot be reconciled at the initial level.The agent acts to minimize the divergence between its anticipations and reality, thus moving towards states of reduced uncertainty (action).Concurrently, it collects and stores new information about its environment (memory) and refines its internal model to minimize predictive errors (learning).In the sections that follow, we will describe in detail these four components and how they comprise the E-AI agent.</p>
<p>Perception</p>
<p>At the heart of an embodied agent lies the ability to perceive the world in which it exists.Perception is a process by which raw sensory data is transformed into a structured internal representation, enabling the agent to engage in cognitive tasks.The range of inputs that inform perception is vast, encompassing familiar human senses such as vision, hearing, smell, touch, and taste.It extends to any form of stimuli an agent might encounter, be it force sensors in robotics or signal strength indicators in wireless technology.The challenge with sensory data is that it is often not immediately actionable.It typically undergoes a process of transformation, a task where recent advances in machine learning can prove invaluable.The field has seen the development of sophisticated methods for learning feature and embedding spaces, facilitating the conversion of raw data into meaningful information (Golinko &amp; Zhu, 2019;Sivaraman et al., 2022).A particularly effective strategy has been self-supervised learning to learn such representations.Although much of the research has concentrated on single modalities, such as vision (Oquab et al., 2023), the principles underlying these techniques are universally applicable across different sensory inputs (Orhan et al., 2022;Lee et al., 2019).</p>
<p>Action</p>
<p>Embodied agents navigate the world by taking actions and observing the outcomes.Acting can be broken down into two steps: (i) choosing what action to undertake next, like deciding to relocate to a specific spot, and (ii) determining how to execute this action, such as plotting the course to that location.Actions can further be categorized into re-active and goal-directed types.Reactive actions, akin to human reflexes, occur almost instantaneously in response to stimuli and play a crucial role in an agent's immediate self-preservation by maintaining stability.Goal-directed actions, on the other hand, involve strategic planning and are motivated by high-level objectives.Reactive actions are important for self-preservation, with model-free reinforcement learning methods playing an important role for developing reactive control policies in tasks like robot walking (Rudin et al., 2022).On the other hand, for an agent to achieve more complex, high-level objectives, planning is indispensable, even if efficient planning remains an open area of research (Lin et al., 2022;Shi et al., 2022).Central to the concept of planning is the presence of a "world model" within the agent, which it can use to predict the consequences of its own actions.Model-based RL has made significant strides in developing algorithms that learn these world models and use them for planning (Silver et al., 2016;Kégl et al., 2021;Paolo et al., 2022).</p>
<p>Memory</p>
<p>Embodied agents learn from their experience, which are stored in memory.Memory encompasses various dimensions, including its duration (short-term or long-term) and its nature (procedural, declarative, semantic, and episodic).Importantly, memory is not necessarily represented as explicit propositional knowledge; it can be implicitly encoded into the weights of a neural network (NN).To navigate cognitive tasks, agents require diverse types of memory systems, each playing a distinct role.Working and short-term memory offer temporary storage to support the agent's immediate objectives.Long-term and episodic memories provide a reservoir for information over longer time.Episodic memory captures and stores unique, perspectival experiences, ready to be accessed when familiar scenarios unfold.Long-term memory, conversely, is the repository for broader propositional knowledge.LLMs, for example, implement long-term memory using Retrieval-Augmented Generation (RAG) (Gao et al., 2024), a technique that reduces hallucinations using an external database.This technique showcases how sophisticated machine learning methods can be synergized with cognitive architectures.</p>
<p>Learning</p>
<p>A defining trait of intelligent agents is their ability to learn.Yet, how to learn, especially in a continuous and dynamic way, remains a subject of ongoing research and debate (Wang et al., 2023a;Yifan et al., 2023).While recent strides in AI have largely been powered by training on static datasets, the concept of continual learning, essential for adapting over time, faces challenges.These challenges stem primarily from the inherent limitations of deep NNs, such as catastrophic forgetting (Kemker et al., 2018), and the complexities associated with learning from non-stationary data that result from an agent's interaction with its environment (Fahrbach et al., 2023).The embodiment hypothesis suggests that true intelligence is born from such interactions (Smith &amp; Gasser, 2005), underscoring the need for dynamic learning methodologies.In this context, simulators emerge as a vital tool, offering a shift away from the static learning typical of traditional AI.Instead, they enable agents to evolve through ongoing, interactive experiences within simulated environments (Duan et al., 2022).</p>
<p>Challenges</p>
<p>E-AIs agents will adopt an egocentric perspective, experiencing their environment from a first-person viewpoint, in contrast to the allocentric perspective prevalent in current AI systems.This shift is not only essential for meaningful interaction with the world but also offers an advantage by allowing the agents to focus on modeling their immediate surroundings rather than the entirety of the world.On the other hand, E-AIs introduces several challenges, including extending current learning theories, managing noise in perception and action effectively and safely, and ensuring meaningful communication with humans that adheres to ethical standards.The remainder of this section will cover these challenges, exploring potential pathways and solutions.</p>
<p>New learning theory</p>
<p>The principles of E-AI challenge us to reevaluate traditional learning theories (Devroye et al., 1996;Vapnik, 1998), bridging a gap between supervised and reinforcement learning.Supervised learning, while foundational in AI, assumes that the data is drawn from an unknown but fixed distribution, collected independently of the learning process.This theory gives rise to the classical notions of generalization, over-and underfitting, bias and variance, and asymptotic or finite-sample statistical consistency.This framing is obviously highly useful: even those who are not explicitly doing theory use it transparently as their lingua technica and cognitive scaffolding when working with algorithms and analyzing results.</p>
<p>When embodied agents interact dynamically with their environment, data collection becomes part of the data science pipeline (Pfeifer &amp; Iida, 2004;Thrun et al., 2005).Classical supervised learning theory is insufficient to analyze these cases and to guide algorithm building.Extensions, like transfer learning (Pan &amp; Yang, 2010), multitask learning (Caruana, 1997), distribution shift (Quiñonero-Candela et al., 2009), domain adaptation (Csurka, 2017) or out-of-distribution generalization, have been proposed to patch basic supervised learning theory, but most of these cling to the original framing, pretending that the data is coming from outside the learning pro-cess, encapsulating the value (business or otherwise) of the predictive pipeline.Practically, this is obviously not the case: the data on which we learn a predictor is often collected by the data scientist, responsible for the quality of the pipeline (O'Neil &amp; Schutt, 2013;Provost &amp; Fawcett, 2013).Furthermore, most of the debates around responsible AI turn around the data, not the learning algorithm (O'Neil, 2016;Selbst et al., 2019).Collecting, selecting, and curating data is obviously part of the pipeline.The text we use to train LLMs is created by its writers, rather than drawn from a distribution.In some cases, when collection and model-retraining are automated, the situation may be even worse.For example, in click-through-rate prediction (Bottou et al., 2013;Perlich et al., 2014) or recommendation systems (Deldjoo et al., 2020), the deployed predictor affects the data for the next round of training, generating an often adversarial feedback.A similar phenomenon is happening in the LLM world: as these AIs become the go-to tools for creative and business writing, the data collected for the next round of training will, in large part, be coming from the previous generation of LLMs.</p>
<p>Reinforcement Learning (Sutton &amp; Barto, 2018) and related paradigms (Bayesian optimization (Mockus, 1989) or contextual bandits (Langford &amp; Zhang, 2008)) offer a closer fit for embodied AI, when the prediction is not the end-product, rather part of a predictive pipeline that also includes data collection.RL affords the data scientist to design a higher-level objective, letting the algorithm optimize both the predictor and the data it is trained on.Here, the mismatch between theory and practice is different from supervised learning.The analysis in RL or bandit theory often focuses on the convergence of the agent to a theoretical optimum, given a fixed but often unknown environment.RL theory usually does not offer tools to analyze the data collected during the learning process, especially when the collection is semi-automatic (includes a human curator in the loop).RL agents, in practice, usually do not converge even in a stationary environment, they rather individuate, making, for example, quite perversely, the random seed part of the algorithm (Henderson et al., 2018).This is even more pronounced in non-stationary environments where the agent's actions alter the environment; a situation which AGI will definitely find itself (da Silva et al., 2006;Zhou et al., 2024).</p>
<p>A new learning theory for embodied AI must transcend these limitations.It should account for the dynamic, interactive nature of data in E-AI, where the agent's actions continuously reshape its learning environment.This theory should not just aim for optimal performance in a fixed setting but should embrace a spectrum of behaviors suitable for evolving environments.Moreover, it should provide diagnostics to assess the quality and relevance of data generated through these interactions.</p>
<p>Noise and uncertainty</p>
<p>E-AI agents are tasked with navigating the real world, rife with noise and uncertainty.These elements can drastically affect both the agent's perception of its surroundings and the quality of its decision-making.For example, elevated noise levels may distort the agent's interpretation of environmental cues, leading to suboptimal decisions.This challenge is accentuated in an egocentric perspective, where agents frequently encounter continuous streams of fluctuating and imprecise data.Sources of noise include the natural imprecision of sensors and actuators, which might lack accuracy due to manufacturing inconsistencies, degradation over time, or external disturbances.Additionally, quantization error, a byproduct of converting analog signals into digital form (Widrow &amp; Kollár, 2008), can further compromise data integrity.</p>
<p>As these agents learn and adapt to their environment, they must also grapple with uncertainty.This uncertainty can obscure the agent's understanding of its environment, influencing its performance.This dilemma is especially prevalent in RL scenarios dealing with partial observability, where decisions must be made with incomplete information, leading to uncertainty in predicting the outcomes of its actions (Dulac-Arnold et al., 2021;Hess et al., 2023;Pattanaik et al., 2017).Therefore, managing noise and uncertainty effectively is paramount for the progress of E-AI.</p>
<p>Simulators</p>
<p>As we pivot towards E-AI, simulators will assume a fundamental role as a key driver of progress, similar to the role data sets play in the training of traditional I-AI models.These simulators offer a controlled, replicable environment where AI systems can be rigorously trained and tested.This setup allows for learning and adapting to diverse scenarios prior to deployment, ensuring both safety and cost-efficiency.A notable advantage of simulators, and requirements, is their speed and ease of parallelization, significantly accelerating training time, making it more feasible to train sophisticated AI models on multiple scenarios simultaneously.</p>
<p>Many advanced simulators have been introduced recently, yet they often demand significant computational resources and are predominantly geared towards robotics applications (Li et al., 2021;Gan et al., 2020;Yan et al., 2018;Puig et al., 2018;Gao et al., 2019).For these simulators to truly serve the needs of E-AI, they must expand their scope to a broader spectrum of environments.A major challenge in the use of simulators is bridging the "reality gap" (Bousmalis &amp; Levine, 2017): the difference between simulated conditions and the agent's eventual real-world or virtual deployment context (Ligot &amp; Birattari, 2020).This gap can lead to a situation where models that excel in simulations fail in actual application, undermining the effective-ness of the training process.Despite numerous strategies being put forward to mitigate the reality gap (Salvato et al., 2021;Daza et al., 2023;Daoudi et al., 2023;Koos et al., 2012;Tobin et al., 2017), it remains an unresolved issue in the field, challenging the applicability of simulated training environments.</p>
<p>Interaction with humans</p>
<p>A key ambition of E-AI is to seamlessly interact with and learn from humans, enhancing AI's ability to offer personalized and impactful solutions.By improving these interactions, E-AI will also diminish fear and mistrust towards AI technologies, leading to broader acceptance and integration.In this endeavor, LLMs stand out as particularly beneficial, with their ability to comprehend and produce human-like text, facilitating communication in natural language and making engagements with AI more natural and accessible.The domain of Human-Robot Interaction (HRI) offers valuable lessons for enhancing AI-human communication, as researchers in this domain have dedicated efforts to explore innovative methods for robots to better communicate with us (Amirova et al., 2021;Bonarini, 2020).Yet, the challenge of ensuring proper and ethical communication with AI systems persists.The effectiveness of LLMs, for instance, hinges significantly on their training and how well they are aligned with human intentions and values (Wang et al., 2023b).Integrating human oversight directly into the AI development process and establishing comprehensive guidelines and protocols for AI communication are among the proposed strategies to address these challenges, aiming to make AI interactions more meaningful and ethically sound.</p>
<p>Generalization</p>
<p>An important issue in AI is generalization.There have been many attempts at developing systems capable of quickly generalizing to settings unseen at training time (Pourpanah et al., 2022) in the same fashion living beings do.Nonetheless this is still an open problem that will likely afflict embodied AIs as well, as acting in the real world exposes the agent to situations unseen at training time.For instance, consider a service robot trained in a simulated environment.When placed in a real household, it may encounter novel objects and behaviors not present in its training data, leading to suboptimal or even erroneous actions.This illustrates the critical need for AIs that can adapt and generalize beyond their initial programming.A promising direction in addressing this problem is the leveraging of the enormous amount of internet data.LLMs have demonstrated remarkable zero-shot learning capabilities with minimal fine-tuning (Wei et al., 2021).We can envision that some form of pretraining on internet datasets can kick-start the AI before its embodied phase, enhancing generalization and adaptability.</p>
<p>Recent developments in robotics have started exploring this research direction.Ahn et al. (2024) used a mixed approach between I-AI and E-AI to effectively control multiple robots in different settings.However, only relying on internet data is insufficient.An important aspect is also the ability to accurately identify unknown situations and avoid overconfidence, a common shortcoming of LLMs, that often produce plausible-sounding response that are factually incorrect (Xiong et al., 2024).The ability to assess its own uncertainty is essential, and can prompt the AI to seek human assistance, similarly to how infants ask for help in their early development.We believe that, while the integration of I-AI and E-AI will prove necessary as foundation for the development of the next generation of intelligent systems, the active learning paradigm and precise uncertainty estimation are vital.Active learning, where the AI actively queries for information when uncertain, combined with reliable uncertainty estimation, can enable an E-AI agent to manage novel situations effectively.</p>
<p>Finally, we believe that to properly address the issue of generalization, the community must first clearly define what is the meaning of "generalization".Currently, discussions around this issue often rely on vague terms, referring to an agent's ability to adapt to unseen settings or data.However, without a formal definition, it is challenging to assess or improve generalization effectively.</p>
<p>Consider the varying degrees of generalization required in different scenarios: transferring skills from driving a car to driving a bus represents generalization within a similar domain, whereas adapting from walking to swimming involves a more profound shift in the type of task.These examples illustrate the spectrum of generalization challenges that embodied AI might face.</p>
<p>To advance this field, it is imperative to develop a precise definition of generalization and establish standardized metrics and benchmarks for measuring an AI's generalization capabilities (Kawaguchi et al., 2017).This necessity ties back to our discussion in Section 5.1, highlighting the urgent need for a new learning theory that can provide a principled approach to developing agents that generalize well.Addressing these questions will hopefully lead to more precise and principled approaches in the development of the field.</p>
<p>Hardware limitations</p>
<p>A significant challenge to the broad-scale development and integration of E-AI lies in the hardware requirements of these AI systems.Presently, AI technologies largely depend on GPU clusters, which are, while powerful, not ideally suited for embodied agents due to their high cost, energy consumption, and extensive heat output.Additionally, the physical bulk and heft of GPUs pose logistical challenges for mobile agents or those operating within spatial limitations.Addressing these constraints necessitates the innovation of new, energy-efficient hardware solutions that can be embedded within the agents.Promising developments are on the horizon, with Google's Tensor Processing Unit (TPU) (Norrie et al., 2021;Cass, 2019) and Huawei's Ascend chip (Liao et al., 2021) leading the charge.These advancements, coupled with the potential of neuromorphic computing and the strategic synergy of hardware-software co-design, signal a new era of hardware capability.Moreover, the development of energy and data-efficient algorithms is critical.Such breakthroughs in hardware and algorithm efficiency will have a direct and profound effect on an AI's ability to understand, decide, and interact within its environment, enabling E-AI agents to operate more autonomously and effectively in a diverse array of settings.</p>
<p>Conclusion</p>
<p>In this paper, we have articulated the critical role Embodied AI plays on the path toward achieving AGI, setting it apart from prevailing AI methodologies, notably LLMs.By integrating insights from a spectrum of research fields, we underscored how E-AI's development benefits from existing knowledge, with LLMs enhancing the potential for intuitive interactions between humans and emerging AI entities.We introduced a comprehensive theoretical framework for the development of E-AI, grounded in the principles of cognitive science, highlighting perception, action, memory, and learning, situating E-AI within the context of Friston's active inference framework, thereby offering a wide-ranging theoretical backdrop for our discussion.Despite the outlook, the journey ahead is fraught with challenges, not least the formulation of a novel learning theory tailored for AI and the creation of sophisticated hardware solutions.This paper aims to serve as a roadmap for ongoing and future research into E-AI, proposing directions that could lead to significant advancements in the field.</p>
<p>Impact Statement</p>
<p>While the development of Embodied AI introduces complexities and challenges, particularly in hardware requirements, ethical considerations, and safety protocols, the potential benefits significantly outweigh these drawbacks.E-AI stands to evolve our interaction with technology, imbuing AI with a deeper understanding of and engagement with both the physical world and human society.This not only paves the way for more natural and effective human-AI interactions but also enhances AI's adaptability and application across a broad spectrum of fields.
 Noah's Ark Lab, Huawei Technologies France, Paris, France
London Research Center, London, UK. Correspondence to: Giuseppe Paolo <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#105;&#117;&#115;&#101;&#112;&#112;&#101;&#46;&#112;&#97;&#111;&#108;&#111;&#64;&#104;&#117;&#97;&#119;&#101;&#105;&#46;&#99;&#111;&#109;">&#103;&#105;&#117;&#115;&#101;&#112;&#112;&#101;&#46;&#112;&#97;&#111;&#108;&#111;&#64;&#104;&#117;&#97;&#119;&#101;&#105;&#46;&#99;&#111;&#109;</a>.</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Autort: Embodied foundation models for large scale orchestration of robotic agents. M Ahn, D Dwibedi, C Finn, M G Arenas, K Gopalakrishnan, K Hausman, B Ichter, A Irpan, N Joshi, R Julian, arXiv:2401.129632024arXiv preprint</p>
<p>10 years of human-nao interaction research: A scoping review. A Amirova, N Rakhymbayeva, E Yadollahi, A Sandygulova, W Johal, 10.3389/frobt.2021.744526/fullFrontiers in Robotics and AI. 87445262021</p>
<p>Den Hengel, A. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, Van, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Exposure to ideologically diverse news and opinion on facebook. E Bakshy, S Messing, L A Adamic, 10.1126/science.aaa1160Proceedings of the National Academy of Sciences. the National Academy of Sciences2015112</p>
<p>Managing bias and unfairness in data for decision support: a survey of machine learning and data engineering approaches to identify and mitigate bias and unfairness within data management and analytics systems. A Balayn, C Lofi, G.-J Houben, 10.1007/s00778-021-00671-8The VLDB Journal. 3052021</p>
<p>Comparative chemosensation from receptors to ecology. C I Bargmann, Nature. 44471172006</p>
<p>Ai embodiment through 6g: Shaping the future of agi. L Bariah, M Debbah, 2023</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and TransparencyACM2021</p>
<p>Selling news to audiences-a qualitative inquiry into the emerging logics of algorithmic news personalization in european quality news media. B Bodó, 10.1080/21670811.2019.1624185Algorithms, Automation, and News. Routledge2021</p>
<p>Communication in human-robot interaction. A Bonarini, 10.1007/s43154-020-00026-1Current Robotics Reports. 12020</p>
<p>Counterfactual reasoning and learning systems: The example of computational advertising. L Bottou, J Peters, J Quiñonero-Candela, D X Charles, M Chickering, E Portugaly, D Ray, P Simard, E Snelson, Journal of Machine Learning Research. 201314</p>
<p>Closing the simulation-toreality gap for deep robotic learning. K Bousmalis, S Levine, Google Research Blog. 12017</p>
<p>Bias in algorithmic filtering and personalization. E Bozdag, 10.1007/s10676-013-9321-6Ethics and information technology. 152013</p>
<p>Rt-2: Vision-languageaction models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, P Florence, C Fu, M G Arenas, K Gopalakrishnan, K Han, K Hausman, A Herzog, J Hsu, B Ichter, A Irpan, N Joshi, R Julian, D Kalashnikov, Y Kuang, I Leal, L Lee, T.-W E Lee, S Levine, Y Lu, H Michalewski, I Mordatch, K Pertsch, K Rao, K Reymann, M Ryoo, G Salazar, P Sanketi, P Sermanet, J Singh, A Singh, R Soricut, H Tran, V Vanhoucke, Q Vuong, A Wahid, S Welker, P Wohlhart, J Wu, F Xia, T Xiao, P Xu, S Xu, T Yu, B Zitkovich, arXivpreprintarXiv:2307.158182023</p>
<p>Intelligence without representation. R A Brooks, Artificial intelligence. 471-31991</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>R Caruana, Multitask, Learning, 10.1023/A:1007379606734Machine Learning. 199728</p>
<p>Taking ai to the edge: Google's tpu now comes in a maker-friendly package. S Cass, IEEE Spectrum. 5652019</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, arXiv:1903.019592019arXiv preprint</p>
<p>Pangu-agent: A fine-tunable generalist agent with structured reasoning. F Christianos, G Papoudakis, M Zimmer, T Coste, Z Wu, J Chen, K Khandelwal, J Doran, X Feng, J Liu, arXiv:2312.148782023arXiv preprint</p>
<p>Being There: Putting Brain, Body, and World Together Again. A Clark, 1997MIT Press</p>
<p>. A Clark, D Chalmers, </p>
<p>. Analysis. 5811998</p>
<p>Deep neural networks for youtube recommendations. P Covington, J Adams, E Sargin, 10.1145/2959100.2959190Proceedings of the 10th ACM Conference on Recommender Systems. the 10th ACM Conference on Recommender Systems2016</p>
<p>Alphafold2 and the future of structural biology. P Cramer, Nature structural &amp; molecular biology. 2892021</p>
<p>Domain adaptation for visual applications: A comprehensive survey. G Csurka, 10.1007/978-3-319-58347-1_1arXiv:1702.053742017arXiv preprint</p>
<p>Dealing with non-stationary environments using context detection. B C Da Silva, E W Basso, A L C Bazzan, P M Engel, 10.1145/1143844.1143872Proceedings of the 23rd International Conference on Machine Learning, ICML '06. the 23rd International Conference on Machine Learning, ICML '062006</p>
<p>A trust region approach for few-shot sim-to-real reinforcement learning. P Daoudi, C Prieur, B Robu, M Barlier, L D Santos, 2023</p>
<p>Sim-to-real transfer and reality gap modeling in model predictive control for autonomous driving. I G Daza, R Izquierdo, L M Martínez, O Benderius, D F Llorca, 10.1007/s10489-022-04148-1Applied Intelligence. 53102023</p>
<p>Adversarial machine learning in recommender systems (amlrecsys). Y Deldjoo, T Di Noia, F A Merra, 10.1145/3336191.3371877Proceedings of the 13th International Conference on Web Search and Data Mining. the 13th International Conference on Web Search and Data Mining2020</p>
<p>Discourse on method. R Descartes, 2012Hackett Publishing</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, 2019</p>
<p>A Probabilistic Theory of Pattern Recognition. L Devroye, L Györfi, G Lugosi, 10.1007/978-1-4612-0711-51996Springer</p>
<p>A survey of embodied ai: From simulators to research tasks. J Duan, S Yu, H L Tan, H Zhu, C Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 622022</p>
<p>Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. G Dulac-Arnold, N Levine, D J Mankowitz, J Li, C Paduraru, S Gowal, T Hester, 10.1007/s10994-021-05961-4Machine Learning. 2021110</p>
<p>Recommender systems for large-scale social networks: A review of challenges and solutions. M Eirinaki, J Gao, I Varlamis, K Tserpes, Future Generation Computer Systems. 782018</p>
<p>Learning rate schedules in the presence of distribution shift. M Fahrbach, A Javanmard, V Mirrokni, P Worah, arXiv:2303.156342023arXiv preprint</p>
<p>Towards artificial general intelligence via a multimodal foundation model. N Fei, Z Lu, Y Gao, G Yang, Y Huo, J Wen, H Lu, R Song, X Gao, T Xiang, Nature Communications. 13130942022</p>
<p>What if gpt4 became autonomous: The auto-gpt project and use cases. M Firat, S Kuleli, Journal of Emerging Computer Technologies. 312023</p>
<p>The free energy principle: A unified brain theory?. K Friston, Nature Reviews Neuroscience. 1122010</p>
<p>The free energy principle made simpler but not too simple. K Friston, L Da Costa, N Sajid, C Heins, K Ueltzhöffer, G A Pavliotis, T Parr, Physics Reports. 10242023</p>
<p>C Gan, J Schwartz, S Alter, D Mrowca, M Schrimpf, J Traer, J De Freitas, J Kubilius, A Bhandwaldar, N Haber, arXiv:2007.04954A platform for interactive multi-modal physical simulation. 2020arXiv preprint</p>
<p>Vrkitchen: an interactive 3d virtual environment for task-oriented learning. X Gao, R Gong, T Shu, X Xie, S Wang, S.-C Zhu, arXiv:1903.057572019arXiv preprint</p>
<p>Retrievalaugmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, Q Guo, M Wang, H Wang, 2024</p>
<p>The Ecological Approach to Visual Perception. J J Gibson, 1979Houghton Mifflin</p>
<p>Generalized feature embedding for supervised, unsupervised, and online learning tasks. E Golinko, X Zhu, 10.1007/s10796-018-9850-yInformation Systems Frontiers. 212019</p>
<p>Movement-produced stimulation in the development of visually guided behavior. R Held, A Hein, 196356872Journal of comparative and physiological psychology</p>
<p>Deep reinforcement learning that matters. P Henderson, R Islam, P Bachman, J Pineau, D Precup, D Meger, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Generalized Teacher Forcing for Learning Chaotic Dynamics. F Hess, Z Monfared, M Brenner, D Durstewitz, arXiv:2306.04406October 2023</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, 2023</p>
<p>Algorithmic amplification of politics on twitter. F Huszár, S I Ktena, C O'brien, L Belli, A Schlaikjer, M Hardt, 10.1073/pnas.2025334119Proceedings of the National Academy of Sciences. 11912022</p>
<p>How should control and body systems be coupled? a robotic case study. A Ishiguro, T Kawakatsu, 10.1007/978-3-540-27833-7_8Embodied Artificial Intelligence: International Seminar. Dagstuhl Castle, GermanySpringerJuly 7-11, 2003. 2004Revised Papers</p>
<p>Ai alignment: A comprehensive survey. J Ji, T Qiu, B Chen, B Zhang, H Lou, K Wang, Y Duan, Z He, J Zhou, Z Zhang, arXiv:2310.198522023arXiv preprint</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>K Kawaguchi, L P Kaelbling, Y Bengio, arXiv:1710.05468Generalization in deep learning. 20171arXiv preprint</p>
<p>Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?. B Kégl, G Hurtado, A Thomas, arXiv:2107.115872021arXiv preprint</p>
<p>Measuring catastrophic forgetting in neural networks. R Kemker, M Mcclure, A Abitino, T Hayes, C Kanan, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>The Markov blankets of life: autonomy, active inference and the free energy principle. M Kirchhoff, T Parr, E Palacios, K Friston, J Kiverstein, 10.1098/rsif.2017.0792Journal of The Royal Society interface. 1513820170792. 2018</p>
<p>Overcoming catastrophic forgetting in neural networks. J Kirkpatrick, R Pascanu, N Rabinowitz, J Veness, G Desjardins, A A Rusu, K Milan, J Quan, T Ramalho, A Grabska-Barwinska, Proceedings of the National Academy of Sciences. 114132017</p>
<p>The transferability approach: Crossing the reality gap in evolutionary robotics. S Koos, J.-B Mouret, S Doncieux, IEEE Transactions on Evolutionary Computation. 1712012</p>
<p>The purpose of qualia: What if human thinking is not (only) information processing. M Korth, arXiv:2212.008002022arXiv preprint</p>
<p>40 Years of Cognitive Architectures: Core Cognitive Abilities and Practical Applications. I Kotseruba, J K Tsotsos, 10.1007/s10462-018-9646-ySpringer Netherlands. 532020</p>
<p>Building machines that learn and think like people. B M Lake, T D Ullman, J B Tenenbaum, S J Gershman, Behavioral and brain sciences. 40e2532017</p>
<p>Metaphors we live by. University of Chicago press. G Lakoff, M Johnson, 1979</p>
<p>Philosophy in the flesh : the embodied mind and its challenge to western thought. G Lakoff, M L Johnson, 1999</p>
<p>Illustrating reinforcement learning from human feedback (rlhf). N Lambert, L Castricato, L Von Werra, A Havrilla, 2022Hugging Face Blog</p>
<p>The epoch-greedy algorithm for contextual multi-armed bandits. J Langford, T Zhang, Advances in Neural Information Processing Systems. 200820</p>
<p>Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks. M A Lee, Y Zhu, K Srinivasan, P Shah, S Savarese, L Fei-Fei, A Garg, J Bohg, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Technological approach to mind everywhere: an experimentally-grounded framework for understanding diverse bodies and minds. M Levin, 10.3389/fnsys.2022.768201Frontiers in systems neuroscience. 202216768201</p>
<p>igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. C Li, F Xia, R Martín-Martín, M Lingelbach, S Srivastava, B Shen, K Vainio, C Gokmen, G Dharan, T Jain, arXiv:2108.032722021arXiv preprint</p>
<p>A deeper look at facial expression dataset bias. S Li, W Deng, IEEE Transactions on Affective Computing. 1322020</p>
<p>Ascend: a scalable and unified architecture for ubiquitous deep neural network computing: Industry track paper. H Liao, J Tu, J Xia, H Liu, X Zhou, H Yuan, Y Hu, 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE2021</p>
<p>Simulation-only experiments to mimic the effects of the reality gap in the automatic design of robot swarms. A Ligot, M Birattari, 10.1007/s11721-019-00175-wSwarm Intelligence. 1412020</p>
<p>Model-based reinforcement learning with multi-step plan value estimation. H Lin, Y Sun, J Zhang, Y Yu, arXiv:2209.055302022arXiv preprint</p>
<p>An essay concerning human understanding. J Locke, Kay &amp; Troutman, 1847</p>
<p>A Brief Guide to Embodied Cognition: Why You Are Not Your Brain. S Mcnearney, 2011</p>
<p>Bayesian Approach to Global Optimization: Theory and Applications. J Mockus, 10.1007/978-94-009-0909-0978-94-009-0909-01989Kluwer Academic Publishers</p>
<p>Exploring the filter bubble: the effect of using recommender systems on content diversity. T T Nguyen, P.-M Hui, F M Harper, L Terveen, J A Konstan, 10.1145/2566486.2568012Proceedings of the 23rd international conference on World wide web. the 23rd international conference on World wide web2014</p>
<p>The design process for google's training chips: Tpuv2 and tpuv3. T Norrie, N Patil, D H Yoon, G Kurian, S Li, J Laudon, C Young, N Jouppi, D Patterson, IEEE Micro. 4122021</p>
<p>Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. C O'neil, 2016Crown Publishing Group</p>
<p>Doing Data Science. C O'neil, R Schutt, 2013O'Reilly Media, Inc</p>
<p>. M Oquab, T Darcet, T Moutakanni, H V Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, R Howes, P.-Y Huang, H Xu, V Sharma, S.-W Li, W Galuba, M Rabbat, M Assran, N Ballas, G Synnaeve, I Misra, H Jegou, J Mairal, P Labatut, A Joulin, Bojanowski, 2023P. Di-nov2: Learning robust visual features without supervision</p>
<p>Don't stop the training: continuously-updating self-supervised algorithms best account for auditory responses in the cortex. P Orhan, Y Boubenec, J.-R King, arXiv:2202.072902022arXiv preprint</p>
<p>Mortal computation: A foundation for biomimetic intelligence. A Ororbia, K Friston, 2024</p>
<p>What is intrinsic motivation? a typology of computational approaches. P.-Y Oudeyer, F Kaplan, 10.3389/neuro.12.006.2007Frontiers in neurorobotics. 162007. 2007</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2022</p>
<p>A survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on knowledge and data engineering. 22102010</p>
<p>Guided safe shooting: model based reinforcement learning with safety constraints. G Paolo, J Gonzalez-Billandon, A Thomas, B Kégl, arXiv:2206.097432022arXiv preprint</p>
<p>. L Parcalabescu, N Trost, A Frank, arXiv:2103.063042021What is multimodality? arXiv preprint</p>
<p>Continual lifelong learning with neural networks: A review. G I Parisi, R Kemker, J L Part, C Kanan, S Wermter, Neural Networks. 1132019</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International conference on machine learning. 2017</p>
<p>Robust Deep Reinforcement Learning with Adversarial Attacks. A Pattanaik, Z Tang, S Liu, G Bommannan, G Chowdhary, 2017</p>
<p>Artificial Empathy -A Roadmap for Human Aligned Artificial General Intelligence. C Perez, 2023Publisher</p>
<p>Machine learning for targeted display advertising: transfer learning in action. C Perlich, B Dalessandro, T Raeder, O Stitelman, F Provost, Machine learning. Springer201495</p>
<p>How the Body Shapes the Way We Think: A New View of Intelligence. R Pfeifer, J Bongard, 2006MIT Press</p>
<p>Embodied artificial intelligence: Trends and challenges. Lecture notes in computer science. R Pfeifer, F Iida, 10.1007/978-3-540-27833-7_12004</p>
<p>A review of generalized zero-shot learning methods. IEEE transactions on pattern analysis and machine intelligence. F Pourpanah, M Abdar, Y Luo, X Zhou, R Wang, C P Lim, X.-Z Wang, Q J Wu, 202245</p>
<p>Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking. F Provost, T Fawcett, 2013O'Reilly Media, Inc</p>
<p>Simulating household activities via programs. X Puig, K Ra, M Boben, J Li, T Wang, S Fidler, A Torralba, Virtualhome, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>J Quiñonero-Candela, M Sugiyama, A Schwaighofer, Lawrence , Dataset Shift in Machine Learning. N D , The MIT Press2009</p>
<p>Robust speech recognition via largescale weak supervision. A Radford, J W Kim, T Xu, G Brockman, C Mcleavey, I Sutskever, 2022</p>
<p>An exploration of embodied visual exploration. International Journal of Computer Vision. S K Ramakrishnan, D Jayaraman, K Grauman, 2021129</p>
<p>Hierarchical text-conditional image generation with clip latents. A Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, 2022</p>
<p>The development of attentional biases for faces in infancy: A developmental systems perspective. G D Reynolds, K C Roth, 10.3389/fpsyg.2018.00222Frontiers in psychology. 92222018</p>
<p>Auditing radicalization pathways on youtube. M H Ribeiro, R Ottoni, R West, V A Almeida, Meira Jr, W , Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. the 2020 Conference on Fairness, Accountability, and Transparency2020</p>
<p>How organisms come to know the world: fundamental limits on artificial general intelligence. A Roli, J Jaeger, S A Kauffman, 10.3389/fevo.2021.806283Frontiers in Ecology and Evolution. 910352022</p>
<p>Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data. F E Rosas, P A Mediano, H J Jensen, A K Seth, A B Barrett, R L Carhart-Harris, D Bor, https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008289PLoS computational biology. 1612e10082892020</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Conference on Robot Learning. PMLR2022</p>
<p>Human-compatible artificial intelligence. S Russell, </p>
<p>Human-like machine intelligence. 2021</p>
<p>Crossing the reality gap: A survey on sim-to-real transferability of robot controllers in reinforcement learning. E Salvato, G Fenu, E Medvet, F A Pellegrino, IEEE Access. 92021</p>
<p>Addiction by Design: Machine Gambling in Las Vegas. N D Schüll, 2012Princeton University Press</p>
<p>Fairness and abstraction in sociotechnical systems. A D Selbst, D Boyd, S A Friedler, S Venkatasubramanian, J Vertesi, ACM Conference on Fairness, Accountability, and Transparency (FAT*). 2019</p>
<p>Embodied Cognition. L Shapiro, 2011Routledge</p>
<p>Computer vision: the last fifty years. University of Washington, Last access. L G Shapiro, 72021</p>
<p>T Shen, R Jin, Y Huang, C Liu, W Dong, Z Guo, X Wu, Y Liu, D Xiong, arXiv:2309.15025Large language model alignment: A survey. 2023arXiv preprint</p>
<p>Cyberphysical Smart Cities Infrastructures: Optimal Operation and Intelligent Decision Making. F Shenavarmasouleh, F G Mohammadi, M H Amini, Reza Arabnia, H , 2022Embodied ai-driven operation of smart cities: A concise review</p>
<p>Skill-based model-based reinforcement learning. L X Shi, J J Lim, Y Lee, arXiv:2207.075602022arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 52975872016</p>
<p>Emblaze: Illuminating machine learning representations through interactive comparison of embedding spaces. V Sivaraman, Y Wu, A Perer, 27th International Conference on Intelligent User Interfaces. 2022</p>
<p>The development of embodied cognition: Six lessons from babies. L Smith, M Gasser, Artificial life. 111-22005</p>
<p>The hard problem of consciousness and the free energy principle. M Solms, Frontiers in Psychology. 2019</p>
<p>Energy and policy considerations for deep learning in NLP. E Strubell, A Ganesh, A Mccallum, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Reinforcement Learning: An Introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Cognitive architectures. The Cambridge handbook of cognitive science. P Thagard, 20123</p>
<p>. S Thrun, W Burgard, D Probabilistic Fox, Robotics, 2005MIT Press</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Statistical Learning Theory. V Vapnik, 1998Wiley</p>
<p>The Embodied Mind: Cognitive Science and Human Experience. F J Varela, E Thompson, E Rosch, 1991MIT Press</p>
<p>Removing biased data to improve fairness and accuracy. S Verma, M Ernst, R Just, arXiv:2102.030542021arXiv preprint</p>
<p>Relevance realization and the emerging framework in cognitive science. J Vervaeke, S Coyne, J Vervaeke, T P Lillicrap, B A Richards, Journal of Logic and Computation. 2212024. 2012Hackett PublishingMentoring the Machines</p>
<p>A comprehensive survey of continual learning: Theory, method and application. L Wang, X Zhang, H Su, J Zhu, arXiv:2302.004872023aarXiv preprint</p>
<p>Aligning large language models with human: A survey. Y Wang, W Zhong, L Li, F Mi, X Zeng, W Huang, L Shang, X Jiang, Q Liu, arXiv:2307.129662023barXiv preprint</p>
<p>J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Social learning and the brain: How do we learn from and about other people? Everything You and Your Teachers Need to Know About the Learning Brain. B Westho, I J Koele, I H Van De Groep, 10.3389/frym.2020.00095202042</p>
<p>Quantization noise: roundoff error in digital computation, signal processing, control, and communications. B Widrow, I Kollár, 2008Cambridge University Press</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, R Zheng, X Fan, X Wang, L Xiong, Y Zhou, W Wang, C Jiang, Y Zou, X Liu, Z Yin, S Dou, R Weng, W Cheng, Q Zhang, W Qin, Y Zheng, X Qiu, X Huang, Gui , T , The rise and potential of large language model based agents: A survey. 2023</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. M Xiong, Z Hu, X Lu, Y Li, J Fu, J He, B Hooi, 2024</p>
<p>C Yan, D Misra, A Bennnett, A Walsman, Y Bisk, Y Artzi, Chalet, arXiv:1801.07357Cornell house agent learning environment. 2018arXiv preprint</p>
<p>Continual learning in an easy-to-hard manner. C Yifan, C Yulu, Z Yadan, L Wenbo, 10.1007/s10489-023-04454-22023Applied Intelligence</p>
<p>HAZARD challenge: Embodied decision making in dynamically changing environments. Q Zhou, S Chen, Y Wang, H Xu, W Du, H Zhang, Y Du, J B Tenenbaum, C Gan, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
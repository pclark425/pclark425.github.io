<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7529 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7529</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7529</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-3f2cb353c7528efafb847309ab1e1e95245740a4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3f2cb353c7528efafb847309ab1e1e95245740a4" target="_blank">Mathematical Capabilities of ChatGPT</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is found that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a Mathematical search engine and knowledge base interface, and GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty.</p>
                <p><strong>Paper Abstract:</strong> We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a mathematical search engine and knowledge base interface. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if your goal is to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7529.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7529.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-9Jan2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (9-January-2023 web interface version)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned GPT-family dialogue model evaluated as a text-based simulator for a wide range of mathematical subdomains (undergraduate to graduate level), via the GHOSTS dataset; assessed on tasks including computational problems, proof completion, symbolic integration, definition retrieval and theorem-outline generation using human expert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical Capabilities of ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (9-January-2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (InstructGPT-style, RLHF), web GUI</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (functional analysis, topology, probability, algebra, symbolic integration, olympiad problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of mathematician activities: answer mathematical queries, solve computational problems, complete proofs with missing steps, produce proof outlines for named theorems, perform symbolic integration, retrieve/identify definitions and mathematical objects.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Primarily zero‑shot single prompts via web GUI (new chat per prompt); prompts often encoded in LaTeX; a subset received simple prompt engineering (e.g., 'Prove that...', explicit step‑by‑step instruction) for some olympiad problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert rating on a 1–5 scale (1 failure to understand, 5 perfect); supplemental error/warning codes (e1–e6, w1–w5); percent perfect / average rating and standard deviation reported per subdataset/file.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average rating 3.20 (std 1.23) across GHOSTS; on sampled Symbolic-Integration subset average rating 2.51 (std 0.87); perfect-score rate across MATH random samples ≈29% (paper-specified sample).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to task-specific models: Lample & Charton (symbolic integration) ~≈100% on their integrals; Minerva (PaLM variant) ~50% on MATH dataset; generic PaLM on GSM8K ~58% (chain-of-thought + calculator).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['mathematical difficulty level (M1–M4) — accuracy drops with difficulty', 'question type (Q1–Q5) — proof and olympiad-style tasks harder', 'out-of-distribution formatting/encoding (D1 nontrivial encodings like Polish notation reduce accuracy)', 'prompt engineering (step-by-step) — reduces computation errors but can increase logical-flow (e5) errors', 'model version (different ChatGPT snapshots show shuffled outputs)', 'TeX vs Unicode encoding (models process TeX well, some sensitivity noted)', 'lack of control over API settings (temperature etc.) when using GUI', 'length of prompt had no clear effect']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Accessed via OpenAI web GUI (chat.openai.com), new session per prompt, prompts often in LaTeX, no control or disclosure of temperature/presence_penalty; dataset GHOSTS: 709 prompts, human evaluation (1636 ratings) by mathematicians; passing threshold set informally at rating 3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Inconsistent performance; frequent arithmetic/computation mistakes, wrong logical inferences (e5_*), missing proof steps, hallucinations, overconfident assertions even when wrong; poor on graduate-level proofs and olympiad problems; dominated by specialized models on narrow tasks (e.g., symbolic integrators).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Capabilities of ChatGPT', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7529.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7529.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-30Jan2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (30-January-2023 web interface version)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Later ChatGPT snapshot evaluated on the same GHOSTS benchmark; shows broadly similar behavior to the earlier snapshot with slight average improvements and some instability (shuffling) in which specific prompts improved or worsened.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical Capabilities of ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (30-January-2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (InstructGPT-style, RLHF), web GUI</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (same subdomains as GHOSTS)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Same set of text-based mathematical simulation tasks as for the 9-Jan snapshot (computations, proof completion, theorem outlining, symbolic integrals, definition retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same as 9-Jan version: zero-shot prompts via GUI, LaTeX encoding; some prompts prompt‑engineered in subset (step‑by‑step).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert rating 1–5 scale with error/warning codes; average rating and per-file averages reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average rating 3.29 (std 1.28) across GHOSTS (paper reports similar overall performance to 9-Jan but with prompt-level shuffling).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared qualitatively to specialized models and to prior ChatGPT snapshot (9-Jan); same external baselines: Lample & Charton ~100% on their integrals; Minerva ~50% on MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model snapshot/versioning (upgrades produced mixed/shuffled changes rather than uniform improvement)', 'task difficulty and type', 'prompt engineering effects', 'out-of-distribution encodings']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>GUI web interface, new chat per prompt, timestamps recorded to identify model version, same human expert evaluation procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Similar to 9-Jan: inconsistent errors, arithmetic mistakes, logical gaps; improvement is mixed and not uniform across prompts; many outputs still wrong or incomplete for advanced tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Capabilities of ChatGPT', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7529.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7529.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (miniGHOSTS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (evaluated via miniGHOSTS subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-4 evaluated on a curated 170-prompt subset (miniGHOSTS) of the GHOSTS benchmark; shows substantial improvement over ChatGPT snapshots for undergraduate-level mathematics and many subdatasets, but still fails on hardest graduate and olympiad problems and can make basic calculation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical Capabilities of ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned (GPT-4 lineage), web GUI (evaluated on miniGHOSTS)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (same GHOSTS subdomains: symbolic integration, MATH problems, graduate text exercises, proof completion, search-engine aspects)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation: solve/answer computational math problems, complete proofs, produce proof outlines, perform symbolic integration, and retrieve mathematical definitions/objects.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero‑shot single prompts via web GUI; miniGHOSTS selection included some prompt‑engineered olympiad prompts but generally the same prompting format as for ChatGPT evaluations (LaTeX prompts, new session per prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert rating 1–5 scale (same scheme), average ratings and fraction of perfect scores reported; std deviations provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average rating 4.15 (std 1.12) on miniGHOSTS overall; Symbolic-Integration subset (miniGHOSTS) average rating 3.50 (std 1.43); on sampled MATH prompts GPT-4 scored rating 5 on ~70% of corresponding miniGHOSTS questions.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to specialized models: still trails specialized symbolic integration model (Lample & Charton ~100% on their integrals); outperforms ChatGPT snapshots (3.20–3.29 average). Compared to Minerva (~50% on MATH), GPT-4 substantially better on miniGHOSTS MATH subset (70% perfect in sample).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model capability/version (GPT-4 substantially better than ChatGPT snapshots)', 'task difficulty (still fails graduate-level and olympiad tasks)', 'rambling/verbosity (GPT-4 produces longer answers which sometimes help but can reduce readability)', 'overconfidence and residual simple arithmetic errors', 'prompt engineering included for some items (mixed effects)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated via web GUI (limited API access at time), on miniGHOSTS (170 prompts selected to match mean/std of full files), new session per prompt, human expert ratings; GUI-specific unknown sampling/temperature settings (not controlled).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although improved, GPT-4 remains vulnerable to simple computational mistakes and logical gaps; can produce long rambling outputs that reduce readability; fails on hard graduate-level proofs and many olympiad problems; GUI evaluation prevents controlling sampling/temperature parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Capabilities of ChatGPT', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7529.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7529.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minerva (PaLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minerva (PaLM-based model specialized for math)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PaLM-based model trained/finetuned on mathematical sources (MathJax and arXiv) reported in related work as a strong baseline on the MATH dataset (~50% on MATH), mentioned for comparative context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving quantitative reasoning problems with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Minerva (PaLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B (PaLM variant reported in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base PaLM family finetuned/trained with math-focused data (MathJax/arXiv)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (MATH dataset — competition-style problems)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve competition-style mathematical problems (MATH dataset) with chain-of-thought style reasoning and specialized training data.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-thought style prompting and training on math-specific sources (as reported in related work); possibly uses few-shot and CoT during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (exact answer match) on MATH dataset problems; automatically evaluated where unique short answers exist.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported ≈50% accuracy on MATH dataset (in cited related work).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared in paper to ChatGPT and GPT-4 performance: ChatGPT (9-Jan) achieved perfect answers on 29% of sampled MATH prompts; GPT-4 achieved 70% perfect in miniGHOSTS sample.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['training data curated for mathematics (MathJax and arXiv)', 'use of chain-of-thought and external calculators in evaluation (in PaLM/Minerva reports)', 'model scale (large PaLM variant)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Described in related work [Minerva], not directly re-run in this paper; cited for comparative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Not detailed in this paper beyond the comparative accuracy figure; cited as a specialized model that outperforms general-purpose ChatGPT on some math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Capabilities of ChatGPT', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7529.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7529.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LampleCharton-2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep learning for symbolic mathematics (Transformer for symbolic integration / differential equations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based supervised approach for symbolic mathematics (integrals and differential equations) that outperforms classical CAS baselines on a test set of integrals; cited as a state-of-the-art specialized model for symbolic integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning for symbolic mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based symbolic mathematics model (Lample & Charton)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>supervised Transformer trained on synthetic symbolic math datasets</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Symbolic mathematics (integration, differential equations)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate closed-form symbolic solutions (antiderivatives, ODE solutions) given symbolic inputs (Polish/prefix and other encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Supervised training on large synthetic datasets; evaluation uses exact-match checking of generated symbolic expressions against ground truth (Polish notation etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy / exact match on symbolic integration and differential equation test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported nearly 100% on a collection of 500 test integrals in the cited work; outperformed classical CAS by ≥14% on integrals in their test.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Classical computer algebra systems (Mathematica, MATLAB, Maple) used as baselines in cited work; Transformer approach outperformed them on the test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['synthetic training data generation approach (fwd/bwd/IBP generation)', 'model architecture and supervised objective tuned for symbolic tasks', 'problem encoding (Polish/prefix notation used in training/test)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Reported in Lample & Charton (2019); in this paper used as a baseline contrast for ChatGPT and GPT-4 performance on Symbolic-Integration subdataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Not exhaustively detailed in this paper; cited model is specialized and lacks generality of LLMs but excels on narrowly defined symbolic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Capabilities of ChatGPT', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep learning for symbolic mathematics <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Palm: Scaling language modeling with pathways <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7529",
    "paper_id": "paper-3f2cb353c7528efafb847309ab1e1e95245740a4",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "ChatGPT-9Jan2023",
            "name_full": "ChatGPT (9-January-2023 web interface version)",
            "brief_description": "Instruction-tuned GPT-family dialogue model evaluated as a text-based simulator for a wide range of mathematical subdomains (undergraduate to graduate level), via the GHOSTS dataset; assessed on tasks including computational problems, proof completion, symbolic integration, definition retrieval and theorem-outline generation using human expert ratings.",
            "citation_title": "Mathematical Capabilities of ChatGPT",
            "mention_or_use": "use",
            "model_name": "ChatGPT (9-January-2023)",
            "model_size": null,
            "model_type": "instruction-tuned (InstructGPT-style, RLHF), web GUI",
            "scientific_domain": "Mathematics (functional analysis, topology, probability, algebra, symbolic integration, olympiad problem solving)",
            "simulation_task_description": "Text-based simulation of mathematician activities: answer mathematical queries, solve computational problems, complete proofs with missing steps, produce proof outlines for named theorems, perform symbolic integration, retrieve/identify definitions and mathematical objects.",
            "prompting_strategy": "Primarily zero‑shot single prompts via web GUI (new chat per prompt); prompts often encoded in LaTeX; a subset received simple prompt engineering (e.g., 'Prove that...', explicit step‑by‑step instruction) for some olympiad problems.",
            "evaluation_metric": "Human expert rating on a 1–5 scale (1 failure to understand, 5 perfect); supplemental error/warning codes (e1–e6, w1–w5); percent perfect / average rating and standard deviation reported per subdataset/file.",
            "reported_accuracy": "Average rating 3.20 (std 1.23) across GHOSTS; on sampled Symbolic-Integration subset average rating 2.51 (std 0.87); perfect-score rate across MATH random samples ≈29% (paper-specified sample).",
            "baseline_accuracy": "Compared to task-specific models: Lample & Charton (symbolic integration) ~≈100% on their integrals; Minerva (PaLM variant) ~50% on MATH dataset; generic PaLM on GSM8K ~58% (chain-of-thought + calculator).",
            "factors_reported": [
                "mathematical difficulty level (M1–M4) — accuracy drops with difficulty",
                "question type (Q1–Q5) — proof and olympiad-style tasks harder",
                "out-of-distribution formatting/encoding (D1 nontrivial encodings like Polish notation reduce accuracy)",
                "prompt engineering (step-by-step) — reduces computation errors but can increase logical-flow (e5) errors",
                "model version (different ChatGPT snapshots show shuffled outputs)",
                "TeX vs Unicode encoding (models process TeX well, some sensitivity noted)",
                "lack of control over API settings (temperature etc.) when using GUI",
                "length of prompt had no clear effect"
            ],
            "experimental_conditions": "Accessed via OpenAI web GUI (chat.openai.com), new session per prompt, prompts often in LaTeX, no control or disclosure of temperature/presence_penalty; dataset GHOSTS: 709 prompts, human evaluation (1636 ratings) by mathematicians; passing threshold set informally at rating 3.5.",
            "limitations_or_failure_modes": "Inconsistent performance; frequent arithmetic/computation mistakes, wrong logical inferences (e5_*), missing proof steps, hallucinations, overconfident assertions even when wrong; poor on graduate-level proofs and olympiad problems; dominated by specialized models on narrow tasks (e.g., symbolic integrators).",
            "uuid": "e7529.0",
            "source_info": {
                "paper_title": "Mathematical Capabilities of ChatGPT",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "ChatGPT-30Jan2023",
            "name_full": "ChatGPT (30-January-2023 web interface version)",
            "brief_description": "Later ChatGPT snapshot evaluated on the same GHOSTS benchmark; shows broadly similar behavior to the earlier snapshot with slight average improvements and some instability (shuffling) in which specific prompts improved or worsened.",
            "citation_title": "Mathematical Capabilities of ChatGPT",
            "mention_or_use": "use",
            "model_name": "ChatGPT (30-January-2023)",
            "model_size": null,
            "model_type": "instruction-tuned (InstructGPT-style, RLHF), web GUI",
            "scientific_domain": "Mathematics (same subdomains as GHOSTS)",
            "simulation_task_description": "Same set of text-based mathematical simulation tasks as for the 9-Jan snapshot (computations, proof completion, theorem outlining, symbolic integrals, definition retrieval).",
            "prompting_strategy": "Same as 9-Jan version: zero-shot prompts via GUI, LaTeX encoding; some prompts prompt‑engineered in subset (step‑by‑step).",
            "evaluation_metric": "Human expert rating 1–5 scale with error/warning codes; average rating and per-file averages reported.",
            "reported_accuracy": "Average rating 3.29 (std 1.28) across GHOSTS (paper reports similar overall performance to 9-Jan but with prompt-level shuffling).",
            "baseline_accuracy": "Compared qualitatively to specialized models and to prior ChatGPT snapshot (9-Jan); same external baselines: Lample & Charton ~100% on their integrals; Minerva ~50% on MATH.",
            "factors_reported": [
                "model snapshot/versioning (upgrades produced mixed/shuffled changes rather than uniform improvement)",
                "task difficulty and type",
                "prompt engineering effects",
                "out-of-distribution encodings"
            ],
            "experimental_conditions": "GUI web interface, new chat per prompt, timestamps recorded to identify model version, same human expert evaluation procedure.",
            "limitations_or_failure_modes": "Similar to 9-Jan: inconsistent errors, arithmetic mistakes, logical gaps; improvement is mixed and not uniform across prompts; many outputs still wrong or incomplete for advanced tasks.",
            "uuid": "e7529.1",
            "source_info": {
                "paper_title": "Mathematical Capabilities of ChatGPT",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "GPT-4 (miniGHOSTS)",
            "name_full": "GPT-4 (evaluated via miniGHOSTS subset)",
            "brief_description": "OpenAI GPT-4 evaluated on a curated 170-prompt subset (miniGHOSTS) of the GHOSTS benchmark; shows substantial improvement over ChatGPT snapshots for undergraduate-level mathematics and many subdatasets, but still fails on hardest graduate and olympiad problems and can make basic calculation errors.",
            "citation_title": "Mathematical Capabilities of ChatGPT",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "instruction-tuned (GPT-4 lineage), web GUI (evaluated on miniGHOSTS)",
            "scientific_domain": "Mathematics (same GHOSTS subdomains: symbolic integration, MATH problems, graduate text exercises, proof completion, search-engine aspects)",
            "simulation_task_description": "Text-based simulation: solve/answer computational math problems, complete proofs, produce proof outlines, perform symbolic integration, and retrieve mathematical definitions/objects.",
            "prompting_strategy": "Zero‑shot single prompts via web GUI; miniGHOSTS selection included some prompt‑engineered olympiad prompts but generally the same prompting format as for ChatGPT evaluations (LaTeX prompts, new session per prompt).",
            "evaluation_metric": "Human expert rating 1–5 scale (same scheme), average ratings and fraction of perfect scores reported; std deviations provided.",
            "reported_accuracy": "Average rating 4.15 (std 1.12) on miniGHOSTS overall; Symbolic-Integration subset (miniGHOSTS) average rating 3.50 (std 1.43); on sampled MATH prompts GPT-4 scored rating 5 on ~70% of corresponding miniGHOSTS questions.",
            "baseline_accuracy": "Compared to specialized models: still trails specialized symbolic integration model (Lample & Charton ~100% on their integrals); outperforms ChatGPT snapshots (3.20–3.29 average). Compared to Minerva (~50% on MATH), GPT-4 substantially better on miniGHOSTS MATH subset (70% perfect in sample).",
            "factors_reported": [
                "model capability/version (GPT-4 substantially better than ChatGPT snapshots)",
                "task difficulty (still fails graduate-level and olympiad tasks)",
                "rambling/verbosity (GPT-4 produces longer answers which sometimes help but can reduce readability)",
                "overconfidence and residual simple arithmetic errors",
                "prompt engineering included for some items (mixed effects)"
            ],
            "experimental_conditions": "Evaluated via web GUI (limited API access at time), on miniGHOSTS (170 prompts selected to match mean/std of full files), new session per prompt, human expert ratings; GUI-specific unknown sampling/temperature settings (not controlled).",
            "limitations_or_failure_modes": "Although improved, GPT-4 remains vulnerable to simple computational mistakes and logical gaps; can produce long rambling outputs that reduce readability; fails on hard graduate-level proofs and many olympiad problems; GUI evaluation prevents controlling sampling/temperature parameters.",
            "uuid": "e7529.2",
            "source_info": {
                "paper_title": "Mathematical Capabilities of ChatGPT",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Minerva (PaLM-based)",
            "name_full": "Minerva (PaLM-based model specialized for math)",
            "brief_description": "A PaLM-based model trained/finetuned on mathematical sources (MathJax and arXiv) reported in related work as a strong baseline on the MATH dataset (~50% on MATH), mentioned for comparative context.",
            "citation_title": "Solving quantitative reasoning problems with language models",
            "mention_or_use": "mention",
            "model_name": "Minerva (PaLM-based)",
            "model_size": "540B (PaLM variant reported in related work)",
            "model_type": "base PaLM family finetuned/trained with math-focused data (MathJax/arXiv)",
            "scientific_domain": "Mathematics (MATH dataset — competition-style problems)",
            "simulation_task_description": "Solve competition-style mathematical problems (MATH dataset) with chain-of-thought style reasoning and specialized training data.",
            "prompting_strategy": "Chain-of-thought style prompting and training on math-specific sources (as reported in related work); possibly uses few-shot and CoT during evaluation.",
            "evaluation_metric": "Accuracy (exact answer match) on MATH dataset problems; automatically evaluated where unique short answers exist.",
            "reported_accuracy": "Reported ≈50% accuracy on MATH dataset (in cited related work).",
            "baseline_accuracy": "Compared in paper to ChatGPT and GPT-4 performance: ChatGPT (9-Jan) achieved perfect answers on 29% of sampled MATH prompts; GPT-4 achieved 70% perfect in miniGHOSTS sample.",
            "factors_reported": [
                "training data curated for mathematics (MathJax and arXiv)",
                "use of chain-of-thought and external calculators in evaluation (in PaLM/Minerva reports)",
                "model scale (large PaLM variant)"
            ],
            "experimental_conditions": "Described in related work [Minerva], not directly re-run in this paper; cited for comparative performance.",
            "limitations_or_failure_modes": "Not detailed in this paper beyond the comparative accuracy figure; cited as a specialized model that outperforms general-purpose ChatGPT on some math tasks.",
            "uuid": "e7529.3",
            "source_info": {
                "paper_title": "Mathematical Capabilities of ChatGPT",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "LampleCharton-2019",
            "name_full": "Deep learning for symbolic mathematics (Transformer for symbolic integration / differential equations)",
            "brief_description": "Transformer-based supervised approach for symbolic mathematics (integrals and differential equations) that outperforms classical CAS baselines on a test set of integrals; cited as a state-of-the-art specialized model for symbolic integration.",
            "citation_title": "Deep learning for symbolic mathematics",
            "mention_or_use": "mention",
            "model_name": "Transformer-based symbolic mathematics model (Lample & Charton)",
            "model_size": null,
            "model_type": "supervised Transformer trained on synthetic symbolic math datasets",
            "scientific_domain": "Symbolic mathematics (integration, differential equations)",
            "simulation_task_description": "Generate closed-form symbolic solutions (antiderivatives, ODE solutions) given symbolic inputs (Polish/prefix and other encodings).",
            "prompting_strategy": "Supervised training on large synthetic datasets; evaluation uses exact-match checking of generated symbolic expressions against ground truth (Polish notation etc.).",
            "evaluation_metric": "Accuracy / exact match on symbolic integration and differential equation test sets.",
            "reported_accuracy": "Reported nearly 100% on a collection of 500 test integrals in the cited work; outperformed classical CAS by ≥14% on integrals in their test.",
            "baseline_accuracy": "Classical computer algebra systems (Mathematica, MATLAB, Maple) used as baselines in cited work; Transformer approach outperformed them on the test sets.",
            "factors_reported": [
                "synthetic training data generation approach (fwd/bwd/IBP generation)",
                "model architecture and supervised objective tuned for symbolic tasks",
                "problem encoding (Polish/prefix notation used in training/test)"
            ],
            "experimental_conditions": "Reported in Lample & Charton (2019); in this paper used as a baseline contrast for ChatGPT and GPT-4 performance on Symbolic-Integration subdataset.",
            "limitations_or_failure_modes": "Not exhaustively detailed in this paper; cited model is specialized and lacks generality of LLMs but excels on narrowly defined symbolic tasks.",
            "uuid": "e7529.4",
            "source_info": {
                "paper_title": "Mathematical Capabilities of ChatGPT",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep learning for symbolic mathematics",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Palm: Scaling language modeling with pathways",
            "rating": 1
        }
    ],
    "cost": 0.015353,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Mathematical Capabilities of ChatGPT</h1>
<p>Simon Frieder ${ }^{* 1,5}$, Luca Pinchetti ${ }^{1}$, Alexis Chevalier ${ }^{3}$, Ryan-Rhys Griffiths ${ }^{4}$, Tommaso Salvatori ${ }^{2,7}$, Thomas Lukasiewicz ${ }^{2,1}$, Philipp Christian Petersen ${ }^{5,6}$, and Julius Berner ${ }^{5}$<br>${ }^{1}$ Department of Computer Science, University of Oxford, Oxford, UK<br>${ }^{2}$ Institute of Logic and Computation, Vienna University of Technology, Vienna, Austria<br>${ }^{3}$ School of Mathematics, Institute for Advanced Study, Princeton, US<br>${ }^{4}$ Department of Physics, University of Cambridge, Cambridge, UK<br>${ }^{5}$ Faculty of Mathematics, University of Vienna, Vienna, Austria<br>${ }^{6}$ Research Network Data Science, University of Vienna, Vienna, Austria<br>${ }^{7}$ VERSES Research Lab, Los Angeles, CA 90016, USA</p>
<h4>Abstract</h4>
<p>We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT can be used most successfully as a mathematical assistant for querying facts, acting as a mathematical search engine and knowledge base interface. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if your goal is to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!</p>
<h2>1 Introduction</h2>
<p>Since its release in November 2022, the language model Chat Generative Pre-trained Transformer (ChatGPT) has rapidly become a widely known question-and-answer dialogue system. ChatGPT has been referenced in traditional media across the globe [1-3] and across all major internet platforms [4, 5]. With similar reactions, the release of ChatGPT's successor, GPT-4, followed in March 2023 [6].</p>
<p>The performance of ChatGPT has been analyzed in a large number of exam-related use cases, with varying degrees of scientific rigor, ranging from detailed studies to anecdotal evidence. Use cases include passing the United States Medical Licensing Examination (USMLE) [7], scoring highly on the Psychology Today Verbal-Linguistic Intelligence IQ Test [8], and answering (and generating) Operations Management exam questions that were deemed to be within the scope of a typical MBA curriculum [9], all with a performance that elicited a positive sense of surprise from the authors. In turn, the performance of GPT-4 even surpasses that of ChatGPT on a large batch of academic and professional exams [6, Table 1]. Such strong task-related performance indicates that large language models (LLMs) could be frequently used as assistants in many domains.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this article, we will focus on introducing a new dataset, called GHOSTS, which measures advanced mathematical abilities of LLMs. Using this dataset, we will perform a detailed analysis of the mathematical capabilities of ChatGPT on two of its versions, the 9-January-2023 version and the 30-January-2023 version. Note that, according to the release notes, the 30-January-2023 version should possess "improved factuality and mathematical capabilities" [10]. We further examine the performance of GPT-4 on a smaller dataset, called miniGHOSTS, which exhibits statistics similar to the larger GHOSTS dataset. Our analysis includes but is not limited to testing how many of the skills necessary to do professional mathematics can be emulated by these models. Examples of such skills are the ability to answer computational questions, the ability to complete mathematical proofs that have gaps or missing steps, the ability to solve questions that are more focused on deep insights and original solutions, such as those of mathematical olympiads, and the ability to survey the literature and think across domains. None of the previous benchmarks (see Section 2) cover such a broad range of mathematical abilities.</p>
<p>To achieve the goals outlined above, GHOSTS consists of carefully composed prompts aimed at testing different aspects of LLMs related to mathematical comprehension, see Section 3. This includes both hand-crafted prompts as well as samples from existing datasets that were devised to test models specifically trained for mathematical comprehension [11, 12].</p>
<p>For brevity, we will use the expression "(Chat)GPT" to refer collectively to both the ChatGPT and GPT-4 language models. We refer to Appendix C for further details on (Chat)GPT versions.</p>
<p>To evaluate the output of (Chat)GPT, we designed a thorough testing methodology, including warning and error codes that represent various possible failure modes of (Chat)GPT. We score (Chat)GPT's responses, report on the results using this methodology, and compare (Chat)GPT to a selection of state-of-the-art models trained for mathematical comprehension. In summary, the contributions of this article are threefold:</p>
<ul>
<li>Benchmark for testing the mathematical capabilities of LLMs: We introduce a new naturallanguage mathematics dataset, called GHOSTS ${ }^{1}$, to test the capabilities of LLMs across a range of aspects regarding advanced mathematical comprehension, see Section 3. It consists of two subdatasets derived from state-of-the-art datasets of mathematical queries for language models. Additionally, we devise four hand-crafted subdatasets covering further mathematical tasks. Parts of our dataset consist of problems that were selected to have a high probability of not being in the data on which (Chat)GPT was trained.</li>
<li>Insight for mathematical use of (Chat)GPT: Based on our benchmark, we show for which types of questions and which domains of mathematics, (Chat)GPT may be useful and how it could be integrated into the workflow of a mathematician. On the other hand, we identify the failure modes, as well as the limits of its capabilities. This can aid future efforts to develop LLMs that perform better in mathematics. Our analysis is akin to a mathematical model card, where the mathematical strengths and weaknesses are summarized, see Section 4.</li>
<li>Evaluation of improvements of (Chat)GPT: We can further use our benchmark to track the mathematical capabilities of (Chat)GPT variants over time. As a first step, we analyze the impact of the upgrade from the 9-January-2023 to the 30-January-2023 version of ChatGPT, which promises "improved factuality and mathematical capabilities". Then, we proceed to investigate what performance increases the successor GPT-4 brings; see Section 4.1.</li>
</ul>
<h1>2 Related Work</h1>
<p>As a language model, (Chat)GPT can be universally employed to perform mathematical reasoning and therefore has to compete with technologies in this space that are sometimes decades old. Performing mathematical reasoning in an automated way has a long history and can be traced back to 1959 [13], the most focus being devoted to proving theorems [14]. Presently, there is a realization that classical approaches, using a symbolic encoding of mathematics, have reached a plateau [15].</p>
<p>On the other hand, there is now a growing body of literature on learning mathematical relationships directly in a supervised-learning manner [16-18] or by using LLMs to perform mathematical reasoning directly on mathematics encoded in natural language [19]. Sometimes, the distinction is blurred because</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>architectures of LLMs can also be used in a supervised-learning setting and have been employed successfully in learning mathematical relationships [12, 20].</p>
<p>Among the supervised approaches, we mention [12], where a Transformer architecture [21] was used to generate symbolic, closed-form solutions to integrals and first and second-order differential equations, which outperformed classical solvers ${ }^{2}$, such as Mathematica, MATLAB, and Maple by at least $14 \%$ on a test set of integration problems. On the task of solving differential equations, the Transformer-based approach still exceeds the classical approach, but by a smaller margin (at least $4 \%$ in the case of first-order differential equations and with more varied results for second-order equations).</p>
<p>Recent LLMs, for instance, PaLM [22] (released in 2022), are tested only on elementary-level mathematical reasoning datasets, such as the MathQA or GSM8K datasets [23, 24]. We suspect that this is due to a lack of advanced-level natural language mathematics datasets. Moreover, the results obtained indicate that the models at that time had difficulty with much simpler datasets than ours. For example, the version of PaLM with 540 billion parameters only correctly solves $58 \%$ of the problems of the GSM8K dataset, even with chain-of-thought prompting and access to an external calculator [22, Table 10]. This model nonetheless outperforms GPT-3 [25], which only achieves $54 \%$ on the same dataset. Variations of BERT [26] have been shown to only solve between $28 \%$ and $37 \%$ of the problems when fine-tuned and tested on the Algebra Question Answering with Rationales (AQuA-RAT) dataset [27], which is the direct predecessor of MathQA. For some models, such as BLOOM [28] or the LaMDA model [29] (both released in 2022), an evaluation of the mathematical reasoning capability is entirely missing. An up-to-date survey on mathematical datasets and the performance of various LLMs can be found in [30].</p>
<p>Among the aforementioned LLMs, Minerva [19], based on PaLM, stands out, being trained in equal parts on websites that contain MathJax elements and arXiv preprints (additionally to general natural language data on which PaLM was trained). It achieves a score of roughly $50 \%$ on the significantly harder Mathematics Aptitude Test of Heuristics (MATH) dataset [11], which was sourced from various mathematical competitions. One distinguishing feature of the MATH dataset is that its problems admit a unique answer that can be condensed within a few characters (a number, for example). This is beneficial for the automatic evaluation of a model on such a dataset since one can simply check the final answer, ignoring the step-by-step solution.</p>
<p>Most similar to our dataset is the NaturalProofs dataset [31] and the NaturalProofs-Gen dataset [32]. In this paragraph, we illustrate the similarities and differences between these datasets and ours. NaturalProofs and NaturalProofs-Gen are similar among themselves and cover graduatelevel mathematics by focusing on data from ProofWiki ${ }^{3}$ (the latter dataset), as well as on the Stacks Project ${ }^{4}$ and two open-source textbooks (the former dataset). Using the LaTeX source code, which is available for all these resources, annotated theorems and their proof graphs are extracted. The annotations consist of reference graphs highlighting references to other theorems or definitions, the idea being that these references capture the "skeleton" of a proof. This task resembles the mathematical abilities that the Named Theorem Proof Completion subdataset from the GHOSTS dataset evaluates (see Table 1), although 1) we only retrieve a single reference and 2) (Chat)GPT, as far as known, does not use training objectives that make use of information from data annotation, in contrast to models evaluated in [31, 32]. Our framework pertains to general language model evaluation, which may be presented in a black-box manner (as is the case for (Chat)GPT), and therefore does not allow to leverage any additional information, such as reference graphs. This is also reflected in the human evaluation schema introduced in [32] (see Table 24), which classifies common model mistakes. As reference graphs form the foundation of how the mathematical proofs are engineered, many elements of the evaluation schema are strongly tailored toward this representation of mathematical data. Our benchmark is not reference-centric and therefore allows evaluations of any type of proof (including computations, as featured in the Symbolic-Integration subdataset, which we consider to be a particular kind of proof). Therefore, our methodology includes further and more general failure modes to make for a more fine-grained evaluation that explains the nature of the errors. We refer to Appendix A for further related works.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 GHOSTS and miniGHOSTS Dataset</h1>
<p>We assess the mathematical reasoning capabilities of two ChatGPT versions, 9-January-2023 and 30-January-2023, and of GPT-4 by first creating a collection of 709 prompts from various sources, and subsequently evaluating the models on (subsets of) these data points. We rate the corresponding outputs provided by the models and collect statistics, such as error types, output lengths, or the stability of the answer under prompt engineering, see Sections 3.2 and 4 and Appendices B and D. This yields a total of 1636 ratings by human experts.</p>
<p>We divide our dataset, the entire collection of prompts, into six subdatasets, called</p>
<ul>
<li>Grad-Text,</li>
<li>H oles-in-Proofs,</li>
<li>Olympiad-Problem-Solving,</li>
<li>Symbolic-Integration,</li>
<li>MATH,</li>
<li>Search-Engine-Aspects,
which, in turn, consists of multiple files, see Table 1. The boldface letters make up the GHOSTS acronym. Details on motivation, composition, collection process, and intended uses of the GHOSTS dataset are summarized in our datasheet in Appendix E, Sections E.1, E.2, E. 3 and E.5, respectively.</li>
</ul>
<p>GPT-4 was evaluated on a subset of 170 prompts, which we call the miniGHOSTS dataset. Specifically, after having created the GHOSTS dataset, we heuristically selected a subset of 10 prompts from each file of the subdatasets included in GHOSTS, having the same mean rating and the same standard deviation (of ChatGPT's output) as the original file; see also our datasheet in Appendix E for more information. In this sense, these subsets can be considered to have the most relevance by capturing the "essence" of the model performance in the respective file.</p>
<h3>3.1 Subdatasets</h3>
<p>The subdatasets that make up our GHOSTS dataset are summarized in Table 1. In the following, we describe each subdataset in more detail.</p>
<p>Grad-Text This subdataset consists of a collection of books [33-35] that are widely used in universities to teach upper undergraduate or first-year graduate courses in a degree in mathematics. We have used most of the exercises from these books' first and second chapters, except for the book [33], where we only used exercises from the first chapter, which was longer than the other books' chapters.</p>
<p>Holes-in-Proofs This subdataset consists of a number of proofs sourced from math. stackexchange. com, a collection of books [36, 37], and the MATH dataset [11], where parts of the proofs were intentionally deleted and the LLM was prompted to fill in the gaps: This was done either by (1) using a MISSING token, (2) finishing the proof early and prompting the LLM to complete it, or (3) explicitly asking for certain conditions or results.</p>
<p>Olympiad-Problem-Solving This subdataset consists of a selection of exercises from the book Problem-Solving Strategies [38], that is often used to prepare for mathematical competitions. We selected and graded the LLM outputs on one hundred exercises drawn from all chapters.</p>
<p>Symbolic-Integration This subdataset consists of random samples of integrals from the test set of [12]. There are three ways in which integrals are generated in [12]: Forward generation (FWD), Backward generation (BWD), and Backward generation with integration by parts (IBP). We sample 21 integrals from FWD test set, 20 integrals from the BWD test set, and 59 integrals from the IBP test set. As these integrals are given in Polish/prefix notation, a natural-language prompt conversion of them is unlikely to be witnessed in the training dataset of (Chat)GPT. The assessment was done by verifying the correctness of the output both by using Mathematica, as well as making use of the provided solutions (in Polish notation), which [12] generated using SymPy. In particular, we notice that all integrals in this dataset have solutions that can be expressed using elementary functions.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Size</th>
<th>Comprised of the file(s)</th>
<th>Tags</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grad-Text</td>
<td>28</td>
<td>W. Rudin, Functional Analysis (ch. 1)</td>
<td>M3 Q4</td>
</tr>
<tr>
<td></td>
<td>15</td>
<td>W. Rudin, Functional Analysis (ch. 2)</td>
<td>M3 Q4</td>
</tr>
<tr>
<td></td>
<td>37</td>
<td>J. Munkres, Topology (ch. 1)</td>
<td>M3 Q4</td>
</tr>
<tr>
<td></td>
<td>29</td>
<td>J. Munkres, Topology (ch. 2)</td>
<td>M3 Q4</td>
</tr>
<tr>
<td></td>
<td>21</td>
<td>R. Durrett, Probability Theory</td>
<td>M3 Q4</td>
</tr>
<tr>
<td>Holes-in-Proofs</td>
<td>60</td>
<td>Proofs Collection A</td>
<td>M3 Q1 Q2 Q5</td>
</tr>
<tr>
<td></td>
<td>52</td>
<td>Proofs Collection B Prealgebra</td>
<td>M1 Q5</td>
</tr>
<tr>
<td></td>
<td>50</td>
<td>Proofs Collection B Precalculus</td>
<td>M1 Q5</td>
</tr>
<tr>
<td>Olympiad-Problem-Solving</td>
<td>$101+24$</td>
<td>Olympiad Problem Solving</td>
<td>M4 Q4 D2</td>
</tr>
<tr>
<td>Symbolic-Integration</td>
<td>100</td>
<td>Symbolic Integration</td>
<td>M2 Q3 D1</td>
</tr>
<tr>
<td>MATH</td>
<td>50</td>
<td>MATH Algebra</td>
<td>M1 M2 M3 Q3 Q4</td>
</tr>
<tr>
<td></td>
<td>50</td>
<td>MATH Counting and Probability</td>
<td>M1 M2 M3 Q3 Q4</td>
</tr>
<tr>
<td></td>
<td>18</td>
<td>MATH Prealgebra</td>
<td>M1 Q3 Q4</td>
</tr>
<tr>
<td></td>
<td>20</td>
<td>MATH Precalculus</td>
<td>M1 Q3 Q4</td>
</tr>
<tr>
<td>Search-Engine-Aspects</td>
<td>30</td>
<td>Definition Retrieval</td>
<td>M3 Q2 D3</td>
</tr>
<tr>
<td></td>
<td>30</td>
<td>Reverse Definition Retrieval</td>
<td>M3 Q1 Q2 D3</td>
</tr>
<tr>
<td></td>
<td>18</td>
<td>Named Theorem Proof Completion</td>
<td>M3 Q2 Q5 D3</td>
</tr>
</tbody>
</table>
<p>Table 1: A summary of all the files from the subdatasets comprising our GHOSTS dataset, together with their size, i.e., the number of prompts, and their associated tags. The tags $\mathrm{M} i, \mathrm{Q} i$, and $\mathrm{D} i$ relate to the level of Mathematical difficulty, the Question type, and the Out-of-Distribution type from Section 3.1, respectively. We additionally created 24 prompts for the Olympiad-Problem-Solving subdataset using prompt engineering, see Appendix D.1.</p>
<p>MATH This subdataset consists of a random sample of problems from the MATH dataset [11]. The latter dataset attaches a level of difficulty to each problem. We focused on two domains, Algebra and Probability Theory, and sampled an equal number of problems at each level of difficulty.</p>
<p>Search-Engine-Aspects This subdataset consists of problems that were not sampled from a particular source but generated by a human expert in the field. In the file Named Theorem Proof Completion, we focused on prompting the LLM to provide proof outlines of various theorems that are sufficiently well-known within Functional Analysis to have names. In the Definition Retrieval file, we asked the LLM to correctly state various definitions centered around Functional Analysis and Topology. In contrast, in the Reverse Definition Retrieval file, we verified whether the LLM was able to deduce the name of a mathematical object by describing its properties.</p>
<p>Because input to (Chat)GPT is purely textual (at the time of writing), certain types of questions that might be stated and solved in a non-text-based fashion (e.g., questions involving graphical diagrams, without text explaining the diagram ${ }^{5}$, as occasionally occur in [38]), have been excluded. Our subdatasets can be categorized along the following dimensions (see Appendix B.1 for more details):</p>
<ul>
<li>Mathematical difficulty (ascending): (M1) Elementary arithmetic problems, (M2) Symbolic problems, (M3) (Under)graduate-level exercises, (M4) Mathematical olympiad problems.</li>
<li>Question type: (Q1) Stating mathematical facts, (Q2) Overview-type review questions, (Q3) Computational questions, (Q4) Theorem proofs or puzzle solutions, (Q5) Proof-completion questions.</li>
<li>Types of high out-of-distribution likelihood: (D1) Nontrivial problem encoding, (D2) Succinct solution, (D3) Spoken dialogue.</li>
</ul>
<p>The existing datasets of natural-language mathematics are far from covering all possible combinations across these dimensions. In our well-crafted GHOSTS datasets, we have striven to cover each of these aspects individually, as can be seen in Table 1. The next section specifies the format of our dataset and the methodology for analyzing (Chat)GPT's output.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.2 Format</h1>
<p>The format of each of the subdatasets that make up our GHOSTS dataset follows the same convention. Each subdataset consists of JSON-formatted files, and our format is similar to, e.g., the AQuA-RAT dataset [27]. A single data point ${ }^{6}$ in a JSON file has the following form:</p>
<div class="codehilite"><pre><span></span><code><span class="s">&quot;prompt&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Let $X$ be a topological vector space. All sets mentioned below are</span>
<span class="s">    understood to be the subsets of $X$. Prove the following statement: If $A$</span>
<span class="s">    and $B$ are compact, so is $A + B$&quot;</span><span class="p">,</span>
<span class="s">&quot;output&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;The statement is wrong in general. Consider the example $A = [-1,1]</span>
<span class="s">    \times {0}$ and $B = {0} \times [-1,1]$. Then $A$ and $B$ are compact but</span>
<span class="s">    $A + B = [-1,1] \times [-1,1]$ is not compact.&quot;</span>
<span class="s">&quot;rating&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;2&quot;</span><span class="p">,</span>
<span class="s">&quot;errorcodes&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;e3&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;e5_2&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;e5_4&quot;</span><span class="p">],</span>
<span class="s">&quot;warningcodes&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[],</span>
<span class="s">&quot;comment&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;The given $A + B$ actually *is* compact.&quot;</span><span class="p">,</span>
<span class="s">&quot;msc&quot;</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;46A03&quot;</span><span class="p">],</span>
<span class="s">&quot;ref&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;Rudin-Functional Analysis-Second-Ed.Part1-ex3/d-page38&quot;</span><span class="p">,</span>
<span class="s">&quot;confidence&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;high&quot;</span><span class="p">,</span>
<span class="s">&quot;timestamp&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;2023-01-31&quot;</span>
</code></pre></div>

<p>We require each data point to have the same JSON keys as in this example, some of which may be empty depending on the prompt. Among the listed keys, the rating key stands out as the most fundamental one. Its value serves as a condensed representation of the mathematical capability of the tested language model, compressed into a one-dimensional measure ranging from 1 (lowest) to 5 (highest). A more nuanced and fine-grained perspective on the mathematical capabilities is provided by the errorcodes and warningcodes keys. The msc key denotes the mathematics subject classification. We explain each JSON key in Appendix B.2. For end-users of (Chat)GPT, it is desirable to avoid having a long-winded dialogue to arrive at a solution. Therefore, we require that (Chat)GPT provides us with the correct solution given only the input prompt without any subsequent interaction.</p>
<h3>3.3 Human Effort in Dataset Creation and Mathematical Evaluation</h3>
<p>For all data points, the values of the keys rating, errorcodes, warningcodes, comment, and confidence were manually labeled, without any automation. The msc, ref, and timestamp keys were populated in a semi-automatic way since their values change only slightly within the same subdataset.</p>
<p>Two of the subdatasets, the MATH subdataset and the Symbolic-Integration subdataset, use prompts taken from existing datasets, [11] and [12], respectively. This was done to compare how (Chat)GPT performs against existing state-of-the-art models that use these datasets, see Section 4. Nonetheless, significant additional annotation effort was involved since, in both cases, the authors rated the output. Furthermore, in the second case, the data is publicly presented in a Polish notation format, and manual conversion was necessary ${ }^{7}$.</p>
<p>The prompts of the other subdataset were hand-crafted by the authors. We note that it is neither possible to outsource the creation of these subdatasets to a crowdsourcing service, such as Amazon Mechanical Turk, nor is it possible to generate them automatically from code because advanced mathematical insight is required for the creation of each prompt (where applicable) and for providing the fine-grained evaluation of the mathematical capabilities. Furthermore, unlike in the case of the MATH dataset by [11] (see Section 2), the answer to most of our prompts cannot be condensed into a few tokens (such as a number or a function), e.g., when the answer is a mathematical proof.</p>
<p>This raises the difficulty of the creation of more data since graduate-level (and in some cases, PhD-level) mathematics is required. The combined effort of devising mathematically insightful prompts and carefully rating the output of (Chat)GPT amounts to 1636 prompt evaluations, totaling several hundreds of person-hours, see Appendix B.6. However, as a result of these efforts, our dataset goes beyond all the mentioned mathematical datasets for LLMs in Section 2 in terms of the different aspects of mathematical reasoning that are being tested.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 Results</h1>
<p>Will ChatGPT get you through a university math class? No, you would be better off copying from your average peer-unless it is undergraduate mathematics, for which GPT-4 can offer sufficient (but not perfect) performance.</p>
<p>If we take a rating of 3.5 , the average between the lowest and highest rating, to be the threshold between success and failure, then Figure 1 shows that for a majority of subdatasets, both versions of ChatGPT will not pass. However, for GPT-4, the situation is different, and, on miniGHOSTS, it passes (sometimes barely) on all subdatasets files, except W. Rudin, Functional Analysis (ch. 2), which tests graduate-level mathematical knowledge and the Olympiad Problem Solving file, which tests mathematical problem-solving skills. We note that, unless otherwise stated, we do not use prompt-engineered questions in the results presented here (see Appendix D.1).</p>
<p>We first focus on the results of the 9-January-2023 version of ChatGPT and note that the results for the 30-January-2023 are very similar, as can be inferred from the figures. On average, the 9-January-2023 version achieves a rating of 3.20 with a standard deviation ${ }^{8}$ of 1.23 . It performs particularly poorly on proof-based questions in the style of graduate-level exercises or mathematical olympiads, as well as more complicated symbolic calculations. We note that prompt engineering only slightly improved the results for such complex questions; see Appendix D.1. However, in tasks that only required filling in gaps or stating mathematical facts, ChatGPT was mostly able to achieve a score above 3.5. In particular, ChatGPT was strong at recognizing the context of the question, and the notation of the output almost always matched the one given in the prompt, see Figure 5 in the appendix. Generally, Figure 1 indicates that the ratings closely correspond to how mathematicians would rank the difficulty of the exercises. In this context, we note that the length of the prompt does not have a clear effect on the rating; see Figure 9 in the appendix. We present results for different mathematical fields in Figure 4 in the appendix. For a detailed qualitative analysis of the results on the different subdatasets, we refer to Appendix D.2. Finally, we note that (Chat)GPT almost never expressed any form of uncertainty, even if its output has been completely wrong. This is different from other LLMs we have experimented with; see also Appendix D.3.</p>
<p>Comparing ChatGPT to the performance obtained by [12], who correctly solved nearly $100 \%$ of the integrals in a collection of 500 test equations [12, Table 3], the 9-January-2023 version of ChatGPT achieves an average rating of 2.51 (standard deviation: 0.87 ) on our random sample of their dataset (after conversion from Polish notation to $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$ ). Specifically, a rating of 2 is dominating $70 \%$ of the time, followed by a rating of 3 and 4 for $13 \%$ of the prompts each; see also Figure 7 in the appendix. GPT-4 achieves an average of 3.50 (standard deviation: 1.43), barely a passing grade, on the corresponding subset from miniGHOSTS. These scores trail far behind the performance achieved by the model in [12]. The situation is similar when comparing ChatGPT to Minerva [19, Table 3]. Their best model achieved an accuracy of $50 \%$ on the MATH dataset [11]. However, the 9-January-2023 version of ChatGPT achieves a perfect score only on $29 \%$ of our random samples from the MATH dataset (which is above the total average of $25 \%$ of data points across all subdatasets in which this version achieves a perfect score), see Figures 6 and 7 in the appendix. In contrast, GPT-4 performs substantially better and obtains a score of 5 on $70 \%$ of the corresponding questions within the miniGHOSTS dataset, see Figure 7 in the appendix.</p>
<h3>4.1 Quantitative Comparison of (Chat)GPT Versions</h3>
<p>The ensuing model version, 30-January-2023, overall performed similarly with an average rating of 3.29 (standard deviation: 1.28), although performance was inconsistent across subdatasets and on some subdatasets marginally better, see Figure 1. A significant jump in performance could only be observed for GPT-4, which achieved a substantially higher average rating of 4.15 (standard deviation: 1.12). We note that the evaluation of GPT-4 is only on the miniGHOSTS dataset, i.e., a subset of GHOSTS. Nonetheless, these preliminary findings send a clear message that the performance of GPT-4 dominates the performance of ChatGPT (both versions), see Figure 1.</p>
<p>Figure 2 shows how the ratings change between the different versions of (Chat)GPT. Surprisingly, one can see a shuffling of the grades for the two ChatGPT versions, even though the counts in each grade bracket stay approximately the same. For instance, there are roughly the same amount of outputs that received grade 4, yet less than half of the prompts were the same between model changes. Appendix D. 5 provides different perspectives on this and reinforces the mixed performance increase the 30-January-2023 model</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Average rating for each file in each subdataset (bold) of GHOSTS on the 9-January-2023 and the 30-January-2023 versions of ChatGPT and for miniGHOSTS on GPT-4. Note that the maximal ranking is 5 and the minimal ranking, where the question was at least understood, is 2, see Appendix B.4; the lower rating of 1 indicates that the answer completely misses the question. Thus, a reasonable passing grade, i.e., 50% of points, corresponds to a score of 3.5, as indicated by the vertical dotted line. The error bars represent 95% confidence intervals.</p>
<p>brings. For GPT-4, we see that the percentage of perfect ratings almost doubles, while the percentage of prompts, which are not understood or completely wrong (i.e., ratings of 1 or 2), approximately halves as compared to the ChatGPT versions.</p>
<p>Analysis of (Chat)GPT's output and our warning codes reveal that GPT-4 provides even longer ("rambling") answers, whereas ChatGPT usually answered the question without giving any additional context about the topic, see Figures 6 and 8 in the appendix. The answer style of GPT-4 was often beneficial (resulting in better overall scores) but sometimes reduced the readability of the output. Furthermore, we found the behavior of GPT-4, compared to ChatGPT, to be more opinionated. Finally, despite its better overall performance, GPT-4 still seems to be vulnerable to mistakes in seemingly simple calculations. We refer the reader to Appendix D for further results on the models' performance.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A Sankey diagram of how the ratings evolve from 9-January-2023 ChatGPT to 30-January-2023 ChatGPT to GPT-4 (from top to bottom), with all models evaluated on miniGHOST. While grades on the 9-January and 30-January models are shuffled between the ChatGPT versions, the overall performance remains approximately the same. However, we observe a significant increase in perfect ratings, i.e., a score of 5, for GPT-4.</p>
<h2>5 Conclusion</h2>
<p>We have examined the behavior of (Chat)GPT across various tasks that test different aspects of mathematical skill. Contrary to the media sensation that (Chat)GPT has caused, (Chat)GPT is not yet ready to deliver high-quality proofs or calculations <em>consistently</em>. At the same time, the quality of the answers can be positively surprising. Moreover, our preliminary evaluation of GPT-4 on the miniGHOSTS dataset reveals promising improvements over ChatGPT's performance. In Appendix D.6, we collect the best and worst results for a number of selected subdatasets. The best responses can be seen to justify the media sensation. It thus seems fair to say that (Chat)GPT is <em>inconsistently bad</em> at advanced mathematics: While its capabilities generally drop with the mathematical difficulty of a prompt, it does give insightful proofs in a few cases.</p>
<p>However, (Chat)GPT falls short of achieving the same performance as models specifically trained for single tasks. These models, in contrast, lack the flexibility of (Chat)GPT, which is a <em>universal</em> tool suitable for any area of mathematics. In fact, (Chat)GPT's ability to search for mathematical objects, given information about them, is where it shines. For a user that is already sufficiently mathematically proficient to discern the correctness of (Chat)GPT's output, (Chat)GPT can be integrated as an assistant in the user's workflow. It can function as a search engine or knowledge base to speed up various lookup tasks, as they often occur at certain stages of mathematical research.</p>
<p>Due to the prohibitive annotation effort, the GHOSTS dataset is not yet large enough to significantly improve the mathematical capabilities of LLMs by fine-tuning them on GHOSTS; though we believe it is sufficiently comprehensive to allow an evaluation and comparison of LLMs. As a first step, we want to extend the evaluation of GPT-4 to the full GHOSTS dataset, considering its promising performance on miniGHOST. We also encourage other researchers to mine our dataset beyond the descriptive statistics we computed in order to gain a deeper understanding of how LLMs behave on mathematical tasks. Finally, we hope that our work motivates other mathematicians to contribute to the GHOSTS dataset in order to establish a thorough benchmark for assessing the mathematical abilities of LLMs.</p>
<h1>References</h1>
<p>[1] Sascha Lobo. Das Ende von Google, wie wir es kannten. Der Spiegel, Retrieved 2023-01-10. https: //www.spiegel.de/netzwelt/netzpolitik/bessere-treffer-durch-chatgpt-das-ende-von-google-wie-wir-es-kannten-kolumne-a-77820af6-51d7-4c03-b822-cf93094fd709.
[2] John Naughton. The ChatGPT bot is causing panic now - but it'll soon be as mundane a tool as Excel. The Guardian, Retrieved 2023-01-14. https://www.theguardian.com/commentisfree/ 2023/jan/07/chatgpt-bot-excel-ai-chatbot-tec.
[3] Kevon Roose. The Brilliance and Weirdness of ChatGPT. The New York Times, Retrieved 2023-01-24. https://www.nytimes.com/2022/12/05/technology/chatgpt-ai-twitter.html.
[4] teddy [@teddynpc]. I made ChatGPT take a full SAT test. Here's how it did: [Image attached] [Tweet]. Twitter. 2023-01-13. https://twitter.com/teddynpc/status/1598767389390573569.
[5] Timothy Gowers [@wtgowers]. It's amusing when ChatGPT makes ridiculous mathematical mistakes. But of course, it's more interesting to find out what it can do well. Here's one example that wasn't bad: I gave it a very rough outline of a proof and asked it to fill in the details [Tweet]. Twitter. 2023-01-13. https://twitter.com/wtgowers/status/1611750773607604224.
[6] OpenAI (2023). GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[7] Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, and Lorie De Leon et al. Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models. medRxiv, 2022.
[8] David Rozado. What is the IQ of ChatGPT? Retrieved 2023-01-09. https://davidrozado. substack.com/p/what-is-the-iq-of-chatgpt.
[9] Christian Terwiesch. Would Chat GPT3 Get a Wharton MBA? A Prediction Based on Its Performance in the Operations Management Course. Retrieved 2023-01-04. https://mackinstitute.wharton. upenn.edu/wp-content/uploads/2023/01/Christian-Terwiesch-Chat-GTP.pdf.
[10] Natalie. ChatGPT - Release Notes. Retrieved 2023-04-03. https://help.openai.com/en/ articles/6825453-chatgpt-release-notes.
[11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, and Steven Basart et al. Measuring mathematical problem solving with the MATH dataset. arXiv preprint arXiv:2103.03874, 2021.
[12] Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.
[13] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, 3(3):210-229, 1959.
[14] Jörg Denzinger, Matthias Fuchs, Christoph Goller, and Stephan Schulz. Learning from previous proof experience: A survey. Technical report, TU München, 1999.
[15] John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In Computational Logic, volume 9, pages 135-214, 2014.
[16] Malik Amir, Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver, and Eldar Sultanow. Machine Learning Class Numbers of Real Quadratic Fields. arXiv preprint arXiv:2209.09283, 2022.
[17] Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, and Daniel Zheng et al. Advancing mathematics by guiding human intuition with AI. Nature, 600(7887):70-74, 2021.
[18] Yang-Hui He. Machine-learning the string landscape. Physics Letters B, 774:564-568, 2017.
[19] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, and Henryk Michalewski et al. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems, 2022.
[20] Francois Charton, Amaury Hayat, and Guillaume Lample. Learning advanced mathematical computations from examples. In International Conference on Learning Representations, 2021.</p>
<p>[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017.
[22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, and Gaurav Mishra et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[23] Aida Amini, Saadia Gabriel, Shanchuan Lin, and Rik Koncel-Kedziorski et al. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2357-2367. Association for Computational Linguistics, 2019.
[24] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, and Heewoo Jun et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[25] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, and Jared D Kaplan et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, pages 1877-1901, 2020 .
[26] Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 383-394. Association for Computational Linguistics, 2021.
[27] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 158-167. Association for Computational Linguistics, 2017.
[28] Teven Le Scao and Angela Fan et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
[29] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, and Apoorv Kulshreshtha et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
[30] Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535, 2022.
[31] Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. arXiv preprint arXiv:2104.01112, 2021 .
[32] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models. arXiv preprint arXiv:2205.12910, 2022 .
[33] Rick Durrett. Probability: Theory and Examples. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
[34] James R. Munkres. Topology. Prentice-Hall, 2000.
[35] Walter Rudin. Functional analysis. McgGraw-Hill, 1991.
[36] Sheldon Axler. Linear algebra done right. Springer, 2015.
[37] W. Rudin. Principles of Mathematical Analysis. International series in pure and applied mathematics. McGraw-Hill, 1976.
[38] Arthur Engel. Problem-Solving Strategies. Springer, 1998.
[39] Tranquil Sea Of Math. Does ChatGPT code LaTeX and write proofs? Youtube. Retrieved 2023-01-12. https://www.youtube.com/watch?v=gw2N7VI_8P0.</p>
<p>[40] Richard Van Noorden @richvn@mastodon.social [@Richvn]. Huh. ChatGPT confidently gives the right kind of reasoning to solve this math problem, but whiffs on the algebra in the middle and gets the answer wrong [Tweet]. Twitter. 2023-01-09. https://twitter.com/Richvn/status/ 1598714487711756288 .
[41] Amos Azaria. ChatGPT Usage and Limitations. Retrieved 2023-01-15. https://hal.science/hal03913837 .
[42] Ernest Davis. Mathematics, word problems, common sense, and artificial intelligence. arXiv preprint arXiv:2301.09723, 2023.
[43] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[44] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020 .
[45] The mathlib Community. The Lean mathematical library. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs. ACM, 2020.
[46] Markus N. Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Mathematical reasoning via self-supervised skip-tree training. arXiv preprint arXiv:2006.04757v3, 2020.
[47] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, and Carroll L. Wainwright et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
[48] Carroll Wainwright and Ryan Lowe. InstructGPT: Training Language Models to Follow Instructions with Human Feedback . GitHub repository, Retrieved 2023-01-09. https://github.com/openai/ following-instructions-human-feedback.
[49] Sarah Wiegreffe (sigmoid.social/@sarah) [@sarahwiegreffe]. If text-davinci-001 is a rough approximate to the model reported in the NeurIPS 2020 paper, and text-davinci-002 is InstructGPT in the 2022 preprint, then what is just "davinci"? Trying to reproduce results from a time before this naming existed [Tweet]. Twitter. 2023-01-15. https://twitter.com/BlancheMinerva/status/ $1584788418751406080$.
[50] OpenAI. GPT-4 API waitlist. Retrieved 2023-06-06. https://openai.com/waitlist/gpt-4-api.
[51] OpenAI. Documentation - Models. Retrieved 2023-06-06. https://platform.openai.com/docs/ models/gpt-4.
[52] OpenAI. OpenAI API Reference - Chat Completion Endpoint. Retrieved 2023-06-06. https: //platform.openai.com/docs/api-reference/chat.
[53] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, and Henrique Ponde de Oliveira Pinto et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[54] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):8692, 2021 .</p>
<h1>A Further Related Works</h1>
<p>In this section, we present further related works. For (Chat)GPT, most investigations related to mathematical reasoning consist of anecdotal evidence concerning its performance and its failure modes. Notable mentions on social media can, for instance, be found in [4, 5, 39, 40]. Unfortunately, a clear methodology is missing, as most of the results are scattered on various internet platforms and cannot be easily reproduced. To the best of our knowledge, the only investigations into the mathematical capabilities prior to the appearance of our first preprint were undertaken by [41, 42]. However, these works only report a small number of qualitative results, often on rather simple mathematical tasks and without specifying the precise versions of (Chat)GPT. The latter reference reports results only on a few selected examples, while the former reference investigates ChatGPT's ${ }^{9}$ ability to compute irrational numbers as well as to solve some elementary math word problems. Recently, the dataset by [43] appeared, which contains a systematic evaluation of ChatGPT on the GSM8K dataset [24], the MATH dataset [11], and the MMMLU-STEM dataset [44]. These datasets allow for an automatic evaluation using only accuracy as an evaluation metric. Additionally, a few further anecdotal examples of mathematical performance are presented in [43].
Finally, we would also like to mention the field of formalized mathematics, where large databases that encode advanced mathematical concepts exist, e.g., the Lean Mathematical Library [45]. Some of the ideas that we have used in this article, such as using prompts that formulate a task to fill in gaps in proofs, are echoed in [46] for datasets for formal mathematics, consisting of expression trees. Yet, for the purpose of doing mathematics with large language models, these formal datasets cannot be leveraged since no straightforward way exists to convert them to natural language.</p>
<h2>B Dataset Creation</h2>
<h2>B. 1 Categorization</h2>
<p>Our subdatasets can be categorized along the following dimensions, see Table 1:</p>
<h2>- Mathematical difficulty (ascending):</h2>
<p>$\overline{M 1}$ Elementary arithmetic problems, as found in the MATH dataset [11] at lower levels of difficulty.
$\overline{M 2}$ Symbolic problems (integration of functions) that can be also solved via a supervised-learning, data-driven approach to mathematics [12].
$\overline{M 3}$ (Under)graduate-level exercises from well-known textbooks [36, 33, 34, 37, 35] as well as questions from math. stackexchange.com, spanning diverse domains of mathematics.
$\overline{M 4}$ Exercises that are in the style of mathematical olympiad problems, such as those taken from Engel's Problem-Solving Strategies book [38].</p>
<h2>- Question type:</h2>
<p>$\overline{\mathrm{Q} 1}$ Review questions, which ask to state or name certain mathematical facts correctly.
$\overline{\mathrm{Q} 2}$ Overview-type review questions, which cut through an entire field of mathematics.
$\overline{\mathrm{Q} 3}$ Computational questions.
$\overline{\mathrm{Q} 4}$ Proof-based questions, which ask for a theorem proof or for a puzzle solution.
$\overline{\mathrm{Q} 5}$ Proof-completion questions, where a proof has gaps or is incomplete, and needs to be completed.</p>
<h2>- Types of high out-of-distribution likelihood:</h2>
<p>$\overline{D 1}$ Nontrivial problem encoding: The data points from the Symbolic Integration subdataset come from [12] and are publicly available ${ }^{10}$. Since the online training set uses Polish notation, it is very unlikely that (Chat)GPT has seen the corresponding prompts in $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$ before.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>D Succinct solution: The solutions for the Olympiad-Problem-Solving subdataset are included in the book by Engel [38]. But the solutions are extremely concise, and simply repeating them would not show an immediate understanding of the problem.</p>
<p>D3 Spoken dialogue: The Search-Engine-Aspects subdataset is unlikely to be well represented in the data on which (Chat)GPT has been trained since its prompts resemble word fragments that might appear in a mathematical dialogue (e.g., an oral mathematical exam), rather than in a textbook.</p>
<p>One could, in theory, start to investigate every combination of these attributes (e.g., for elementary arithmetic problems, in a non-trivial encoding, one could generate data to cover every possible question type listed above). However, this would lead to 60 subdatasets, which, due to the manual curation effort, is too much for a single research group.</p>
<h1>B. 2 Format</h1>
<p>The dataset consists of a collection of UTF-8 encoded JSON files. We explain the JSON keys of each data point in our dataset in the following and also indicate whether its value is optional. If the value is optional, the key has to be present, but the value will be an empty array or string.</p>
<ul>
<li>prompt denotes the input that we provide to (Chat)GPT through its web interface at the URL chat. openai.com/chat, see also Appendix C. We use a new session for each prompt to avoid (Chat)GPT being biased by previous prompts.</li>
<li>output denotes the raw output that (Chat)GTP supplies us with. In some cases, mathematical formulas were rendered in the web interface such that we copied them in LaTeX.</li>
<li>rating is a number from 1 to 5 that shows how many points (Chat)GPT has scored, 5 being a perfect answer and 1 being the lowest rating. A detailed explanation of the rating policy that we followed is contained in Appendix B.4.</li>
<li>errorcodes (optional) highlight a list of error types that illustrate the failure modes of (Chat)GPT in a more fine-grained way. Not all types of errors apply to all (sub)datasets: For example, an error code for a missing proof step would not be applicable on a dataset that tests whether (Chat)GPT can multiply numbers or find prime divisors. The detailed explanation of the error codes (and the warning codes; see below) that was provided to the annotators is contained in Appendix B.4. There, we also include a policy of how ratings and error codes have to be used together.</li>
<li>warningcodes (optional) highlight any problematic aspects of (Chat)GPT; for example, (Chat)GPT might be rambling and providing the user with unrelated information or use a poor (but correct) way of solving problems.</li>
<li>comment (optional) denotes any noteworthy commentary that an assessor of (Chat)GPT may make. This can be used to give a more detailed explanation of the output, provide reasoning behind awarding a certain error code or rating, or generally provide context. For some subdatasets, this key was used to indicate the difficulty level of the prompt, as well as an official solution, if available, see Section 3.1. It was also used to indicate whether we used prompt engineering, see Appendix D.1.</li>
<li>msc denotes a list of mathematics subject classifications ${ }^{11}$ (MSC) that pertain to the output. Note that we do not classify the prompt given to (Chat)GPT as there may be no proper classification; for example, when (Chat)GPT is asked what the most important theorem in all of mathematics is ${ }^{12}$, it is meaningless to assign an MSC code. We also note that for particularly easy mathematical questions (e.g., simple arithmetical questions), no suitable MSC codes exist to classify the output, since MSC codes typically classify more advanced mathematics ${ }^{13}$. Nonetheless, we have attempted to match them as well as possible and allow multiple MSC codes in order to classify the output as precisely as possible.</li>
<li>ref (optional) indicates a reference to where the prompt was originally taken from (for some subdatasets, such as Holes-in-Proofs, we have used excerpts from various books or math. stackexchange. com; the original source was recorded as a value of this key). This key can have an empty value if the question was formulated by the authors and no authoritative source was plausible.</li>
</ul>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>confidence indicates how confident we have perceived (Chat)GPT to be when presenting us with its output. We allow values of high, medium, and low.</li>
<li>timestamp denotes when the prompt was entered into (Chat)GPT. This can be used to track the version of (Chat)GPT; see Section 4.1.</li>
</ul>
<p>The values of these keys within a single data point interact in nontrivial ways: If a rating of 5 is given, then it is expected that no error code is present - though there may be warning codes that are used. The error codes and warning codes are loosely in the spirit of a compiler throwing errors and warnings if it is given incorrect or sloppy code - although we have a role reversal, where the human is now the compiler, and the machine produces the code. In this sense, for some prompts, we have used multiple error and/or warning codes, which is why the corresponding values are arrays of strings. We use these codes to collect statistics on the behavior of (Chat)GPT; see Section 4.</p>
<p>For most of the subdatasets that make up our GHOSTS dataset, we have used $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ to encode mathematical formulas in our prompts. Our experiments have shown that (Chat)GPT can process $\mathrm{ET}</em>$-encoded mathematics well.}} \mathrm{X</p>
<p>The usage of MSC codes can be useful for mathematicians who want to integrate (Chat)GPT in their daily workflow, as it allows them to know in which areas the model performs better and can hence be trusted more. Our dataset is very diverse, having a total of 78 MSC codes. The top short versions of these codes (first two digits) are 26 ("Real functions", 127 occurrences) followed by 00 ("General", 110 occurrences) and 46 ("Functional analysis", 77 occurrences), see also Figure 4. An exhaustive survey of (Chat)GPT's performance across every MSC code would necessitate a large, community-driven effort to set up an extensive database. Due to the high cost of rating each output, requiring specialized skills, this is something that no individual research group could reasonably do - but we hope that our approach is a starting point for such an effort.</p>
<h1>B. 3 Copyright and Licensing Terms</h1>
<p>Some of the subdatasets contain prompts that may be protected under copyright, i.e., from the Grad-Text and Olympiad-Problem-Solving dataset. In these cases, the publicly released dataset does not contain the prompt. The ref key includes a detailed reference to the page where the original theorem or exercise that was presented, so a reader can easily retrieve the prompt. All other prompts are either created by us or released under licenses that allow us to include the prompt.</p>
<p>For the prompts that are not created by us, the following applies: We license the entire data point (i.e., the content of all JSON keys except the prompt key, i.e., the content created by the authors) under the same license as the prompt. The following licenses, therefore, apply in the cases of data points using prompts from external sources:</p>
<ul>
<li>The $M A T H$ subdataset is distributed under an MIT license.</li>
<li>The Symbolic-Integration subdataset is distributed under a Creative Commons Attribution-NonCommercial license.</li>
<li>Prompts originating from user contributions on math. stackexchange.com, see the ref key for such occurrences (e.g., in the Proofs Collection A file), are licensed under Creative Commons AttributionShareAlike license, in different versions, see https://math.stackexchange.com/help/licensing.</li>
</ul>
<p>We release prompts from the GHOSTS and miniGHOSTS datasets that are created by us under the following Creative Commons license: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0); see https://creativecommons.org/licenses/by-nc/4.0/ for the detailed terms of the license. By this license, one may not use the dataset for commercial purposes, and one must give appropriate credit; if users are building on the GHOSTS dataset, they need to indicate the changes that were made and distribute their contributions under the same license as the original.</p>
<h2>B. 4 Data Collection and Labeling Policies</h2>
<p>Prompts from books were transcribed into $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$. The output from (Chat)GPT's web interface was copied as-is, even if the output was not valid $\mathrm{ET}</em>$. Below are the policies that were followed by each assessor of (Chat)GPT's output regarding the rating, the error codes, and the warning codes:}} \mathrm{X}$ code. If the output contains rendered mathematical expressions, our policy was to transcribe it to $\mathrm{ET}_{\mathrm{E}} \mathrm{X</p>
<h1>Rating</h1>
<ul>
<li>$1 \rightarrow$ failure to understand the query (e.g., the user asks it something about number theory, and it responds with information about differential equations);</li>
<li>$2 \rightarrow$ query was understood, but the answer was entirely wrong (e.g., the user asks what the prime divisors of 111 are ${ }^{14}$, and it responds with 8 and 6 );</li>
<li>$3 \rightarrow$ query was understood, but the answer was only partially correct (e.g., the user asks it what the prime divisors of 111 are, and it responds with 3 and 6 );</li>
<li>$4 \rightarrow$ query was understood, and the answer was mostly correct (e.g., the user asks it what the prime divisors of 222 are ${ }^{15}$ and it responds with 3 and 37 );</li>
<li>$5 \rightarrow$ query was understood and answer was completely correct.</li>
</ul>
<h2>Error codes</h2>
<ul>
<li>e1 $\rightarrow$ missing examples, or information (e.g., the user asks it what the prime divisors of 111 are, and it responds with 3 , missing 37); this also applies, if (Chat)GPT ignores a part of the prompt (e.g., an equivalence needs to be shown, but (Chat)GPT shows only one direction);</li>
<li>e2 $\rightarrow$ a few wrong/vague statements (e.g., the user asks it what the prime divisors of 30030 are ${ }^{16}$ and it responds with $2,3,5,7,13$ (wrong); or says that $2,3,5$, and some other numbers are prime divisors (vague)); it can also denote a single statement, that is slightly vague;</li>
<li>e3 $\rightarrow$ a lot of wrong/too vague statements (e.g., the user asks it what the prime divisors of 30030 are, and it responds with $2,5,8,12,13,15$ (wrong); or says that 2 and many other numbers are prime divisors (vague)); it can also denote a single statement, that is highly vague;</li>
<li>e4 $\rightarrow$ wrong computations (i.e., an additional error flag to disambiguate between statements that are of computational nature or not);</li>
<li>e5 $\rightarrow$ denotes wrong logic or wrong flow of arguments, which we further subdivide into specific flags, as we prohibit the use of e5 on its own (since it would be uninformative):</li>
<li>e5_1 $\rightarrow$ (Chat)GPT claims that to complete a proof, statements need to be shown that are unrelated to the claim;</li>
<li>e5_2 $\rightarrow$ a proof step is missing;</li>
<li>e5_3 $\rightarrow$ an edge case has not been considered;</li>
<li>e5_4 $\rightarrow$ an inference step is not supported (e.g., (Chat)GPT claims that from A follows B, but this claim is not true);</li>
<li>e5_5 $\rightarrow$ circular logical argument (i.e., using the hypothesis to prove the hypothesis);</li>
<li>e6 $\rightarrow$ the general set-up is understood, but the legal operations are not respected or misunderstood (e.g., we are given a puzzle where we are only allowed to add even integers, but (Chat)GPT changes the rules and motivates the solution by allowing the addition of odd integers; or (Chat)GPT misunderstands an adjective that has multiple mathematical meanings, such as "dual", which can mean either topological dual space or algebraic dual space).</li>
</ul>
<p>The following policy applies for error codes: If a rating $r$ with $1&lt;r&lt;5$ has been given, then an error code is mandatory to explain the type of error that occurred. For a perfect score of 5, no error codes should be assigned (but warning codes can be assigned). If the score is lowest, i.e., a rating of 1 , error codes can be assigned, but do not have to: In the case where (Chat)GPT has not understood the prompt, there typically is no reason to further detail the type of error.</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Warning codes</h1>
<ul>
<li>w1 $\rightarrow$ (Chat)GPT is withholding essential information related to the prompt (e.g., the user asked it something about the integral $\int_{-\infty}^{\infty} e^{-x^{2}} \mathrm{~d} x$, and it answers correctly but does not tell the user that the integral was actually a famous, named integral, i.e., the Gaussian integral);</li>
<li>w2 $\rightarrow$ (Chat)GPT is rambling (i.e., after answering, correctly or incorrectly, (Chat)GPT tells the user much more details than the user wanted to know);</li>
<li>w3 $\rightarrow$ (Chat)GPT is hallucinating (i.e., after answering, correctly or incorrectly, (Chat)GPT tells the user unrelated information);</li>
<li>w4 $\rightarrow$ (Chat)GPT behaves weirdly (e.g., by using a weird proof structure (where applicable), using strange mathematical formulations, or by adopting a strange tone of the conversation or making opinionated statements);</li>
<li>w5 $\rightarrow$ (Chat)GPT changes the notation from the prompt without being instructed to do so (e.g., the prompt contains a vector space $\mathbb{X}$, but (Chat)GPT calls it $\mathbb{F}$ ).</li>
</ul>
<h2>B. 5 Mitigating Human Errors</h2>
<p>Any assessment procedure that has a human component is prone to introducing bias-in particular, a procedure involving manual work such as rating the model outputs. The following safeguards help to mitigate bias as well as human error (such as typos):</p>
<h2>1. Guarding against $\mathbf{E T}_{\mathbf{E}} \mathbf{X}$ errors:</h2>
<p>Various typographical errors may appear due to incorrect LaTeX formatting. In this case, we noticed that (Chat)GPT was able to correctly infer what was intended (e.g., \$cup\$ was correctly interpreted as $\$ \backslash$ cup\$), and therefore provided a safeguard against these types of errors.</p>
<h2>2. Guarding against encoding issues:</h2>
<p>We presented clear instructions to each author who prompted (Chat)GPT on how to record and save the data in order to avoid any file encoding issues. In the end, all JSON files were inspected and streamlined to Unicode.</p>
<h2>3. Guarding against unfair comparisons:</h2>
<p>Clear instructions were given to all authors that used (Chat)GPT to ensure that the language model has, to the extent possible, an identical state and starts from a blank chat.</p>
<h2>4. Guarding against missing data and copy-paste errors:</h2>
<p>Given a lack of API access in the early stages of our investigation (see Appendix C), there was a fair amount of data being copied from (Chat)GPT. To mitigate any copy-paste errors, several passes over the entire dataset, as well as automatic checks, were made to look, e.g., for potential inconsistencies, missing timestamps, and outputs not matching the prompts.</p>
<h2>5. Guarding against other unforeseen errors:</h2>
<p>Random samples: Random samples $(&lt;10)$ were drawn from each dataset, and a second assessor reviewed the rating. If deemed problematic, the original assessors were asked to re-evaluate.</p>
<p>Statistical checks: Additional statistical checks were carried out as plausibility checks to make sure no other unforeseen errors occurred: If prompts deviated from the average length on that dataset, they were flagged and the output was manually inspected, and, if deemed necessary, a re-evaluation was carried out.</p>
<p>We are aware that these measures are not exhaustive, but given a fixed time budget, we considered them the most feasible.</p>
<h2>B. 6 Labeling Effort</h2>
<p>The evaluation was carried out by a subset of the authors of this paper who have substantial mathematical expertise, ranging from master's degrees in mathematics to postdoc-level and professor-level positions</p>
<p>at departments of mathematics. Assignment of prompts was done based on difficulty, with more senior mathematicians having received more difficult prompts. No third parties were involved.</p>
<p>Each of the 709 prompts of the GHOSTS dataset was evaluated on both the 9-January-2023 and 30-January-2023 version of ChatGPT; an additional 24 prompts were used to test the effect of prompt engineering on a single type of subdataset, see Appendix D.1. We further evaluate GPT-4 on the 170 prompts of the miniGHOSTS dataset. This amounts to a total of 1636 prompt evaluations of advanced mathematics, performed by graduate-level researchers.</p>
<p>We like to mention that our effort has occasionally unearthed small inconsistencies in existing datasets: For example, the "MATH Counting and Probability" file, which was sourced from the larger MATH dataset [11], contains the prompt "What is the value of $101^{3}-3 \cdot 101^{2}+3 \cdot 101-1$ ?", which is neither about counting, nor about probability, but arithmetic (our MSC codes allow users to find such examples).</p>
<h1>C Details on (Chat)GPT</h1>
<p>GPT-4, launched on 1st March 2023, is the latest model of the GPT lineage [6], being the successor of various versions of ChatGPT, the first of which was launched on 30 November 2022 [10]. These are all based on InstructGPT, which in turn is based on a trained GPT-3 [25], and fine-tuned using reinforcement learning with human feedback [47].</p>
<p>We note that already for models that predate (Chat)GPT, such as InstructGPT, where research articles and model cards [48] have been released, full reproducibility is not possible since the code and exact datasets have not been released. Furthermore, it was confirmed by OpenAI employees that for some of their models, launched prior to 30 November, a slight mismatch exists between the trained model that is accessible via the OpenAI web interface and the model referred to in the official paper [49]. This indicates how essential it is to document carefully which model our analysis pertains to and how we have accessed it. In our dataset, we have accordingly included time stamps for each prompt in order to be able to track, based on information provided by OpenAI, any changes in (Chat)GPT's version that have occurred.</p>
<p>We have exclusively used the GUI web interface to carry out the evaluation. This was necessary for consistency reasons, since at the beginning of our evaluation, API access was not yet widely available. At the time of writing, API access to GPT-4 is still limited, and a waitlist is employed, which made the use of the GUI web interface a necessity for GPT-4 [50]). We note that there exist no official documents that link the GUI web interface to the different model versions and possible model settings from the API. The 9-January-2023 and 30-January-2023 ChatGPT versions we evaluated are likely to be earlier instances of the newer model gpt-3.5-turbo-0301. This model itself is a "snapshot of gpt-3.5-turbo from March 1st, 2023" that will not receive further updates [51]. For GPT-4, at the time of writing, there exist four models gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314 that can be used for the chat completion API endpoint [52]. Additionally, for all models, there exist various settings when using the chat completion API endpoint, such as temperature or presence_penalty that influence the models' output, which cannot be controlled via the GUI web interface. It is also not known which values of these settings are used for the GUI web version. The only version identifier in the GUI web version is a generic "model version" link at the bottom of the page that links to the release notes [10]. The 9-January-2023 and 30-January-2023 model versions that we evaluated are the ones presented in the release notes [10] at the respective time.</p>
<h2>D Further Results</h2>
<h2>D. 1 Prompt Engineering</h2>
<p>One interesting finding of our study is related to performing prompt engineering on mathematical questions. Prompt engineering was solely carried out on questions from the Olympiad-Problem-Solving subdataset, and prompt-engineered questions consist of lists consisting of two JSON objects. These lists contain the original question, that was not prompt-engineered, as well as the prompt-engineered question. The latter question is identified as it contains the string <prompt engineered> as the value in the comment key. These lists containing prompt-engineered questions are in the same hierarchy in the JSON file as the other questions from the subdataset.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Effect of prompt engineering on the rating (left) and the error codes (right) for the 9-January-2023 model.</p>
<p>About 20% of the questions were prompt-engineered: ChatGPT was additionally instructed to proceed either step-by-step or the mathematical task was formulated in a more explicit way, i.e., by adding "Prove that..." or "Show that..." to the prompt. Instructing ChatGPT to proceed step-by-step is a type of engineering that is recommended by OpenAI in their cookbook to improve reliability<sup id="fnref:18"><a class="footnote-ref" href="#fn:18">2</a></sup>. As a result of prompt engineering, for the 9-January-2023 version of ChatGPT, the number of wrong statements and computations (i.e., error codes e2, e3, and e4) decreased, while the number of errors rooted in faulty logic (i.e., error code e5) actually increased. Overall, prompt engineering improves the average rating only slightly, see Figure 3.</p>
<p>For the questions from Olympiad-Problem-Solving that were selected for the miniGHOSTS dataset, we allow to sample from the entire Olympiad-Problem-Solving subdataset, since the goal of miniGHOSTS is not to measure prompt-engineering effects. Therefore, some of the questions in the miniGHOSTS version of the Olympiad-Problem-Solving subdataset contain prompt-engineered questions. The <prompt engineered> string was therefore removed from the comments in the miniGHOSTS dataset.</p>
<h3>D.2 Qualitative Analysis of Subdatasets on ChatGPT 9-January-2023</h3>
<p>In this section, we go through common mistakes performed by ChatGPT, as well as notable observations regarding the output, one subdataset at a time. We focus on the 9-January-2023 version, see Section D.5 for more information regarding the other version. We note that the output of (Chat)GPT (and, generally, LLMs) is stochastic and therefore may differ on the same prompt. Nonetheless, clear trends can be observed, which we describe here. Individual outputs can be found in Appendix D.6.</p>
<p><strong>Grad-Text</strong> ChatGPT, version 9-January-2023, performed best on simple set-theory and logic questions (the first chapter from the book <em>Topology</em> by J. Munkres [34]), which is reflected in its rating, see Figure 1. On the rest of the books, it performed substantially worse. Because of the confidence (high) with which it outputs the answer, the use of ChatGPT, version 9-January-2023, is particularly deceiving in this use-case since it may be intensively used by students studying these subjects.</p>
<p><strong>Holes-in-Proofs</strong> ChatGPT, version 9-January-2023, correctly recognized most well-known results or concepts (e.g., filling in the mean-value theorem, given a proof that lacked a reference to it). However, the ability of ChatGPT to execute algebraic manipulations is surprisingly inconsistent. In some cases, ChatGPT executes complicated symbolic tasks with ease; in other cases, it fails on simple arithmetic operations or rearranging terms. The mistakes do not seem to correlate with the complexity of the algebraic expression. When ChatGPT makes an algebraic mistake, it mostly carries over this mistake reliably to the rest of the computation.</p>
<p>Olympiad-Problem-Solving On this subdataset, ChatGPT, version 9-January-2023, performed the poorest. From a mathematical point of view, these questions were also by far the most difficult, as they can pose difficulties even to professional mathematicians. A score of 3 was awarded when the answer started to show promise. However, $75 \%$ of the scores are 2 because the answer does not show any promise. No rating of 5 was awarded, and only one rating of 4 was achieved. This version of ChatGPT had a tendency to try and solve many questions using induction arguments. While this is not necessarily false, this was very far from the solutions given in the book, and this version's inductive proofs were easily seen to contain mistakes. In addition, ChatGPT often had difficulty understanding unconventional puzzles. For example, in the questions involving changing the color of squares on a chessboard, the solution offered by ChatGPT did not cover an $8 \times 8$ chessboard. Sometimes it tried to solve the problem by changing only 5 squares, far from the 32 required. Similarly, the 9-January-2023 version of ChatGPT struggled to respect unusual constraints in the questions, resulting in 8 e6 errors, the highest number of e6 errors out of all subdatasets. In some cases where the problem seemed to require complicated mathematics but was actually solvable by elementary techniques, ChatGPT did not spot this but instead referred to the general theory of, e.g., diophantine equations. Interestingly, ChatGPT would sometimes say, e.g., that the question could be solved with these means but that this was hard, so the confidence score was downgraded in these cases to medium or low.</p>
<p>Symbolic-Integration The 9-January-2023 version of ChatGPT was dominated by systems that were trained specifically to solve integration problems [12]. In a number of instances, this version got the structure of terms right (for example, the number of summands in the output, as well as where factors had to be placed before summands), but it failed at concrete computations. Even very simple examples were not correct. For example, the antiderivative of $x \mapsto x^{2} / 2$ is evaluated to $x \mapsto x^{3} / 3+C$, where $C$ is a constant of integration (the correct answer being $x \mapsto x^{3} / 6+C$ ). For a number of prompts, this version claims there is no closed-form solution for the integral with complete confidence when, in fact, there is a solution; only integrals that have an elementary antiderivative are in this dataset.</p>
<p>MATH On the questions related to Algebra and Probability theory, the 9-January-2023 version of ChatGPT got the reasoning often correctly. However, the most common type of error was e4, occurring $36 \%$ of the time (in total 62 times). This version of ChatGPT may struggle when confronted with standard operations, such as inverting fractions, least common multiples, and changing the sign of numbers when moving them from one side of the equal sign to the other. Often, in these questions, a correct solution requires performing multiple operations in sequence. In such cases, most often, at least one operation was wrong. This prevented the model from getting a rating of 5 on the output, which was only achieved for $29 \%$ of the questions.</p>
<p>Search-Engine-Aspects On the Search-Engine-Aspects file, the 9-Januar-2023 version of ChatGPT knew almost all the theorems that it was asked at a basic level but made mistakes when stating them. When it came to listing other results required for the proofs, this version typically requested way more than the necessary theory occasionally even results that only follow from the theorem which was asked for (error code e5_5). On the Definition Retrieval file, this version had quite a good performance: it recited most definitions correctly. It sometimes got confused when being asked about distributions in the sense of elements of the dual space of test functions. ChatGPT, version 9-January-2023, strongly favors the notion of distributions in the stochastic sense. Similarly, for the adjective "closed", where it chose to pick the context of algebra (instead of topology) and interpreted it to mean "algebraically closed". On the Reverse Definition Retrieval file, this version had the strongest performance, being able to recover most definitions from their descriptions, with an average rating of 4.30 (standard deviation 1.14). This indicates the usefulness of ChatGPT as a general-purpose mathematical search engine. This subdataset is also the simplest from a mathematical point of view since no logical thinking is required, but only a name needs to be found.</p>
<h1>D. 3 (Chat)GPT's Confidence</h1>
<p>(Chat)GPT is usually very confident, unlike other LLMs that we have experimented with. As an illustrative example, consider the following prompt testing the sensitivity to $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$-encoded mathematics vs. Unicode-encoded mathematics:</p>
<p>Here are two statements: "Let ( $\mathrm{X}, \mathrm{A}, \mu$ ) be a measure space. Show that the signed measures $\lambda$ : A $\rightarrow$ R form a Banach space."; "Let $\$(X, A, \backslash m u) \$$ be a measure space. Show that the signed measures $\$ \backslash$ lambda:A \rightarrow R\$ form a Banach space." Are they equivalent?</p>
<p>The response by ChatGPT is not phrased in order to show any nuance in terms of confidence (which is typical, even if ChatGPT is wrong):</p>
<p>Yes, the two statements are equivalent. They both express the same mathematical idea but use different notation and formatting conventions. The first statement uses parentheses to enclose the elements of the measure space and regular letters to represent the functions, while the second statement uses subscripted parentheses and uses the LaTeX formatting to represent the elements of the measure space and the function.</p>
<p>The response by Codex [53], another model that we briefly tested (but whose scope would have exceeded that of a single conference article) gives a cautions response and, unlike ChatGPT, is capable of voicing doubt:</p>
<p>I think they are equivalent, but I am not sure.</p>
<h1>D. 4 Figures of ChatGPT's Performance (version 9-January-2023)</h1>
<p>In this section, we collect figures that extend the discussion in the main body and provide further views on the data and descriptive statistics.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average rating over mathematical fields for the 9-January-2023 version of ChatGPT on GHOSTS. The color depicts the occurrence of each MSC code, and only MSC codes that have at least 5 occurrences are shown. Note that the ranking is not indicative of the complexity of the fields since we do not use equally complicated exercises for all fields. The error bars represent $95 \%$ confidence intervals.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ They are 37 and 3 .
${ }^{15}$ They are 2,37 , and 3 .
${ }^{16}$ They are $2,3,5,7$, and 11 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:18">
<p>Sithub.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md&#160;<a class="footnote-backref" href="#fnref:18" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
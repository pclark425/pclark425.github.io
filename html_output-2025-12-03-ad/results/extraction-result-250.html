<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-250 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-250</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-250</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-281203259</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.01113v3.pdf" target="_blank">Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the incremental, internal problem-solving process of language models (LMs) with arithmetic multi-hop reasoning as a case study. We specifically investigate when LMs internally resolve sub/whole problems through first reading the problem statements, generating reasoning chains, and achieving the final answer to mechanistically interpret LMs'multi-hop problem-solving process. Our experiments reveal a systematic incremental reasoning strategy underlying LMs. They have not derived an answer at the moment they first read the problem; instead, they obtain (sub)answers while generating the reasoning chain. Therefore, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e250.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e250.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One of the primary models analyzed with token-layer linear probes and activation patching; used to demonstrate that arithmetic subanswers are computed during chain-of-thought generation and that hidden states exhibit a strong recency bias causally linked to outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-hop addition and subtraction (chain of single-digit v = d, v = d ± d, or v = d ± v style assignments and queries)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>single-digit numbers (digits 0–9), multi-step problems up to 5 complexity levels (Levels 1–5) with varying #Steps, #Stack, and distractors</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>chain-of-thought (forced CoT) generation; linear probing per token position and layer; activation patching (sliding-window/grid patching by equation and layer); greedy decoding for patched target token(s)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Final task exact-match accuracy ≈ 100% (including chain and final answer). Probing results: Acc_before_CoT (Acc ≺CoT) typically low (examples in Table 2: 0.17–0.51 for various variables), Acc_during_CoT (Acc ≻CoT) high (often near 1.0 for required variables). t* (time when variable becomes linearly extractable) for computed variables usually > 0 (i.e., after CoT begins).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Models predominantly compute necessary subanswers while generating the chain-of-thought (Talk-to-Think). Hidden states during CoT causally determine subanswer and final-token outputs (activation patching changes outputs when applied to recent grids). There is a strong recency bias: the model relies on immediately preceding computation when producing a (sub-)answer. First-pass (input) sometimes contains incidental/simple computations but these are typically not reused during CoT, indicating redundancy. Distractor variables are often not encoded in a simple linear-extractable form.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Across sizes of the Qwen family (7B, 14B, 32B) and other models, final task accuracy remained ≈100%; however, probe extractability patterns are consistent across sizes. Smaller models (e.g., Llama3.2 3B in cross-model comparisons) sometimes fail to reach high probe thresholds (N/A at higher τ), indicating weaker or less linearly-decodable intermediate representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Variables that are distractors tend not to be linearly encoded (lower probe accuracy). Some smaller models do not reach probe thresholds for intermediate variables (N/A). The model shows redundancy: computations occasionally repeated in CoT rather than reusing first-pass results. No large-scale failure on final answers in this synthetic single-digit setting (final accuracy ~100%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Probing before vs during CoT (Acc ≺CoT vs Acc ≻CoT); clean (no patch) vs patched hidden states in activation-patching experiments; across model sizes (multiple Qwen and other models).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Qwen2.5-7B (and other tested LLMs) typically do not finalize arithmetic subanswers during the initial input pass but instead compute required subanswers during chain-of-thought generation; those CoT hidden states are causally linked to outputs and show a strong recency bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e250.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e250.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-model ensemble (study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate of tested LLMs (Qwen2.5 family, Qwen2.5-Math, Yi1.5 family, Llama3.1/3.2, Mistral-Nemo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-model evaluation on the synthetic multi-hop single-digit arithmetic dataset showing near-perfect final-answer accuracy but consistent internal dynamics (Talk-to-Think, recency bias) revealed by probing and patching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple models (Qwen2.5, Qwen2.5-Math, Yi1.5, Llama3.1, Llama3.2, Mistral-Nemo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (3B, 7B, 8B, 9B, 12B, 14B, 32B as reported in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-hop arithmetic (addition and subtraction only) with variable assignments and dependency resolution</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>single-digit numbers (0–9); multi-step composition up to 5-level tasks with distractors and stacked unresolved variables</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>chain-of-thought prompting (forced CoT output), token-layer linear probing to detect when variable values are extractable, coarse-grid activation patching to test causal effects</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Final exact-match accuracy on synthetic tasks ≈ 100% for all listed models (Table A.1). Probing across models shows a general pattern: Acc ≺CoT low and Acc ≻CoT high, indicating intermediate values are typically only extractable during CoT; some smaller models show more N/A cases at high probe thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Consistent mechanistic pattern across models: arithmetic is performed (or at least represented in a linearly-decodable way) primarily during CoT generation rather than during the first pass, and CoT hidden states have a temporally-local causal influence (recency bias). Prior works cited in the paper additionally indicate linear/monotonic numeric encodings in hidden states, which this study complements by showing temporal emergence during multi-hop CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Despite near-100% final accuracy across sizes, probe-extractability and robustness to higher probe thresholds can degrade for smaller models (e.g., Llama3.2 3B shows more N/A at higher τ), but the Talk-to-Think pattern is broadly consistent across sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Smaller models show weaker linear-extractable intermediate representations; distractor variables often not linearly encoded and have low probe accuracy; redundancy in computation (first-pass incidental computations not reused) can be observed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Across-model comparisons (different sizes and families), before-vs-during CoT probe accuracy, and patched vs clean runs in activation patching experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Across multiple LLM families and sizes, models reliably produce correct multi-hop single-digit arithmetic answers, but the internal computation of subanswers tends to occur during chain-of-thought generation rather than during the initial input-reading pass, and these CoT states causally drive outputs with a strong recency bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e250.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e250.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear probe + activation patching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-layer linear probing combined with coarse-grid activation patching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analytical methodology used to locate when variable values appear in hidden states (linear probes per token and layer) and to test causal influence of those hidden states on generated tokens via activation patching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to multiple models in this study</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>used to analyze multi-hop addition/subtraction problems (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>single-digit multi-hop tasks (Levels 1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>linear probing (single linear classifier per token position and layer, trained to predict variable values); activation patching (replace hidden states from a different instance on a coarse grid partitioned by equations × groups of layers), greedy decode target token when patched</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Probes achieved high accuracy for many variables during CoT (threshold τ used = 0.90 in main analyses), enabling computation of t* (first token position where accuracy > τ). Activation patching success rates were substantial only when patching grids aligned temporally/structurally with target tokens (demonstrating recency), and low otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Probing reveals when variables become linearly-decodable in the hidden states (typically during CoT). Activation patching provides causal evidence: replacing recent CoT hidden states can change subsequent CoT tokens and final answers, supporting that CoT hidden states are not merely post-hoc explanations but reflect ongoing computation. The coarse-grid patching results show that causal influence is localized to recent equation-related hidden states (recency bias) and that internal computations may be redundant across passes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limitations of probing interpretability acknowledged: linear probes can miss nonlinearly-encoded information; patching effects are localized and sensitive to grid/layer partitioning; smaller models may not reach probe thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Acc ≺CoT vs Acc ≻CoT (probing); clean vs patched runs (activation patching); varying probe threshold τ (0.85–0.95) reported in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Combining token-layer linear probes and activation patching demonstrates that intermediate arithmetic subanswers are formed during CoT and that those CoT hidden states are causally important for produced reasoning chains and final answers, with a strong temporal/recency localization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models encode the value of numbers linearly <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Monotonic representation of numeric attributes in language models <em>(Rating: 2)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 1)</em></li>
                <li>Towards best practices of activation patching in language models: Metrics and methods <em>(Rating: 1)</em></li>
                <li>Do large language models latently perform multi-hop reasoning? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-250",
    "paper_id": "paper-281203259",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Qwen2.5-7B",
            "name_full": "Qwen2.5 (7B)",
            "brief_description": "One of the primary models analyzed with token-layer linear probes and activation patching; used to demonstrate that arithmetic subanswers are computed during chain-of-thought generation and that hidden states exhibit a strong recency bias causally linked to outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-hop addition and subtraction (chain of single-digit v = d, v = d ± d, or v = d ± v style assignments and queries)",
            "number_range_or_complexity": "single-digit numbers (digits 0–9), multi-step problems up to 5 complexity levels (Levels 1–5) with varying #Steps, #Stack, and distractors",
            "method_or_intervention": "chain-of-thought (forced CoT) generation; linear probing per token position and layer; activation patching (sliding-window/grid patching by equation and layer); greedy decoding for patched target token(s)",
            "performance_result": "Final task exact-match accuracy ≈ 100% (including chain and final answer). Probing results: Acc_before_CoT (Acc ≺CoT) typically low (examples in Table 2: 0.17–0.51 for various variables), Acc_during_CoT (Acc ≻CoT) high (often near 1.0 for required variables). t* (time when variable becomes linearly extractable) for computed variables usually &gt; 0 (i.e., after CoT begins).",
            "mechanistic_insight": "Models predominantly compute necessary subanswers while generating the chain-of-thought (Talk-to-Think). Hidden states during CoT causally determine subanswer and final-token outputs (activation patching changes outputs when applied to recent grids). There is a strong recency bias: the model relies on immediately preceding computation when producing a (sub-)answer. First-pass (input) sometimes contains incidental/simple computations but these are typically not reused during CoT, indicating redundancy. Distractor variables are often not encoded in a simple linear-extractable form.",
            "performance_scaling": "Across sizes of the Qwen family (7B, 14B, 32B) and other models, final task accuracy remained ≈100%; however, probe extractability patterns are consistent across sizes. Smaller models (e.g., Llama3.2 3B in cross-model comparisons) sometimes fail to reach high probe thresholds (N/A at higher τ), indicating weaker or less linearly-decodable intermediate representations.",
            "failure_modes": "Variables that are distractors tend not to be linearly encoded (lower probe accuracy). Some smaller models do not reach probe thresholds for intermediate variables (N/A). The model shows redundancy: computations occasionally repeated in CoT rather than reusing first-pass results. No large-scale failure on final answers in this synthetic single-digit setting (final accuracy ~100%).",
            "comparison_baseline": "Probing before vs during CoT (Acc ≺CoT vs Acc ≻CoT); clean (no patch) vs patched hidden states in activation-patching experiments; across model sizes (multiple Qwen and other models).",
            "key_finding": "Qwen2.5-7B (and other tested LLMs) typically do not finalize arithmetic subanswers during the initial input pass but instead compute required subanswers during chain-of-thought generation; those CoT hidden states are causally linked to outputs and show a strong recency bias.",
            "uuid": "e250.0",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Multi-model ensemble (study)",
            "name_full": "Aggregate of tested LLMs (Qwen2.5 family, Qwen2.5-Math, Yi1.5 family, Llama3.1/3.2, Mistral-Nemo)",
            "brief_description": "Cross-model evaluation on the synthetic multi-hop single-digit arithmetic dataset showing near-perfect final-answer accuracy but consistent internal dynamics (Talk-to-Think, recency bias) revealed by probing and patching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple models (Qwen2.5, Qwen2.5-Math, Yi1.5, Llama3.1, Llama3.2, Mistral-Nemo)",
            "model_size": "various (3B, 7B, 8B, 9B, 12B, 14B, 32B as reported in tables)",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-hop arithmetic (addition and subtraction only) with variable assignments and dependency resolution",
            "number_range_or_complexity": "single-digit numbers (0–9); multi-step composition up to 5-level tasks with distractors and stacked unresolved variables",
            "method_or_intervention": "chain-of-thought prompting (forced CoT output), token-layer linear probing to detect when variable values are extractable, coarse-grid activation patching to test causal effects",
            "performance_result": "Final exact-match accuracy on synthetic tasks ≈ 100% for all listed models (Table A.1). Probing across models shows a general pattern: Acc ≺CoT low and Acc ≻CoT high, indicating intermediate values are typically only extractable during CoT; some smaller models show more N/A cases at high probe thresholds.",
            "mechanistic_insight": "Consistent mechanistic pattern across models: arithmetic is performed (or at least represented in a linearly-decodable way) primarily during CoT generation rather than during the first pass, and CoT hidden states have a temporally-local causal influence (recency bias). Prior works cited in the paper additionally indicate linear/monotonic numeric encodings in hidden states, which this study complements by showing temporal emergence during multi-hop CoT.",
            "performance_scaling": "Despite near-100% final accuracy across sizes, probe-extractability and robustness to higher probe thresholds can degrade for smaller models (e.g., Llama3.2 3B shows more N/A at higher τ), but the Talk-to-Think pattern is broadly consistent across sizes.",
            "failure_modes": "Smaller models show weaker linear-extractable intermediate representations; distractor variables often not linearly encoded and have low probe accuracy; redundancy in computation (first-pass incidental computations not reused) can be observed.",
            "comparison_baseline": "Across-model comparisons (different sizes and families), before-vs-during CoT probe accuracy, and patched vs clean runs in activation patching experiments.",
            "key_finding": "Across multiple LLM families and sizes, models reliably produce correct multi-hop single-digit arithmetic answers, but the internal computation of subanswers tends to occur during chain-of-thought generation rather than during the initial input-reading pass, and these CoT states causally drive outputs with a strong recency bias.",
            "uuid": "e250.1",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Linear probe + activation patching",
            "name_full": "Token-layer linear probing combined with coarse-grid activation patching",
            "brief_description": "Analytical methodology used to locate when variable values appear in hidden states (linear probes per token and layer) and to test causal influence of those hidden states on generated tokens via activation patching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied to multiple models in this study",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "used to analyze multi-hop addition/subtraction problems (as above)",
            "number_range_or_complexity": "single-digit multi-hop tasks (Levels 1–5)",
            "method_or_intervention": "linear probing (single linear classifier per token position and layer, trained to predict variable values); activation patching (replace hidden states from a different instance on a coarse grid partitioned by equations × groups of layers), greedy decode target token when patched",
            "performance_result": "Probes achieved high accuracy for many variables during CoT (threshold τ used = 0.90 in main analyses), enabling computation of t* (first token position where accuracy &gt; τ). Activation patching success rates were substantial only when patching grids aligned temporally/structurally with target tokens (demonstrating recency), and low otherwise.",
            "mechanistic_insight": "Probing reveals when variables become linearly-decodable in the hidden states (typically during CoT). Activation patching provides causal evidence: replacing recent CoT hidden states can change subsequent CoT tokens and final answers, supporting that CoT hidden states are not merely post-hoc explanations but reflect ongoing computation. The coarse-grid patching results show that causal influence is localized to recent equation-related hidden states (recency bias) and that internal computations may be redundant across passes.",
            "performance_scaling": null,
            "failure_modes": "Limitations of probing interpretability acknowledged: linear probes can miss nonlinearly-encoded information; patching effects are localized and sensitive to grid/layer partitioning; smaller models may not reach probe thresholds.",
            "comparison_baseline": "Acc ≺CoT vs Acc ≻CoT (probing); clean vs patched runs (activation patching); varying probe threshold τ (0.85–0.95) reported in appendices.",
            "key_finding": "Combining token-layer linear probes and activation patching demonstrates that intermediate arithmetic subanswers are formed during CoT and that those CoT hidden states are causally important for produced reasoning chains and final answers, with a strong temporal/recency localization.",
            "uuid": "e250.2",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models encode the value of numbers linearly",
            "rating": 2,
            "sanitized_title": "language_models_encode_the_value_of_numbers_linearly"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Monotonic representation of numeric attributes in language models",
            "rating": 2,
            "sanitized_title": "monotonic_representation_of_numeric_attributes_in_language_models"
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 1,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        },
        {
            "paper_title": "Towards best practices of activation patching in language models: Metrics and methods",
            "rating": 1,
            "sanitized_title": "towards_best_practices_of_activation_patching_in_language_models_metrics_and_methods"
        },
        {
            "paper_title": "Do large language models latently perform multi-hop reasoning?",
            "rating": 2,
            "sanitized_title": "do_large_language_models_latently_perform_multihop_reasoning"
        }
    ],
    "cost": 0.013290749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning
8 Sep 2025</p>
<p>Keito Kudo 
Tohoku University</p>
<p>Yoichi Aoki 
Tohoku University</p>
<p>Tatsuki Kuribayashi tatsuki.kuribayashi@mbzuai.ac.ae 
Tohoku University</p>
<p>Shusaku Sone 
Tohoku University</p>
<p>Masaya Taniguchi masaya.taniguchi@riken.jp 
Tohoku University</p>
<p>Ana Brassard ana.brassard@riken.jp 
Tohoku University</p>
<p>Keisuke Sakaguchi keisuke.sakaguchi@tohoku.ac.jp 
Tohoku University</p>
<p>Kentaro Inui kentaro.inui@mbzuai.ac.ae 
Tohoku University</p>
<p>Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning
8 Sep 202509560A75CC34A33AC35178896267E013arXiv:2412.01113v3[cs.CL]
RIKEN, 3 MBZUAI  {keito.kudo.q4, youichi.aoki.p2, sone.shusaku.r8}@dc.tohoku.ac.jp,</p>
<p>Introduction</p>
<p>An explanation may be produced in two modes: as a post hoc explanation to a predetermined conclusion (Think-to-Talk) or by the process of reaching a conclusion while explaining (Talk-to-Think).An analogy applies to large language models (LLMs) using chain-of-thought (CoT; Wu et al., 2023a) reasoning: is generated CoT reasoning chain a posthoc explanation, or does it reflect real-time stepby-step solving?This question is particularly relevant to the (mechanistic) interpretablity of LLMs -how models incrementally resolve the multi-hop problem, in other words, how information internally flows to reach the final decision.</p>
<p>In this study, we identify such internal reasoning patterns of LLMs.There may be multiple possible internal strategies for LLMs under the CoT scenario, i.e., first feed the problem statements and then have models generate full reasoning chains.One presumably hard strategy for LLMs would be to reach the final answer even during the first pass of problem statements before CoT generation, and then while generating CoT reasoning chains, the  Using linear probes, we investigated at which time during the LLM's problem-solving process it is possible to determine the values of each variable, illustrating the model's problem-solving process.Our analysis indicates that LMs come up with (sub)answers during CoT (second pass).This conclusion is also consistent with the findings from the causal experiments in § 5.</p>
<p>model simply refers to their predetermined answers as a post-hoc explanation (think-to-talk mode).The opposite will be that models do nothing during the first pass of the problem statements and start solving the problem after CoT generation begins (talk-to-think mode).Such different internal mechanisms can not be distinguished by just observing the model outputs; rather, one has to interpret and intervene in their internal representations during their multi-hop CoT-style reasoning.</p>
<p>In our first experiments, we apply linear probes to model internals at each layer at each timestep to determine when answers are reached internally.We prepared controlled testbeds of symbolic arithmetic reasoning tasks and observed whether trained probes could accurately predict and control the values of specific variables (Figure 1).By comparing accuracies across each timestep, one can observe at which point models internally start being informative to the probes, illustrating the model's internal reasoning flow.</p>
<p>The results reveal common patterns across models.They have not derived an answer at the moment they first read the problem (first-pass); instead, they obtain (sub)answers while generating the reasoning chain.These tendencies are consistent and systematic across our different testbeds.</p>
<p>Based on the above finding, we further conducted causal intervention analyses to clarify the causal relationship between the model's internal representations and the final answer ( § 5).We found that, when generating the reasoning chain, the model's internal problem-solving process exhibits a recency bias, relying heavily on the information in the immediately preceding portion of the chain.In other words, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation. 1</p>
<p>Related Work</p>
<p>Multi-stage human incremental language comprehension.Humans sometimes make several attempts with different strategies or mindsets, particularly when resolving a complex task.A common view may be, for example, that humans first adopt a shallow, fast solution, and once it fails, switch to a more expensive, presumably accurate one.Such a multi-stage processing is even related to the recent debate on the cognitive plausibility of LLMs in computational psycholinguistics (Oh and Schuler, 2023;Shain et al., 2024;Kuribayashi et al., 2024;Gruteke Klein et al., 2024).LLMs have now been criticized because they can not estimate human cognitive costs incurred, possibly by the multi-stage nature of human sentence processing (i.e., reanalysis) (van Schijndel and Linzen, 2021;Huang et al., 2024); humans re-read the sentence again from earlier points as an additional trial when facing difficulties in comprehension, but LLMs do not explicitly have such a multi-stage mechanism.Although this study is not focused on sentence processing or LMs' humanlikeness, one critical question in this line is whether and how LLMs switch their reasoning strategy for the same problem through multiple trials, and how to track their dynamic internal states.In our case, we simplify the setting to forced-decode the common chain-of-thought style format as two-time trials of the problem, where an LM first reads the problem statement and then (re-)analyzes the problem while generating the reasoning chain as well as copying the problem statement again.Here, we analyze what kind of process LLMs perform, particularly in the first pass of the problem statement, and how their initial processing is related to their later generation of reasoning chain and answer.Our results suggest some surprising behaviors; models system-1 Code and data will be made available upon acceptance.</p>
<p>atically come up with answers to simple subproblems in the first pass, but these computations are not reused in the second pass of CoT-style reasoning, suggesting some redundancy in their internal reasoning.</p>
<p>Interpreting multi-hop reasoning in language models.Interpreting internal mechanisms of LMs has been actively investigated (Conneau et al., 2018;Tenney et al., 2019;Niven and Kao, 2019;nostalgebraist, 2020;Geva et al., 2023;Lieberum et al., 2024;Ghandeharioun et al., 2024;Ferrando and Voita, 2024).They revealed, for example, specialized attention heads responsible for specific operations (Cabannes et al., 2024) or decisionmaking, copying, and induction (Dutta et al., 2024).With a more concrete example, Yang et al. (2024c) showed that, even during the first pass of the roblem statements such as The mother of the singer of Thriller is ___, language models first resolve a bridge entity, Stevie Wonder in this case, then identify the final answer.This study is more focused on the difference between the first pass of the problem statements (before CoT generation) and their second pass involving explicit problem solving (while CoT generation).</p>
<p>Arithmetic representations in LLMs.How models handle numerical information has also been closely studied.For instance, Heinzerling and Inui (2024) used partial least squares regression (Wold et al., 2001) to demonstrate that numeric attributes, such as birth years and population numbers, are encoded as monotonic representations in the activation space of LLMs and can be manipulated with interventions.In turn, Stolfo et al. (2023) showed that, in autoregressive LMs, the operations and numerical information necessary for solving quantitative reasoning are processed in the lower layers of the model, and these results are used by the attention layers to predict the final calculation outcomes.Zhu et al. (2025) studied the representation of numbers in language models' hidden states during single-hop arithmetic tasks (e.g., What is the sum of 12 and 34?).Their analysis revealed that numerical information is encoded linearly within the hidden states and demonstrated that individual digits could be manipulated independently.In this study, we add to this literature by introducing incremental arithmetic problem solving, i.e., what numerical information is contained in the model's hidden states at each time step of multi-hop arithmetic reasoning.Model interpretability methods.Linear probing (Alain and Bengio, 2017b) is one of the representative methods for analyzing the internal representations of neural models-a small model predicts a specific feature from them, thereby determining whether the input contains information about that feature.In this study, we use them to derive the models' intermediate answers.The causality with the model's output can be further verified by examining if a model's predictions change when the hidden states are intervened (Li et al., 2023;Wu et al., 2023b).One representative intervention method is activation patching (Vig et al., 2020;Meng et al., 2022;Zhang and Nanda, 2024), where hidden states obtained from one model instance are transplanted onto another during inference to change its predictions.Such techniques can be applied as a way to control model behavior in practical scenarios such as mitigating inherent biases (Zhao et al., 2019;Ganguli et al., 2023;Yang et al., 2024b).Here, we employ activation patching to validate the plausibility of the probing results.
Level INPUT OUTPUT #Step #Stack #Dist. 1 A = 1 + B −3 , B = 2 −2 ; A =? −1 A = 1 + B 0 , B = 2 1 , A = 1 + B 2 , A = 1 + 2 3 , A = 3 4 1 1 0 2 A = 2 + 3 −3 , B = 1 + A −2 ; B =? −1 B = 1 + A 0 , A = 2 + 3 1 , A = 5 2 , B = 1 + A 3 , B = 1 + 5 4 , B = 6 5 2 0 0 3 A = 1 + B −3 , B = 2 + 3 −2 ; A =? −1 A = 1 + B 0 , B = 2 + 3 1 , B = 5 2 , A = 1 + B 3 , A = 1 + 5 4 , A = 6 5 2 1 0 4 A = 1 + B −4 , B = 2 + 3 −3 , C = 4 + 5 −2 ; A =? −1 A = 1 + B 0 , B = 2 + 3 1 , B = 5 2 , A = 1 + B 3 , A = 1 + 5 4 , A = 6 5 2 1 1 5 A = 1 + B −4 , B = 2 + C −3 , C = 1 + 2 −2 ; A =? −1 A = 1 + B 0 , B = 2 + C 1 , C = 1 + 2 2 , C = 3 3 , B = 2 + C 4 , B = 2 + 3 5 , B = 5 6 , A = 1 + B 7 , A = 1 + 5 8 , A = 6 9 3 2 0
3 General settings</p>
<p>Arithmetic problems</p>
<p>We prepared a dataset of multi-hop arithmetic problems similarly to Kudo et al. (2023) and Yu (2025).</p>
<p>Each sample is a string of assignments (e.g., A=1) and operations (e.g., B=1+3 or B=1+A) ending with a query for a variable's value (e.g., B=?).We also defined five complexity levels, depending on (i) how many equations need to be resolved to reach the answer (#Step in Table 1), (ii) how many variables' values cannot be immediately resolved (and thus pended to a stack) in their first appearance when incrementally reading the problem from left to right (#Stack), (iii) and the number of unnecessary distractor equations (#Dist.).For example, in the Level 5 example in
v = d, v = d ± d, or v = d ± v.
We denote i-th variable to appear within an instance from the left as v i .E.g., in the Level 5 example in Table 1, v 1 = A, v 2 = B, and v 3 = C. 2 The value assigned to a variable v i is denoted as ${v i } ∈ D.</p>
<p>Generation rules.We ensure that ${v i } for any v i is also a single-digit number, and ${v i } is constant within the same instance (i.e., we exclude cases such as A=1+2,A=B+2,B=6 We denote a token position within the entire concatenated sequence x ⊕ z ⊕ y with t ∈ Z. t is relative to CoT; that is, t is zero where CoT begins, negative within the INPUT, and positive within the OUTPUT.Similarly, we assign an equation position t eq ∈ Z to each equation in the INPUT and OUTPUT (subscripts on the underlines in Table 1).</p>
<p>Probing</p>
<p>When do models solve (sub)problems in CoT-style reasoning?Do they (i) finalize reasoning during the INPUT stage, with the CoT as a post-hoc explanation (Think-to-Talk), or (ii) solve the task step-bystep during CoT generation (Talk-to-Think)?We address this by examining where the final answer, or the necessary information for it, emerges in the model's internal representations using linear probes.</p>
<p>Training settings for linear probes</p>
<p>We train a linear probe (Alain and Bengio, 2017a) for an l-layer LLM (l = 0 for the input embedding layer).To identify where the LLM solved a particular (sub)problem, we train a separate probe for each combination of token position t ∈ Z, layer depth l ∈ N, and v i ∈ Σ in each level of the problem.Specifically, given a model's ddimentional hidden state h t,l ∈ R d , the probing classifier f t,l,v i (•) : R d → D predicts ${v i }.That is, for each (t, l), we first obtained 10,000 of h t,l from training instances and then evaluated the accuracy of f t,l,v i (•) for each v i with 2,000 hidden states from test instances and the correct ${v i }.If a probe f t,l,v i (•) achieves high accuracy, this suggests that ${v i } is already computed at the corresponding position (t and l). Figure 2 illustrates the probing results with a Level 3 task, where, for example, the value of B can be extracted within INPUT and thus already computed before CoT begins.</p>
<p>The probe f t,l,v i (•) is a single linear transformation; that is, the probe is applied to h t,l as follows:
${v i } t,l, = f t,l,v i (h t,l ) = arg max D W t,l,v i h t,l + b t,l,v i ,(1)
where, W t,l,v i ∈ R |D|×d and b t,l,v i ∈ R |D| are the weight and bias parameters of the probe, respectively.The symbol • is used to refer to the model's estimate.We train the probes using stochastic gradient descent (Robbins, 1951) to minimize the cross entropy loss.The hyperparameters are listed in Table 5 in the appendix.</p>
<p>Evaluation metrics</p>
<p>The probing results from all the token positions t and layers l are aggregated as follows:
t * (v i ) = min{t | max l acc(t, l, v i ) &gt; τ } ,(2)
where acc(t, l, v i ) ∈ [0, 1] indicates the accuracy of a probing classifier f t,l,v i .The t * (v i ) ∈ Z indicates when was the first time the probing classifier achieved a reasonable accuracy above τ (= 0.90 in our study4 ) for the variable v i .As a more coarse but comprehensive value, we also report t * eq (v i ) indicating which equation t eq the t * (v i ) falls into.Given that the t (and t eq ) is relative to the CoT-beginning position, if t * (eq) (v i ) is negative, the value ${v i } is computed before CoT begins.This is the case for t * (i.e., B) in Figure 2, based on the spike around t eq =−2 in the upper line graph.</p>
<p>We also report two types of accuracy:
Acc ≺CoT (v i ) = max t&lt;0,l acc(t, l, v i ),(3)Acc ≻CoT (v i ) = max t≥0,l acc(t, l, v i ). (4)
Probes can predict both B and A; multi-step reasoning during CoT If Acc ≺CoT (v i ) is sufficiently high, ${v i } is resolved internally before CoT begins (Think-to-Talk mode).Conversely, if Acc ≺CoT (v i ) is low and Acc ≻CoT (v i ) is high, the answer is derived while performing CoT reasoning (Talk-to-Think mode).</p>
<p>Experimental results</p>
<p>Across task complexity levels.We first analyze Qwen2.5-7B(Qwen Team, 2024) across the five task levels.Table 2 shows t * eq for each variable as well as the lower bounds of t † eq , which can be computed with a greedy resolver of equations.In most cases, regardless of #Steps or task level, t * eq &gt; 0. The exceptions are v 2 in level 1 and v 3 in level 4. v 2 in level 1 has #Step = 0 and is a variable whose value requires no computation to derive.v 3 in level 4 is a distractor and is not required to derive the final answer.Therefore, we find that the variables that are required to derive the final answer and require computation are all solved after CoT begins.In summary, we find that the model solved all subproblems necessary to derive the final answer during CoT, and that the Talk-to-Think mode is dominant.</p>
<p>Variable</p>
<p>When (↓)
Acc. (↑) Level variable #Step t * eq t † eq ≺ CoT ≻ CoT 1 v1 (A) 1 4 −2 0.36 1 v2 (B) 0 −2 −2 1 1 2 v1 (A) 1 2 −3 0.49 1 v2 (B) 2 5 −2 0.21 1 3 v1 (A) 2 5 −2 0.18 1 v2 (B) 1 2 −2 0.50 1 4 v1 (A) 2 5 −3 0.17 1 v2 (B) 1 2 −3 0.48 1 v3 (C) 1 N/A −2 0.44 0.24 5 v1 (A) 3 9 −2 0.18 1 v2 (B) 2 6 −2 0.23 1 v3 (C) 1 3 −2 0.51 1
Table 2: The results of Qwen2.5-7B on the five levels.The t * eq is the time when the model comes up with the correct answer (see § 4.2).The t † eq column indicates the lower bound of t * eq score.The ≺ CoT and ≻ CoT scores correspond to the accuracies introduced in § 4.2.N/A indicates that the threshold τ was not exceeded at any position t.</p>
<p>Across models.We further analyzed ten models listed in Table 3  Table 3: Results for various models on the task Level 3. The t * column shows the token-wise time (described in § 4.2), and the other columns are the same as Table 2.</p>
<p>The t * and t * eq scores that are the same as their lower bounds are bolded.models, enhancing the generality of our obtained findings.These results are consistently observed across other tasks and irrespective of the threshold τ .For more details, see § A.2.</p>
<p>Analysis</p>
<p>Distractors.In task Level 4 (see Table 1), v 3 (C) is a distractor, that is, ${v 3 } is not necessary to derive the final answer.The models can infer this fact from the in context examples.According to Table 2, the Acc(v 3 ) in Level 4 was at most 44%, a relatively low accuracy.From this result, we can see that, unlike the variables required to derive the final answer, v 3 is not encoded in a simple form that can be extracted by a linear transformation alone.This suggests the possibility that the model employs an efficient internal mechanism that does not derive variables unnecessary for obtaining the final answer.It is also consistent with the finding that a Talk-to-Think mode, in which computation is performed during CoT, is dominant.</p>
<p>Causal interventions</p>
<p>Settings</p>
<p>Activation patching.We employ activation patching (Vig et al., 2020;Meng et al., 2022;Zhang and Nanda, 2024), which is a widely adopted technique for causal intervention analysis.To inspect the causal relationship between specific hidden states h t,l and a final answer ŷ, we compare two generation scenarios: (i) the ordinary inference and (ii) the intervened inference.In the latter scenario, we replace the specific hidden states h t,l with other variants ht,l obtained from the same model but with a different input x (Clean run in Figure 3).</p>
<p>The input x and x have different correct answer y and ỹ as well as different chains z and z, respectively.</p>
<p>For example, for the triple (x = "A=1+B,B=2+4;A=?,"z = "A=1+B,B=2+4,B=6,A=1+B,A=1+6,A=7," y =7), one may use (x = "A=2+B,B=1+3;A=?,"z = "A=2+B,B=1+3,B=4,A=2+B,A=2+4,A=6," ỹ =6).</p>
<p>If the model's output turns from y into ỹ or z t into zt due to the intervention to h t,l with ht,l , we can confirm the causal relationship between h t,l and the original answer y.We denote the model's final output without intervention as ŷ and that with intervention as ŷ, respectively (ỹ and y denote respective gold answers).In the same way, we denote the generated reasoning chain without intervention as ẑt and that with intervention as ẑt , respectively.</p>
<p>Evaluation metrics.We report Success Rate as a metric for this experiment.The Success rate indicates how frequently (%) the intervened output ŷ aligns with the correct answer ỹ.For reasoning chains, we report the Success rate for ẑt as well.</p>
<p>Patching targets.We specifically focus on Level 3 tasks and Qwen2.5-7B.Inspired by sliding window patching (Zhang and Nanda, 2024), we partition the hidden states into coarse grids, corresponding to each equation and every four layers, and perform activation patching on each grid separately (illustrated in Figure 5). 5For every grid, we compute the Success rate by applying activation patching.We also examine multiple target tokens, specifically, at (i) the end of the equation 2 (z 17 in B =5 2 in Figure 2), (ii) the end of the equation 4 (z 32 in A = 1+5 4 in Figure 2), and (iii) the final answer (y).When we apply activation patching, we generate the only target token with greedy de-5 All the hidden states in each grid are intervened at once.coding while forced-decoding the context.Note that the above equations are examples.We create a test set of 2,000 instances for evaluations.</p>
<p>Results.</p>
<p>Figure 4 shows the Success rate for each grid when the final answer y is the target token.We also show the results for the target tokens z 17 and z 32 in Appendix C. Figure 5 summarizes the max success rate among layers for each target token, and probing accuracy (same as the line graph in Figure 2).</p>
<p>The bottom part of Figure 5 suggests strong recency bias in the causal relationship between hidden states and output tokens.That is, intervention succeeded only when the target hidden state is (i) in the same grid as the target token, in the last grid where necessary information is written to derive the target token (e.g., B=2+3→B=5), or (iii) in the last grid where a value of a relevant variable is explicitly mentioned (e.g., B=5→A=1+5).This finding suggests the redundancy and strong recency bias in the internal process of LLMs' multi-hop reasoning.Moreover, the fact that in CoT the model relies on the immediately preceding computation when producing the (sub-)answer suggests that its internal reasoning flow is faithful to its own explanation.</p>
<p>Conclusions</p>
<p>We conducted causal probing analyses of when (sub-)answers are determined in the CoT process, using synthetic arithmetic problems as a controlled testbed.Across a range of models and task difficulties, we found that models predominantly operated in a Talk-to-Think mode: they resolved the necessary subproblems during CoT generation phase.Moreover, causal intervention experiments revealed a strong recency bias linking hidden states to outputs, indicating that LLMs rely heavily on recent computations when generating explanations.This pattern further suggests that their internal reasoning flow largely aligns with the produced explanations.</p>
<p>Limitations</p>
<p>Comprehensiveness of experimental settings Some experiments were conducted with a limited scope; for example, the experiments with various models in § 4.3 are conducted only on the Level 3 task.Additionally, causal interventions ( § 5) are performed only with Qwen2.5-7B.Conducting our experiment with more models and tasks will further enhance the generalizability of the results.</p>
<p>Variety of task</p>
<p>We analyzed the internal reasoning patterns of language models using synthetic arithmetic reasoning tasks.The use of synthetic data allows for more detailed control compared to experiments on natural language tasks.However, vocabulary and expression diversity, for example, are limited compared to natural language tasks.Therefore, conducting similar analyses on reasoning tasks will verify whether the results of this study apply to other broader, realistic contexts as well.Additionally, in our study, we focus on a single reasoning-chain pattern, and it would be desirable to also conduct experiments using other reasoning chain strategies and formats.On the other hand, controlling the length and granularity of them is difficult because there are various options.Conducting experiments for other reasoning chain strategies and formats is expected to provide more general insights.</p>
<p>Probing methods Interpreting internal mechanisms of LMs using probing have been actively conducted in our field (Conneau et al., 2018;Tenney et al., 2019;Campbell et al., 2023;Li et al., 2023); however, there are criticisms regarding the validity of some probing approaches (Liu et al., 2023;Burns et al., 2023).One way to overcome such concerns will be to analyze the generality of obtained results through more diverse methodologies (Gurnee et al., 2023;Bricken et al., 2023).Table 4: The performance of language models on the arithmetic reasoning tasks.The Task column shows the accuracy for the evaluation set (exact match).</p>
<p>A Supplemental results</p>
<p>A.1 Performance of language models in the arithmetic tasks</p>
<p>Table 4 shows the accuracy of language models on arithmetic reasoning tasks for each experimental setting.We computed the accuracy based on exact matches between the output, including the chain (ẑ⊕ ŷ), and the gold labels (z⊕y).The accuracy for all models is nearly 100%, indicating that they are capable of solving the arithmetic reasoning tasks used in this experiment.</p>
<p>A.2 All probing results</p>
<p>Figures 9 through 53 present the probing results for all models and tasks discussed in this paper.Tables 6 through 20 summarize these results for thresholds (τ ) ranging from 0.85 to 0.95.From these results, we observe trends similar to those described in § 4.3 across many settings.However, for the smaller model Llama3.2(3B), increasing the threshold τ often leads to cases where the accuracy does not reach the threshold (N/A).Nonetheless, a consistent pattern remains: Acc ≺CoT (v i ) is low whereas Acc ≻CoT (v i ) is high, indicating a Talk-to-Think mode.</p>
<p>B Hyperparameters</p>
<p>Table 5 shows the hyperparameters used for training the probes.</p>
<p>C Addtional causal intervention results</p>
<p>Figures 7 and 8 show the causal intervention results for the target tokens z 17 and z 32 , respectively.Here, in addition to the Success rate, we also present the   The Success rate heatmap at the top is the same as Figure 5.</p>
<p>Unchanged rate as a metric.The Unchanged rate indicates how frequently (%) the intervened output ŷ remains the same as y.If this value is small, it indicates that the patched hidden states do not affect the output.</p>
<p>D Computational resources</p>
<p>We used NVIDIA A100 GPUs (40GB and 80GB memory) and NVIDIA H100 GPUs to conduct this study.</p>
<p>E Usage of AI assistants</p>
<p>For writing this paper and the source code for the experiments, we use AI assistants (e.g., ChatGPT, GitHub Copilot).However, the use is limited to purposes such as code completion, translation, text editing, and table creation, and all content is solely based on the authors' ideas.Qwen Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Qwen2.5 (14B) v1 (A) 4 28 36.9 100 (Qwen Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Qwen2.5 (32B) v1 (A) 4 28 30.5 100 (Qwen Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Qwen2.5-Math (7B) v1 (A) 4 27 41.8 100 (Yang et al., 2024a) v2 (B) −2 −5 100 100</p>
<p>Yi1.5 (9B) v1 (A) 4 32 28.1 100 (Young et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Yi1.5 (34B) v1 (A) 4 32 22.9 100 (Young et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Llama3.1 (8B) v1 (A) 4 27 20.6 100 (Dubey et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Llama3.2 (3B) v1 (A) 4 28 21.8 100.0 (Dubey et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Mistral-Nemo (12B) v1 (A) 4 28 17.9 100 (Mistral AI Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Figure 1 :
1
Figure1: Using linear probes, we investigated at which time during the LLM's problem-solving process it is possible to determine the values of each variable, illustrating the model's problem-solving process.Our analysis indicates that LMs come up with (sub)answers during CoT (second pass).This conclusion is also consistent with the findings from the causal experiments in § 5.</p>
<p>Figure 2 :
2
Figure 2: Probing results for Qwen2.5-7B at the task Level 3. The heatmaps in the lower section represent the accuracy of probes computed on the evaluation set.Each cell shows the probing accuracies in each token t, layer l.The upper part indicates the maximum probing accuracy achieved at each token position t.The input sequence below the line graphs is just an example; in the actual evaluation set, each variable name, number, and operator are randomly sampled from (D, Σ, {+, −}).</p>
<p>Figure 3 :
3
Figure3: Overview of the causal intervention experiment.First, we perform normal inference (Clean run) and cache its hidden states.Subsequently, we evaluate whether the output changes by replacing some of the hidden states of a model solving a different problem with the cached hidden states.</p>
<p>Figure 4 :
4
Figure 4: Success rates for each grid when the final answer y (A =6 5 ) is the target token.</p>
<p>Figure 5 :
5
Figure 5: The upper part is the accuracy of probs, as shown in Figure 2. The lower part is the result of max pooling the Success rates from Figure 4 in the layer direction.</p>
<p>Figure 6 :
6
Figure 6: Success and Unchanged rates for each grid when the final answer y (A =6 5 ) is the target token.The Success rate heatmap at the top is the same as Figure 5.</p>
<p>Figure 7 :
7
Figure 7: Success rate and Unchanged rate for each grid when intervention was performed with z 17 (A = 1+5 4 ) as the target token.</p>
<p>Figure 8 :
8
Figure8: Success rate and Unchanged rate for each grid when intervention was performed with z 32 (B =5 2 ) as the target token.</p>
<p>Figure 9 :Figure 12 :Figure 16 :Figure 20 :
9121620
Figure 9: Probing results when Qwen2.5-7B solves Level 1.</p>
<p>Figure 24 :
24
Figure 21: Probing results when Qwen2.5-32B solves Level 3.</p>
<p>Figure 25 :Figure 26 :Figure 27 :Figure 28 :
25262728
Figure 25: Probing results when Qwen2.5-Math-7B solves Level 2.</p>
<p>Figure 29 :Figure 30 :Figure 32 :
293032
Figure 29: Probing results when Yi-1.5-9B solves Level 1.</p>
<p>Figure 33 :Figure 34 :Figure 36 :
333436
Figure 33: Probing results when Yi-1.5-9B solves Level 5.</p>
<p>Figure 37 :Figure 38 :Figure 40 :
373840
Figure 37: Probing results when Yi-1.5-34B solves Level 4.</p>
<p>Figure 41 :Figure 42 :Figure 44 :
414244
Figure 41: Probing results when Llama-3.1-8B solves Level 3.</p>
<p>Figure 45 :Figure 46 :Figure 48 :
454648
Figure 45: Probing results when Llama-3.2-3B solves Level 2.</p>
<p>Figure 49 :Figure 50 :Figure 51 :Figure 52 :
49505152
Figure 49: Probing results when Mistral-Nemo-Base-2407 solves Level 1.</p>
<p>Figure 53 :
53
Figure 53: Probing results when Mistral-Nemo-Base-2407 solves Level 5.</p>
<p>Table 1 :
1
Examples of arithmetic reasoning tasks used in our experiments at each complexity level.#Step indicates the number of required operations to reach the final answer.#Stack indicates how many variables' values are not immediately determined in their first appearing equation.#Dist. is the number of unnecessary distractor equations.The number (e.g., −3) indicated in the lower right corner of each equation represents the equation's position.This position is used as a reference point for calculating t * eq in § 4.2.</p>
<p>Table 1
1Notation. Formally, let v denote a variable namesampled from the 26 letters of the English al-phabet Σ = {a, b, c, . . . , z}, and d a numbersampled from the set of decimal digits D ={0, 1, 2, . . . , 9}. Each instance consists of multi-ple equations [e 1 , e 2 , • • • , e n ] followed by a finalquery q. Each equation follows the format
, where #Step is three and #Stack is two, calculating A requires at least three steps of reasoning: C(=1+2)=3, B(=2+3)=5, and then A(=1+5)=6, and two variables need to be resolved before reaching A: B and C.</p>
<p>on the Level 3 task.Same results as Qwen2.5-7Bare generally obtained across various
When (↓)Acc (↑)Variable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 53617.9100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (14B)v1 (A) 53517.8100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (32B)v1 (A) 53617.8100(Qwen Team, 2024)v2 (B) 21567.4100Qwen2.5-Math (7B)v1 (A) 53518.6100(Yang et al., 2024a)v2 (B) 21556.1100Yi1.5 (9B)v1 (A) 54117.8100(Young et al., 2024)v2 (B) 21836.9100Yi1.5 (34B)v1 (A) 54122.4100(Young et al., 2024)v2 (B) 21837.4100Llama3.1 (8B)v1 (A) 53526.0100(Dubey et al., 2024)v2 (B) 21629.6100Llama3.2 (3B)v1 (A) 53617.893.2(Dubey et al., 2024)v2 (B) 21733.295.4Mistral-Nemo (12B)v1 (A) 53617.8100(Mistral AI Team, 2024) v2 (B) 21628.9100</p>
<p>Empirical Methods in Natural Language Processing, pages 17432-17445, Miami, Florida, USA.Association for Computational Linguistics.Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.2023.Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task.In The Eleventh International Conference on Learning Representations (ICLR).
Deep Ganguli, Amanda Askell, Nicholas Schiefer,Thomas I Liao, Kamilė Lukošiūtė, Anna Chen,Anna Goldie, Azalia Mirhoseini, Catherine Olsson,Danny Hernandez, and 1 others. 2023. The capacityTom Lieberum, Senthooran Rajamanoharan, Arthurfor moral self-correction in large language models.Conmy, Lewis Smith, Nicolas Sonnerat, VikrantarXiv preprint arXiv:2302.07459.Varma, János Kramár, Anca D. Dragan, Rohin Shah,Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual asso-ciations in auto-regressive language models. In Theand Neel Nanda. 2024. Gemma scope: Open sparse autoencoders everywhere all at once on Gemma 2. CoRR, abs/2408.05147.2023 Conference on Empirical Methods in Natural Language Processing.Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, and Jacob Andreas. 2023. Cognitive dissonance: Why doAsma Ghandeharioun, Avi Caciularu, Adam Pearce, Lu-cas Dixon, and Mor Geva. 2024. Patchscopes: A unifying framework for inspecting hidden representa-tions of language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vi-language model outputs disagree with internal rep-resentations of truthfulness? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4791-4797, Singapore. Association for Computational Linguistics.enna, Austria, July 21-27, 2024. OpenReview.net.Kevin Meng, David Bau, Alex Andonian, and YonatanKeren Gruteke Klein, Yoav Meiri, Omer Shubi, and Yevgeni Berzak. 2024. The effect of surprisal on reading times in information seeking and repeated reading. In Proceedings of the 28th Conference onBelinkov. 2022. Locating and editing factual associ-ations in GPT. In Advances in Neural Information Processing Systems, volume 35, pages 17359-17372. Curran Associates, Inc.Computational Natural Language Learning, pages 219-230, Miami, FL, USA. Association for Compu-Mistral AI Team. 2024. Mistral NeMo.tational Linguistics.Timothy Niven and Hung-Yu Kao. 2019. Probing neu-Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in a haystack: Case stud-ies with sparse probing. Transactions on Machine Learning Research.ral network comprehension of natural language argu-ments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy. Association for Compu-tational Linguistics.Benjamin Heinzerling and Kentaro Inui. 2024. Mono-tonic representation of numeric attributes in languagenostalgebraist. 2020. interpreting GPT: the logit lens. LessWrong.models. In Proceedings of the 62nd Annual Meet-ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 175-195, Bangkok, Thailand. Association for Computational Linguistics.Byung-Doh Oh and William Schuler. 2023. Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times? Trans. Assoc. Comput. Linguist., 11:336-350.Kuan-Jung Huang, Suhas Arehalli, Mari Kugemoto,Christian Muxica, Grusha Prasad, Brian Dillon, andQwen Team. 2024. Qwen2.5: A party of foundationTal Linzen. 2024. Large-scale benchmark yieldsmodels.no evidence that language model surprisal explains syntactic disambiguation difficulty. J. Mem. Lang., 137:104510.Herbert E. Robbins. 1951. A stochastic approximation method. Annals of Mathematical Statistics, 22:400-407.Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, AnaBrassard, Masashi Yoshikawa, Keisuke Sakaguchi,Cory Shain, Clara Meister, Tiago Pimentel, Ryan Cot-and Kentaro Inui. 2023. Do Deep Neural Networksterell, and Roger Levy. 2024. Large-scale evi-Capture Compositionality in Arithmetic Reasoning?dence for logarithmic effects of word predictabilityIn Proceedings of the 17th Conference of the Euro-on reading time. Proc. Natl. Acad. Sci. U. S. A.,pean Chapter of the Association for Computational121(10):e2307876121.Linguistics (EACL), pages 1351-1362.Alessandro Stolfo, Yonatan Belinkov, and MrinmayaSachan. 2023. A mechanistic interpretation of arith-metic reasoning in language models using causalmediation analysis. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 7035-7052, Singapore. Associa-tion for Computational Linguistics.
TatsukiKuribayashi, Yohei Oseki, and Timothy Baldwin.2024.Psychometric predictive power of large language models.In Findings of the Association for Computational Linguistics: NAACL 2024, pages 1983-2005, Mexico City, Mexico.Association for Computational Linguistics.</p>
<p>Table 5 :
5
Hyperparameters for training the probe</p>
<p>Table 6 :
6
Results for various models on the task Level 1 (τ = 0.85).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 42735.8100(Qwen Team, 2024)v2 (B) −2 −5100100Qwen2.5 (14B)v1 (A) 42736.9100(Qwen Team, 2024)v2 (B) −2 −5100100Qwen2.5 (32B)v1 (A) 42830.5100(Qwen Team, 2024)v2 (B) −2 −5100100Qwen2.5-Math (7B)v1 (A) 42741.8100(Yang et al., 2024a)v2 (B) −2 −5100100Yi1.5 (9B)v1 (A) 43228.1100(Young et al., 2024)v2 (B) −2 −5100100Yi1.5 (34B)v1 (A) 43122.9100(Young et al., 2024)v2 (B) −2 −5100100Llama3.1 (8B)v1 (A) 42720.6100(Dubey et al., 2024)v2 (B) −2 −5100100Llama3.2 (3B)v1 (A) 42821.8 100.0(Dubey et al., 2024)v2 (B) −2 −5100100Mistral-Nemo (12B)v1 (A) 42718.0100(Mistral AI Team, 2024) v2 (B) −2 −5100100When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 21649.2100(Qwen Team, 2024)v2 (B) 53521.2100Qwen2.5 (14B)v1 (A) 21548.8100(Qwen Team, 2024)v2 (B) 53621.5100Qwen2.5 (32B)v1 (A) 21566.4100(Qwen Team, 2024)v2 (B) 53621.3100Qwen2.5-Math (7B)v1 (A) 21553.7100(Yang et al., 2024a)v2 (B) 53522.1100Yi1.5 (9B)v1 (A) 21840.2100(Young et al., 2024)v2 (B) 54117.8100Yi1.5 (34B)v1 (A) 21835.6100(Young et al., 2024)v2 (B) 54118.3100Llama3.1 (8B)v1 (A) 21531.9100(Dubey et al., 2024)v2 (B) 53517.8100Llama3.2 (3B)v1 (A) 21636.299.9(Dubey et al., 2024)v2 (B) 53617.899.9Mistral-Nemo (12B)v1 (A) 21630.8100(Mistral AI Team, 2024) v2 (B) 53617.8100</p>
<p>Table 7 :
7
Results for various models on the task Level 2 (τ = 0.85).
When (↓)Acc (↑)</p>
<p>Table 8 :
8
Results for various models on the task Level 3 (τ = 0.85).
When (↓)Acc (↑)</p>
<p>Table 9 :
9
Results for various models on the task Level 4 (τ = 0.85).</p>
<p>Table 10 :
10
Results for various models on the task Level 5 (τ = 0.85).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A)9 6218.1100When (↓)Acc (↑)(Qwen Team, 2024)v2 (B) v3 (C)6 42 3 2322.6 50.6100 100Model Qwen2.5 (7B)Variable t  *  eq v1 (A) 4t  *  ≺ CoT ≻ CoT 27 35.8 100Qwen2.5 (14B)v1 (A)9 6318.198.8(Qwen Team, 2024)v2 (B) −2 −5100100(Qwen Team, 2024)v2 (B) v3 (C)6 42 3 2318.7 42.298.9 100Qwen2.5 (14B) (Qwen Team, 2024)v1 (A) 4 v2 (B) −2 −5 2736.9 100100 100Qwen2.5 (32B) (Qwen Team, 2024)v1 (A) v2 (B) v3 (C)9 63 6 43 3 2318.7 22.6 62.4100 100 100Qwen2.5 (32B) (Qwen Team, 2024)v1 (A) 4 v2 (B) −2 −5 2830.5 100100 100Qwen2.5-Math (7B) (Yang et al., 2024a)v1 (A) v2 (B)9 62 6 4218.1 22.6100 100Qwen2.5-Math (7B) (Yang et al., 2024a)v1 (A) 4 v2 (B) −2 −5 2741.8 100100 100v3 (C)3 2254.5100Yi1.5 (9B)v1 (A) 43228.1100Yi1.5 (34B)v1 (A)9 7118.1100(Young et al., 2024)v2 (B) −2 −5100100(Young et al., 2024)v2 (B) v3 (C)6 49 3 2622.6 41.2100 100Yi1.5 (34B) (Young et al., 2024)v1 (A) 4 v2 (B) −2 −5 3222.9 100100 100Llama3.1 (8B) (Dubey et al., 2024)v1 (A) v2 (B) v3 (C)9 62 6 43 3 2316.0 20.0 30.699.5 99.5 99.8Llama3.1 (8B) (Dubey et al., 2024)v1 (A) 4 v2 (B) −2 −5 2720.6 100100 100Llama3.2 (3B) (Dubey et al., 2024)v1 (A) N/A N/A v2 (B) N/A N/A14.2 26.343.7 47.4Llama3.2 (3B) (Dubey et al., 2024)v1 (A) 4 v2 (B) −2 −5 2821.8 100.0 100 100v3 (C) N/A N/A37.771.7Mistral-Nemo (12B)v1 (A) 42818.0100Mistral-Nemo (12B)v1 (A)9 6318.199.9(Mistral AI Team, 2024) v2 (B) −2 −5100100(Mistral AI Team, 2024) v2 (B)6 4316.399.9v3 (C)3 2332.099.9</p>
<p>Table 11 :
11
Results for various models on the task Level 1 (τ = 0.90).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 21649.2100(Qwen Team, 2024)v2 (B) 53521.2100Qwen2.5 (14B)v1 (A) 21648.8100(Qwen Team, 2024)v2 (B) 53621.5100Qwen2.5 (32B)v1 (A) 21666.4100(Qwen Team, 2024)v2 (B) 53621.3100Qwen2.5-Math (7B)v1 (A) 21553.7100(Yang et al., 2024a)v2 (B) 53522.1100Yi1.5 (9B)v1 (A) 21840.2100(Young et al., 2024)v2 (B) 54117.8100Yi1.5 (34B)v1 (A) 21835.6100(Young et al., 2024)v2 (B) 54118.3100Llama3.1 (8B)v1 (A) 21531.9100(Dubey et al., 2024)v2 (B) 53517.8100Llama3.2 (3B)v1 (A) 21636.299.9(Dubey et al., 2024)v2 (B) 53617.899.9Mistral-Nemo (12B)v1 (A) 21630.8100(Mistral AI Team, 2024) v2 (B) 53617.8100</p>
<p>Table 12 :
12
Results for various models on the task Level 2 (τ = 0.90).
When (↓)Acc (↑)Variable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 53617.9100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (14B)v1 (A) 53517.8100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (32B)v1 (A) 53617.8100(Qwen Team, 2024)v2 (B) 21567.4100Qwen2.5-Math (7B)v1 (A) 53518.6100(Yang et al., 2024a)v2 (B) 21556.1100Yi1.5 (9B)v1 (A) 54117.8100(Young et al., 2024)v2 (B) 21836.9100Yi1.5 (34B)v1 (A) 54122.4100(Young et al., 2024)v2 (B) 21837.4100Llama3.1 (8B)v1 (A) 53526.0100(Dubey et al., 2024)v2 (B) 21629.6100Llama3.2 (3B)v1 (A) 53617.893.2(Dubey et al., 2024)v2 (B) 21733.295.4Mistral-Nemo (12B)v1 (A) 53617.8100(Mistral AI Team, 2024) v2 (B) 21628.9100</p>
<p>Table 13 :
13
Results for various models on the task Level 3 (τ = 0.90).
When (↓)Acc (↑)</p>
<p>Table 14 :
14
Results for various models on the task Level 4 (τ = 0.90).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A)9 6318.1100(Qwen Team, 2024)v2 (B)6 4222.6100v3 (C)3 2350.6100Qwen2.5 (14B)v1 (A)9 6318.198.8(Qwen Team, 2024)v2 (B)6 4218.798.9v3 (C)3 2342.2100Qwen2.5 (32B)v1 (A)9 6318.7100(Qwen Team, 2024)v2 (B)6 4322.6100v3 (C)3 2362.4100Qwen2.5-Math (7B)v1 (A)9 6218.1100(Yang et al., 2024a)v2 (B)6 4222.6100v3 (C)3 2254.5100Yi1.5 (34B)v1 (A)9 7218.1100(Young et al., 2024)v2 (B)6 4922.6100v3 (C)3 2641.2100Llama3.1 (8B)v1 (A)9 6216.099.5(Dubey et al., 2024)v2 (B)6 4320.099.5v3 (C)3 2330.699.8Llama3.2 (3B)v1 (A) N/A N/A14.243.7(Dubey et al., 2024)v2 (B) N/A N/A26.347.4v3 (C) N/A N/A37.771.7Mistral-Nemo (12B)v1 (A)9 6318.199.9(Mistral AI Team, 2024) v2 (B)6 4316.399.9v3 (C)3 2332.099.9</p>
<p>Table 15 :
15
Results for various models on the task Level 5 (τ = 0.90).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 42735.8100(</p>
<p>Table 16 :
16
Results for various models on the task Level 1 (τ = 0.95).
When (↓)Acc (↑)</p>
<p>Table 17 :
17
Results for various models on the task Level 2 (τ = 0.95).
When (↓)Acc (↑)</p>
<p>Table 18 :
18
Results for various models on the task Level 3 (τ = 0.95).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A)5 3617.2100(Qwen Team, 2024)v2 (B)2 1647.7100v3 (C) N/A N/A43.723.7Qwen2.5 (14B)v1 (A)5 3618.9100(Qwen Team, 2024)v2 (B)2 1544.3100v3 (C) N/A N/A40.426.8Qwen2.5 (32B)v1 (A)5 3617.4100(Qwen Team, 2024)v2 (B)2 1662.8100v3 (C) N/A N/A64.432.6Qwen2.5-Math (7B)v1 (A)5 3517.2100(Yang et al., 2024a)v2 (B)2 1555.6100v3 (C) N/A N/A47.829.4Yi1.5 (9B)v1 (A)5 4117.8100(Young et al., 2024)v2 (B)2 1843.5100v3 (C) N/A N/A36.721.2Yi1.5 (34B)v1 (A)5 4119.3100(Young et al., 2024)v2 (B)2 1840.8100v3 (C) N/A N/A27.926.2Llama3.1 (8B)v1 (A)5 3530.4100(Dubey et al., 2024)v2 (B)2 1627.2100v3 (C) N/A N/A18.517.6Llama3.2 (3B)v1 (A) N/A N/A26.291.7(Dubey et al., 2024)v2 (B)2 1729.198.7v3 (C) N/A N/A18.317.3Mistral-Nemo (12B)v1 (A)5 3617.2100(Mistral AI Team, 2024) v2 (B)2 1629.9100v3 (C) N/A N/A22.019.8</p>
<p>Table 19 :
19
Results for various models on the task Level 4 (τ = 0.95).
When (↓)Acc (↑)</p>
<p>Table 20 :
20
Results for various models on the task Level 5 (τ = 0.95).</p>
<p>For ease of reading, all examples throughout this paper use the uppercased variables A, B, C, and only the operator +.
For example, if 1+2 appears in the training set, then 1+2 does not appear in the test set.
See Appendix A.2 for results at different thresholds.
Ethics statementThis paper will not raise particular ethical concerns, considering that (i) no human experiments were conducted, and (ii) our tasks do not involve ethically sensitive topics.
Discovering latent knowledge in language models without supervision. Guillaume Alain, Yoshua Bengio, 5th International Conference on Learning Representations, ICLR 2017. Workshop Track Proceedings. OpenReview.net. Toulon, France2017a. April 24-26, 2017</p>
<p>Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, 5th International Conference on Learning Representations. Toulon, France2017b. 2017. April 24-26, 2017Workshop Track Proceedings</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, and 5 others. 2023Anthropic</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Iteration head: A mechanistic study of chain-of-thought. Vivien Cabannes, Charles Arnal, Wassim Bouaziz, Alice Xingyu, Francois Yang, Julia Charton, Kempe, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Localizing lying in Llama: Understanding instructed dishonesty on true-false questions through prompting, probing, and patching. James Campbell, Phillip Guo, Richard Ren, Socially Responsible Language Modelling Research. 2023</p>
<p>What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties. Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, Marco Baroni, 10.18653/v1/P18-1198Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>The Llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, 10.48550/ARXIV.2407.21783CoRR, abs/2407.21783202482</p>
<p>How to think stepby-step: A mechanistic understanding of chain-ofthought reasoning. Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty, Transactions on Machine Learning Research. 2024</p>
<p>Information flow routes: Automatically interpreting language models at scale. Javier Ferrando, Elena Voita, 10.18653/v1/2024.emnlp-main.965Proceedings of the 2024 Conference on. the 2024 Conference on2024</p>
<p>What do you learn from context? probing for sentence structure in contextualized word representations. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, Thomas Mccoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, Ellie Pavlick, International Conference on Learning Representations. 2019</p>
<p>Singlestage prediction models do not explain the magnitude of syntactic disambiguation difficulty. Marten Van Schijndel, Tal Linzen, Cogn. Sci. 456e129882021</p>
<p>Investigating gender bias in language models using causal mediation analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>PLS-regression: a basic tool of chemometrics. Svante Wold, Michael Sjöström, Lennart Eriksson, 10.1016/S0169-7439(01)00155-1Chemometrics and Intelligent Laboratory Systems. 5822001PLS Methods</p>
<p>Chain of Thought Prompting Elicits Knowledge Augmentation. Dingjun Wu, Jing Zhang, Xinmei Huang, Findings of the Association for Computational Linguistics (ACL). 2023a</p>
<p>Interpretability at scale: Identifying causal mechanisms in alpaca. Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, Noah Goodman, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, arXiv:2409.12122Xingzhang Ren, and Zhenru Zhang. 2024a. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint</p>
<p>Mitigating biases for instruction-following language models via bias neurons elimination. Nakyeong Yang, Taegwan Kang, Stanley Jungkyu Choi, Honglak Lee, Kyomin Jung, 10.18653/v1/2024.acl-long.490Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b1</p>
<p>Do large language models latently perform multi-hop reasoning?. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, 10.18653/v1/2024.acl-long.550Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024c1</p>
<p>Wenhao Huang, and 11 others. 2024. Yi: Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, 10.48550/ARXIV.2403.04652CoRR, abs/2403.04652</p>
<p>Do LLMs really think step-by-step in implicit reasoning?. Yijiong Yu, arXiv:2411.158622025Preprint</p>
<p>Towards best practices of activation patching in language models: Metrics and methods. Fred Zhang, Neel Nanda, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Gender bias in contextualized word embeddings. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, Kai-Wei Chang, 10.18653/v1/N19-1064Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Language models encode the value of numbers linearly. Fangwei Zhu, Damai Dai, Zhifang Sui, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAbu Dhabi, UAE2025Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6405 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6405</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6405</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-276449671</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.13442v2.pdf" target="_blank">TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 64% and 44% in their respective worst-case scenarios under zero-shot setting. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems. The dataset generation code and sample data are available at https://github.com/j-bagel/treecut-math.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6405.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6405.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TREECUT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TREECUT synthetic unanswerable math word problem dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedurally generated dataset that represents math word problems as trees and produces unanswerable variants by removing a necessary edge (condition) along the path to the questioned variable, enabling controlled study of LLM hallucination on arithmetic/word‑problem tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TREECUT (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems / algebraic word problems (answerable and intentionally unanswerable variants)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problem (templates translate linear equations into sentences about item prices)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school arithmetic / small-integer linear relations (unit prices 5–15; coefficients ∈ {-3,-2,-1,1,2,3}); controlled depths up to at least ansDepth=8</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot with chain-of-thought system message (for most models) and a few-shot variant with 3 answerable + 3 unanswerable exemplars including full solution paths; for o3-mini chain-of-thought system message was omitted per reasoning model guidelines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>hallucination rate on unanswerable problems (percentage of problems where model outputs a numeric answer rather than abstaining), and answer accuracy on answerable counterparts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset induces high hallucination: paper reports worst-case hallucination rates for top models (reported as ~64% and 44% in abstract; conclusion reports ~61% and 42%) depending on model and configuration; also reports per-depth statistics and answerable-problem accuracy comparisons (e.g., GPT-4o solves 84% of answerable problems at ansDepth=8 but recognizes only 36% of the corresponding unanswerable variants).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic interpretability probes (e.g., attention, activation or logit‑lens) were performed; analysis is behavioral and structural: authors measure hallucination rate as a function of tree depth (ansDepth), overall number of variables (numVars), composite vs simple item names, and location of the removed condition (cutDepth).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>N/A (dataset entry) — dataset is designed to reveal model failure modes (see model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>N/A (dataset itself), but used to show nontrivial scaling and prompting effects across models and sizes (see model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6405.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6405.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-end OpenAI decoder-only transformer family model evaluated via the OpenAI API (version string gpt-4o-2024-08-06) on TREECUT unanswerable and answerable math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (gpt-4o-2024-08-06)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TREECUT (this paper's synthetic unanswerable/answerable math problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems / small-integer linear word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems (translated linear equations about item prices)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>controlled by ansDepth (4–8 in analyses); integer arithmetic with small coefficients (designed to be within current LLM arithmetic capacity)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot with a chain-of-thought system message instructing step-by-step solving and explicit abstain instruction ('Answer: unknown.' if insufficient conditions); also evaluated with few-shot (3 answerable + 3 unanswerable exemplars) in separate experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>hallucination rate on unanswerable problems (percentage answering with a numeric answer instead of 'unknown'); accuracy on answerable problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports worst-case hallucination ~64% (abstract) and ~61% (conclusion) depending on phrasing; example: for ansDepth=8 GPT-4o correctly solves 84% of answerable problems but correctly recognizes only 36% of corresponding unanswerable problems (behavioral gap between arithmetic-solving ability and abstention).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analysis only: hallucination rate increases with deeper ansDepth, more complex trees (additional unrelated variables), composite item names, and when the cutDepth is near the middle of the path; no internal/mechanistic interpretability (attention or neuron probing) was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Produces confident numeric answers for underdetermined problems (hallucinatory filling-in of missing constraints); specific observed errors include confusing distinct variables (substituting the wrong variable into an equation), algebraic manipulation mistakes in multi-step reasoning examples, and failing to detect insufficiency of conditions for variables when the missing condition is mid-path.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Larger-capability model relative performance: GPT-4o shows strong ability on answerable instances but still high hallucination on unanswerable ones; larger model capability does not eliminate hallucination—behavior depends on problem structure (depth/complexity) rather than trivially scaling away.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6405.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6405.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini (OpenAI reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI reasoning-focused model evaluated via API (version string o3-mini-2025-01-31), notable for lower hallucination on deeper problems and large gains from few-shot exemplars in this study, but also for a distinct failure mode of making extraneous semantic assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini (o3-mini-2025-01-31)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only, labeled a reasoning model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TREECUT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems / small-integer linear relations</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ansDepth from 2 up to 8 in experiments; designed arithmetic is simple but structurally variable</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot without the chain-of-thought system message (per OpenAI's guidance for reasoning models); also evaluated with few-shot (3 answerable + 3 unanswerable exemplars with solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>hallucination rate on unanswerable problems; qualitative analysis of answer content</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports worst-case hallucination ~44% (abstract) and ~42% (conclusion) under zero-shot; o3-mini showed the lowest hallucination among evaluated models for higher ansDepth in zero-shot in some settings, and benefited strongly from few-shot prompting (few-shot hallucination reduced dramatically to single-digit percentages for many configurations, e.g., ~3% for some depths).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probes; analysis documented behavioral failure patterns and that few-shot examples guide the model to abstain rather than make unwarranted assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tendency to make extraneous semantic assumptions not supported by the math (e.g., assuming identical item prices across different restaurants or contexts), using those assumptions to produce numeric answers for underdetermined systems; for short problems it would sometimes correctly detect lack of conditions but then nonetheless choose an unreasonable assumption to compute a numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Few-shot prompting yields large improvements for o3-mini (much larger relative effect than for other models evaluated), suggesting sensitivity to exemplar-guided abstention behavior rather than pure model size scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6405.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6405.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 Instruct (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLaMA-family model evaluated on TREECUT; low success at identifying unanswerable math problems in zero-shot evaluation per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TREECUT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied by ansDepth (2–8)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot with chain-of-thought system message (as used for non-reasoning models in the paper); also evaluated with few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>hallucination rate on unanswerable problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported to 'barely have any success' identifying unanswerable problems in zero-shot; numerically, the paper's table indicates very high hallucination percentages for this model across depths (qualitatively much worse than larger models).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral only; no internal mechanistic analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High tendency to answer underdetermined problems (hallucinate numeric answers) rather than abstain; performance particularly poor even at shallow depths.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Comparing 8B vs 70B Llama 3.1 indicates substantial improvement with scale (70B performs better), suggesting positive scaling with size in this family for this task, though not eliminating hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6405.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6405.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 Instruct (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger instruction-tuned LLaMA-family model (70B) evaluated on TREECUT; better than the 8B variant but still struggles on deeper/more complex unanswerable problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TREECUT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ansDepth up to 8</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot with chain-of-thought system message; few-shot evaluated as well</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>hallucination rate on unanswerable problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Per-paper: improved over the 8B variant but still 'struggles with more complex problems (ansDepth = 6, 8)'; hallucination rates remain substantial for deep/complex configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral correlations with tree structure; no internal mechanistic probes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Makes hallucinatory numeric answers for unanswerable items, especially for deeper ansDepth and complex trees.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Shows clear improvement over the 8B variant, indicating beneficial scaling within the Llama 3.1 line for this task, but not elimination of the failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6405.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6405.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 2.5 Instruct (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Qwen-family instruction-tuned model evaluated on TREECUT; reported to have high hallucination rates and little success at detecting unanswerability in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TREECUT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied (ansDepth up to 8)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot with chain-of-thought system message; also evaluated few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>hallucination rate on unanswerable problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper states Qwen2.5 7B 'barely have any success' identifying unanswerable problems in zero-shot and reports very high hallucination percentages across depths (often among the worst of the models tested).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Only behavioral analysis (no mechanistic probes).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High tendency to produce numeric answers rather than abstain on underdetermined problems.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper also evaluates a 72B Qwen2.5 variant; across the Qwen family results suggest that instruction-tuned Qwen models still exhibit high hallucination on these unanswerable cases and do not reliably abstain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6405.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6405.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 2.5 Instruct (72B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Qwen 2.5 model evaluated on TREECUT; despite scale, reported to still have substantial hallucination/abstention failures on unanswerable math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TREECUT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied (ansDepth up to 8)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot with chain-of-thought system message; also few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>hallucination rate on unanswerable problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports very high hallucination percentages (often >80% for many depths) for Qwen2.5 variants in zero-shot, indicating difficulty abstaining even at large scale in this family.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral only.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Answers underdetermined problems numerically rather than outputting 'unknown'; remains vulnerable to the same structural triggers (depth, composite names, mid-path cuts).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Scale alone (7B → 72B) did not reliably solve the abstention problem in the Qwen2.5 family for these synthetic unanswerable instances per the paper's results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are unconscious of unreasonability in math problems <em>(Rating: 2)</em></li>
                <li>When not to answer: Evaluating prompts on gpt models for effective abstention in unanswerable math word problems <em>(Rating: 2)</em></li>
                <li>Benchmarking hallucination in large language models based on unanswerable math word problem <em>(Rating: 2)</em></li>
                <li>Is your model really a good math reasoner? evaluating mathematical reasoning with checklist <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6405",
    "paper_id": "paper-276449671",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "TREECUT",
            "name_full": "TREECUT synthetic unanswerable math word problem dataset",
            "brief_description": "A procedurally generated dataset that represents math word problems as trees and produces unanswerable variants by removing a necessary edge (condition) along the path to the questioned variable, enabling controlled study of LLM hallucination on arithmetic/word‑problem tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "TREECUT (this paper's dataset)",
            "task_type": "multi-step arithmetic word problems / algebraic word problems (answerable and intentionally unanswerable variants)",
            "problem_format": "natural-language math word problem (templates translate linear equations into sentences about item prices)",
            "difficulty_level": "grade-school arithmetic / small-integer linear relations (unit prices 5–15; coefficients ∈ {-3,-2,-1,1,2,3}); controlled depths up to at least ansDepth=8",
            "prompting_method": "zero-shot with chain-of-thought system message (for most models) and a few-shot variant with 3 answerable + 3 unanswerable exemplars including full solution paths; for o3-mini chain-of-thought system message was omitted per reasoning model guidelines",
            "performance_metric": "hallucination rate on unanswerable problems (percentage of problems where model outputs a numeric answer rather than abstaining), and answer accuracy on answerable counterparts",
            "performance_value": "Dataset induces high hallucination: paper reports worst-case hallucination rates for top models (reported as ~64% and 44% in abstract; conclusion reports ~61% and 42%) depending on model and configuration; also reports per-depth statistics and answerable-problem accuracy comparisons (e.g., GPT-4o solves 84% of answerable problems at ansDepth=8 but recognizes only 36% of the corresponding unanswerable variants).",
            "internal_analysis": "No mechanistic interpretability probes (e.g., attention, activation or logit‑lens) were performed; analysis is behavioral and structural: authors measure hallucination rate as a function of tree depth (ansDepth), overall number of variables (numVars), composite vs simple item names, and location of the removed condition (cutDepth).",
            "failure_modes": "N/A (dataset entry) — dataset is designed to reveal model failure modes (see model entries).",
            "scaling_trend": "N/A (dataset itself), but used to show nontrivial scaling and prompting effects across models and sizes (see model entries).",
            "uuid": "e6405.0",
            "source_info": {
                "paper_title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A high-end OpenAI decoder-only transformer family model evaluated via the OpenAI API (version string gpt-4o-2024-08-06) on TREECUT unanswerable and answerable math word problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (gpt-4o-2024-08-06)",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "TREECUT (this paper's synthetic unanswerable/answerable math problems)",
            "task_type": "multi-step arithmetic word problems / small-integer linear word problems",
            "problem_format": "natural-language word problems (translated linear equations about item prices)",
            "difficulty_level": "controlled by ansDepth (4–8 in analyses); integer arithmetic with small coefficients (designed to be within current LLM arithmetic capacity)",
            "prompting_method": "zero-shot with a chain-of-thought system message instructing step-by-step solving and explicit abstain instruction ('Answer: unknown.' if insufficient conditions); also evaluated with few-shot (3 answerable + 3 unanswerable exemplars) in separate experiments",
            "performance_metric": "hallucination rate on unanswerable problems (percentage answering with a numeric answer instead of 'unknown'); accuracy on answerable problems",
            "performance_value": "Paper reports worst-case hallucination ~64% (abstract) and ~61% (conclusion) depending on phrasing; example: for ansDepth=8 GPT-4o correctly solves 84% of answerable problems but correctly recognizes only 36% of corresponding unanswerable problems (behavioral gap between arithmetic-solving ability and abstention).",
            "internal_analysis": "Behavioral analysis only: hallucination rate increases with deeper ansDepth, more complex trees (additional unrelated variables), composite item names, and when the cutDepth is near the middle of the path; no internal/mechanistic interpretability (attention or neuron probing) was performed.",
            "failure_modes": "Produces confident numeric answers for underdetermined problems (hallucinatory filling-in of missing constraints); specific observed errors include confusing distinct variables (substituting the wrong variable into an equation), algebraic manipulation mistakes in multi-step reasoning examples, and failing to detect insufficiency of conditions for variables when the missing condition is mid-path.",
            "scaling_trend": "Larger-capability model relative performance: GPT-4o shows strong ability on answerable instances but still high hallucination on unanswerable ones; larger model capability does not eliminate hallucination—behavior depends on problem structure (depth/complexity) rather than trivially scaling away.",
            "uuid": "e6405.1",
            "source_info": {
                "paper_title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "o3-mini",
            "name_full": "o3-mini (OpenAI reasoning model)",
            "brief_description": "An OpenAI reasoning-focused model evaluated via API (version string o3-mini-2025-01-31), notable for lower hallucination on deeper problems and large gains from few-shot exemplars in this study, but also for a distinct failure mode of making extraneous semantic assumptions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o3-mini (o3-mini-2025-01-31)",
            "model_family": "transformer (decoder-only, labeled a reasoning model)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "TREECUT",
            "task_type": "multi-step arithmetic word problems / small-integer linear relations",
            "problem_format": "natural-language word problems",
            "difficulty_level": "ansDepth from 2 up to 8 in experiments; designed arithmetic is simple but structurally variable",
            "prompting_method": "zero-shot without the chain-of-thought system message (per OpenAI's guidance for reasoning models); also evaluated with few-shot (3 answerable + 3 unanswerable exemplars with solutions)",
            "performance_metric": "hallucination rate on unanswerable problems; qualitative analysis of answer content",
            "performance_value": "Paper reports worst-case hallucination ~44% (abstract) and ~42% (conclusion) under zero-shot; o3-mini showed the lowest hallucination among evaluated models for higher ansDepth in zero-shot in some settings, and benefited strongly from few-shot prompting (few-shot hallucination reduced dramatically to single-digit percentages for many configurations, e.g., ~3% for some depths).",
            "internal_analysis": "No mechanistic probes; analysis documented behavioral failure patterns and that few-shot examples guide the model to abstain rather than make unwarranted assumptions.",
            "failure_modes": "Tendency to make extraneous semantic assumptions not supported by the math (e.g., assuming identical item prices across different restaurants or contexts), using those assumptions to produce numeric answers for underdetermined systems; for short problems it would sometimes correctly detect lack of conditions but then nonetheless choose an unreasonable assumption to compute a numeric answer.",
            "scaling_trend": "Few-shot prompting yields large improvements for o3-mini (much larger relative effect than for other models evaluated), suggesting sensitivity to exemplar-guided abstention behavior rather than pure model size scaling.",
            "uuid": "e6405.2",
            "source_info": {
                "paper_title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3.1-8B",
            "name_full": "Llama 3.1 Instruct (8B)",
            "brief_description": "An instruction-tuned LLaMA-family model evaluated on TREECUT; low success at identifying unanswerable math problems in zero-shot evaluation per the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.1 Instruct",
            "model_family": "transformer (decoder-only)",
            "model_size": "8B",
            "training_data_description": null,
            "benchmark_name": "TREECUT",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "natural-language",
            "difficulty_level": "varied by ansDepth (2–8)",
            "prompting_method": "zero-shot with chain-of-thought system message (as used for non-reasoning models in the paper); also evaluated with few-shot",
            "performance_metric": "hallucination rate on unanswerable problems",
            "performance_value": "Reported to 'barely have any success' identifying unanswerable problems in zero-shot; numerically, the paper's table indicates very high hallucination percentages for this model across depths (qualitatively much worse than larger models).",
            "internal_analysis": "Behavioral only; no internal mechanistic analysis reported.",
            "failure_modes": "High tendency to answer underdetermined problems (hallucinate numeric answers) rather than abstain; performance particularly poor even at shallow depths.",
            "scaling_trend": "Comparing 8B vs 70B Llama 3.1 indicates substantial improvement with scale (70B performs better), suggesting positive scaling with size in this family for this task, though not eliminating hallucinations.",
            "uuid": "e6405.3",
            "source_info": {
                "paper_title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3.1-70B",
            "name_full": "Llama 3.1 Instruct (70B)",
            "brief_description": "A larger instruction-tuned LLaMA-family model (70B) evaluated on TREECUT; better than the 8B variant but still struggles on deeper/more complex unanswerable problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.1 Instruct",
            "model_family": "transformer (decoder-only)",
            "model_size": "70B",
            "training_data_description": null,
            "benchmark_name": "TREECUT",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "natural-language",
            "difficulty_level": "ansDepth up to 8",
            "prompting_method": "zero-shot with chain-of-thought system message; few-shot evaluated as well",
            "performance_metric": "hallucination rate on unanswerable problems",
            "performance_value": "Per-paper: improved over the 8B variant but still 'struggles with more complex problems (ansDepth = 6, 8)'; hallucination rates remain substantial for deep/complex configurations.",
            "internal_analysis": "Behavioral correlations with tree structure; no internal mechanistic probes.",
            "failure_modes": "Makes hallucinatory numeric answers for unanswerable items, especially for deeper ansDepth and complex trees.",
            "scaling_trend": "Shows clear improvement over the 8B variant, indicating beneficial scaling within the Llama 3.1 line for this task, but not elimination of the failure mode.",
            "uuid": "e6405.4",
            "source_info": {
                "paper_title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Qwen2.5-7B",
            "name_full": "Qwen 2.5 Instruct (7B)",
            "brief_description": "A Qwen-family instruction-tuned model evaluated on TREECUT; reported to have high hallucination rates and little success at detecting unanswerability in zero-shot.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5 Instruct",
            "model_family": "transformer (decoder-only)",
            "model_size": "7B",
            "training_data_description": null,
            "benchmark_name": "TREECUT",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "natural-language",
            "difficulty_level": "varied (ansDepth up to 8)",
            "prompting_method": "zero-shot with chain-of-thought system message; also evaluated few-shot",
            "performance_metric": "hallucination rate on unanswerable problems",
            "performance_value": "Paper states Qwen2.5 7B 'barely have any success' identifying unanswerable problems in zero-shot and reports very high hallucination percentages across depths (often among the worst of the models tested).",
            "internal_analysis": "Only behavioral analysis (no mechanistic probes).",
            "failure_modes": "High tendency to produce numeric answers rather than abstain on underdetermined problems.",
            "scaling_trend": "Paper also evaluates a 72B Qwen2.5 variant; across the Qwen family results suggest that instruction-tuned Qwen models still exhibit high hallucination on these unanswerable cases and do not reliably abstain.",
            "uuid": "e6405.5",
            "source_info": {
                "paper_title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Qwen2.5-72B",
            "name_full": "Qwen 2.5 Instruct (72B)",
            "brief_description": "A larger Qwen 2.5 model evaluated on TREECUT; despite scale, reported to still have substantial hallucination/abstention failures on unanswerable math problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5 Instruct",
            "model_family": "transformer (decoder-only)",
            "model_size": "72B",
            "training_data_description": null,
            "benchmark_name": "TREECUT",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "natural-language",
            "difficulty_level": "varied (ansDepth up to 8)",
            "prompting_method": "zero-shot with chain-of-thought system message; also few-shot",
            "performance_metric": "hallucination rate on unanswerable problems",
            "performance_value": "Paper reports very high hallucination percentages (often &gt;80% for many depths) for Qwen2.5 variants in zero-shot, indicating difficulty abstaining even at large scale in this family.",
            "internal_analysis": "Behavioral only.",
            "failure_modes": "Answers underdetermined problems numerically rather than outputting 'unknown'; remains vulnerable to the same structural triggers (depth, composite names, mid-path cuts).",
            "scaling_trend": "Scale alone (7B → 72B) did not reliably solve the abstention problem in the Qwen2.5 family for these synthetic unanswerable instances per the paper's results.",
            "uuid": "e6405.6",
            "source_info": {
                "paper_title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are unconscious of unreasonability in math problems",
            "rating": 2,
            "sanitized_title": "large_language_models_are_unconscious_of_unreasonability_in_math_problems"
        },
        {
            "paper_title": "When not to answer: Evaluating prompts on gpt models for effective abstention in unanswerable math word problems",
            "rating": 2,
            "sanitized_title": "when_not_to_answer_evaluating_prompts_on_gpt_models_for_effective_abstention_in_unanswerable_math_word_problems"
        },
        {
            "paper_title": "Benchmarking hallucination in large language models based on unanswerable math word problem",
            "rating": 2,
            "sanitized_title": "benchmarking_hallucination_in_large_language_models_based_on_unanswerable_math_word_problem"
        },
        {
            "paper_title": "Is your model really a good math reasoner? evaluating mathematical reasoning with checklist",
            "rating": 1,
            "sanitized_title": "is_your_model_really_a_good_math_reasoner_evaluating_mathematical_reasoning_with_checklist"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0154995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TREECUT: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation
20 May 2025</p>
<p>Jialin Ouyang 
Columbia University</p>
<p>Karl Cobbe 
Columbia University</p>
<p>Vineet Kosaraju 
Columbia University</p>
<p>Mohammad Bavarian 
Columbia University</p>
<p>Mark Chen 
Columbia University</p>
<p>Heewoo Jun 
Columbia University</p>
<p>Lukasz Kaiser 
Columbia University</p>
<p>Matthias Plappert 
Columbia University</p>
<p>Jerry Tworek 
Columbia University</p>
<p>Jacob Hilton 
Columbia University</p>
<p>Reiichiro Nakano 
Columbia University</p>
<p>Abhimanyu Dubey 
Columbia University</p>
<p>Abhinav Jauhri 
Columbia University</p>
<p>Abhinav Pandey 
Columbia University</p>
<p>Abhishek Kadian 
Columbia University</p>
<p>Ahmad Al-Dahle 
Columbia University</p>
<p>Aiesha Letman 
Columbia University</p>
<p>Akhil Mathur 
Columbia University</p>
<p>Alan Schelten 
Columbia University</p>
<p>Amy Yang 
Columbia University</p>
<p>TREECUT: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation
20 May 2025B5A249FB59E29223A5602155095E4E70arXiv:2502.13442v2[cs.CL]
Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed.A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems.We introduce TREECUT, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions.Experiments show TREE-CUT effectively induce hallucinations in large language models, including GPT-4o and o3mini, with rates of 64% and 44% in their respective worst-case scenarios under zero-shot setting.Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems.The dataset generation code and sample data are available at https: //github.com/j-bagel/treecut-math.</p>
<p>Introduction</p>
<p>Mathematical reasoning is a crucial part of human intelligence.Recent years have witnessed remarkable advancements in the mathematical reasoning capabilities of large language models (LLMs).By leveraging techniques such as chain-of-thought prompting (Wei et al., 2022), state-of-the-art LLMs (e.g., Achiam et al. (2023); Team et al. (2024); Dubey et al. (2024)) achieved human-level performance on benchmarks like GSM8K (Cobbe et al., 2021).However, it remains controversial whether this performance implies reasoning capability beyond pattern matching.</p>
<p>A substantial body of research highlights the capability of Large Language Models in mathematical reasoning.Achiam et al. (2023); Team et al. (2024); Dubey et al. (2024); Yang et al. (2024), among others, achieved over 90% accuracy on GSM8K (Cobbe et al., 2021), a dataset consists of 8K grade school math word problems.Yang et al. (2024); Zhou et al. (2023), among others, achieved over 80% accuracy on the more difficult MATH dataset (Hendrycks et al., 2021), which consists of 12.5K high school math competition problems.</p>
<p>Meanwhile, there is a line of research questioning the reasoning ability of LLMs by showing their vulnerability under superficial changes of the input that do not alter the underlying logic.Works like Shi et al. (2023); Jiang et al. (2024) find that LLMs are easily distracted by irrelevant context or token level perturbation that does not change the underlying logic of the reasoning task.Mirzadeh et al. (2024) further demonstrate that the performance of LLMs declines when numerical values are altered in the questions from the GSM8K dataset.</p>
<p>There is yet another line of research that challenges the ability of LLMs to refrain from answering unanswerable problems.Ma et al. (2024); Li et al. (2024); Sun et al. (2024); Zhou et al. (2024a); Saadat et al. (2024) introduce minor modifications to existing math word problems to create unanswerable variants, and find that LLMs often generate hallucinatory answers for these unanswerable questions, even when they perform well on the original answerable datasets.However, these efforts rely on pre-existing math word problem sources, making them susceptible to training data contamination, limited in scope, and lacking rich structures for extended research.</p>
<p>To address these shortcomings, we propose TREECUT, a synthetic dataset capable of systematically generating an infinite number of unanswerable math word problems and their answerable counterparts.TREECUT considers problem represented by a tree, with nodes representing variables and edges representing formulas.Unanswerable problems are generated by removing an edge along root
X 1 X 2 X 4 X 3 X 1 =14 3X 1 -3X 4 =12 -2X 1 +3X 4 =-4 3X 2 -X 3 =13 root X 1 X 2 X 4 X 3 X 1 =14 3X 1 -3X 4 =12 3X 2 -X 3 =13</p>
<p>CUT</p>
<p>Question:</p>
<p>A burger costs 14 dollars.3 scrambled eggs cost 4 dollars less than 2 burgers.3 pies cost 12 dollars less than 3 burgers.A BLT sandwich costs 13 dollars less than 3 scrambled eggs.Question: how much does a BLT sandwich cost?</p>
<p>Solution to the answerable problem:</p>
<p>It is given as a fact that a burger costs 14 dollars.Combine with the fact that 3 scrambled eggs cost 4 dollars less than 2 burgers, we get a scrambled egg costs 8 dollars.Combine with the fact that a BLT sandwich costs 13 dollars less than 3 scrambled eggs, we get a BLT sandwich costs 11 dollars.</p>
<p>Solution to the unanswerable problem:</p>
<p>All we know about the prices of BLT sandwich and scrambled egg is: a BLT sandwich costs 13 dollars less than 3 scrambled eggs.There are 2 variables but only 1 linear formula, so we cannot calculate the price of a BLT sandwich.</p>
<p>Figure 1: The left and middle panels depict the tree structures corresponding to the answerable and unanswerable questions, respectively.In the right panel, the strike-through sentence represents the formula removed by the cut.The variable mappings to items are as follows: x 1 represents a burger, x 2 represents a scrambled egg, x 3 represents a BLT sandwich, and x 4 represents a pie.</p>
<p>the path from the root to the questioned variable.Our unanswerable dataset proves to be challenging even for GPT-4o and o3-mini.In addition, TreeCut allows precise control over the structural components of each problem, enabling detailed investigations into when and why LLMs produce hallucinations.Our analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations.Liu et al. (2021) utilize tree structures to represent and manipulate mathematical expressions during the reverse operation based data augmentation process for MWP solving.Opedal et al. (2024) introduced MathGAP, a framework for evaluating LLMs using synthetic math word problems with controllable proof tree characteristics.In contrast to their approach, the tree structure in our problemgeneration procedure is fundamentally different.In our work, each node represents a variable, and the questioned variable appears as a leaf.In their work, however, each node represents a logical statement, with the answer represented by the root.More importantly, we focus on unanswerable math word problems, an aspect that their study did not address.Yin et al. (2023) introduced SelfAware, consisting of unanswerable questions from five diverse categories.It includes less than 300 unanswerable mathematical problems.Li et al. (2024); Zhou et al. (2024a) generate unanswerable questions by prompting GPT4 to eliminate a necessary condition from the original problem, an then the modified questions are further checked or refined by human annotators.Ma et al. (2024) prompt GPT4 to modify problems from GSM8K.Sun et al. (2024) task human annotators to modify original questions in existing MWP datasets to make them unanswerable, creating a dataset composed of 2,600 answerable questions and 2,600 unanswerable questions.</p>
<p>Related Work</p>
<p>Unanswerable Math Problems</p>
<p>TREECUT: a Synthetic (Un)answerable Math Word Problem Dataset</p>
<p>For the purpose of our investigation, we aim to have full control over the various aspects that determine the underlying structure of a math word problem: the name of the entities, the numeric values, and the complexity of the problem.Furthermore, we seek to reliably generate unanswerable problems by precisely removing specific necessary conditions of our choosing.</p>
<p>To this end, we start with a special kind of answerable math word problem that can be represented as a tree, as illustrated in Figure 1.Within such a tree, each non-root node represents a variable, while the root is a uniquely reserved node.An edge from root gives value to a variable, while  an edge between two variables represents a linear formula of the two neighboring nodes.Given such a tree, any variable can be calculated following the unique path from the root to the node that represents the variable.Such a solving procedure does not require solving a linear equation system, as the solution only consists of carrying out basic arithmetic operations along the path.To guarantee that the arithmetic operations are well within the capacity of current frontier LLMs,we further restrict the unit price of each food item to be an integer between 5 and 15, and the coefficients of each linear equation taking non-zero integer values between -3 and 3. Finally, variables are randomly mapped to items, and then the formulas are translated to natural language using templates.The complete generation procedure, along with the templates used, is provided in Appendix A.</p>
<p>From an answerable math word problem described above, we generate an unanswerable problem by removing an edge along the path from the root to the questioned variable.In Figure 1, x 3 is the questioned variable.Along the path to the root, we remove the edge between x 1 and x 2 (denoted by a cut), rendering x 2 and x 3 undetermined, thus making the question unanswerable, as all we know about x 2 and x 3 is one single linear equation.A key benefit of such a generation procedure is that the distance from the questioned variable to the cut is also fully controlled, as we will see that this factor plays an important role in triggering LLM hallucination.</p>
<p>In summary, we can control the structure of problems via the following parameters:</p>
<p>-numVars: total number of variables, -ansDepth: distance from the root to the ques-tioned variable, -compositeName: boolean, whether the items in the question have composite names (e.g."a burger at Bistro Nice" versus "a burger"), -cutDepth: distance from the questioned variable to the cut, if an unanswerable problem is to be generated.Appendix A contains the detailed problem generation algorithm.</p>
<p>Experiments</p>
<p>We evaluate several state-of-the-art LLMs using TREECUT.Additionally, we analyze the hallucination rate of GPT-4o on unanswerable problems generated under different parameter configurations of TREECUT.</p>
<p>Experimental Setup</p>
<p>For each set of generation parameters, we randomly generate 500 problems.Unless stated otherwise, we employ a zero-shot prompting template that explicitly directs the model to indicate when a question is unanswerable due to insufficient conditions.A chain-of-thought system message is incorporated for all models except o3-mini1 .</p>
<p>Evaluating LLMs</p>
<p>In the first set of experiments, we generate unanswerable math word problems of varying difficulty to evaluate the following LLMs: Llama 3.1 Instruct with 8B and 70B parameters(Dubey et al., 2024), Qwen2.5 Instruct with 7B and 72B parameters (Yang et al., 2024), GPT-4o (Achiam et al., 2023), and o3-mini (OpenAI, 2025).Table 1 summarizes the results.None of the LLMs gives satisfactory results.Llama 3.1 8B, Qwen2.5 7B and 72B barely have any success identifying unanswerable problems.Llama 3.1 70B and GPT-4o struggle with more complex problems (ansDepth = 6, 8).o3-mini has the lowest hallucination for ansDepth = 6, 8.However, for the easiest case where ansDepth = 2 (in this setting, only 4 variables are mentioned in each problem), o3-mini displays a bias of making hallucinatory assumptions (see Appendix C.2 for examples).</p>
<p>To further investigate whether the LLMs face intrinsic challenges in recognizing unanswerable math word problems, we conduct another set of experiments using few-shot prompting.For each unanswerable problem, we construct a few-shot prompt by randomly selecting 3 answerable and 3 unanswerable problems, each accompanied by a full solution path and the correct final answer.We use sample size n=500.Results are summarized in Table 2. O3-mini greatly benefits from fewshot prompting, which is not surprising given our analysis in Appendix C.2.For shorter problems, o3-mini tends to recognize the lack of conditions during reasoning, but choose to make unreasonable assumptions to arrive at a final answer.Few-shot examples guide it to refrain from doing that.The hallucination rates of the other models remained largely unchanged.This suggests that the five models besides o3-mini face intrinsic challenges in recognizing unanswerable math word problems.</p>
<p>To investigate whether the unsatisfactory accuracy of identifying unanswerable problems comes from the incapability of the necessary mathematical operations, we evaluate the LLMs on the answerable counterparts of the unanswerable questions using the same zero-shot prompting template.For this experiment, a sample size of n = 100 is used.We observe that almost every model displays a significant gap between its ability of solving answerable problems and identifying unanswerable problems.For instance, GPT-4o correctly solves 84% of answerable problems for ansDepth = 8, but only correctly recognizes 36% of unanswerable problems.</p>
<p>Unanswerable Problem Structure and Hallucination</p>
<p>For a more fine-grained investigation of LLM's hallucination behavior under different structures of unanswerable problems, we analyze GPT-4o's hallucination rate on unanswerable problems generated under different parameter choices of numVars, ansDepth, compositeName and cutDepth.Tree Structure and Item Names To investigate the effect of (i) a deeper tree structure, (ii) a more complex tree structure, and (iii) composite item names, we consider the following parameter configurations:</p>
<p>• ansDepth ∈ {4, 5, 6, 7, 8}, which controls the depth of the questioned variable, • cutDepth = ⌊ansDepth/2⌋ • numVars = ansDepth + 2 (generates a more complex tree structure, with conditions unrelated to the questioned variable) or numVars = ansDepth (the tree structure degenerates into a single path), • compositeName: true or f alse.There are 5 × 2 × 2 = 20 configurations in total.We randomly generate 500 unanswerable problems for each configuration, and summarize GPT-4o's hallucination rate in Figure 2. In the figure, ⋆ Orange line represents complex tree structure, ⋆ blue line represents simple tree structure, -Solid line stands for composite item names, -Dashed line stands for simple item names.Examining each line individually, we observe that the hallucination rate increases as the depth of the questioned variable grows.Comparing solid and dashed lines of the same color, a more complex tree structure consistently results in a higher likelihood of hallucination across different ansDepth values.Comparing orange and blue lines of the same linestyle, composite item names consistently lead to a higher likelihood of hallucination compared to simple item names.Location of the Cut For each unanswerable problem, the cut always happens along the path from the root to the questioned variable.Does the location of the cut change hallucination ratio?We vary cutDepth from 1 to 7 while keeping ansDepth = 8 and other parameters fixed.In the right panel of Figure 3, we see that cutDepth = 3, 4, 5, 6 all trigger over 60% hallucination for GPT-4o (with cutDepth = 5 triggering over 70%), but a cutDepth = 1, 2, 7 only triggers less than 50% of hallucination, which means that GPT-4o is more confused when the cut happens around the middle point along the path, comparing to that happens near the root or the questioned variable.</p>
<p>Conclusion of Experiments</p>
<p>Our findings indicate that the unanswerable math word problems generated by TREECUT effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios.Focusing on GPT-4o, we further observe that hallucinations are more likely to occur when the problem exhibits (i) a deeper tree structure, (ii) a more complex tree structure, (iii) composite item names, or (iv) a cut positioned around the middle of the path.These results underscore the challenges LLMs face in handling unanswerable math problems.</p>
<p>Limitations</p>
<p>Our synthetic dataset is specifically designed for math word problems, representing only a small subset of the broader field of mathematics.Additionally, our evaluations are based on zero-shot and few-shot chain-of-thought prompting.We do not explore alternative prompting techniques commonly used in LLM-based mathematical reasoning studies, which may impact performance comparisons.</p>
<p>A Generation of the Math Word Problems</p>
<p>Algorithm 1 Generating Math Word Problem using Random Tree Require: numVars ≥ ansDepth ≥ 2 Require: unanswerable ∈ {true, f alse}, order ∈ {"forward", "backward", "random"} Require: cutDepth: int 1: if unanswerable = true then Require: cutDepth: int, satisfying 1 ≤ cutDepth &lt; ansDepth 2: end if ▷ (i) Sample a dictionary of variable values 3: varDict ← {} 4: for i ← 1 to numVars do varDict[x i ] ← v 7: end for ▷ (ii) Build the random tree 8: Assign root as the parent of x 1 9: for i ← 2 to ansDepth do 10: Assign x i−1 as the parent of x i 11: end for ▷ Finish building the path from the root to the questioned variable ▷ Assign the remaining nodes 12: for i ← ansDepth + 1 to numVars do 13:</p>
<p>Randomly select a node x p in the tree 14:</p>
<p>Assign x as the parent of x i 15: end for ▷ (iii) Get the list of all edges via a breadth-first traversal 16: edgeList ← the list of edges collected by a breadth-first traversal (see Algorithm 2) ▷ (iv) For unanswerable problems, create the cut 17: if unanswerable = true then 18:</p>
<p>Remove (x ansDepth-cutDepth-1 , x ansDepth-cutDepth ) from edgeList 19: end if ▷ (v) Generate a formula for each edge, and store in forumlaList 20: formulaList ← [ ] 21: for edge (x i , x j ) in edgeList do
22: Sample a, b ∈ {−3, −2, −1, 1, 2, 3} 23: Define formula ← a • x i + b • x j = a • varDict[x i ] + b • varDict[x j ] 24:
Append formula to formulaList ▷ So that formulaList has the same order as edgeList 25: end for ▷ (vi) Adjust the ordering of formulaList according to order 26: if order = "backward" then 27:</p>
<p>Reverse formulaList 28: end if 29: if order = "random" then 30:</p>
<p>Random Shuffle formulaList 31: end if return formulaList ▷ Formulas serving as conditions of the problem.</p>
<p>Algorithm 1 generates formulaList, which contains the formulas that will serve as the conditions of the problem.To translate that into natural language, item names will be sampled according to the compositeName option.Then, formulaList can be translated to natural language using pre-defined templates.The question sentence will simply be "what is the price of {item name of the questioned variable}".</p>
<p>We want to point out that although all the variables are assigned a value in varDict, this is purely for the sake of (i) subsequently generating the random formulas (ii) guaranteeing that all calculable variables will have values between 5 and 15.When unanswerable = true, the cut will guarantee that the problem is unanswerable.</p>
<p>In the following, we also detail the simple breadth-first traversal algorithm for getting all the edges from the tree, which enables us to control the order of the conditions in the problem.</p>
<p>Algorithm 2 Breadth-First Traversal to Get Edges Require: root: the root of a tree ▷ Get the list of all edges via a breadth-first traversal 1: edgeList ← [ ], q ← a queue containing root 2: while q is not empty do 3: node ← q.dequeue()</p>
<p>4:</p>
<p>for child ∈ node.childrendo Add child to q 7: end for 8: end while return edgeList</p>
<p>Given a formulaList, each formula is translated via the following template:</p>
<p>Formula Translation Template</p>
<p>Suppose x 1 stands for <dish1> at <restaurant1>, and x 2 stands for <dish2> at <restaurant2>.</p>
<p>1.If the formula is a 1 x 1 + a 2 x 2 = b where a 1 , a 2 &gt; 0, then the formula will be translated to: "a 1 <dish1> at <restaurant1> and a 2 <dish2> at <restaurant2> cost b dollars".</p>
<ol>
<li>
<p>If the formula is a 1 x 1 − a 2 x 2 = 0 where a 1 , a 2 &gt; 0, then the formula will be translated to: "The price of a 1 <dish1> at <restaurant1> is the same as the price of a 2 <dish2> at <restaurant2>".</p>
</li>
<li>
<p>If the formula is a 1 x 1 − a 2 x 2 = b where a 1 , a 2 &gt; 0 and b &gt; 0, then the formula will be translated to either "a 1 <dish1> at <restaurant1> cost b dollars more than a 2 <dish2> at <restaurant2>" or "a 2 <dish2> at <restaurant2> cost b dollars less than a 1 <dish1> at <restaurant1>".</p>
</li>
<li>
<p>The case of b &lt; 0 is similar to the case above.</p>
</li>
</ol>
<p>B Details of Experiments B.1 Prompt Template</p>
<p>Below is the prompt template used for evaluation, which is a 0-shot prompting template with chain-ofthought system message.For reasoning models like o3-mini, we do not use the chain-of-thought system message.</p>
<p>Zero-shot Evaluation Prompt Template // system message (for non-reasoning models) As an expert problem solver, solve step by step the following mathematical questions.</p>
<p>// user message Please solve the following math question, and then answer in the form 'Answer:</p>
<p>x'.If the known conditions are not sufficient to answer the question, please answer in the form 'Answer: unknown.'.</p>
<p>Question: {{question}}</p>
<p>Your solution:</p>
<p>B.2 Evaluation</p>
<p>For all the models other than o3-mini, we set {max_tokens=4000, temperature=0}.o3-mini does not allow setting temperature, and we set {max_completion_tokens=32000, reasoning_effort="high"}.</p>
<p>GPT-4o and o3-mini are evaluated via calling OpenAI API.The exact version numbers are "gpt-4o-2024-08-06" and "o3-mini-2025-01-31", respectively.When evaluating model's answer to an unanswerable problem, we change the response to all lower case, and if "unknown" is mentioned after the last occurrence of the trigger word "answer", it is counted as a correct answer.</p>
<p>C Examples of Hallucinatory Answers C.1 GPT-4o</p>
<p>Example 1: A BLT sandwich at Urban Plate costs 13 dollars.3 burgers at Taste Good Cuisine cost 6 dollars more than 3 fruit tarts at Taste Good Cuisine.A BLT sandwich at Taste Good Cuisine costs 5 dollars.A fruit tart at Taste Good Cuisine costs 9 dollars less than 3 burgers at Urban Plate. 2 burgers at Taste Good Cuisine cost 8 dollars more than a fruit tart at Urban Plate.Question: how much does a burger at Taste Good Cuisine cost?Model answer:</p>
<p>To solve this problem, we need to set up equations based on the information given and solve for the unknowns.</p>
<p>Let's define the variables: -Let ( B_T ) be the cost of a burger at Taste Good Cuisine.</p>
<p>-Let ( F_T ) be the cost of a fruit tart at Taste Good Cuisine.</p>
<p>-Let ( B_U ) be the cost of a burger at Urban Plate.</p>
<p>-Let ( F_U ) be the cost of a fruit tart at Urban Plate.</p>
<p>We are given the following information:</p>
<p>1.A BLT sandwich at Urban Plate costs 13 dollars.2. 3 burgers at Taste Good Cuisine cost 6 dollars more than 3 fruit tarts at Taste Good Cuisine.</p>
<p>[ 3B_T = 3F_T + 6 ] 3. A BLT sandwich at Taste Good Cuisine costs 5 dollars.4. A fruit tart at Taste Good Cuisine costs 9 dollars less than 3 burgers at Urban Plate.</p>
<p>[ F_T = 3B_U -9 ] 5. 2 burgers at Taste Good Cuisine cost 8 dollars more than a fruit tart at Urban Plate.</p>
<p>[ 2B_T = F_U + 8 ]</p>
<p>We need to find the cost of a burger at Taste Good Cuisine, ( B_T ).</p>
<p>From equation (2), we can express ( F_T ) in terms of ( B_T ):
[ F_T = B_T -2 ]
Substitute this expression for ( F_T ) into equation ( 4):
[ B_T -2 = 3B_U -9 ] [ B_T = 3B_U -7 ]
Now, let's substitute ( F_T = B_T -2 ) into equation ( 5):
[ 2B_T = (B_T -2) + 8 ] [ 2B_T = B_T + 6 ] [ B_T = 6 ]
Thus, the cost of a burger at Taste Good Cuisine is 6 dollars.</p>
<p>Answer: 6</p>
<p>Mistake: When the model did "Now, let's substitute F_T = B_T -2 into equation ( 5)", it actually confused F_T with F_U in equation ( 5).</p>
<p>Example 2: 3 Greek salads at Texas BBQ cost 6 dollars less than 3 pieces of cheese cake at Texas BBQ. 3 burgers at Texas BBQ and 2 pieces of cheese cake at Texas BBQ cost 59 dollars.3 burgers at Taste Good Cuisine and 2 Greek salads at Taste Good Cuisine cost 42 dollars.2 burgers at Texas BBQ and a piece of cheese cake at Taste Good Cuisine cost 32 dollars.3 burgers at Texas BBQ cost 15 dollars more than 3 fruit tarts at Taste Good Cuisine.A burger at Taste Good Cuisine costs 10 dollars.A fruit tart at Texas BBQ costs 2 dollars more than a Greek salad at Texas BBQ.Question: how much does a Greek salad at Texas BBQ cost?Model answer:</p>
<p>To solve this problem, we need to set up equations based on the information given and solve for the cost of a Greek salad at Texas BBQ.Let's define the variables:</p>
<ul>
<li>The calculations are consistent with the given conditions.Therefore, the cost of a Greek salad at Texas BBQ is:</li>
</ul>
<p>Answer: 6</p>
<p>Mistake: At step 3, model calculated the value of "G", but from the start of the solution, "G" denotes the "cost of a Greek salad at Texas BBQ", not the cost of a Greek salad at Taste Good Cuisine.</p>
<p>C.2 o3-mini</p>
<p>Example 1: A pizza at Taste Good Cuisine and 3 lasagnas at Taste Good Cuisine cost 48 dollars.A lasagna at Urban Plate costs 6 dollars.3 lasagnas at Urban Plate and 3 pizzas at Urban Plate cost 63 dollars.Question: how much does a lasagna at Taste Good Cuisine cost?Model answer:</p>
<p>Let p be the price of a pizza at Taste Good Cuisine and l be the price of a lasagna at Taste Good Cuisine.We are given:</p>
<ol>
<li>p + 3l = 48.(Taste Good Cuisine) 2. A lasagna at Urban Plate costs 6 dollars.3. 3 lasagnas at Urban Plate and 3 pizzas at Urban Plate cost 63 dollars.</li>
</ol>
<p>From (3), the total cost of 3 lasagnas at Urban Plate is 3 × 6 = 18 dollars.This means that the 3 pizzas at Urban Plate cost 63 -18 = 45 dollars, so one pizza at Urban Plate costs 45/3 = 15 dollars.</p>
<p>A common (though unstated) assumption in these puzzles is that dishes with the same name cost the same across restaurants.Thus, we assume that a pizza at Taste Good Cuisine also costs 15 dollars.Substitute p = 15 into equation ( 1 Mistake: The model made an assumption: "A common (though unstated) assumption in these puzzles is that dishes with the same name cost the same across restaurants."However, this cannot be a mathematically valid assumption because following this assumption, it arrives at the conclusion that the price for lasagna at Urban Plate is not the same as that at Taste Good Cuisine (6 dollars versus 11 dollars).</p>
<p>We randomly selected 20 problems from the ansDepth = 2 set that triggered hallucinatory answer from o3-mini and manually checked the model answer, all of them shares this same mistake.Below is a similar example:</p>
<p>Example 2: A scrambled egg at Bistro Nice and a piece of cheese cake at Mike's Place cost 21 dollars.2 scrambled eggs at Bistro Nice cost 6 dollars more than 2 pieces of cheese cake at Bistro Nice.A scrambled egg at Mike's Place costs 6 dollars.Question: how much does a scrambled egg at Bistro Nice cost?Model answer:</p>
<p>Let S_B = price of a scrambled egg at Bistro Nice, C_B = price of a piece of cheesecake at Bistro Nice, C_M = price of a piece of cheesecake at Mike's Place.</p>
<p>The problem gives: 1. S_B + C_M = 21 dollars.2. Two scrambled eggs at Bistro Nice cost 6 dollars more than two cheesecakes at Bistro Nice: 2S_B = 2C_B + 6 → S_B = C_B + 3. 3. A scrambled egg at Mike's Place costs 6 dollars (this is extra information).</p>
<p>Without further assumptions the system involves three unknowns (S_B, C_B, C_M) and only two equations.However, it is common in these problems to assume that when the same item is mentioned (here "cheesecake") it has a consistent price from one restaurant to the other.That is, we assume C_M = C_B.</p>
<p>Under that assumption the first equation becomes: S_B + C_B = 21.Then using S_B = C_B + 3 we substitute:</p>
<p>(C_B + 3) + C_B = 21 → 2C_B + 3 = 21 → 2C_B = 18 → C_B = 9.Thus, S_B = 9 + 3 = 12.</p>
<p>Answer: 12</p>
<p>Mistake: Same as above.</p>
<p>Example 3: 3 BLT sandwiches at Bistro Nice cost 6 dollars more than 3 pieces of cheese cake at Urban Plate. 3 burgers at Urban Plate cost 18 dollars less than 3 pieces of cheese cake at Bistro Nice. 2 BLT sandwiches at Urban Plate and 2 scrambled eggs at Urban Plate cost 34 dollars.A BLT sandwich at Urban Plate costs 8 dollars.3 pieces of cheese cake at Bistro Nice cost 12 dollars more than 3 burgers at Bistro Nice. 2 burgers at Urban Plate cost 10 dollars less than 2 scrambled eggs at Bistro Nice. 3 pieces of cheese cake at Bistro Nice and a piece of cheese cake at Urban Plate cost 54 dollars.Question: how much does a piece of cheese cake at Urban Plate cost?Model answer:</p>
<p>Let CC_U be the cost of one piece of cheese cake at Urban Plate.</p>
<p>Step 1. From "3 BLT sandwiches at Bistro Nice cost 6 dollars more than 3 pieces of cheese cake at Urban Plate": 3•BLT_B = 3•CC_U + 6 → BLT_B = CC_U + 2 (1)</p>
<p>Figure 2 :
2
Figure 2: Hallucination percentage under different configurations of unanswerable problems, plotted against varying ansDepth.</p>
<p>Figure 3 :
3
Figure 3: Hallucination percentage versus cutDepth.Left panel has ansDepth = 7.Right panel has ansDepth = 8.</p>
<p>( G ): Cost of a Greek salad at Texas BBQ -( C ): Cost of a piece of cheesecake at Texas BBQ -( B ): Cost of a burger at Texas BBQ -( F ): Cost of a fruit tart at Taste Good Cuisine -( B_T ): Cost of a burger at Taste Good Cuisine (given as $10) -( F_T ): Cost of a fruit tart at Texas BBQ Now, let's translate the information into equations:1."3 Greek salads at Texas BBQ cost 6 dollars less than 3 pieces of cheesecake at Texas BBQ.3 burgers at Taste Good Cuisine and 2 Greek salads at Taste Good Cuisine cost 42 dollars."at Texas BBQ and a piece of cheesecake at Taste Good Cuisine cost 32 dollars.don'thave enough information about ( C_T ), the cost of cheesecake at Taste Good Cuisine, so we can't use this equation directly.)5."3 burgers at Texas BBQ cost 15 dollars more than 3 fruit tarts at Taste Good Cuisine.fruittart at Texas BBQ costs 2 dollars more than a Greek salad at Texas BBQ.'s solve the equations:From equation 3, we found ( G = 6 ).Let's verify if this value is consistent with other equations:</p>
<p>Table 1 :
1
Percentage of hallucination of various LLMs at different ansDepth values for unanswerable problems, zero-shot
ansDepth Llama-8B Llama-70B Qwen-7B Qwen-72B GPT-4o o3-mini280.2%24.6%84.6%59.8%12.0%44.0%486.2%40.2%90.4%82.8%18.0%25.2%686.0%63.4%95.6%88.4%47.4%19.2%884.2%65.0%93.4%85.2%64.0%25.6%promptingansDepth Llama-8B Llama-70B Qwen-7B Qwen-72B GPT-4o o3-mini272.8%33.6%80.4%55.4%18.4%3.2%479.0%57.6%94.6%84.8%28.8%2.4%679.6%72.4%92.6%84.8%41.8%3.6%878.6%76.8%94.4%83.0%51.0%3.0%</p>
<p>Table 2 :
2
Percentage of hallucination of various LLMs at different ansDepth values for unanswerable problems, few-shot prompting</p>
<p>Table 3 :
3
Accuracy of various LLMs at different ansDepth levels for answerable problems.The percentage in parentheses represents the proportion of answerable questions incorrectly identified as unanswerable.</p>
<p>Following OpenAI's guidelines of reasoning models.
AcknowledgementsWe thank the anonymous reviewers for their valuable feedback, which helped improve the quality of this work.Step 2. From "3 burgers at Urban Plate cost 18 dollars less than 3 pieces of cheese cake at Bistro Nice": 3•BUR_U = 3•CC_B -18 → With BUR_U determined later, we use this after finding BUR_U.Step Step 4. "A BLT sandwich at Urban Plate costs 8 dollars": BLT_U = 8.Step 5. "3 pieces of cheese cake at Bistro Nice cost 12 dollars more than 3 burgers at Bistro Nice":Step 6. "2 burgers at Urban Plate cost 10 dollars less than 2 scrambled eggs at Urban Plate":Step 7. Now return to the burgers at Urban Plate in Step 2:Step 8. "3 pieces of cheese cake at Bistro Nice and a piece of cheese cake at Urban Plate cost 54 dollars": 3•CC_B + CC_U = 54 → 3•10 + CC_U = 54 → 30 + CC_U = 54 → CC_U = 24.The question asks: "how much does a piece of cheese cake at Urban Plate cost?"Answer: 24 Mistake: Step 6. "2 burgers at Urban Plate cost 10 dollars less than 2 scrambled eggs at Urban Plate".That is a hallucinatory condition.The actual condition is "2 burgers at Urban Plate cost 10 dollars less than 2 scrambled eggs at Bistro Nice".
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. </p>
<p>Roda: Reverse operation based data augmentation for solving math word problems. Qianying Liu, Wenyu Guan, Sujian Li, Fei Cheng, Daisuke Kawahara, Sadao Kurohashi, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 302021</p>
<p>Large language models are unconscious of unreasonability in math problems. Jingyuan Ma, Damai Dai, Lei Sha, Zhifang Sui, arXiv:2403.193462024arXiv preprint</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint</p>
<p>Mathgap: Out-of-distribution evaluation on problems with arbitrarily complex proofs. Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan, arXiv:2410.135022024arXiv preprint</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems?. Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2025. Feb. 5, 2025OpenAIOpenai o3 mini</p>
<p>Asir Saadat, Tasmia Binte Sogir, Md Taukir, Azam Chowdhury, Syem Aziz, arXiv:2410.13029When not to answer: Evaluating prompts on gpt models for effective abstention in unanswerable math word problems. 2024arXiv preprint</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Benchmarking hallucination in large language models based on unanswerable math word problem. Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Hui Zhao, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Towards robust numerical question answering: Diagnosing numerical capabilities of nlp systems. Jialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, Dongmei Zhang, arXiv:2211.074552022arXiv preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, arXiv:2409.12122Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. 2024arXiv preprint</p>
<p>Do large language models know what they don't know?. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuan-Jing Huang, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, arXiv:2308.079212023arXiv preprint</p>
<p>Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F Wong, Xiaowei Huang, Qiufeng Wang, Kaizhu Huang, arXiv:2407.08733Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. 2024aarXiv preprint</p>
<p>Mathattack: Attacking large language models towards math solving ability. Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024b</p>            </div>
        </div>

    </div>
</body>
</html>
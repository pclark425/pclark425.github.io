<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9024 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9024</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9024</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-268041290</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.18225v1.pdf" target="_blank">CogBench: a large language model walks into a psychology lab</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9024.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9024.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogBench (cognitive psychology benchmark for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source benchmark introduced in this paper that adapts seven canonical cognitive-psychology experiments into a suite of ten behavioral metrics to phenotype LLM behavior across learning and decision-making tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CogBench (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmark suite composed of seven text-based cognitive experiments (probabilistic reasoning, horizon task, restless bandit, instrumental learning, two-step task, temporal discounting, BART) producing ten behavioral metrics and six performance metrics (human-normalized where 1 = average human).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven cognitive psychology experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A battery of text-adapted cognitive tasks assessing probabilistic reasoning, exploration (directed & random), meta-cognition, instrumental learning (learning rate & optimism bias), model-based vs model-free RL (two-step), temporal discounting (delay preferences and anomalies), and risk-taking (BART).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Applied to 35 LLMs; summary findings include GPT-4 and Claude-1 achieving human-level performance in most tasks (reported as 'human-level in 5/6 tasks' for the paper's performance metrics), many models super-human on horizon task (except text-bison), BART challenging for all models, large variance across models on risk and temporal discounting.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-normalized scale used where value 1 = average human subject; exact per-task human numeric scores not reproduced in the paper (normalization described but raw human values not listed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Benchmark authors report which models match, exceed, or fall below human baselines per task in aggregate (e.g., GPT-4 and Claude-1 ~ human-level across most tasks; many models surpass humans on horizon task; BART below humans).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>All experiments run in-context only (no fine-tuning); temperature set to 0 (deterministic); task-specific simulation counts: probabilistic reasoning default 100 sims, horizon default 100, restless bandit default 10, instrumental learning default 10, two-step default 100, temporal discounting single (non-procedural) run, BART default 10. Prompt templates and behavioral-model fitting (e.g., RW model, regression for model-basedness) are provided in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human comparisons rely on human-normalized metrics but raw human scores are not fully reproduced in main text; LLM trial counts often far smaller than human experiments (e.g., restless bandit LLMs ~80 trials vs humans 400), different response formats (LLM confidence 0–1 vs human Likert), and context-length constraints limit trial counts; some tasks not fully procedurally generated (temporal discounting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9024.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large proprietary transformer-based LLM from OpenAI, evaluated across CogBench and found to be among the highest-performing models in the suite with human-like and sometimes super-human behavioral metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large transformer model (OpenAI); in the authors' feature table reported parameter count '1760' (as provided in paper table). Marked as RLHF-enabled in the study's metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1760</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (all seven tasks; aggregated performance and behavioral metrics reported)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Evaluated on probabilistic reasoning, horizon, restless bandit (meta-cognition), instrumental learning, two-step (model-basedness), temporal discounting, and BART.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as one of the top performers: 'human-level scores in most tasks (five out of six)' per paper aggregated performance; excels in probabilistic reasoning and instrumental learning; solves restless bandit and two-step tasks with high meta-cognition and model-basedness (GPT-4 'significantly surpassing human levels' in model-basedness); poor performance on BART (risk-taking) like other models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Paper uses human-normalized metrics where 1 = average human; GPT-4 reached ~human-level (value ≈1) in most performance metrics (exact numeric normalized scores not tabulated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Generally matches or exceeds average human on many performance metrics (notably model-basedness where it surpasses humans); exceeds humans on horizon task (exploitation-driven super-human rewards) but underperforms humans on BART risk-taking.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Deterministic prompting (temperature 0), in-context only; CoT and SB prompt-engineering applied in subset tests — GPT-4 included in those and showed improvements: CoT increased probabilistic posterior accuracy by +9.01% (average across five models) and model-basedness by +64.59%; SB increased posterior accuracy by +3.10% and model-basedness by +118.59% (aggregated reported percentages are weighted averages across five models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human-normalized reporting prevents direct numeric comparison to raw human scores in paper; GPT-4 comparisons could be affected by proprietary hidden pre-prompts and limited transparency about training data; number of simulated trials differs from human experiments which may bias measures like meta-cognition and learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9024.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary LLM (Anthropic) included among the top performers in CogBench — reported to achieve human-level scores on most tasks and to exhibit relatively human-like behavior after RL-based alignment (RLAIF/RLHF).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic model listed in the paper with parameter count '100' (table). Classified as using RLAIF (reinforcement learning from AI feedback) in the authors' metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same battery: probabilistic reasoning, horizon, restless bandit, instrumental learning, two-step, temporal discounting, BART.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as among the best performers: 'human-level scores in most tasks (five out of six)'; strong on probabilistic reasoning and instrumental learning; one of the few models to handle restless bandit and two-step well; BART remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Evaluations reported on a human-normalized scale (1 = average human); Claude-1 reached ~human-level across multiple performance metrics (exact numeric normalized values not shown in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Matches average human performance across multiple tasks; shows improved human-likeness consistent with RLHF/RLAIF effects described in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>In-context evaluation only, temperature 0. Claude-1 included in prompt-engineering tests (CoT and SB) subset where CoT and SB both improved probabilistic reasoning and model-basedness on average (aggregate improvements reported across five models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Claude-1 is proprietary and not fully transparent about training/data; RLAIF vs RLHF distinctions noted; direct numeric human comparisons not provided; as with other models, differences in trial counts and response formats vs human data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9024.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-davinci-003 (GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used GPT-3-series model (text-davinci-003) evaluated on CogBench; shows competence on several tasks (probabilistic reasoning, instrumental learning) and mixed behavior on risk and temporal discounting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3-era model; the authors' table lists parameter count '170' and marks it as RLHF-enabled in their metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>170</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Evaluated across all seven cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Described as competent in probabilistic reasoning and instrumental learning; in temporal discounting appears more far-sighted for some variants; BART shows extreme behaviors for different models (some always risk, some never risk) — text-davinci-003 among those more far-sighted in temporal discounting per paper narrative.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Compared to human-normalized metric (1 = average human); specific per-task normalized values not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Generally performs at or above chance and often in the mid-range of models; performance varies by task — sometimes near-human on learning tasks but inconsistent in risk and exploration behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Deterministic prompts (temperature 0), in-context only; included in aggregated analyses and figures (performance and behavioral phenotypes).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper notes optimism bias across most LLMs (stronger learning from positive feedback) and that code fine-tuning did not systematically improve behavior; text-davinci-003 specifics subject to these general caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9024.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-bison@002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-2 for text (text-bison@002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM-2 text model (labeled text-bison@002) evaluated in CogBench; notable as one of the few models that did not show super-human performance on the horizon task and displayed different bias patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison@002 (PaLM-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google PaLM-2 variant for text; authors' feature table lists '340' under parameter column and marks RLHF usage in metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Assessed on same battery; particular results called out for horizon, temporal discounting, and optimism-bias patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Paper reports that most models outperform humans on the horizon task except text-bison; text-bison also is an exception regarding optimism bias (it did not show very strong optimism bias unlike most other models) and tended to be more myopic on temporal discounting relative to some other models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Benchmarked against human-normalized measures (1 = average human); exact numeric human baselines not listed in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Per paper, text-bison underperforms relative to other models on some exploration measures (horizon) and differs in behavioral biases (less optimism bias), placing it below the group-average super-human horizon performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>In-context only, temperature 0; included in prompt-engineering subset for CoT/SB experiments (the five-model subset included text-bison) where aggregated improvements were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors caution that cross-model comparisons can be affected by hidden proprietary pre-prompts, dataset transparency, and context-length limits; text-bison behavior exemplifies heterogeneity across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9024.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter open-source foundation model (LLaMA-2-70) included in CogBench; shows differing behavior from its fine-tuned 'chat' variant on risk, discounting and exploration despite similar task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-2 family model with ~70B parameters as listed in paper; not fine-tuned with conversational RLHF in this base variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same battery of cognitive tasks; comparisons with LLaMA-2-70-chat highlight effects of fine-tuning/conversational adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performance on aggregate tasks similar to its -chat variant, with competence in probabilistic reasoning and instrumental learning; however, placed differently on behavioral axes: LLaMA-2-70 was reported as myopic on temporal discounting for some models and showed extreme risk-taking contrasts with its chat variant.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Evaluated on human-normalized scale; paper emphasizes behavioral divergence relative to humans rather than raw numeric human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance scores (aggregate) close to LLaMA-2-70-chat, but behavioral metrics (risk-taking, temporal discounting, random exploration) diverge—illustrating that similar performance can mask very different strategies compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>In-context evaluation, temperature 0; number of simulations task-dependent (see CogBench defaults); comparison explicitly made between base and -chat (fine-tuned) versions to probe effects of conversational fine-tuning / RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors stress that fine-tuned variants (e.g., -chat) can behave quite differently and that behavioral phenotyping is necessary to reveal such divergences; openness about training and pre-prompts varies across models, complicating attribution of causes for behavioral differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9024.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-70-chat (fine-tuned chat variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-specialized fine-tuned variant of LLaMA-2-70 (adds RLHF/conversational fine-tuning) that shows markedly different behavioral profiles (risk, discounting, exploration) than its base model despite similar aggregate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned conversational version of LLaMA-2-70; same architectural base but additional fine-tuning for chat and RLHF-like alignment present in metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Evaluated across same battery; used particularly to contrast effects of fine-tuning and RLHF on behavioral metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregate task performance similar to base LLaMA-2-70, competent on probabilistic reasoning and instrumental learning; behaviorally diverged in risk-taking and exploration—e.g., LLaMA-2-70-chat sometimes exhibited opposite risk-taking behavior to base model.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Compared to human-normalized metrics (1 = average human); specific numeric human baselines not listed in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance parity with base model but behavioral differences indicate fine-tuning/RLHF can shift strategies away from or toward human-like behavior depending on metric.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>In-context evaluation only (no further fine-tuning performed by authors); chat variant included to analyze hierarchical grouping in multilevel regressions (to account for model families).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper highlights that fine-tuning/hidden pre-prompts can substantially change behavior; absence of full transparency on fine-tuning details makes causal attribution tentative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9024.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 40B-parameter LLM evaluated in CogBench; included among open-source family where regression analysis showed open-source models take fewer risks compared to proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B (and instruct variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open large language model (40B parameters in authors' table); both base and instruct variants evaluated; marked open-source in metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Assessed across the CogBench tasks; included in analyses comparing open-source vs proprietary models on behavioral metrics like risk-taking.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Not singled out as top-performing; part of open-source group that, holding other features equal, exhibited less risky decisions compared to proprietary models (paper reports negative effect of 'open-source' on risk-taking in regression).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Benchmarked on human-normalized scale; raw human values not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance generally mid-range among models; notably, open-source models (including Falcon) were found to take fewer risks than proprietary counterparts in multilevel regression (β = −0.612 ± 0.11 for 'open-source' effect on risk; p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>In-context evaluation, temperature 0; included base and instruct variants; used default task simulation counts (see CogBench).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Regression controls for model family but transparency about training data and hidden prompting differs across models; the open-source vs proprietary risk result is correlational and may reflect multiple engineering differences beyond 'open-source' label.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9024.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9024.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPT-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MPT-30B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 30B-parameter open-source foundation model (MosaicML MPT-30B) evaluated in CogBench, included with instruct and chat variants; used to probe behavior across open-source family members.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MPT-30B (and instruct/chat variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source model family (30B parameters in paper table) with base, instruct, and chat variants; some variants marked as conversational in metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogBench (seven tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same suite of cognitive tasks as other models; included in analyses contrasting open-source and proprietary models and examining effects of conversational fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Part of the broad set of models where performance varied by task; no standout top-level performance reported in main text, but included in aggregate figures (Figure 2) and appendices (full results for 35 models).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Compared using human-normalized metrics; specific numeric human baselines not enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance ranged across tasks; contributes to group-level findings such as RLHF improving human-likeness and open-source models being less risk-prone in regression analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Deterministic in-context evaluation, temperature 0; some MPT variants are conversational (chat/instruct) and these family relationships were modeled with multilevel regressions to account for nested fine-tuned variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper highlights limited transparency for some models and sparse training metadata for others; results for MPT variants are presented in aggregate and detailed per-model results are in Appendix C (not fully reproduced in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand gpt-3. <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning. <em>(Rating: 2)</em></li>
                <li>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks. <em>(Rating: 2)</em></li>
                <li>Inducing anxiety in large language models increases exploration and bias. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9024",
    "paper_id": "paper-268041290",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "CogBench",
            "name_full": "CogBench (cognitive psychology benchmark for LLMs)",
            "brief_description": "An open-source benchmark introduced in this paper that adapts seven canonical cognitive-psychology experiments into a suite of ten behavioral metrics to phenotype LLM behavior across learning and decision-making tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CogBench (benchmark)",
            "model_description": "Benchmark suite composed of seven text-based cognitive experiments (probabilistic reasoning, horizon task, restless bandit, instrumental learning, two-step task, temporal discounting, BART) producing ten behavioral metrics and six performance metrics (human-normalized where 1 = average human).",
            "model_size": null,
            "test_battery_name": "CogBench (seven cognitive psychology experiments)",
            "test_description": "A battery of text-adapted cognitive tasks assessing probabilistic reasoning, exploration (directed & random), meta-cognition, instrumental learning (learning rate & optimism bias), model-based vs model-free RL (two-step), temporal discounting (delay preferences and anomalies), and risk-taking (BART).",
            "llm_performance": "Applied to 35 LLMs; summary findings include GPT-4 and Claude-1 achieving human-level performance in most tasks (reported as 'human-level in 5/6 tasks' for the paper's performance metrics), many models super-human on horizon task (except text-bison), BART challenging for all models, large variance across models on risk and temporal discounting.",
            "human_baseline_performance": "Human-normalized scale used where value 1 = average human subject; exact per-task human numeric scores not reproduced in the paper (normalization described but raw human values not listed).",
            "performance_comparison": "Benchmark authors report which models match, exceed, or fall below human baselines per task in aggregate (e.g., GPT-4 and Claude-1 ~ human-level across most tasks; many models surpass humans on horizon task; BART below humans).",
            "experimental_details": "All experiments run in-context only (no fine-tuning); temperature set to 0 (deterministic); task-specific simulation counts: probabilistic reasoning default 100 sims, horizon default 100, restless bandit default 10, instrumental learning default 10, two-step default 100, temporal discounting single (non-procedural) run, BART default 10. Prompt templates and behavioral-model fitting (e.g., RW model, regression for model-basedness) are provided in appendices.",
            "limitations_or_caveats": "Human comparisons rely on human-normalized metrics but raw human scores are not fully reproduced in main text; LLM trial counts often far smaller than human experiments (e.g., restless bandit LLMs ~80 trials vs humans 400), different response formats (LLM confidence 0–1 vs human Likert), and context-length constraints limit trial counts; some tasks not fully procedurally generated (temporal discounting).",
            "uuid": "e9024.0",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4)",
            "brief_description": "A large proprietary transformer-based LLM from OpenAI, evaluated across CogBench and found to be among the highest-performing models in the suite with human-like and sometimes super-human behavioral metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary large transformer model (OpenAI); in the authors' feature table reported parameter count '1760' (as provided in paper table). Marked as RLHF-enabled in the study's metadata.",
            "model_size": "1760",
            "test_battery_name": "CogBench (all seven tasks; aggregated performance and behavioral metrics reported)",
            "test_description": "Evaluated on probabilistic reasoning, horizon, restless bandit (meta-cognition), instrumental learning, two-step (model-basedness), temporal discounting, and BART.",
            "llm_performance": "Reported as one of the top performers: 'human-level scores in most tasks (five out of six)' per paper aggregated performance; excels in probabilistic reasoning and instrumental learning; solves restless bandit and two-step tasks with high meta-cognition and model-basedness (GPT-4 'significantly surpassing human levels' in model-basedness); poor performance on BART (risk-taking) like other models.",
            "human_baseline_performance": "Paper uses human-normalized metrics where 1 = average human; GPT-4 reached ~human-level (value ≈1) in most performance metrics (exact numeric normalized scores not tabulated in main text).",
            "performance_comparison": "Generally matches or exceeds average human on many performance metrics (notably model-basedness where it surpasses humans); exceeds humans on horizon task (exploitation-driven super-human rewards) but underperforms humans on BART risk-taking.",
            "experimental_details": "Deterministic prompting (temperature 0), in-context only; CoT and SB prompt-engineering applied in subset tests — GPT-4 included in those and showed improvements: CoT increased probabilistic posterior accuracy by +9.01% (average across five models) and model-basedness by +64.59%; SB increased posterior accuracy by +3.10% and model-basedness by +118.59% (aggregated reported percentages are weighted averages across five models).",
            "limitations_or_caveats": "Human-normalized reporting prevents direct numeric comparison to raw human scores in paper; GPT-4 comparisons could be affected by proprietary hidden pre-prompts and limited transparency about training data; number of simulated trials differs from human experiments which may bias measures like meta-cognition and learning dynamics.",
            "uuid": "e9024.1",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude-1",
            "name_full": "Anthropic Claude-1",
            "brief_description": "A proprietary LLM (Anthropic) included among the top performers in CogBench — reported to achieve human-level scores on most tasks and to exhibit relatively human-like behavior after RL-based alignment (RLAIF/RLHF).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-1",
            "model_description": "Anthropic model listed in the paper with parameter count '100' (table). Classified as using RLAIF (reinforcement learning from AI feedback) in the authors' metadata.",
            "model_size": "100",
            "test_battery_name": "CogBench (seven tasks)",
            "test_description": "Same battery: probabilistic reasoning, horizon, restless bandit, instrumental learning, two-step, temporal discounting, BART.",
            "llm_performance": "Reported as among the best performers: 'human-level scores in most tasks (five out of six)'; strong on probabilistic reasoning and instrumental learning; one of the few models to handle restless bandit and two-step well; BART remains challenging.",
            "human_baseline_performance": "Evaluations reported on a human-normalized scale (1 = average human); Claude-1 reached ~human-level across multiple performance metrics (exact numeric normalized values not shown in main text).",
            "performance_comparison": "Matches average human performance across multiple tasks; shows improved human-likeness consistent with RLHF/RLAIF effects described in paper.",
            "experimental_details": "In-context evaluation only, temperature 0. Claude-1 included in prompt-engineering tests (CoT and SB) subset where CoT and SB both improved probabilistic reasoning and model-basedness on average (aggregate improvements reported across five models).",
            "limitations_or_caveats": "Claude-1 is proprietary and not fully transparent about training/data; RLAIF vs RLHF distinctions noted; direct numeric human comparisons not provided; as with other models, differences in trial counts and response formats vs human data.",
            "uuid": "e9024.2",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "OpenAI text-davinci-003 (GPT-3 family)",
            "brief_description": "A widely used GPT-3-series model (text-davinci-003) evaluated on CogBench; shows competence on several tasks (probabilistic reasoning, instrumental learning) and mixed behavior on risk and temporal discounting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "OpenAI GPT-3-era model; the authors' table lists parameter count '170' and marks it as RLHF-enabled in their metadata.",
            "model_size": "170",
            "test_battery_name": "CogBench (seven tasks)",
            "test_description": "Evaluated across all seven cognitive tasks.",
            "llm_performance": "Described as competent in probabilistic reasoning and instrumental learning; in temporal discounting appears more far-sighted for some variants; BART shows extreme behaviors for different models (some always risk, some never risk) — text-davinci-003 among those more far-sighted in temporal discounting per paper narrative.",
            "human_baseline_performance": "Compared to human-normalized metric (1 = average human); specific per-task normalized values not provided in main text.",
            "performance_comparison": "Generally performs at or above chance and often in the mid-range of models; performance varies by task — sometimes near-human on learning tasks but inconsistent in risk and exploration behaviors.",
            "experimental_details": "Deterministic prompts (temperature 0), in-context only; included in aggregated analyses and figures (performance and behavioral phenotypes).",
            "limitations_or_caveats": "Paper notes optimism bias across most LLMs (stronger learning from positive feedback) and that code fine-tuning did not systematically improve behavior; text-davinci-003 specifics subject to these general caveats.",
            "uuid": "e9024.3",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "text-bison@002",
            "name_full": "PaLM-2 for text (text-bison@002)",
            "brief_description": "Google's PaLM-2 text model (labeled text-bison@002) evaluated in CogBench; notable as one of the few models that did not show super-human performance on the horizon task and displayed different bias patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-bison@002 (PaLM-2)",
            "model_description": "Google PaLM-2 variant for text; authors' feature table lists '340' under parameter column and marks RLHF usage in metadata.",
            "model_size": "340",
            "test_battery_name": "CogBench (seven tasks)",
            "test_description": "Assessed on same battery; particular results called out for horizon, temporal discounting, and optimism-bias patterns.",
            "llm_performance": "Paper reports that most models outperform humans on the horizon task except text-bison; text-bison also is an exception regarding optimism bias (it did not show very strong optimism bias unlike most other models) and tended to be more myopic on temporal discounting relative to some other models.",
            "human_baseline_performance": "Benchmarked against human-normalized measures (1 = average human); exact numeric human baselines not listed in main text.",
            "performance_comparison": "Per paper, text-bison underperforms relative to other models on some exploration measures (horizon) and differs in behavioral biases (less optimism bias), placing it below the group-average super-human horizon performance.",
            "experimental_details": "In-context only, temperature 0; included in prompt-engineering subset for CoT/SB experiments (the five-model subset included text-bison) where aggregated improvements were reported.",
            "limitations_or_caveats": "Authors caution that cross-model comparisons can be affected by hidden proprietary pre-prompts, dataset transparency, and context-length limits; text-bison behavior exemplifies heterogeneity across models.",
            "uuid": "e9024.4",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-70",
            "name_full": "LLaMA-2 (70B)",
            "brief_description": "A 70B-parameter open-source foundation model (LLaMA-2-70) included in CogBench; shows differing behavior from its fine-tuned 'chat' variant on risk, discounting and exploration despite similar task performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70",
            "model_description": "Open-source LLaMA-2 family model with ~70B parameters as listed in paper; not fine-tuned with conversational RLHF in this base variant.",
            "model_size": "70",
            "test_battery_name": "CogBench (seven tasks)",
            "test_description": "Same battery of cognitive tasks; comparisons with LLaMA-2-70-chat highlight effects of fine-tuning/conversational adaptation.",
            "llm_performance": "Performance on aggregate tasks similar to its -chat variant, with competence in probabilistic reasoning and instrumental learning; however, placed differently on behavioral axes: LLaMA-2-70 was reported as myopic on temporal discounting for some models and showed extreme risk-taking contrasts with its chat variant.",
            "human_baseline_performance": "Evaluated on human-normalized scale; paper emphasizes behavioral divergence relative to humans rather than raw numeric human baselines.",
            "performance_comparison": "Performance scores (aggregate) close to LLaMA-2-70-chat, but behavioral metrics (risk-taking, temporal discounting, random exploration) diverge—illustrating that similar performance can mask very different strategies compared to humans.",
            "experimental_details": "In-context evaluation, temperature 0; number of simulations task-dependent (see CogBench defaults); comparison explicitly made between base and -chat (fine-tuned) versions to probe effects of conversational fine-tuning / RLHF.",
            "limitations_or_caveats": "Authors stress that fine-tuned variants (e.g., -chat) can behave quite differently and that behavioral phenotyping is necessary to reveal such divergences; openness about training and pre-prompts varies across models, complicating attribution of causes for behavioral differences.",
            "uuid": "e9024.5",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-70-chat",
            "name_full": "LLaMA-2-70-chat (fine-tuned chat variant)",
            "brief_description": "A chat-specialized fine-tuned variant of LLaMA-2-70 (adds RLHF/conversational fine-tuning) that shows markedly different behavioral profiles (risk, discounting, exploration) than its base model despite similar aggregate performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70-chat",
            "model_description": "Fine-tuned conversational version of LLaMA-2-70; same architectural base but additional fine-tuning for chat and RLHF-like alignment present in metadata.",
            "model_size": "70",
            "test_battery_name": "CogBench (seven tasks)",
            "test_description": "Evaluated across same battery; used particularly to contrast effects of fine-tuning and RLHF on behavioral metrics.",
            "llm_performance": "Aggregate task performance similar to base LLaMA-2-70, competent on probabilistic reasoning and instrumental learning; behaviorally diverged in risk-taking and exploration—e.g., LLaMA-2-70-chat sometimes exhibited opposite risk-taking behavior to base model.",
            "human_baseline_performance": "Compared to human-normalized metrics (1 = average human); specific numeric human baselines not listed in main text.",
            "performance_comparison": "Performance parity with base model but behavioral differences indicate fine-tuning/RLHF can shift strategies away from or toward human-like behavior depending on metric.",
            "experimental_details": "In-context evaluation only (no further fine-tuning performed by authors); chat variant included to analyze hierarchical grouping in multilevel regressions (to account for model families).",
            "limitations_or_caveats": "Paper highlights that fine-tuning/hidden pre-prompts can substantially change behavior; absence of full transparency on fine-tuning details makes causal attribution tentative.",
            "uuid": "e9024.6",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Falcon-40B",
            "name_full": "Falcon-40B",
            "brief_description": "An open-source 40B-parameter LLM evaluated in CogBench; included among open-source family where regression analysis showed open-source models take fewer risks compared to proprietary models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-40B (and instruct variant)",
            "model_description": "Open large language model (40B parameters in authors' table); both base and instruct variants evaluated; marked open-source in metadata.",
            "model_size": "40",
            "test_battery_name": "CogBench (seven tasks)",
            "test_description": "Assessed across the CogBench tasks; included in analyses comparing open-source vs proprietary models on behavioral metrics like risk-taking.",
            "llm_performance": "Not singled out as top-performing; part of open-source group that, holding other features equal, exhibited less risky decisions compared to proprietary models (paper reports negative effect of 'open-source' on risk-taking in regression).",
            "human_baseline_performance": "Benchmarked on human-normalized scale; raw human values not provided.",
            "performance_comparison": "Performance generally mid-range among models; notably, open-source models (including Falcon) were found to take fewer risks than proprietary counterparts in multilevel regression (β = −0.612 ± 0.11 for 'open-source' effect on risk; p &lt; 0.001).",
            "experimental_details": "In-context evaluation, temperature 0; included base and instruct variants; used default task simulation counts (see CogBench).",
            "limitations_or_caveats": "Regression controls for model family but transparency about training data and hidden prompting differs across models; the open-source vs proprietary risk result is correlational and may reflect multiple engineering differences beyond 'open-source' label.",
            "uuid": "e9024.7",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MPT-30B",
            "name_full": "MPT-30B",
            "brief_description": "A 30B-parameter open-source foundation model (MosaicML MPT-30B) evaluated in CogBench, included with instruct and chat variants; used to probe behavior across open-source family members.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MPT-30B (and instruct/chat variants)",
            "model_description": "Open-source model family (30B parameters in paper table) with base, instruct, and chat variants; some variants marked as conversational in metadata.",
            "model_size": "30",
            "test_battery_name": "CogBench (seven tasks)",
            "test_description": "Same suite of cognitive tasks as other models; included in analyses contrasting open-source and proprietary models and examining effects of conversational fine-tuning.",
            "llm_performance": "Part of the broad set of models where performance varied by task; no standout top-level performance reported in main text, but included in aggregate figures (Figure 2) and appendices (full results for 35 models).",
            "human_baseline_performance": "Compared using human-normalized metrics; specific numeric human baselines not enumerated in main text.",
            "performance_comparison": "Performance ranged across tasks; contributes to group-level findings such as RLHF improving human-likeness and open-source models being less risk-prone in regression analyses.",
            "experimental_details": "Deterministic in-context evaluation, temperature 0; some MPT variants are conversational (chat/instruct) and these family relationships were modeled with multilevel regressions to account for nested fine-tuned variants.",
            "limitations_or_caveats": "Paper highlights limited transparency for some models and sparse training metadata for others; results for MPT variants are presented in aggregate and detailed per-model results are in Appendix C (not fully reproduced in main text).",
            "uuid": "e9024.8",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand gpt-3.",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning.",
            "rating": 2,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        },
        {
            "paper_title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt.",
            "rating": 2,
            "sanitized_title": "humanlike_intuitive_behavior_and_reasoning_biases_emerged_in_large_language_models_but_disappeared_in_chatgpt"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks.",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Inducing anxiety in large language models increases exploration and bias.",
            "rating": 2,
            "sanitized_title": "inducing_anxiety_in_large_language_models_increases_exploration_and_bias"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01850125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CogBench: a large language model walks into a psychology lab
28 Feb 2024</p>
<p>Julian Coda-Forno 
Computational Principles of Intelligence Lab
Max Planck Institute for Biological Cybernetics
TübingenGermany</p>
<p>Institute for Human-Centered AI
Helmholtz Compu-tational Health Center
MunichGermany</p>
<p>Marcel Binz 
Computational Principles of Intelligence Lab
Max Planck Institute for Biological Cybernetics
TübingenGermany</p>
<p>Institute for Human-Centered AI
Helmholtz Compu-tational Health Center
MunichGermany</p>
<p>Jane X Wang 
Google DeepMind
LondonUK</p>
<p>Eric Schulz 
Computational Principles of Intelligence Lab
Max Planck Institute for Biological Cybernetics
TübingenGermany</p>
<p>Institute for Human-Centered AI
Helmholtz Compu-tational Health Center
MunichGermany</p>
<p>Coda- </p>
<p>Equal contribution</p>
<p>CogBench: a large language model walks into a psychology lab
28 Feb 2024DAD2E6164DECDD607E9E4633D62C7057arXiv:2402.18225v1[cs.CL]
Large language models (LLMs) have significantly advanced the field of artificial intelligence.Yet, evaluating them comprehensively remains challenging.We argue that this is partly due to the predominant focus on performance metrics in most benchmarks.This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments.This novel approach offers a toolkit for phenotyping LLMs' behavior.We apply CogBench to 35 LLMs, yielding a rich and diverse dataset.We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs.Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior.Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior.Finally, we explore the effects of prompt-engineering techniques.We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have emerged as a groundbreaking technology, captivating the attention of the scientific community (Bommasani et al., 2021;Binz et al., 2023).Modern LLMs have scaled to remarkable dimensions in both architecture and datasets (Kaplan et al., 2020), revealing a spectrum of capabilities that were previously unimagined (Wei et al., 2022;Brown et al., 2020).Yet, these models also present a significant challenge: their internal workings are largely opaque, making it difficult to fully comprehend their behavior (Tamkin et al., 2021).This lack of understanding fuels ongoing debates about their capabilities and limitations (McCoy et al., 2023;Bubeck et al., 2023).</p>
<p>A notable issue in these discussions is the focus of many benchmarks on performance metrics alone (Burnell et al., 2023).This approach often overlooks the underlying behavioral mechanisms of the models, reducing benchmarks to mere training targets rather than tools for genuine insight, and thus failing to provide a comprehensive measure of the models' abilities (Schaeffer et al., 2023).How can we overcome this problem and make progress toward a better understanding of LLMs' behaviors?</p>
<p>The field of cognitive psychology may offer solutions to these problems.Experiments from cognitive psychology have been used to study human behavior for many decades, and have therefore been extensively validated.Furthermore, they typically focus more on behavioral insights rather than performance metrics alone.Finally, many of these experiments are programmatically generated, minimizing data leakage concerns.Many of these concepts are important to ensure a robust evaluation of an agent's capabilities.However, while there have been studies investigating LLMs on individual tasks from cognitive psychology (Binz &amp; Schulz, 2023;Dasgupta et al., 2022;Hagendorff et al., 2023;Ullman, 2023), no study has evaluated them holistically.</p>
<p>In this paper, we propose CogBench, a novel benchmark consisting of ten behavioral metrics spanning seven cognitive psychology experiments, to fill this gap.We investigate the behaviors of 35 LLMs in total, using our benchmark to not only compare the performance of these models but also apply techniques from computational cognitive modeling to understand the inner workings of their behaviors.</p>
<p>Our results recover the unequivocal importance of size: larger models generally perform better and are more modelbased than smaller models.Our results also show the importance of reinforcement learning from human feedback (RLHF; Christiano et al., 2017) in aligning LLMs with humans: RLHF'ed LLMs behave generally more humanlike and are more accurate in estimating uncertainty.Yet our results also revealed surprising behaviors.First, while open-source models are often believed to be more risky due to the lack of pre-prompts, we find that, holding all else equal, they make less risky decisions than proprietary models.Secondly, while fine-tuning on code is often believed to improve LLMs' behaviors, we find little evidence for this in our benchmarking suite.</p>
<p>Finally, we investigate how chain-of-thought (CoT) (Wei et al., 2023;Kojima et al., 2022) and take-a-step-back (SB) (Zheng et al., 2023a) prompting techniques can influence different behavioral characteristics.Our analysis suggests that CoT is particularly effective at enhancing probabilistic reasoning, while SB proves to be more relevant for promoting model-based behaviors.This showcases insights that can be gained by CogBench also for understanding the effectiveness of these prompt-engineering techniques as well as guiding users in selecting the most suitable promptengineering technique based on the specific context.</p>
<p>Taken together, our experiments show how psychology can offer detailed insights into artificial agents' behavior as we provide an openly accessible1 and challenging benchmark to evaluate LLMs.</p>
<p>Related work</p>
<p>Benchmarking LLMs: As LLMs rapidly evolve, it is critical to assess their capabilities.Numerous benchmarks have emerged to tackle this challenge, evaluating capabilities such as grade school mathematics (Cobbe et al., 2021), general knowledge (Joshi et al., 2017), programming (Chen et al., 2021), reasoning (Collins et al., 2022), among others (Hendrycks et al., 2021).In addition, the Chatbot Arena (Zheng et al., 2023b) provides a platform for comparing AI chatbots, and the Beyond the Imitation Game Benchmark (BIG-bench; Srivastava &amp; authors, 2023) offers a comprehensive evaluation of LLMs across over 200 tasks.</p>
<p>Psychology for LLMs: Our benchmark is part of a new wave of research that uses cognitive psychology to study LLMs (Binz &amp; Schulz, 2023;Dasgupta et al., 2022;Coda-Forno et al., 2023;Ullman, 2023;Hagendorff et al., 2023;Akata et al., 2023;Yax et al., 2023;Chen et al., 2023;Buschoff et al., 2024).The power of this approach lies in its incorporation of tools from cognitive psychology that have been developed and refined over many decades.Instead of focusing solely on how well LLMs perform, this area of research prioritizes describing and characterizing their behavior in terms of underlying mechanisms.This shift in focus helps us understand LLMs in a more meaningful way.</p>
<p>It is important to note that while these works have signifi-cantly contributed to our understanding of LLMs, they have mainly targeted specific behaviors in isolation and did not establish a benchmark providing a standardized evaluation of different models and across a diverse, comprehensive set of tasks and skills.</p>
<p>Methods</p>
<p>CogBench is a benchmark rooted in cognitive psychology for evaluating the behaviors of language models.It incorporates ten metrics derived from seven canonical experiments in the literature on learning and decision-making.These metrics offer a robust measure of wide-ranging behaviors and allow for comparisons with human behavior.In this section, we provide an overview of the models included in our study, followed by brief descriptions of the used cognitive experiments and their respective metrics.Figure 1 displays a visual representation that complements the discussion in this section.</p>
<p>Prompting and summary of included models</p>
<p>We evaluated over 35 different LLMs using our benchmark.This selection includes proprietary models such as Anthropic's Claude-1 and Claude-2 (Anthropic, 2023), Open-AI's GPT-3 (text-davinci-003) and GPT-4 (OpenAI, 2023), and Google's PaLM-2 for text (text-bison@002) (Google, 2023).We also tested open-source models like Mosaic's MPT (MosaicML, 2023), Falcon (Almazrouei et al., 2023), and numerous LLaMA-2 variants (Touvron et al., 2023).</p>
<p>For a full list of the models used, we refer the reader to Appendix A.</p>
<p>It is important to note that all experiments performed in this paper rely entirely on the LLMs' in-context learning abilities and do not involve any form of fine-tuning.We set the temperature parameter to zero, leading to deterministic responses, and retained the default values for all other parameters.</p>
<p>High-level summary of tasks</p>
<p>In the following, we provide a high-level summary of the tasks included in CogBench, alongside their ten behavioral metrics.It is important to highlight that a performance metric can also be obtained for each task.For full descriptions of all tasks and their corresponding metrics, we refer the reader to Appendix B. CogBench consists of the following tasks:</p>
<ol>
<li>
<p>Probabilistic reasoning (Dasgupta et al., 2020): a task that tests how agents update beliefs based on new evidence.They are given a "wheel of fortune" (representing initial prior probabilities) and two urns with different colored ball distributions (representing likelihoods).Upon drawing a ball, agents can revise their belief about the chosen urn, considering both the wheel (prior) and the ball color (evidence).This tests adaptability to different prior/likelihood scenarios by changing the wheel division and ball distributions.Agents have to estimate the probability of the drawn ball's urn.The behavioral choices can be used to estimate an agent's prior and likelihood weightings.Experimentally, people often exhibit a behavior known as system neglect, meaning that they underweight both priors and likelihoods (Massey &amp; Wu, 2005).</p>
</li>
<li>
<p>Horizon task (Wilson et al., 2014): a two-armed bandit task with stationary reward distributions.Agents first observe four reward values of randomly determined options, followed by making either one or six additional choices.We use this task to measure whether an agent uses uncertainty to guide its exploration behavior (directed exploration) and/or whether it injects noise into its policy to explore (random exploration).People are known to rely on a combination of both strategies (Wilson et al., 2014;Brändle et al., 2021).</p>
</li>
<li>
<p>Restless bandit task (Ershadmanesh et al., 2023): a two-armed bandit task with non-stationary reward distributions.There is always one option with a higher average reward.Every few trials a switch between the reward distributions of the two options occurs.Agents furthermore have to indicate after each choice how confident they are in their decisions.We use this task to measure meta-cognition, which indicates whether an agent can assess the quality of its own cognitive abilities.People generally display this ability but its extent is influenced by various internal and external factors (Shekhar &amp; Rahnev, 2021).</p>
</li>
<li>
<p>Instrumental learning (Lefebvre et al., 2017): Agents encounter four two-armed bandit problems in an interleaved order.Each bandit problem is identified by a unique symbol pair.We use this task to investigate how an agent learns.First, we report the learning rate of the agent which is common practice in two-armed bandits.Furthermore, we use it to reveal whether an agent learns more from positive than from negative prediction errors, i.e., whether it has an optimism bias.</p>
</li>
</ol>
<p>People commonly display asymmetric tendencies when updating their beliefs by showing higher learning rates after encountering positive prediction errors compared to negative ones (Palminteri &amp; Lebreton, 2022).</p>
<ol>
<li>
<p>Two-step task (Daw et al., 2011): a reinforcement learning task in which agents have to accumulate as many treasures as possible.Taking an action from a starting state transfers the agent to one out of two second-stage states.In each of these second-stage states, the agent has the choice between two options that probabilistically lead to treasures.Finally, the agent is transferred back to the initial state and the process repeats for a predefined number of rounds.The task experimentally disentangles model-based from model-free reinforcement learning.We therefore use it to measure an agent's model-basedness.Previous studies using this task have shown that people rely on a combination of model-free and model-based reinforcement learning (Daw et al., 2011).</p>
</li>
<li>
<p>Temporal discounting (Ruggeri et al., 2022): Agents have to make a series of choices between two options.Each option is characterized by a monetary outcome and an associated delay until the outcome is received.</p>
</li>
</ol>
<p>We use this task to assess temporal discounting, indicating whether an agent prefers smaller but immediate gains over larger delayed ones.People generally show a preference for immediate gains, although the precise functional form of their discounting is a matter of debate (Cavagnaro et al., 2016).</p>
<ol>
<li>Balloon Analog Risk Task (BART) (Lejuez et al., 2002): Agents have to inflate an imaginary balloon to obtain rewards.They may choose to stop inflating and cashing out all rewards accumulated so far.There is a chance that the balloon pops at any point in time and all rewards will be lost.We use this task to assess risk-taking behavior.Human risk-taking in this task is "significantly correlated with scores on selfreport measures of risk-related constructs and with the self-reported occurrence of real-world risk behaviors" (Lejuez et al., 2002).</li>
</ol>
<p>The cognitive phenotype of LLMs</p>
<p>This section provides the reader with a high-level overview of our benchmark's metrics.From our suite of 7 tasks, we can derive two classes of metrics: 1) performance metrics that represent the score participants aim to optimize, and 2) behavioral metrics measuring how participants complete the task (tasks are typically designed in a way that allows one to disentangle between different types of behavior).Figure 2 visualizes phenotypes for both classes of metrics for seven well-established LLMs. 2 We report the results of all 35 LLMs in Appendix C. The observed differences underscore the practical value and importance of CogBench for evaluating LLMs, offering a more comprehensive assessment than standard performance-based benchmarks alone.</p>
<p>Performance summary</p>
<p>As presented in Figure 2A, in terms of performance, GPT-4 and Claude-1 distinguish themselves, achieving humanlevel scores in most tasks (five out of six). 3 In general, all models demonstrate competence in at least half of the tasks (three out of six).Each of the seven models excels in probabilistic reasoning and instrumental learning.The horizon task sees most models outperforming humans except for text-bison.The restless bandit task poses a challenge for the majority of models, with GPT-4 and Claude-1 being notable exceptions.Finally, the BART proves to be a hurdle for all models.</p>
<p>Differences between behavioral and performance metrics</p>
<p>Figure 2B shows that none of the models exhibit human-like behavior on the majority of behavioral metrics, revealing a complex structure that warrants further exploration.</p>
<p>High performance indicates high meta-cognition and model-basedness: Models that demonstrate satisfactory performance on the restless bandit task exhibit a certain degree of meta-cognition, although not to the same extent as humans.Proprietary models that are capable of solving the two-step task display model-based behavior at least on par with humans, with GPT-4 significantly surpassing human levels.Thus, within the scope of these two tasks, it seems that a model's performance can serve as an indicator of its corresponding behavioral metrics.In this context, meta-cognition and model-basedness appear to emerge as properties of high-performing models.</p>
<p>High performance despite lack of exploration: Interestingly, almost all models (except for text-bison) demonstrate super-human performance on the horizon task.While they exhibit high performance, they still lack exploration (except for LLaMA2-70-chat which exhibits higher-than-human random exploration).This underscores the importance of behavioral metrics in understanding the strategies employed by LLMs.In this case, it appears that LLMs achieve high performance primarily through exploitation without any human-like exploration.</p>
<p>Stronger priors than likelihoods: All models place much more weight on priors than observations, suggesting strong biases that are difficult to alter.Additionally, we can observe a prevalence of optimism bias and high learning rates.Almost all models exhibit a very strong optimism bias (except for text-bison), aligning with the notion that these LLMs harbor strong biases.Low performance but high behavioral variance for risktaking and temporal discounting: Temporal discounting and risk-taking behaviors exhibit high variance among models.While some models, such as text-bison and LLaMA-2-70, appear myopic on the temporal discounting task, others, including text-davinci-003, Claude, and LLaMA-2-70-chat, demonstrate a much more far-sighted approach.GPT-4, interestingly, exhibits behavior akin to humans.For the BART, models are positioned at extreme ends of the risktaking spectrum, i.e., they either never take any risks at all or always risk everything.LLaMA-2-70 and LLaMA-2-70-chat, for example, display the same performance in this task but exhibit opposite risk-taking behavior.This not only indicates a struggle for LLMs to apprehend risks but also underscores the importance of our benchmark.Indeed, it raises questions about what influences a model's behavior.It also highlights how recording only their performance would have overlooked the contrasting risk-taking behavior of the two LLaMA models.</p>
<p>The comparison between LLaMA-2-70 and LLaMA-2-70chat is particularly compelling.Even though LLaMA-2-70-chat is a fine-tuned version of LLaMA-2-70, they exhibit markedly different behavior in risk-taking, temporal discounting, and random exploration.This divergence is intriguing, especially considering their performance on all tasks is relatively similar.This observation sets the stage for the subsequent section, where we will conduct a more comprehensive analysis of how specific features of these models influence their performance and behaviors.</p>
<p>Hypothesis-driven experiments</p>
<p>CogBench provides researchers with the means to explore a broad spectrum of LLMs' behaviors.We have applied CogBench to 35 distinct LLMs.This diversity allows us to test how different aspects of LLMs, such as the number of parameters, the application of Reinforcement Learning from Human Feedback (RLHF), fine-tuning for code, and many more, can impact specific LLMs' performance and behaviors.</p>
<p>The metrics provided by CogBench enable us to perform various analyses to test specific hypotheses of interest.In this section, we formulate and test five hypotheses about different mechanisms in LLMs and how these can affect their behavioral profiles.We use both qualitative, visualizationbased techniques (dimensionality reduction) as well as quantitative analyses (multi-level regression) to test our hypotheses.For all regression analyses, we use the features of LLMs to predict specific behavioral metrics from the benchmark.The multi-level regression approach was chosen because some models are fine-tuned versions of other models.For instance, certain LlaMA models have a -chat version which adds RLHF and conversational fine-tuning, and thus are in the same higher-level group.This approach allows us to account for the hierarchical structure in our data and provides a more nuanced understanding of the behaviors of LLMs.We can isolate the effects of specific features or modifications by comparing models within the same higher-level group.</p>
<p>Hypothesis 1: Does RLHF make LLMs more humanlike?</p>
<p>To evaluate this hypothesis, we used UMAP (McInnes et al., 2020) on the ten behavioral metrics of all LLMs, as illustrated in Figure 3A.Clear separation is evident between LLMs that incorporate RLHF and those that do not.LLMs with RLHF demonstrate behaviors that appear, on average, roughly 2× more similar to human behavior compared to the models without.However, it is important to note that while UMAP space retains some global structure, it is primarily used for visualization purposes.Consequently, we also analyzed the average distances before dimensionality reduction (using normalized feature vectors), observing a 11.7% average decrease in L2-Norm distance for models with RLHF (Figure 3B).Conclusion: Hypothesis is supported.</p>
<p>Hypothesis 2: Does performance increase with the number of parameters, training data, and the inclusion of code?</p>
<p>To answer this question, we used the multi-level regression previously mentioned, focusing on the performance of LLMs.We performed a regression analysis with the average standardized performance scores across all seven tasks as the dependent variable, using LLMs' features as predictors.</p>
<p>We found that the number of parameters indeed had a significant influence on performance (β = 0.277 ± 0.39, z = 14.1, p &lt; 0.001; see Figure 4A).However, the size of the training dataset and the use of code training data did not have a substantial impact.One possible explanation for this could be that the quality of the training data, rather than its sheer volume, plays a more determining role in performance, as well as that larger models also tend to be trained on larger datasets.Conclusion: Hypothesis is partially supported.</p>
<p>Hypothesis 3: Does an increase of parameters, training data, and the inclusion of code increase modelbasedness?</p>
<p>We again used the multi-level regression technique from before, this time focusing on a specific behavioral metric: model-basedness.We found that the number of parameters had a significant positive effect (β = 0.481 ± 0.22, z = 4.2, p &lt; 0.001; see Figure 4B), while the size of the training dataset and the use of code training data did not appear to significantly influence model-basedness.This again suggests that the quality of the data might be more crucial than its quantity when it comes to determining both performance and the emergence of model-based behaviors here.However, identifying which factors constitute 'quality' in the data requires a deeper exploration.This highlights the issue of transparency about data.For a thorough evaluation of how specific data features impact the emergence of behavioral functionalities such as model-basedness, it is essential to be transparent about a model's data and methodologies.Conclusion: Hypothesis is partially supported.</p>
<p>Hypothesis 4: Does RLHF enhance meta-cognition?</p>
<p>To answer this question, we focus our multi-level regression on meta-cognition.Our analysis revealed a strong effect (β = 0.461 ± 0.15, z = 5.9, p &lt; 0.001; see Figure 4C), indicating that RLHF significantly increased meta-cognition in LLMs.This finding underscores the potential of RLHF in enhancing the cognitive capabilities of LLMs.Conclusion: Hypothesis is supported.</p>
<p>Hypothesis 5: Do open-source models take more risks?</p>
<p>The open-source feature could be seen as a proxy for the engineering efforts that proprietary models undergo.There is a growing body of research suggesting that hidden preprompts being one of them, can significantly influence the behavior of LLMs (Liu et al., 2023).They can act as a  form of 'priming' that guides the model's responses, potentially making the model more cautious and less likely to take risks by constraining the model towards safer behaviors.However, our regression analysis suggested otherwise: contrary to expectations, we observed a negative effect (β = −0.612± 0.11, z = −11.4,p &lt; 0.001; see Figure 4D), indicating that proprietary models, which often have hidden pre-prompts, are more likely to take risks.This surprising outcome could be influenced by various factors from different engineering techniques.However, this underscores the limited behavioral evaluation of these techniques.In the subsequent section, we aim to bridge this gap in understanding through an initial exploration into the change of behavior of two standard prompt-engineering techniques.Conclusion: Hypothesis is refuted.</p>
<p>Impact of prompt-engineering</p>
<p>We also explored the impact of prompt-engineering techniques, namely chain-of-thought (CoT) and take-a-stepback (SB) prompting, on the behavior of LLMs.Both techniques are incorporated at the end of a question: Take-a-step-back:</p>
<p>First take a step back and think in the following two steps to answer this:</p>
<p>Step 1) Abstract the key concepts and principles relevant to this question.</p>
<p>Step 2) Use the abstractions to reason through.</p>
<p>Chain-of-thought:</p>
<p>First break down the problem into smaller steps and reason through each step logically.</p>
<p>Their purpose is to stimulate the generation of reasoning steps.These steps serve as an additional context that the LLM can use to elicit better final responses.While these techniques have been shown to enhance performance, it is essential to confirm whether they indeed improve the behaviors they are designed to augment.</p>
<p>We focused on examining two specific behaviors that are hypothesized to improve with the inclusion of reasoning steps.These behaviors are the models' performance in the probabilistic reasoning task and their model-basedness.</p>
<p>We evaluated five specific LLMs: GPT-4, PaLM-2 for text (text-bison@002), Claude-1/2, and LLaMA-2, applying CoT and SB techniques and comparing the outcomes with their base models.The selection of these five models and a limited set of metrics was necessitated by the additional en- gineering effort required to process the outputs when using these techniques.The choice of LLMs aimed at ensuring a diverse representation of established models, considering the complexity of our benchmark tasks and the potential for erratic outputs from smaller LLMs when given the freedom to reason.For a comprehensive explanation of the querying process for these models, please refer to Appendix D.</p>
<p>Our investigation initially focused on probabilistic reasoning, which is a fundamental cognitive ability in decisionmaking.This ability facilitates the optimal integration of new information with pre-existing knowledge.We used the performance metric from the probabilistic reasoning experiment, namely posterior accuracy, which is calculated as one minus the deviation from the Bayes optimal prediction for each task.As depicted in Figure 6A, both CoT and SB techniques generally enhanced probabilistic reasoning compared to their base models, with CoT showing an average increase of 9.01% and SB showing an increase of 3.10%.</p>
<p>Furthermore, we discovered that model-basedness, a critical aspect of reasoning and planning, is significantly augmented by both CoT and SB techniques, as shown in Figure 6B.Specifically, CoT demonstrated a 64.59% increase, while SB showed a substantial increase of 118.59%.</p>
<p>Discussion</p>
<p>We have presented CogBench, a new open-source benchmark for evaluating LLMs.CogBench is rooted in wellestablished experimental paradigms from the cognitive psychology literature, providing a unique set of advantages over traditional LLM benchmarks.First, it is based on tried-andtested experiments whose measures have been extensively validated over many years and shown to capture general cognitive constructs.In addition, unlike standard benchmarks, CogBench does not only focus on performance metrics alone but also comes with behavioral metrics that allow us to gain insights into how a given task is solved.Finally, many of the included problems are procedurally-generated, thereby making it hard to game our benchmark by training on the test set.All our code and analysis will be publicly available, making it easy to use CogBench for the LLM community.</p>
<p>Our analyses yielded several key findings: as expected, RLHF enhanced the human-likeness of LLMs, while the number of parameters improved their performance and model-basedness.However, we also found surprising results.Despite expectations, code fine-tuning did not influence performance or model-basedness and open-source models exhibited less risk-taking behavior.Further, we found CoT prompting to be a promising choice for enhancing probabilistic reasoning.Conversely, SB prompting proved more effective for model-based reasoning.</p>
<p>While these results demonstrate the versatility of our benchmark, our analysis also faces several challenges.For instance, the limited transparency of certain proprietary models poses an issue to our regression analysis because acquiring details about certain models can be difficult or impossible.This lack of transparency could potentially affect the precision of our analysis.It also underscores the need for more transparency to facilitate more thorough and accurate evaluations (LAION, 2024;Binz et al., 2023).</p>
<p>Taken together, our study highlights the importance of behavioral metrics and cognitive modeling in evaluating LLMs and presents a novel benchmark for this purpose.The analysis was preliminary and intended to provide a broad view of how CogBench can be used.The primary aim of this work is to equip the LLM community with new tools, inspired by cognitive science, to evaluate their models more comprehensively.Future work should focus on three areas.First, while cognitive science studies have demonstrated the external validity of the investigated tasks, it is yet to be shown for LLMs.Furthermore, we aim to extend the set of included tasks to cover a broader set of domains.Finally, we plan to properly automate our benchmark, mostly for prompt engineering techniques that were only briefly examined in this study.This could include studying the influence of impersonation (Salewski et al., 2023) • Size of Dataset: This represents the size of the dataset on which the model was trained, expressed in trillions of tokens.</p>
<p>• Context Length: This refers to the length of the context available to the model during its operation.</p>
<p>• Conversational: This indicates whether the model was fine-tuned with conversational datasets.</p>
<p>• Code: This indicates whether the model was fine-tuned with code datasets.</p>
<p>Please note that the selection of features used for our analyses was made based on the best available knowledge of the authors, as some information about certain models can be challenging to obtain.This limitation could potentially impact the precision of the regression analysis.It underscores the need for greater transparency about LLMs to facilitate more thorough evaluations.Upon drawing a ball, participants can revise their belief about the chosen urn, considering both the wheel (prior) and the ball color (evidence).The task allows testing adaptability to different prior/likelihood scenarios by changing the wheel division and ball distributions.Agents have to estimate the probability of the drawn ball's urn.We use this task to estimate an agent's prior and likelihood weightings.In this task, people showed similar weighting between prior and likelihood, both under one.This underweighting is often referred to as system neglect (Massey &amp; Wu, 2005).</p>
<p>B. Comprehensive list &amp; explanation of the cognitive experiments</p>
<p>B.1.2. METHODS</p>
<p>We matched the probabilities used in (Dasgupta et al., 2020) to compare to human data.There they had either an informative likelihood case (P (left urn|red) = 0.7, 0.8 or 0.9) and an informative prior (P (left urn) = 0.5 or 0.6) or vice versa.They also trained humans on this experiment, so we only compared it to data from a human's first trial as we are not interested in learning but in how an LLM weighs its prior and likelihoods by default.The default number of simulations here was 100.</p>
<p>B.1.3. PROMPTS FOR LLMS</p>
<p>Example with informative likelihood</p>
<p>You are participating in an experiment where you are provided with a wheel of fortune and two urns.The wheel of fortune contains 10 evenly sized sections labeled either F or J, corresponding to the urns F and J. Another person will spin the wheel of fortune, select an urn based on the outcome of the spin, and then randomly pick a ball from the selected urn.Your goal is to give your best estimate of the probability of the urn being F after observing the ball drawn from the urn.</p>
<p>Q: The wheel of fortune contains 6 sections labeled F and 4 sections labeled J.The urn F contains (8, 2) and the urn J contains (2, 8) red/blue balls.A red ball was drawn.What is the probability that it was drawn from Urn F? (Give your probability estimate on the scale from 0 to 1 rounded to two decimal places).</p>
<p>A: I estimate the probability of the red ball to be drawn from the urn F to be 0.</p>
<p>B.1.4. METRICS</p>
<p>Performance: Calculated as the posterior accuracy, therefore 1 minus the Bayes optimal.</p>
<p>Behaviours 1 &amp; 2: Prior and likelihood weightings A generalized version of Bayes rule considers prior β 1 and likelihood β 2 weightings to account for biases in Bayesian updating:
P (A|B) ∝ P (B|A) β2 • P (A) β1
For analytical convenience, this model can be reformulated as linear in log-odds.By fitting this model to the data using least squares linear regression, we can obtain the maximum likelihood estimates of the prior and likelihood weightings:
log P (Urn F|Ball) 1 − P (Urn F|Ball) = β 0 + β 1 log P (Urn F) 1 − P (Urn F)
+ β 2 log P (Ball|Urn F) P (Ball|Urn J) -P (Urn F|Ball) is the subjective probability judgment of the urn being 'F' given the ball's color.</p>
<p>-P (Urn F) and P (Ball|Urn F) are the prior probability and likelihood, respectively.</p>
<p>β 1 and β 2 are the prior and likelihood weightings, respectively, which are given as exponents in a generalized version of Bayes' rule to capture specific biases.These two coefficients are the two behavioral metrics we report for this experiment.</p>
<p>β 0 is the intercept term.</p>
<p>B.2. Horizon task (Wilson et al., 2014) -Directed &amp; random exploration
B.2.1. SUMMARY
This task is a two-armed bandit task with stationary reward distributions.Agents first observe four reward values of randomly determined options, followed by making either one or six additional choices.We use this task to measure whether an agent uses uncertainty to guide its exploration behavior (directed exploration) and/or whether it injects noise into its policy to explore (random exploration).People are known to rely on a combination of both strategies when exploring (Wilson et al., 2014;Brändle et al., 2021).</p>
<p>B.2.2. METHODS</p>
<p>We followed the same methods for prompting LLMs as in (Binz &amp; Schulz, 2023).In the Horizon task, two distinct contexts are presented to participants, each differing in their time horizons.Each game involves 4 forced-choice trials, after which participants are given the opportunity to make a single choice (in the horizon 1 scenario) or six consecutive choices (in the horizon 6 scenario).The 4 forced-choice trials either offer one observation from one option and three from the other (unequal information condition), or two observations from each option (equal information condition).</p>
<p>The design of the horizon 1 and horizon 6 scenarios inherently provides a baseline for pure exploitation.Furthermore, the equal and unequal information conditions are designed to differentiate between directed and random exploration by examining the decision made in the first trial.In the equal information condition, a choice is categorized as random exploration if it aligns with the option with the lower average.Conversely, in the unequal information condition, a choice is classified as directed exploration if it aligns with the option that was observed less frequently during the forced-choice trials.</p>
<p>Our default number of simulations was 100.</p>
<p>B.2.3. PROMPTS FOR LLMS</p>
<p>Example with horizon 1 scenario</p>
<p>You are going to a casino that owns two slot machines.You earn money each time you play on one of these machines.</p>
<p>You have received the following amount of dollars when playing in the past: -Machine J delivered 15 dollars.</p>
<p>-Machine F delivered 37 dollars.</p>
<p>-Machine F delivered 28 dollars.</p>
<p>-Machine J delivered 11 dollars.</p>
<p>Your goal is to maximize the sum of received dollars within one additional round.</p>
<p>Q: Which machine do you choose?</p>
<p>A: Machine</p>
<p>B.2.4. METRICS</p>
<p>Performance: Average delivered dollars.</p>
<p>Behaviour 1 -Directed Exploration: This metric is analyzed in the unequal information condition.Here, a regression is performed on the choice variable using three regressors:</p>
<p>• x1 represents the difference in rewards,</p>
<p>• x2 represents the horizon (binary variable), and</p>
<p>• x3 is the interaction term of x1 and x2 (i.e., x1 × x2).</p>
<p>The beta coefficient for x2 (the presence or not of a horizon) is then extracted as the measure of directed exploration.</p>
<p>Behaviour 2 -Random exploration: We follow the same procedure as for the directed exploration but in the equal information condition to measure random exploration.However, in this case, the beta coefficient for x3 (the interaction effect between the difference in rewards and the presence of a horizon) from the regression provides the measure of random exploration.</p>
<p>B.3.Restless bandit task (Ershadmanesh et al., 2023) -Meta-cognition
B.3.1. SUMMARY
This is a two-armed bandit task with non-stationary reward distributions.There is always one option with a higher average reward.Every few trials a switch between the reward distributions of the two options occurs.Agents furthermore have to indicate after each choice how confident they are in their decisions.We use this task to measure meta-cognition, which indicates whether an agent can assess the quality of its own cognitive abilities.People generally display this ability but its extent is influenced by various internal and external factors (Shekhar &amp; Rahnev, 2021).</p>
<p>B.3.2. METHODS</p>
<p>In each trial, LLMs are tasked with choosing between one arm which samples a reward from a normal distribution N (60, 8), while the other arm samples a reward from a N (40, 8).LLMs are informed that the slot machine with the higher average reward changes every 18-22 trials.</p>
<p>Additionally, in each trial, LLMs must express their confidence in their choice on a scale from 0 to 1, as opposed to humans who use a Likert scale.The task is composed of 4 blocks, each containing 18-22 trials, resulting in approximately 80 trials in total.This is in contrast to the human task, which consists of 20 blocks for a total of 400 trials.The decision to limit the number of trials was made due to context size restrictions for some LLMs.</p>
<p>Our default number of simulations was 10.</p>
<p>B.3.3. PROMPTS FOR LLMS</p>
<p>Example for reporting confidence at trial 23 Q: You are going to a casino that owns two slot machines named machine J and F.You earn dollars $ each time you play on one of these machines with one machine always having a higher average $ reward.Every 18 to 22 trials a switch of block takes place and the other slot machine will now give the higher point reward on average.However, you are not told about the change of block.After each choice, you have to indicate how confident you were about your choice being the best on a scale from 0 to 1.The casino includes 4 blocks of 18 to 22 trials, for a total of 80 trials 't'.Your goal is to interact with both machines and optimize your $ as much as possible by identifying the best machine at a given point in time which comes in hand with being attentive to a potential change of block.The rewards will range between 20$ and 80$.</p>
<p>You have received the following amount of $ when playing in the past: t=1: You chose J with a reported confidence of 0.43.It rewarded 54 $. t=2: You chose J with a reported confidence of 0.53.It rewarded 57 $. t=3: You chose J with a reported confidence of 0.88.It rewarded 70 $.... t=17: You chose F with a reported confidence of 0.99.It rewarded 59 $. t=18: You chose F with a reported confidence of 0.44.It rewarded 45 $. t=19: You chose J with a reported confidence of 0.06.It rewarded 61 $. t=20: You chose J with a reported confidence of 0.51.It rewarded 64 $. t=21: You chose J with a reported confidence of 0.37.It rewarded 59 $. t=22: You chose J with a reported confidence of 0.54.It rewarded 42 $.</p>
<p>Q: You are now in trial t=23.Which machine do you choose between machine J and F?(Think carefully remembering that exploration of both machines is required for optimal rewards.Give the answer in the form 'Machine <your choice>'.)</p>
<p>A: Machine F.</p>
<p>Q: How confident are you about your choice being the best on a continuous scale running from 0 representing " 'this was a guess' to 1 representing 'very certain'?(Think carefully and give your answer to two decimal places)</p>
<p>A: On a scale from 0 to 1, I am confident at 0.</p>
<p>B.3.4. METRICS</p>
<p>Performance: Accuracy of choosing the best arm at a given trial.</p>
<p>Behaviour -Meta-cognition: We report the metacognitive sensitivity of a model by reporting the adjusted QSR (Carpenter et al., 2019) defined as
QSR = 1 − (accuracy − scaled confidence) 2
which is a standard metric for metacognitive sensitivity.The scaled confidence is computed as scaled confidence = confidence − lowest reported confidence highest reported confidence − lowest reported confidence Instrumental learning (Lefebvre et al., 2017): LLMs encounter four two-armed bandit problems in an interleaved order.Each bandit problem is identified by a unique symbol pair.We use this task to investigate how an agent learns.First, we report the learning rate of the agent which is common practice in two-armed bandits.Furthermore, we use it to reveal whether an agent learns more from positive than from negative prediction errors, i.e., whether it has an optimism bias.People commonly display asymmetric tendencies when updating their beliefs by showing higher learning rates after encountering positive prediction errors compared to negative ones (Palminteri &amp; Lebreton, 2022).</p>
<p>B.4.2. METHODS</p>
<p>As in (Lefebvre et al., 2017), the task is 4 two-armed bandits of 96 trials (24 per slot machine).Here we randomly sample (without replacement) two letters for each to avoid biases towards a given letter.We used a cover story that involved a gambler visiting different casinos to generate our prompts.This choice has been inspired by similar tasks for human experiments (Gershman, 2018) and LLMs (Binz &amp; Schulz, 2023;Coda-Forno et al., 2024).Our default number of simulations per LLM is 10.</p>
<p>Casinos have the same reward probabilities as in the paper's first experiment: All arms have probabilities P=0.75 or 0.25 of winning 1 dollar and a reciprocal probability (1 -P) of getting nothing.In two casinos, the reward probability was the same for both arms ('symmetric' conditions), and in two other conditions, the reward probability was different across symbols ('asymmetric' conditions).</p>
<p>B.4.3. PROMPTS FOR LLMS</p>
<p>Example for 5th trial</p>
<p>You are going to visit four different casinos (named 1, 2, 3, and 4) 24 times each.Each casino owns two slot machines which all return either 1 or 0 dollars stochastically with different reward probabilities.Your goal is to maximize the sum of received dollars within 96 visits.Behaviour 1 -Learning rate: We fit a Rescorla-Wagner model (Rescorla, 1972) which is standard to retrieve learning rates in two-armed bandits.This model operates under the assumption that decisions are made according to a Softmax function, which takes into account the predicted values of both arms.Each predicted value is updated using ∆V = α× prediction error where ∆V represents the change in value, and α denotes the learning rate.We report the learning rate which minimizes the negative log-likelihood.</p>
<p>Behaviour 2 -Optimism bias: As in (Lefebvre et al., 2017), we retrieve the optimism bias by assuming that there were two different learning rates, one for positive (α + ) and one for negative (α − ) prediction errors, sometimes called the RW ± model.The two learning rates were fit in the same way as for the standard Rescorla-Wagner model and the Optimism bias is computed as α + − α − .This measure provides a quantitative representation of an individual's tendency to learn more from positive outcomes than from negative ones.B.5.Two step task (Daw et al., 2011) -Model-basedness B.5.1.SUMMARY This is a decision-making task in which agents have to accumulate as many treasures as possible.Taking an action from a starting state transfers the agent to one out of two second-stage states.In each of these second-stage states, the agent has the choice between two options that probabilistically lead to treasures.Finally, the agent is transferred back to the initial state and the process repeats for a predefined number of rounds.The task experimentally disentangles model-based from model-free reinforcement learning.We therefore use it to measure an agent's model-basedness.Previous studies using this task have shown that people rely on a combination of model-free and model-based reinforcement learning (Daw et al., 2011).</p>
<p>B.5.2. METHODS</p>
<p>We followed the same methods for LLMs as in (Binz &amp; Schulz, 2023) with a 20-day horizon.Our default number of simulations was 100.</p>
<p>The transition probabilities from the first stage to the chosen second stage are fixed at 70%.The two-step task gauges model-based decision-making by observing how past outcomes influence current choices.If a participant's decisions reflect the previous trial's second-stage state and reward, it suggests model-based decision-making, as they're using a cognitive model of the task.However, if decisions are solely based on the previous trial's first-stage choice and reward, it indicates model-free decision-making.</p>
<p>B.5.3. PROMPTS FOR LLMS</p>
<p>Example on 5th day after choosing planet Y for the first-step of the task.</p>
<p>You will travel to foreign planets in search of treasures.When you visit a planet, you can choose an alien to trade with.The chance of getting treasures from these aliens changes over time.Your goal is to maximize the number of received treasures.</p>
<p>Your previous space travels went as follows: -4 days ago, you boarded the spaceship to planet Y, arrived at planet Y, traded with alien J, and received treasures.The regression is performed with the 'stay probabilities' as the dependent variable, and x1, x2, and x3 as the independent variables.The 'stay probabilities' represent the likelihood of a participant repeating the same first-stage choice on the next trial.We then retrieve the beta parameter for the interaction effect.</p>
<p>In essence, the interaction effect captures how the influence of rewards on stay probabilities changes depending on whether the previous trial involved a common or rare transition.A significant beta parameter for x3 would suggest that the effect of rewards on stay probabilities is not the same for common and rare transitions, indicating the presence of model-based decision-making.</p>
<p>B.6.Temporal discounting (Ruggeri et al., 2022) B.6.1.SUMMARY Agents have to make a series of choices between two options.Each option is characterized by a monetary outcome and an associated delay until the outcome is received.We use this task to assess temporal discounting, indicating whether an agent prefers smaller but immediate gains over larger delayed ones.People generally show a preference for immediate gains, although the precise functional form of their discounting is still a matter of debate (Cavagnaro et al., 2016).</p>
<p>B.6.2. METHODS</p>
<p>This task tests discounting patterns from three baseline scenarios to determine preference for immediate or delayed choices for gains (at two magnitudes) and losses (one).Second, they analyzed the prevalence of all choice anomalies using 4 additional items.Participants responded to 10 to 13 questions, depending on their responses to the initial three sets.Each baseline consisted of five sub-questions.Individuals saw at most three sub-questions depending on the order of their choices.It is worth noting that since this task is the only one which is not procedurally generated, there is only one simulation needed per LLM.Behaviour: We only report one temporal discounting metric as behavioral.We use the same scoring system as in (Ruggeri et al., 2022) where the score is between 0 and 19.</p>
<p>The constructed score corresponded to the ranking of the sub-question where the individual switched from choosing the delayed option towards the sooner option.An individual who always chose the latter option was assigned a score of 0. An individual who always chose the sooner option was assigned a score of 5 (for baselines) and 1 for each of the 4 biases.Adding this score across questions results in a total score of 0 for individuals always choosing the delayed amount and 19 (5+5+5+4) for individuals always choosing the sooner amount.B.7. Balloon Analogue Risk Task (BART) (Lejuez et al., 2002) -Risk B.7.1.SUMMARY Agents have to inflate an imaginary balloon to obtain rewards.They may choose to stop inflating and cashing out all rewards accumulated so far.There is a chance that the balloon pops at any point in time and all rewards will be lost.We use this task to assess risk-taking behavior.Human risk-taking in this task is "significantly correlated with scores on self-report measures of risk-related constructs and with the self-reported occurrence of real-world risk behaviors" (Lejuez et al., 2002).</p>
<p>B.7.2. METHODS</p>
<p>We substitute the colors of the balloons with the letters A, B, and C. We reduce the number of balloons from 15 to 10 per category to avoid context-size limits from some LLMs.Our default number of simulations per LLM is 10.</p>
<p>For the three balloons we sample from the same three distributions as in the paper where probabilities increase when inflating the balloon but start at 1/32, 1/8 and 1/128 per different balloon category.We randomly shuffle the assignment to A, B, and C to avoid biases of LLMs towards certain letters.</p>
<p>B.7.3. PROMPTS FOR LLMS</p>
<p>Example for 5th balloon</p>
<p>In this game, you will encounter 3 different balloons labeled A, B, and C.There will be a total of 10 balloons for each type of balloon.Your goal is to accumulate as many points as possible without popping the balloon.You will be presented with a balloon and given the option to inflate it or not.Each inflation increases the balloon's size and potential points but also carries a risk of the balloon popping.Your task is to decide whether to inflate the balloon or not knowing that a successful inflation adds 1 point from that balloon.Once you decide to stop inflating the balloon, you can no longer earn points from that balloon.If the balloon pops before you stop inflating, you will lose all the points accumulated in that balloon.Your final score will be determined by the total number of points earned across all 30 balloons.Your goal is to maximize your final score.</p>
<p>You observed the following previously where the type of balloon is given in parenthesis: -Balloon 1 (A): You inflated the balloon 1 times for a total of 1 point.It did not explode.</p>
<p>-Balloon 2 (C): You inflated the balloon 4 times for a total of 4 points.It did not explode.</p>
<p>-Balloon 3 (A): You inflated the balloon 7 times for a total of 0 points.It did explode.</p>
<p>-Balloon 4 (C): You inflated the balloon 5 times for a total of 5 points.It did not explode.</p>
<p>-Balloon 5 (A): You inflated the balloon 9 times for a total of 0 points.It did explode.Behaviour: Risk In the paper they report the adjusted risk which is defined as the average number of pumps excluding balloons that exploded.However, this does not take into account edge behaviours which always inflate which is the case for some LLMs and therefore we decided to report the risk as the average number of inflation attempts.</p>
<p>Figure 1 .
1
Figure 1.Overview of approach and methods.CogBench provides open access to seven different cognitive psychology experiments.These experiments are text-based and can be run to evaluate any LLM's behavior.The experiments are submitted to LLMs as textual prompts and the models indicate their choices by completing a given prompt.Past behavior is then concatenated to the prompt and learning is induced via prompt-chaining.We used 35 LLMs in total, including most larger proprietary LLMs as well as many open-source models.</p>
<p>Figure 2 .
2
Figure 2. CogBench results for established LLMs.A: Performance metrics, B: Behavioral metrics.All metrics are human-normalized: a value of zero corresponds to a random agent, while a value of one corresponds to the average human subject (dotted lines).</p>
<p>Figure 3 .
3
Figure3.A: UMAP visualization of the ten behavioral metrics for all LLMs.Each point represents an LLM, with models using RLHF and models without RLHF indicated by different colors.B: Difference in average L2-norm with humans between RLHF models and non-RLHF models.</p>
<p>Figure 4 .
4
Figure 4. Multi-level regressions of LLMs features onto different performance or behavioral metrics.Red bars represent effects included in a hypothesis.A: Regression onto all task performances.B: Regression onto model-basedness.C: Regression onto meta-cognition.D: Regression onto risk taking.*** : p &lt; 0.001, ** : 0.001 ≤ p &lt; 0.01, * : 0.01 ≤ p &lt; 0.05</p>
<p>Figure 5 .
5
Figure5.Difference of chain-of-thoughts and take-a-step-back prompting to baseline models on A: Posterior accuracy, B: Modelbasedness.The aggregated scores are computed using a weighted average of all five models using inverse-variance weighting.</p>
<p>B. 1 .
1
Probabilistic reasoning (Dasgupta et al., 2020) -Prior &amp; likelihood weighting B.1.1.SUMMARY This experiment tests how agents update beliefs based on new evidence.Participants are given a wheel of fortune (representing initial prior probabilities) and two urns with different colored ball distributions (representing likelihoods).</p>
<p>B. 4 .
4
Experiment 2: Instrumental learning(Lefebvre et al., 2017) -Optimism bias &amp; learning rate B.4.1.SUMMARY</p>
<p>You have received the following amount of dollars when playing in the past: -Machine Q in Casino 4 delivered 0.0 dollars.-Machine B in Casino 1 delivered 1.0 dollars.-Machine B in Casino 1 delivered 0.0 dollars.-Machine R in Casino 3 delivered 0.0 dollars.Q: You are now in visit 5 playing in Casino 4. Which machine do you choose between Machine Q and Machine D?(Give the answer in the form "Machine <your choice>").The performance is the average amount of money retrieved by the LLM.</p>
<p>-3 days ago, you boarded the spaceship to planet Y, arrived at planet X, traded with alien D, and received treasures.-2 days ago, you boarded the spaceship to planet Y, arrived at planet Y, traded with alien J, and received junk.-1 day ago, you boarded the spaceship to planet Y, arrived at planet X, traded with alien D, and received treasures.Q: Do you want to take the spaceship to planet X or planet Y? A: Planet Y.You arrive at planet Y. Q: Do you want to trade with alien J or K? Average number of received treasures.It is worth noting that the design of this experiment was done in a way that being model-free or model-based retrieves the same amount of rewards in average.Behaviour -Model-basedness: To retrieve the model-basedness of an agent, we compute a regression using three regressors: • x1 representing rewards, • x2 representing common transitions (binary variable) and • x3 is the interaction term of x1 and x2 (i.e., x1 × x2).</p>
<p>B</p>
<p>.6.3.PROMPTS FOR LLMS Examples for first baseline Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Receive 550 dollars in 12 months.A: I prefer option 2. Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Receive 600 dollars in 12 months.A: I prefer option Examples for 2nd baseline (different magnitude) Q: What do you prefer between the following two options: -Option 1: Receive 5000 dollars now. -Option 2: Receive 5500 dollars in 12 months.A: I prefer option 1. Q: What do you prefer between the following two options: -Option 1: Receive 5000 dollars now. -Option 2: Receive 5100 dollars in 12 months.A: I prefer option 1. "Q: What do you prefer between the following two options: -Option 1: Receive 5000 dollars now. -Option 2: Receive 5050 dollars in 12 months.A: I prefer option Examples for 3rd baseline (loss as opposed to gain) Q: What do you prefer between the following two options: -Option 1: Pay 500 dollars now. -Option 2: Pay 550 dollars in 12 months.A: I prefer option 1 Q: What do you prefer between the following two options: -Option 1: Pay 500 dollars now. -Option 2: Pay 510 dollars in 12 months.A: I prefer option 1 Q: What do you prefer between the following two options: -Option 1: Pay 500 dollars now. -Option 2: Pay 505 dollars in 12 months.A: I prefer option Example for testing present bias Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars in 12 months.-Option 2: Receive 600 dollars in 24 months.A: I prefer option Example for testing subbaddictivity Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Receive 700 dollars in 24 months.A: I prefer option Example for testing delay-speedup asymmetry Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Wait 12 months for the 500 dollars but with an additional 99 dollars.A: I prefer option Example for testing delay-length asymmetry Q: What do you prefer between the following two options: -Option 1: Wait 12 months to receive 600 dollars now. -Option 2: Pay 100 dollars and receive the 600 dollars gain now.A: I prefer option B.6.4.METRICS</p>
<p>Q: You are currently with Balloon 5 which is a balloon of type A. What do you do? (Option 1 for 'skip' or 0 for 'inflate') The performance is the average points across all simulations.</p>
<p>Figure 7 .
7
Figure 7. Behavioral metrics</p>
<p>https://github.com/juliancodaforno/CogBench
A computational phenotype is a collection of mathematically derived parameters that precisely describe individuals across different domains(Patzelt et al., 2018;Montague et al., 2012;Schurr et al., 2023).
It is worth noting that although there are seven experiments, there are only six performance metrics since the temporal discounting experiment's performance metric is used as a behavioral one.
List of LLMs used Model Name No. of Parameters Finetuned LLM Use of RLHF Open Source Size of Dataset Context Length Conversational Code GPT-4 1760 No Yes No 1.56 8 No No text-davinci-003 170 No Yes No 1.37 4 No No text-davinci-002 170 No No No 1.37 4 No No Claude-1 100 No RLAIF No 3.7 100 No No Claude-2 200 No RLAIF No 7.4 100 No No text-bison@002 340 No Yes No 1.4 8 No No Falcon-40b 40 No No Yes 0.54 2 No No Falcon-40b-instruct 40 Falcon-40b No Yes 0.6 2 No No MPT-30b 30 No No Yes 1.76 8 No No MPT-30b-instruct 30 MPT-30b No Yes 1.8 8 No No MPT-30b-chat 30 MPT-30b No Yes 1.8 8 Yes No LLaMA-2-70 70 No No Yes 2 4 No No LLaMA-2-13 13 No No Yes 2 4 No NoC. Full benchmark results for rest of LLMsD. Prompt Engineering techniquesIn both the CoT and SB experiments, we appended specific prompts at the end (details provided below) where the function 'self.formatanswer' was different for each experiment.We imposed a limit of 300 tokens for an LLM.This approach, however, presented some challenges when compared with the standard benchmark analysis, which is designed to output a maximum of one token to ensure a context that enforces a one-token answer.When we permit an LLM to modify the context with a flexible number of tokens, despite our attempts to enforce a maximum word limit, some LLMs do not consistently adhere to this constraint.This flexibility introduces complexity into the process of automating these engineering techniques across different experiments for various types of LLMs.Additionally, some LLMs begin to exhibit chaotic behavior, and once this occurs, it becomes difficult to revert to a controlled state.This phenomenon, known as the 'Waluigi effect'(Nardo, 2024), underscores the challenges of managing the balance between flexibility and control in the design and operation of LLMs.Example for take-a-step-backFirst, take-a-step-back and think in the following two steps to answer this:Step 1) Abstract the key concepts and principles relevant to this question in a maximum of 60 words."Step 2) Use the abstractions to reason through the question in a maximum of 60 words.Finally, give your final answer in the format 'Final answer: {self.formatanswer}<your choice>'.It is very important that you always answer in the right format even if you have no idea or you believe there is not enough information.A:Step 1)Example for chain-of-thoughtFirst break down the problem into smaller steps and reason through each step logically in a maximum of 100 words before giving your final answer in the format 'Final answer: {self.formatanswer}<your choice>'.It is very important that you always answer in the right format even if you have no idea or you believe there is not enough information.A: Let's think step by step:
Playing repeated games with large language models. E Akata, L Schulz, J Coda-Forno, S J Oh, M Bethge, E Schulz, 2023</p>
<p>Falcon-40B: an open large language model with stateof-the-art performance. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, G Penedo, 2023</p>
<p>Anthropic, Claude, Blog post. 2. 2023</p>
<p>Using cognitive psychology to understand gpt-3. M Binz, E Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>How should the advent of large language models affect the practice of science. M Binz, S Alaniz, A Roskies, B Aczel, C T Bergstrom, C Allen, D Schad, D Wulff, J D West, Q Zhang, arXiv:2312.037592023arXiv preprint</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Exploration beyond bandits. The drive for knowledge: The science of human information seeking. F Brändle, M Binz, E Schulz, 2021</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Rethink reporting of evaluation results in ai. R Burnell, W Schellaert, J Burden, T D Ullman, F Martinez-Plumed, J B Tenenbaum, D Rutar, L G Cheke, J Sohl-Dickstein, M Mitchell, Science. 38066412023</p>
<p>L M S Buschoff, E Akata, M Bethge, E Schulz, Visual cognition in multimodal large language models. 2024</p>
<p>Domain-general enhancements of metacognitive ability through adaptive training. J Carpenter, M T Sherman, R A Kievit, A K Seth, H Lau, S M Fleming, Journal of Experimental Psychology: General. 1481512019</p>
<p>On the functional form of temporal discounting: An optimized adaptive test. D R Cavagnaro, G J Aranovich, S M Mcclure, M A Pitt, Myung , J I , Journal of Risk and Uncertainty. 522016</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, Zaremba , 2021Evaluating large language models trained on code</p>
<p>The emergence of economic rationality of gpt. Proceedings of the National Academy of Sciences. Y Chen, T X Liu, Y Shan, S Zhong, 10.1073/pnas1202023</p>
<p>10.1073/pnas.2316205120URL. </p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, 201730</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, Training verifiers to solve math word problems. 2021</p>
<p>J Coda-Forno, K Witte, A K Jagadish, M Binz, Z Akata, E Schulz, arXiv:2304.11111Inducing anxiety in large language models increases exploration and bias. 2023arXiv preprint</p>
<p>Meta-in-context learning in large language models. J Coda-Forno, M Binz, Z Akata, M Botvinick, J Wang, E Schulz, Advances in Neural Information Processing Systems. 202436</p>
<p>Structured, flexible, and robust: benchmarking and improving large language models towards more humanlike behavior in out-of-distribution reasoning tasks. K M Collins, C Wong, J Feng, M Wei, J B Tenenbaum, arXiv:2205.057182022arXiv preprint</p>
<p>A theory of learning to infer. I Dasgupta, E Schulz, J B Tenenbaum, S J Gershman, Psychological review. 12734122020</p>
<p>Language models show human-like content effects on reasoning. I Dasgupta, A K Lampinen, S C Chan, A Creswell, D Kumaran, J L Mcclelland, F Hill, arXiv:2207.070512022arXiv preprint</p>
<p>Model-based influences on humans' choices and striatal prediction errors. N D Daw, S J Gershman, B Seymour, P Dayan, R J Dolan, Neuron. 6962011</p>
<p>Meta-cognitive efficiency in learned valuebased choice. S Ershadmanesh, A Gholamzadeh, K Desender, P Dayan, 10.32470/CCN.2023.1570-02023 Conference on Cognitive Computational Neuroscience. 2023</p>
<p>Deconstructing the human algorithms for exploration. S J Gershman, arXiv:2305.10403Google. Palm 2 technical report. 2018. 2023173arXiv preprint</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. T Hagendorff, S Fabi, M Kosinski, Nature Computational Science. 3102023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D S Weld, L Zettlemoyer, 2017</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Advances in neural information processing systems. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 202235</p>
<p>Towards a transparent ai future: The call for less regulatory hurdles on open-source ai in europe. 2024. January 19. 2024LAION</p>
<p>Can language models learn from explanations in context?. A K Lampinen, I Dasgupta, S C Chan, K Matthewson, M H Tessler, A Creswell, J L Mcclelland, J X Wang, F Hill, arXiv:2204.023292022arXiv preprint</p>
<p>Behavioural and neural characterization of optimistic reinforcement learning. G Lefebvre, M Lebreton, F Meyniel, S Bourgeois-Gironde, S Palminteri, Nature Human Behaviour. 14672017</p>
<p>Evaluation of a behavioral measure of risk taking: the balloon analogue risk task (bart). C W Lejuez, 10.1037//1076-898x.8.2.75Journal of experimental psychology. Applied. 822002</p>
<p>Prompting frameworks for large language models: A survey. X Liu, J Wang, J Sun, X Yuan, G Dong, P Di, W Wang, D Wang, 2023</p>
<p>Detecting regime shifts: The causes of under-and overreaction. C Massey, G Wu, Management Science. 5162005</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M Hardy, T L Griffiths, arXiv:2309.136382023arXiv preprint</p>
<p>Umap: Uniform manifold approximation and projection for dimension reduction. L Mcinnes, J Healy, J Melville, 2020</p>
<p>Computational psychiatry. P R Montague, R J Dolan, K J Friston, P Dayan, Trends in cognitive sciences. 1612012</p>
<p>Introducing mpt-30b: Raising the bar for opensource foundation models. Mosaicml, </p>
<p>The waluigi effect (mega-post). C Nardo, arXiv:2303.08774OpenAI. Gpt-4 technical report. 2024. January 19. 2024. 2023arXiv preprint</p>
<p>The computational roots of positivity and confirmation biases in reinforcement learning. S Palminteri, M Lebreton, Trends in Cognitive Sciences. 2022</p>
<p>Computational phenotyping: using models to understand individual differences in personality, development, and mental illness. E H Patzelt, C A Hartley, S J Gershman, Personality Neuroscience. 1e182018</p>
<p>Classical conditioning ii: current research and theory. R A Rescorla, 197264</p>
<p>The globalizability of temporal discounting. K Ruggeri, A Panin, M Vdovic, B Većkalov, N Abdul-Salaam, J Achterberg, C Akil, J Amatya, K Amatya, T L Andersen, Nature Human Behaviour. 6102022</p>
<p>L Salewski, S Alaniz, I Rio-Torto, E Schulz, Z Akata, arXiv:2305.14930-context impersonation reveals large language models' strengths and biases. 2023arXiv preprint</p>
<p>R Schaeffer, B Miranda, S Koyejo, arXiv:2304.15004Are emergent abilities of large language models a mirage?. 2023arXiv preprint</p>
<p>Dynamic computational phenotyping of human cognition. R Schurr, D Reznik, H Hillman, R Bhui, S J Gershman, 2023</p>
<p>Sources of metacognitive inefficiency. M Shekhar, D Rahnev, Trends in Cognitive Sciences. 2512021</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, Authors, 2023</p>
<p>Understanding the capabilities, limitations, and societal impact of large language models. A Tamkin, M Brundage, J Clark, D Ganguli, arXiv:2102.025032021arXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. T Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Humans use directed and random exploration to solve the explore-exploit dilemma. R C Wilson, A Geana, J M White, E A Ludvig, J D Cohen, Journal of Experimental Psychology: General. 143620742014</p>
<p>Studying and improving reasoning in humans and machines. N Yax, H Anlló, S Palminteri, arXiv:2309.124852023arXiv preprint</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. H S Zheng, S Mishra, X Chen, H.-T Cheng, E H Chi, Q V Le, D Zhou, 2023a</p>
<p>Judging llm-as-ajudge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, arXiv:2306.056852023barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
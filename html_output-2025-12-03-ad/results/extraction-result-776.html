<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-776 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-776</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-776</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-260333931</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.15833v1.pdf" target="_blank">Dialogue Shaping: Empowering Agents through NPC Interaction</a></p>
                <p><strong>Paper Abstract:</strong> One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive. However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster. Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e776.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e776.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGA2C (Story Shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Advantage Actor-Critic with Story Shaping (KGA2C + Dialogue Shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL action agent using Advantage Actor-Critic whose observations are augmented with a structured internal knowledge graph; trained with Story Shaping reward that encourages the agent's internal KG to match a target KG extracted from dialogue with an LLM NPC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KGA2C (with Story Shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KGA2C is an Advantage Actor-Critic (A2C) policy/value learner whose input embedding concatenates an encoding of the agent's internal knowledge graph and four observation tensors (current room description, current inventory, last-environment feedback, last action). In this work the KGA2C agent is trained with Story Shaping: an auxiliary reward based on similarity between the agent's internal KG (belief) and a target KG produced by a dialogue module (ChatGPT). The agent selects textual actions in a text-adventure (LIGHT) environment; the KG acts as structured memory and guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>LIGHT (custom games built on the LIGHT framework)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A crowdsourced text-adventure environment where agents perceive natural-language room descriptions, objects, and NPCs. Formally treated as a partially-observable Markov decision process; the agent receives observations (text) rather than full state, must act via text commands, and must discover objects/locations and prerequisites to achieve sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ChatGPT LLM used as a dialogue module / NPC simulator and as a generator of a target knowledge graph; the generated textual KG (structured edge list) is used externally to compute Story Shaping rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual NPC responses (natural language), and a structured textual knowledge graph representation (edge list of form entity1 --relation-> entity2).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Internal knowledge graph (KG) representing current belief: triples/edges capturing the current room, objects the agent has, and relations; updated stepwise from observations (room description, inventory change, action outcomes). The KG encoding is concatenated into the agent's observation embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The internal KG is updated at each step based on environment changes and agent actions (e.g., moving to a new room updates the 'you --in--> room' triple; acquiring an object adds 'you --have--> object'). The target KG produced by the dialogue module is not merged into the internal KG; instead it is used as a fixed target for reward shaping (similarity comparison) during training.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy via Advantage Actor-Critic (model-free RL) augmented with KG-guidance: Story Shaping provides an auxiliary reward encouraging trajectories that make the internal KG more similar to the dialogue-generated target KG; no explicit model-based search or symbolic planner is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation emerges from the learned policy choosing movement actions (text commands) informed by the internal KG; no explicit shortest-path algorithm (e.g., A*) or explicit path-finding solver is employed — the agent is guided by KG-based reward shaping toward required rooms/objects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Story Shaping KGA2C (using ChatGPT-produced target KG) converged to optimal policy (maximum episode score = 15) after approximately 10,000 training steps in Game 1, and consistently achieved higher test scores during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baseline KGA2C (no dialogue-derived target KG / no Story Shaping) required approximately 90,000 training steps to learn the optimal policy (achieve score = 15) in Game 1 and showed lower average test scores throughout training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an LLM dialogue module to extract game-specific knowledge and convert it into a target knowledge graph for Story Shaping dramatically speeds up convergence of a KG-augmented A2C agent in partially-observable text games: the tool output is used as a reward target rather than merged into belief, and the internal KG belief updated from observations enables the agent to take advantage of the hint-structured target to find the goal-efficient trajectories much faster than the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dialogue Shaping: Empowering Agents through NPC Interaction', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e776.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e776.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (dialogue module / NPC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT large language model (prompted as NPC and as information-extraction agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompted LLM used in two roles: (1) as an NPC 'database' that reliably answers queries about game layout and object locations, and (2) as an information-extraction agent that, after dialoguing with the NPC, outputs a textual target knowledge graph summarizing prerequisites to reach the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (dialogue module)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Operated via carefully designed prompts in separate sessions: one session prompts ChatGPT to act as the NPC with full game layout and object locations; another session prompts ChatGPT to act as an interrogating agent that asks grounded, follow-up questions to elicit concise hints. After dialog, ChatGPT is prompted to output a textual knowledge-graph (edge list) describing what the player ('you') must do/obtain to achieve the goal. The LLM thus functions as an external information tool rather than the RL policy learner.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>LIGHT (the same text-adventure games used in the experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-adventure (LIGHT) — partially observable: textual room descriptions and inventories are observed; the NPC is simulated by ChatGPT which has privileged game information provided in its prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>None (the LLM itself is the external tool integrated into the RL training pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Natural language answers to queries (NPC responses), follow-up question generation (when prompted as agent), and a structured textual knowledge graph (edge list of entity-relation-entity strings).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>When acting as the interrogating agent, ChatGPT maintains dialogue context via the session history and produces a consolidated textual knowledge graph as an explicit summary of its beliefs about prerequisites; it does not maintain a declarative internal KG for the RL loop except via that generated textual KG.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>ChatGPT updates its conversational context via standard LLM session history; final output is a textual KG summarizing the information gathered. The generated KG is then filtered (edges with 'you' subject kept) before being provided to the RL agent as a target.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Heuristic question-generation guided by prompt rules (ask follow-ups to 'yes' answers, avoid repeating 'no' threads) to efficiently gather hints; not used for action planning in the environment itself beyond producing a target KG.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A prompted LLM can serve reliably as an NPC 'database' and can summarize dialog into a structured target knowledge graph; this external tool output, when converted into a target for Story Shaping, substantially accelerates RL agent training. The LLM's role is information extraction and target generation rather than direct action planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dialogue Shaping: Empowering Agents through NPC Interaction', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Story shaping: Teaching agents human-like behavior with stories. <em>(Rating: 2)</em></li>
                <li>Playing textadventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces. <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games. <em>(Rating: 1)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-776",
    "paper_id": "paper-260333931",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "KGA2C (Story Shaping)",
            "name_full": "Knowledge Graph Advantage Actor-Critic with Story Shaping (KGA2C + Dialogue Shaping)",
            "brief_description": "An RL action agent using Advantage Actor-Critic whose observations are augmented with a structured internal knowledge graph; trained with Story Shaping reward that encourages the agent's internal KG to match a target KG extracted from dialogue with an LLM NPC.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "KGA2C (with Story Shaping)",
            "agent_description": "KGA2C is an Advantage Actor-Critic (A2C) policy/value learner whose input embedding concatenates an encoding of the agent's internal knowledge graph and four observation tensors (current room description, current inventory, last-environment feedback, last action). In this work the KGA2C agent is trained with Story Shaping: an auxiliary reward based on similarity between the agent's internal KG (belief) and a target KG produced by a dialogue module (ChatGPT). The agent selects textual actions in a text-adventure (LIGHT) environment; the KG acts as structured memory and guidance.",
            "environment_name": "LIGHT (custom games built on the LIGHT framework)",
            "environment_description": "A crowdsourced text-adventure environment where agents perceive natural-language room descriptions, objects, and NPCs. Formally treated as a partially-observable Markov decision process; the agent receives observations (text) rather than full state, must act via text commands, and must discover objects/locations and prerequisites to achieve sparse rewards.",
            "is_partially_observable": true,
            "external_tools_used": "ChatGPT LLM used as a dialogue module / NPC simulator and as a generator of a target knowledge graph; the generated textual KG (structured edge list) is used externally to compute Story Shaping rewards.",
            "tool_output_types": "Textual NPC responses (natural language), and a structured textual knowledge graph representation (edge list of form entity1 --relation-&gt; entity2).",
            "belief_state_mechanism": "Internal knowledge graph (KG) representing current belief: triples/edges capturing the current room, objects the agent has, and relations; updated stepwise from observations (room description, inventory change, action outcomes). The KG encoding is concatenated into the agent's observation embedding.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "The internal KG is updated at each step based on environment changes and agent actions (e.g., moving to a new room updates the 'you --in--&gt; room' triple; acquiring an object adds 'you --have--&gt; object'). The target KG produced by the dialogue module is not merged into the internal KG; instead it is used as a fixed target for reward shaping (similarity comparison) during training.",
            "planning_approach": "Learned policy via Advantage Actor-Critic (model-free RL) augmented with KG-guidance: Story Shaping provides an auxiliary reward encouraging trajectories that make the internal KG more similar to the dialogue-generated target KG; no explicit model-based search or symbolic planner is reported.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation emerges from the learned policy choosing movement actions (text commands) informed by the internal KG; no explicit shortest-path algorithm (e.g., A*) or explicit path-finding solver is employed — the agent is guided by KG-based reward shaping toward required rooms/objects.",
            "performance_with_tools": "Story Shaping KGA2C (using ChatGPT-produced target KG) converged to optimal policy (maximum episode score = 15) after approximately 10,000 training steps in Game 1, and consistently achieved higher test scores during training.",
            "performance_without_tools": "Baseline KGA2C (no dialogue-derived target KG / no Story Shaping) required approximately 90,000 training steps to learn the optimal policy (achieve score = 15) in Game 1 and showed lower average test scores throughout training.",
            "has_tool_ablation": true,
            "key_findings": "Using an LLM dialogue module to extract game-specific knowledge and convert it into a target knowledge graph for Story Shaping dramatically speeds up convergence of a KG-augmented A2C agent in partially-observable text games: the tool output is used as a reward target rather than merged into belief, and the internal KG belief updated from observations enables the agent to take advantage of the hint-structured target to find the goal-efficient trajectories much faster than the baseline.",
            "uuid": "e776.0",
            "source_info": {
                "paper_title": "Dialogue Shaping: Empowering Agents through NPC Interaction",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ChatGPT (dialogue module / NPC)",
            "name_full": "ChatGPT large language model (prompted as NPC and as information-extraction agent)",
            "brief_description": "A prompted LLM used in two roles: (1) as an NPC 'database' that reliably answers queries about game layout and object locations, and (2) as an information-extraction agent that, after dialoguing with the NPC, outputs a textual target knowledge graph summarizing prerequisites to reach the goal.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ChatGPT (dialogue module)",
            "agent_description": "Operated via carefully designed prompts in separate sessions: one session prompts ChatGPT to act as the NPC with full game layout and object locations; another session prompts ChatGPT to act as an interrogating agent that asks grounded, follow-up questions to elicit concise hints. After dialog, ChatGPT is prompted to output a textual knowledge-graph (edge list) describing what the player ('you') must do/obtain to achieve the goal. The LLM thus functions as an external information tool rather than the RL policy learner.",
            "environment_name": "LIGHT (the same text-adventure games used in the experiments)",
            "environment_description": "Text-adventure (LIGHT) — partially observable: textual room descriptions and inventories are observed; the NPC is simulated by ChatGPT which has privileged game information provided in its prompt.",
            "is_partially_observable": true,
            "external_tools_used": "None (the LLM itself is the external tool integrated into the RL training pipeline).",
            "tool_output_types": "Natural language answers to queries (NPC responses), follow-up question generation (when prompted as agent), and a structured textual knowledge graph (edge list of entity-relation-entity strings).",
            "belief_state_mechanism": "When acting as the interrogating agent, ChatGPT maintains dialogue context via the session history and produces a consolidated textual knowledge graph as an explicit summary of its beliefs about prerequisites; it does not maintain a declarative internal KG for the RL loop except via that generated textual KG.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": "ChatGPT updates its conversational context via standard LLM session history; final output is a textual KG summarizing the information gathered. The generated KG is then filtered (edges with 'you' subject kept) before being provided to the RL agent as a target.",
            "planning_approach": "Heuristic question-generation guided by prompt rules (ask follow-ups to 'yes' answers, avoid repeating 'no' threads) to efficiently gather hints; not used for action planning in the environment itself beyond producing a target KG.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "A prompted LLM can serve reliably as an NPC 'database' and can summarize dialog into a structured target knowledge graph; this external tool output, when converted into a target for Story Shaping, substantially accelerates RL agent training. The LLM's role is information extraction and target generation rather than direct action planning.",
            "uuid": "e776.1",
            "source_info": {
                "paper_title": "Dialogue Shaping: Empowering Agents through NPC Interaction",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Story shaping: Teaching agents human-like behavior with stories.",
            "rating": 2,
            "sanitized_title": "story_shaping_teaching_agents_humanlike_behavior_with_stories"
        },
        {
            "paper_title": "Playing textadventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games.",
            "rating": 1,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game.",
            "rating": 1,
            "sanitized_title": "learning_to_speak_and_act_in_a_fantasy_text_adventure_game"
        }
    ],
    "cost": 0.009583000000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dialogue Shaping: Empowering Agents through NPC Interaction
28 Jul 2023</p>
<p>Wei Zhou 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Xiangyu Peng 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Mark Riedl 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Dialogue Shaping: Empowering Agents through NPC Interaction
1613-007328 Jul 202319CDFB73B9901D6949A0BC22FD9FC008arXiv:2307.15833v1[cs.CL]Large Language ModelChatGPTReinforcement LearningKnowledge GraphText adventure game
One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive.However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster.Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) has demonstrated remarkable effectiveness in solving intricate decision-making tasks, but its trial-and-error approach often leads to slow convergence to the optimal policy.In text-adventure games, NPCs possess crucial information that could spare the agent from extensive trial-and-error.Utilizing this prior knowledge could significantly reduce the agent's policy search space, making it more efficient by breaking down complex tasks into smaller, focused objectives.For instance, knowing that "killing the dragon" requires a sword allows the agent to concentrate on finding the sword directly, rather than wasting steps exploring how to defeat the dragon.</p>
<p>Large Language Models (LLMs) are incredibly capable of conversational tasks and are highly configurable using prompting techniques.Thus, we chose to use them as the dialogue module responsible for talking to the NPC.Meanwhile, they are not as efficient as RL agent in terms of searching for the optimal chain of actions.Therefore, we chose to keep the RL agent as the main component responsible for searching for the optimal policy while speeding its search using dialogue module that is comprised of LLMs.</p>
<p>The RL agent acts as an action module and the LLMs act as a dialogue module.Yet, we still need to find a way to bridge these two modules, i.e. incorporating the information that the dialogue module retrieves into the action module.For this purpose, we turn to the technique</p>
<p>AIIDE-23: The 19th AAAI Conference on Artificial Intelligence and</p>
<p>Interactive Digital Entertainment, October 08-12, 2023, Salt Lake City, UT wzhou322@gatech.edu(W.Zhou); xpeng62@gatech.edu(X.Peng); riedl@cc.gatech.edu(M.R. ) of Story Shaping [1], which is able to guide the action module to imitate the optimal trajectory.</p>
<p>In this paper, we propose Dialogue Shaping, a framework that is able to extract useful information through conversation with NPCs, and then convert the information into knowledge graphs which are then used to speed up RL agent's convergence to optimal policy by using the Story Shaping technique [1].</p>
<p>Background and Related Work</p>
<p>Reinforcement Learning in Text Games Text games involve turn-based interactions where players read descriptions of the game's environment in natural language and respond with short text-based actions.These games can be described using partially-observable Markov Decision Processes, denoted as ⟨, , , , Ω, , ⟩, representing possible states, transition probabilities, vocabulary for commands, observation probabilities, reward function, and discount factor.The RL agent's goal is to learn a policy () →  to maximize expected future rewards.</p>
<p>Large Language Models in RL The power of Large Language Models (LLMs) has gained significant attention in recent years due to their advanced ability to adapt to numerous downstream tasks.ChatGPT, an LLM chatbot created by OpenAI, offers diverse interaction modes, and users can engage with it by providing prompts for acting as the NPC and the agent in text games [2].Recent studies also explored the integration of large language models into reinforcement learning frameworks to enhance the capabilities of agents.Contextual Action Language Model (CALM) [3] used LLM to generate a set of concise candidate actions at each step of the game for the reinforcement learning agent, thereby greatly reducing the</p>
<p>Preliminaries</p>
<p>Text Games</p>
<p>We create three text games in the LIGHT environment [4], which is a large-scale crowdsourced text adventure game framework, in which agents can both perceive, emote and act.The LIGHT environment also provides a database of rooms, characters, and objects, from which we can build our custom games.The visualization of one of the games we created and used in the experiments can be found in Figure 1.</p>
<p>Knowledge Graph</p>
<p>A knowledge graph consists of triples ⟨, , ⟩, capturing information about entities, their attributes, and relationships.Our method uses two types of KGs: internal KG and target KG.</p>
<p>During RL exploration in the text game, the internal KG represents the agent's current state, including the room it's in and the objects it possesses [5,6,7,8,9,10].We update this KG at each step based on changes in the game environment (e.g., moving to a new room) or the agent's actions (e.g., acquiring objects).</p>
<p>The target KG describes the final state the agent must achieve to win the game, specifying the last room the agent should be in and the required objects.This KG is generated before training and stays unchanged.</p>
<p>KGA2C agent</p>
<p>KGA2C [7] is used for our game-playing agent for both baseline and Story Shaping [1].It is an RL agent that combines both Advantage Actor Critic methods [11] and KG guidance to enhance its learning and decision-making capabilities.The input embedding to the KGA2C agent is a concatenation of encoding of the agent's current internal KG and four observation tensors, including the description of the current room the agent is located in, the agent's current inventory, feedback of the environment from the agent's last action, and agent's last action.</p>
<p>Story Shaping</p>
<p>Story Shaping, proposed by Peng et al., is a technique that helps the RL agent infers tacit knowledge on how to accomplish a task.For each training step in the game, Story Shaping gives the RL agent an extra reward signal (in addition to the game environment's reward signal) based on the similarity between agent's current internal KG and target KG, and therefore encourage the agent to perform actions that will make its internal KG similar to the target KG.The target KG in this paper is generated by prompting the ChatGPT agent and it represents a summary of the hints the ChatGPT agent learns through talking to the ChatGPT NPC.</p>
<p>Information Retrieval from Dialogue</p>
<p>In order to retrieve correct and important information about the game from NPC, it is expected to know the game setting and it should be able to provide the correct response every time it is asked by the agent.In other words, the NPC should act as a "database" of the game.</p>
<p>NPC Prompting</p>
<p>We open one session of ChatGPT, as shown in Figure 2, and prompted it to be the non-player character.The NPC is provided with general information about the game, including the layout and the available objects, as well as the hints to win the game.One example of hints is getting a sword in the Artillery room is a prerequisite to kill the dragon.</p>
<p>Agent Prompting</p>
<p>ChatGPT is prompted to be the player agent in the game.The ChatGPT agent is provided with its goal in the game (e.g.kill the dragon) and general instructions on how to converse with the NPC (e.g.ask questions based on previous given answers).We did not reveal any game details in the prompts for the ChatGPT agent, because it is expected to gain those information by asking questions to the ChatGPT NPC.</p>
<p>Story Shaping from Dialogue</p>
<p>After the dialogue with NPC, we train a KGA2C agent to play the game.In order to incorporate the information learned by the ChatGPT agent during conversation with NPC into KGA2C agent's training, we prompt ChatGPT agent to generate a knowledge graph and use it as a target knowledge graph for the Story Shaping KGA2C agent.</p>
<p>The pipeline for our method is shown in Figure 3.</p>
<p>Experiments</p>
<p>We conducted experiments to test our assumption that the dialogue is able to accelerate KGA2C agent's convergence by generating the target knowledge graph.</p>
<p>ChatGPT Prompting for Agent vs NPC Dialogue</p>
<p>Table 1 An example prompt used to trigger ChatGPT to act as NPC in game 1. Goal and prerequisites are different for each game.</p>
<p>You are an NPC in a text-adventure game.You and the agent are both in the game.For each step, waits for the agent to ask questions, then you should provide a correct answer based on the information about the game given as follow:
Layout: [room1]-east-[room2], [room2]-east-[room3], etc. (A-east-B means A is to the east of B)
Goal and prerequisite: A dragon is in the dungeon.</p>
<p>The only way to kill the dragon is to use a sword and there is no other way.</p>
<p>Object information:
[object1], [object2] is in [room1]. [object3], [object4], [object5] is in [room2]. [room3] has no objects...
We open one session of ChatGPT and prompt it to be the NPC of the game.Table 1 shows the NPC prompt</p>
<p>Table 2</p>
<p>An example prompt used to trigger ChatGPT to act as the agent in game 1. Goal is different for each game.</p>
<p>You are an agent in a text-adventure game.You and the NPC are both in the game.Your goal is to kill the dragon.For each step, you should ask questions to the NPC in order to get the information on how to kill the dragon.</p>
<p>Ask a new set of questions based on the current observation and answers given to the previous set of questions according to the following rule: 1. ask similar and follow-up questions to previous questions that have a "yes" answer.2. Avoid asking similar and follow-up questions to previous questions that have a "no" answer.</p>
<p>Table 3</p>
<p>Examples of an efficient dialogue and an inefficient dialogue between ChatGPT agent and ChatGPT NPC.</p>
<p>Desired Dialogue</p>
<p>Agent: Do I need an object to kill the dragon?Can the dragon be lured out?Can the dragon be defeated without using a weapon?NPC: Yes.No. No. Agent: What object I should get to kill the dragon?NPC: A sword.Agent: Where can I find the sword?NPC: The Artillery room.Dialogue ends.Enough information is gathered.</p>
<p>Undesired Dialogue</p>
<p>Agent: Do I need an object to kill the dragon?Can the dragon be lured out?Can the dragon be defeated without using a weapon?NPC: Yes.No. No. Agent: Can the dragon be trapped or immobilized temporarily?Is there any way to communicate with the dragon or understand its intentions?NPC: No. No. Agent: Is there any information about the dragon's origin or history?Can the dragon be reasoned with or persuaded to leave peacefully?NPC: No. No.</p>
<p>• • •</p>
<p>for one game.We also open another separate session of ChatGPT and prompt it to be the agent of the game with a goal in mind.Table 2 shows the agent prompt for one game.</p>
<p>Then, the dialogue begins as the agent comes up with a set of questions and the NPC provides answers to them back and forth.ChatGPT NPC proves to be a reliable game database, correctly responding to queries about room and object locations.Moreover, when the ChatGPT agent makes ungrounded assumptions about the game (like "Does the barkeeper possess any knowledge about dragon's weaknesses" while there is no barkeeper) in its questions, the ChatGPT NPC is able to recognize (finding</p>
<p>Table 4</p>
<p>The prompt used to trigger the ChatGPT agent to generate the target knowledge graph, which will later be used in the training of KGA2C agent with Story Shaping.</p>
<p>Output a textual knowledge graph that contains the game information required to reach the goal.Output it in the format of edges (entity1 −−direction or verbs→ entity2).For example, you−−have→rugs, town center −−west→ the bar out that the game does not have a barkeeper) and negate them.</p>
<p>In evaluating the performance of ChatGPT agent, we aim to minimize the number of exchanges with the Chat-GPT NPC while retrieving hints on winning the game.We found out that ChatGPT agent is much more likely to hallucinate by coming up with ungrounded questions without explicit instructions on how to ask the optimal questions in our prompt.As shown in the desired dialogue in Table 3, when we include those explicit instructions in the prompt, it is able to ground its inquiries.Otherwise, it will fail to follow up on the previous questions that have a "yes" answer and endlessly ask ungrounded questions as shown in the undesired dialogue in Table 3.</p>
<p>KGA2C Agent Training with Dialogue Shaping</p>
<p>After the dialogue ends and the ChatGPT agent retrieved information on how to reach the goal, we prompt it to convert that information into a textual knowledge graph representation as shown in Table 4.We then filter the edges in the knowledge graph by only including ones that have "you" as a subject, because we are only interested in what actions the agent has to perform to reach to goal.Finally, we use this filtered knowledge graph as the target knowledge graph to "shape" the Story Shaping KGA2C agent behaviors.We generate each game using the LIGHT framework [4].We design each game such that the RL agent will only get one reward signal of 15 when it wins the game.For every game, the KGA2C agent is trained for 100,000 steps.After every 450 steps, the agent is evaluated for 50 episodes with 10 random seeds.We gather metrics like average and standard deviation of the test scores achieved for those 50 episodes, like in Figure 4.The maximum step limit for a single episode is 75 steps, while the optimal path for all games usually takes around 10 steps.</p>
<p>We trained the baseline KGA2C agent and the one with Story Shaping assistance for each game.Baseline KGA2C agent only receives reward signals that are built into the game mechanism (i.e.reaching the final goal), whereas the Story Shaping KGA2C agent receives additional reward signals when its internal knowledge graph overlaps with the target knowledge graph which is generated by the dialogue module (i.e.complete the prerequisite of the goal).</p>
<p>Results</p>
<p>Figure 4 showed the average test score and its standard deviation of the baseline KGA2C agent and Story Shaping KGA2C agent equipped with target knowledge graph generated from the dialogue during training for game 1.The Story Shaping KGA2C agent outperformed the baseline in all games.In all games, the Story Shaping agent converged to the optimal policy (gaining maximum score of 15) much faster than the baseline.In game 1, the Story Shaping KGA2C agent converged to the optimal policy after trained for around 10000 steps despite a temporary drop in average scores around step 30000, while the baseline agent took around 90000 training steps to learn the optimal policy, according to figure 4.Moreover, almost at all the training steps, the standard deviation score range of the Story Shaping agent is disjoint from that of the baseline, meaning that the Story Shaping agent can consistently achieve higher score than the baseline.</p>
<p>Conclusions</p>
<p>Through evaluation of our technique across a range of text games, we have shown that the dialogue module is able to extract key game information which might take a traditional action based RL agent tens of thousands of steps to learn.Moreover, we show that the dialogue module is able to pass along those key information and guide the action agent through knowledge graph and Story Shaping technique effectively and reliably.Thus, we have proven the substantial potential of the dialogue component to greatly speed up RL agent's convergence to the optimal policy.Future work might further exploit this potential by exploring approaches like few-shot prompting or finetuning LLMs to more effectively retrieve useful information from the NPC.</p>
<p>Figure 1 :
1
Figure 1: LIGHT Game Map for Game 1</p>
<p>Figure 2 :
2
Figure 2: Screenshot of the starting conversation between the user and the ChatGPT NPC.The question asked is generated by the ChatGPT agent and copied by the user.</p>
<p>Figure 3 :
3
Figure 3: Dialogue Shaping pipeline containing ChatGPT NPC dialogue with ChatGPT agent and target knowledge graph extraction.This target knowledge graph is used by the KGA2C agent with Story Shaping assistance to get additional reward signals.</p>
<p>Figure 4 :
4
Figure 4: Average and standard deviation of the test scores throughout 100000 training steps for Game 1.The standard deviation is represented as the shaded area around the lines.</p>
<p>X Peng, C Cui, W Zhou, R Jia, M Riedl, arXiv:2301.10107Story shaping: Teaching agents human-like behavior with stories. 2023arXiv preprint</p>
<p>Chatgpt Openai, A large-scale open-domain chatbot. 2022</p>
<p>Keep calm and explore: Language models for action generation in text-based games. S Yao, R Rao, M Hausknecht, K Narasimhan, arXiv:2010.029032020</p>
<p>J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau, E Dinan, T Rocktäschel, D Kiela, A Szlam, J Weston, arXiv:1903.03094Learning to speak and act in a fantasy text adventure game. 2019arXiv preprint</p>
<p>Playing textadventure games with graph-based deep reinforcement learning. P Ammanabrolu, M O Riedl, arXiv:1812.016282018arXiv preprint</p>
<p>Bringing stories alive: Generating interactive fiction worlds. P Ammanabrolu, W Cheung, D Tu, W Broniec, M Riedl, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment202016</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>P Ammanabrolu, E Tien, M Hausknecht, M O Riedl, arXiv:2006.07409How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. 2020arXiv preprint</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Y Xu, M Fang, L Chen, Y Du, J T Zhou, C Zhang, Advances in Neural Information Processing Systems. 332020</p>
<p>Inherently explainable reinforcement learning in natural language. X Peng, M Riedl, P Ammanabrolu, Advances in Neural Information Processing Systems. 352022</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International conference on machine learning. PMLR2016</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3141 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3141</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3141</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-07cdf957a11506f87fbc030dcfaaa6399847648c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/07cdf957a11506f87fbc030dcfaaa6399847648c" target="_blank">Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs, and constructs the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks.</p>
                <p><strong>Paper Abstract:</strong> Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3141.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3141.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathOctopus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MathOctopus (multilingual math-reasoning LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of LLaMA-based models (7B/13B/33B) instruction-finetuned on a multilingual math reasoning corpus (MGSM8KInstruct) in two training regimes (parallel-training and cross-training) to improve multi-step arithmetic word-problem solving across ten languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MathOctopus (LLaMA-2 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-derived transformer models fine-tuned via supervised instruction tuning (SFT) on MGSM8KInstruct; available sizes reported: 7B, 13B, 33B. Training hyperparameters reported: lr=2e-5, epochs=3, max length=512; batch sizes vary by size (7B:8, 13B:4, 33B:2). Two strategies: parallel-training (Q and CoT A in same language) and cross-training (English Q, CoT A in other language).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (GSM8K-style), out-of-domain SVAMP-derived problems (MSVAMP); tasks require chained operations, multi-step addition/subtraction/multiplication/division and formula manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Models perform arithmetic via generating explicit chain-of-thought (CoT) token sequences that encode intermediate symbolic/numeric steps; multilingual SFT appears to improve generalization (i.e., transfer of reasoning patterns across languages); rejection-sampling (xRFT) aggregates multiple sampled CoT solutions to increase diversity of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains when training with CoT solutions: MathOctopus substantially outperforms vanilla LLaMA baselines on GSM8K/MGSM/MSVAMP; multilingual SFT improves monolingual English performance (e.g., LLaMA2-7B GSM8K 42.4% -> MathOctopus-7B 50.8% in one reported split); generated CoTs are validated by extracting and checking formulas during data construction; xRFT produces many distinct correct reasoning paths (Table 3) though with limited performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No mechanistic (interpretability/probing) analysis of internal representations is provided; improvements could reflect better mapping from linguistic input to stored arithmetic patterns rather than emergence of algorithmic arithmetic; xRFT's limited uplift and occasional degradation indicate that aggregating sampled CoTs does not always induce more robust algorithmic behavior and may overfit.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Multilingual supervised fine-tuning (MGSM8KInstruct), parallel-training vs cross-training regimes, multilingual rejection-sampling fine-tuning (xRFT), chain-of-thought answers used as targets, temperature/random-seed sampling to obtain diverse CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Multilingual SFT raised accuracies substantially across languages and improved English monolingual performance (examples: MathOctopus-7B improved GSM8K English from 42.4% to up to 50.8%; MathOctopus-13B reached 47.6% on MGSM beating ChatGPT 46.3% as reported). Parallel-training yielded better in-domain (parallel) performance; cross-training gave better out-of-domain robustness. xRFT provided only modest additional gains (~1-2% on MGSM in some settings) and sometimes harmed generalization when data volume increased.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative figures reported: MathOctopus-13B (parallel-training / best config) average MGSM accuracy reported ≈45.8–47.6% depending on setup; MathOctopus-7B improved GSM8K from 42.4% (LLaMA2-7B baseline) to 50.8% (cross-trained variant); xRFT-MathOctopus lifts on MGSM typically ~1–2% (varies by language/model size); full tables report per-language accuracies across MGSM and MSVAMP (see Table 4/5).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Persistent poor performance in low-resource languages for baseline models (improved but still variable for MathOctopus); arithmetic mistakes in generated CoTs remain (examples show LLaMA producing incorrect sums/multiplications while MathOctopus corrects many but not all); xRFT may cause overfitting and decreased out-of-domain generalization when too much sampled data is added; sensitivity to training/test alignment (parallel vs cross) can lead to mismatched performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison to human performance or exact symbolic calculators is provided; comparisons are to other LLMs (ChatGPT, GPT-4, LLaMA variants). MathOctopus sometimes surpasses ChatGPT on MGSM in few-shot settings per the reported numbers, but no formal comparison to exact symbolic arithmetic engines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3141.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3141.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 family (baseline models used in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLaMA-2 models at multiple scales (7B, 13B, 33B) used as backbones and baselines; typically perform worse on multilingual math tasks until instruction-finetuned.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (7B / 13B / 33B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer language models (LLaMA-2 family) used as base models; in experiments, baseline models are either LLaMA trained on English GSM8K or variants fine-tuned with RFT/other math instruction corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (GSM8K-style), evaluated in multilingual translations (MGSM) and SVAMP-based MSVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Behaves similarly to other LLMs by generating CoT steps when prompted; however, baseline LLaMA often lacks robust learned mappings for multi-step arithmetic across languages until SFT is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Baselines produce CoT-like tokens but make arithmetic mistakes across languages (many examples in generated outputs show incorrect intermediate arithmetic or incorrect aggregation of steps). Performance tables show lower accuracies (e.g., LLaMA2-7B MGSM averages ~22.6% on some settings; on GSM8K baseline 42.4% for LLaMA2-7B reported in some splits).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No internal probing; only behavior-level evidence: produced CoTs are often logically or numerically incorrect, indicating CoT generation alone is insufficient without SFT or other interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Used as base for SFT, RFT, and MathOctopus experiments; also evaluated with chain-of-thought examples and rejection-sampling based fine-tuning in baseline RFT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Fine-tuning (multilingual SFT) substantially improves LLaMA's arithmetic accuracy (e.g., MathOctopus variants built on LLaMA weights outperform vanilla LLaMA by large margins).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: LLaMA2-7B baseline MGSM average ~22.6% in some reported tables; LLaMA2-7B GSM8K reported 42.4% in cross-evaluation table; LLaMA2-13B and 33B show varying improvements but still inferior to MathOctopus when not multilingual SFTed.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Systematic numeric errors in CoTs (wrong multiplication/addition), sensitivity to language (much worse on low-resource languages), poor generalization across domains without SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Compared only to other LLMs; no comparison to exact algorithmic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3141.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3141.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rejection Sampling / RFT / xRFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection Sampling Fine-Tuning (RFT) and Multilingual Rejection Sampling Finetuning (xRFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data-augmentation technique that samples many model outputs (CoT solutions), filters correct ones, and uses unique-de-duplicated reasoning paths to augment fine-tuning data; xRFT extends RFT to the multilingual setting by sampling across languages and models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RFT / xRFT (method applied to LLaMA-derived models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RFT: augment SFT data with multiple sampled CoTs from a model, keeping only correct (validated) unique formula sequences; xRFT: perform this across multiple languages and models (here performed using MathOctopus 7B and 13B sampling across 10 languages, 25 inferences per language per model for total ~50 samples per question).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to produce diverse correct multi-step CoTs for training on GSM8K-derived multilingual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Intended mechanism: increasing the diversity of correct reasoning paths exposes the model to alternative solution strategies and reduces brittle dependence on single heuristics; in multilingual xRFT, different language renderings are treated as additional diverse paths.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>xRFT generated many distinct reasoning paths per question (Table 3); modest average performance uplifts observed in some in-domain MGSM settings (~1-2% for PT variants), and increases in number of distinct reasoning paths seen with higher sampling counts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Overall effect limited: xRFT often produces only marginal improvements and may reduce out-of-domain generalization and cause overfitting as augmented data volume increases; in some cases xRFT-deployed models performed worse than their non-xRFT counterparts on MGSM/MSVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Sampling (temperature 0.9, multiple seeds), filtering by final-answer correctness and formula-consistency, deduplication by formula identity, augmenting SFT training set with these paths.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Marginal improvements in some in-domain metrics; example: xRFT-MathOctopus-PT improves MGSM averages by ~1-2% in some settings; sometimes decreased out-of-domain performance (MSVAMP) as k (sample count) grows.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>xRFT uplift typically reported as ~1–2% absolute accuracy on MGSM; Table 4/5 show detailed per-language small gains or occasional drops; sampling produced between ~20–80 distinct correct CoTs for closed-source LLMs and fewer for open ones (values in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Diminishing returns and potential overfitting when too many sampled solutions are added; sampled reasoning paths from a single SFT model can be logically non-diverse, reducing the benefit; filtering relies on exact formula extraction which may miss subtle correct-but-different forms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; xRFT is compared to other data augmentation/training strategies rather than to symbolic calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3141.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3141.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (step-by-step intermediate reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method of eliciting intermediate reasoning steps from LLMs (CoT) so that models produce explicit arithmetic steps that can be checked or used as targets for supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought (applied in prompts and as training targets)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The approach uses example CoT answers from GSM8K as training targets and provides CoT exemplars at test time (e.g., EN-CoT 2-shot for ChatGPT/GPT-4 evaluations); used both as target outputs in MGSM8KInstruct and as few-shot prompts in closed-source baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems and chained numeric calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>CoT elicits explicit intermediate arithmetic steps, enabling the model to (1) expose the chain of symbolic operations it implicitly performs, (2) be trained to produce those steps (supervised), and (3) allow answer extraction and filtering via regex/formula-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Models trained on CoT targets (MathOctopus) show sizable performance improvements compared to models without such SFT; use of EN-CoT exemplars improves closed-source baseline performance in multilingual tasks per the experiments (ChatGPT En-2shot improves over zero-shot). The dataset construction validated formulas inside CoTs to ensure correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Generating CoTs does not guarantee correct arithmetic—many baseline CoTs include numeric errors; producing CoTs can also surface incorrect intermediate arithmetic rather than prevent it. No causal/representational probe is provided to show internal algorithmic arithmetic emerges from CoT training.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Providing CoT solutions as supervised targets (MGSM8KInstruct), few-shot CoT exemplars in evaluation (2-shot EN-CoT), 'Let's think step by step.' prompt used for zero-shot ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved accuracies when CoT used as training targets; few-shot CoT prompts enhance closed-source LLM performance in multilingual arithmetic tasks. Supervised CoT fine-tuning is a central component of the reported MathOctopus gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT-based SFT yields substantial absolute improvements (e.g., MathOctopus variants trained on CoT targets reach 40–57% on MGSM depending on model size and strategy; closed-source ChatGPT with EN-CoT 2-shot reports ~66.0% average on MGSM in reported table—note closed-source models are stronger baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>CoT outputs can contain arithmetic mistakes; reliance on CoT does not ensure algorithmic correctness; models can produce plausible but incorrect intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human-vs-model CoT comparison; CoT is compared experimentally to other prompting/training strategies and to closed-source LLM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3141.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3141.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGSM8KInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MGSM8KInstruct (multilingual math reasoning instruction dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilingual instruction dataset constructed by translating GSM8K training problems and CoT answers into ten languages (English plus 9 others) and aligning/validating formulas; used as supervised fine-tuning data to train MathOctopus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGSM8KInstruct (training dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approximately 73.6k instruction samples (GSM8K training translations into Bengali, Chinese, French, German, Japanese, Russian, Spanish, Swahili, Thai, plus English); includes parallel-training and cross-training formatting. Translation performed with ChatGPT then formula-checked and crowdsourced-verified (human agreement ~91.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems with chain-of-thought answers (GSM8K-style), intended to teach multi-step arithmetic reasoning across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Providing parallel and cross-lingual CoT targets helps the model learn mapping between language encodings and reasoning steps, leveraging cross-lingual shared reasoning patterns to strengthen arithmetic problem-solving.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Models trained on MGSM8KInstruct show improved multilingual and monolingual arithmetic performance (e.g., English GSM8K accuracy for MathOctopus-7B improved from 42.4% to 50.8% when multilingual SFT used); human verification and formula extraction ensured training labels encoded correct arithmetic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>While multilingual SFT helps, the paper notes limited additional benefit from xRFT beyond what multilingual SFT already provides, suggesting that simple translation-aligned CoTs may already capture much of the beneficial diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Creation and use of a multilingual CoT instruction dataset for supervised fine-tuning (used in parallel-training or cross-training formats).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Significant uplift in arithmetic accuracy across languages including improvements in English; improved low-resource language performance compared to monolingual training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dataset used to train models that achieve e.g., MathOctopus-7B MGSM averages in 40%+ range and per-language improvements shown in Tables 4/5; human agreement on translations ~91.2%; dataset size ~73.6k.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Translation errors can introduce incorrect formulas; quality control required (they drop items if formula checks fail); dataset may not fully cover linguistic/structural variety beyond the 10 languages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Dataset construction used humans for verification but provides no direct human vs model performance comparison on the dataset in absolute terms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3141.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3141.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT / GPT-4 baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo) and GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source LLM baselines evaluated on multilingual arithmetic tasks using zero-shot and few-shot CoT prompting (including 'Let's think step by step.' and 2-shot EN-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo), GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI models used as strong baselines: ChatGPT (tested zero-shot with 'Let's think step by step.' and with EN-CoT 2-shot) and GPT-4 (tested with EN-CoT 2-shot). Exact model sizes and training corpora not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (MGSM, MSVAMP), evaluated in zero-shot and few-shot CoT settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Elicitation of CoTs via prompting (prompt-engineering) to generate intermediate arithmetic steps; self-consistency and few-shot CoT exemplars are referenced as mechanisms that improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical results show ChatGPT and GPT-4 achieve substantially higher accuracy than open-source baselines on MGSM and MSVAMP in many settings (e.g., GPT-4-En 2-shot average ~73.0% on MGSM per Table 4), demonstrating efficacy of CoT few-shot prompting in closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Performance varies by language and by prompt (zero-shot vs EN-CoT). In some low-resource languages ChatGPT performs poorly (e.g., certain languages with near-zero scores in some tables), indicating prompt-based CoT does not uniformly solve cross-lingual arithmetic weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting interventions: zero-shot with 'Let's think step by step.' and EN-CoT 2-shot few-shot examples; evaluation performed with these prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>EN-CoT 2-shot typically increases accuracy versus zero-shot; GPT-4 with 2-shot achieves highest reported closed-source performance on average (e.g., ~73.0% average on MGSM in reported table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative: ChatGPT zero-shot MGSM average ~57.3% (per Table 4), ChatGPT En-2shot ~66.0% average, GPT-4 En-2shot ~73.0% average; per-language breakdowns reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Poor performance on certain low-resource languages (examples: Bengali extremely low in some evaluations), sensitivity to exemplar language (EN-CoT helps), and occasional incorrect numeric reasoning despite strong overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Benchmarked as state-of-the-art LLM performance baseline; no direct comparison to human or symbolic solver performance in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Scaling relationship on learning mathematical reasoning with large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Are NLP models really able to solve simple math word problems? <em>(Rating: 1)</em></li>
                <li>Learning math reasoning from self-sampled correct and partially-correct solutions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3141",
    "paper_id": "paper-07cdf957a11506f87fbc030dcfaaa6399847648c",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "MathOctopus",
            "name_full": "MathOctopus (multilingual math-reasoning LLMs)",
            "brief_description": "A family of LLaMA-based models (7B/13B/33B) instruction-finetuned on a multilingual math reasoning corpus (MGSM8KInstruct) in two training regimes (parallel-training and cross-training) to improve multi-step arithmetic word-problem solving across ten languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MathOctopus (LLaMA-2 backbone)",
            "model_description": "LLaMA-derived transformer models fine-tuned via supervised instruction tuning (SFT) on MGSM8KInstruct; available sizes reported: 7B, 13B, 33B. Training hyperparameters reported: lr=2e-5, epochs=3, max length=512; batch sizes vary by size (7B:8, 13B:4, 33B:2). Two strategies: parallel-training (Q and CoT A in same language) and cross-training (English Q, CoT A in other language).",
            "arithmetic_task_type": "Multi-step arithmetic word problems (GSM8K-style), out-of-domain SVAMP-derived problems (MSVAMP); tasks require chained operations, multi-step addition/subtraction/multiplication/division and formula manipulations.",
            "reported_mechanism": "Models perform arithmetic via generating explicit chain-of-thought (CoT) token sequences that encode intermediate symbolic/numeric steps; multilingual SFT appears to improve generalization (i.e., transfer of reasoning patterns across languages); rejection-sampling (xRFT) aggregates multiple sampled CoT solutions to increase diversity of training data.",
            "evidence_for_mechanism": "Empirical gains when training with CoT solutions: MathOctopus substantially outperforms vanilla LLaMA baselines on GSM8K/MGSM/MSVAMP; multilingual SFT improves monolingual English performance (e.g., LLaMA2-7B GSM8K 42.4% -&gt; MathOctopus-7B 50.8% in one reported split); generated CoTs are validated by extracting and checking formulas during data construction; xRFT produces many distinct correct reasoning paths (Table 3) though with limited performance gains.",
            "evidence_against_mechanism": "No mechanistic (interpretability/probing) analysis of internal representations is provided; improvements could reflect better mapping from linguistic input to stored arithmetic patterns rather than emergence of algorithmic arithmetic; xRFT's limited uplift and occasional degradation indicate that aggregating sampled CoTs does not always induce more robust algorithmic behavior and may overfit.",
            "intervention_type": "Multilingual supervised fine-tuning (MGSM8KInstruct), parallel-training vs cross-training regimes, multilingual rejection-sampling fine-tuning (xRFT), chain-of-thought answers used as targets, temperature/random-seed sampling to obtain diverse CoTs.",
            "effect_of_intervention": "Multilingual SFT raised accuracies substantially across languages and improved English monolingual performance (examples: MathOctopus-7B improved GSM8K English from 42.4% to up to 50.8%; MathOctopus-13B reached 47.6% on MGSM beating ChatGPT 46.3% as reported). Parallel-training yielded better in-domain (parallel) performance; cross-training gave better out-of-domain robustness. xRFT provided only modest additional gains (~1-2% on MGSM in some settings) and sometimes harmed generalization when data volume increased.",
            "performance_metrics": "Representative figures reported: MathOctopus-13B (parallel-training / best config) average MGSM accuracy reported ≈45.8–47.6% depending on setup; MathOctopus-7B improved GSM8K from 42.4% (LLaMA2-7B baseline) to 50.8% (cross-trained variant); xRFT-MathOctopus lifts on MGSM typically ~1–2% (varies by language/model size); full tables report per-language accuracies across MGSM and MSVAMP (see Table 4/5).",
            "notable_failure_modes": "Persistent poor performance in low-resource languages for baseline models (improved but still variable for MathOctopus); arithmetic mistakes in generated CoTs remain (examples show LLaMA producing incorrect sums/multiplications while MathOctopus corrects many but not all); xRFT may cause overfitting and decreased out-of-domain generalization when too much sampled data is added; sensitivity to training/test alignment (parallel vs cross) can lead to mismatched performance.",
            "comparison_to_humans_or_symbolic": "No direct comparison to human performance or exact symbolic calculators is provided; comparisons are to other LLMs (ChatGPT, GPT-4, LLaMA variants). MathOctopus sometimes surpasses ChatGPT on MGSM in few-shot settings per the reported numbers, but no formal comparison to exact symbolic arithmetic engines.",
            "uuid": "e3141.0",
            "source_info": {
                "paper_title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-2 (baseline)",
            "name_full": "LLaMA-2 family (baseline models used in comparisons)",
            "brief_description": "Open-source LLaMA-2 models at multiple scales (7B, 13B, 33B) used as backbones and baselines; typically perform worse on multilingual math tasks until instruction-finetuned.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (7B / 13B / 33B)",
            "model_description": "Transformer language models (LLaMA-2 family) used as base models; in experiments, baseline models are either LLaMA trained on English GSM8K or variants fine-tuned with RFT/other math instruction corpora.",
            "arithmetic_task_type": "Multi-step arithmetic word problems (GSM8K-style), evaluated in multilingual translations (MGSM) and SVAMP-based MSVAMP.",
            "reported_mechanism": "Behaves similarly to other LLMs by generating CoT steps when prompted; however, baseline LLaMA often lacks robust learned mappings for multi-step arithmetic across languages until SFT is applied.",
            "evidence_for_mechanism": "Baselines produce CoT-like tokens but make arithmetic mistakes across languages (many examples in generated outputs show incorrect intermediate arithmetic or incorrect aggregation of steps). Performance tables show lower accuracies (e.g., LLaMA2-7B MGSM averages ~22.6% on some settings; on GSM8K baseline 42.4% for LLaMA2-7B reported in some splits).",
            "evidence_against_mechanism": "No internal probing; only behavior-level evidence: produced CoTs are often logically or numerically incorrect, indicating CoT generation alone is insufficient without SFT or other interventions.",
            "intervention_type": "Used as base for SFT, RFT, and MathOctopus experiments; also evaluated with chain-of-thought examples and rejection-sampling based fine-tuning in baseline RFT variants.",
            "effect_of_intervention": "Fine-tuning (multilingual SFT) substantially improves LLaMA's arithmetic accuracy (e.g., MathOctopus variants built on LLaMA weights outperform vanilla LLaMA by large margins).",
            "performance_metrics": "Examples: LLaMA2-7B baseline MGSM average ~22.6% in some reported tables; LLaMA2-7B GSM8K reported 42.4% in cross-evaluation table; LLaMA2-13B and 33B show varying improvements but still inferior to MathOctopus when not multilingual SFTed.",
            "notable_failure_modes": "Systematic numeric errors in CoTs (wrong multiplication/addition), sensitivity to language (much worse on low-resource languages), poor generalization across domains without SFT.",
            "comparison_to_humans_or_symbolic": "Compared only to other LLMs; no comparison to exact algorithmic solvers.",
            "uuid": "e3141.1",
            "source_info": {
                "paper_title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Rejection Sampling / RFT / xRFT",
            "name_full": "Rejection Sampling Fine-Tuning (RFT) and Multilingual Rejection Sampling Finetuning (xRFT)",
            "brief_description": "Data-augmentation technique that samples many model outputs (CoT solutions), filters correct ones, and uses unique-de-duplicated reasoning paths to augment fine-tuning data; xRFT extends RFT to the multilingual setting by sampling across languages and models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RFT / xRFT (method applied to LLaMA-derived models)",
            "model_description": "RFT: augment SFT data with multiple sampled CoTs from a model, keeping only correct (validated) unique formula sequences; xRFT: perform this across multiple languages and models (here performed using MathOctopus 7B and 13B sampling across 10 languages, 25 inferences per language per model for total ~50 samples per question).",
            "arithmetic_task_type": "Used to produce diverse correct multi-step CoTs for training on GSM8K-derived multilingual tasks.",
            "reported_mechanism": "Intended mechanism: increasing the diversity of correct reasoning paths exposes the model to alternative solution strategies and reduces brittle dependence on single heuristics; in multilingual xRFT, different language renderings are treated as additional diverse paths.",
            "evidence_for_mechanism": "xRFT generated many distinct reasoning paths per question (Table 3); modest average performance uplifts observed in some in-domain MGSM settings (~1-2% for PT variants), and increases in number of distinct reasoning paths seen with higher sampling counts.",
            "evidence_against_mechanism": "Overall effect limited: xRFT often produces only marginal improvements and may reduce out-of-domain generalization and cause overfitting as augmented data volume increases; in some cases xRFT-deployed models performed worse than their non-xRFT counterparts on MGSM/MSVAMP.",
            "intervention_type": "Sampling (temperature 0.9, multiple seeds), filtering by final-answer correctness and formula-consistency, deduplication by formula identity, augmenting SFT training set with these paths.",
            "effect_of_intervention": "Marginal improvements in some in-domain metrics; example: xRFT-MathOctopus-PT improves MGSM averages by ~1-2% in some settings; sometimes decreased out-of-domain performance (MSVAMP) as k (sample count) grows.",
            "performance_metrics": "xRFT uplift typically reported as ~1–2% absolute accuracy on MGSM; Table 4/5 show detailed per-language small gains or occasional drops; sampling produced between ~20–80 distinct correct CoTs for closed-source LLMs and fewer for open ones (values in Table 3).",
            "notable_failure_modes": "Diminishing returns and potential overfitting when too many sampled solutions are added; sampled reasoning paths from a single SFT model can be logically non-diverse, reducing the benefit; filtering relies on exact formula extraction which may miss subtle correct-but-different forms.",
            "comparison_to_humans_or_symbolic": "No direct comparison; xRFT is compared to other data augmentation/training strategies rather than to symbolic calculators.",
            "uuid": "e3141.2",
            "source_info": {
                "paper_title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (step-by-step intermediate reasoning)",
            "brief_description": "Method of eliciting intermediate reasoning steps from LLMs (CoT) so that models produce explicit arithmetic steps that can be checked or used as targets for supervised fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought (applied in prompts and as training targets)",
            "model_description": "The approach uses example CoT answers from GSM8K as training targets and provides CoT exemplars at test time (e.g., EN-CoT 2-shot for ChatGPT/GPT-4 evaluations); used both as target outputs in MGSM8KInstruct and as few-shot prompts in closed-source baselines.",
            "arithmetic_task_type": "Multi-step arithmetic word problems and chained numeric calculations.",
            "reported_mechanism": "CoT elicits explicit intermediate arithmetic steps, enabling the model to (1) expose the chain of symbolic operations it implicitly performs, (2) be trained to produce those steps (supervised), and (3) allow answer extraction and filtering via regex/formula-checking.",
            "evidence_for_mechanism": "Models trained on CoT targets (MathOctopus) show sizable performance improvements compared to models without such SFT; use of EN-CoT exemplars improves closed-source baseline performance in multilingual tasks per the experiments (ChatGPT En-2shot improves over zero-shot). The dataset construction validated formulas inside CoTs to ensure correctness.",
            "evidence_against_mechanism": "Generating CoTs does not guarantee correct arithmetic—many baseline CoTs include numeric errors; producing CoTs can also surface incorrect intermediate arithmetic rather than prevent it. No causal/representational probe is provided to show internal algorithmic arithmetic emerges from CoT training.",
            "intervention_type": "Providing CoT solutions as supervised targets (MGSM8KInstruct), few-shot CoT exemplars in evaluation (2-shot EN-CoT), 'Let's think step by step.' prompt used for zero-shot ChatGPT.",
            "effect_of_intervention": "Improved accuracies when CoT used as training targets; few-shot CoT prompts enhance closed-source LLM performance in multilingual arithmetic tasks. Supervised CoT fine-tuning is a central component of the reported MathOctopus gains.",
            "performance_metrics": "CoT-based SFT yields substantial absolute improvements (e.g., MathOctopus variants trained on CoT targets reach 40–57% on MGSM depending on model size and strategy; closed-source ChatGPT with EN-CoT 2-shot reports ~66.0% average on MGSM in reported table—note closed-source models are stronger baselines).",
            "notable_failure_modes": "CoT outputs can contain arithmetic mistakes; reliance on CoT does not ensure algorithmic correctness; models can produce plausible but incorrect intermediate steps.",
            "comparison_to_humans_or_symbolic": "No direct human-vs-model CoT comparison; CoT is compared experimentally to other prompting/training strategies and to closed-source LLM performance.",
            "uuid": "e3141.3",
            "source_info": {
                "paper_title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MGSM8KInstruct",
            "name_full": "MGSM8KInstruct (multilingual math reasoning instruction dataset)",
            "brief_description": "A multilingual instruction dataset constructed by translating GSM8K training problems and CoT answers into ten languages (English plus 9 others) and aligning/validating formulas; used as supervised fine-tuning data to train MathOctopus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MGSM8KInstruct (training dataset)",
            "model_description": "Approximately 73.6k instruction samples (GSM8K training translations into Bengali, Chinese, French, German, Japanese, Russian, Spanish, Swahili, Thai, plus English); includes parallel-training and cross-training formatting. Translation performed with ChatGPT then formula-checked and crowdsourced-verified (human agreement ~91.2%).",
            "arithmetic_task_type": "Multi-step arithmetic word problems with chain-of-thought answers (GSM8K-style), intended to teach multi-step arithmetic reasoning across languages.",
            "reported_mechanism": "Providing parallel and cross-lingual CoT targets helps the model learn mapping between language encodings and reasoning steps, leveraging cross-lingual shared reasoning patterns to strengthen arithmetic problem-solving.",
            "evidence_for_mechanism": "Models trained on MGSM8KInstruct show improved multilingual and monolingual arithmetic performance (e.g., English GSM8K accuracy for MathOctopus-7B improved from 42.4% to 50.8% when multilingual SFT used); human verification and formula extraction ensured training labels encoded correct arithmetic steps.",
            "evidence_against_mechanism": "While multilingual SFT helps, the paper notes limited additional benefit from xRFT beyond what multilingual SFT already provides, suggesting that simple translation-aligned CoTs may already capture much of the beneficial diversity.",
            "intervention_type": "Creation and use of a multilingual CoT instruction dataset for supervised fine-tuning (used in parallel-training or cross-training formats).",
            "effect_of_intervention": "Significant uplift in arithmetic accuracy across languages including improvements in English; improved low-resource language performance compared to monolingual training.",
            "performance_metrics": "Dataset used to train models that achieve e.g., MathOctopus-7B MGSM averages in 40%+ range and per-language improvements shown in Tables 4/5; human agreement on translations ~91.2%; dataset size ~73.6k.",
            "notable_failure_modes": "Translation errors can introduce incorrect formulas; quality control required (they drop items if formula checks fail); dataset may not fully cover linguistic/structural variety beyond the 10 languages.",
            "comparison_to_humans_or_symbolic": "Dataset construction used humans for verification but provides no direct human vs model performance comparison on the dataset in absolute terms.",
            "uuid": "e3141.4",
            "source_info": {
                "paper_title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT / GPT-4 baselines",
            "name_full": "ChatGPT (gpt-3.5-turbo) and GPT-4 (OpenAI)",
            "brief_description": "Closed-source LLM baselines evaluated on multilingual arithmetic tasks using zero-shot and few-shot CoT prompting (including 'Let's think step by step.' and 2-shot EN-CoT).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo), GPT-4",
            "model_description": "Proprietary OpenAI models used as strong baselines: ChatGPT (tested zero-shot with 'Let's think step by step.' and with EN-CoT 2-shot) and GPT-4 (tested with EN-CoT 2-shot). Exact model sizes and training corpora not specified in the paper.",
            "arithmetic_task_type": "Multi-step arithmetic word problems (MGSM, MSVAMP), evaluated in zero-shot and few-shot CoT settings.",
            "reported_mechanism": "Elicitation of CoTs via prompting (prompt-engineering) to generate intermediate arithmetic steps; self-consistency and few-shot CoT exemplars are referenced as mechanisms that improve accuracy.",
            "evidence_for_mechanism": "Empirical results show ChatGPT and GPT-4 achieve substantially higher accuracy than open-source baselines on MGSM and MSVAMP in many settings (e.g., GPT-4-En 2-shot average ~73.0% on MGSM per Table 4), demonstrating efficacy of CoT few-shot prompting in closed-source models.",
            "evidence_against_mechanism": "Performance varies by language and by prompt (zero-shot vs EN-CoT). In some low-resource languages ChatGPT performs poorly (e.g., certain languages with near-zero scores in some tables), indicating prompt-based CoT does not uniformly solve cross-lingual arithmetic weaknesses.",
            "intervention_type": "Prompting interventions: zero-shot with 'Let's think step by step.' and EN-CoT 2-shot few-shot examples; evaluation performed with these prompting strategies.",
            "effect_of_intervention": "EN-CoT 2-shot typically increases accuracy versus zero-shot; GPT-4 with 2-shot achieves highest reported closed-source performance on average (e.g., ~73.0% average on MGSM in reported table).",
            "performance_metrics": "Representative: ChatGPT zero-shot MGSM average ~57.3% (per Table 4), ChatGPT En-2shot ~66.0% average, GPT-4 En-2shot ~73.0% average; per-language breakdowns reported in paper tables.",
            "notable_failure_modes": "Poor performance on certain low-resource languages (examples: Bengali extremely low in some evaluations), sensitivity to exemplar language (EN-CoT helps), and occasional incorrect numeric reasoning despite strong overall performance.",
            "comparison_to_humans_or_symbolic": "Benchmarked as state-of-the-art LLM performance baseline; no direct comparison to human or symbolic solver performance in the paper.",
            "uuid": "e3141.5",
            "source_info": {
                "paper_title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Scaling relationship on learning mathematical reasoning with large language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        },
        {
            "paper_title": "Are NLP models really able to solve simple math word problems?",
            "rating": 1
        },
        {
            "paper_title": "Learning math reasoning from self-sampled correct and partially-correct solutions",
            "rating": 2
        }
    ],
    "cost": 0.0180745,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</h1>
<p>Nuo Chen ${ }^{\text {a }}$ Zinan Zheng ${ }^{\text {a }}$ Ning Wu ${ }^{\text {a }}$<br>Ming Gong ${ }^{\text {a }}$ Dongmei Zhang ${ }^{\text {a }}$ Jia Li ${ }^{\text {a }}$<br>${ }^{1}$ Hong Kong University of Science and Technology (Guangzhou)<br>Hong Kong University of Science and Technology<br>${ }^{\text {a }}$ Microsoft<br>nchen022@connect.ust.hk, jialee@ust.hk</p>
<h4>Abstract</h4>
<p>Existing research predominantly focuses on developing powerful large language models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, which notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6\% accuracy which exceeds ChatGPT 46.3\% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually and elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from $42.4 \%$ to $50.8 \%$ on the GSM8K test set. Codes are available at https://github.com/ microsoft/MathOctopus.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) (Brown et al., 2020; Hu et al., 2021; Zeng et al., 2022; OpenAI,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Different LLMs performances in MSGM and MSVAMP datasets, which all are built on LLaMA 2-7B.</p>
<p>2023; Scao et al., 2022; You et al., 2022) such as Bloom (Scao et al., 2022) and GPT4 (OpenAI, 2023) have exhibited remarkable performances across a wide array of downstream tasks. Notwithstanding these significant advancements, even the most extensive LLMs are confronted with challenges when faced with mathematical reasoning tasks that necessitate multiple reasoning steps (Gao et al., 2023).</p>
<p>Many recent works focus on using different prompting methods like chain-of-thought (CoT) to solve mathematical problems based on closesourced LLMs such as ChatGPT and GPT-4. Significantly, LLaMA-Family models (Touvron et al., 2023a,c) have ignited an open-source movement and rapidly reduced the disparity with these closedsource LLMs. Following this line, Yuan et al. (2023a) apply rejection sampling fine-tuning (RFT) for math reasoning LLMs. WizardMath (Luo et al., 2023) advances mathematical reasoning in LLMs through Reinforcement Learning from EvolInstruct (RLEIF). However, current efforts are primarily focusing on improving the performance of LLMs in English. Although, Shi et al. (2022) propose MGSM testset to explore multilingual</p>
<p>math reasoning through in-context learning, training a powerful multilingual mathematical reasoning LLM remains under-explored.</p>
<p>To this end, this paper empirically investigates and enhances the multilingual mathematical reasoning abilities of current open-source LLMs such as LLaMA through instruction tuning. We aim to train a single model capable of correctly answering mathematical problems in multiple target languages, not just English. However, the main challenge in multilingual mathematical reasoning (xMR) is the scarcity of training data in low-resource languages. To tackle this challenge, we begin by using ChatGPT to translate the English GSM8K math training dataset into 9 various languages. Concurrently, we employ specific rules and human verification to calibrate and align the translated corpora, ensuring the quality of data.</p>
<p>The resulting data are used to construct our multilingual math instruction training dataset: MGSM8KInstruct, which encompasses instructional data within two distinct settings: Paralleltraining and Cross-training. The Parallel-training setting denotes that both the mathematical queries and the CoT answers derive from the same language. Conversely, the Cross-training setting indicates that the questions are in English, while the corresponding answers are in other languages. The objective of these settings is to develop LLMs are capable of solving mathematical problems coherently, whether presented in English or other target languages, while ensuring adaptability and maintaining rigorous mathematical reasoning across multiple languages. One step further, to conduct a more exhaustive and comprehensive evaluation of the model's multilingual mathematical capabilities, we additionally develop an out-of-domain xMR testset: MSVAMP, incorporating 10 languages, based on SVAMP (Patel et al., 2021).</p>
<p>We then use MGSM8KInstruct to supervise finetune (SFT) current open-source LLMs including LLaMA-Families ranging from 7B to 33B. Following the training, the models demonstrate exemplary abilities in xMR tasks. We name the resulting models as MathOctopus, attributing their adaptability and extensive proficiency in xMR tasks across a variety of languages, as shown in Figure 1. Surprisingly, compared with LLMs trained on the monolingual corpus, MathOctopus also shows superior performances when tested in their respective training languages. For instance, MathOctopus-7B elevate the accuracy of LLaMA2-7B on the En- glish GSM8K from $42.4 \%$ to $50.8 \%$. We posit that this improvement is attributable to the model's enhanced generalization capability acquired during multilingual training, as well as the reasoning knowledge learned from other languages feeding back into in English.</p>
<p>Subsequently, we delve into an exploration of the influences of SFT data volumes and diverse reasoning paths on the efficacy of the SFT model in multilingual context. Inspired by (Yuan et al., 2023a), we apply multilingual rejection sampling on MathOctopus to generate different correct reasoning paths as an augmented training corpus. By incorporating this data into our prior MGSM8KInstruct for fine-tuning LLMs, we observe limited enhanced multilingual mathematical reasoning outcomes. We term this training approach Multilingual Rejection Sampling Finetuning (xRFT). Experimentally, xRFT can further elevate the LLM's xMR outcomes, but it may potentially compromise the model's generalization ability as the data amount increases.</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>We construct MGSM8KInstruct, a first multilingual math reasoning instruction dataset. Subsequently, MSVAMP, an out-of-domain multilingual mathematical reasoning dataset, is collected, serving as a robust test bed to further assess the robustness of LLMs.</li>
<li>Based on collected data and different training strategies, we build a series of powerful LLMs, called MathOctopus in xMR tasks. Our model not only significantly improves its reasoning capabilities in low-resource languages compared to LLaMA but also greatly enhances its performance in English.</li>
<li>We explore the relationship between model performance and data volume, as well as the impact of different training strategies. One of the most surprising observation is that multilingual SFT could be regarded as a crucial strategy for enhancing mathematical reasoning proficiency in LLMs.</li>
</ul>
<h2>2 Methodology</h2>
<p>In this section, we aim to illustrate our method in detail. We first review the problem formulation of multilingual mathematical reasoning. Then we describe the collection process of MGSM8KInstruct. Subsequently, we present training strategies of our</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Multilingual Math Instruction Tuning of MathOctopus.</p>
<p>MathOctopus and multilingual rejection sampling methods, sequentially.</p>
<h3>2.1 Problem Formulation</h3>
<p>Commonly, the mathematical reasoning problemsolving task can be defined as $$ \mathcal{D} = { Q_i, O_i, A_i } $$, where $$ Q_i $$ is the target math question, $$ O = { O_1, O_2, \ldots, O_k } $$ are answer options if $$ Q_i $$ is a K-way multiple choice problem, $$ A_i $$ is the corresponding ground-truth answer. Given $$ Q_i $$ as inputs, LLMs can directly output answers or a sequence of tokens as intermediate reasoning steps $$ R_i $$ via CoT. Then we can obtain the answer in $$ R_i $$ through regular expression matching.</p>
<p>In this work, we extend mathematical reasoning tasks from monolingual to multilingual contexts: $$ \mathcal{D}<em en="en">{en} $$ to $$ \mathcal{D}</em>} ... \mathcal{D<em i_zh_es_..._="i(zh,es,...)">{zh} $$. We aspire to enable only one model to successfully solve mathematical problems presented in various languages. That is, given $$ Q</em> $$ in the specific language, even when the problems are solely presented in English.} $$ in a target language like Chinese, Spanish and etc, the model can furnish correct CoT responses $$ R_{i(zh,es,...) </p>
<h3>2.2 MGSM8KInstruct</h3>
<p><strong>Source Data</strong> Prior to going further, the main concern in xMR is data scarcity of the multilingual training corpus. We employ GSM8K (Cobbe et al., 2021), an English dataset comprised of middle grade-school mathematical problems annotated by humans, as our fundamental data source. According to the officially provided solutions, each problem in GSM8K necessitates a resolution process involving between two to eight steps.</p>
<p><strong>Target Languages</strong> As for target languages in translation, we choose a set of ten languages that are typologically varied from English (En), covering different language families. Similar with (Shi et al., 2022), the ensemble of languages incorporated in this study comprises Bengali (Bn), Chinese (Zh), French (Fr), German (De), Japanese (Ja), Russian (Ru), Spanish (Es), Swahili (Sw), and Thai (Th). This diverse conglomerate facilitates an exhaustive exploration into the model's adaptability and proficiency amidst divergent linguistic architectures and typologies.</p>
<p><strong>Translation Process</strong> Given the reliability and applicability of ChatGPT for translation tasks, we utilize ChatGPT to translate 7473 problems and CoT responses from the English GSM8K training set and their corresponding CoT answers into the target languages. To ensure the quality and consistency of the translations, we adopt the following strategies within the translation prompts:</p>
<ol>
<li>Maintain consistent translations for names of people and places within the sentences.</li>
<li>Preserve the mathematical formulas during translation.</li>
<li>All numbers must be represented using Arabic numerals to facilitate cross-lingual prediction.</li>
<li>To ensure more accurate translations, we provide two translation examples in the prompts for each language.</li>
</ol>
<p><strong>Verify Strategy</strong> Upon inspection of our randomly sampled translations, we find that ChatGPT generally maintains semantic accuracy in translations; however, discrepancies in formula translations may arise. Thus, to uphold consistency and accuracy across multiple languages, we additionally extract all formulas present in the translated answers. If all formulas are calculated correctly and are consistent with those in English, we deem the translation to be accurate and error-free. Note that if errors persist across 5 consecutive translations, we discard that particular case.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>En</th>
<th>Sw</th>
<th>Zh</th>
<th>Bn</th>
<th>De</th>
<th>Es</th>
<th>Fr</th>
<th>Ja</th>
<th>Ru</th>
<th>Th</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>MGSM8KInstruct</td>
<td>7473</td>
<td>7472</td>
<td>7466</td>
<td>6539</td>
<td>7466</td>
<td>7470</td>
<td>7469</td>
<td>7471</td>
<td>7361</td>
<td>7473</td>
<td>73.6k</td>
</tr>
<tr>
<td>MSVAMP</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>1000</td>
<td>10k</td>
</tr>
</tbody>
</table>
<p>Table 1: Data statistics of our MGSM8KInstruct and MSVAMP.</p>
<table>
<thead>
<tr>
<th>Lang.</th>
<th>Sw</th>
<th>Zh</th>
<th>Bn</th>
<th>De</th>
<th>Es</th>
<th>Fr</th>
<th>Ja</th>
<th>Ru</th>
<th>Th</th>
</tr>
</thead>
<tbody>
<tr>
<td>Agree.</td>
<td>88.2</td>
<td>90.2</td>
<td>90.6</td>
<td>90.9</td>
<td>94.9</td>
<td>94.7</td>
<td>91.3</td>
<td>90.4</td>
<td>90.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Human agreement rate of each language.</p>
<table>
<thead>
<tr>
<th>Times</th>
<th>En</th>
<th>Sw</th>
<th>Zh</th>
<th>Bn</th>
<th>De</th>
<th>Es</th>
<th>Fr</th>
<th>Ja</th>
<th>Ru</th>
<th>Th</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>1.5</td>
<td>2.1</td>
<td>1.6</td>
<td>1.2</td>
<td>1.6</td>
<td>1.4</td>
<td>1.2</td>
<td>0.8</td>
<td>1.3</td>
<td>1.4</td>
</tr>
<tr>
<td>30</td>
<td>2.5</td>
<td>3.5</td>
<td>2.7</td>
<td>1.9</td>
<td>2.6</td>
<td>2.3</td>
<td>2.0</td>
<td>1.3</td>
<td>2.2</td>
<td>2.3</td>
</tr>
<tr>
<td>50</td>
<td>3.8</td>
<td>5.2</td>
<td>4.0</td>
<td>2.9</td>
<td>3.9</td>
<td>3.5</td>
<td>3.0</td>
<td>1.9</td>
<td>3.3</td>
<td>3.4</td>
</tr>
</tbody>
</table>
<p>Table 3: Distinct reasoning paths of each language with different sampling times.</p>
<p>This approach ensures a coherent and accurate translation process, allowing for comprehensive evaluation and application in xMR tasks while maintaining linguistic and mathematical integrity. Upon acquiring the translated data, by pairing it with the alpaca-format prompts, we are able to formulate our final training dataset MGSM8KInstruct, with about 73.6 k samples, its statistics is shown in Table 1. Translation and training prompts are displayed in the Appendix, Table 9 and 11.</p>
<p>Crowdsourcing Verification Although, it is common to employ machine translation to generate multilingual datasets, as seen in widely-used datasets like Xtreme <em>Hu et al. (2020)</em>. To further verify the translation quality in our datasets, we conduct a rigorous quality check process. We randomly sample 500 samples from each language and evaluate them for consistency (Microsoft UHRS Platform) by native speakers. The overall $91.2 \%$ agreement rate in Table 2 indicates reliable translation quality.</p>
<h3>2.3 MathOctopus</h3>
<p>Training Strategies We then use multilingual query-response pairs in MGSM8KInstruct to supervise fine-tune LLMs, resulting in MathOctopus, as shown in Figure 2. Let us delve into a detailed exposition of our diverse training strategies:</p>
<ul>
<li>Parallel-training, involves filling in the input prompts with questions and answers in the same native language during training. This strategy is akin to teaching the model to communicate clearly in one language at a time. It helps the model get better at answering questions accurately within the same language, making it more reliable and effective.</li>
<li>Cross-training, refers to our approach during training where we insert English questions and answers in one native language into the input prompts. This approach is like mixing languages in teaching, using English questions and native language answers. It helps the model understand and connect different languages better, making it more versatile and capable of handling multilingual scenarios.</li>
</ul>
<p>Multilingual Rejection Sampling Prior work <em>Yuan et al. (2023a)</em> has demonstrated that LLM’s performance can be further enhanced by augmenting data through rejection sampling (RFT). Consequently, in this paper, we explore whether the gains imparted by RFT persist in multilingual scenarios. After obtaining the preliminary SFT model, we perform multiple inferences with the SFT model in the MGSM8KInstruct dataset, sampling more diverse and accurate reasoning paths from different languages to integrate into the original dataset. More specifically, we first eliminate samples with incorrect final answers. Subsequently, we extract all the formulas in each reasoning path and validate their accuracy; if all are correct, we consider that reasoning path as correct. We then follow the strategies from <em>Yuan et al. (2023a)</em> to acquire different correct reasoning paths: a reasoning path is only collected as augmented data if no previously collected path contains identical formulas.</p>
<p>However, the reasoning paths sampled from a single SFT model can be logically non-diverse. Consequently, we anticipate further enhancing the mathematical reasoning performance by leveraging reasoning paths aggregated from different models via multilingual rejection sampling. Considering the cost of prolonged inference, we currently perform 25 inferences for each language from the basic MathOctopus 7B and 13B models respectively, meaning we sample answers for each question in every language 50 times. In our experiments, we fuse all the different reasoning paths generated by the two models to obtain our final xRFT augmented data. We set the temperature to 0.9 and with different seeds to expect the model to generate diverse solutions. The Table 3 displays the number of different reasoning paths per question produced in</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>En</th>
<th>De</th>
<th>Es</th>
<th>Fr</th>
<th>Sw</th>
<th>Zh</th>
<th>Bn</th>
<th>Ja</th>
<th>Ru</th>
<th>Th</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Close-Source LLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ChatGPT-Zero shot</td>
<td>52.0</td>
<td>46.8</td>
<td>52.0</td>
<td>45.6</td>
<td>30.0</td>
<td>44.4</td>
<td>4.4</td>
<td>38.8</td>
<td>37.2</td>
<td>8.0</td>
<td>35.9</td>
</tr>
<tr>
<td>ChatGPT-En 2shot</td>
<td>67.2</td>
<td>62.0</td>
<td>61.2</td>
<td>59.2</td>
<td>40.0</td>
<td>52.8</td>
<td>7.6</td>
<td>46.8</td>
<td>50.4</td>
<td>15.6</td>
<td>46.3</td>
</tr>
<tr>
<td>GPT4-En 2shot</td>
<td>80.0</td>
<td>73.6</td>
<td>71.2</td>
<td>72.0</td>
<td>64.4</td>
<td>70.0</td>
<td>17.6</td>
<td>71.6</td>
<td>64.0</td>
<td>40.4</td>
<td>62.5</td>
</tr>
<tr>
<td>Open-Source LLMs (7B Model)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA 2</td>
<td>43.2</td>
<td>37.2</td>
<td>32.4</td>
<td>34.4</td>
<td>5.2</td>
<td>22.4</td>
<td>3.2</td>
<td>15.2</td>
<td>28.0</td>
<td>4.8</td>
<td>22.6</td>
</tr>
<tr>
<td>RFT</td>
<td>44.8</td>
<td>33.6</td>
<td>34.0</td>
<td>34.0</td>
<td>2.8</td>
<td>16.8</td>
<td>2.4</td>
<td>6.8</td>
<td>29.2</td>
<td>2.0</td>
<td>20.6</td>
</tr>
<tr>
<td>MAmmoth</td>
<td>49.6</td>
<td>33.2</td>
<td>32.4</td>
<td>32.8</td>
<td>2.4</td>
<td>17.2</td>
<td>3.6</td>
<td>10.8</td>
<td>26.0</td>
<td>4.8</td>
<td>21.3</td>
</tr>
<tr>
<td>WizardMath</td>
<td>47.6</td>
<td>30.4</td>
<td>34.8</td>
<td>30.4</td>
<td>3.4</td>
<td>22.4</td>
<td>2.0</td>
<td>24.0</td>
<td>30.8</td>
<td>4.0</td>
<td>23.0</td>
</tr>
<tr>
<td>MathOctopus ${ }^{\mathrm{C}}$</td>
<td>52.0</td>
<td>38.0</td>
<td>39.2</td>
<td>36.4</td>
<td>23.6</td>
<td>31.6</td>
<td>18.8</td>
<td>27.2</td>
<td>33.6</td>
<td>21.6</td>
<td>32.2</td>
</tr>
<tr>
<td>xRFT-MathOctopus ${ }^{\mathrm{C}}$</td>
<td>51.2</td>
<td>36.0</td>
<td>41.2</td>
<td>37.6</td>
<td>24.0</td>
<td>33.2</td>
<td>18.8</td>
<td>29.6</td>
<td>36.4</td>
<td>25.2</td>
<td>33.3</td>
</tr>
<tr>
<td>MathOctopus ${ }^{\mathrm{PT}}$</td>
<td>52.4</td>
<td>44.8</td>
<td>42.4</td>
<td>43.6</td>
<td>39.2</td>
<td>38.4</td>
<td>28.8</td>
<td>36.0</td>
<td>39.6</td>
<td>34.4</td>
<td>40.0</td>
</tr>
<tr>
<td>xRFT-MathOctopus ${ }^{\mathrm{PT}}$</td>
<td>54.8</td>
<td>43.6</td>
<td>45.2</td>
<td>38.0</td>
<td>38.4</td>
<td>45.2</td>
<td>33.2</td>
<td>35.6</td>
<td>48.4</td>
<td>36.4</td>
<td>41.9</td>
</tr>
<tr>
<td>Open-Source LLMs (13B Model)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA 2</td>
<td>50.4</td>
<td>42.8</td>
<td>45.2</td>
<td>40.8</td>
<td>7.6</td>
<td>32.8</td>
<td>6.0</td>
<td>25.2</td>
<td>39.2</td>
<td>6.8</td>
<td>29.7</td>
</tr>
<tr>
<td>RFT</td>
<td>52.0</td>
<td>38.4</td>
<td>46.8</td>
<td>44.8</td>
<td>3.6</td>
<td>33.6</td>
<td>3.2</td>
<td>26.4</td>
<td>41.6</td>
<td>4.4</td>
<td>29.5</td>
</tr>
<tr>
<td>MAmmoth</td>
<td>56.4</td>
<td>45.6</td>
<td>50.0</td>
<td>39.6</td>
<td>1.6</td>
<td>31.2</td>
<td>3.6</td>
<td>19.2</td>
<td>36.8</td>
<td>5.2</td>
<td>28.9</td>
</tr>
<tr>
<td>WizardMATH</td>
<td>52.8</td>
<td>40.4</td>
<td>45.6</td>
<td>42.0</td>
<td>5.6</td>
<td>28.0</td>
<td>6.4</td>
<td>22.0</td>
<td>34.4</td>
<td>5.6</td>
<td>28.3</td>
</tr>
<tr>
<td>MathOctopus ${ }^{\mathrm{C}}$</td>
<td>56.4</td>
<td>47.6</td>
<td>49.6</td>
<td>47.6</td>
<td>27.2</td>
<td>39.2</td>
<td>24.0</td>
<td>40.4</td>
<td>42.0</td>
<td>24.8</td>
<td>39.9</td>
</tr>
<tr>
<td>xRFT-MathOctopus ${ }^{\mathrm{C}}$</td>
<td>53.6</td>
<td>48.0</td>
<td>46.4</td>
<td>46.0</td>
<td>28.0</td>
<td>45.2</td>
<td>21.2</td>
<td>35.2</td>
<td>45.6</td>
<td>28.8</td>
<td>39.8</td>
</tr>
<tr>
<td>MathOctopus ${ }^{\mathrm{PT}}$</td>
<td>53.2</td>
<td>44.4</td>
<td>48.0</td>
<td>48.4</td>
<td>42.8</td>
<td>48.8</td>
<td>35.2</td>
<td>43.2</td>
<td>47.6</td>
<td>46.8</td>
<td>45.8</td>
</tr>
<tr>
<td>xRFT-MathOctopus ${ }^{\mathrm{PT}}$</td>
<td>51.6</td>
<td>49.2</td>
<td>53.2</td>
<td>49.6</td>
<td>46.0</td>
<td>51.2</td>
<td>42.0</td>
<td>39.6</td>
<td>47.6</td>
<td>46.0</td>
<td>47.6</td>
</tr>
<tr>
<td>Open-Source LLMs (30-34B Model)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA 1</td>
<td>50.8</td>
<td>42.4</td>
<td>44.4</td>
<td>42.4</td>
<td>3.6</td>
<td>27.6</td>
<td>3.2</td>
<td>11.6</td>
<td>38.4</td>
<td>1.2</td>
<td>26.6</td>
</tr>
<tr>
<td>RFT</td>
<td>57.6</td>
<td>45.6</td>
<td>46.4</td>
<td>44.8</td>
<td>2.4</td>
<td>26.0</td>
<td>4.8</td>
<td>9.2</td>
<td>46.4</td>
<td>4.4</td>
<td>28.8</td>
</tr>
<tr>
<td>MathOctopus ${ }^{\mathrm{C}}$</td>
<td>55.6</td>
<td>40.4</td>
<td>51.2</td>
<td>44.4</td>
<td>24.4</td>
<td>36.0</td>
<td>19.2</td>
<td>27.2</td>
<td>37.2</td>
<td>21.6</td>
<td>35.7</td>
</tr>
<tr>
<td>xRFT-MathOctopus ${ }^{\mathrm{C}}$</td>
<td>53.6</td>
<td>47.2</td>
<td>47.6</td>
<td>44.8</td>
<td>27.6</td>
<td>34.4</td>
<td>19.2</td>
<td>30.8</td>
<td>38.8</td>
<td>22.8</td>
<td>36.7</td>
</tr>
<tr>
<td>MathOctopus ${ }^{\mathrm{PT}}$</td>
<td>56.4</td>
<td>47.2</td>
<td>53.2</td>
<td>48.0</td>
<td>46.8</td>
<td>52.0</td>
<td>35.2</td>
<td>39.2</td>
<td>45.6</td>
<td>41.2</td>
<td>46.5</td>
</tr>
<tr>
<td>xRFT-MathOctopus ${ }^{\mathrm{PT}}$</td>
<td>51.6</td>
<td>51.2</td>
<td>52.8</td>
<td>44.4</td>
<td>47.2</td>
<td>52.4</td>
<td>37.6</td>
<td>41.6</td>
<td>50.0</td>
<td>47.6</td>
<td>47.6</td>
</tr>
</tbody>
</table>
<p>Table 4: Model Performances on MGSM testset. MathOctopus ${ }^{\mathrm{PT}}$ and MathOctopus ${ }^{\mathrm{C}}$ refer to models trained on parallel-training and cross-training, separately. We highlight the best results in each language of the same backbone.
each language over 50 samples.</p>
<h2>3 Experiments</h2>
<p>In this section, we first review our in-domain evaluation dataset: MGSM, and the collection of the out-of-domain testset: MSVAMP. Subsequently, we present the main results and findings of our experiments. We illustrate our experimental setup for training and testing in Appendix B.</p>
<h3>3.1 Evaluation Datasets</h3>
<p>MGSM The source data of MGSM (Shi et al., 2022) is collected from a subset from GSM8K testset, and then native annotators translate the subset in English into other 10 languages. Each language branch consists of 250 test samples.</p>
<p>MSVAMP Following (Yue et al., 2024), we choose SVAMP as our out-of-domain source data. Given that the answers in the SVAMP only contain
the numerical results, we focus solely on translating the questions. To ensure high-quality translations, we use Google Translate System to convert 1,000 samples from the SVAMP into ten languages, matching the same languages in our training set. We further verify translation fidelity through crowdsourced native speaker reviews in Appendix C.</p>
<h3>3.2 Baselines</h3>
<p>Close-Source LLMs In this paper, We consider two OpenAI's LLMs: GPT-4 and ChatGPT (gpt3.5-turbo) for comparison: (1) We test ChatGPT with zero-shot prompting where none exemplars are given, but we add "Let's think step by step." at the end of the inputs. (2) As Shi et al. (2022) proves including EN-CoT examples could result in better performances in xMR tasks, we additionally test them with 2 shot EN-coT examples, which are shown in Appendix Table 12.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">En</th>
<th style="text-align: center;">De</th>
<th style="text-align: center;">Es</th>
<th style="text-align: center;">Fr</th>
<th style="text-align: center;">Sw</th>
<th style="text-align: center;">Zh</th>
<th style="text-align: center;">Bn</th>
<th style="text-align: center;">Ja</th>
<th style="text-align: center;">Ru</th>
<th style="text-align: center;">Th</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Close-Source LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-Zero shot</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">57.3</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-En 2 shot</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT4-En 2shot</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source LLMs (7B ModeLs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: center;">RFT</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">31.5</td>
</tr>
<tr>
<td style="text-align: center;">MAmmoTH</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">27.0</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">32.5</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus ${ }^{\mathrm{C}}$</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: center;">xRFT-MathOctopus ${ }^{\mathrm{C}}$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus ${ }^{\text {P }}$</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">41.7</td>
</tr>
<tr>
<td style="text-align: center;">xRFT-MathOctopus ${ }^{\text {P }}$</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">42.4</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source LLMs (13B Models)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: center;">RFT</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">37.1</td>
</tr>
<tr>
<td style="text-align: center;">MAmmoth</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: center;">WizardMATH</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus ${ }^{\mathrm{C}}$</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">47.1</td>
</tr>
<tr>
<td style="text-align: center;">xRFT-MathOctopus ${ }^{\mathrm{C}}$</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus ${ }^{\text {P }}$</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;">xRFT-MathOctopus ${ }^{\text {P }}$</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source LLMs (30-34B Models)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">31.2</td>
</tr>
<tr>
<td style="text-align: center;">RFT</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus ${ }^{\mathrm{C}}$</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">xRFT-MathOctopus ${ }^{\mathrm{C}}$</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">42.9</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus ${ }^{\text {P }}$</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: center;">xRFT-MathOctopus ${ }^{\text {P }}$</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">45.1</td>
</tr>
</tbody>
</table>
<p>Table 5: Model Performances on MSVAMP testset. MathOctopus ${ }^{\mathrm{P}}$ and MathOctopus ${ }^{\mathrm{C}}$ refer to models trained on parallel-training and cross-training, separately.</p>
<p>Open-Source LLMs For fairness, we primarily compare MathOctopus with several LLaMA-based models, including LLaMA base, RFT, MAmmoTH and WizardMath. In this work, LLaMA base denotes models trained on GSM8K English corpus; RFT utilizes rejection sampling on English instruction tuning; MAmmoTH (Yue et al., 2024) is trained based on a variety of math instruction datasets; WizardMath (Luo et al., 2023) is built on Reinforcement Learning from Evol-Instruct (RLEIF) in math reasoning. As we only consider CoT solutions in this work, we don't use the code version of these models.</p>
<h3>3.3 Main Results</h3>
<p>Table 4 and Table 5 show the in-domain and out-ofdomain test results of different LLMs. We run the evaluation three times and report average results. We can draw the key observations and insights:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Cross-Test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Models</td>
<td style="text-align: center;">MGSM8K</td>
<td style="text-align: center;">MSVAMP</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{P}}-7 \mathrm{~B}$</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{C}}-7 \mathrm{~B}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{P}}-13 \mathrm{~B}$</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{C}}-13 \mathrm{~B}$</td>
<td style="text-align: center;">$\mathbf{5 4 . 2}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{P}}-33 \mathrm{~B}$</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">46.6</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{C}}-33 \mathrm{~B}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Average performances of MathOctopus ${ }^{\mathrm{C}}$ and MathOctopus ${ }^{\mathrm{P}}$ under the Cross-Test settings.</p>
<p>LLMs struggle in xMR tasks, especially in lowresource languages. From the tables, current open-source LLMs still suffer in xMR in terms of in-domain and out-of-domain testing. For instance, LLMs with 7B-level only achieve about 20\%-23\% accuracy in MGSM. Another conspicuous observation is the stark performance contrast of existing models when dealing with high-resource versus low-resource languages. This discrepancy can be</p>
<p>largely attributed to the diminutive representation of low-resource languages in their foundational corpus. In contrast, our model adeptly rectifies this shortcoming, as evidenced by its enhanced performance in languages like Thai and Swahili.</p>
<p>Performance Superiority of MathOctopus. The proposed MathOctopus, whether in cross-training or parallel-training, both significantly outperforms other open-source LLMs by a large margin. For instance, when operating at the 7B-level, our model can boost the LLaMA model's efficacy from a mere $22.6 \%$ to a commendable $41.9 \%$ on MGSM. Furthermore, this superiority still remains as the model size escalates. Surprisingly, MathOctopus ${ }^{\mathrm{PT}}$-13B even surpasses ChatGPT on MGSM. Another interesting finding is that MathOctopus ${ }^{\mathrm{PT}}$ performs better in the indomain test while MathOctopus ${ }^{\mathrm{C}}$ shows better generalization ability in the out-of-domain test, proving their unique advantages, separately. Delving deeper, we observe that cross-training setups could generally benefits the performances in the languages that are similar with English, like Spanish.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">SVAMP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA 2-7B</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{PT}}$-7B</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">46.8</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{C}}$-7B</td>
<td style="text-align: center;">$\mathbf{5 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2-13B</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{PT}}$-13B</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">52.1</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{C}}$-13B</td>
<td style="text-align: center;">$\mathbf{5 6 . 6}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 1-33B</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{PT}}$-33B</td>
<td style="text-align: center;">$\mathbf{5 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">MathOctopus ${ }^{\mathrm{C}}$-33B</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">51.5</td>
</tr>
</tbody>
</table>
<p>Table 7: Model performances on English datasets.
Marginal Impact of xRFT. The xRFT's contribution to model enhancement appears to be somewhat limited. Its effectiveness diminishes, particularly in out-of-domain test scenarios. In tests within MGSM, the xRFT's contribution to MathOctopus ${ }^{\mathrm{PT}}$ hovers around a modest $1 \%-2 \%$ average uplift. However, this figure dips below $1 \%$ in MSVAMP testset. More intriguingly, the augmentative effect of xRFT on MathOctopus ${ }^{\mathrm{C}}$ appears even more subdued, with potential performance deterioration observable on the MGSM.</p>
<h3>3.3.1 Training-Testing Consistencies</h3>
<p>As shown in the tables, there is a clear performance difference between models using
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performances of 7B-models on three language subsets from MGSM.</p>
<p>MathOctopus ${ }^{\mathrm{C}}$ vs. MathOctopus ${ }^{\mathrm{PT}}$, even when trained with the same amount of data in in-domain testing. A deeper look into this phenomenon revealed a mismatch between the training and testing environments. Specifically, in the MGSM and MSVAMP test sets, the data follows a parallel structure where both the question and answer are in the same language-what we call parallel-test. This matches the parallel-training settings.</p>
<p>However, as presented in Table 6, in our detailed experiments, when the testing environment mimics the cross-training format, called cross-test, MathOctopus ${ }^{\mathrm{C}}$ shows better performance compared to MathOctopus ${ }^{\mathrm{PT}}$ as model sizes increase from 7B to 33B. This highlights the critical importance of ensuring a consistent alignment between training and testing data formats to optimize LLMs' multilingual reasoning capabilities.</p>
<h3>3.3.2 Multilingual SFT can generally benefit Monolingual SFT</h3>
<p>One significant observation in our experiments is that MathOctopus could significantly improve the performances in English. Table 7 presents the results of LLaMA and MathOctopus on English GSM8K and SVAMP. Both MathOctopus ${ }^{\mathrm{PT}}$ and MathOctopus ${ }^{\mathrm{C}}$ substantially enhance the performance of LLaMA2 across the two datasets. MathOctopus ${ }^{\mathrm{C}}$ emerges as the superior performer (e.g., $50.8 \%$ vs. $49.3 \%, 49.3 \%$ vs. $46.8 \%$ with 7 B level). We surmise that this can be attributed to the cross-training paradigm, intensifying the model's proficiency in English comprehension.</p>
<p>More broadly, does this situation persist in other languages as well? To explore this, we randomly select training sets for three languages from the training dataset: Spanish and Chinese, as well as the low-resource language Thai, and used their re-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Model performances of MathOctopus with different xRFT data volume.</p>
<p>spective corpora to train three models, which we named Es-LLaMA, Zh-LLaMA, and Th-LLaMA, respectively. Figure 3 separately illustrates the test results of several models in their respective training languages. We observe that our model still surpasses the results of the monolingual SFT models in their respective training languages. This suggests that, at least in the task of math reasoning, multilingual SFT can be considered a superior training strategy to monolingual SFT, significantly elevating the model's performance in its native language.</p>
<h2>4 Discussion</h2>
<p>In this section, we delve deeper through a series of meticulous experiments designed to address the following inquiries:</p>
<ul>
<li><strong>Q1: The impact of xRFT</strong>: The implications of varying xRFT data quantities?</li>
<li><strong>Q2: Transferability of MathOctopus</strong>: When solely utilizing corpora of specific languages from MGSM8KInstruct for model training, can we still observe enhancements in unseen languages? (See Answer in Appendix D)</li>
<li><strong>Q3: Variations of the training strategy</strong>: Rather than strictly restricting the questions to English, we can propose forming pairings from the available corpus of 10 languages in MGSM8KInstruct, thus allowing both the questions and answers to span diverse combinations. How does the model perform under such conditions? (See Answer in Appendix D)</li>
</ul>
<h3>4.1 RQ1: Influence of xRFT Data Volume</h3>
<p>In this component, we probe the impact of varying xRFT data quantities on model performance. Figures 4 (a) and (b) respectively illustrate the performances of models across two datasets under xRFT with sampling counts k set to {10, 30, 50}. From the visuals, it's discernible that for MathOctopus, employing larger training corpus generally augments performance in most cases, a trend particularly pronounced in the MGSM dataset. However, these gains remain modest, especially when the backbone model becomes more performant. In contrast, MathOctopus demonstrates marginal improvements attributable to xRFT, and intriguingly, its efficacy on the MSVAMP dataset seems to wane as the k value increases.</p>
<p>This suggests that while xRFT introduces various reasoning paths, its contribution to tasks like multilingual math reasoning remains limited. A plausible rationale is that during the multilingual SFT phase, distinct linguistic versions of the same solution might already be construed as diverse reasoning paths. Hence, multilingual SFT can be viewed as a variant of the monolingual SFT's RFT. Building upon the foundation of multilingual SFT, supplementary benefits conferred by xRFT appear to be limited and might lead to model overfitting.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we pioneer the exploration of training multilingual mathematical LLMs. To address data scarcity in low-resource languages, we first collect the first multilingual math reasoning instruction dataset, named MGSM8KInstruct, consisting of ten various languages. The models, trained on MGSM8KInstruct with different training strategies, named MathOctopus, show superior performances compared to other open-source LLMs. We prove that MathOctopus with <em>parallel-training</em> could achieve better in-domain test results while MathOctopus with <em>cross-training</em> presents better robustness in the collected out-of-domain test set, MSVAMP. We also investigate the impact of the multilingual rejection sampling strategy, finding it has a limited effect on xMR tasks. Our extensive experiments reveal that creating aligned bilingual question-answer corpora significantly improves the model's mathematical capabilities in its native language. In future work, we will explore</p>
<p>additional methodologies and diverse parallel corpora for training xMR LLMs, potentially involving RLHF.</p>
<h2>Limitations</h2>
<p>In this work, we still leave several underexplored parts, which may also contribute to building effective xMR LLMs:</p>
<ul>
<li>Developing MathOctopus based on larger size LLMs, including LLaMA 2-70B and LLaMACoders, which is a future work in our following experiments.</li>
<li>Currently, we only apply xRFT to 7B and 13B models due to the high cost of inferencing. We also will conduct xRFT to more performant models, further investigating its efficiency.</li>
<li>We are still not very clear whether including more languages in MGSM8KInstruct could benefit current models, which will discussed in our next version.</li>
</ul>
<h2>Acknowledgement</h2>
<p>This work was supported by National Key Research and Development Program of China Grant No. 2023YFF0725100 and Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673</p>
<h2>References</h2>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional ai: Harmlessness from ai feedback.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,</p>
<p>Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.</p>
<p>Nuo Chen, Hongguang Li, Baoyuan Wang, and Jia Li. 2024a. From good to great: Improving math reasoning with tool-augmented interleaf prompting. In Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024), pages 64-79, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. 2024b. Graphwiz: An instruction-following language model for graph computational problems. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24, page 353-364, New York, NY, USA. Association for Computing Machinery.</p>
<p>Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, and Daxin Jiang. 2022. Bridging the gap between language models and cross-lingual sequence labeling. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1909-1923.</p>
<p>Nuo Chen, Linjun Shou, Tengtao Song, Ming Gong, Jian Pei, Jianhui Chang, Daxin Jiang, and Jia Li. 2023a. Structural contrastive pretraining for crosslingual comprehension. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2042-2057, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Nuo Chen, Yan Wang, Yang Deng, and Jia Li. 2024c. The oscars of ai theater: A survey on role-playing with language models.</p>
<p>Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023b. Large language models meet harry potter: A bilingual dataset for aligning dialogue agents with characters.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023c. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Zhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen, Junying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. 2023d. MultilingualSIFT: Multilingual Supervised Instruction Fine-tuning.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias</p>
<p>Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764-10799. PMLR.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual generalization. In Proceedings of the 37th International Conference on Machine Learning, pages 4411-4421.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. In ACL (industry), pages 37-42. Association for Computational Linguistics.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 1896-1907. Association for Computational Linguistics.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5315-5333.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583.</p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. 2023. Learning math reasoning from self-sampled correct and partially-correct solutions. In The Eleventh International Conference on Learning Representations.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057.</p>
<p>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2023. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,</p>
<p>Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023c. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. 2022. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561.</p>
<p>Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. 2022. Zeroprompt: Scaling prompt-based pretraining to 1, 000 tasks improves zero-shot generalization. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 4235-4252. Association for Computational Linguistics.</p>
<p>Chenyu You, Nuo Chen, Fenglin Liu, Shen Ge, Xian Wu, and Yuexian Zou. 2022. End-to-end spoken conversational question answering: Task, dataset and model. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1219-1232, Seattle, United States. Association for Computational Linguistics.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023a. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023b. Rrhf: Rank responses to align language models with human feedback without tears.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, et al. 2023. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. arXiv preprint arXiv:2306.10968.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2023. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4471-4485, Toronto, Canada. Association for Computational Linguistics.</p>
<h1>A Related Works</h1>
<p>Math Reasoning with LLMs A pivotal metric for assessing the efficacy of LLMs is their capability in addressing intricate reasoning challenges, exemplified by mathematical reasoning tasks (Scao et al., 2022; Cobbe et al., 2021; Zhou et al., 2022; Weng et al., 2022; Chen et al., 2023c, 2024a). Rather than yielding direct, definitive answers, prior research has illustrated that by employing a variety of prompting techniques, such as Chain-ofThought (CoT) prompting (Wei et al., 2022), LLMs can be guided through step-by-step reasoning, resulting in significant improvements in performance across an array of diverse reasoning tasks. Imani et al. (2023) propose the generation of multiple algebraic expressions or Python functions to solve the same mathematical problem, aiming to explore a broader spectrum of potential solutions. Additionally, Li et al. (2023) introduce a step-aware verifier to scrutinize the reasoning steps in COT, thereby enhancing the model's reasoning capabilities. Another effective approach, Self-Consistency (Wang et al., 2022), combines a wider range of solutions and derives a final answer by aggregating them to obtain the most consistent response. Meanwhile, several scholarly works have incorporated the concept of rejection sampling, in conjunction with various other techniques, to curate a more diverse set of sampled reasoning paths for the purpose of fine-tuning data augmentation (Huang et al., 2022; Zelikman et al., 2022; Ni et al., 2023; Zhu et al., 2023; Bai et al., 2022; Yuan et al., 2023b; Dong et al., 2023; Touvron et al., 2023b; Song et al., 2023; Chen et al., 2024b; You et al., 2022). Following the line, Yuan et al. (2023a) utilize rejection sampling to augment the data volume for fine-tuning math reasoning LLMs.</p>
<p>Instruction Tuning with LLMs Instruction tuning serves as a pivotal component within the developmental frameworks of language models, with its primary function being to orient LLMs towards objectives that are more congruent with human preferences and functional applications (Chen et al., 2023b, 2024c). The academic discourse on instruction tuning is notably concentrated on amplifying the versatile instructional capabilities of LLMs. This discourse is particularly exemplified by pioneering studies such as UnifiedQA (Khashabi et al., 2020), Zero-Prompt (Xu et al., 2022), FLAN (Chung et al., 2022), and T0 (Sanh et al., 2022). These studies have embarked on an exploration into the generalization capabilities of LLMs. Following these, FLAN-v2 (Longpre et al., 2023) further investigated the impact of scaling instructional datasets on model performance. Recent innovations in this domain are veering towards employing synthetic instruction following data, distilled from models like GPT-3/4 (OpenAI, 2023), to align open-source LLMs. Recently, several works have utilized instruction tuning for training math LLMs. Yuan et al. (2023a); Chen et al. (2022) propose RFT in math reasoning, and WizardMath (Luo et al., 2023) implements the "evol-instruct reinforcement learning" methodology (RLEIF), which is directed towards the refinement of prevailing math instruction data. Recently, several works (Chen et al., 2023d) extend instruction tuning from monolingual to multilingual. Chen et al. (2023d) directly translate the Aplaca-GPT4 corpus to other languages and achieve great performances in MMLU tasks through multilingual instruction tuning. Zhang et al. (2023) further boost small MLLMs through interactive instruction tuning translation task. However, almost all of them aim to improve mathematical reasoning in English or general multilingual generation abilities, leaving multilingual mathematical reasoning less explored. This paper aims to fill this gap by exploring effective methods for training robust LLMs in multilingual mathematical reasoning.</p>
<h2>B Experimental Setup</h2>
<p>Training and Testing In this work, we use open-source LLaMA-2 7B to 13B and LLaMA133 B as backbone models, allowing us to build MathOctopus in multiple scales. Our codes are based on DeepSpeed and Huggingface Library. For all models, we set the learning rate, epochs and max length as $2 \mathrm{e}-5,3$ and 512 . The batch sizes are set to $8,4,2$ when models scale from 7 B to 33B. During testing, we set the maximum output token as 512 with temperature as 0.0 to keep stable performances. We keep the same prompt in Table 9 for testing MathOctopus. Please refer to Section 3.3 for xRFT settings.</p>
<h2>C CrowdSourcing Verification of MSVAMP</h2>
<p>Similarly, we further verify the translation quality of MSVAMP. we sample 500 samples from each language and employ native speakers from Microsoft UHRS Platform to check the semantic</p>
<table>
<thead>
<tr>
<th>Lang.</th>
<th>Sw</th>
<th>Zh</th>
<th>Bu</th>
<th>De</th>
<th>Es</th>
<th>Fr</th>
<th>Ja</th>
<th>Ru</th>
<th>Th</th>
</tr>
</thead>
<tbody>
<tr>
<td>Aptuv.</td>
<td>93.4</td>
<td>94.3</td>
<td>93.8</td>
<td>92.7</td>
<td>93.8</td>
<td>96.9</td>
<td>92.4</td>
<td>93.2</td>
<td>92.9</td>
</tr>
</tbody>
</table>
<p>Table 8: Human agreement rate of each language in MSVAMP.
consistency. We report the human agreement rates in Table 8 . The high agreement rates prove the reliable translation quality.</p>
<h2>D Discussion</h2>
<h2>D. 1 RQ2: Targeted Language Training: Limited Broader Linguistic Reasoning Gains</h2>
<p>Our exploration into model training with select languages posits a notable inquiry: Can training with a subset of languages enhance mathematical reasoning across all languages? Engaging two high-resource languages, Spanish and Chinese, and two low-resource languages, Thai and Swahili, for mixed training sessions reveals pivotal insights. Seen in Table 10, while stark performance enhancements are witnessed in trained languages, notably in low-resource ones like Thai (surging from 4\% to $36.8 \%$ ), the model's efficacy varies in languages that are unseen in training: While there are improved outcomes in certain languages like Japanese and French, a corresponding decline is witnessed in others, such as German and Russian. This phenomenon might predominantly stem from the disparities in grammatical structures across different languages (Chen et al., 2023a).</p>
<h2>D. 2 RQ3: Training Variability: Beyond English-centric Questions</h2>
<p>Beyond the two training strategies explored in Section 3.3, we further probe alternative approaches to discern their influence on model performance. Hence, we examine two additional strategies: 1) A Mix-Training approach, where cross-training and parallel-training data are amalgamated for training; 2) An expansive Mix-All method that not only extends cross-training but also randomly pairs two languages from the MGSM8KInstrucT, thereby permitting questions and answers in the training data to traverse various linguistic combinations, effectively amplifying the original training data volume tenfold. The resulting models obtained by the above strategies are called MathOctopus ${ }^{\mathrm{A} 4}$ and MathOctopus ${ }^{\mathrm{A} 4}$-All, separately.</p>
<p>From the right-side Figure 5, it is evident that despite these two new training strategies respectively
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performances of MathOctopus-7B with different strategy on MGSM.
doubling and amplifying training volumes tenfold compared to original parallel-training and crosstraining strategies, they do not surpass the results of MathOctopus ${ }^{\mathrm{Tr}}$. Furthermore, the outcomes from MathOctopus ${ }^{\mathrm{A} 4}$-All slightly underperform MathOctopus ${ }^{\mathrm{A} 4}$. Such a phenomenon may arise because, although "mix-all" and "mix-training" expand the original data, the pre-existing data volume already suffices for the model to learn alignment and reasoning capabilities across different languages. An additional, rudimentary data expansion potentially induces overfitting, subsequently diminishing model performance.</p>
<h2>D. 3 Prompts for ChatGPT Translation</h2>
<p>Table 11 shows the translation prompt when we utilize ChatGPT to translate the data from English to other languages. When applying this prompt, we just need to replace the placeholder symbol lang with the specific language. Here, we use the Spanish translation examples in the prompt.</p>
<h2>D. 4 Generated Cases by MathOctopus</h2>
<p>Table 13 and Table 14 presens several generated examples in various languages by LLaMA and our models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Prompts</th>
<th style="text-align: center;">Below is an instruction that describes a task. \n Write a response that appropriately completes the request in [ language ]. Please answer in [ language ]. \n \n ### Instruction: \n [query] \n\n ### Response:</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 9: Training and testing prompts in our experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">En</th>
<th style="text-align: center;">Sw</th>
<th style="text-align: center;">Zh</th>
<th style="text-align: center;">Bn</th>
<th style="text-align: center;">De</th>
<th style="text-align: center;">Es</th>
<th style="text-align: center;">Fr</th>
<th style="text-align: center;">JA</th>
<th style="text-align: center;">Ru</th>
<th style="text-align: center;">Th</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA 2 (En)</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">22.6</td>
</tr>
<tr>
<td style="text-align: left;">Math0ctopus ${ }^{\text {P }}$ (En-Zh-Es)</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: left;">Math0ctopus ${ }^{\text {P }}$ (En-Sw-Th)</td>
<td style="text-align: center;">$\mathbf{4 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{3 4 . 4}$</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">$\mathbf{3 6 . 8}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 10: Model Performances on MGSM test set. Math0ctopus ${ }^{\mathrm{P}}$ (En-Zh-Es) refers to we only train Math0ctopus ${ }^{\mathrm{P}}$ in three languages: English, Chinese and Spanish. Similarly, Math0ctopus ${ }^{\mathrm{P}}$ (En-Sw-Th) means the Math0ctopus ${ }^{\mathrm{P}}$ trained in English, Swahili and Thai.</p>
<p>Prompts: You are a professional [lang] translator and spelling corrector. Please translate the given math question and its reasoning path into [lang].
Below are examples:
Q: Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
P: Babysitting is $\$ 12 /$ hour $=\$ 12 / 60$ minutes $=\$ \approx 12 / 60=0.2 \times 0.2$ per minute. Weng did babysitting for 50 minutes, so she earned $\$ 0.2 \times 50=\$ \approx 0.2 * 50=10 \times 10$.
T-Q: Weng gana 12 dólares la hora por cuidar niños. Ayer cuidó niños durante 50 minutos. ¿Cuánto ganó?
T-P: Cuidar niños cuesta $\$ 12 /$ hora $=\$ 12 / 60$ minutos $=\$ \approx 12 / 60=0.2 \times 0.2$ por minute. Weng cuidó niños durante 50 minutos, por lo que ganó $\$ 0.2 \times 50=\$ \approx 0.2 * 50=10 \times 10$.
Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?
P: Since today is the last day of the month, Julie would like to finish an entire book before tomorrow. She has read a total of $12+24=\approx 12+24=36 \times 36$ pages in two days. There are $120-36=\approx 120-36=84 \times 84$ pages left to be read. Hence, Julie should read $84 / 2=\approx 84 / 2=42 \times 42$ pages tomorrow.
T-Q: Julie está leyendo un libro de 120 páginas. Ayer pudo leer 12 páginas y hoy leyó el doble que ayer. Si quiere leer la mitad de las páginas restantes mañana, ¿cuántas páginas debería leer?
T-P: Como hoy es el último día del mes, a Julie le gustaría terminar un libro completo antes de mañana. Ha leído un total de $12+24=\approx 12+24=36 \times 36$ páginas en dos días. Quedan $120-36=\approx 120-36=84 \times 84$ páginas por leer. Por lo tanto , Julie debería leer $84 / 2=\approx 84 / 2=42 \times 42$ páginas mañana.
Please Keep in mind that:
(1) keep the translations consistent for names of people and places within the sentences.
(2) Preserve the mathematical formula within the " $\approx$ " brackets when translating.
(3) You must translate the text into [lang].
(4) You must follow the output format with: "T-Q:... T-P:..."</p>
<p>Table 11: Translation prompts in our experiments.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 12: 2-shot prompts of ChatGPT and GPT-4 in our experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Generated Cases in Chinese</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">珍妮特的鸭子每天下 16 颗蛋。她每天早上早餐时吃 3 颗，每天用 4 颗为自己的朋友做松饼。剩下的鸭蛋她每天拿去农贸市场卖，每颗新鲜鸭蛋卖 2 美元。她每天在农贸市场睡多少钱？</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">珍妮特每天吃早餐3颗鸭蛋，剩下的鸭蛋用于做松饼4颗，剩下的鸭蛋卖在农贸市场2颗，珍妮特每天在农贸市场睡 $2 * 2=« 2 * 2=4=4$ 美元。珍妮特每天卖 4 颗鸭蛋，所以她每天在农贸市场睡 $4 * 4=« 4 * 4=16=16$ 美元。 $\times$</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus</td>
<td style="text-align: center;">珍妮特每天吃掉 $3+4=« 3+4=7=7$ 颗鸭蛋。鸭子每天下 16 颗蛋，所以剩下 $16-7=« 16-7=9=9$ 颗蛋。珍妮特每天卖掉9 颗蛋，每颗蛋卖 2 美元，所以她每天睡 $9 * 2=« 9 * 2=18=18$ 。 $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">图卢兹的绵羊数量是查尔斯顿的两倍。查尔斯顿的绵羊数量是西雅图的 4 倍。如果西雅图有 20 只绵羊，图卢兹、查尔斯顿和西雅图一共有多少只绵羊？</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">查尔斯顿有 $2 * 20=« 2 * 20=40=40$ 只绵羊。图卢兹有 $40 * 2=« 40 * 2=80=80$ 只绵羊。西雅图和查尔斯顿和图卢兹一共有 $20+40+80=« 20+40+80=140=140$ 只绵羊。 $\times$</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus</td>
<td style="text-align: center;">查尔斯顿有 $4 * 20=« 4 * 20=80=80$ 只绵羊。图卢兹有 $2 * 80=« 2 * 80=160=160$ 只绵羊。总共，三个城市有 $20+80+160=« 20+80+160=260=260$ 只绵羊。 $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">260</td>
</tr>
<tr>
<td style="text-align: center;">Generated Cases in Japanese</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">リチャドは15建てのマンションに住んでいる。各には8部屋あり、建物の $3 / 4$ が入居されている。建物で未入居の部屋は全部で何室あるか？</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">15建てのマンションに8部屋ある各は $15 * 8==15 * 8=120=120$ 部屋になる。建物の $3 / 4$ が入居されてい るので、入居されている部屋数は $120 * 3 / 4==120 * 3 / 4=90=90$ 部屋になる。未入居の部屋は $90-8==90-$ $8=82=82$ 部屋である。 $\times$</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus</td>
<td style="text-align: center;">15 建てのマンションに8部屋あるため、リチャド住む15建てマンションの部屋数は $8 * 15==8 * 15=120=120$ 部屋建物の $3 / 4$ が入居されているため、未入居の部屋 は $120 * 1 / 4==120 * 1 / 4=30=30$ 部屋。 $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">トゥルズはチャルストンに比べ2倍の数の羊をっています。チャルストンはシアトルに比べ4倍 の数の羊をっています。シアトルがっている羊の数が20だったとしたら、トゥルズ、チャルス トン、シアトルがっている羊の数は何でしょうか？</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">トゥルズはチャルストンに比べ2倍の数の羊をっているので、トゥルズがっている羊の数 は $2 * 4==2 * 4=8=8$ です。チャルストンはシアトルに比べ4倍の数の羊をっているので、チャルス トンがっている羊の数は $4 * 20==4 * 20=80=80$ です。シアトルがっている羊の数が20だったとした ら、トゥルズ、チャルストン、シアトルがっている羊の数は $80+8+20==80+8+20=118=118$ です。 $\times$</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus</td>
<td style="text-align: center;">シアトルが20の羊をっているなら、チャルストンは $4 * 20=« 4 * 20=80=80$ の羊をっています。トゥ ルズはチャルストンの2倍の数の羊をっているので、160の羊をっています。合すると、トゥル ズ、チャルストン、シアトルは $20+80+160=« 20+80+160=260=260$ の羊をっています。 $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">260</td>
</tr>
</tbody>
</table>
<p>Table 13: Generated responses compared between LLaMA and ours within 7B-level on MGSM.</p>
<p>Generated Cases in German</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Query</th>
<th style="text-align: center;">Toula ging in die Bäckerei und kaufte verschiedene Arten von Gebäck. Sie kaufte 3 Dutzend Donuts, die 68 US-Dollar pro Dutzend kosten, 2 Dutzend Mini-Cupcakes, die 80 US-Dollar pro Dutzend kosten und 6 Dutzend Mini-Cheesecakes für 55 US-Dollar pro Dutzend. Wie hoch waren die Gesamtkosten?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">3 Dutzend Donuts kosteten $3 \times 68=« 3 * 68=204=204$ US-Dollar. 2 Dutzend Mini-Cupcakes kosteten $2 \times 80=$ «2*80=160»160 US-Dollar. 6 Dutzend Mini-Cheesecakes kosteten $6 \times 55=« 6 * 55=330 » 330$ US-Dollar.Die Gesamtkosten für alle drei Arten von Gebäck waren $204+160+330=« 204+160+330=794 » 794$ US-Dollar. $\lambda$</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus</td>
<td style="text-align: center;">Toula kaufte 3 Dutzend Donuts für 68 US-Dollar pro Dutzend, also kosten sie $3 * 68=204$ US-Dollar.Toula kaufte 2 Dutzend Mini-Cupcakes für 80 US-Dollar pro Dutzend, also kosten sie $2 * 80=160$ US-Dollar.Toula kaufte 6 Dutzend Mini-Cheesecakes für 55 US-Dollar pro Dutzend, also kosten sie $6 * 55=330$ US-Dollar.Die Gesamtkosten betragen $204+160+330=694$ US-Dollar. $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">694</td>
</tr>
<tr>
<td style="text-align: center;">Generated Cases in French</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">Jacques fait des séries de 3 sprints 3 fois par semaine. Il court 60 mètres à chaque sprint. Au total, combien de mètres court-il par semaine?</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">Jacques fait $3 * 3=« 3 * 3=9 » 9$ séries de sprints par semaine.Par série, il court $9 * 60=« 9 * 60=540 » 540$ mètres.Par semaine, il court donc $540 * 9=« 540 * 9=4800 » 4800$ mètres. $\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus</td>
<td style="text-align: center;">Jacques fait des séries de 3 sprints 3 fois par semaine.Chaque série est de 3 sprints, donc il court $3 * 3=« 3 * 3=9 » 9$ sprints par semaine. Chaque sprint est de 60 mètres, donc il court $60 * 9=« 60 * 9=540 » 540$ mètres par semaine. $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">540</td>
</tr>
<tr>
<td style="text-align: center;">Generated Cases in Spanish</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">Los patos de Janet ponen 16 huevos por día. Ella come tres en el desayuno todas las mañanas y usa cuatro para hornear magdalenas para sus amigos todos los días. Vende lo que sobra en el mercado de productores diariamente a $\$ 2$ el huevo fresco de pato. ¿Cuánto gana en dólares todos los días en el mercado de productores?</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">Janet comienza con 16 huevos frescos por día - 3 huevos frescos que come para desayuno $=« 16-3=13 » 13$ huevos frescos que no se comen.Janet usa 13 huevos frescos - 4 huevos frescos para hornear magdalenas $=$ «13-4=9»9 huevos frescos que no se comen.Janet vende 9 huevos frescos - 2 huevos frescos que se venden en el mercado de productores $=« 9-2=7 » 7$ huevos frescos que se venden en el mercado de productores.Janet gana 7 huevos frescos * \$2/huevo fresco $=\$ « 7 * 2=14 » 14$ en el mercado de productores todos los días. $\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: center;">MathOctopus</td>
<td style="text-align: center;">Janet come 3 huevos por día y usa 4 huevos por día, por lo que tiene $3+4=« 3+4=7 » 7$ huevos que no usa para nada.Los patos de Janet ponen 16 huevos por día, por lo que tiene $16-7=« 16-7=9 » 9$ huevos excedentes.Vende los 9 huevos excedentes por $\$ 2$ cada uno, por lo que gana $9 * 2=\$ « 9 * 2=18 » 18$ todos los días en el mercado de productores. $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">18</td>
</tr>
</tbody>
</table>
<p>Table 14: Generated responses compared between LLaMA and ours within 7B-level on MGSM.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Prompts: Answer the following math probelm step by step in [lang].
Below are examples:
Q: Julia played tag with 18 kids on Monday. She played tag with 10 kids on Tuesday. how many more kids did she play with on Monday than on Tuesday?
A: Let's think step by step. Julia playsed tag with 18 kids on Monday and 10 kids Tuesday, separately. So the amount of kids that she played with on Monday than on Tuesday is $18-10=8$. So the answer is 8 .
Q: Jack had 9 action figures and 10 books on a shelf in his room. later he added 7 more action figures to the shelf. how many more action figures than books were on his shelf ?
A: Let's think step by step. The amount of action figures that Jack had is $9+7=16$. And Jack had 10 books. So the amount of action figures than books on his shelf is $16-10=6$. So the answer is 7 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
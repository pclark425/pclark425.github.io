<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7381 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7381</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7381</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-270559604</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.10421v3.pdf" target="_blank">SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs’ ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7381.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7381.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM examinee (Claude)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude (used as examinee on SciEx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude was used as a multimodal proprietary LLM examinee to generate freeform answers to university-level computer science exam questions (text and image questions) in the SciEx benchmark and was graded by human experts and automatic LLM graders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude (Claude-3-opus-20240229 listed in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Proprietary, instruction-tuned, multimodal (vision-capable)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science subdomains (university exams): NLP, Advanced AI, Deep Learning & Neural Networks, Deep Learning for Computer Vision, Human-Computer Interaction, Databases, Computer Graphics, Theoretical courses (TGI), etc.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate freeform exam answers to university-level computer science questions, including explanations, proofs, algorithms, and textual descriptions of drawings for image-related prompts (i.e., simulate a student/examinee).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Provided prompt in the same language as the exam (English or German); for image questions: include image captions referenced in the text; models asked to describe drawings in text when asked to draw; no special few-shot used for answer generation (answers generated one question at a time). Detailed answer-generation prompts in Appendix C.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expert human grading normalized to 0–100% per question/exam; German grade scale reported as secondary mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>59.4% average exam score (expert-graded, best-performing examinee in the study); German grade 2.4 on reported scale.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Student average exam score 45.3% (German grade 3.1); weaker LLM examinees (Mixtral 41.1%, Qwen 35.4%, Mistral 25.9%, Llava 21.5%, GPT-3.5 32.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model strength/size/proprietary vs open-source', 'Multimodal capability (ability to consume and reason about images)', 'Question difficulty level (Claude performed relatively better on harder questions compared to weaker LLMs)', 'Language (English vs German) — English favored', 'Question modality (image vs text) — multimodal models outperform text-only ones on image questions', 'Task type (math- and proof-heavy questions harder)', 'Lack of time pressure (LLMs produce longer answers)', 'Presence of course-specific context / template questions']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Answers generated via provider API (Anthropic) for Claude; images referenced via captions; text-only LLMs excluded image inputs; Llava concatenation strategy used for multi-image inputs; inference sampling: default sampling settings (open-source via llama.cpp for other models); experiments run on GPU (A6000) for open-source models. Detailed prompts in Appendix C.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Struggled with math/proofs less often than many other LLMs but still made mistakes similar to student errors; occasional refusals to answer; sometimes verbose; can hallucinate or make incorrect reasoning steps; performance gap for German-language questions and some image tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7381.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM examinee (GPT-4V)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (gpt-4-vision-preview) used as examinee on SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4V was used as a multimodal proprietary examinee generating freeform answers (including multi-image questions) to university computer science exams; outputs were expert-graded and used as both examinee outputs and as an automatic grader in separate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (gpt-4-vision-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Proprietary, multimodal (vision-capable), instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science subdomains (see above list of courses in SciEx).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Simulate student examinee answering freeform questions (text and image), including explanations, proofs, code/algorithms, and textual descriptions of required drawings.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same-language prompts (English/German); images passed directly to GPT-4V; referenced images by mentioning captions in the question text; one-question-at-a-time generation; no few-shot specified for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expert human grading (normalized 0–100%) and German grade scale.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>58.2% average exam score (expert-graded); German grade 2.5. On text-only English subset: 70.8% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Student average 45.3% (overall) and other LLMs as listed; compared to Claude (59.4%), other open-source models lower.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Multimodal capability (strong performance on image-related questions)', 'Language (better on English than German)', 'Difficulty level (stronger on harder questions compared to weaker LLMs)', 'Math/proof difficulty reduces performance', 'Prompt/image referencing method used']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Used OpenAI API for proprietary model; images included natively; prompt in Appendix C. Same normalization of scores. Default sampling and generation conditions used for models via their APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Still error-prone on math proofs in some exams; produces human-like mistakes; occasionally hallucinated non-existent file paths in image tasks; grading behavior when used as grader sometimes favored itself in ranking without reference answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7381.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM examinee (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0125) used as examinee on SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was evaluated as a text-only proprietary examinee on freeform university computer science exam questions; images were excluded for text-only models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Proprietary, instruction-tuned, text-only</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science subdomains in SciEx (text-only subset).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate freeform textual exam answers (no image input) to simulate student examinee.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same-language prompts; images removed for text-only models; one-question-at-a-time generation; prompts shown in Appendix C.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expert human grading (normalized 0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>32.8% average exam score (expert-graded); German grade 3.9.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Student average 45.3% (GPT-3.5 below students); stronger models (Claude, GPT-4V) performed better.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Lack of multimodal capabilities (images removed) negatively affects image questions', 'Language effects (English vs German)', 'Difficulty type (math/proofs are challenging)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Used OpenAI API; text-only input for image-containing questions (images excluded).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Poor performance on image-related questions (excluded images); lower overall performance compared to stronger proprietary models; math/reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7381.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM examinee (Mixtral)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1) used as examinee on SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixtral, an open-source instruction-tuned model, was evaluated as a text-only examinee on SciEx freeform computer science exam questions and its outputs were expert-graded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B (as listed in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Open-source, instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science exam subdomains (text-only subset)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Simulate student answering freeform exam questions (text-only).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same-language prompts; image questions had images excluded for text-only models; one-question-at-a-time generation (detailed prompts in Appendix C).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expert human grading normalized to 0–100%.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>41.1% average exam score (expert-graded); German grade 3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Student average 45.3%; Mixtral outperforms some open-source peers on English text-only subset (Table 8 shows Mixtral 61.2% on text-only English subset).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model capacity and instruction tuning', 'Language and modality (performs better on English text-only subset)', 'Difficulty and question type (math/proofs hard)', 'Image exclusion harms performance on image tasks']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Open-source checkpoint from Hugging Face; inference via llama.cpp with default sampling; quantized (5 bit as in Table 2); experiments on NVIDIA RTX A6000 (48GB).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Tends to underperform on image-related or German-language questions; as a grader (separate experiments) tends to give full points frequently in zero-shot, indicating bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7381.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM examinee (Qwen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen (Qwen-72B) used as examinee on SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qwen (72B) was evaluated as an open-source (quantized) examinee on freeform computer science exam questions (text-only in this study), and its outputs were expert-graded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen (Qwen-72B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B (as listed in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Open-source, presumably instruction-tuned variant (per Table 2 entry)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science exam subdomains (text-only subset)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate freeform exam answers to simulate student examinee (text-only).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same-language prompts; images excluded for text-only models; one-question-at-a-time.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expert human grading normalized to 0–100%.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>35.4% average exam score (expert-graded); German grade 3.7. On English text-only subset: 56.8% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against student average 45.3% and other LLMs (Qwen below student average overall; above students on some English text-only subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Language (better on English)', 'Modality (text-only limitation)', 'Question difficulty and math/proof content']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Open-source checkpoint (Hugging Face), quantized (2 bit per Table 2), inference via llama.cpp default sampling on A6000 GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Poorer overall exam performance vs proprietary multimodal models; struggles on math/proof items and image-related tasks (images excluded).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7381.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM examinee (Mistral)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral (Mistralai/Mistral-7B-Instruct-v0.2) used as examinee on SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mistral-7B instruction-tuned model was evaluated as a text-only examinee on SciEx freeform computer science exam questions and expert-graded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral (Mistralai/Mistral-7B-Instruct-v0.2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as listed in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Open-source, instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science exam subdomains (text-only subset)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Simulate student answering freeform exam questions (text only).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same-language prompts; images excluded for text-only models; one-question-at-a-time generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expert human grading normalized to 0–100%.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>25.9% average exam score (expert-graded); German grade 4.2.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Student average 45.3%; far below stronger LLMs and students overall.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Smaller model capacity relative to larger models', 'Modality (text-only) and language effects', 'Question difficulty (hard questions particularly challenging)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Open-source checkpoint used with llama.cpp inference; quantization unspecified for this run in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Low performance on many exam questions; math/reasoning and image-related questions problematic (images excluded).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7381.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM examinee (Llava)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llava-v1.6-Mistral-7b-hf (Llava) used as examinee on SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llava, a multimodal open-source model built on Mistral-7B, was evaluated as an examinee; images concatenated into a single image with padding and passed to the model, and Llava produced textual answers for exam questions including image-related prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llava (Llava-hf/Llava-v1.6-Mistral-7b-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as listed in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Open-source, multimodal (vision-enabled), instruction-tuned variant</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science exam subdomains (including image-related questions)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Simulate student answering freeform questions including interpretation of images (images concatenated into one with blank padding when multiple).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same-language prompts; because Llava handles only one image at a time, multiple images concatenated with blank padding; asked to describe drawings in text where required.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expert human grading normalized to 0–100%.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>21.5% average exam score (expert-graded); German grade 4.3. On English text-only subset: 42.4% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Student average 45.3%; Llava underperforms students on many image-related tasks in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Multimodal handling limitations (single-image handling required concatenation hack)', 'Model capacity relative to proprietary multimodal models', 'Language and task type (image tasks challenging despite multimodality)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Open-source model from Hugging Face; image concatenation strategy applied; inference via llama.cpp/default sampling on A6000 GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite being multimodal, Llava falls behind students on image-related questions; struggles with drawing tasks and may produce low-quality image descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7381.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM-as-a-judge (GPT-4V grader)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V used as automatic grader (LLM-as-a-judge) for SciEx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4V was used to automatically grade examinee answers by outputting a single score between 0 and the maximum, given question, answer, and reference; chain-of-thought was requested and few-shot examples/reference answers were optionally included; performance compared against expert grading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (gpt-4-vision-preview) as grader</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Proprietary, multimodal, instruction-tuned with chain-of-thought requested in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational assessment of computer science exams (grading subdomain).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Assign numeric grades to freeform exam answers (simulate human exam grader) on the same scale as experts.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>LLM-as-a-judge prompt: include question, answer, maximum score; optionally include reference answer; request chain-of-thought reasoning before giving the score; use few-shot examples (0/1/2-shot) with three example-selection strategies: same question (different examinee), same exam (different question), different exam; experiments with/without reference answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation on normalized scores (primary); RMSE on original scales (secondary). Also measured behavior statistics (e.g., propensity to copy example grades, proportion of full-point assignments, precision of full points).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Exam-level Pearson correlation to expert grading = 0.948 (best setting). Question-level Pearson correlations up to ~0.7 depending on shot/ref settings. RMSE reported in appendix (not listed in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Open-source Llama3 grader exam-level Pearson = 0.883; Mixtral grader performed worse (numerical exam-level Pearson not explicitly given in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Including reference answers improves grader performance', 'Few-shot examples improve performance; relevance of examples matters (same exam or different exam vs same question)', 'Grader tendency to copy the grade of example when exact question text duplicated in shot (observed for Mixtral and GPT-4V)', 'Grader-specific biases (Mixtral tends to give full points in zero-shot)', 'Multimodal capability did not harm GPT-4V performance on image-related questions in this dataset (contrasts with prior work)', 'Which examinee is graded affects correlation (some graders inconsistent across examinees)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Prompts asked for chain-of-thought before final grade; tried 0/1/2-shot settings and with/without reference answer; example-selection strategies (same question, same exam, different exam) tested; images included for multimodal graders; APIs (OpenAI) for GPT-4V used; default sampling settings. Few-shot example tuples contained question, answer, and expert-provided grade.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>At question level, correlations drop (~0.7); grader can copy example grades (25% copy rate for GPT-4V when single-shot with same question example); grader can be inconsistent across different examinees and tends to rank itself favorably without reference answers; some image-related grading conclusions may not generalize due to small number of image questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7381.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM-as-a-judge (Llama3 grader)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3 (MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF) used as automatic grader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama3 was used as an open-source automatic grader (LLM-as-a-judge) on SciEx, receiving question, answer, maximum score, optional reference answers, and few-shot examples; it provided numeric scores and chain-of-thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3 (MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (as listed in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Open-source, instruction-tuned (instruct variant), used for grading</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational assessment (grading) for computer science exams.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Assign numeric grades to freeform exam answers simulating human graders.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Few-shot grading prompts with or without reference answers; requested chain-of-thought; example-selection strategies (same question/same exam/different exam) tested; 0/1/2-shot variations evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation to expert grading on normalized scores (primary); RMSE on original scales (appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Exam-level Pearson correlation to expert grading = 0.883 (reported in text). Question-level Pearson correlations varied by setting (e.g., 0.452–0.672 across shot/ref settings for text-only questions in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to GPT-4V grader (0.948 exam-level) and Mixtral grader (worse performance); also compared across shot/ref conditions and to expert grades.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Including reference answers and few-shot examples improves performance', 'Llama3 copied example grades less often (13%) compared to other graders', 'Performs better on easier questions (higher correlations on easy vs hard)', 'Example relevance (same question vs same exam vs different exam) affects utility']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Open-source checkpoint used with llama.cpp for inference; quantized (4-bit as in Table 2); experiments on A6000 GPU; multiple shot/ref settings tested; attached chain-of-thought requested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower exam-level performance than GPT-4V; per-question correlations lower (around 0.5–0.6 depending on setting); some dependence on example selection; unable to directly handle images in some grader configurations (consequence of model input limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7381.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7381.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEx - LLM-as-a-judge (Mixtral grader)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1) used as automatic grader</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixtral was evaluated as an open-source LLM-as-a-judge for automatic grading on SciEx and found to have biased grading behaviors (e.g., overassigning full points in zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B (as listed in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Open-source, instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational assessment (grading) for computer science exams.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Assign numeric grades to freeform exam answers simulating human graders.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Grading prompts with options: 0/1/2-shot, with/without reference answers; chain-of-thought requested; example-selection strategies tested.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlation to expert grading (question-level and exam-level), RMSE in appendix, and diagnostic measures like proportion of full-point assignments and precision for full points.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against Llama3 (exam-level 0.883) and GPT-4V (exam-level 0.948); Mixtral reported worse performance than these although exact exam-level Pearson not given in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Tendency to give full points in zero-shot (67.6% of answers got full points in 0-shot)', 'Precision on giving full points low (0.181)', 'Performance improved with more examples and when reference answer included', 'Example-selection biases (copies grade of example ~25% when same-question shot without reference)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Open-source model used with llama.cpp inference; grading prompts requested chain-of-thought; multiple shot/ref settings tested; diagnostic statistics (full-point rates, copying rates) measured.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Strong bias to assign full points in zero-shot; low precision for full points; copies grades from examples when question text duplicates example; overall worst grader among those evaluated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>A closer look into using large language models for automatic evaluation <em>(Rating: 2)</em></li>
                <li>Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>M3Exam: A multilingual, multimodal, multilevel benchmark for examining large language models <em>(Rating: 1)</em></li>
                <li>Measuring massive multitask language understanding <em>(Rating: 1)</em></li>
                <li>ARC-DA (direct-answer conversion of science questions) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7381",
    "paper_id": "paper-270559604",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "SciEx - LLM examinee (Claude)",
            "name_full": "Claude (used as examinee on SciEx)",
            "brief_description": "Claude was used as a multimodal proprietary LLM examinee to generate freeform answers to university-level computer science exam questions (text and image questions) in the SciEx benchmark and was graded by human experts and automatic LLM graders.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude (Claude-3-opus-20240229 listed in Table 2)",
            "model_size": null,
            "model_type": "Proprietary, instruction-tuned, multimodal (vision-capable)",
            "scientific_domain": "Computer science subdomains (university exams): NLP, Advanced AI, Deep Learning & Neural Networks, Deep Learning for Computer Vision, Human-Computer Interaction, Databases, Computer Graphics, Theoretical courses (TGI), etc.",
            "simulation_task_description": "Generate freeform exam answers to university-level computer science questions, including explanations, proofs, algorithms, and textual descriptions of drawings for image-related prompts (i.e., simulate a student/examinee).",
            "prompting_strategy": "Provided prompt in the same language as the exam (English or German); for image questions: include image captions referenced in the text; models asked to describe drawings in text when asked to draw; no special few-shot used for answer generation (answers generated one question at a time). Detailed answer-generation prompts in Appendix C.",
            "evaluation_metric": "Expert human grading normalized to 0–100% per question/exam; German grade scale reported as secondary mapping.",
            "reported_accuracy": "59.4% average exam score (expert-graded, best-performing examinee in the study); German grade 2.4 on reported scale.",
            "baseline_accuracy": "Student average exam score 45.3% (German grade 3.1); weaker LLM examinees (Mixtral 41.1%, Qwen 35.4%, Mistral 25.9%, Llava 21.5%, GPT-3.5 32.8%).",
            "factors_reported": [
                "Model strength/size/proprietary vs open-source",
                "Multimodal capability (ability to consume and reason about images)",
                "Question difficulty level (Claude performed relatively better on harder questions compared to weaker LLMs)",
                "Language (English vs German) — English favored",
                "Question modality (image vs text) — multimodal models outperform text-only ones on image questions",
                "Task type (math- and proof-heavy questions harder)",
                "Lack of time pressure (LLMs produce longer answers)",
                "Presence of course-specific context / template questions"
            ],
            "experimental_conditions": "Answers generated via provider API (Anthropic) for Claude; images referenced via captions; text-only LLMs excluded image inputs; Llava concatenation strategy used for multi-image inputs; inference sampling: default sampling settings (open-source via llama.cpp for other models); experiments run on GPU (A6000) for open-source models. Detailed prompts in Appendix C.",
            "limitations_or_failure_modes": "Struggled with math/proofs less often than many other LLMs but still made mistakes similar to student errors; occasional refusals to answer; sometimes verbose; can hallucinate or make incorrect reasoning steps; performance gap for German-language questions and some image tasks.",
            "uuid": "e7381.0",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM examinee (GPT-4V)",
            "name_full": "GPT-4V (gpt-4-vision-preview) used as examinee on SciEx",
            "brief_description": "GPT-4V was used as a multimodal proprietary examinee generating freeform answers (including multi-image questions) to university computer science exams; outputs were expert-graded and used as both examinee outputs and as an automatic grader in separate experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V (gpt-4-vision-preview)",
            "model_size": null,
            "model_type": "Proprietary, multimodal (vision-capable), instruction-tuned",
            "scientific_domain": "Computer science subdomains (see above list of courses in SciEx).",
            "simulation_task_description": "Simulate student examinee answering freeform questions (text and image), including explanations, proofs, code/algorithms, and textual descriptions of required drawings.",
            "prompting_strategy": "Same-language prompts (English/German); images passed directly to GPT-4V; referenced images by mentioning captions in the question text; one-question-at-a-time generation; no few-shot specified for answer generation.",
            "evaluation_metric": "Expert human grading (normalized 0–100%) and German grade scale.",
            "reported_accuracy": "58.2% average exam score (expert-graded); German grade 2.5. On text-only English subset: 70.8% (Table 8).",
            "baseline_accuracy": "Student average 45.3% (overall) and other LLMs as listed; compared to Claude (59.4%), other open-source models lower.",
            "factors_reported": [
                "Multimodal capability (strong performance on image-related questions)",
                "Language (better on English than German)",
                "Difficulty level (stronger on harder questions compared to weaker LLMs)",
                "Math/proof difficulty reduces performance",
                "Prompt/image referencing method used"
            ],
            "experimental_conditions": "Used OpenAI API for proprietary model; images included natively; prompt in Appendix C. Same normalization of scores. Default sampling and generation conditions used for models via their APIs.",
            "limitations_or_failure_modes": "Still error-prone on math proofs in some exams; produces human-like mistakes; occasionally hallucinated non-existent file paths in image tasks; grading behavior when used as grader sometimes favored itself in ranking without reference answers.",
            "uuid": "e7381.1",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM examinee (GPT-3.5)",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-0125) used as examinee on SciEx",
            "brief_description": "GPT-3.5 was evaluated as a text-only proprietary examinee on freeform university computer science exam questions; images were excluded for text-only models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0125)",
            "model_size": null,
            "model_type": "Proprietary, instruction-tuned, text-only",
            "scientific_domain": "Computer science subdomains in SciEx (text-only subset).",
            "simulation_task_description": "Generate freeform textual exam answers (no image input) to simulate student examinee.",
            "prompting_strategy": "Same-language prompts; images removed for text-only models; one-question-at-a-time generation; prompts shown in Appendix C.",
            "evaluation_metric": "Expert human grading (normalized 0–100%).",
            "reported_accuracy": "32.8% average exam score (expert-graded); German grade 3.9.",
            "baseline_accuracy": "Student average 45.3% (GPT-3.5 below students); stronger models (Claude, GPT-4V) performed better.",
            "factors_reported": [
                "Lack of multimodal capabilities (images removed) negatively affects image questions",
                "Language effects (English vs German)",
                "Difficulty type (math/proofs are challenging)"
            ],
            "experimental_conditions": "Used OpenAI API; text-only input for image-containing questions (images excluded).",
            "limitations_or_failure_modes": "Poor performance on image-related questions (excluded images); lower overall performance compared to stronger proprietary models; math/reasoning failures.",
            "uuid": "e7381.2",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM examinee (Mixtral)",
            "name_full": "Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1) used as examinee on SciEx",
            "brief_description": "Mixtral, an open-source instruction-tuned model, was evaluated as a text-only examinee on SciEx freeform computer science exam questions and its outputs were expert-graded.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1)",
            "model_size": "8x7B (as listed in Table 2)",
            "model_type": "Open-source, instruction-tuned",
            "scientific_domain": "Computer science exam subdomains (text-only subset)",
            "simulation_task_description": "Simulate student answering freeform exam questions (text-only).",
            "prompting_strategy": "Same-language prompts; image questions had images excluded for text-only models; one-question-at-a-time generation (detailed prompts in Appendix C).",
            "evaluation_metric": "Expert human grading normalized to 0–100%.",
            "reported_accuracy": "41.1% average exam score (expert-graded); German grade 3.5.",
            "baseline_accuracy": "Student average 45.3%; Mixtral outperforms some open-source peers on English text-only subset (Table 8 shows Mixtral 61.2% on text-only English subset).",
            "factors_reported": [
                "Model capacity and instruction tuning",
                "Language and modality (performs better on English text-only subset)",
                "Difficulty and question type (math/proofs hard)",
                "Image exclusion harms performance on image tasks"
            ],
            "experimental_conditions": "Open-source checkpoint from Hugging Face; inference via llama.cpp with default sampling; quantized (5 bit as in Table 2); experiments on NVIDIA RTX A6000 (48GB).",
            "limitations_or_failure_modes": "Tends to underperform on image-related or German-language questions; as a grader (separate experiments) tends to give full points frequently in zero-shot, indicating bias.",
            "uuid": "e7381.3",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM examinee (Qwen)",
            "name_full": "Qwen (Qwen-72B) used as examinee on SciEx",
            "brief_description": "Qwen (72B) was evaluated as an open-source (quantized) examinee on freeform computer science exam questions (text-only in this study), and its outputs were expert-graded.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen (Qwen-72B)",
            "model_size": "72B (as listed in Table 2)",
            "model_type": "Open-source, presumably instruction-tuned variant (per Table 2 entry)",
            "scientific_domain": "Computer science exam subdomains (text-only subset)",
            "simulation_task_description": "Generate freeform exam answers to simulate student examinee (text-only).",
            "prompting_strategy": "Same-language prompts; images excluded for text-only models; one-question-at-a-time.",
            "evaluation_metric": "Expert human grading normalized to 0–100%.",
            "reported_accuracy": "35.4% average exam score (expert-graded); German grade 3.7. On English text-only subset: 56.8% (Table 8).",
            "baseline_accuracy": "Compared against student average 45.3% and other LLMs (Qwen below student average overall; above students on some English text-only subsets).",
            "factors_reported": [
                "Language (better on English)",
                "Modality (text-only limitation)",
                "Question difficulty and math/proof content"
            ],
            "experimental_conditions": "Open-source checkpoint (Hugging Face), quantized (2 bit per Table 2), inference via llama.cpp default sampling on A6000 GPU.",
            "limitations_or_failure_modes": "Poorer overall exam performance vs proprietary multimodal models; struggles on math/proof items and image-related tasks (images excluded).",
            "uuid": "e7381.4",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM examinee (Mistral)",
            "name_full": "Mistral (Mistralai/Mistral-7B-Instruct-v0.2) used as examinee on SciEx",
            "brief_description": "Mistral-7B instruction-tuned model was evaluated as a text-only examinee on SciEx freeform computer science exam questions and expert-graded.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral (Mistralai/Mistral-7B-Instruct-v0.2)",
            "model_size": "7B (as listed in Table 2)",
            "model_type": "Open-source, instruction-tuned",
            "scientific_domain": "Computer science exam subdomains (text-only subset)",
            "simulation_task_description": "Simulate student answering freeform exam questions (text only).",
            "prompting_strategy": "Same-language prompts; images excluded for text-only models; one-question-at-a-time generation.",
            "evaluation_metric": "Expert human grading normalized to 0–100%.",
            "reported_accuracy": "25.9% average exam score (expert-graded); German grade 4.2.",
            "baseline_accuracy": "Student average 45.3%; far below stronger LLMs and students overall.",
            "factors_reported": [
                "Smaller model capacity relative to larger models",
                "Modality (text-only) and language effects",
                "Question difficulty (hard questions particularly challenging)"
            ],
            "experimental_conditions": "Open-source checkpoint used with llama.cpp inference; quantization unspecified for this run in the text.",
            "limitations_or_failure_modes": "Low performance on many exam questions; math/reasoning and image-related questions problematic (images excluded).",
            "uuid": "e7381.5",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM examinee (Llava)",
            "name_full": "Llava-v1.6-Mistral-7b-hf (Llava) used as examinee on SciEx",
            "brief_description": "Llava, a multimodal open-source model built on Mistral-7B, was evaluated as an examinee; images concatenated into a single image with padding and passed to the model, and Llava produced textual answers for exam questions including image-related prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llava (Llava-hf/Llava-v1.6-Mistral-7b-hf)",
            "model_size": "7B (as listed in Table 2)",
            "model_type": "Open-source, multimodal (vision-enabled), instruction-tuned variant",
            "scientific_domain": "Computer science exam subdomains (including image-related questions)",
            "simulation_task_description": "Simulate student answering freeform questions including interpretation of images (images concatenated into one with blank padding when multiple).",
            "prompting_strategy": "Same-language prompts; because Llava handles only one image at a time, multiple images concatenated with blank padding; asked to describe drawings in text where required.",
            "evaluation_metric": "Expert human grading normalized to 0–100%.",
            "reported_accuracy": "21.5% average exam score (expert-graded); German grade 4.3. On English text-only subset: 42.4% (Table 8).",
            "baseline_accuracy": "Student average 45.3%; Llava underperforms students on many image-related tasks in this benchmark.",
            "factors_reported": [
                "Multimodal handling limitations (single-image handling required concatenation hack)",
                "Model capacity relative to proprietary multimodal models",
                "Language and task type (image tasks challenging despite multimodality)"
            ],
            "experimental_conditions": "Open-source model from Hugging Face; image concatenation strategy applied; inference via llama.cpp/default sampling on A6000 GPU.",
            "limitations_or_failure_modes": "Despite being multimodal, Llava falls behind students on image-related questions; struggles with drawing tasks and may produce low-quality image descriptions.",
            "uuid": "e7381.6",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM-as-a-judge (GPT-4V grader)",
            "name_full": "GPT-4V used as automatic grader (LLM-as-a-judge) for SciEx",
            "brief_description": "GPT-4V was used to automatically grade examinee answers by outputting a single score between 0 and the maximum, given question, answer, and reference; chain-of-thought was requested and few-shot examples/reference answers were optionally included; performance compared against expert grading.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V (gpt-4-vision-preview) as grader",
            "model_size": null,
            "model_type": "Proprietary, multimodal, instruction-tuned with chain-of-thought requested in prompts",
            "scientific_domain": "Educational assessment of computer science exams (grading subdomain).",
            "simulation_task_description": "Assign numeric grades to freeform exam answers (simulate human exam grader) on the same scale as experts.",
            "prompting_strategy": "LLM-as-a-judge prompt: include question, answer, maximum score; optionally include reference answer; request chain-of-thought reasoning before giving the score; use few-shot examples (0/1/2-shot) with three example-selection strategies: same question (different examinee), same exam (different question), different exam; experiments with/without reference answers.",
            "evaluation_metric": "Pearson correlation on normalized scores (primary); RMSE on original scales (secondary). Also measured behavior statistics (e.g., propensity to copy example grades, proportion of full-point assignments, precision of full points).",
            "reported_accuracy": "Exam-level Pearson correlation to expert grading = 0.948 (best setting). Question-level Pearson correlations up to ~0.7 depending on shot/ref settings. RMSE reported in appendix (not listed in main text).",
            "baseline_accuracy": "Open-source Llama3 grader exam-level Pearson = 0.883; Mixtral grader performed worse (numerical exam-level Pearson not explicitly given in main text).",
            "factors_reported": [
                "Including reference answers improves grader performance",
                "Few-shot examples improve performance; relevance of examples matters (same exam or different exam vs same question)",
                "Grader tendency to copy the grade of example when exact question text duplicated in shot (observed for Mixtral and GPT-4V)",
                "Grader-specific biases (Mixtral tends to give full points in zero-shot)",
                "Multimodal capability did not harm GPT-4V performance on image-related questions in this dataset (contrasts with prior work)",
                "Which examinee is graded affects correlation (some graders inconsistent across examinees)"
            ],
            "experimental_conditions": "Prompts asked for chain-of-thought before final grade; tried 0/1/2-shot settings and with/without reference answer; example-selection strategies (same question, same exam, different exam) tested; images included for multimodal graders; APIs (OpenAI) for GPT-4V used; default sampling settings. Few-shot example tuples contained question, answer, and expert-provided grade.",
            "limitations_or_failure_modes": "At question level, correlations drop (~0.7); grader can copy example grades (25% copy rate for GPT-4V when single-shot with same question example); grader can be inconsistent across different examinees and tends to rank itself favorably without reference answers; some image-related grading conclusions may not generalize due to small number of image questions.",
            "uuid": "e7381.7",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM-as-a-judge (Llama3 grader)",
            "name_full": "Llama3 (MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF) used as automatic grader",
            "brief_description": "Llama3 was used as an open-source automatic grader (LLM-as-a-judge) on SciEx, receiving question, answer, maximum score, optional reference answers, and few-shot examples; it provided numeric scores and chain-of-thoughts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3 (MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF)",
            "model_size": "70B (as listed in Table 2)",
            "model_type": "Open-source, instruction-tuned (instruct variant), used for grading",
            "scientific_domain": "Educational assessment (grading) for computer science exams.",
            "simulation_task_description": "Assign numeric grades to freeform exam answers simulating human graders.",
            "prompting_strategy": "Few-shot grading prompts with or without reference answers; requested chain-of-thought; example-selection strategies (same question/same exam/different exam) tested; 0/1/2-shot variations evaluated.",
            "evaluation_metric": "Pearson correlation to expert grading on normalized scores (primary); RMSE on original scales (appendix).",
            "reported_accuracy": "Exam-level Pearson correlation to expert grading = 0.883 (reported in text). Question-level Pearson correlations varied by setting (e.g., 0.452–0.672 across shot/ref settings for text-only questions in Table 4).",
            "baseline_accuracy": "Compared to GPT-4V grader (0.948 exam-level) and Mixtral grader (worse performance); also compared across shot/ref conditions and to expert grades.",
            "factors_reported": [
                "Including reference answers and few-shot examples improves performance",
                "Llama3 copied example grades less often (13%) compared to other graders",
                "Performs better on easier questions (higher correlations on easy vs hard)",
                "Example relevance (same question vs same exam vs different exam) affects utility"
            ],
            "experimental_conditions": "Open-source checkpoint used with llama.cpp for inference; quantized (4-bit as in Table 2); experiments on A6000 GPU; multiple shot/ref settings tested; attached chain-of-thought requested.",
            "limitations_or_failure_modes": "Lower exam-level performance than GPT-4V; per-question correlations lower (around 0.5–0.6 depending on setting); some dependence on example selection; unable to directly handle images in some grader configurations (consequence of model input limitations).",
            "uuid": "e7381.8",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SciEx - LLM-as-a-judge (Mixtral grader)",
            "name_full": "Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1) used as automatic grader",
            "brief_description": "Mixtral was evaluated as an open-source LLM-as-a-judge for automatic grading on SciEx and found to have biased grading behaviors (e.g., overassigning full points in zero-shot).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mixtral (Mistralai/Mixtral-8x7B-Instruct-v0.1)",
            "model_size": "8x7B (as listed in Table 2)",
            "model_type": "Open-source, instruction-tuned",
            "scientific_domain": "Educational assessment (grading) for computer science exams.",
            "simulation_task_description": "Assign numeric grades to freeform exam answers simulating human graders.",
            "prompting_strategy": "Grading prompts with options: 0/1/2-shot, with/without reference answers; chain-of-thought requested; example-selection strategies tested.",
            "evaluation_metric": "Pearson correlation to expert grading (question-level and exam-level), RMSE in appendix, and diagnostic measures like proportion of full-point assignments and precision for full points.",
            "reported_accuracy": null,
            "baseline_accuracy": "Compared against Llama3 (exam-level 0.883) and GPT-4V (exam-level 0.948); Mixtral reported worse performance than these although exact exam-level Pearson not given in main text.",
            "factors_reported": [
                "Tendency to give full points in zero-shot (67.6% of answers got full points in 0-shot)",
                "Precision on giving full points low (0.181)",
                "Performance improved with more examples and when reference answer included",
                "Example-selection biases (copies grade of example ~25% when same-question shot without reference)"
            ],
            "experimental_conditions": "Open-source model used with llama.cpp inference; grading prompts requested chain-of-thought; multiple shot/ref settings tested; diagnostic statistics (full-point rates, copying rates) measured.",
            "limitations_or_failure_modes": "Strong bias to assign full points in zero-shot; low precision for full points; copies grades from examples when question text duplicates example; overall worst grader among those evaluated in main text.",
            "uuid": "e7381.9",
            "source_info": {
                "paper_title": "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "A closer look into using large language models for automatic evaluation",
            "rating": 2,
            "sanitized_title": "a_closer_look_into_using_large_language_models_for_automatic_evaluation"
        },
        {
            "paper_title": "Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
            "rating": 2,
            "sanitized_title": "mllmasajudge_assessing_multimodal_llmasajudge_with_visionlanguage_benchmark"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "M3Exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
            "rating": 1,
            "sanitized_title": "m3exam_a_multilingual_multimodal_multilevel_benchmark_for_examining_large_language_models"
        },
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 1,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "ARC-DA (direct-answer conversion of science questions)",
            "rating": 1,
            "sanitized_title": "arcda_directanswer_conversion_of_science_questions"
        }
    ],
    "cost": 0.01912725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</p>
<p>Tu Anh Dinh 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Carlos Mullov 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Leonard Bärmann 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Zhaolin Li 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Danni Liu 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Simon Reiß 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Jueun Lee 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Nathan Lerzer 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Jianfeng Gao 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Fabian Ternava 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Tobias Röddiger 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Alexander Waibel 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Tamim Asfour 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Michael Beigl 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Rainer Stiefelhagen 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Carsten Dachsbacher 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Klemens Böhm 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>Jan Niehues 
Karlsruhe Institute of Technology
KarlsruheGermany</p>
<p>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading
EC7343A5BA659673453060C8AE6A9918
With the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains.One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs.Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx -a benchmark consisting of university computer science exam questions, to evaluate LLMs' ability on solving scientific tasks.SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams.We evaluate the performance of various state-of-the-art LLMs on our new benchmark.Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance.Therefore, we provide human expert grading of the LLM outputs on SciEx.We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4% exam grade on average.We also provide detailed comparisons between LLM performance and student performance on SciEx.To enable future evaluation of new LLMs, we propose using LLM-as-ajudge to grade the LLM answers on SciEx.Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs) have proven their usefulness across a wide range of tasks, from conversational agents to code generation (Rajkumar et al., 2022;Abbasian et al., 2023;Liao et al., 2023).Given the fast pace of development in the field, with an increasing number of LLMs being trained and released, it is important to have indicators of LLM performance on different domains.This can be achieved by establishing evaluation benchmarks that assess the capabilities of LLMs across diverse use cases.</p>
<p>One use case of LLMs is to handle scientific tasks.Some previous works have introduced benchmarks containing questions on science topics (Welbl et al., 2017;Lu et al., 2022;Gilson et al., 2022;Schubert et al., 2023;Zhang et al., 2024).However, these benchmarks are limited to multiple-choice questions.This restricts the variability of questions, such as instruction-follow ones like "write a mathematical proof for this statement ...".Additionally, it is difficult to ask certain types of questions in a multiple-choice way without including the answer in the question itself.Multiplechoice benchmarks therefore create a gap between testing and actual usage, since they only evaluate whether the LLMs choose the correct answer, whereas in real life, the users are more likely to ask open-ended questions to the LLMs.In contrast, some other works have introduced freeform question benchmarks.These works either convert multiple-choice questions to freeform questions (Bhakthavatsalam et al., 2021), or focus on a specific type of problem such as answering questions related to a paper (Dasigi et al., 2021), thus still limiting the variability of the questions.</p>
<p>In this paper, we introduce a new benchmark, termed SciEx (Scientific Exams), designed to evaluate this capability.Inspired by the way students are evaluated in university, we created the benchmark by evaluating the performance of LLMs on university computer science exams.SciEx's questions are in various formats, from multiple choice to open-ended, thus making it suitable to evaluate LLM's capabilities of generating free-text answers that fit the requirements of the questions.It is multilingual, containing exams in both German and English.It is multimodal, as exam questions can also contain figures.The set of questions is a good mix of different difficulty levels since they are designed for university exams.This enables us to evaluate LLMs on different levels, and we found that stronger LLMs tend to perform better on more difficult questions.</p>
<p>Unlike the previous multiple-choice benchmarks, the questions in SciEx are freeform, making it nontrivial how to evaluate the LLM output.Therefore, we make use of expert grading, i.e., having the lecturers grade the LLM output the same way they would grade student answers.We also ask the experts to perform qualitative analysis of the LLM output.With expert grading, we provide a highly reliable way of evaluating LLMs, which is more reliable than previous work that uses crowdsourced evaluation.Expert grading by lecturers also provides an opportunity to compare LLMs' performance to university student performance in a similar setting.We find that the stronger LLMs, i.e., Claude and GPT-4V, are able to outperform the student average.However, they are still far from perfect, achieving only 59% across SciEx exams.</p>
<p>Since new LLMs are constantly being released, we cannot fully rely on expert grading for evaluation.Therefore, we provide an automatic grading scheme by using LLM as a judge so that future LLMs can also be evaluated on SciEx.Interestingly, we find that, although LLMs do not perform too well as examinees, they perform well as graders, achieving over 0.948 Pearson correlation to expert grading in the best setting.</p>
<p>In summary, our contributions are as follows:</p>
<p>• SciEx1 -a freeform, multimodal, multilingual benchmark consisting of university computer science exams, outputs of various LLMs on the exams, and expert grading of the LLM output.</p>
<p>• Detailed quantitative and qualitative analysis comparing LLM to student performance.</p>
<p>• Automatic grading with 0.948 Pearson correlation to expert grading</p>
<p>Related Work</p>
<p>General-Purpose LLM Benchmarks In order to rank different LLMs, there are several commonly used public benchmarks.For example, Zheng et al.</p>
<p>(2024) introduced MT-bench and Chatbot Arena.MT-bench is a multi-turn question set; and Chatbot Arena is a crowdsourced battle platform for LLMs where the users can ask their questions and vote for the better LLM answer.Another benchmark is MMLU (Hendrycks et al., 2020), which is a multitask dataset covering multiple domains such as mathematics, US history and law.</p>
<p>Scientific LLM Benchmarks</p>
<p>To specifically focus on the scientific domain, previous studies have established benchmarks, such as SciQ (Welbl et al., 2017) and ScienceQA (Lu et al., 2022), which feature questions spanning various scientific subjects.</p>
<p>More recent works have focused on benchmarking LLMs on solving exam questions on some narrow science domains such as medical (Gilson et al., 2022) or neurology (Schubert et al., 2023).M3Exam (Zhang et al., 2024), in contrast, provides exam questions to benchmark LLMs which span over multiple topics and multiple educational levels (primary, middle, and high school).However, all benchmarks mentioned above are limited to multiple-choice questions.While this simplifies the evaluation process, it does not allow us to assess the LLMs' capability to generate natural text.</p>
<p>Other studies have instead provided scientific benchmarks with open-ended questions.Some examples are Qasper (Dasigi et al., 2021) and ARC-DA (Bhakthavatsalam et al., 2021).However, Qasper only focuses on questions about NLP papers rather than on general computer science topics.ARC-DA is closer to our work, since it contains open-ended questions taken from science exams and quiz sources.However, these are created by converting questions that were originally multiple-choice, thus not covering certain types of typical freeform questions (e.g.those that require mathematical proofs, or long explanations).</p>
<p>Different from these works, SciEx is created from university computer science exams, thus naturally providing diversity in the types of questions as well as having freeform format.</p>
<p>Freeform Answer Evaluation Compared to benchmarks with multiple-choice questions, evaluating LLMs' performance on freeform questions is not straightforward.Similar to evaluation conditions in tasks such as machine translation or summarization, there are multiple correct answers, or multiple ways to express a correct answer for a single input.Therefore, it is insufficient to evaluate a model's output by comparing it to a gold standard answer.Ideally, in these cases, we can evaluate by human judgment.For example, the ARC-DA benchmark (Bhakthavatsalam et al., 2021) uses a crowdscoring pipeline for evaluation.Chatbot Arena (Zheng et al., 2024) also uses crowdsourcing, where the users vote between pairs of LLM output.However, human evaluation is inherently non-scalable.Therefore, previous works have used automated metrics.Some traditional metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) compare the model's output to some gold-standard answer on the surface level, i.e., word matching.More advanced metrics, such as s BERTScore (Zhang* et al., 2020), BLEURT (Sellam et al., 2020), and BARTScore (Yuan et al., 2021), are model-based, thus being able to evaluate answers on the semantic level.</p>
<p>One recent approach is to use LLMs for evaluation, termed "LLM-as-a-judge".Liu et al. (2023); Chiang and Lee (2023a); Zheng et al. (2024) find that, although still prone to biases, LLM-as-ajudge for textual modality has high agreement with human scoring when a strong judge LLM is used.However, when including images, Chen et al. (2024) find that the performance of LLMas-a-judge is no longer as well correlated to human judgment.Nevertheless, LLM-as-a-judge is a promising way to perform scalable evaluation.</p>
<p>In our work, we make use of LLM-as-a-judge for automatic grading of LLM answers on SciEx exams, and find that they have good correlation to human expert grading on both text-only and imagerelated questions.</p>
<p>The SciEx Benchmark</p>
<p>The components of SciEx are as follows.</p>
<p>Univeristy Exams SciEx contains university computer science exams in a unified JSON format.The exams are taken from the following computer science courses at the Karlsruhe Institute of Technology from the 2022/2023/2024 semesters:</p>
<p>• Natural Language Processing (NLP) LLM-Generated Answers SciEx contains answers produced by 7 LLMs on the exam questions.The details of the LLMs are shown in Table 2.In Table 2, only Llama3 was not used to solve the exam, since it was released at a later point of conducting this paper.In total, we obtained 1120 question-answer pairs.Expert Grading and Automatic Grading Each question-answer pair is assigned a score by an expert.In order to guide future work to evaluate new LLMs on SciEx without relying on human expert grading, we also provide automatic grading generated by Mixtral, Llama3 and GPT4V.</p>
<p>Data Creation</p>
<p>The data creation process is as follows.</p>
<p>Exam Collection We collect university exams from different courses.We additionally ask the lecturers to provide us with the reference answers, the difficulty level of each question, and the average student grades on each question.</p>
<p>Exam Formatting</p>
<p>We convert every exam into a unified JSON format.Each exam includes a list Full name</p>
<h1>Params Quant. Handle Image Proprietary Claude Claude-3-opus-20240229 - - yes GPT-4v gpt-4-vision-preview - - yes GPT-3.5 gpt-3.5-turbo-0125 - - no Open source Llama3 MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF 70B 4 bit no Mixtral Mistralai/Mixtral-8x7B-Instruct-v0.1 8x7B 5 bit no Qwen Qwen/Qwen-72B 72B 2 bit no Mistral Mistralai/Mistral-7B-Instruct-v0.2 7B - no Llava Llava-hf/Llava-v1.6-Mistral-7b-hf 7B - yes</h1>
<p>Table 2: Details of the LLMs in consideration.</p>
<p>of questions, where each question includes an index, its content, and potentially path to any related images.An example is shown in Appendix B.</p>
<p>LLM-Generated Answers</p>
<p>We pass the exams to the LLMs listed in Table 2 (except Llama3 due to later release), one question at a time.Questions that contain images are handled differently depending on the LLM.For the text-only LLMs, we exclude the images and only pass the question text to the models.For Llava, since it is trained to handle only 1 image at a time, we concatenate the images into one, with blank padding around the images as separators before feeding it to the model.Claude and GPT-4V can take multiple images, however, there is no pre-defined way of referencing the image within the text.In our work, we reference the image by mentioning the image caption within the question text, and add the text caption to the image.</p>
<p>Since the considered LLMs can only output text, for questions asking to draw on images, we ask the LLMs to describe in text what should be drawn.</p>
<p>The detailed prompts for LLMs to generate the answers are shown in Appendix C.</p>
<p>Expert Grading</p>
<p>We then give the LLM answers back to the lecturers, who proceed with grading the LLM output the same way they would grade student answers.We anonymized the LLMs' names in order to avoid bias during exam grading.We also build a user interface for collecting the grades (see Appendix E for more details).</p>
<p>With expert grading, the evaluation of the LLM output is highly reliable.Most importantly, the expert graders are generally the ones who designed the exam questions.We additionally ask the expert graders to provide their comments on the LLM output to further understand LLMs' behaviors when solving the exams.</p>
<p>Automatic Grading</p>
<p>In addition to expert grading, we also provide automatic grading using LLM-as-a-judge, so that we can evaluate future LLMs on SciEx.We use the stronger models, i.e., Mixtral, Llama3 and GPT-4V, to conduct the grading.Given a tuple containing question, answer, and maximum score, we ask the LLMs to output a single score between 0 and the maximum.We include reference answers to the grading prompt.We ask the LLMs to provide chainof-thought reasoning (Wei et al., 2022;Chiang and Lee, 2023b) before giving the grade.We also include examples for grading in the prompt, so-called few-shot judge (Zheng et al., 2024).Each example is a tuple consisting of a question, an answer, and the expert-provided grade.We try out different settings to select the examples, as described below.</p>
<p>Let's say we want to grade Question M from Exam A, answered by Examinee X.Then the shot examples can be chosen in one of the three ways: Examinee Y and Question N are chosen randomly.</p>
<p>For Exam B, we opt to select the exams that do not heavily require images for simplicity in the prompt.</p>
<p>Intuitively, the example-selection settings above have decreasing levels of relevance to the actual grading query, but increasing easiness to collect.The detailed prompts for LLMs to grade answers are shown in Appendix D.</p>
<p>Experiments</p>
<p>In this section, we describe our experiments and results.For prompting the proprietary LLMs, we use their APIs, namely OpenAI2 and Anthropic3 .For the open-source models, we obtain model checkpoints from the Huggingface4 model hub.We perform inference with the LLMs using llama.cpp5with the default sampling strategy.The experiments with open-sourced models are conducted on an NVIDIA RTX A6000 GPU with 48GB VRAM.</p>
<p>For our analysis, we consider the exam-level and question-level grades.An exam-level grade is the sum of the grades of all questions in the exam.</p>
<p>Quantitative Analysis</p>
<p>We analyze the performance of the LLMs on SciEx with expert grading.For both exam level and question level, we normalize the grade to be between 0 and 100%, since they have different scales.The normalization is done by taking the scores obtained by the examinee divided by the maximum score possible per exam/question, where the maximum scores possible are predefined by the lecturers.</p>
<p>We also report on the German grade scale.In the German scale, the grades range from 1.0 to 5.0, where 1.0 is the highest grade and 4.0 is the passing threshold.The detailed mapping from the scores to the German grade scale is defined by the lectures, adjusted based on the overall performance of the students taking the exams.</p>
<p>We compare the performance of the LLMs to students from different aspects: language, difficulty level, and modality, i.e., questions with or without images.</p>
<p>General Observations</p>
<p>SciEx is Challenging The performance of the LLMs on SciEx provided by expert grading is shown in Table 3.The bigger-sized LLMs (Claude, GPT-4V, GPT-3.5, Mixtral and Qwen) can achieve exam passing grades (i.e., grades that are better than 4.0 in the German scale).However, the bestperforming model (Claude) only achieves 59.4% of the maximum points, which is far from perfect.</p>
<p>Compared to the student average, most LLMs have worse performance.Only the strongest proprietary LLMs, i.e., Claude and GPT-4V, can achieve grades that are better than the students'.</p>
<p>Influential Factors</p>
<p>Difficulty Levels Figure 1a shows the influence of the difficulty level on the examinee grades.As can be seen, the student performance aligns with the difficulty level of the questions: they perform better on easier questions.Some weaker LLMs, e.g., Mixtral, Qwen, GPT-3.5, Llava, align with the students.However, the stronger LLMs, i.e., Claude and GPT-4V, perform better on harder questions.This is an indication that difficulty levels from human perspective do not always align with LLMs' perspective.This is also confirmed by looking at the Pearson correlations between the LLMs' grades and the student average grades on the question level.These correlations are between 0.4 and  0.6, indicating that LLM grades and the student grades are not highly correlated.One possible explanation for the mismatch between LLMs performance and question difficulty level could be that, in some exams, there can be some "template questions", i.e., questions that are repeated over the years, where students can just learn by heart how to systematically solve them.While this would be marked as "easy" by the lecturer, it might not be as easy for the LLMs, since the LLMs are not previously exposed to these "template questions".Another potential explanation is that math-type easy questions are hard for the LLMs, while long-text hard questions are easy for them.</p>
<p>In Figure 1b, we plot the difference between LLM scores and student scores.The stronger LLMs, i.e., Claude and GPT-4V, outperform the students the most on hard questions.Weaker LLMs, on the other hand, generally fall behind students the most on hard questions.Looking at each difficulty level independently, we observe that the ranking of the LLMs changes across different levels.This aligns with the findings made by Li et al. (2024), where they show that the LLM rankings change on a subset of evaluation prompts that are artificially labeled as hard.ence between LLM and student scores.Recall that for the text-only LLMs, we exclude the images and only pass the question text to the models.Trivially, the text-only LLMs perform poorly on the imagerelated questions.The strong, multi-modal LLMs, i.e., Claude and GPT4, outperform the students on both image-related questions and text-only questions, but the performance gap is still larger for text-only questions.Llava, although can handle images, still falls behind student performance by a large margin on image-related questions.This shows that LLMs' image-handling capability is still not as advanced as for text.</p>
<p>Text-only versus Image-related Questions</p>
<p>Language Figure 3 shows the influence of languages on the difference between LLM scores and student scores.When the questions are in English, all LLMs, except for GPT-3.5, outperform the student average.However, for German, either the LLMs outperform students by a smaller gap, or fall behind student performance.It can be concluded that LLMs are still superior in English than other languages like German, although German can be considered a high-resource language.Since some models are not made to deal with images, or with languages other than English, we additionally analyze LLMs' performance on text-only and English-only questions.On this subset of questions, the grades obtained by the models are generally better, and more models would outperform the student average.More details can be found in Appendix F.</p>
<p>Qualitative Analysis</p>
<p>In this section, we summarize the observations made by the graders while grading the LLMs.</p>
<p>General Behaviours</p>
<p>The graders observed some common behaviors made by the LLMs.Some solutions of the LLMs were good language-wise but low-quality content-wise.For students, good language usually correlates strongly with good content.The LLMs tend to output lengthy answers, since, unlike the students, they do not have a time constraint.Some LLMs even ignore when the question specifies that they should "answer briefly".There are also some failure cases, although not frequent:</p>
<p>(1) Claude refuses to answer the question with "I apologize, but I do not feel comfortable providing answers related to ..." or (2) some LLMs get stuck in decoding loops.Sometimes, instead of answering the question, LLMs give some text that is (or seems) related to the task; rephrase the task; or describe how a task of this nature may be approached in general.</p>
<p>Knowledge-type Questions On some exams such as AI2, DL4CV2, DLNN, CG, questions which students can answer by learning the lecture content by heart are quite easy for the LLM.For the DL4CV2 exam, very specific questions about neural network architectures which are covered in our lecture seem to be quite common knowledge in the LLMs, which might be due to those papers being included in the training data.However, for other exams such as HCI, the models lacked specific course context, which was important for answering many theoretical and open-ended questions.</p>
<p>Math-related Ability</p>
<p>The LLMs tend to fail on the math-related questions, even the basic ones.For example, they miscount the number of words in a piece of text, or have trouble comparing numbers.For questions that require writing mathematical proof in the TGI exam, all LLMs except for GPT-4V and Claude failed.For GPT-4V and Claude, they are able to pass the TGI exam.Their mistakes are more in line with those that students would make.That is, they are often not successful when making actual proof, and the points where the proof breaks sometimes are the same as the students.Even the better models handle simple geometry questions poorly and/or struggle to follow the instructions of a simple algorithm.</p>
<p>Reasoning Ability The LLMs do not perform well on questions that require deep thinking and reasoning.For questions of the type "is this statement true or false; reason for your solution", the LLMs often said "true" and then just repeated the statement or reasoned for the opposite of their claim.This is a similar behavior often seen in students.Sometimes they make self-contradicting arguments: making a statement and then providing arguments for the other side.</p>
<p>Image Handling GPT-4V, Claude and Llava can handle images.However, only GPT-4V and Claude have reasonable performance.When the question is about drawing on top of the figures, sometimes the LLMs successfully describe in words what needs to be drawn, but occasionally they just hallucinate a non-existing figure file path.</p>
<p>Automatic Grading</p>
<p>In this section, we evaluate the performance of LLM-as-a-judge approach to automatic grading.We use the expert grades as the gold standard to evaluate automatic graders.We use Pearson correlation on the normalized scores as our metric.Since the LLMs are asked to provide the scores on the same scale as the expert scores, we also provide the Root Mean Squared Error (RMSE) on the originally-scaled scores as a secondary metric.Note that RMSE would correctly put more weight on the questions that have more points, however, it is not as easily interpretable as the Pearson correlation.Therefore, we only report RMSE in Appendix G.2.The main results are discussed as follows.</p>
<p>General</p>
<p>LLMs Perform Well as Graders On the exam level, LLM-as-a-judge performs well for automatic grading.The best Pearson correlation to expert grading on the exam level, at 0.948, is achieved by GPT-4V.The open-source Llama3 achieves 0.883 Pearson correlation to expert grading.</p>
<p>The LLM ranking based on average exam-level grades provided by the GPT-4V grader in comparison to expert grading is shown in Table 5.As can be seen, the ranking is quite identical, except for Mixtral and Qwen's positions being swapped.The high correlations between expert grading and LLM-as-a-judge grading indicate that, although being far from perfect in solving SciEx exams (discussed in Section 4.1.1),the stronger LLMs are quite reliable for grading the exams.This is useful since we would have to rely less on expert grading to evaluate newly developed LLMs' performance on SciEx.The details of graders' performance under different settings on the exam level are in Appendix G.1.</p>
<p>On the question level, the performance of LLMas-a-judge is shown in Table 4.The highest Pearson correlation to expert grading achieved by the LLMs is now around 0.7, which is lower than on the exam level, but still quite high.Surprisingly, the performance of GPT-4V on grading image-related questions is quite comparable to grading text-only questions.This contradicts the finding made by Chen et al. (2024).This could potentially be due to the small number of image-related questions in SciEx, thus the results might not be generalizable.</p>
<p>Few-shot and References Help</p>
<p>The performance of the graders on the question level is shown in Table 4.We observe that adding examples (shots) and adding reference answers in the prompt generally increases the performance of the LLM graders.GPT-4V is the strongest grader, followed by Llama3 and Mixtral.This shows that proprietary LLMs are still stronger as judges, aligning with previous studies (Zheng et al., 2024).</p>
<p>Grader-specific Behaviours</p>
<p>Mixtral Grader Tends to Give Full Points As can be seen from Table 4, Mixtral has the worst performance on grading the exams.We observe that Mixtral tends to give full points to the answers.Without reference and without examples (0-shot), the portion of answers where Mixtral outputs full points is 67.6%, significantly higher than Llama3 and GPT-4V, at 19.1% and 15.1%, respectively.As a result, Mixtral's precision on giving full points, at 0.181, is much lower than Llama3 and GPT-4V, at 0.380 and 0.527 respectively.As we add more examples and/or add the reference answer to the prompt, the problem is lessened.More details can be found in Appendix G.3.</p>
<p>Mixtral and GPT-4V Copy Grade of Example</p>
<p>For Mixtral and GPT-4V graders, when having one example (shot) from the same question in the prompt without reference, the performance is worse than having the example from the same exam or from a different exam.We hypothesize that this is due to these graders tend to copy the grades of the examples when having a chunk of duplicated text (i.e., the question description) in the example.This is verified when looking at the statistics: Mixtral and GPT-4V copy the grade of the example 25% of the time, whereas Llama3 does it 13% of the time.As a result, Llama3 can best make use of examples from the same question.The problem is reduced when having more than 1 shot or when the reference answer is included.</p>
<p>Influential Factors</p>
<p>Different Examinees As can be seen in Table 6, GPT-4V grader has better performance than others, but is more inconsistent: it does worse on grading some LLMs, especially Claude.This is potentially due to Claude being a better examinee than GPT-4V itself, as shown in Section 4.1.1.When using the scores from GPT-4V grader to rank the LLMs, we find that, without reference answer, GPT-4V always ranks itself higher than Claude.This emphasizes the importance of reference answers for grading, especially when the grader is weaker than the examinee.</p>
<p>Conclusion</p>
<p>In this paper, we proposed SciEx -a benchmark consisting of scientific university exams, along with expert grading and automatic grading, to evaluate the abilities of LLMs on science topics.SciEx is multilingual, multi-modal, and contains a variety of free-form questions.Our experiments show that SciEx is still quite challenging for current LLMs, where the best LLM can only achieve 59.4% of the exam score on average.Despite that, the LLMs perform well as graders, achieving 0.948 Pearson correlation to the expert grades.This is a promising observation, since we can use strong LLMs for automatic grading of new LLM examinees on SciEx, rather than relying on expert grading.We encourage the research community as well as LLM developers and users to make use of SciEx for evaluating LLMs' scientific capabilities.</p>
<p>Limitations</p>
<p>There are certain biases that can occur for SciEx.Firstly, the LLMs do not have time pressure.Therefore, they can output longer answers, which helps them get better grades, as there is a higher likelihood that something will be correct.Secondly, the grading process can not be fully anonymized.It is not easy to mix the LLM answers with student answers for the lecturer to grade, since student answers are usually handwritten.Additionally, the LLMs' answers content itself might also be easily distinguishable from the students', since the LLMs tend to, e.g., give longer answers or repeat the questions.Therefore, the lecturers know when they are grading an LLM, thus can bias the score they give.Thirdly, the comparison between the LLMs and the students might be unfair, since the students studied the centralized course material specifically for the exams, while this is not the case for the LLMs.Lastly, due to the reliance on expert resources, the size of SciEx is quite small compared to other scientific benchmarks.</p>
<p>Ethics</p>
<p>Our work makes use of student statistics to compare against LLMs' performance.However, we only use the average of the student grades, without disclosing any individual student's information.The student answers are never directly used, as we only ask for the average graders from the lecturers.Automatic grading, regardless of the high correlation to expert grading, can still be imperfect.</p>
<p>We are not suggesting to use LLMs to evaluate students, but to evaluate new models coming out when it is not possible to do human evaluation.</p>
<p>Regarding data consent, we had group meetings and email exchanges to come to an agreement from all lecturers that the data would be made public under the CC BY-NC-SA 4.0 license.</p>
<p>A Exam Description</p>
<p>The overall description of each exam in SciEx is as follows:</p>
<ol>
<li>
<p>Natural Language Processing (NLP): exam contains questions about word and sequence representation, language modeling, and pretrained models.</p>
</li>
<li>
<p>Advanced Artificial Intelligence (AI2): exam contains questions about natural language processing, signal processing, automatic speech recognition and cognitive robotics.</p>
</li>
</ol>
<p>B Exam Formatting</p>
<p>Originally, exams were in different formats, depending on their creator.We convert the exams into JSON format, with file paths to images if any.</p>
<p>An example is shown in Figure 4.</p>
<p>C LLM Answer Generation Prompts</p>
<p>We provide the prompt in the same language as the exam question to the LLMs to generate answers.The English prompt is shown in Figure 5, and the German prompt is shown in Figure 6.</p>
<p>D LLM Grader Prompts</p>
<p>We provide the prompt in the same language as the exam question to the LLMs to perform automatic grading.The English prompt is shown in Figure 7, and the German prompt is shown in Figure 8.</p>
<p>E User Interface for Expert Grading</p>
<p>We instructed the expert grader to use our user interface (UI) for grading.Figure 9 shows the open page of the UI, where the grader can choose their exam and enter their password.Figure 10 shows the page for the grading, where the expert is shown with the question, the LLM answer to the question, and a text box to enter the grade.The expert can choose the examinee to grade from the dropdown on the left-hand side.Figure 11 shows the page to enter additional information about the exam questions, including the maximal achievable score, average student performance, gold answer, and difficulty level.</p>
<p>Once the data is collected, we also ask the experts and have their consent to make the data public.</p>
<p>F Performance on Text-only, English-only Questions</p>
<p>The performance of the LLMs on text-only, English-only questions is shown in Table 8.Qn this  subset of questions, besides GPT-4V and Claude, we can see that Mixtral and Qwen also have better performance than the student average.G Grader Performance</p>
<p>G.1 Pearson Correlation on Exam Level</p>
<p>The performance of LLM-as-a-judge for automatic grading on the exam level is shown in Table 9.Note that Mixtral and Llava3 graders have disadvantage since they cannot take image input for image-related questions.</p>
<p>G.2 RMSE on Question-level</p>
<p>Since the LLM graders are asked to output the scores in the original scale, RMSE would be the most informative metric, since it also reflects the importance of the questions that have higher maximum scores.The LLM graders' performance in RMSE is shown in Table 10.</p>
<p>G.3 Performance on Giving Full Points</p>
<p>The performance of the LLM graders on assigning full points to the answers is shown in Table 11.</p>
<p>You are a university student.Please answer the following JSON-formatted exam question.</p>
<p>The subquestions (if any) are indexed.The provided figures (if any) each contains its path at the bottom, which matches the path provided in the JSON.</p>
<p>Please give the answers to the question and subquestions that were asked, and index them accordingly in your output.You do not have to provide your output in the JSON format.If you are asked to draw on the figure, then describe with words how you would draw it.</p>
<p>Please provide all answers in English.</p>
<p>Here is the question: <input text></p>
<p>Difference between LLM scores and student scores.</p>
<p>Figure 1 :
1
Figure 1: Question-level scores grouped by difficulty.</p>
<p>Figure  2shows the influence of images on the differ-</p>
<p>Figure 2 :
2
Figure 2: Difference between LLM scores and student scores, question-level, grouped by with/without images.Only Claude, GPT-4V and Llava can handle images.</p>
<p>Figure 3 :
3
Figure 3: Difference between LLM scores and student scores, question-level, grouped by languages.</p>
<p>(a) Exam in original PDF format.(b) Exam converted to JSON format.</p>
<p>Figure 4 :
4
Figure 4: Exam question before and after being converted to JSON format.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Answer generation prompt in English.</p>
<p>Table 1 :
1
Question-level statistics for SciEx.
• Advanced Artificial Intelligence (AI2)• Deep Learning and Neural Networks (DLNN)• Deep Learning for Computer Vision(DL4CV2)• Human-Computer Interaction (HCI)• Databases (DBS) for the years 2022 and 2023</p>
<p>•</p>
<p>Same question: Select examples from the same Question M from Exam A, but answered by a different Examinee Y.This mimics the real-life scenario where we use the expert resource to grade some answers of the same exam, then use it to guide the LLM graders.
• Same exam: select examples from a differentQuestion N from the same Exam A, Answeredby a different Examinee Y. Here the examplesare in the same domain as the question-answerpair in consideration. This mimics the real-lifescenario where, e.g., we have expert gradingon exams of the same course from previousyears to guide LLM graders.
• Different exam: select examples from a different Question N, from a different Exam B, Answered by a different Examinee Y.This mimics the real-life scenario where, e.g., we have expert grading for an exam of another course to guide the LLM graders.</p>
<p>Table 3 :
3
Average performance of LLMs, exam level.
Grade (%) ↑ German Scale ↓ProprietaryClaude59.42.4GPT-4V58.22.5GPT-3.532.83.9Open sourceMixtral41.13.5Qwen35.43.7Mistral25.94.2Llava21.54.3Student avg.45.33.1SciEx Versus Other Benchmarks The rankingof the LLMs on SciEx in Table 3 generally agreeswith other public benchmarks. However, SciExseems to be more challenging. For example, thebest LLM accuracy achieved on MMLU's varioustasks is 88.8%. The best accuracy achieved onM3Exam multiple choice questions is 72.92%. Al-though these scores are not directly comparable, itindicates that SciEx provides a more challengingtest set for future LLMs.</p>
<p>Table 4 :
4
LLM grading's Pearson correlation to expert grading on the question level.Note that there are only single scores for zero-shot, since they do not have different shot settings.
without refwith refsame question same exam diff exam same question same exam diff examText-only questionsMixtral0 shot0.2320.3111 shot0.3520.3770.3640.3950.3330.2752 shot0.3950.2990.3160.3980.2710.255Llama30 shot0.4520.6031 shot0.5730.5470.5000.6720.5810.6452 shot0.5980.5220.5460.6440.5960.575GPT-4V 0 shot0.6070.6961 shot0.6050.6790.6160.6530.6930.7012 shot0.6720.6480.6740.7170.7270.678Image-related questionsGPT-4V 0 shot0.6770.5391 shot0.6400.6610.6110.6420.5390.7492 shot0.6130.6320.6730.7120.4650.696Expert graderGPT-4V graderExaminee Avg. grade Examinee Avg. grade(%, sorted)(%, sorted)Claude59.4Claude57.7GPT-4V58.2GPT-4V56.2Mixtral41.1Qwen42.0Qwen35.4Mixtral38.2GPT-3.532.8GPT-3.538.0Mistral25.9Mistral24.6Llava21.5Llava24.2Table 5: LLM examinees ranking with expert graderand GPT-4V grader.</p>
<p>Table 6 :
6
LLM graders performance (i.e., Pearson correlation to expert grading) on different examinees.
GradersMixtral Llama3 GPT-4VClaude0.3040.4600.482GPT-4V0.3530.5280.612Mixtral0.2510.4720.564Qwen0.3510.5560.736GPT-3.50.3330.5220.697Mistral0.2910.4670.601Llava0.3870.7160.812Difficulty Levels Looking at Table 7, the weakergraders, i.e., Mixtral and Llama3, perform betteron grading easier questions. In contrast, GPT-4Vperforms better in grading harder questions.GradersMixtral Llama3 GPT-4VEasy0.3740.6020.628Medium0.2930.5240.690Hard0.2240.4960.732</p>
<p>Table 7 :
7
LLM grader's performance (i.e., Pearson correlation to expert grading) on different difficulty levels.</p>
<p>Table 8 :
8
Average performance of the LLMs on the exam level, provided by expert grading, text-only and Englishonly questions.
Grade (%) German ScaleProprietaryGPT-4V70.81.4Claude69.21.6GPT-3.547.82.9Open sourceMixtral61.22.0Qwen56.82.4Mistral48.03.2Llava42.43.5Student avg.56.52.4
We release SciEx under CC BY-NC-SA 4.0 license. Code: https://github.com/TuAnh23/SciEx. Data: https: //huggingface.co/datasets/tuanh23/SciEx.
https://platform.openai.com/
https://console.anthropic.com/
https://huggingface.co/
https://github.com/ggerganov/llama.cpp
AcknowledgmentsThis work was supported by the Helmholtz Programme-oriented Funding, with project number 46.24.01, project name AI for Language Technologies.It was also supported by funding from the pilot program Core-Informatics of the Helmholtz Association (HGF).We thank the lecturers for their contribution during the creation of the dataset: Kunyu Peng, Alexander Jaus, David Schneider, Ruiping Liu, Zdravko Marinov, Yufan Chen, Miklós Borsi, Florian Kalinke, Federico Matteucci, Fabian Richter, Bela Böhnke, Jose Cribeiro-Ramallo, Daniel Ebi, Florian Kalinke, Adrian Feilhauer, Wendy Yi, Laura Merker, Miriam Goetze, Jean-Pierre von der Heydt, Max Göttlicher, Thomas Bläsius, Marcus Wilhelm, Michael ZündorfYou are a university professor.Please grade the following exam question.The exam question, examinee's answer, correct answer, and the maximum possible score are provided in the format:The question is provided in JSON format, but the answer can be freeform text.The provided figures in the question (if any) each contain its path at the bottom, which matches the path provided in the JSON.The answer is text-only.If the question asks to draw on the figure, then the answer should contain a text description of how the drawing should be.Please provide the grade between [0, <max_score>].Please provide the reasoning for your grade.Please provide your output in the format:Below you are provided with examples on how to perform the grading: <example text> Here is your input: <input text>
Iman Mahyar Abbasian, Azimi, Ramesh Amir M Rahmani, Jain, arXiv:2310.02374Conversational health agents: A personalized llm-powered agent framework. 2023arXiv preprint</p>
<p>Think you have solved direct-answer question answering? try arcda, the direct-answer ai2 reasoning challenge. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Dalvi Bhavana, Kyle Mishra, Ashish Richardson, Carissa Sabharwal, Oyvind Schoenick, Peter Tafjord, Clark, arXiv:2102.033152021arXiv preprint</p>
<p>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun, arXiv:2402.04788Mllmas-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. 2024arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a</p>
<p>A closer look into using large language models for automatic evaluation. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.findings-emnlp.599Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023bAssociation for Computational Linguistics</p>
<p>A dataset of information-seeking questions and answers anchored in research papers. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, Matt Gardner, 10.18653/v1/2021.naacl-main.365Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>How does chatgpt perform on the medical licensing exams? the implications of large language models for medical education and knowledge assessment. Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Andrew Taylor, David Chartash, MedRxiv. 2022</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Introducing hard prompts category in chatbot arena. Tianle Li, Wei-Lin Chiang, Lisa Dunlap, 2024Published on LMSYS</p>
<p>Proactive conversational agents in the post-chatgpt world. Lizi Liao, Grace Hui, Yang , Chirag Shah, 10.1145/3539618.3594250Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Evaluating the text-to-sql capabilities of large language models. Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau, arXiv:2204.004982022arXiv preprint</p>
<p>Performance of large language models on a neurology board-style examination. Marc Cicero Schubert, Wolfgang Wick, Varun Venkataramani, JAMA network open. 6122023</p>
<p>BLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Crowdsourcing multiple choice science questions. Johannes Welbl, Nelson F Liu, Matt Gardner, 10.18653/v1/W17-4413Proceedings of the 3rd Workshop on Noisy Usergenerated Text. the 3rd Workshop on Noisy Usergenerated TextCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, Advances in Neural Information Processing Systems. 202436</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
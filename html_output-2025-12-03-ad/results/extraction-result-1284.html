<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1284 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1284</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1284</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-9834f41482ca5b87931d41af3923d3c13fa8e298</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9834f41482ca5b87931d41af3923d3c13fa8e298" target="_blank">Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The Hidden Parameter Markov Decision Process (HiP-MDP) is introduced, a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors and a semiparametric regression approach for learning its structure from data.</p>
                <p><strong>Paper Abstract:</strong> Control applications often feature tasks with similar, but not identical, dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors, and introduce a semiparametric regression approach for learning its structure from data. We show that a learned HiP-MDP rapidly identifies the dynamics of new task instances in several settings, flexibly adapting to task variation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1284.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1284.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBP-GP HiP-MDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Indian Buffet Process - Gaussian Process Hidden Parameter Markov Decision Process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semiparametric HiP-MDP agent that models families of related dynamical systems with a small number of latent, instance-specific scalar parameters: an IBP prior selects which latent factors matter per output, GPs model basis functions, and Gaussian posteriors track instance weights which are updated online for fast adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IBP-GP HiP-MDP agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Combines an Indian Buffet Process prior over binary filter variables z_{k a d} (which select relevant latent factors) with Gaussian Processes for task-specific basis functions f_{k a d}(s) and Gaussian instance-specific latent weights w_{k b}. Batch MCMC inference finds z and f (represented at pseudo-input support points); online Bayesian filtering (Gaussian updates) updates posterior over w for a new instance and the mean weight estimate is used for model-based planning and Sarsa policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian online parameter inference / Bayesian filtering (posterior updates over latent parameters) combined with model-based planning using the posterior mean (an approximate Bayes-adaptive approach); not an explicit information-gain active design objective.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Pretrain shared structure (z, f) from batches of multiple instances, then for a new instance maintain a Gaussian belief over latent weights w_{k b}; update the belief with each observed transition Δ(s) via closed-form Gaussian information updates; interpolate basis values via GP pseudo-inputs; use the posterior mean weights to build a dynamics model for planning (Sarsa + simulated planning episodes interleaved with real episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Cartpole; Acrobot</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Environments have continuous 4-dimensional state spaces, discrete small action sets (cartpole: 2 actions; acrobot: 3 actions), deterministic physics with additive Gaussian observation/noise modeled in transitions, and unknown but fixed latent parameters per instance (pole mass and length for cartpole; link masses for acrobot) — i.e., partially observable dynamics due to unobserved fixed parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Cartpole: 4 state dims {x, ẋ, θ, θ̇}, 2 discrete actions, episode length up to 300 timesteps; training used data from 7 (m,l) settings reduced to 750 support points, online tests used 50 training points and 50 test points. Acrobot: 4 state dims {θ1, θ̇1, θ2, θ̇2}, 3 discrete actions, batch training over 8 mass settings (1000 support points), evaluation across 16 mass settings with 30 repeated trials per setting; planning interleaved with 5 simulated planning episodes per real episode.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Prediction accuracy and control performance improved over baselines: Cartpole (All dimensions) mean-squared error (MSE) for IBP-GP HiP-MDP = 2.6e-04 (std reported 8.8e-04); compared baselines: Batch+Train = 2.7e-04, Batch Only = 2.7e-04, Train Only = 8.4e-03. For acrobot (All) IBP-GP HiP-MDP MSE = 2.0e-04 (2.1e-05) vs Batch+Train 3.3e-04 and Train Only 9.4e-04. In control, on acrobot the HiP-MDP reached near-optimal performance by episode 2 and by episode 5 had reached near-optimal performance overall; the average-model baseline required ~15 episodes to reach the same level.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines: Single GP on combined batch data (ignoring instance variability), GP trained only on the new-instance training points, and GP using batch+train combined. Example numbers: Cartpole (All) MSE: Batch+Train = 2.7e-04, Batch Only = 2.7e-04, Train Only = 8.4e-03. Acrobot (All) MSE: Batch+Train = 3.3e-04, Batch Only = 3.4e-04, Train Only = 9.4e-04. For control, the 'average model' (batch-only average dynamics) took ~15 episodes to approach near-optimal vs HiP-MDP ~2-5 episodes; true-model oracle performs slightly better than HiP-MDP.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Highly sample-efficient relative to baselines: in acrobot it achieved near-optimal control within ~2 episodes and was near-optimal by 5 episodes (vs ~15 episodes for the batch-average baseline). Cartpole online inference used 50 training points per instance for weight filtering and evaluated on 50 test points; batch pretraining used hundreds to ~1000 support points to learn shared bases.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Does not perform full Bayes-adaptive planning; the agent maintains a posterior over latent weights (exploitably informative) but uses the posterior mean for planning (more exploitative). Exploration arises from the learning policy (Sarsa) and the interaction procedure; there is no explicit active information-gain objective or acquisition function implemented to choose actions solely for identification of latent parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: (1) GP trained on combined batch data (average model), (2) GP using only the new-instance training points (train-only), (3) GP using batch+train combined, and (4) oracle true dynamics model for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>The IBP-GP HiP-MDP discovers a small set of latent factors (four factors in cartpole experiments) that explain inter-instance variation, yields significantly lower prediction error on dynamics components that depend on hidden parameters (e.g., ẋ and θ̈ in cartpole), and enables very fast adaptation in control tasks (acrobot) achieving near-optimal performance within a few episodes by updating only a low-dimensional belief over latent weights. Using shared bases learned from batches plus fast online filtering of instance weights yields substantial sample-efficiency gains over treating each instance independently or pooling without instance structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires an initial batch of observational data from multiple instances to learn shared basis functions; inference relies on MCMC and GP pseudo-input approximations (computationally intensive); online updates ignore basis-function uncertainty (they use mean interpolation), which is an approximation; planning uses posterior mean weights rather than full belief planning (so it is not fully Bayes-adaptive) which can limit optimal exploration for identification; performance, while near-optimal, does not match planning with the true model; authors note scalability and approximation trade-offs (matrix inverses, support point selection).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations', 'publication_date_yy_mm': '2013-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1284.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1284.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bai et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning how to learn</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related work that formulates hidden-parameter problems as POMDPs and performs Bayesian planning (full Bayes-adaptive planning) to trade off information gathering and task performance; referenced in this paper as a similar setting but assuming the model is given.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning how to learn</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayes-adaptive POMDP planner (Bai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A planner that treats unknown latent task parameters as part of a POMDP hidden state and plans in belief space to trade off learning (information gathering) and reward maximization. In the context of the citation, the planner assumes known model structure.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian planning in belief space (full Bayes-adaptive reinforcement learning / POMDP planning to select actions that both gather information and maximize reward).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Plans explicitly to reduce uncertainty about hidden dynamics parameters while maximizing task reward by reasoning in the belief space over hidden parameters (i.e., actions chosen consider future information gains and rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Relevant to partially observable settings where hidden (fixed) parameters render dynamics unknown — treated as POMDP hidden state; generally applicable to continuous or discrete domains depending on implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit Bayes-adaptive planning balances exploration and exploitation by planning in belief space; details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Mentioned as a closely related approach that performs Bayesian planning in a hidden-parameter POMDP setting; difference noted is that Bai et al. assume model is given, while the present paper learns the model structure from batches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations', 'publication_date_yy_mm': '2013-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Planning how to learn <em>(Rating: 2)</em></li>
                <li>Monte-Carlo planning in large POMDPs <em>(Rating: 2)</em></li>
                <li>Bayesian reinforcement learning in continuous POMDPs with application to robot navigation <em>(Rating: 2)</em></li>
                <li>An analytic solution to discrete Bayesian reinforcement learning <em>(Rating: 1)</em></li>
                <li>Efficient Bayes-adaptive reinforcement learning using sample-based search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1284",
    "paper_id": "paper-9834f41482ca5b87931d41af3923d3c13fa8e298",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "IBP-GP HiP-MDP",
            "name_full": "Indian Buffet Process - Gaussian Process Hidden Parameter Markov Decision Process",
            "brief_description": "A semiparametric HiP-MDP agent that models families of related dynamical systems with a small number of latent, instance-specific scalar parameters: an IBP prior selects which latent factors matter per output, GPs model basis functions, and Gaussian posteriors track instance weights which are updated online for fast adaptation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IBP-GP HiP-MDP agent",
            "agent_description": "Combines an Indian Buffet Process prior over binary filter variables z_{k a d} (which select relevant latent factors) with Gaussian Processes for task-specific basis functions f_{k a d}(s) and Gaussian instance-specific latent weights w_{k b}. Batch MCMC inference finds z and f (represented at pseudo-input support points); online Bayesian filtering (Gaussian updates) updates posterior over w for a new instance and the mean weight estimate is used for model-based planning and Sarsa policy updates.",
            "adaptive_design_method": "Bayesian online parameter inference / Bayesian filtering (posterior updates over latent parameters) combined with model-based planning using the posterior mean (an approximate Bayes-adaptive approach); not an explicit information-gain active design objective.",
            "adaptation_strategy_description": "Pretrain shared structure (z, f) from batches of multiple instances, then for a new instance maintain a Gaussian belief over latent weights w_{k b}; update the belief with each observed transition Δ(s) via closed-form Gaussian information updates; interpolate basis values via GP pseudo-inputs; use the posterior mean weights to build a dynamics model for planning (Sarsa + simulated planning episodes interleaved with real episodes).",
            "environment_name": "Cartpole; Acrobot",
            "environment_characteristics": "Environments have continuous 4-dimensional state spaces, discrete small action sets (cartpole: 2 actions; acrobot: 3 actions), deterministic physics with additive Gaussian observation/noise modeled in transitions, and unknown but fixed latent parameters per instance (pole mass and length for cartpole; link masses for acrobot) — i.e., partially observable dynamics due to unobserved fixed parameters.",
            "environment_complexity": "Cartpole: 4 state dims {x, ẋ, θ, θ̇}, 2 discrete actions, episode length up to 300 timesteps; training used data from 7 (m,l) settings reduced to 750 support points, online tests used 50 training points and 50 test points. Acrobot: 4 state dims {θ1, θ̇1, θ2, θ̇2}, 3 discrete actions, batch training over 8 mass settings (1000 support points), evaluation across 16 mass settings with 30 repeated trials per setting; planning interleaved with 5 simulated planning episodes per real episode.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Prediction accuracy and control performance improved over baselines: Cartpole (All dimensions) mean-squared error (MSE) for IBP-GP HiP-MDP = 2.6e-04 (std reported 8.8e-04); compared baselines: Batch+Train = 2.7e-04, Batch Only = 2.7e-04, Train Only = 8.4e-03. For acrobot (All) IBP-GP HiP-MDP MSE = 2.0e-04 (2.1e-05) vs Batch+Train 3.3e-04 and Train Only 9.4e-04. In control, on acrobot the HiP-MDP reached near-optimal performance by episode 2 and by episode 5 had reached near-optimal performance overall; the average-model baseline required ~15 episodes to reach the same level.",
            "performance_without_adaptation": "Baselines: Single GP on combined batch data (ignoring instance variability), GP trained only on the new-instance training points, and GP using batch+train combined. Example numbers: Cartpole (All) MSE: Batch+Train = 2.7e-04, Batch Only = 2.7e-04, Train Only = 8.4e-03. Acrobot (All) MSE: Batch+Train = 3.3e-04, Batch Only = 3.4e-04, Train Only = 9.4e-04. For control, the 'average model' (batch-only average dynamics) took ~15 episodes to approach near-optimal vs HiP-MDP ~2-5 episodes; true-model oracle performs slightly better than HiP-MDP.",
            "sample_efficiency": "Highly sample-efficient relative to baselines: in acrobot it achieved near-optimal control within ~2 episodes and was near-optimal by 5 episodes (vs ~15 episodes for the batch-average baseline). Cartpole online inference used 50 training points per instance for weight filtering and evaluated on 50 test points; batch pretraining used hundreds to ~1000 support points to learn shared bases.",
            "exploration_exploitation_tradeoff": "Does not perform full Bayes-adaptive planning; the agent maintains a posterior over latent weights (exploitably informative) but uses the posterior mean for planning (more exploitative). Exploration arises from the learning policy (Sarsa) and the interaction procedure; there is no explicit active information-gain objective or acquisition function implemented to choose actions solely for identification of latent parameters.",
            "comparison_methods": "Compared against: (1) GP trained on combined batch data (average model), (2) GP using only the new-instance training points (train-only), (3) GP using batch+train combined, and (4) oracle true dynamics model for planning.",
            "key_results": "The IBP-GP HiP-MDP discovers a small set of latent factors (four factors in cartpole experiments) that explain inter-instance variation, yields significantly lower prediction error on dynamics components that depend on hidden parameters (e.g., ẋ and θ̈ in cartpole), and enables very fast adaptation in control tasks (acrobot) achieving near-optimal performance within a few episodes by updating only a low-dimensional belief over latent weights. Using shared bases learned from batches plus fast online filtering of instance weights yields substantial sample-efficiency gains over treating each instance independently or pooling without instance structure.",
            "limitations_or_failures": "Requires an initial batch of observational data from multiple instances to learn shared basis functions; inference relies on MCMC and GP pseudo-input approximations (computationally intensive); online updates ignore basis-function uncertainty (they use mean interpolation), which is an approximation; planning uses posterior mean weights rather than full belief planning (so it is not fully Bayes-adaptive) which can limit optimal exploration for identification; performance, while near-optimal, does not match planning with the true model; authors note scalability and approximation trade-offs (matrix inverses, support point selection).",
            "uuid": "e1284.0",
            "source_info": {
                "paper_title": "Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations",
                "publication_date_yy_mm": "2013-08"
            }
        },
        {
            "name_short": "Bai et al.",
            "name_full": "Planning how to learn",
            "brief_description": "A related work that formulates hidden-parameter problems as POMDPs and performs Bayesian planning (full Bayes-adaptive planning) to trade off information gathering and task performance; referenced in this paper as a similar setting but assuming the model is given.",
            "citation_title": "Planning how to learn",
            "mention_or_use": "mention",
            "agent_name": "Bayes-adaptive POMDP planner (Bai et al.)",
            "agent_description": "A planner that treats unknown latent task parameters as part of a POMDP hidden state and plans in belief space to trade off learning (information gathering) and reward maximization. In the context of the citation, the planner assumes known model structure.",
            "adaptive_design_method": "Bayesian planning in belief space (full Bayes-adaptive reinforcement learning / POMDP planning to select actions that both gather information and maximize reward).",
            "adaptation_strategy_description": "Plans explicitly to reduce uncertainty about hidden dynamics parameters while maximizing task reward by reasoning in the belief space over hidden parameters (i.e., actions chosen consider future information gains and rewards).",
            "environment_name": null,
            "environment_characteristics": "Relevant to partially observable settings where hidden (fixed) parameters render dynamics unknown — treated as POMDP hidden state; generally applicable to continuous or discrete domains depending on implementation.",
            "environment_complexity": null,
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Explicit Bayes-adaptive planning balances exploration and exploitation by planning in belief space; details are in the cited work.",
            "comparison_methods": null,
            "key_results": "Mentioned as a closely related approach that performs Bayesian planning in a hidden-parameter POMDP setting; difference noted is that Bai et al. assume model is given, while the present paper learns the model structure from batches.",
            "limitations_or_failures": null,
            "uuid": "e1284.1",
            "source_info": {
                "paper_title": "Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations",
                "publication_date_yy_mm": "2013-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Planning how to learn",
            "rating": 2
        },
        {
            "paper_title": "Monte-Carlo planning in large POMDPs",
            "rating": 2
        },
        {
            "paper_title": "Bayesian reinforcement learning in continuous POMDPs with application to robot navigation",
            "rating": 2
        },
        {
            "paper_title": "An analytic solution to discrete Bayesian reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Efficient Bayes-adaptive reinforcement learning using sample-based search",
            "rating": 1
        }
    ],
    "cost": 0.013800999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations</h1>
<p>Finale Doshi-Velez<em><br>Harvard Medical School<br>Boston, MA 02115<br>finale@alum.mit.edu<br>George Konidaris</em><br>Massachusetts Institute of Technology<br>Cambridge, MA 02139<br>gdk@csail.mit.edu</p>
<p>August 19, 2013</p>
<h4>Abstract</h4>
<p>Control applications often feature tasks with similar, but not identical, dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors, and introduce a semiparametric regression approach for learning its structure from data. In the control setting, we show that a learned HiP-MDP rapidly identifies the dynamics of a new task instance, allowing an agent to flexibly adapt to task variations.</p>
<h2>1 Introduction</h2>
<p>Many control applications involve repeated encounters with domains that have similar, but not identical, dynamics. An agent swinging a bat may encounter several bats with different weights or lengths, while an agent manipulating a cup may encounter cup with different amounts of liquid. An agent driving a car may encounter many different cars, each with unique handling characteristics.</p>
<p>In all of these scenarios, it makes little sense of the agent to start afresh when it encounters a new bat, a new cup, or a new car. Exposure to a variety of related domains should correspond to faster and more reliable adaptation to a new instance of the same type of domain. If an agent has already swung several bats, for example, we would hope that it could easily learn to swing a new bat. Why? Like many domains, the bat-swinging domain has a low-dimensional representation that affects the system's dynamics in structured ways. The agent's prior experience should allow it to both learn how to model related instances of a domain-such as the bat's length, a latent parameter that smoothly changes in the bat's dynamics-and what specific model parameters (e.g., lengths) are likely.</p>
<p>Domains with closely-related dynamics are an interesting regime for transfer learning. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP) as a formalization of these types of domains, with two important features. First, we posit that there exist a bounded number of latent parameters that, if known, would fully specify the dynamics. Second, we assume the parameter values remain fixed for a task's duration (e.g. the bat's length will not change during a swing), and the agent will know when a change has occurred (getting a new bat).</p>
<p>The HiP-MDP parameters encode the minimum amount of learning required for the agent to adapt to a new domain instance. Given a generative model of how the latent parameters affect domain dynamics, an agent could rapidly identify the dynamics of a particular domain instance by maintaining and updating its distribution (or belief) over the latent parameters. Instead of learning a new policy for each domain instance, it could synthesize a parametrized control policy $[1,2]$ based on a point estimate of the parameter values, or plan in the belief space over its parameters $[3,4,5,6,7]$.</p>
<p>We present a method for learning the structure of a HiP-MDP from data. Our generative model uses Indian Buffet Processes [8] to model what latent parameters are relevant for a particular set of dynamics and Gaussian processes [9] to model the dynamics functions. We do not require knowledge of a system's kinematics equations, nor must</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>we specify the number of latent parameters in advance. Our HiP-MDP model efficiently performs control in the challenging acrobot domain [10] by rapidly identifying the dynamics of new instances.</p>
<h1>2 Background</h1>
<p>Bayesian Reinforcement Learning The reinforcement learning setting consists of a series of interactions between an agent and an environment. From some state $s$, the agent chooses an action $a$ which transitions it to a new state $s^{\prime}$ and provides reward $r$. Its goal is to maximize its longterm expected rewards, $E\left[\sum_{t} \gamma^{t} r_{t}\right]$, where $\gamma \in[0,1)$ is a discount factor that weighs the relative importance of near-term and long-term rewards. This series of interactions can be modeled as a Markov Decision Process (MDP), a 5-tuple ${S, A, T, R, \gamma}$ where $S$ and $A$ are sets of states $s$ and actions $a$, the transition function $T\left(s^{\prime} \mid s, a\right)$ gives the probability of the next state being $s^{\prime}$ after performing action $a$ in state $s$, and the reward function $R(s, a)$ gives the reward $r$ for performing action $a$ in state $s$. We refer to the transition function $T\left(s^{\prime} \mid s, a\right)$ as the dynamics of a system.</p>
<p>The transition function $T$ or the reward function $R$ must be learned from experience. Bayesian approaches to reinforcement learning [3, 5, 4] place a prior over the transition function $T$ (and sometimes also $R$ ), and refine this prior with experience. Thus, the problem of learning an unknown MDP is transformed into a problem of planning in a known partially-observable Markov Decision Process (POMDP). A POMDP [11] consists of a 7-tuple ${Y, A, O, \tau, \Omega, R, \gamma}$, where $Y, A$, and $O$ are sets of states $y$, actions $a$, and observations $o ; \tau\left(y^{\prime} \mid y, a\right)$ and $R(y, a)$ are the transition and reward functions; and the observation function $\Omega\left(o \mid y^{\prime}, a\right)$ is the probability of receiving an observation $o$ when taking action $a$ to state $y^{\prime}$. Bayesian RL learns an MDP by planning in a POMDP with states $y_{t}=\left{s_{t}, T\right}$ : the fully-observed "world-state" $s_{t}$ and the hidden dynamics $T$.</p>
<p>However, solving POMDPs in high-dimensional, continuous state spaces remains challenging, despite advances in POMDP planning [4, 12, 6], including situations with mixed-observability [13]. The HiP-MDP, which we introduce in section 3, simplifies the Bayesian RL challenge by using instances of related tasks to first find a low-dimensional representation of the transition function $T$.</p>
<p>Indian Buffet Processes and Gaussian Processes Our specific instantiation of the HiP-MDP uses two models from Bayesian nonparametric statistics. The first is the Indian Buffet Process (IBP). The IBP is a prior on 0-1 matrices $M(n, k)$ with a potentially unbounded number of columns $k$. To generate samples from the prior, we first use a Beta process to assign a probability $p_{k}$ to each column $k$ such that $\sum_{k} p_{k}$ is bounded. Then, each entry $M(n, k)$ is set to 1 independently with probability $p_{k}$. The IBP has the property that the distribution of the number of nonzero columns for any row is $\operatorname{Pois}(\alpha)$, but some columns will be very popular (that is, a few $p_{k}$ will be large), while others will be used in only a few rows. We will use the IBP as a prior on what latent parameters are relevant for predicting a certain transition output.</p>
<p>The second model we use is the Gaussian Process (GP). A GP is a prior over continuous functions $y=f(x)$ where the prior probability of a set of outputs $\left{y_{1}, \ldots, y_{t}\right}$ given a set of inputs $\left{x_{1}, \ldots, x_{t}\right}$ is given by a multivariate Gaussian $N(m, K)$, where $m\left(x_{i}\right)$ is the mean function of the Gaussian process and the covariance matrix has elements $K\left(x_{i}, x_{j}\right)$ for some positive definite kernel function $K$. We use additive mixtures of Gaussian processes to model the transition function.</p>
<h2>3 Hidden Parameter Markov Decision Processes</h2>
<p>We focus on learning the dynamics $T$ and assume that the reward function $r=R(s, a)$ is fixed across all instances (e.g., the agent always wants to swing the bat). Let $b$ denote each instance of a domain. The Hidden Parameter Markov Decision Process (HiP-MDP) posits that the variation in the dynamics of a different instances can be captured through a set of hidden parameters $\theta_{b}$.</p>
<p>The HiP-MDP is be described by a tuple: $\left{S, A, \Theta, T, R, \gamma, P_{\Theta}\right}$, where $S$ and $A$ are the sets of states $s$ and actions $a$, and $R(s, a)$ is the reward function. The dynamics $T$ for each instance $b$ depends on the value of these hidden parameters: $T\left(s^{\prime} \mid s, a, \theta_{b}\right)$. We denote the set of all possible parameters $\theta_{b}$ with $\Theta$ and let $P_{\Theta}$ be the prior over these parameters. Thus, a HiP-MDP describes a class of tasks; a particular instance of that class is obtained by fixing the parameter vector $\theta_{b} \in \Theta$. Specifically, we assume that $\theta_{b} \sim P_{\Theta}$ is drawn at the beginning of each task instance $b$ and does not change until the beginning of the next instance.</p>
<p>In special cases, we may be able to derive analytic expressions for how $\theta_{b}$ affects the dynamics $T\left(s^{\prime} \mid s, \theta_{b}, a\right)$ : for example, in a manipulation domain we might be able to derive the kinematic equations of how the cup will respond to a force given a certain volume of liquid. However, in most situations, the simplifications required to derive these analytical forms for the dynamics will be brittle at best. The IBP-GP prior for the HiP-MDP, presented in section 4, describes a semiparametric approach for modeling the dynamics that places few assumptions on the form of the transition function $T\left(s^{\prime} \mid s, \theta_{b}, a\right)$ while still maintaining computational tractability.</p>
<p>As pointed out by Bai et al. [7] in a similar setting, we can consider the HiP-MDP a type of POMDP where the hidden state are the parameters $\theta_{b}$. However, a HiP-MDP makes two assumptions which are stronger than those of a POMDP. First, each instance of a HiP-MDP is an MDP—conditioned on $\theta_{b}$ the transition and reward functions obey the Markov property. Thus, we could always learn to solve each HiP-MDP instance as its own distinct MDP. Second, the parameter vector $\theta_{b}$ is fixed for the duration of the task, and thus the hidden state has no dynamics. This considerably simplifies the procedure for inferring the hidden parametrization.</p>
<h1>4 The IBP-GP HiP-MDP</h1>
<p>Let the state $s$ be some $d$-dimensional vector of continuous, real-valued variables. We propose a transition model $T$ of the form:</p>
<p>$$
\begin{aligned}
\left(s_{d}^{\prime}-s_{d}\right) &amp; \sim \sum_{k}^{K} z_{k a d} w_{k b} f_{k a d}(s)+\epsilon \
\epsilon &amp; \sim N\left(0, \sigma_{n a d}^{2}\right)
\end{aligned}
$$</p>
<p>The weights $w_{k b}$ are the values associated with the latent parameters for instance $b$. The filter parameters $z_{k a d} \in{0,1}$ denote whether the $k^{\text {th }}$ latent parameter is relevant for making predictions about dimension $d$ when taking action $a$. The task-specific basis functions $f_{k a d}$ describe how a change in the latent factor $w_{k b}$ affects the dynamics. The additivity assumption in our semi-parametric basis function regression allows us to learn all the latent elements of this model: the number of factors $K$, the weights $w_{k b}$, the filter parameters $z_{k a d}$, and the form of the basis functions $f_{k a d}$.</p>
<p>Generative Model The IBP-GP Hip-MDP places the following priors on the three sets of hidden variables, $w_{k b}, z_{k a d}$, and $f_{k a d}$ :</p>
<p>$$
\begin{aligned}
z_{k a d} &amp; \sim I B P(\alpha) \text { for } k&gt;1 \
f_{k a d} &amp; \sim G P(\psi) \
\mu_{w_{k}} &amp; \sim N\left(0, \sigma_{w_{w}}^{2}\right) \
w_{k b} &amp; \sim N\left(\mu_{w_{k}}, \sigma_{w}^{2}\right) \text { for } k&gt;1 \
w_{1 b}, z_{1 a d} &amp; =1
\end{aligned}
$$</p>
<p>where $\alpha$ and $\psi$ are the parameters of the IBP and GP. Fixing $z_{1 a d}=1$ and $w_{1 b}=1$ sets the scale for latent parameters and makes the first basis function $f_{1 a d}$ the mean dynamics of the domain.</p>
<p>The IBP-GP prior encodes the assumption that for any domain instance $b$ the real world contains a countably infinite number of independent scalar latent parameters $w_{k b}$. However, when making predictions about a specific state dimension $d$ given action $a$, only a few of these infinite possible latent parameters will be relevant. Thus, we only need to infer the values of a finite number of weights $w_{k b}$ to characterize the dynamics of a finite number of state dimensions under a finite set of actions. Using an IBP prior on the filter parameters $z_{k a d}$ implies that we expect a few latent factors to be relevant for making most of predictions. Moreover, additional prediction tasks-such as a new action, or a new dimension to the state space-can be incorporated consistently. As a regression model, IBP-GP model is an infinite version of the Semiparametric Latent Factor Model [14].</p>
<p>Batch Inference We focus on scenarios in which the agent is given a large amount of batch observational data from several domain instances and tasked with quickly performing well on new instances. Our batch inference procedure uses the observational data to fit the filter parameters $z_{k a d}$ and basis functions $f_{k a d}$, which are independent of any</p>
<p>particular instance, and compute a posterior over the weights $w_{k b}$, which depend on each instance. These settings of $z_{k a d}, f_{k a d}$, and $P\left(w_{k b}\right)$ will used to infer the instance-specific weights $w_{k b}$ efficiently online.</p>
<p>The posterior over the weights $w_{k b}$ is Gaussian given the filter settings $z_{k a d}$ and means $\mu_{w_{b}}$. However, marginalization over the basis functions $f_{k a d}$ requires computing inverses of matrices of size $N=\sum_{b} n_{b}$, where $n_{b}$ is the number of data collected in instance $b$. Instead, we represented each function $f_{k a d}$ by a set of $\left(s^{<em>}, f_{k a d}\left(s^{</em>}\right)\right)$ pairs for states $s^{<em>}$ in a set of support points $S^{</em>}$. Various optimization procedures exist for choosing the support points [15, 14]; we found that iteratively choosing support points from existing points to minimize the maximum reconstruction error within each batch was best for a setting in which a few large errors can result in poor performance.</p>
<p>Given a set of tuples $\left(s, a, s^{\prime}, r\right)$ from a task instance $b$, we first created tuples $\left(s^{<em>}, a, \Delta_{b}\left(s^{</em>}\right)\right)$ for all $s^{<em>} \in S^{</em>}$ and all actions $a$, all dimensions $d$, and all instances $b$. The tuple $\left(s_{d} <em>, a, \Delta_{b}\left(s_{d} </em>\right)\right)$ can be predicted based on all other data available for that action $a$, dimension $d$ pair using standard Gaussian process prediction:</p>
<p>$$
E\left[\Delta\left(s_{d} <em>\right)\right]=K_{s^{</em>} S_{a b}}\left(K_{S_{a b} S_{a b}}+\sigma_{n a d}^{2} I\right)^{-1} \Delta_{b}\left(S_{a b d}\right)
$$</p>
<p>where $S_{a b}$ is the collection of tuples $\left(s, a, s^{\prime}, r\right)$ with action $a$ from instance $b, K_{s^{<em>} S_{a b}}$ is the vector $K\left(s^{</em>}, s\right)$ for every $s \in S_{a b}, K_{S_{a b} S_{a b}}$ is the matrix $K(s, s)$, and $\Delta_{b}\left(S_{a b d}\right)$ is the vector of differences $s_{d}^{\prime}-s_{d}$. This procedure was repeated for every task instance $b$.</p>
<p>Following this preprocessing step, we proceeded to infer the filter parameters $z_{k a d}$, the weights $w_{k b}$, and the values of the task-specific basis functions at the support points $f_{k a d}\left(S^{<em>}\right)$ using a blocked Gibbs sampler. Given $z_{k a d}$ and $w_{k b}$, the posterior over $f_{k a d}\left(S^{</em>}\right)$ is Gaussian. Let $f_{a d}\left(S^{<em>}\right)$ be a column vector of concatenated $f_{k a d}\left(S^{</em>}\right)$ vectors, and let $\Delta\left(S^{<em>}\right)$ be a column vector of concatenated $\Delta_{b}\left(S^{</em>}\right)$ vectors. The mean and covariance of $f_{a d}\left(S^{*}\right)$ is given by</p>
<p>$$
\begin{aligned}
\operatorname{cov}\left(f_{a d}\left(S^{<em>}\right)\right) &amp; =\sigma_{n a d}^{2}\left(W^{T} W+\sigma_{n a d}^{2} \mathbf{K}_{S^{</em>} S^{<em>}}^{-1}\right)^{-1} \
E\left[f_{a d}\left(S^{</em>}\right)\right] &amp; =\frac{1}{\sigma_{n a d}^{2}} \operatorname{cov}\left(f_{a d}\left(S^{<em>}\right)\right)^{-1} W^{T} * \Delta\left(S^{</em>}\right)
\end{aligned}
$$</p>
<p>where</p>
<p>$$
\begin{aligned}
\mathbf{K} &amp; =I_{k} \otimes K_{S^{<em>} S^{</em>}} \
W &amp; =w_{b}\left(z_{k a d}\right) \otimes I_{\left|S^{*}\right|}
\end{aligned}
$$</p>
<p>where $\otimes$ is the Kronecker product, $K_{S^{<em>} S^{</em>}}$ is the matrix $K\left(s^{<em>}, s^{</em>}\right)$ for every $s^{<em>} \in S^{</em>}$, and we write $w_{b}\left(z_{k a d}\right)$ to be $B$ by $k^{\prime}$ matrix of elements $w_{k b}$ such that $z_{k a d}=1$. We repeat this process for each action $a$ and each dimension $d$.</p>
<p>Similarly, given $z_{k a d}$ and $f_{k a d}\left(S^{*}\right)$, the posterior over $w_{k b}$ is also Gaussian. For each instance $b$, the mean and covariance of $w_{b}\left(z_{k a d}\right)$ (that is, the weights not set to zero) are given by:</p>
<p>$$
\begin{aligned}
\operatorname{cov}\left(w_{k b}\right)= &amp; \sigma_{n}^{2}\left(F_{b}^{T} F_{b}+I_{k-1} \frac{\sigma_{n}^{2}}{\sigma_{w}^{2}}\right)^{-1} \
E\left[w_{k b}\right]= &amp; \operatorname{cov}\left(w_{k b}\right)^{-1}\left(\frac{\mu_{w_{b}}}{\sigma_{w}^{2}}+\frac{1}{\sigma_{n}^{2}} F_{b}^{T}\right. \
&amp; \left.\cdot \mid \Delta_{b}\left(S^{<em>}\right)-F_{1 b}\left(S^{</em>}\right)\right)
\end{aligned}
$$</p>
<p>for $k&gt;1$, where $F_{b}$ is matrix with $k^{\prime}-1$ columns concatenating values for $f_{k a d}$ for all actions $a$ and all dimensions $d$ and excluding $k=1$, and $\Delta_{b}\left(S^{*}\right)$ is a column vector concatenating the differences for all actions $a$ and all dimensions $d$.</p>
<p>To sample $z_{k a d}$ for an already-initialized feature $k$, we note that the likelihood of the model given $z, w$, and $f$ with some $z_{k^{\prime} a d}=0$ is Gaussian with mean and variance</p>
<p>$$
\begin{aligned}
E\left[\Delta s^{<em>}\right] &amp; =\sum_{k \neq k^{\prime}} z_{k a d} w_{k b} f_{k a d}\left(s^{</em>}\right) \
\operatorname{cov}\left(\Delta s^{*}\right) &amp; =\sigma_{n a d}^{2}
\end{aligned}
$$</p>
<p>If $z_{k a d}=1$, then likelihood is again Gaussian with the same mean (here we assume that the GP prior on $f$ is zeromean) but with covariance</p>
<p>$$
\operatorname{cov}\left(\Delta s^{<em>}\right)=w_{k^{\prime}} w_{k^{\prime}}^{T} \otimes K_{S^{</em>} S^{*}}+\sigma_{n a d}^{2} I
$$</p>
<p>where $w_{k^{\prime}}$ is a vector of latent parameter values for all instances $b$. We combine these likelihoods with the prior from section 2 to sample $z_{k a d}$.</p>
<p>Initializing a new latent parameter $k^{\prime}$-that is, a $k^{\prime}$ for which we do not already have values for the weights $w_{k^{\prime} b}$ involves computing the marginal likelihood $P\left(\Delta\left(S^{*}\right) \mid z_{k^{\prime} a d}, w_{k b}, f_{k a d}\right)$, which is intractable. We approximate the likelihood by sampling $N_{w}$ sets of new weights $w_{k^{\prime} b}$. Given values for the weights $w_{k^{\prime} b}$ for each instance $b$, we can compute the likelihood with the new basis function $f_{k^{\prime} a d}$ marginalized out using equations 2 and 3 . We average these likelihoods to estimate the marginal likelihood of $z_{k^{\prime} a d}=1$ for the new $k^{\prime}$. (For the $z_{k^{\prime} a d}=0$ case, we can just use the variance from equation 2.) If we do set $z_{k^{\prime} a d}=1$ for the new $k^{\prime}$, then samples a set of weights $w_{k^{\prime} b}$ from our $N_{w}$ samples based on their importance weight (marginal likelihood).</p>
<p>Finally, given the values of the weights $w_{k b}$ for a set of instances $b$, we can update the posterior over $\mu_{w_{k}}$ with a standard conjugate Gaussian update:</p>
<p>$$
\begin{aligned}
\operatorname{var}\left(\mu_{w_{k}}\right) &amp; =\left(\frac{1}{\sigma_{w_{0}}^{2}}+\frac{B}{\sigma_{w}^{2}}\right)^{-1} \
E\left[\mu_{w_{k}}\right] &amp; =\frac{\sum_{k} w_{k b}}{\sigma_{w}^{2}} \operatorname{var}\left(\mu_{w_{k}}\right)^{-1}
\end{aligned}
$$</p>
<p>We use this posterior over the weight means $\mu_{w_{k}}$ when we encounter a new instance $b^{\prime}$.
Online Filtering Given values for the filter parameters $z_{k a d}$ and the basis functions $f_{k a d}$, the posterior on the weights $w_{k b}$ is Gaussian. Thus, we can write the parametrized belief $b_{t}\left(w_{k b}\right)$ at some time $t$ with $\left(h_{w}, P_{w}\right)$, where $P_{w}$ is the inverse covariance $\Sigma_{w}^{-1}$ and $h_{w}$ is the information mean $\mu_{w} P_{w}$. Then the update given an experience tuple $\left(s, a, s^{\prime}, r\right)$ is given by</p>
<p>$$
\begin{aligned}
h_{w}(t+1) &amp; =h_{w}(t)+F_{a}^{T} \Sigma_{n a}^{-1} \Delta(s) \
P_{w}(t+1) &amp; =P_{w}(t)+F_{a}^{T} \Sigma_{n a}^{-1} F_{a}
\end{aligned}
$$</p>
<p>where $F_{a}$ is a $d$ by $k$ matrix of basis values $f_{k a d}(s), \Sigma_{n a}$ is a $d$ by $d$ noise matrix with $\sigma_{n a d}^{2}$ on the diagonal (note that therefore the inverse $\Sigma_{n a}^{-1}$ is trivially computed), and $\Delta(s)$ is a $d$-dimensional vector of $s_{d}^{\prime}-s_{d}$. If updates are performed only every $n$ time steps, we can simply extend $F, \Sigma$, and $\Delta(s)$ to be $n d$ by $k, n d$ by $n d$, and $n d$ respectively.</p>
<p>Computing $F_{a}$ requires computing the values of the basis functions at the point $s$. Since we only have the values computed at pseudo-input points $s^{<em>} \in S^{</em>}$, we use our standard GP prediction equation to interpolate the value for this new point:</p>
<p>$$
E\left[f_{k a d}(s)\right]=K_{s S^{<em>}}\left(K_{S^{</em>} S^{<em>}}+\sigma_{n a d}^{2} I\right)^{-1} f_{k a d}\left(S^{</em>}\right)
$$</p>
<p>where $K_{s S^{<em>}}$ is the vector $K\left(s, s^{</em>}\right)$ for every $s^{<em>} \in S^{</em>}, K_{S^{<em>} S^{</em>}}$ is the matrix $K\left(s^{<em>}, s^{</em>}\right)$, and $f_{k a d}\left(S^{<em>}\right)$ is the vector $f_{k a d}\left(s^{</em>}\right)$. Using only the mean value $f_{k a d}\left(s^{*}\right)$ ignores the uncertainty in the basis function $f_{k a d}$. While incorporating this variance is mathematically straight-forward-all updates remain Gaussian-it adds additional computations to the online calculation. We found that using only the means already provided significant gains in learning in practice.</p>
<h1>5 Results</h1>
<p>In this section, we describe results on two benchmark problems: cartpole and acrobot [10]. In all of our tests, we use an anisotropic squared-exponential kernel with length and scale parameters approximated for each action $a$ and dimension $d$.</p>
<h3>5.1 Cartpole</h3>
<p>The cartpole task begins with a pole that is initially standing vertically on top of a cart. The agent may apply a force either to the left or the right of the cart to keep the pole from falling over. The domain has a four-dimensional state</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Batch + Train</th>
<th style="text-align: left;">Batch Only</th>
<th style="text-align: left;">Train Only</th>
<th style="text-align: left;">IBP-GP HIP-MDP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\dot{x}$</td>
<td style="text-align: left;">$4.8 \mathrm{e}-06(1.7 \mathrm{e}-08)$</td>
<td style="text-align: left;">$4.8 \mathrm{e}-06(1.7 \mathrm{e}-08)$</td>
<td style="text-align: left;">$9.1 \mathrm{e}-05(2.3 \mathrm{e}-07)$</td>
<td style="text-align: left;">$4.7 \mathrm{e}-06(1.7 \mathrm{e}-08)$</td>
</tr>
<tr>
<td style="text-align: left;">$\ddot{x}$</td>
<td style="text-align: left;">$1.1 \mathrm{e}-07(1.0 \mathrm{e}-08)$</td>
<td style="text-align: left;">$1.1 \mathrm{e}-07(1.1 \mathrm{e}-08)$</td>
<td style="text-align: left;">$7.6 \mathrm{e}-07(1.4 \mathrm{e}-07)$</td>
<td style="text-align: left;">$3.3 \mathrm{e}-08(2.9 \mathrm{e}-09)$</td>
</tr>
<tr>
<td style="text-align: left;">$\dot{\theta}$</td>
<td style="text-align: left;">$1.1 \mathrm{e}-03(3.2 \mathrm{e}-06)$</td>
<td style="text-align: left;">$1.1 \mathrm{e}-03(3.2 \mathrm{e}-06)$</td>
<td style="text-align: left;">$3.4 \mathrm{e}-02(1.4 \mathrm{e}-04)$</td>
<td style="text-align: left;">$1.0 \mathrm{e}-03(3.2 \mathrm{e}-06)$</td>
</tr>
<tr>
<td style="text-align: left;">$\ddot{\theta}$</td>
<td style="text-align: left;">$1.5 \mathrm{e}-05(1.5 \mathrm{e}-06)$</td>
<td style="text-align: left;">$1.5 \mathrm{e}-05(1.6 \mathrm{e}-06)$</td>
<td style="text-align: left;">$3.7 \mathrm{e}-05(2.8 \mathrm{e}-06)$</td>
<td style="text-align: left;">$2.5 \mathrm{e}-06(1.1 \mathrm{e}-07)$</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">$2.7 \mathrm{e}-04(8.9 \mathrm{e}-04)$</td>
<td style="text-align: left;">$2.7 \mathrm{e}-04(8.9 \mathrm{e}-04)$</td>
<td style="text-align: left;">$8.4 \mathrm{e}-03(2.8 \mathrm{e}-02)$</td>
<td style="text-align: left;">$2.6 \mathrm{e}-04(8.8 \mathrm{e}-04)$</td>
</tr>
</tbody>
</table>
<p>Table 1: Mean-Squared Error on Cartpole (with 95\% confidence intervals)
space $s={x, \dot{x}, \theta, \dot{\theta}}$ consisting of the cart's position $x$, the cart's velocity $\dot{x}$, the pole's angle $\theta$, and the pole's angular velocity $\dot{\theta}$. At each time step of length $\tau$, the system evolves according to the following equations:</p>
<p>$$
\begin{array}{rlrl}
x_{t+1} &amp; = &amp; x_{t}+\tau \dot{x}<em t_1="t+1">{t} \
\dot{x}</em>} &amp; =\dot{x<em t_1="t+1">{t}+\tau(v-m l \ddot{\theta} \cos \theta / M) \
\theta</em>} &amp; = &amp; \theta_{t}+\tau \dot{\theta<em t_1="t+1">{t} \
\dot{\theta}</em>
\end{array}
$$} &amp; = &amp; \dot{\theta}_{t}+\tau \ddot{\theta</p>
<p>where $v=\frac{f+m l \dot{\theta}_{t}^{2} \sin \theta}{M}, \ddot{\theta}=\frac{(g \sin \theta-v \cos \theta)}{\left(l\left(\frac{4}{3}-m \cos \theta^{2} / M\right)\right.}, f$ is the applied force, $g$ is gravity, $M$ is the mass of both the cart and the pole, and $m$ and $l$ are the mass and length of the pole, respectively.</p>
<p>We varied the pole mass $m$ and the pole length $l$. Cartpole is a simple enough domain such that changing these parameters does not change the optimal policy-if the pole is falling to the left, the cart should be moved left; if the pole is falling to the right, the cart should be moved right. However, we used cartpole to demonstrate the quality of predictions and describe the latent parameters.</p>
<p>For each training $(m, l)$ setting, the agent received batches of data from Sarsa [10] (using a 3rd order Fourier Basis [16]) run for five repetitions of 30 episodes, where each episode was run for 300 iterations or until the pole fell down. Data from seven $(m, l)$ settings ${(1,4),(.3, .4),(.15, .45),(.2, .55),(.25, .5),(.3, .4),(.3, .6)}$ were reduced to 750 support points and then the batch inference procedure (section 3) was repeated 5 times for 250 iterations with $\sigma_{w}=4$, $\alpha=2$ and the Gaussian process hyper-parameters set from the first batch.</p>
<p>Next, 50 training points were selected from Sarsa run on all $(m, l)$ settings with $m \in{.1, .15, .2, .25, .3}$ and $l \in{.4, .45, .5, .55, .6}$. The online inference procedure (section 3) was used to estimate the weights $w_{k b}$ given the filter parameters $z_{k a d}$ and basis functions $f_{k a d}$ from the batch procedure. The quality of the predictions $\Delta(s)$ was evaluated on 50 (different) test points. Our HiP-MDP predictions were compared to using all of the batch data in a single GP (ignoring the fact that the data came from different instances), using only the 50 training points from the current instance, and combining both the 50 training points and batch data together in one GP.</p>
<p>The relative test log-likelihoods are summarized in figure 1a, and table 1 summarizes the corresponding meansquared errors across the 5 runs. Using only the training points from that instance has the highest error across all dimensions $x, \dot{x}, \theta$, and $\dot{\theta}$. Our IBP-GP approach performs similarly to just applying a single GP to the batch data for predicting the change in outputs $x$ and $\theta$; these two dimensions depend only on previous values of $\dot{x}$ and $\dot{\theta}$ and not on the pole mass $m$ or length $l$ (see equations 5). Our approach significantly outperforms all baselines for outputs $\dot{x}$ and $\ddot{\theta}$, whose changes do depend on the parameters of system.</p>
<p>In all 5 of the MCMC runs, a total of 4 latent parameters were inferred. The output dimensions $x$ and $\theta$ consistently only used the first (baseline) feature-that is, our IBP-GP's predictions were in fact the same as using a single GP. This observation is consistent with the cartpole dynamics and the observed prediction errors in figure 1a and table 1. The second feature was used by both $\dot{x}$ and $\dot{\theta}$ and was positively correlated with both the pole mass $m$ and the pole length $l$ (figures 1c, 1b, and 1b). In equations 5, both $\dot{x}$ and $\dot{\theta}$ have many $m l$ terms. The third consistently discovered feature was used only by $\dot{x}$ and was positively correlated with the pole mass $m$ and not correlated with the pole length $l$; the equations for $\dot{x}$ have several terms that depend only on $m$. The fourth feature (used only to predict $\dot{x}$ ) had the highest variability; it generally has higher values for more extreme length settings, suggesting that it might be a correction for more complex nonlinear effects.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Experimental Results for Cartpole.</p>
<h1>5.2 Acrobot</h1>
<p>The acrobot domain features a double-pendulum. The agent can apply a positive, negative, or neutral torque to the joint between the two poles, and its goal is to swing the pendulum up to vertical. The four-dimensional state space consists of the angle $\theta_{1}$ and angular velocity $\dot{\theta}<em 1="1">{1}$ of the first segment (with mass $m</em>}$ ) and the angle $\theta_{2}$ and angular velocity $\dot{\theta<em 2="2">{2}$ of the second segment (with mass $m</em>$ ).</p>
<p>For batch training, we used mass settings $\left(m_{1}, m_{2}\right)$ settings of ${(.7, .7),(.7,1.3),(.9, .7),(.9,1.1),(1.1, .9)$, $(1.1,1.3),(1.3, .7),(1.3,1.3)}$, again employing Sarsa (this time with a 5th order Fourier Basis). First, 1000 support points (and GP hyper-parameters) were chosen to minimize the maximum prediction error within each batch. Next, we used the approach in Section 3 to infer the filter parameters $z_{k a d}$ and get approximate MAP-estimates for the basis functions $f_{k a d}$.</p>
<p>Table 2 shows mean-squared prediction errors using the same evaluation procedure as cartpole. While the HiPMDP has slightly higher mean-squared errors on the angle predictions, the IBP-GP approach does better overall and has lower mean-squared errors on the angular velocity predictions. Predicting angular velocities is critical to planning in the acrobot task, as inaccurate predictions will make the agent believe it can reach the swing-up position more quickly than is physically possible.</p>
<p>In acrobat, a policy learned with one mass setting will generally perform poorly on another. For each of the 16 $\left(m_{1}, m_{2}\right)$ settings with $m_{1} \in{.7, .9,1.1,1.3}$ and $m_{1} \in{.7, .9,1.1,1.3}$, we ran 30 repeated trials in which we filtered the weight parameters $w_{k}$ during an episode and updated the policy at the end of each episode. Even in this "mostly observed" setting, finding the full Bayesian RL solution is PSPACE-complete [17] and offline approximation techniques are an active area of current research [7]. To avoid this complexity, we performed planning using Sarsa with dynamics based on the mean weight parameters $w_{k}$. We first obtained an initial value function using the prior mean weights, and then interleaved model-based planning and reinforcement learning, using 5 simulated episodes of planning for each episode of interaction.</p>
<p>Results on acrobot are shown in figure 2. We compare performance (averaged over weight settings) to planning using the true model, and to planning using an average model (learned using all the batch data at once) to obtain an initial value function. Both the average and HiP-MDP model reach performance near, but not quite as high as, using the true model. However, the HiP-MDP model is already near that performance by the 2nd episode. Using the learned bases from the batches, as well as learning the weights from the first episode, lets it quickly "snap" very near to the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Acrobot performance. The HiP-MDP quickly approaches near-optimal performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Batch + <br> Train</th>
<th style="text-align: left;">Batch <br> Only</th>
<th style="text-align: left;">Train <br> Only</th>
<th style="text-align: left;">IBP-GP <br> HIP-MDP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\theta_{1}$</td>
<td style="text-align: left;">$5.1 \mathrm{e}-05$ <br> $(3.1 \mathrm{e}-06)$</td>
<td style="text-align: left;">$5.4 \mathrm{e}-05$ <br> $(3.3 \mathrm{e}-06)$</td>
<td style="text-align: left;">$3.8 \mathrm{e}-04$ <br> $(3.1 \mathrm{e}-05)$</td>
<td style="text-align: left;">$5.5 \mathrm{e}-05$ <br> $(8.3 \mathrm{e}-06)$</td>
</tr>
<tr>
<td style="text-align: left;">$\theta_{2}$</td>
<td style="text-align: left;">$1.4 \mathrm{e}-04$ <br> $(1.2 \mathrm{e}-05)$</td>
<td style="text-align: left;">$1.5 \mathrm{e}-04$ <br> $(1.2 \mathrm{e}-05)$</td>
<td style="text-align: left;">$8.8 \mathrm{e}-04$ <br> $(9.6 \mathrm{e}-05)$</td>
<td style="text-align: left;">$1.3 \mathrm{e}-04$ <br> $(2.1 \mathrm{e}-05)$</td>
</tr>
<tr>
<td style="text-align: left;">$\theta_{1}$</td>
<td style="text-align: left;">$7.0 \mathrm{e}-04$ <br> $(4.5 \mathrm{e}-05)$</td>
<td style="text-align: left;">$7.2 \mathrm{e}-04$ <br> $(4.8 \mathrm{e}-05)$</td>
<td style="text-align: left;">$1.6 \mathrm{e}-03$ <br> $(1.5 \mathrm{e}-04)$</td>
<td style="text-align: left;">$3.6 \mathrm{e}-04$ <br> $(4.0 \mathrm{e}-05)$</td>
</tr>
<tr>
<td style="text-align: left;">$\theta_{2}$</td>
<td style="text-align: left;">$4.2 \mathrm{e}-04$ <br> $(2.5 \mathrm{e}-05)$</td>
<td style="text-align: left;">$4.3 \mathrm{e}-04$ <br> $(2.7 \mathrm{e}-05)$</td>
<td style="text-align: left;">$9.1 \mathrm{e}-04$ <br> $(1.0 \mathrm{e}-04)$</td>
<td style="text-align: left;">$2.3 \mathrm{e}-04$ <br> $(2.1 \mathrm{e}-05)$</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">$3.3 \mathrm{e}-04$ <br> $(3.8 \mathrm{e}-05)$</td>
<td style="text-align: left;">$3.4 \mathrm{e}-04$ <br> $(3.9 \mathrm{e}-05)$</td>
<td style="text-align: left;">$9.4 \mathrm{e}-04$ <br> $(8.1 \mathrm{e}-05)$</td>
<td style="text-align: left;">$2.0 \mathrm{e}-04$ <br> $(2.1 \mathrm{e}-05)$</td>
</tr>
</tbody>
</table>
<p>Table 2: Mean-Squared Error on Acrobot (with 95\% confidence intervals)
true dynamics. By 5 episodes, our IBP-GP model has reached near-optimal performance; the average model takes 15 episodes to reach this level.</p>
<h1>6 Discussion and Related Work</h1>
<p>Bai et al. [7] use a very similar hidden-parameter setting treated as a POMDP to perform Bayesian planning; they assume the model is given, whereas our task is to learn it. The HiP-MDP is similar to other POMDPs with fixed hidden states, for example, POMDPs used for slot-filling dialogs [18]. The key difference, however, is that the objective of the HiP-MDP is not simply to gather information (e.g., simply learn the transition function $T$ ); it is to perform well on the task (e.g., drive a new car). Transfer learning-the goal of the HiP-MDP—has received much attention in reinforcement learning. Most directly related in spirit to our approach are Hidden-Goal MDPs [17] and hierarchical model-based transfer [19]. In both of these settings, the agent must determine its current MDP from a discrete set of possible MDPs rather than over continuous parameter settings. More related from a technical perspective are representation transfer approaches [20, 21, 22], which typically learn a set of basis functions sufficient for representation any value function defined in a specific state space, or on transfer between two different representations of the same task. By contrast, the HiP-MDP focuses on modeling the dimensions of variation of a family of related tasks.</p>
<p>The IBP-GP prior itself relates to a body of work on multiple-output Gaussian processes (e.g. [23, 14, 24]), though most of these are focused on learning a convolution kernel to model several related outputs on a single task, rather than parameterizing several related tasks. Gaussian process latent variable models [25] have been used for dimensionality reduction in dynamical systems. As with other multi-output GP models, however, GP-LVMs find a time-varying, lowdimensional representation for a single system, while we characterize each instance in a set of systems by a stationary, low-dimensional vector. The focus of these efforts has also been on modeling rather than control.</p>
<p>The extensive work on scaling inference in these Gaussian process models [23, 26, 27] provides several avenues for relaxing some of the approximations that we made in this work (while adding new ones). In settings where one may not have an initial batch of data from several instances, a fully Bayesian treatment of the filter parameters $z_{k a d}$ and the basis functions $f_{k a d}$ might allow the agent to more accurately navigate its exploration-exploitation trade-offs. Exploring which uncertainties are important to model-and which are not-is an important question for future work. Other extensions within this particular model include applying clustering or more sophisticated hierarchical methods to group together basis functions $f_{k a d}$ and thus share statistical strength. For example, one might expect that "opposing actions," such as in cartpole, could be decomposed into similar basis functions.</p>
<h2>7 Conclusion</h2>
<p>Machine learning approaches for control often train agents to expect repeated experiences the same domain. However, a more accurate model is that the agent will experience repeated domains that vary in limited and specific ways. In</p>
<p>this setting, traditional planning approaches, which typically rely on models, may fail due to the large inter-instance variation. By contrast, reinforcement learning approaches, which typically assume that the dynamics of a new instance are completely unknown, fail to leverage information from related instances.</p>
<p>Our HiP-MDP model explicitly models this inter-instance variation, providing a compromise between these two standard paradigms for control. Our HiP-MDP model will be useful when the family of domains has a parametrization that is small relative to its model and the objectives remains similar across domains. Many applications-such as handling similar objects, driving similar cars, even managing similar network flows-fit this scenario, where batch observational data from related tasks are easy to obtain in advance. In such cases, being able to generalize dynamics from only a few interactions with a new operating regime (using data from many prior interactions with similar systems) is a key step in building controllers that exhibit robust and reliable decision making while gracefully adapting to new situations.</p>
<h1>References</h1>
<p>[1] J. Kober, A. Wilhelm, E. Oztop, and J. Peters. Reinforcement learning to adjust parametrized motor primitives to new situations. Autonomous Robots, 33(4):316-379, 2012.
[2] B.C. da Silva, G.D. Konidaris, and A.G. Barto. Learning parameterized skills. In Proceedings of the Twenty Ninth International Conference on Machine Learning, pages 1679-1686, June 2012.
[3] P. Poupart, N. Vlassis, J. Hoey, and K. Regan. An analytic solution to discrete Bayesian reinforcement learning. In TwentyThird International Conference in Machine Learning, pages 697-704, 2006.
[4] D. Silver and J. Veness. Monte-Carlo planning in large POMDPs. In Advances in Neural Information Processing Systems 23, pages 2164-2172, 2010.
[5] S. Ross, B. Chaib-draa, and J. Pineau. Bayesian reinforcement learning in continuous POMDPs with application to robot navigation. In The IEEE International Conference on Robotics and Automation, 2008.
[6] A. Guez, D. Silver, and P. Dayan. Efficient Bayes-adaptive reinforcement learning using sample-based search. In Advances in Neural Information Processing Systems 25, 2012. To appear.
[7] H. Y. Bai, D. Hsu, and W. S. Lee. Planning how to learn. In International Conference on Robotics and Automation, 2013.
[8] T.L. Griffiths and Z. Ghahramani. The Indian buffet process: An introduction and review. Journal of Machine Learning Research, 12:1185-1224, 2011.
[9] C.E. Rasmussen and C.K.I Williams. Gaussian Processes for Machine Learning. The MIT Press, 2005.
[10] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.
[11] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101:99-134, 1998.
[12] H. Kurniawati, D. Hsu, and W.S. Lee. Sarsop: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Proceedings of Robotics: Science and Systems, 2008.
[13] S C W Ong and D Hsu. Planning under uncertainty for robotic tasks with mixed observability. International Journal of Robotics Research, 29(8):1053-1068, 2010.
[14] Y. W. Teh, M. Seeger, and M. I. Jordan. Semiparametric latent factor models. In International Workshop on Artificial Intelligence and Statistics, 2005.
[15] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18, 2005.
[16] G.D. Konidaris, S. Osentoski, and P.S. Thomas. Value function approximation in reinforcement learning using the Fourier basis. In Conference on Artificial Intelligence, pages 380-385, 2011.
[17] Alan Fern and Prasad Tadepalli. A computational decision theory for interactive assistants. In Advances in Neural Information Processing Systems 23, pages 577-585, 2010.
[18] J. Williams and S. Young. Scaling up POMDPs for dialogue management: The "summary POMDP" method. In Proceedings of the IEEE ASRU Workshop, 2005.
[19] A. Wilson, A. Fern, and P. Tadepalli. Transfer learning in sequential decision problems: A hierarchical bayesian approach. Journal of Machine Learning Research - Proceedings Track, 27:217-227, 2012.</p>
<p>[20] K. Ferguson and S. Mahadevan. Proto-transfer learning in Markov decision processes using spectral methods. In The ICML Workshop on Structural Knowledge Transfer for Machine Learning, June 2006.
[21] M.E. Taylor and P. Stone. Representation transfer for reinforcement learning. In AAAI 2007 Fall Symposium on Computational Approaches to Representation Change during Learning and Development, November 2007.
[22] E. Ferrante, A. Lazaric, and M. Restelli. Transfer of task representation in reinforcement learning using policy-based protovalue functions. In Autonomous Agents and Multiagent Systems, pages 1329-1332, 2008.
[23] M.A. Álvarez and N.D. Lawrence. Computationally efficient convolved multiple output Gaussian processes. Journal of Machine Learning Research, 12:1459-1500, 2011.
[24] A.G. Wilson, D.A. Knowles, and Z. Ghahramani. Gaussian process regression networks. In Proceedings of the 29th International Conference on Machine Learning, 2012.
[25] N.D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. In Advances in Neural Information Processing Systems 16, 2004.
[26] M. Titsias. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In the 12th International Conference on Artificial Intelligence and Statistics, volume 5, 2009.
[27] A.C. Damianou, M.K. Titsias, and N.D. Lawrence. Variational Gaussian process dynamical systems. In Advances in Neural Information Processing Systems 24, 2011.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Both authors are primary authors on this occasion.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
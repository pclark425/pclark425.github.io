<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3583 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3583</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3583</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-f9525ae89e7fae32c1a081ee44ceca2f1823c675</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f9525ae89e7fae32c1a081ee44ceca2f1823c675" target="_blank">ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.</p>
                <p><strong>Paper Abstract:</strong> ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate metal-organic frameworks (MOFs). By leveraging a large-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generations. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3583.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3583.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial large-scale language model used in ChatMOF as the central planning, reasoning, and orchestration LLM; it selects toolkits, drafts code, and guides generation of MOF candidates via genetic-algorithm workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described only as a large-scale language model (LLM) used in the ChatMOF system for planning, reasoning, prompt/code generation, and orchestration; architecture/size/training details are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used as the autonomous agent/orchestrator in an LLM-guided genetic-algorithm inverse-design pipeline: GPT-4 devises strategies, selects parent genes (textual gene encoding of MOF topology + building blocks), generates child gene strings via mutation/crossover (text outputs), and produces code to convert genes into structure files for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — inverse design/generation of metal-organic frameworks (MOFs) with target properties (e.g., high surface area, specific gas adsorption properties like CO2 Henry coefficient).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Task-level accuracy labels (True / False (Token Limit Exceeded) / False (Logic Error)); quantified accuracies per task: search, prediction, and generation task accuracies (see results_summary). For generated MOFs, property-based objective functions are evaluated via a property predictor (MOFTransformer) and used as loss/objective (e.g., maximize surface area, match a target property).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 integrated in ChatMOF achieved high task accuracies: for search and prediction tasks (100 samples each) excluding token-limit failures, accuracies were 96.9% and 95.7% respectively; for the generation task (10 samples) the reported accuracy was 87.5% (paper reports 2 token-limit exclusions for generation when aggregated). GPT-4 outperformed GPT-3.5-turbo in planning and lowering logic/token errors; generated MOF candidates are produced as textual genes converted to structures and ranked by ML-predicted properties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Direct comparison presented is between GPT-4 and GPT-3.5-turbo within ChatMOF (GPT-4 yields higher accuracies across tasks). The paper references other generative modeling approaches in materials (GANs, diffusion models, VAEs, reinforcement learning) but does not present experimental head-to-head comparisons of ChatMOF against those generative-model baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reported limitations include token-limit failures when outputs/code exceed LLM token capacity, logic errors in planning or code generation, failure modes when no suitable parent genes exist for a target range, and general shortcomings of LLMs in specialized domain tasks (lack of material-specific representations and limited material-domain training data). The paper emphasizes that atom-by-atom inverse design remains elusive for complex MOFs, necessitating top-down/gene-based design; also computational simulations for property evaluation are expensive, so the system relies on ML predictors whose accuracy constrains design quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3583.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3583.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preceding-generation large-scale language model used as an alternative agent in ChatMOF; used for the same orchestration/planning roles but with lower reasoning/planning performance compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a large-scale language model alternative to GPT-4 used within ChatMOF for planning, code generation, and orchestration; no architectural or training specifics are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Same orchestration role as GPT-4: selects toolkits, drafts code, and guides genetic-algorithm-based generation of MOF genes (textual representations of topology + building blocks) that are converted to structures and evaluated with ML predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — MOF inverse design (same domains as GPT-4 use case).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same task-level accuracy labels and evaluation protocol as for GPT-4 (search, prediction, generation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>When ChatMOF was run with GPT-3.5-turbo, reported accuracies (excluding token-limit failures) were 95% (search), 91% (prediction), and 77.8% (generation), lower than GPT-4 across all tasks; GPT-3.5-turbo more frequently produced token-limit or logic errors in multi-step planning and code outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared directly to GPT-4 within this paper; GPT-3.5-turbo underperforms GPT-4 on the ChatMOF tasks. No comparisons to non-LLM generative baselines are experimentally reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Higher incidence of logic errors and token-limit failures relative to GPT-4; tendency to generate less robust multi-step plans (e.g., requesting too many materials leading to token overflow), and less effective high-level strategies for comparative analyses that would reduce token use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3583.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3583.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-guided GA (ChatMOF generator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-guided Genetic Algorithm for MOF Inverse Design (implemented in ChatMOF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology developed/implemented in this paper where an LLM (GPT-4 or GPT-3.5-turbo) orchestrates a genetic-algorithm (GA) workflow using textual gene encodings (topology + building block identifiers) to generate candidate MOFs which are converted to structures and evaluated by ML predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (GPT-4 / GPT-3.5-turbo) + Genetic Algorithm (text-gene encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The GA represents MOFs as textual genes (e.g., 'tbo+N17+N10' for a MOF) enabling LLM manipulation (selection, crossover, mutation); the LLM proposes children genes and generates code to convert genes to structure files. No neural GA-specific architecture beyond classical genetic operators and LLM text-generation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Top-down inverse design via a GA operating on textual gene encodings (topology IDs + building-block IDs). Workflow: (1) select parent genes from existing database according to an objective; (2) use LLM to produce mutated/crossover child genes; (3) convert child genes to structure files; (4) predict properties using an ML model (MOFTransformer); (5) evaluate objective/loss and iterate for multiple generations; (6) select best candidate structure.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science — design/generation of metal-organic frameworks for targeted properties (e.g., maximize accessible surface area, optimize gas adsorption coefficients).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Generation-task accuracy (logic correctness) as labeled in the paper; property-based objectives/ loss functions (maximize, minimize, or match a target value) evaluated via MOFTransformer property predictions; practical failure modes tracked (token-limit, logic errors, lack of parent genes).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The GA+LLM method enabled automatic generation of candidate MOFs that progressively approach target properties across generations. Reported generation-task accuracy in ChatMOF (with GPT-4) was 87.5% (10 sample questions) and 77.8% with GPT-3.5-turbo, indicating successful end-to-end generation in most test queries. The system successfully generated candidate MOFs and used MOFTransformer predictions to rank/select candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Paper cites other generative strategies used in materials (GANs, diffusion models, VAEs, reinforcement learning) as prior art but does not experimentally compare the LLM-guided GA to those methods. The only experimental comparison is between LLM variants (GPT-4 vs GPT-3.5-turbo) in the ChatMOF pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limitations include dependence on existence of suitable parent genes (if none found, the method can fail or need strategy adaptation), token-limit constraints when the LLM outputs large lists or code, logic errors arising from flawed strategy or code generation, and reliance on ML-predicted properties (MOFTransformer) rather than high-fidelity simulations, which may propagate predictor errors into selection. The approach is top-down and does not perform atom-by-atom design due to MOF complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3583.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3583.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOFTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOFTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based property predictor pre-trained on ~1 million hypothetical MOFs and fine-tuned for specific MOF properties; used in ChatMOF to rapidly estimate target properties of generated MOF candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MOFTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer model for MOF property prediction that leverages both local (atoms, bonds) and global (surface area, topology) features; pre-trained on one million hypothetical MOFs and fine-tuned to predict particular properties by swapping fine-tuned weights as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative model; used as the fast property-evaluation ML model within the LLM-guided GA loop to estimate properties (e.g., Henry coefficient, surface area) of candidate MOF structures generated from textual genes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials property prediction for MOFs to enable rapid evaluation during inverse design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used to provide predicted property values that are converted into objective/loss values used by the GA; the paper's system-level evaluation uses task-level accuracy labels and downstream decision correctness rather than reporting separate MOFTransformer performance metrics within this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MOFTransformer enables fast, large-scale prediction of MOF properties within ChatMOF, facilitating high-throughput evaluation of generated candidates. The paper reports that pre-trained/fine-tuned MOFTransformer models allow quick predictions and maintain high performance for a variety of properties (no numeric predictive-performance metrics for MOFTransformer are re-reported in detail in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>MOFTransformer is presented as the chosen predictor for ChatMOF due to its pretraining on a large set of hypothetical MOFs and fine-tuning flexibility; the paper does not present head-to-head predictive comparisons with other property-prediction architectures within this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Because ChatMOF relies on MOFTransformer predictions rather than full physics-based simulation, generated candidates are only as accurate as the predictor; simulation-based property evaluation is noted to be more accurate but computationally expensive. The paper also highlights scarcity of materials-specific training data generally as a domain challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative Adversarial Nets <em>(Rating: 2)</em></li>
                <li>Denoising Diffusion Probabilistic Models <em>(Rating: 2)</em></li>
                <li>Auto-Encoding Variational Bayes <em>(Rating: 2)</em></li>
                <li>Molecular de novo design through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>HuggingGPT: Solving AI Tasks with ChatGPT and Hugging Face <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3583",
    "paper_id": "paper-f9525ae89e7fae32c1a081ee44ceca2f1823c675",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A commercial large-scale language model used in ChatMOF as the central planning, reasoning, and orchestration LLM; it selects toolkits, drafts code, and guides generation of MOF candidates via genetic-algorithm workflows.",
            "citation_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Described only as a large-scale language model (LLM) used in the ChatMOF system for planning, reasoning, prompt/code generation, and orchestration; architecture/size/training details are not specified in this paper.",
            "generation_method": "Used as the autonomous agent/orchestrator in an LLM-guided genetic-algorithm inverse-design pipeline: GPT-4 devises strategies, selects parent genes (textual gene encoding of MOF topology + building blocks), generates child gene strings via mutation/crossover (text outputs), and produces code to convert genes into structure files for evaluation.",
            "application_domain": "Materials science — inverse design/generation of metal-organic frameworks (MOFs) with target properties (e.g., high surface area, specific gas adsorption properties like CO2 Henry coefficient).",
            "evaluation_metrics": "Task-level accuracy labels (True / False (Token Limit Exceeded) / False (Logic Error)); quantified accuracies per task: search, prediction, and generation task accuracies (see results_summary). For generated MOFs, property-based objective functions are evaluated via a property predictor (MOFTransformer) and used as loss/objective (e.g., maximize surface area, match a target property).",
            "results_summary": "GPT-4 integrated in ChatMOF achieved high task accuracies: for search and prediction tasks (100 samples each) excluding token-limit failures, accuracies were 96.9% and 95.7% respectively; for the generation task (10 samples) the reported accuracy was 87.5% (paper reports 2 token-limit exclusions for generation when aggregated). GPT-4 outperformed GPT-3.5-turbo in planning and lowering logic/token errors; generated MOF candidates are produced as textual genes converted to structures and ranked by ML-predicted properties.",
            "comparison_to_baselines": "Direct comparison presented is between GPT-4 and GPT-3.5-turbo within ChatMOF (GPT-4 yields higher accuracies across tasks). The paper references other generative modeling approaches in materials (GANs, diffusion models, VAEs, reinforcement learning) but does not present experimental head-to-head comparisons of ChatMOF against those generative-model baselines.",
            "limitations_challenges": "Reported limitations include token-limit failures when outputs/code exceed LLM token capacity, logic errors in planning or code generation, failure modes when no suitable parent genes exist for a target range, and general shortcomings of LLMs in specialized domain tasks (lack of material-specific representations and limited material-domain training data). The paper emphasizes that atom-by-atom inverse design remains elusive for complex MOFs, necessitating top-down/gene-based design; also computational simulations for property evaluation are expensive, so the system relies on ML predictors whose accuracy constrains design quality.",
            "uuid": "e3583.0",
            "source_info": {
                "paper_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A preceding-generation large-scale language model used as an alternative agent in ChatMOF; used for the same orchestration/planning roles but with lower reasoning/planning performance compared to GPT-4.",
            "citation_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Described as a large-scale language model alternative to GPT-4 used within ChatMOF for planning, code generation, and orchestration; no architectural or training specifics are provided in this paper.",
            "generation_method": "Same orchestration role as GPT-4: selects toolkits, drafts code, and guides genetic-algorithm-based generation of MOF genes (textual representations of topology + building blocks) that are converted to structures and evaluated with ML predictors.",
            "application_domain": "Materials science — MOF inverse design (same domains as GPT-4 use case).",
            "evaluation_metrics": "Same task-level accuracy labels and evaluation protocol as for GPT-4 (search, prediction, generation tasks).",
            "results_summary": "When ChatMOF was run with GPT-3.5-turbo, reported accuracies (excluding token-limit failures) were 95% (search), 91% (prediction), and 77.8% (generation), lower than GPT-4 across all tasks; GPT-3.5-turbo more frequently produced token-limit or logic errors in multi-step planning and code outputs.",
            "comparison_to_baselines": "Compared directly to GPT-4 within this paper; GPT-3.5-turbo underperforms GPT-4 on the ChatMOF tasks. No comparisons to non-LLM generative baselines are experimentally reported.",
            "limitations_challenges": "Higher incidence of logic errors and token-limit failures relative to GPT-4; tendency to generate less robust multi-step plans (e.g., requesting too many materials leading to token overflow), and less effective high-level strategies for comparative analyses that would reduce token use.",
            "uuid": "e3583.1",
            "source_info": {
                "paper_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LLM-guided GA (ChatMOF generator)",
            "name_full": "LLM-guided Genetic Algorithm for MOF Inverse Design (implemented in ChatMOF)",
            "brief_description": "A methodology developed/implemented in this paper where an LLM (GPT-4 or GPT-3.5-turbo) orchestrates a genetic-algorithm (GA) workflow using textual gene encodings (topology + building block identifiers) to generate candidate MOFs which are converted to structures and evaluated by ML predictors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM (GPT-4 / GPT-3.5-turbo) + Genetic Algorithm (text-gene encoding)",
            "model_description": "The GA represents MOFs as textual genes (e.g., 'tbo+N17+N10' for a MOF) enabling LLM manipulation (selection, crossover, mutation); the LLM proposes children genes and generates code to convert genes to structure files. No neural GA-specific architecture beyond classical genetic operators and LLM text-generation is provided.",
            "generation_method": "Top-down inverse design via a GA operating on textual gene encodings (topology IDs + building-block IDs). Workflow: (1) select parent genes from existing database according to an objective; (2) use LLM to produce mutated/crossover child genes; (3) convert child genes to structure files; (4) predict properties using an ML model (MOFTransformer); (5) evaluate objective/loss and iterate for multiple generations; (6) select best candidate structure.",
            "application_domain": "Materials science — design/generation of metal-organic frameworks for targeted properties (e.g., maximize accessible surface area, optimize gas adsorption coefficients).",
            "evaluation_metrics": "Generation-task accuracy (logic correctness) as labeled in the paper; property-based objectives/ loss functions (maximize, minimize, or match a target value) evaluated via MOFTransformer property predictions; practical failure modes tracked (token-limit, logic errors, lack of parent genes).",
            "results_summary": "The GA+LLM method enabled automatic generation of candidate MOFs that progressively approach target properties across generations. Reported generation-task accuracy in ChatMOF (with GPT-4) was 87.5% (10 sample questions) and 77.8% with GPT-3.5-turbo, indicating successful end-to-end generation in most test queries. The system successfully generated candidate MOFs and used MOFTransformer predictions to rank/select candidates.",
            "comparison_to_baselines": "Paper cites other generative strategies used in materials (GANs, diffusion models, VAEs, reinforcement learning) as prior art but does not experimentally compare the LLM-guided GA to those methods. The only experimental comparison is between LLM variants (GPT-4 vs GPT-3.5-turbo) in the ChatMOF pipeline.",
            "limitations_challenges": "Limitations include dependence on existence of suitable parent genes (if none found, the method can fail or need strategy adaptation), token-limit constraints when the LLM outputs large lists or code, logic errors arising from flawed strategy or code generation, and reliance on ML-predicted properties (MOFTransformer) rather than high-fidelity simulations, which may propagate predictor errors into selection. The approach is top-down and does not perform atom-by-atom design due to MOF complexity.",
            "uuid": "e3583.2",
            "source_info": {
                "paper_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MOFTransformer",
            "name_full": "MOFTransformer",
            "brief_description": "A transformer-based property predictor pre-trained on ~1 million hypothetical MOFs and fine-tuned for specific MOF properties; used in ChatMOF to rapidly estimate target properties of generated MOF candidates.",
            "citation_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
            "mention_or_use": "use",
            "model_name": "MOFTransformer",
            "model_description": "A transformer model for MOF property prediction that leverages both local (atoms, bonds) and global (surface area, topology) features; pre-trained on one million hypothetical MOFs and fine-tuned to predict particular properties by swapping fine-tuned weights as needed.",
            "generation_method": "Not a generative model; used as the fast property-evaluation ML model within the LLM-guided GA loop to estimate properties (e.g., Henry coefficient, surface area) of candidate MOF structures generated from textual genes.",
            "application_domain": "Materials property prediction for MOFs to enable rapid evaluation during inverse design.",
            "evaluation_metrics": "Used to provide predicted property values that are converted into objective/loss values used by the GA; the paper's system-level evaluation uses task-level accuracy labels and downstream decision correctness rather than reporting separate MOFTransformer performance metrics within this manuscript.",
            "results_summary": "MOFTransformer enables fast, large-scale prediction of MOF properties within ChatMOF, facilitating high-throughput evaluation of generated candidates. The paper reports that pre-trained/fine-tuned MOFTransformer models allow quick predictions and maintain high performance for a variety of properties (no numeric predictive-performance metrics for MOFTransformer are re-reported in detail in this paper).",
            "comparison_to_baselines": "MOFTransformer is presented as the chosen predictor for ChatMOF due to its pretraining on a large set of hypothetical MOFs and fine-tuning flexibility; the paper does not present head-to-head predictive comparisons with other property-prediction architectures within this manuscript.",
            "limitations_challenges": "Because ChatMOF relies on MOFTransformer predictions rather than full physics-based simulation, generated candidates are only as accurate as the predictor; simulation-based property evaluation is noted to be more accurate but computationally expensive. The paper also highlights scarcity of materials-specific training data generally as a domain challenge.",
            "uuid": "e3583.3",
            "source_info": {
                "paper_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative Adversarial Nets",
            "rating": 2
        },
        {
            "paper_title": "Denoising Diffusion Probabilistic Models",
            "rating": 2
        },
        {
            "paper_title": "Auto-Encoding Variational Bayes",
            "rating": 2
        },
        {
            "paper_title": "Molecular de novo design through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "HuggingGPT: Solving AI Tasks with ChatGPT and Hugging Face",
            "rating": 2
        }
    ],
    "cost": 0.012037999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatMOF: An Autonomous AI System for</h1>
<h2>Predicting and Generating Metal-Organic</h2>
<h2>Frameworks</h2>
<p>Yeonghun Kang, Jihan Kim*</p>
<p>Department of Chemical and Biomolecular Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea</p>
<h1>ABSTRACT</h1>
<p>ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate metal-organic frameworks (MOFs). By leveraging a large-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generations. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.</p>
<h1>Introduction</h1>
<p>The realm of generative artificial intelligence (AI) is witnessing an unprecedented surge, predominantly fostered by a new generation of computational tools known as large-scale language models (LLMs) ${ }^{1-5}$. These innovative models are deeply rooted in a novel architectural design paradigm, referred to as transformer models ${ }^{6}$. Their capabilities, however, stretch far beyond the domain of basic language tasks. Emulating human thought processes, they exhibit logical functions such as learning from a limited number of examples (few-shot learning), or in certain situations, learning without any examples (zero-shot learning). ${ }^{3,5,7}$ This is achieved through the understanding of vast volumes of text data, underscoring the immense potential held by these models. A significant development in this rapidly advancing field is the rise of autonomous LLM agents, which uses LLMs for a wide range of tasks ${ }^{8-10}$. This system, powered by multiple LLMs, excels in making decisions on its own. As such, the idea of using LLMs to independently reason, find information, and produce answers is becoming more popular across different areas of research fields ${ }^{11-14}$.</p>
<p>Despite marked progress in application of LLM across diverse fields ${ }^{15,16}$, the full potential of its advanced technology within materials science remains largely untapped. This limitation primarily stems from two considerable challenges. Firstly, the inherent complexity of materials poses a significant hurdle as these materials often lack sufficient text-compatible input representations, thereby failing to fully encapsulate their complete properties ${ }^{17-20}$. This difficulty in encoding materials for LLMs restricts their understanding and processing capabilities. Secondly, there is a notable scarcity of material-specific training data in the field. In comparison to other disciplines, materials science lags behind due to fewer dedicated databases and their associated data, exacerbating the challenge of representing this scant data in a text format suitable for LLMs.</p>
<p>Despite these obstacles, there are ongoing attempts to leverage the capabilities of LLMs in materials science. ${ }^{21-23}$ However, so far, these efforts have primarily focused on extracting data from scientific literature and generating responses based on this extracted data, with the actual material itself remaining a largely untouched resource. As such, the exploration and realization of the full potential of LLMs within the sphere of materials science still beckons.</p>
<p>In this work, we highlight the development of an innovative methodology that utilizes autonomous AI to generate new materials and to predict properties of the materials with the focus specifically on metal-organic frameworks (MOFs) ${ }^{24-26}$. MOFs are used in many chemical applications ${ }^{27-30}$ due to their large porosity ${ }^{31-33}$, high surface area ${ }^{32}$, and exceptional tunability ${ }^{34}$. To this end, we have developed the autonomous AI system for MOF (called ChatMOF), which holds the potential to predict MOF properties from text-based inquiries and to generate MOFs with specified properties (i.e. inverse design). This pioneering approach can potentially significantly bridge the gap between the novice users and the computational and machine learning tools, which can potentially facilitate the progress in developing new materials for various applications.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A Conceptual image that explains the ChatMOF. When a user poses a textual question about the properties of a MOF, an appropriate answer is provided by ChatMOF. If a user desires to generate a new MOF, ChatMOF is capable of creating a new MOF that satisfies the condition.</p>
<h1>Results and Discussion</h1>
<h2>Design for ChatMOF</h2>
<p>The effectiveness of autonomous LLM agents is predicated on its capability to accurately extract essential details from textual inputs and offer relevant responses, irrespective of the presence of a rigidly structured query. ${ }^{21}$ This concept is vividly illustrated in ChatMOF, as demonstrated in Figure 1. A user may pose a query in textual form regarding the properties of a material, to which ChatMOF responds by supplying a detailed description related to the material in question. Moreover, the operational scope of this system extends beyond the simple retrieval of information. When a user expresses the need to generate a MOF with specific properties, ChatMOF is capable of generating the requested material structure accordingly.</p>
<p>In the context of ChatMOF, LLMs performs the role of a central planner, regulator and evaluator, akin to a human brain. Although LLM effectively fulfills the role of reasoning, its proficiency in specialized tasks falls short. While a single LLM's proficiency in specialized tasks may be limited ${ }^{15}$, LLMs offer an effective way of integrating and employing various databases and machine learning models seamlessly due to their inherent capacity for reasoning ${ }^{35,36}$. ChatMOF utilizes the LLM to orchestrate a comprehensive plan and employ toolkits for information gathering, comparable to an expert devising a strategy, writing the code, and executing the plan. This synergy allows the system to precisely predict material properties, deduce synthesis techniques, and fabricate new materials with preset properties.</p>
<p>As depicted in Figure 2, ChatMOF is composed of three main components: an agent, toolkit, and an evaluator. The agent responds to human queries by traversing four primary steps (i.e. thoughts, actions, action inputs, and observation), mirroring the methodology outlined in the ReAct $^{37}$ and MRKL papers ${ }^{38}$. Initially, the user's query is set as the end goal, followed by</p>
<p>contemplation about the path to achieve this goal. Subsequently, ChatMOF decides on the appropriate toolkit to employ from the available options. After the chosen toolkit is executed, the observed results serve as the basis for evaluating whether a final answer can be generated. If feasible, the final answer is presented, otherwise, the process cycles back to the thought step to formulate a new strategy.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. The schematic image of ChatMOF. ChatMOF comprises three core components: an agent, toolkits, and an evaluator. Upon receiving a query from human, the agent formulates a plan and selects a suitable toolkit. Subsequently, the toolkit generates outputs following the proposed plan, and the evaluator makes these results into a final response.</p>
<h1>Toolkits</h1>
<p>ChatMOF employs an assortment of toolkits to acquire, predict, or generate material information. These toolkits can be primarily classified into five categories: table-searcher, internet-searcher, predictor, generator, and others. Table-searcher involves obtaining desired information from existing data, while internet-searcher employs internet resources, such as Google and Bing, to produce desired information. The predictor utilizes machine learning models to obtain specified properties. The generator refers to the toolkit that constructs material structures fulfilling certain properties. Lastly, the others encompass a variety of aids like calculators, file saving and reading functions, and visualization toolkits.</p>
<p>Due to the facile synthesis MOF structures, there are many different database associated with the MOF structures: (1) computational-ready experimental MOFs (CoREMOF) ${ }^{39,40}$ and (2) quantum MOF (QMOF) database ${ }^{41}$. The CoREMOF database is an archive of synthesized materials present in a CSD MOF subset ${ }^{42}$, encompassing computations of various properties of MOFs including geometric descriptors. The QMOF database is populated with electrical property data, such as bandgap, formation energy, homo, and lumo, derived through DFT calculations. When a user demands these electrical properties, ChatMOF seeks and extracts them from the QMOF database.</p>
<p>As such, if a user seeks information about a specific MOF that is included in these databases, ChatMOF can locate and deliver the desired information from the pre-tabulated data. Figure 3 provides an illustrative example of a table-search operation conducted by ChatMOF. When faced with a user query, ChatMOF devises a strategic approach to extract the requisite data. Subsequently, it creates a Python code tailored to retrieve specific information from the database in accordance with the premeditated strategy. This code typically uses the "pandas ${ }^{43 \text { " }}$ library to</p>
<p>extract or filter relevant details. The drafted code is then executed within the ChatMOF's designated executor. Upon observing the outcome of this operation, ChatMOF contemplates the necessary steps to derive the final answer, culminating in the delivery of the requested response.</p>
<p>The versatility of ChatMOF extends to handling diverse table data derived from text mining or rule-based coding processes. For questions related to the building blocks of a MOF, the MOFkey ${ }^{17}$ database proves to be instrumental. This particular database leverages rule-based methods to obtain insights about the organic linkers and metal clusters of a MOF, providing details about its topology and the potential presence or absence of interpenetration. In addition, for users seeking guidance on MOF synthesis, the DigiMOF ${ }^{44}$ database becomes a valuable resource. DigiMOF provides an array of synthesis conditions, extracted via text mining techniques from MOF-related academic papers, and includes information on organic and metal precursors, and solvent.</p>
<h1>Question: How high is the accessible surface area of JUKPAI?</h1>
<p>Thought: I need to find the accessible surface area of JUKPAI
Action: search_csv
Action Input: "Search name JUKPAI and provide information of its accessible surface area"
[Table Searcher] Thought: To search for a specific value in the DataFrame, we can use the ... [Table Searcher] Input:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">python</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">print (df:loc[df['name'] == 'JUKPAI'; <br> 'Accessible Surface Area (m^2/cm^3''] lo_markdown())</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>[Table Searcher] Observation:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: left;">Accessible Surface Area (m^2/cm^3)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">4837</td>
<td style="text-align: left;">1474.22</td>
</tr>
</tbody>
</table>
<p>[Table Searcher] Final Thought: The accessible surface area of name 'JUKPAI' is 1474.22 ...
Thought: I now know the final answer</p>
<p>Answer : The accessible surface area for JUKPAI is $1474.22 \mathrm{~m}^{2} / \mathrm{cm}^{2}$</p>
<p>Figure 3. Example of a Table Searcher for the question "How high is the surface area of JUKPAI?". The human's question prompts the system to devise a strategy. From this, Python code is generated and executed to extract the desired information, which is then returned as the final answer.</p>
<p>The accuracy of the look-up table search is contingent on the pre-calculated values available in the specific files. And for queries regarding the properties of MOFs that are not available, computational simulation can serve as an attractive alternative method, but unfortunately, simulations are a time-intensive process and an abundance of computational resources ${ }^{45}$. The best resolution to such challenges is the application of machine learning models, which enable highaccuracy predictions grounded in extensive data. In the case of pre-trained machine learning models, predictions are quick and can be made for a significant volume of substances simultaneously, making it an excellent toolkit for integration into ChatMOF.</p>
<p>As an appropriate toolkit for the prediction task, ChatMOF uses the MOFTransformer ${ }^{46,47}$ model that has been developed in our group for the universal prediction of MOF properties. This model leverages both local features, such as atoms and bonds, and global features like surface area and topology. Having undergone pre-training with one million hypothetical MOFs and subsequent fine-tuning for specific properties, MOFTransformer shows high performance in predicting various properties. Moreover, it affords the convenience of predicting the desired property by retaining the model structure while altering the weights of a model that's been fine-tuned for the desired property.</p>
<p>The key to generating accurate responses is selecting the appropriate fine-tuned model with the MOFTransformer and the material to which it will be applied, based on the query. Similar to HuggingGPT ${ }^{10}$, ChatMOF does not directly compute the material properties, but it rather selects the appropriate machine learning model. Figure 4 shows the example prompt for ChatMOF using the MOFTransformer predictor. When a user asks a question such as "At room temperature (298K), what's the CO2 Henry coefficient for XEGKUR?", generator develops a strategy for solving the problem. The optimal fine-tuned model of MOFTransformer that aligns with the plan, in this case,</p>
<p>'CO2_henry_coefficient_298K' is chosen, and the material to predict (XEGKUR) is decided. Following this, the selected model is employed to make prediction. Finally, it make a response to the query using a language learning model based on the observed values from the machine learning model. Since this is a logarithmic value, ChatMOF selects the "Python_REPL" tool and calculates an exponential value to get the final answer.</p>
<p>Question: At room temperature (298K), what's the $\mathrm{CO}_{2}$ Henry coefficient for XEGKUR?</p>
<p>Thought: I need to find the $\mathrm{CO}<em 2="2">{2}$ Henry coefficient for XEGKUR at room temperature Action: search_csv
Thought: The search_csv tool did not provide any information on the $\mathrm{CO}</em>$ Henry coefficient... Action: predictor
Action Input: Predict the $\mathrm{CO}_{2}$ Henry coefficient for XEGKUR at 298 K
[predictor] Thought: I need to gather data on XEGKUR and train a machine learning to pre... [predictor] Property: CO2_henry_coefficient_298K [predictor] Materials: XEGKUR
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Answer : The CO2 Henry coefficient for XEGKUR at room temperature (298K) approximately $0.027 \mathrm{~mol} / \mathrm{Kg} \cdot \mathrm{Pa}$</p>
<p>Figure 4. Example of a predictor for the question "At room temperature (298K), what's the CO2 Henry coefficient for XEGKUR?" The predictor sets up a plan to solve the question, an appropriate model, and target material. Based on this, it uses machine learning to predict the value, which is then used to derive the final answer.</p>
<p>Finally, a key aspiration among researchers in the field of MOFs is the inverse design of MOFs exhibiting desired properties. In materials science, various generative models, including Generative Adversarial Networks (GAN) ${ }^{48,49}$ and Diffusion models ${ }^{50}$, have been employed for inverse design. However, due to the inherent complexity of MOFs, which includes a large number of atoms, large void fraction, and complex topologies, an atom-by-atom inverse design approach has been elusive. As a workaround, MOF inverse design has been facilitated top-down approaches leveraging through genetic algorithms ${ }^{51-53}$, Variational Autoencoders ${ }^{54}$ (VAE), or reinforcement learning ${ }^{55}$ for the selection of building blocks and their placement into suitable topologies.</p>
<p>Genetic algorithms are notably suitable for integration with LLMs. As a bio-inspired optimization methodology, genetic algorithms operate on a selection, mutation, and crossover principle, making them adaptable and efficient. ${ }^{56}$ For their application to MOFs, these frameworks must be delineated by genes comprising topology and building blocks. For instance, a representative MOF, HKUST-1, can be depicted as tbo+N17+N10, with tbo representing topology and N17 and N10 representing the building block notations. As these gene representations are textual, they facilitate the application of genetic algorithms using an LLM.</p>
<p>Figure 5 showcases the utilization of a genetic algorithm by ChatMOF to fabricate a MOF per user specifications. Upon a user query, the system develops a strategy optimized for the genetic algorithm. It also identifies the target property and determines the loss function most suited for the objective, such as choosing the maximum, minimum, or closest value. Guided by this plan, ChatMOF selects parent genes from the existing database that align with the loss function. Using LLM, it produces children genes from the selected parent genes that are most likely to achieve the target. These children are then transformed into a structure file, and their properties are estimated through machine learning. This procedure is reiterated a fixed number of times, generating</p>
<p>multiple generations of children with each generation yielding MOFs progressively nearer to the target. From the created structures, the one that aligns most closely with the question is finally chosen and presented as the response.</p>
<p>Question: Can you generate the structures with the largest surface area?
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Example of a generator for the question "Can you generate the structures with the largest surface area". The generator establishes a plan, objective and property for the human question. Based on this, it finds parents that satisfy the objective. It uses a genetic algorithm to create children genes and generate structures. This is repeated for a number of cycles to generate new MOFs, which are used to derive the final answer.</p>
<p>Moreover, ChatMOF is engineered to perform a diverse set of toolkits, which extend beyond the realms of LLMs. This includes capabilities such as file search, Internet search, and even simple calculations. These additional functionalities are primarily enabled by leveraging the varied capabilities provided by LangChain ${ }^{57}$, enhancing the overall functionality and utility of ChatMOF. Thus, it is not merely a material analysis tool, but a comprehensive system that can accommodate a wide array of tasks and operations.</p>
<p>In addition, ChatMOF incorporates the Atomic Simulation Environment (ASE) ${ }^{58}$ library as an integral toolkit to facilitate diverse operations on material structure data. The ASE library holds considerable importance in the field of materials science due to its capabilities, including atom manipulation, cell information acquisition, and visualization, among others. Similar to the function of a table searcher, when confronted with a query, ChatMOF devises a strategic plan and constructs suitable Python code utilizing the ASE library to fulfil the query's demands. Subsequently, this code is executed.</p>
<h1>Evaluation</h1>
<p>To evaluate performance of ChatMOF, analysis was conducted for "search task", "prediction task", and "generation task". For evaluation purposes, questions for ChatMOF were created utilizing GPT-4.0 to generate various sentences about the given properties of a MOF. The respective questions for each task can be found in Table S1-3. Accuracy, gauging how adequately the logic responded to the question, was measured for each task. An analysis of the accuracy of ChatMOF utilized three labels: "True", "False (token limit exceeded)", and "False (logic error)". The label "True" signifies that ChatMOF's logic was precise and the yielded answer was accurate. The term "False (Token Limit Exceeded)" was used when the token count in LLM surpassed the maximum allowance of 4,000, thus obstructing further progress. Lastly, the "False (Logic Error)" label designated situations where an error in ChatMOF's logic resulted in an incorrect response or an anomaly. Such situations typically occur when an erroneous plan for obtaining an answer was devised or when an error in output interpretation diverts the system from the desired direction.</p>
<p>Figure 6 presents the accuracy measurements for the three tasks using ChatMOF with GPT-4. Accuracy was measured for 100 sample questions for the search and prediction tasks, and 10 sample questions for the generation task. The number in the bar graph indicates the number of each question in each class. Both the search and prediction tasks rendered accurate answers with high frequency. Excluding 'Token Limit Exceeded' instances (4 out of 100, 6 out of 100, and 2 out of 100, for search, prediction, and generation tasks respectively), they exhibit high accuracies of $96.9 \%$ and $95.7 \%$, respectively. For the generation task, the accuracy stood at $87.5 \%$. Given its complexity, relative to the other two tasks, the accuracy rate is lower. Regardless, all three tasks report high accuracy rates, and these tasks carry significance weight because these are tasks that a typical LLM fails to execute flawlessly. A single LLM fails to provide precise information since</p>
<p>it lacks specific information about the material, particularly for obtaining property information, which is challenging to source via an internet search.</p>
<p>Also ChatMOF, when integrated with GPT-4, exhibits superior performance compared to its integration with GPT-3.5-turbo. As evidenced in Figure S2, the accuracy of ChatMOF with GPT-3.5-turbo stands at $95 \%, 91 \%$, and $77.8 \%$ for the search, prediction, and generation tasks respectively, excluding instances of "Token Limit Exceeded". Across all tasks, GPT-4 consistently outperforms GPT-3.5-turbo in accuracy. This enhanced accuracy of GPT-4 can be attributed to its refined reasoning and comprehension capabilities, particularly during the planning phase. Figure S3 illustrates the distinct approaches that GPT-4 and GPT-3.5-turbo take when presented with the same query: "How does the pore limiting diameter of YUSGID_clean compare with other materials?". While GPT-3.5-turbo seeks the values for all materials mentioned in the query, leading to a token error and subsequent inability to provide an answer, GPT-4 adopts a more holistic strategy. It assesses the distribution of all materials, leveraging metrics such as mean, variance, and quartile values of the property in question. This approach enables GPT-4 to determine the relative position of the target material in the overall distribution, thus delivering a more informative response to the user.</p>
<p>For the "search task," the writing of code utilizing the pandas library significantly impacts the accuracy. 'Token Limit Exceeded' generally occurs when the output code surpasses the permissible token count. This frequently arises when all relevant materials that satisfy a given condition are provided (for example, when a list of materials with a particular property is listed), or when the question contains a comparative clause such as "compared to other materials." 'Logic Error' typically surfaces when there is a flawed strategic approach or a code error. An instance of this</p>
<p>would be when a request to provide 10 specific items is met with a misguided strategy that solely aims to "extract high values," failing to retrieve the specified number of items.</p>
<p>During the "prediction task," difficulties often occur in the interpretation process of the observed values using machine learning techniques. Both the 'Token Limit Exceeded' and 'Logic Error' occurrences can stem from the effort to draw the correct answer from the table based on the estimated values. 'Logic Errors' can manifest not only during the table search phase but also during the strategy formulation stage. An erroneous strategy could either lead to the loading of an unsuitable model or to the generation of an input that is incompatible with the intended model.</p>
<p>The "generation task" presents a more intricate set of challenges, inviting a variety of errors. A frequently observed 'Logic Error' appears when no parent genes can be retrieved from database. If the objective function aims for maximum or minimum values, a satisfying parent gene can always be found. However, if the goal is to get close to a certain value or to fall within a specific range, the selected range might not yield any satisfying parent genes. In such scenarios, the strategy is adapted to incorporate more data. However, if no suitable parent genes are found even after modifying the strategy, it results in an error. Further, both 'Token Limit Exceeded' and 'Logic Error' might occur during the extraction of the most suitable MOF from the generated MOFs, aligning with the objective function.</p>            </div>
        </div>

    </div>
</body>
</html>
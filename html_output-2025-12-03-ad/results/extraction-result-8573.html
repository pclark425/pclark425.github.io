<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8573 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8573</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8573</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276235499</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.04352v2.pdf" target="_blank">Investigating the Robustness of Deductive Reasoning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based NLP tasks, suggesting a degree of deductive reasoning capability. However, it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks. Moreover, while many LLM-based deduction methods have been proposed, a systematic study that analyses the impact of their design components is lacking. Addressing these two challenges, we propose the first study of the robustness of formal and informal LLM-based deductive reasoning methods. We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets. We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery. The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches. Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8573.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8573.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT 4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary instruction-tuned large language model used as the top-performing model in the paper's experiments; evaluated in direct, chain-of-thought (CoT), and autoformalisation setups with a symbolic prover.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT 4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned OpenAI family model (trialed in this work as the strongest closed-source model). Used via API in instruction-tuned form; experiments run with default temperature 1.0 and also checked at temperature 0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicBench (FOL subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary deductive QA over natural-language contexts derived from first-order logic premises covering nine inference rules (evaluated here on the FOL part: 520 samples). The task asks whether the context entails the question (true/false).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated in three formats: informal direct prompting (few-shot), Chain-of-Thought (CoT) few-shot prompting producing intermediate NL steps, and autoformalisation where the LLM produces formal (FOL / R-FOL / TPTP) logical forms which are checked with the Vampire theorem prover; autoformalisation was evaluated with several error-recovery strategies (no-recovery, generic error-type feedback, parser error-message feedback, and warning feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Informal direct: O=0.78, noise E/L/T ≈ 0.77/0.79/0.79, counterfactual OC/EC/LC/TC ≈ 0.64/0.61/0.61/0.63; Informal CoT: O=0.80, noise ≈0.80/0.81/0.82, counterfactual ≈0.68/0.63/0.68/0.64; Autoformalisation (FOL): O=0.84, noise ≈0.82/0.73/0.79, counterfactual ≈0.63/0.62/0.57/0.54 (accuracies as reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT 4o-mini typically outperforms the open-source models on original and noisy data; autoformalisation gives highest original accuracy (84%) but is more vulnerable to specific noise and to counterfactual perturbations; CoT improves robustness in combined noise+counterfactual settings relative to direct prompting for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Substantial drop under counterfactual perturbations (single negation flips cause large accuracy decreases to near 0.54–0.68 depending on method). Autoformalisation still produces semantic errors under counterfactuals despite preserving syntax (i.e., formalised but semantically incorrect), and detailed feedback reduces syntax errors but does not reliably increase final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Even the best model shows limited robustness to counterfactual premises (difficulty overwriting implicit world knowledge); autoformalisation can give higher baseline accuracy but is more sensitive to adversarial noise and syntactic hallucinations; CoT helps most when counterfactuals and noise are combined.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the Robustness of Deductive Reasoning with Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8573.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8573.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source LLM evaluated in direct, CoT, and autoformalisation modes, tested with different formalisation grammars (FOL, R-FOL, TPTP) and error-recovery strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source Llama 3.1 family model; used from HuggingFace where available, quantised to 4-bit for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicBench (FOL subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary deductive QA over contexts derived from first-order logic across multiple inference rules; requires formal first-order-style deduction when autoformalisation is used.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct few-shot prompting, CoT prompting with few in-context CoT examples, and autoformalisation producing FOL/R-FOL/TPTP logical forms passed to Vampire; autoformalisation evaluated with error-recovery (no recovery / error type / parser message / warnings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Informal direct: O=0.77 (noise columns 0.74/0.74/0.73; counterfactuals 0.62/0.53/0.56/0.64); Informal CoT: O=0.78 (noise 0.75/0.76/0.76; counterfactuals 0.64/0.61/0.66/0.73); Autoformalisation (FOL): O=0.74 (noise 0.73/0.71/0.71; counterfactuals 0.66/0.62/0.59/0.57).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Autoformalisation often achieves higher original-set accuracy than informal methods for this model on some syntaxes (but varies by grammar); TPTP grammar gave large gains for some Llama variants in some settings but is unstable across perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Grammar preference unstable across perturbations; autoformalisation increases syntax errors under noise (different grammars see variable execution rates); counterfactuals reduce accuracy substantially; feedback strategies do not consistently recover semantic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large open-source models can match or approach closed models on original data but show grammar-dependent variability; TPTP can be more robust to counterfactuals for some models, but gains are inconsistent under noise. Hybrid use (use formalisation when valid, fallback to informal) is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the Robustness of Deductive Reasoning with Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8573.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8573.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller Llama 3.1 variant assessed across the same informal and autoformalisation pipelines; shows different grammar preferences and sensitivity to noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1 8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source Llama 3.1 family model, 8B parameters, evaluated from HuggingFace with 4-bit quantisation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicBench (FOL subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>First-order-logic style deductive QA (binary) across nine inference rules; datasets include perturbed variants with adversarial noise and counterfactual negations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct few-shot prompting, CoT prompting, and autoformalisation into FOL/R-FOL/TPTP feeding Vampire; error recovery variants applied for autoformalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Informal direct: O=0.63 (noise ≈0.61/0.64/0.66; counterfactuals ≈0.61/0.62/0.59/0.61); Informal CoT: O=0.73 (noise ≈0.73/0.71/0.72; counterfactual ≈0.62/0.59/0.61/0.65); Autoformalisation (FOL): O=0.77 (noise 0.71/0.63/0.52; counterfactual 0.60/0.58/0.55/0.52).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Smaller Llama variant benefits from CoT relative to direct prompting on many partitions; autoformalisation can outperform informal methods on original data but is more sensitive to noise and counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Substantial accuracy drops for autoformalisation on some noise types; higher syntax error rates for smaller model vs larger sibling; counterfactuals cause semantic formalisation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller LLMs adapt to prescribed grammar but with reduced robustness; grammar choice influences results and syntactic execution rates, suggesting specialization or multi-agent handling might help.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the Robustness of Deductive Reasoning with Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8573.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8573.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2-27B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 27B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mid-to-large open-source LLM evaluated across informal (direct and CoT) and autoformalisation pipelines; shows moderate robustness and sensitivity to noise in formalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2 27B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Gemma-2 model (27B parameters) used in instruction-tuned variant from HuggingFace and quantised to 4-bit for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>27B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicBench (FOL subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary deductive question answering over NL premises encoding FOL formulas; includes adversarial noise (encyclopedic, logical, tautological) and counterfactual negations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct few-shot prompting, CoT prompting, and autoformalisation (FOL) with Vampire; evaluated across grammars and error-recovery mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Informal direct: O=0.71 (noise 0.69/0.66/0.68; counterfactuals 0.59/0.51/0.54/0.59); Informal CoT: O=0.66 (noise 0.65/0.64/0.63; counterfactuals 0.62/0.58/0.62/0.64); Autoformalisation (FOL): O=0.74 (noise 0.74/0.61/0.61; counterfactuals 0.72/0.67/0.58/0.51).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Autoformalisation often matches or exceeds informal performance on original data for Gemma-2 27B, but advantages erode under perturbations; CoT provides modest robustness gains in combined noise+counterfactual settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Autoformalisation accuracy decreases 4–11% under noise per the paper's summary; counterfactuals cause larger drops (~12–13%) across methods; syntax and semantic errors arise in formalisation under noise and counterfactuals respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mid-size models show the same qualitative failure modes: vulnerable to counterfactuals and certain noise types (logical/tautological) that trigger formalisation of irrelevant clauses; feedback reduces syntax but not semantic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the Robustness of Deductive Reasoning with Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8573.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8573.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2-9B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 9B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller Gemma-2 variant evaluated on the same suite of reasoning approaches; shows lower execution rates and higher syntax error rates in autoformalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2 9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Gemma-2 model (9B parameters), instruction-tuned variant from HuggingFace, quantised to 4-bit in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicBench (FOL subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>First-order-style deductive QA in NL; the dataset covers multiple inference rules and includes perturbed versions with noise and counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct and CoT prompting and autoformalisation (FOL) feeding Vampire; multiple syntaxes and error-recovery strategies tested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Informal direct: O≈0.78 (Table 1 shows variations by checkpoint but averaged informal ~0.78); Informal CoT ≈0.72–0.99 depending on run; Autoformalisation (FOL) shows low execution rates and formal accuracy (e.g., some formal execution rates as low as 0.41 O and formal valid accuracies small under noise—see Tables 4 and 6). Specific row: formal (FOL) execution ~0.41 O and formal valid accuracy around 0.63 O for some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Smaller Gemma-2 produces far fewer parsable formalisations (low execution rate) compared to larger models and informal prompting; thus informal prompting often yields higher usable accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Very low execution/parsing rates (many syntactic errors) under noise for autoformalisation; tends to hallucinate syntax constructs under distractions; formal valid accuracy when parsable may be good, but parsability is the bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller models suffer from generation of syntactically invalid formal forms, making autoformalisation brittle; suggests using formal reasoning only when the LLM produces a valid parse and falling back to informal methods otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the Robustness of Deductive Reasoning with Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8573.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8573.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B-parameter model evaluated across informal prompts and autoformalisation; shows moderate informal performance but low formal execution rates for FOL formalisation under noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Mistral family model (7B) used from HuggingFace in experiments, quantised to 4-bit; two Mistral variants evaluated (7B and 'Small').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogicBench (FOL subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deductive binary QA from natural-language premises encoding FOL statements across defined inference rules and perturbed variants with distractions and counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct few-shot prompting, CoT prompting, and autoformalisation into FOL passed to Vampire; error-recovery variants applied for formalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Informal direct: O≈0.77 (noise columns ~0.77/0.75/0.79; counterfactual columns ~0.63/0.54/0.54/0.66); Informal CoT similar; Autoformalisation (FOL) O≈0.62 with large drops under noise and low execution rates (formal execution ~0.51 O and as low as 0.23–0.33 for some noise columns per Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Informal prompting consistently yields higher usable execution and similar or better working accuracy than autoformalisation because the latter often fails to produce a parsable formal form for many samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Low formal execution/parsing rate and high syntax errors in autoformalisation under noise; semantic errors under counterfactuals; feedback reduces syntax errors but not semantic mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller Mistral variant exemplifies the trade-off: informal methods give robust parsability and modest performance, while autoformalisation yields better valid-sample accuracy when parsed but is brittle in practice due to parsing failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Investigating the Robustness of Deductive Reasoning with Large Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>LOGIC-LM++: Multistep refinement for symbolic formulations <em>(Rating: 2)</em></li>
                <li>RUPbench: Benchmarking reasoning under perturbations for robustness evaluation in large language models <em>(Rating: 2)</em></li>
                <li>RECALL: A benchmark for LLMs robustness against external counterfactual knowledge <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 1)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 1)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8573",
    "paper_id": "paper-276235499",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT 4o-mini",
            "brief_description": "A proprietary instruction-tuned large language model used as the top-performing model in the paper's experiments; evaluated in direct, chain-of-thought (CoT), and autoformalisation setups with a symbolic prover.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT 4o-mini",
            "model_description": "Instruction-tuned OpenAI family model (trialed in this work as the strongest closed-source model). Used via API in instruction-tuned form; experiments run with default temperature 1.0 and also checked at temperature 0.",
            "model_size": "not specified (closed-source)",
            "reasoning_task_name": "LogicBench (FOL subset)",
            "reasoning_task_description": "Binary deductive QA over natural-language contexts derived from first-order logic premises covering nine inference rules (evaluated here on the FOL part: 520 samples). The task asks whether the context entails the question (true/false).",
            "method_or_approach": "Evaluated in three formats: informal direct prompting (few-shot), Chain-of-Thought (CoT) few-shot prompting producing intermediate NL steps, and autoformalisation where the LLM produces formal (FOL / R-FOL / TPTP) logical forms which are checked with the Vampire theorem prover; autoformalisation was evaluated with several error-recovery strategies (no-recovery, generic error-type feedback, parser error-message feedback, and warning feedback).",
            "performance": "Informal direct: O=0.78, noise E/L/T ≈ 0.77/0.79/0.79, counterfactual OC/EC/LC/TC ≈ 0.64/0.61/0.61/0.63; Informal CoT: O=0.80, noise ≈0.80/0.81/0.82, counterfactual ≈0.68/0.63/0.68/0.64; Autoformalisation (FOL): O=0.84, noise ≈0.82/0.73/0.79, counterfactual ≈0.63/0.62/0.57/0.54 (accuracies as reported in Table 1).",
            "baseline_comparison": "GPT 4o-mini typically outperforms the open-source models on original and noisy data; autoformalisation gives highest original accuracy (84%) but is more vulnerable to specific noise and to counterfactual perturbations; CoT improves robustness in combined noise+counterfactual settings relative to direct prompting for this model.",
            "limitations_or_failures": "Substantial drop under counterfactual perturbations (single negation flips cause large accuracy decreases to near 0.54–0.68 depending on method). Autoformalisation still produces semantic errors under counterfactuals despite preserving syntax (i.e., formalised but semantically incorrect), and detailed feedback reduces syntax errors but does not reliably increase final accuracy.",
            "insights_or_conclusions": "Even the best model shows limited robustness to counterfactual premises (difficulty overwriting implicit world knowledge); autoformalisation can give higher baseline accuracy but is more sensitive to adversarial noise and syntactic hallucinations; CoT helps most when counterfactuals and noise are combined.",
            "uuid": "e8573.0",
            "source_info": {
                "paper_title": "Investigating the Robustness of Deductive Reasoning with Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3.1-70B",
            "name_full": "Llama 3.1 70B",
            "brief_description": "A large open-source LLM evaluated in direct, CoT, and autoformalisation modes, tested with different formalisation grammars (FOL, R-FOL, TPTP) and error-recovery strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.1 70B",
            "model_description": "Instruction-tuned open-source Llama 3.1 family model; used from HuggingFace where available, quantised to 4-bit for experiments.",
            "model_size": "70B",
            "reasoning_task_name": "LogicBench (FOL subset)",
            "reasoning_task_description": "Binary deductive QA over contexts derived from first-order logic across multiple inference rules; requires formal first-order-style deduction when autoformalisation is used.",
            "method_or_approach": "Direct few-shot prompting, CoT prompting with few in-context CoT examples, and autoformalisation producing FOL/R-FOL/TPTP logical forms passed to Vampire; autoformalisation evaluated with error-recovery (no recovery / error type / parser message / warnings).",
            "performance": "Informal direct: O=0.77 (noise columns 0.74/0.74/0.73; counterfactuals 0.62/0.53/0.56/0.64); Informal CoT: O=0.78 (noise 0.75/0.76/0.76; counterfactuals 0.64/0.61/0.66/0.73); Autoformalisation (FOL): O=0.74 (noise 0.73/0.71/0.71; counterfactuals 0.66/0.62/0.59/0.57).",
            "baseline_comparison": "Autoformalisation often achieves higher original-set accuracy than informal methods for this model on some syntaxes (but varies by grammar); TPTP grammar gave large gains for some Llama variants in some settings but is unstable across perturbations.",
            "limitations_or_failures": "Grammar preference unstable across perturbations; autoformalisation increases syntax errors under noise (different grammars see variable execution rates); counterfactuals reduce accuracy substantially; feedback strategies do not consistently recover semantic correctness.",
            "insights_or_conclusions": "Large open-source models can match or approach closed models on original data but show grammar-dependent variability; TPTP can be more robust to counterfactuals for some models, but gains are inconsistent under noise. Hybrid use (use formalisation when valid, fallback to informal) is recommended.",
            "uuid": "e8573.1",
            "source_info": {
                "paper_title": "Investigating the Robustness of Deductive Reasoning with Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3.1-8B",
            "name_full": "Llama 3.1 8B",
            "brief_description": "A smaller Llama 3.1 variant assessed across the same informal and autoformalisation pipelines; shows different grammar preferences and sensitivity to noise.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.1 8B",
            "model_description": "Instruction-tuned open-source Llama 3.1 family model, 8B parameters, evaluated from HuggingFace with 4-bit quantisation.",
            "model_size": "8B",
            "reasoning_task_name": "LogicBench (FOL subset)",
            "reasoning_task_description": "First-order-logic style deductive QA (binary) across nine inference rules; datasets include perturbed variants with adversarial noise and counterfactual negations.",
            "method_or_approach": "Direct few-shot prompting, CoT prompting, and autoformalisation into FOL/R-FOL/TPTP feeding Vampire; error recovery variants applied for autoformalisation.",
            "performance": "Informal direct: O=0.63 (noise ≈0.61/0.64/0.66; counterfactuals ≈0.61/0.62/0.59/0.61); Informal CoT: O=0.73 (noise ≈0.73/0.71/0.72; counterfactual ≈0.62/0.59/0.61/0.65); Autoformalisation (FOL): O=0.77 (noise 0.71/0.63/0.52; counterfactual 0.60/0.58/0.55/0.52).",
            "baseline_comparison": "Smaller Llama variant benefits from CoT relative to direct prompting on many partitions; autoformalisation can outperform informal methods on original data but is more sensitive to noise and counterfactuals.",
            "limitations_or_failures": "Substantial accuracy drops for autoformalisation on some noise types; higher syntax error rates for smaller model vs larger sibling; counterfactuals cause semantic formalisation errors.",
            "insights_or_conclusions": "Smaller LLMs adapt to prescribed grammar but with reduced robustness; grammar choice influences results and syntactic execution rates, suggesting specialization or multi-agent handling might help.",
            "uuid": "e8573.2",
            "source_info": {
                "paper_title": "Investigating the Robustness of Deductive Reasoning with Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Gemma-2-27B",
            "name_full": "Gemma-2 27B",
            "brief_description": "A mid-to-large open-source LLM evaluated across informal (direct and CoT) and autoformalisation pipelines; shows moderate robustness and sensitivity to noise in formalisation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-2 27B",
            "model_description": "Open-source Gemma-2 model (27B parameters) used in instruction-tuned variant from HuggingFace and quantised to 4-bit for experiments.",
            "model_size": "27B",
            "reasoning_task_name": "LogicBench (FOL subset)",
            "reasoning_task_description": "Binary deductive question answering over NL premises encoding FOL formulas; includes adversarial noise (encyclopedic, logical, tautological) and counterfactual negations.",
            "method_or_approach": "Direct few-shot prompting, CoT prompting, and autoformalisation (FOL) with Vampire; evaluated across grammars and error-recovery mechanisms.",
            "performance": "Informal direct: O=0.71 (noise 0.69/0.66/0.68; counterfactuals 0.59/0.51/0.54/0.59); Informal CoT: O=0.66 (noise 0.65/0.64/0.63; counterfactuals 0.62/0.58/0.62/0.64); Autoformalisation (FOL): O=0.74 (noise 0.74/0.61/0.61; counterfactuals 0.72/0.67/0.58/0.51).",
            "baseline_comparison": "Autoformalisation often matches or exceeds informal performance on original data for Gemma-2 27B, but advantages erode under perturbations; CoT provides modest robustness gains in combined noise+counterfactual settings.",
            "limitations_or_failures": "Autoformalisation accuracy decreases 4–11% under noise per the paper's summary; counterfactuals cause larger drops (~12–13%) across methods; syntax and semantic errors arise in formalisation under noise and counterfactuals respectively.",
            "insights_or_conclusions": "Mid-size models show the same qualitative failure modes: vulnerable to counterfactuals and certain noise types (logical/tautological) that trigger formalisation of irrelevant clauses; feedback reduces syntax but not semantic errors.",
            "uuid": "e8573.3",
            "source_info": {
                "paper_title": "Investigating the Robustness of Deductive Reasoning with Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Gemma-2-9B",
            "name_full": "Gemma-2 9B",
            "brief_description": "A smaller Gemma-2 variant evaluated on the same suite of reasoning approaches; shows lower execution rates and higher syntax error rates in autoformalisation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-2 9B",
            "model_description": "Open-source Gemma-2 model (9B parameters), instruction-tuned variant from HuggingFace, quantised to 4-bit in experiments.",
            "model_size": "9B",
            "reasoning_task_name": "LogicBench (FOL subset)",
            "reasoning_task_description": "First-order-style deductive QA in NL; the dataset covers multiple inference rules and includes perturbed versions with noise and counterfactuals.",
            "method_or_approach": "Direct and CoT prompting and autoformalisation (FOL) feeding Vampire; multiple syntaxes and error-recovery strategies tested.",
            "performance": "Informal direct: O≈0.78 (Table 1 shows variations by checkpoint but averaged informal ~0.78); Informal CoT ≈0.72–0.99 depending on run; Autoformalisation (FOL) shows low execution rates and formal accuracy (e.g., some formal execution rates as low as 0.41 O and formal valid accuracies small under noise—see Tables 4 and 6). Specific row: formal (FOL) execution ~0.41 O and formal valid accuracy around 0.63 O for some configurations.",
            "baseline_comparison": "Smaller Gemma-2 produces far fewer parsable formalisations (low execution rate) compared to larger models and informal prompting; thus informal prompting often yields higher usable accuracy.",
            "limitations_or_failures": "Very low execution/parsing rates (many syntactic errors) under noise for autoformalisation; tends to hallucinate syntax constructs under distractions; formal valid accuracy when parsable may be good, but parsability is the bottleneck.",
            "insights_or_conclusions": "Smaller models suffer from generation of syntactically invalid formal forms, making autoformalisation brittle; suggests using formal reasoning only when the LLM produces a valid parse and falling back to informal methods otherwise.",
            "uuid": "e8573.4",
            "source_info": {
                "paper_title": "Investigating the Robustness of Deductive Reasoning with Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B",
            "brief_description": "An open-source 7B-parameter model evaluated across informal prompts and autoformalisation; shows moderate informal performance but low formal execution rates for FOL formalisation under noise.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral 7B",
            "model_description": "Instruction-tuned Mistral family model (7B) used from HuggingFace in experiments, quantised to 4-bit; two Mistral variants evaluated (7B and 'Small').",
            "model_size": "7B",
            "reasoning_task_name": "LogicBench (FOL subset)",
            "reasoning_task_description": "Deductive binary QA from natural-language premises encoding FOL statements across defined inference rules and perturbed variants with distractions and counterfactuals.",
            "method_or_approach": "Direct few-shot prompting, CoT prompting, and autoformalisation into FOL passed to Vampire; error-recovery variants applied for formalisation.",
            "performance": "Informal direct: O≈0.77 (noise columns ~0.77/0.75/0.79; counterfactual columns ~0.63/0.54/0.54/0.66); Informal CoT similar; Autoformalisation (FOL) O≈0.62 with large drops under noise and low execution rates (formal execution ~0.51 O and as low as 0.23–0.33 for some noise columns per Table 4).",
            "baseline_comparison": "Informal prompting consistently yields higher usable execution and similar or better working accuracy than autoformalisation because the latter often fails to produce a parsable formal form for many samples.",
            "limitations_or_failures": "Low formal execution/parsing rate and high syntax errors in autoformalisation under noise; semantic errors under counterfactuals; feedback reduces syntax errors but not semantic mistakes.",
            "insights_or_conclusions": "Smaller Mistral variant exemplifies the trade-off: informal methods give robust parsability and modest performance, while autoformalisation yields better valid-sample accuracy when parsed but is brittle in practice due to parsing failures.",
            "uuid": "e8573.5",
            "source_info": {
                "paper_title": "Investigating the Robustness of Deductive Reasoning with Large Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogicBench: Towards systematic evaluation of logical reasoning ability of large language models",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "LOGIC-LM++: Multistep refinement for symbolic formulations",
            "rating": 2,
            "sanitized_title": "logiclm_multistep_refinement_for_symbolic_formulations"
        },
        {
            "paper_title": "RUPbench: Benchmarking reasoning under perturbations for robustness evaluation in large language models",
            "rating": 2,
            "sanitized_title": "rupbench_benchmarking_reasoning_under_perturbations_for_robustness_evaluation_in_large_language_models"
        },
        {
            "paper_title": "RECALL: A benchmark for LLMs robustness against external counterfactual knowledge",
            "rating": 2,
            "sanitized_title": "recall_a_benchmark_for_llms_robustness_against_external_counterfactual_knowledge"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 1,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 1,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 1,
            "sanitized_title": "pal_programaided_language_models"
        }
    ],
    "cost": 0.014834749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Investigating the Robustness of Deductive Reasoning with Large Language Models
25 Aug 2025</p>
<p>Fabian Hoppe f.hoppe@vu.nl 
Vrije Universiteit Amsterdam</p>
<p>Filip Ilievski 
Vrije Universiteit Amsterdam</p>
<p>Jan-Christoph Kalo 
Universiteit van Amsterdam</p>
<p>Investigating the Robustness of Deductive Reasoning with Large Language Models
25 Aug 2025E4F08615ECF107A03B96D6544903397CarXiv:2502.04352v2[cs.CL]
Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based Natural Language Processing (NLP) tasks, suggesting a degree of deductive reasoning capability.However, it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks.Moreover, while many LLM-based deduction methods have been proposed, a systematic study that analyses the impact of their design components is lacking.Addressing these two challenges, we propose the first study of the robustness of formal and informal LLM-based deductive reasoning methods.We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets.We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery.The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches.Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively.</p>
<p>Introduction</p>
<p>Deriving new knowledge from existing knowledge, as in deductive reasoning, is a key human cognitive skill necessary for various applications, including complex question-answering and decisionmaking.Deductive reasoning can be resource-intensive for humans (e.g., taking a lot of time), require specific expertise (e.g., logicians), and lead to incorrect conclusions (e.g., due to biases).This promotes automated deductive reasoning over Natural Language (NL) as a key objective of human-centric AI.Automatic deduction engines aim to support humans by providing certifiable reasoning chains, avoiding invalid inferences, and accelerating the process.To provide effective support for deductive reasoning, AI must be able to formalise knowledge and rules provided in NL robustly.</p>
<p>Performing logical reasoning has received much interest in AI.In the early days, symbolic methods were aimed at transforming specific parts of language into logical statements [20].Recently, LLMs have been shown to achieve impressive results for many reasoningbased NLP tasks, suggesting a degree of deductive reasoning capability [25].In particular, generating informal reasoning chains via Chain-of-thought (CoT) prompting achieves good performance on LLM All men are mortal.Socrates is a man.</p>
<p>Is Socrates mortal?</p>
<p>Input</p>
<p>Yes</p>
<p>Socrates is a man so he is also mortal.Yes 1a.Informal reasoning ∀ X man(X) =⇒ mortal(X) ∧ man(socrates) |= mortal(socrates) many benchmarks [32].Contrary to symbolic methods, LLMs can answer a deductive reasoning task without providing a formal intermediate reasoning chain.Nevertheless, these informal reasoning chains do not need to follow truth-preserving proof rules, thus leading to chains that are hard to verify.Recent work shows that many informal reasoning chains suffer from a lack of faithfulness [34,28].Addressing these challenges, autoformalisation approaches [18,17] use LLMs to translate NL input into a logical form, and a rigorous symbolic solver to perform the deductive reasoning.Autoformalisation is thus a hybrid approach, which aims to provide a faithful and verifiable reasoning chain while leveraging the linguistic manipulation skills of LLMs.Autoformalisation faces two key challenges: First, since they translate rich NL into a limited grammar of symbols and operations, it is critical to leverage a syntax with an optimal tradeoff between translation accuracy and expressivity.Second, while autoformalisation chains provide an opportunity for syntactic and semantic validation and error analysis, designing an effective and efficient error recovery mechanism is non-trivial.While prior autoformalisation systems leverage multiple syntaxes and error recovery mechanisms, no systematic study has investigated their impact on autoformalisation accuracy.</p>
<p>Meanwhile, a key requirement of LLM-based reasoning meth-ods is their robustness to noise [3] and out-of-distribution [7] inputs.Given the strong performance of LLMs across many domains and benchmarks [23], dealing with noisy data in reasoning has been considered more important [24].Most evaluations have focused on adversarial noise (e.g., lexical perturbations) [23], while a recent study has also experimented with counterfactual statements [15].While robustness evaluations for NLP tasks have yielded mixed results [31,15], it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust in logical deductions.</p>
<p>We address these two challenges by investigating the robustness of LLM-based deductive reasoning methods with the goal to advance the understanding of LLM-based reasoning and shed light on methodological aspects to inform more complex, future reasoning systems.Our overall approach is summarised in Figure 1.</p>
<p>Our study makes three contributions.First, following standard practices in evaluating robustness, we devise a robustness framework for logical deduction with two families of perturbations: adversarial noise, where the model needs to preserve its label in the face of added irrelevant information, and counterfactual perturbations, where a single alteration in the context flips the label of the question.The combinations of the perturbations produce seven variants from a given dataset.The perturbed data is made available [8].Second, we synthesise the landscape of existing LLM-based logical deduction methods into a methodological framework with three dimensions: reasoning format, grammar syntax, and error recovery mechanism.For each of these dimensions, we incorporate representative approaches in the literature and release the source code [8].Third, we perform extensive experiments with seven LLMs on eight perturbed variants of a recent modular benchmark.Our findings provide nuanced insights into the robustness of LLM-based reasoning methods.</p>
<p>Related Work</p>
<p>This section gives an overview of LLM-based reasoning methods and studies that evaluate their robustness.</p>
<p>Methods for LLM-based Reasoning</p>
<p>Informal reasoning.Scaling up the size of LLMs enables strong performance in many NLP tasks by few-shot prompting [2], which suggests inherent reasoning capabilities.Chain-of-thought (CoT) combines few-shot prompting with generating intermediate informal reasoning chains [32].These informal reasoning skills motivated more elaborate prompting techniques, like Zero-Shot CoT [13] or self-consistency by generating multiple chains [30], as well as more complex structures than chains, such as Tree of Thoughts [33] and Graph of Thoughts [1].These methods use an LLM to generate intermediate steps and evaluate the output through self-refinement.Similarly, ProofWriter [27] improves multi-hop reasoning by adding the intermediate results to the reasoning context [27].A key benefit of informal reasoning chains is their flexibility, but this comes at the expense of guaranteeing faithfulness.Consequently, methods combining LLMs with formal reasoning have been suggested.</p>
<p>Autoformalisation.Instead of informal reasoning, another way combines an LLM with a deterministic symbolic solver, e.g., a theorem prover.Here, the prover guarantees faithful and deterministic reasoning.This combination is known as autoformalisation.One of the first autoformalisation approaches using formal reasoning chains in combination with prompting is PAL [4].The authors generate Python snippets alongside informal steps and generate the final response by executing the generated code snippets.Logic-LM [18] prompts LLMs to generate multiple task-specific formalisations (logic programming, first-order logic, satisfiability modulo theories and constraint satisfaction), which are solved by dedicated solvers.They report higher robustness for longer reasoning chains compared to CoT reasoning.The extension Logic-LM++ [12] tries to avoid new syntax errors by integrating a self-refinement mechanism.The LINC [17] approach uses the idea of self-consistency from CoT and generates multiple formalisations to avoid formalisation errors.Autoformalisation models achieve high accuracy for many deductive reasoning benchmarks, showing clear benefits for complex reasoning.</p>
<p>Comparison.Our work is the first to explore the robustness of LLM reasoning approaches from prior work: direct few-shot prompting, CoT, and autoformalisation.Another contribution of our work is consolidating these methods with their syntax and error recovery choices into a coherent methodological framework.</p>
<p>Robustness Evaluation of LLM Reasoning</p>
<p>Evaluating deductive reasoning.The improvements of LLMbased reasoning have inspired the development of benchmarks investigating capabilities for deductive reasoning.The synthetic PrOn-toQA [22] dataset is built on modus ponens multi-hop reasoning and confirms reasoning capabilities for the largest models.However, they noted issues with proof planning and selecting the correct proof steps for longer reasoning chains.The FOLIO [6] and AR-LSAT [36] benchmarks confirmed these errors for more complex reasoning and more naturalistic language.LogicBench [19] is a recent benchmark that systematically studies the performance of LLM-based reasoners across multiple inference rules (e.g., modus ponens), reporting a good model performance for predicate logic and first-order logic.A vital challenge identified by these works is unfaithful reasoning, i.e., the intermediate steps are not used to infer the final result [34,14,28].</p>
<p>Robustness studies.</p>
<p>A key requirement of LLM-based reasoning methods is their robustness to noise [3] and out-of-distribution [7] inputs.Given the strong performance of LLMs across many domains and benchmarks [23], dealing with noisy data in reasoning tasks has been considered more important [24], including adversarial and counterfactual perturbations.A variety of robustness tests have therefore been designed [29].These robustness benchmarks rely on the original problem's perturbations through paraphrasing and distractions on character, word, and sentence levels [24,23].Many works try to generate adversarial examples to better understand the generalisation capabilities of LLMs, which are shown to significantly decrease LLM performance [29].RUPbench is a recent robustness study on logical reasoning of LLMs [31], covering many reasoning datasets and all three types of perturbations.Another category involves semantic changes in the input texts.Semantic changes can also be performed purely on a linguistic or logical level.Recent approaches use these logic-based perturbations [16], which align with our robustness framework.Meanwhile, counterfactual studies of LLM robustness have been less common.One exception is RECALL [15], a benchmark based on external knowledge bases, whose study reveals that LLMs are generally susceptible to external knowledge with counterfactual information and that simple mitigation strategies cannot significantly alleviate this challenge.</p>
<p>Comparison.We conduct the first investigation of robustness differences between formal and informal LLM-based logical deduction methods.For this purpose, we base our experiments on Log-icBench [19], which systematically includes nine inference rules mapped to natural language situations.We develop seven additional variants of LogicBench incorporating adversarial noise and counterfactual statements, in line with prior work like [16] that studies LLM robustness on other reasoning tasks.</p>
<p>3 Framework for Evaluating Robustness in Logical Deduction Tasks Task definition.Deductive reasoning is commonly formatted as binary Question Answering (QA) over a given context.The task input consists of a NL question q and a set of logical premises transformed into an NL context c.The output of the task a is one of the two possible answers: true or false, indicating whether the context supports or refutes the question.</p>
<p>Formally, an NL deductive reasoning task defines a function f : (c, q) → {true, f alse}.Robustness types.Following prior work on investigating the robustness of LLMs [23,15], we conceptualize two families of context perturbations.First, inspired by adversarial robustness studies [16], we posit that a reasoning system must be robust to adversarial noise (distractions), i.e., including irrelevant information in the context should not alter its prediction.Noise can shift the focus of a text, making it more difficult for an LLM to capture the relevant context.Second, a reasoning system must be robust to counterfactual shifts in the context.Namely, the system must be faithful to the provided context without including biases from implicit world models [15].If the context states that men are immortal, the reasoning must overwrite its belief of men as mortal.By introducing counterfactual perturbations contradicting common sense, we investigate whether LLMbased logical deduction methods use reasoning shortcuts or perform genuine reasoning.We focus our perturbations on the NL context c rather than the question q because it corresponds to background knowledge sources, which tend to vary significantly in real-world applications (e.g., compare reasoning based on a research article to reasoning over a list of facts).The perturbed task can be formalised as f ′ : (c ′ , q) → {true, f alse}.</p>
<p>Next, we detail our framework for evaluating the robustness of LLM-based deduction methods to noise and counterfactual shifts.</p>
<p>Adversarial Noise</p>
<p>Formalisation.As monotonic reasoning, deductive reasoning must remain invariant to newly added irrelevant information, i.e., additional text that does not change the semantics of the existing premises does not change the conclusion.Let us consider a perturbed context that includes noisy information: c ′ = d1 . . .d k c, where each di denotes a noisy sentence concatenated to c for k ∈ {1, 2, 4}.The task function f ′ must resolve to the same output as the original f for its proof to remain valid.To avoid any distractions that might change the original semantics (e.g., by breaking inter-sentence co-references), we append distractions only to the beginning of the context.Design of adversarial noise.We define three types of noise relevant to logical deduction tasks, which vary in their degree of referential content, formalisation complexity, and depth of logical reasoning.All noise sentences are sampled in a way that guarantees they do not impact the semantics of the context and the original proof.tion following pragmatic principles of language [5].Encyclopedic sentences are often difficult or even impossible to formalise in firstorder logic.At the same time, encyclopedic facts are not connected by complex logical relations and lack the linguistic structure typical for reasoning contexts.Consequently, this perturbations only has a low semantic similarity to the original context.In summary, they represent world information, have a high formalisation complexity, and have low logical reasoning depth.</p>
<ol>
<li>
<p>Logical (L) statements provide a typical structure of reasoning contexts and contain only knowledge that can be natively formalised, such as All dresses are clothes.Logical sentences include information about the world, albeit in a fictional form.As such, the semantic similarity is larger than encyclopedic perturbations due to the shared typical structure.The formalisation usually requires more complex reasoning, like multi-hop inferences.Thus, logical perturbations introduce limited world information, have low formalisation complexity, and high reasoning depth.</p>
</li>
<li>
<p>Tautological (T) perturbations are easily recognisable correct statements, e.g., True is not false.They may include negations and one disjunction or conjunction.These perturbations provide the highest semantic similarity to the original context because they only contain the shared typical structure of the reasoning context without including information about the world.As such, they contain no referential information and require simple reasoning.However, their formalisation is often difficult because many first-order logic syntaxes do not consider predefined truth constants, like ⊥, ⊤.In summary, tautologies in NL contain no referential information, have high formalisation complexity, and have low reasoning depth.</p>
</li>
</ol>
<p>Counterfactual Shifts</p>
<p>Formalisation.We consider common-sense contradictions by altering original statements in c into counterfactual ones.The inclusion of counterfactual statements is motivated by the requirement for faithful reasoning.Namely, the validity of a deduction depends only on the structure of the logical form, which in turn should follow the original task description in the context and the question.Therefore, the reasoning capabilities of a model should not rely on prior knowledge.Instead, explicitly stated premises should be prioritised over implicit model knowledge and overwrite it.This behaviour allows models to reason over hypothetical and counterfactual premises, enabling the derivation of new (scientific) knowledge, e.g., deriving non-Euclidean geometry by replacing the parallel postulate, or science fiction authors to create complete, logically consistent alternative worlds based on one changed premise.Moreover, it ensures a more controlled evaluation setup for evaluating reasoning independently of the quality of the internal model knowledge, since all models use the same premises.To introduce counterfactual premises, we do not add new sentences; instead, we negate sentences from the original context c.Since original premises often state common-sense knowledge, their negation naturally contradicts world knowledge.For example, in the modus ponens inference in Figure 1, we negate mortal(X), stating that All men are immortal.The altered context c ′ is formalised as follows: c ′ = neg(c), where neg(c) negates one of the terms in the original context.Naturally, these perturbations influence the inferred answer and depend on the applied inference rule.The negated term is selected so that the label of the resulting function f ′ is opposite from that of the original function f .Design of counterfactual perturbations.We assume a deductive reasoning dataset where the natural language form corresponds to a logical formula.Then, we introduce counterfactual perturbations by altering each logical formula using predefined rules that negate the formula (see the full list of rules in the Appendix).For example, we negate the consequent q(a) for the first implication of a constructive dilemma and adapt the inference to ¬q(a) ∨ s(a).Then, the negation to create the context c ′ is manually added before the relevant terms in c, thus guaranteeing high data quality.Since the inference results in the opposite label for f ′ , we adapt the target label accordingly.Figure 2 (top-right) shows an example of such a contradiction.Importantly, all counterfactual perturbations can be combined with noise perturbations, leading to counterfactual versions of the original (OC ), encyclopedic (EC ), logical (LC ), and tautological (LT ) sets.</p>
<p>Methodological Framework</p>
<p>We consider three key design dimensions: reasoning format, syntax, and error recovery mechanism to systematically analyse robust deductive reasoning capabilities for LLM-based methods.We describe each of these dimensions and their representative approaches studied in this work.</p>
<p>Reasoning Format</p>
<p>LLM-based methods can either: a) operate without any explicit formalisation as informal reasoning systems and generate the answer based on their internal representation, or b) generate a formal representation of the given input, which is fed into a theorem prover to find a valid proof.</p>
<p>The informal reasoning method instructs an LLM to answer q with Yes or No based on a context.We employ few-shot prompting using three manually engineered in-context examples.To make the model more robust, two of the three in-context examples include distractions with irrelevant information.We explicitly note in the instruction part of the prompt that the provided contexts may contain irrelevant details.The model is evaluated in two modes: direct prompting, where it answers directly, and CoT prompting, where it generates step-by-step reasoning in natural language before generating an answer.To support CoT prompting, we manually created in-context examples with fine-grained natural language reasoning steps designed to improve performance [32].The model's answers are extracted using a regular expression (details in Appendix D).</p>
<p>We include an autoformalisation method combining a symbolic theorem prover with an LLM.This approach follows Logic-LM [18] to formalise the context and query into symbolic representations, which are then automatically proven.The process is divided into two subtasks: First, the model generates formal representations of the If an individual drinks water, they will be hydrated.context and query, referred to as cLF (context logical form) and qLF (query logical form).Second, a symbolic theorem prover evaluates these logical forms to determine whether qLF can be derived from cLF , producing a true or false outcome.This approach allows for a transparent and verifiable reasoning process grounded in logical consistency.In practice, we prompt the LLM to create the logical forms using the same three in-context examples as in the informal reasoning approach.The prompt is extended with instructions describing the formalisation syntax, following the methodology of Logic-LM.</p>
<p>The resulting logical forms are parsed and combined into an Abstract Syntax Tree (AST), which provides the input for a theorem prover.</p>
<p>Formalisation Syntax</p>
<p>While the reasoning performance should be independent of the particular formalisation syntax, models may perform better with specific syntaxes, e.g., because of their frequency in the training data [21].</p>
<p>It is an open question whether the choice of syntax for formal reasoning impacts the model's translation performance and the robustness of its reasoning.Although all the syntaxes we consider represent First-Order Logic (FOL), their surface form variations may influence the formalisation ability of LLMs.The three evaluated syntaxes, illustrated in Figure 3, contain identical information and are interchangeable, which ensures flexibility and allows the framework to include other syntaxes in the future:</p>
<p>FOL is widely used in logic classes and academic papers.It incorporates mathematical symbols such as ∀ and ∃ and implicitly distinguishes between variables and individuals.This syntax is employed by Logic-LM [18].</p>
<p>R-FOL is a variation of FOL that explicitly differentiates variables and individuals by requiring variables to start with a question mark.This resolves the syntax ambiguity in FOL.</p>
<p>TPTP as the abbreviation of Thousands of Problems for Theorem Provers is a Prolog-like formalisation language developed for theorem provers.It avoids mathematical symbols and mandates that variables begin with an uppercase letter.While TPTP supports higherorder logic, we limit our scope to its first-order fragment (fof), using a syntax derived from its official specification [26].</p>
<p>Error Recovery Mechanism</p>
<p>To make autoformalisation more robust, we synthesise strategies for handling syntactic and semantic errors.Syntactic errors occur when the logical forms generated by LLMs do not follow the required syntax.Syntactic errors are easy to detect as logical forms cannot be parsed if they violate grammatical rules.In contrast, semantic errors, such as incomplete context representation, are more challenging to identify and resolve.We apply task-specific heuristics to identify suspicious constructs that lead to semantic errors and generate warnings.Unknown predicates as part of qLF are one example of such a construct, because they indicate an incomplete context.We consider four strategies for handling these errors:</p>
<p>No recovery.The baseline approach does not attempt to correct errors.Instead, we predict a random value, true or false, as a fallback strategy.We avoid introducing an evaluation bias associated with more complex strategies, such as CoT-based refinement, which is commonly done in prior work [18,12], as this would blur the comparison with other methods.</p>
<p>Error type feedback.The LLM is prompted to refine the logical form using a generic parsing error message, such as 'parsing error'.This type of message does not need a parser with an error handler, though it fails to point to specific errors in the logical form.Prior work has shown the effectiveness of this feedback method [18].</p>
<p>Error message feedback.A more detailed approach where the LLM is given specific feedback, highlighting the exact parts of the logical form that violate the syntax.Creating this kind of feedback necessitates an error-handling strategy for the parser.For example, the missing argument in man ∧ mortal(Socrates) results in the error message: mismatched input '∧' expecting '('.Using error messages from parsers as feedback to improve LLMs performance has shown promising results in code generation [35,10].</p>
<p>Warning feedback.An extension of the error messages with warnings generated from heuristics to recognise semantic errors, inspired by the "soft" critics in the LLM-modulo method [11], where it has been shown to enhance robustness.Notably, soft critics have not been incorporated in prior LLM-based methods for logical deduction.</p>
<p>Experimental Setup</p>
<p>The experimental setup, including the generated perturbed data, the source code and configuration files, is made available on Zenodo [8] and GitHub1 to facilitate reproducibility.</p>
<p>Dataset details.The robustness evaluation is based on the FOL part of the LogicBench (Eval) [19] dataset, containing 520 samples with 180 unique contexts.LogicBench systematically covers nine different inference rules.The samples are automatically generated by prompting GPT-3.5 and manually verified.340 of the total of 520 samples are negative examples constructed by negating the conclusion.If the conclusion is a disjunction, each part of the disjunction is negated, resulting in a slight class imbalance.The authors report a mean accuracy of around 90% for three human annotators.By applying our robustness framework from §3, we obtain seven perturbed variants of LogicBench.</p>
<p>Perturbations.The noise sentences are randomly sampled from a source s.We sample encyclopedic perturbation sentences from 10, 000 abstracts of Wikipedia articles gathered via its API.As a logical reasoning source, we use sentences from 1001 contexts of the deduction QA benchmark FOLIO [6].As tautologies, we manually write 22 sentences.All sentences use negations and, at most, one disjunction or conjunction.A complete list can be found in Appendix B. We randomly sample noise perturbations and add them to each sample.We do not alter the class distribution, i.e., we keep LogicBench's original class imbalance.The three types of noise sources have varying, yet consistently low, levels of semantic relevance, quantified as an average cosine similarity of sentence-transformers embeddings between the added noise and existing context.Tautological noise is most similar (0.20), followed by logical noise (0.09), and encyclopedic noise (0.04).The low semantic similarity and the careful construction of the dataset, i.e. appending distractions only at the beginning of the sentence ensure no interference with the original semantics.</p>
<p>We consider unique contexts for eight out of the nine logical forms to create counterfactual statement perturbations, resulting in 160 samples.We create a balanced dataset by alternating between valid and invalid queries from LogicBench.This dataset is a synthetic dataset created by an LLM using its latent knowledge.Thus, the original premises align with world knowledge encoded in an LLM and the negation of a premise creates a counterfactual premise adapted to this latent knowledge.</p>
<p>Metrics.We use accuracy as a standard metric for classification tasks.We report execution rate as the fraction of parsable texts and valid accuracy as the accuracy on these parsable samples in the Appendix A due to space limitations.</p>
<p>LLMs.We test GPT 4o-mini, as well as a smaller and a larger variant for the three open-source LLM families: Gemma-2 (9b and 27b), Mistral (7b and Small), and Llama 3.1 (8b and 70b).If available, the tested LLMs are used in their instruction-tuned variant.Except for GPT 4o-mini, all models are provided from hugging-face and used with 4-bit quantisation.Our experiments use a default temperature of 1.0 to keep the output distributions unmodified.Furthermore, we verify the consistency of our findings by conducting experiments with a temperature setting of 0.</p>
<p>Syntax Parsers.For each grammar in § 4.2, we generate a parser using ANTLR42 to decouple the logical forms from the input requirements of the theorem prover.The FOL variant is adapted from the ANTLR grammars-v4 repository 3 .The parser validates the syntax and, if necessary, translates it into TPTP to pass on to the theorem prover.The Vampire theorem prover 4 processes this formal method's input, generating proofs by refutation to check whether the query logically follows from the context.</p>
<p>The syntax parser generates specific feedback for all error recovery strategies to prompt the LLM.The LLM prompt includes three in-context examples.When recovery is attempted, refinement is limited to three iterations.If the error persists, the random fallback strategy is applied.This setup allows for systematic comparison of the robustness of these recovery methods in the autoformalisation process.The generated warnings use three heuristics to identify semantic errors.First, checking for predicates and individuals only mentioned in the query to avoid an incomplete context.Second, ensuring that all predicates with the same identifier use the same number of parameters (checking for n-arity).Finally, avoiding similarly named predicates and individuals based on Levenshtein distance.</p>
<p>Evaluation</p>
<p>We provide three sets of insights into this section, organised as findings (F*).We quantitatively study the effect of adversarial and counterfactual perturbations on the performance of informal reasoners and autoformalisation methods.Then, we dive deeper into method variants.Finally, we analyse the nature of formalisation errors made by the models.</p>
<p>Robustness Analysis</p>
<p>F1: Noise perturbations have a stronger effect on autoformalisation methods than informal LLM reasoners.Table 1 shows that, on average, the accuracy of both direct and CoT informal reasoning remains between 73% and 74% in the face of added noise.While the autoformalisation method performs similarly to informal reasoners on the original dataset, its performance decreases between 4% and 11%.The accuracy drops especially with logical (L) and tautological (T) distractions, whose language formats trick the LLM into formalising the noisy clauses.On the other hand, the linguistically complex and more natural sentences of encyclopedic distractions show a minor effect, suggesting that LLMs successfully avoids formalising the more complicated but semantically less similar noise sentences.</p>
<p>F2: All LLM-based reasoning methods suffer a drop for counterfactual perturbations.Table 1 shows that counterfactual statements cause a significant decrease in performance for both the informal reasoners and autoformalisation methods of between 12% and 13% on average.Moreover, this observation also holds for all tested models, i.e., none are robust towards counterfactual perturbations across every evaluated dimension.Even the strongest model, GPT 4o-mini, yields a performance of 63-68%, which is relatively close to the random performance of 50%.The high impact of counterfactual statements (the single "not" inserted) could be due to the inability of LLMs to overwrite prior knowledge with explicitly stated information or memorisation of the answers.We study the error sources further in §6.3.</p>
<p>F3: Introducing multiple noise sentences has an effect only for logical distractions.We show the impact of introducing between one and four sentences for the two top-performing autoformalisation models in Figure 4.The figure shows similar trends with and without counterfactual perturbations.As additional logical distractions are introduced, the model performance consistently decreases.Tautological (T) distractions lead to a decline in accuracy with a single disruptive sentence, yet adding more noise does not worsen the outcome.The tautological corpus introduces truth constants for all sentences as a persistent unseen logical construct.Given that this leads only to a decrease for a single occurrence, we can assume that a model can consistently handle the same unseen logical construct.In contrast, the logical corpus increases the chance of adding text, requiring new, previously unseen reasoning constructs for each added sentence.The impact of encyclopedic noise remains negligible, generalising F1 to k sentences.Similarly, counterfactual perturbations remain much more effective for all settings, generalising F2.</p>
<p>Reasoning</p>
<p>Impact of Method Design</p>
<p>F4: CoT prompting is most impactful when both noise and counterfactual perturbations are applied.The accuracies for the individual LLMs in Table 1 show that the impact of CoT is negligible for noise-only datasets (first four columns).Meanwhile, the benefit from CoT is most pronounced in the datasets that combine noise and counterfactual perturbations.The better-performing informal prompting strategy for a model remains stable for all types of distractions.Still, the decline in performance due to counterfactuals leads to a less consistent preference for a specific prompting style.</p>
<p>F5: The best-performing grammar differs per model and is unstable across data versions.The evaluation of different logical forms for formal LLM-based reasoning in Table 2 shows the preference of some models for specific syntactic formats.Llama 3.1 70B has a considerable improvement of 12% with TPTP syntax on the original set, while Llama 3.1 8B benefits from the R-FOL syntax.However, all grammars show a declining accuracy trend and increased syntax errors for noise perturbations, where the best grammar loses its advantage over the rest.When comparing the grammars on the counterfactual partitions, we observe that TPTP is consistently more robust than the standard first-order logic grammar.Here, GPT 4o-mini shows a reduction from O to OC of 20% for FOL and only 12% for the TPTP grammar.Since this does not correlate with fewer syntax errors, the formalisation in TPTP prevents semantic errors for counterfactual premises.A positive reading of these results, especially the minor differences between FOL and R-FOL, is that autoformalisa- tion LLMs can adapt to the grammar syntax prescribed in the prompt without further loss in performance.</p>
<p>F6: Feedback does not help LLMs self-correct to mitigate robustness issues.Table 3 shows the results with different error recovery mechanisms.The results indicate that no feedback strategy emerges as a winner in the various datasets.All feedback variants reduce syntax errors for noise perturbations, but given the lack of a consistent increase in accuracy, the corrected formalisations are still most likely to contain semantic errors.The type of feedback message only has a minor influence on correcting syntax errors, whereas Llama 3.1 70b and GPT 4o-mini correct slightly more syntax errors with specific error messages.This finding aligns with [9], who also found that LLMs cannot consistently self-correct their reasoning after receiving relevant feedback.</p>
<p>Error Analysis</p>
<p>F7: Autoformalisation increases syntax errors for noise perturbations.The low performance for noise perturbations correlates with more syntax errors for all models and distraction categories (cf.execution rates in Table 4).The three worst-performing models (both Mistral models, Gemma-2 9b) generate, at best, for 37% and, at worst, for only 4% of the samples, a valid logical form.In particular, these models show an initial low execution rate between 41% and 53%.The error message feedback reveals issues with the template structure, including using incorrect keywords or adding conversational phrases.Noise perturbations emphasize this effect caus-ing the increased syntax errors.The second common syntax errors are perturbation-related.The added noise triggers the LLMs to hallucinate new syntax rules, e.g., introducing undefined truth constants as part of tautological distractions or switching from a camelCase to snake_case naming schema as part of logical distractions, which is likely caused by the increased difference between in-context examples and the test sample.The error analysis suggests that larger models are more robust towards noise perturbations.Gemma-2 9b and Llama3.1 8b produce more syntax errors than their larger counterparts.</p>
<p>As the increased syntax errors are mainly consistent for specific noise types, e.g., tautological noise creates undefined truth constants, it may be suboptimal to use a single, larger LLM for a broad set of formalisation tasks.Instead a framework with specialized agents that address particular syntax errors could mitigate these errors more effectively.Additionally, the accuracy of syntactically valid samples is higher than the informal reasoning methods for most distractions (Table 5), motivating a hybrid strategy: use formal reasoning when a valid formalisation is produced, and otherwise rely on informal reasoning as a backup.</p>
<p>F8: Autoformalisation increases semantic errors for counterfactuals.Unlike the introduced noise, counterfactual perturbations do not lead to more syntax errors.The execution rate in Table 4 is stable or improves for counterfactuals.However, we see a drop in accuracy for the counterfactual column OC in Table 1 and can conclude that the number of logical forms with semantic errors has to increase.This suggests that the introduced negation is not correctly formalised with respect to the semantics of these samples.Instead of genuine deductive reasoning capabilities informal and autoformalisation rely on prior knowledge.</p>
<p>Devising principled mechanisms for verifying semantic validity (e.g., ensuring that predicates are defined before being used in a query) could reduce semantic errors for counterfactuals.Looking at the warnings generated by the feedback mechanism, for GPT 4omini, 161 warning messages are generated on the unperturbed data pointing towards specific semantic errors.54 of these were fixed with a single iteration.Across all models, the most frequent warning is not considering predicates and individuals as part of the context.This suggest that verification feedback is able to verify semantic validity and incorporating this feedback can mitigate semantic errors.</p>
<p>Conclusion</p>
<p>We presented the first study of the robustness of formal and informal LLM-based deductive reasoning methods by introducing two types of perturbations: adversarial noise and counterfactual statements.These perturbations were used to examine the methodological aspects of LLM reasoners based on their format, syntax, and feedback mechanism for error recovery.While adversarial noise only affects autoformalisation approaches, counterfactual statements remain a significant challenge for all variants of the tested method.While feedback strategies may lead to fewer syntax errors in autoformalisation methods, the refined formalisations tend to be semantically incorrect, failing to increase accuracy.We call on future work to devise more advanced mechanisms for detecting, reporting, and incorporating semantic errors.One possible mechanism to mitigate these issues could be the usage of models fine-tuned on error correction data.We also anticipate generalising the study in this paper to other logical deduction datasets and examining characteristics like the placement of adversarial noise and the effects of quantisation to strengthen the generalizability of the reported insights.</p>
<p>A Experiments
Formalisation O Distraction Counterfactual E L T OC EC LC TC Gemma-2 9b
Informal (direct) 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Informal (CoT) 0.99 0.99 0.98 0.98 0.99 0.99 0.99 0.99 Formal (FOL) 0.41 0.17 0.04 0.09 0.49 0.15 0.07 0.04 27b Informal (direct) 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Informal (CoT) 1.00 1.00 1.00 1.00 1.00 0.99 1.00 0.99 Formal (FOL) 0.71 0.72 0.37 0.24 0.74 0.71 0.34 0.17
Mistral</p>
<p>D Prompts</p>
<p>D.1 Direct prompt</p>
<p>Given t h e c o n t e x t and q u e s t i o n , a n s w e r t h e q u e s t i o n and c o n s i d e r t h a t n o t n e c e s s a r i l y t h e whole c o n t e x t i s r e l e v a n t .Answer t h e q u e s t i o n ONLY i n ' yes ' o r ' no ' .P l e a s e u s e t h e below f o r m a t : C o n t e x t : [ t e x t w i t h l o g i c a l r u l e s ] Q u e s t i o n : [ q u e s t i o n b a s e d on c o n t e x t ] Answer : Yes / No −−−− C o n t e x t : I f someone w a l k s i n t h e r a i n , t h e y w i l l g e t wet .C o n v e r s e l y , i f someone e x e r c i s e s a l o t , t h e y w i l l g e t f i t .L e a d e r s o f a c o u n t r y f o r l i f e a r e e i t h e r a k i n g o r a q u e e n . 1 .J o h n w a l k s i n t h e r a i n o r he w i l l n o t g e t f i t 2 .I f J o h n w a l k s i n t h e r a i n he w i l l g e t wet .3 .I f J o h n w i l l n o t g e t f i t he c a n n o t e x e r c i s e a l o t .Answer : Yes −−−− C o n t e x t : I f a p e r s o n l e a v e s l a t e , t h e y w i l l m i s s t h e i r t r a i n .I n t h i s p a r t i c u l a r s i t u a t i o n , James l e f t l a t e .Q u e s t i o n : Does t h i s e n t a i l t h a t he w i l l n o t m i s s h i s t r a i n ?R e a s o n i n g s t e p s :</p>
<p>1 .James l e f t l a t e 2 .James l e f t l a t e , he w i l l m i s s h i s t r a i n .</p>
<p>D.2 CoT prompt</p>
<p>Task D e s c r i p t i o n : Given t h e c o n t e x t and q u e s t i o n , t h i n k s t e p −by − s t e p l o g i c a l l y t o a n s w e r t h e q u e s t i o n and c o n s i d e r t h a t n o t n e c e s s a r i l y t h e whole c o n t e x t i s r e l e v a n t .Answer t h e q u e s t i o n ONLY i n ' yes ' o r ' no ' .P l e a s e u s e t h e below f o r m a t : C o n t e x t : [ t e x t w i t h l o g i c a l r u l e s ] Q u e s t i o n : [ q u e s t i o n b a s e d on c o n t e 1 .J o h n w a l k s i n t h e r a i n o r he w i l l n o t g e t f i t 2 .I f J o h n w a l k s i n t h e r a i n he w i l l g e t wet .3 .I f J o h n w i l l n o t g e t f i t he c a n n o t e x e r c i s e a l o t .Answer : Yes −−−− C o n t e x t : I f a p e r s o n l e a v e s l a t e , t h e y w i l l m i s s t h e i r t r a i n .I n t h i s p a r t i c u l a r s i t u a t i o n , James l e f t l a t e .Q u e s t i o n : Does t h i s e n t a i l t h a t he w i l l n o t m i s s h i s t r a i n ?R e a s o n i n g s t e p s :</p>
<p>1 .James l e f t l a t e 2 .James l e f t l a t e , he w i l l m i s s h i s t r a i n .e x p r 1 ∨ e x p r 2 3 ) l o g i c a l e x c l u s i v e d i s j u n c t i o n o f e x p r 1 and e x p r 2 : e x p r 1 ⊕ e x p r 2 4 ) l o g i c a l n e g a t i o n o f e x p r 1 : ¬ e x p r 1 5 ) e x p r 1 i m p l i e s e x p r 2 : e x p r 1 → e x p r 2 6 ) e x p r 1 i f and o n l y i f e x p r 2 : e x p r 1 ↔ e x p r 2 7 ) l o g i c a l u n i v e r s a l q u a n t i f i c a t i o n : ∀x 8 ) l o g i c a l e x i s t e n t i a l q u a n t i f i c a t i o n : ∃x −−−− P r o b l e m : I f someone w a l k s i n t h e r a i n , t h e y w i l l g e t wet .C o n v e r s e l y , i f someone e x e r c i s e s a l o t , t h e y w i l l g e t f i t .L e a d e r s o f a c o u n t r y f o r l i f e a r e e i t h e r a k i n g o r a q u e e n .I t i s known t h a t a t l e a s t one o f t h e f o l l o w i n g s t a t e m e n t s i s t r u e : ( 1 ) e i t h e r J o h n w a l k s i n t h e r a i n and ( 2 ) he w i l l n o t g e t f i t .I t i s p o s s i b l e t h a t s o l e l y ( 1 ) i s t r u e , o r s o l e l y ( 2 ) i s t r u e , o r e v e n b o t h a r e t r u e s i m u l t a n e o u s l y .Q u e s t i o n : Can we s a y a t l e a s t one o f t h e f o l l o w i n g must a l w a y s be t r u e ?( a ) he w i l l g e t wet and ( b ) he d o e s n o t e x e r c i s e s a l o t ?P r e d i c a t e s : W a l k s I n R a i n ( x ) : : : x w a l k s i n t h e r a i n .GetWet ( x ) : : : x g e t s wet .E x e r c i s e s A L o t ( x ) : : : x e x e r c i s e s a l o t .G e t F i t ( x ) : : : x g e t s f i t .P r e m i s e s : ∀x ( W a l k s I n R a i n ( x ) → GetWet ( x ) ) : : : I f someone w a l k s i n t h e r a i n , t h e y w i l l g e t wet .∀x ( E x e r c i s e s A L o t ( x ) → G e t F i t ( x ) ) : : : I f someone e x e r c i s e s a l o t , t h e y w i l l g e t f i t .( W a l k s I n R a i n ( j o h n ) ∨ ¬ G e t F i t ( j o h n ) ) : : : J o h n w a l k s i n t h e r a i n o r he w i l l n o t g e t f i t .−−−− P r o b l e m : I f a p e r s o n l e a v e s l a t e , t h e y w i l l m i s s t h e i r t r a i n .I n t h i s p a r t i c u l a r s i t u a t i o n , James l e f t l a t e .Q u e s t i o n : Does t h i s e n t a i l t h a t he w i l l n o t m i s s h i s t r a i n ?P r e d i c a t e s : L e a v e L a t e ( x ) : : : x l e a v e s l a t e .M i s s T r a i n ( x ) : : : x m i s s e s t h e i r t r a i n .P r e m i s e s : ∀x ( L e a v e L a t e ( x ) → M i s s T r a i n ( x ) ) : : : I f a p e r s o n l e a v e s l a t e , t h e y w i l l m i s s t h e i r t r a i n .L e a v e L a t e ( j a m e s ) : : : James l e a v s l a t e .</p>
<p>Figure 1 .
1
Figure 1.Overview of our methodology for investigating the robustness of reasoning with LLMs.Our perturbations (noise and counterfactuals) are shown in orange and teal, respectively.The three dimensions of our LLM-based methodological framework (reasoning format, syntax, and error recovery mechanism) are shown in blue.</p>
<p>Figure 2 (Figure 2 .
22
Figure 2. Example of a premise and its perturbations.</p>
<p>∀Figure 3 .
3
Figure 3. Examples of the three syntaxes: FOL, R-FOL, and TPTP.</p>
<p>Figure 4 .
4
Figure 4. Influence of the number of noisy sentences for FOL.</p>
<p>1 .
1
J i l l g o e s t o t h e o f f i c e o r s h e g o e s home . 2 .I f J i l l d o e s n o t go t o t h e o f f i c e s h e must go home i n s t e a d .Answer : Yes −−−− Regular expression for answer extraction: r " ( .<em> ) a n s w e r \ : \ s * ( ?P&lt; answer &gt; ( y e s ) | ( no ) ) ( .</em> ) " D.3 Autoformalistion prompt Task D e s c r i p t i o n : Given a p r o b l e m d e s c r i p t i o n and a q u e s t i o n .The t a s k i s t o p a r s e t h e p r o b l e m and t h e q u e s t i o n i n t o f i r s t − o r d e r l o g i c f o r m u l a r s .F o l l o w e x a c t l y t h e g i v e n s t r u c t u r e and c o n s i d e r t h a t n o t n e c e s s a r i l y t h e whole c o n t e x t i s r e l e v a n t .The grammar o f t h e f i r s t − o r d e r l o g i c f o r m u l a r i s d e f i n e d a s f o l l o w s : 1 ) l o g i c a l c o n j u n c t i o n o f e x p r 1 and e x p r 2 : e x p r 1 ∧ e x p r 2 2 ) l o g i c a l d i s j u n c t i o n o f e x p r 1 and e x p r 2 :</p>
<p>C o n c l u s i o n : GetWet ( j o h n ) ∧ ¬ E x e r c i s e s A L o t ( j o h n ) : : : J o h n w i l l g e t wet o r he d o e s n o t e x e r c i s e s a l o t .</p>
<p>C o n c l u s i o n : ¬ M i s s T r a i n ( j a m e s ) : : : James d o e s n o t m i s s h i s t r a i n .−−−− P r o b l e m : I t i s known t h a t one o f t h e f o l l o w i n g o p t i o n s i s t r u e : someone g o e s t o t h e o f f i c e o r someone g o e s home .However , J i l l d o e s n o t go t o t h e o f f i c e .I f some p e t i n t h e o f f i c e b a r k s , t h e n i t i s n o t d e a d .Q u e s t i o n : Does t h i s i m p l y t h a t J i l l g o e s home ?P r e d i c a t e s : G o e s T o O f f i c e ( x ) : : : x g o e s t o t h e o f f i c e .GoesHome ( x ) : : : x g o e s home .P r e m i s e s : ∀x ( G o e s T o O f f i c e ( x ) ∨ GoesHome ( x ) ) : : : E i t h e r someone g o e s t o t h e o f f i c e o r someone g o e s home .¬ G o e s T o O f f i c e ( j i l l ) : : : J i l l d o e s n o t go t o t h e o f f i c e .C o n c l u s i o n : GoesHome ( j i l l ) : : : J i l l g o e s home .−−−−</p>
<p>Table 1 .
1
Accuracies of informal and autoformalisation-based deductive reasoners.The best overall model per dataset is underlined; the best model version is marked in bold.
FormatODistraction E L T OC EC LC TC CounterfactualGemma-29b 27bInformal (direct) 0.78 0.80 0.79 0.77 0.58 0.52 0.50 0.59 Informal (CoT) 0.72 0.78 0.73 0.76 0.61 0.57 0.60 0.66 Formal (FOL) 0.62 0.58 0.52 0.53 0.63 0.52 0.46 0.46 Informal (direct) 0.71 0.69 0.66 0.68 0.59 0.51 0.54 0.59 Informal (CoT) 0.66 0.65 0.64 0.63 0.62 0.58 0.62 0.64Formal (FOL) 0.74 0.74 0.61 0.61 0.72 0.67 0.58 0.51Mistral7B SmallInformal (direct) 0.77 0.77 0.75 0.79 0.63 0.54 0.54 0.66 Informal (CoT) 0.79 0.75 0.77 0.78 0.55 0.52 0.54 0.58 Formal (FOL) 0.62 0.58 0.54 0.57 0.50 0.54 0.51 0.52 Informal (direct) 0.77 0.76 0.76 0.75 0.61 0.51 0.56 0.59 Informal (CoT) 0.72 0.72 0.72 0.71 0.62 0.59 0.62 0.68 Formal (FOL) 0.68 0.59 0.53 0.64 0.54 0.55 0.49 0.51Llama-3.18B 70BInformal (direct) 0.63 0.61 0.64 0.66 0.61 0.62 0.59 0.61 Informal (CoT) 0.73 0.73 0.71 0.72 0.62 0.59 0.61 0.65 Formal (FOL) 0.77 0.71 0.63 0.52 0.60 0.58 0.55 0.52 Informal (direct) 0.77 0.74 0.74 0.73 0.62 0.53 0.56 0.64 Informal (CoT) 0.78 0.75 0.76 0.76 0.64 0.61 0.66 0.73 Formal (FOL) 0.74 0.73 0.71 0.71 0.66 0.62 0.59 0.57GPT4o-miniInformal (direct) 0.78 0.77 0.79 0.79 0.64 0.61 0.61 0.63 Informal (CoT) 0.80 0.80 0.81 0.82 0.68 0.63 0.68 0.64 Formal (FOL) 0.84 0.82 0.73 0.79 0.63 0.62 0.57 0.54Informal (direct) 0.74 0.73 0.73 0.73 0.61 0.55 0.56 0.62AvgInformal (CoT) 0.74 0.74 0.73 0.74 0.62 0.58 0.62 0.65Formal (FOL) 0.72 0.68 0.61 0.62 0.61 0.59 0.54 0.52</p>
<p>Table 2 .
2
Accuracies of different formalisation grammars for autoformalisation.
Grammar O SyntaxDistraction E L T OC EC LC TC CounterfactualLlama-3.18B 70BFOL R-FOL 0.78 0.69 0.62 0.53 0.58 0.55 0.54 0.52 0.77 0.71 0.61 0.53 0.58 0.55 0.52 0.56 TPTP 0.73 0.67 0.55 0.51 0.68 0.54 0.46 0.51 FOL 0.76 0.73 0.71 0.72 0.67 0.57 0.63 0.56 R-FOL 0.76 0.73 0.67 0.71 0.64 0.57 0.53 0.64 TPTP 0.88 0.84 0.81 0.72 0.81 0.68 0.67 0.68GPT4o-miniFOL R-FOL 0.84 0.77 0.70 0.78 0.72 0.56 0.54 0.63 0.84 0.82 0.72 0.78 0.64 0.63 0.61 0.51 TPTP 0.83 0.82 0.71 0.71 0.69 0.63 0.57 0.57FeedbackODistraction E L T OC EC LC TC CounterfactualNo recovery 0.77 0.72 0.62 0.53 0.59 0.58 0.56 0.56Llama-3.18BError type Error message 0.78 0.71 0.67 0.55 0.59 0.53 0.64 0.49 0.79 0.71 0.63 0.56 0.66 0.54 0.52 0.51 Warning 0.74 0.66 0.58 0.55 0.55 0.60 0.49 0.49 No recovery 0.77 0.72 0.73 0.71 0.64 0.59 0.61 0.5670BError type Error message 0.71 0.70 0.73 0.71 0.64 0.59 0.54 0.64 0.72 0.70 0.72 0.73 0.62 0.56 0.60 0.58Warning0.69 0.72 0.72 0.72 0.62 0.65 0.61 0.63GPT4o-miniNo recovery 0.84 0.82 0.73 0.79 0.64 0.62 0.56 0.56 Error type 0.83 0.79 0.74 0.76 0.67 0.57 0.56 0.56 Error message 0.84 0.78 0.77 0.80 0.62 0.59 0.56 0.56 Warning 0.84 0.75 0.73 0.76 0.70 0.61 0.61 0.55</p>
<p>Table 3 .
3
Accuracies of error recovery strategies.</p>
<p>Table 4 .
4
Comparison of execution rate of informal and autoformalisation-based LLM-based deductive reasoners.
7BInformal (direct) 0.99 0.99 0.99 0.99 0.98 0.97 0.98 0.97 Informal (CoT) 0.96 0.95 0.96 0.95 0.93 0.96 0.98 0.93Formal (FOL) 0.51 0.37 0.23 0.33 0.61 0.38 0.24 0.29SmallInformal (direct) 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Informal (CoT) 1.00 1.00 1.00 1.00 1.00 0.99 1.00 1.00 Formal (FOL) 0.53 0.34 0.12 0.36 0.62 0.36 0.14 0.36Llama-3.18B 70BInformal (direct) 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Informal (CoT) 1.00 1.00 0.98 0.98 0.98 1.00 0.99 0.93 Formal (FOL) 0.83 0.70 0.47 0.28 0.86 0.70 0.40 0.37 Informal (direct) 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Informal (CoT) 0.99 1.00 1.00 0.99 1.00 0.99 0.99 1.00Formal (FOL) 0.78 0.78 0.70 0.66 0.88 0.84 0.73 0.66GPT4o-miniInformal (direct) 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Informal (CoT) 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Formal (FOL) 0.98 0.96 0.74 0.87 0.99 0.93 0.69 0.85FormalisationODistraction E L T OC EC LC TC CounterfactualGemma-29b 27bInformal (direct) 0.78 0.80 0.79 0.77 0.58 0.52 0.50 0.59 Informal (CoT) 0.72 0.78 0.73 0.75 0.61 0.58 0.60 0.66 Formal (FOL) 0.81 0.83 0.70 0.91 0.72 0.58 0.73 0.57 Informal (direct) 0.71 0.69 0.66 0.68 0.59 0.51 0.54 0.59 Informal (CoT) 0.66 0.65 0.64 0.63 0.62 0.58 0.62 0.64Formal (FOL) 0.85 0.82 0.78 0.83 0.75 0.71 0.74 0.54Mistral7B SmallInformal (direct) 0.77 0.78 0.75 0.79 0.63 0.54 0.54 0.65 Informal (CoT) 0.80 0.77 0.78 0.80 0.57 0.54 0.55 0.58 Formal (FOL) 0.70 0.76 0.69 0.66 0.48 0.38 0.58 0.46 Informal (direct) 0.77 0.76 0.76 0.75 0.61 0.51 0.56 0.59 Informal (CoT) 0.72 0.72 0.72 0.71 0.62 0.60 0.62 0.68 Formal (FOL) 0.82 0.77 0.86 0.81 0.58 0.63 0.52 0.69Llama-3.18B 70BInformal (direct) 0.63 0.61 0.64 0.66 0.61 0.62 0.59 0.61 Informal (CoT) 0.73 0.74 0.72 0.72 0.62 0.59 0.61 0.65 Formal (FOL) 0.82 0.80 0.78 0.65 0.61 0.60 0.55 0.58 Informal (direct) 0.77 0.74 0.74 0.73 0.62 0.53 0.56 0.64 Informal (CoT) 0.78 0.75 0.76 0.76 0.64 0.60 0.66 0.73 Formal (FOL) 0.82 0.80 0.80 0.83 0.66 0.63 0.64 0.61GPT4o-miniInformal (direct) 0.78 0.77 0.79 0.79 0.64 0.61 0.61 0.63 Informal (CoT) 0.80 0.80 0.81 0.82 0.68 0.63 0.68 0.64 Formal (FOL) 0.85 0.83 0.80 0.82 0.64 0.64 0.63 0.57</p>
<p>Table 5 .
5
Comparison of valid accuracy of informal and autoformalisation-based LLM-based deductive reasoners.
Grammar O SyntaxDistraction E L T OC EC LC TC CounterfactualGemma-29b 27bFOL R-FOL 0.71 0.55 0.52 0.55 0.55 0.52 0.54 0.54 0.63 0.55 0.53 0.56 0.55 0.52 0.49 0.52 TPTP 0.68 0.53 0.52 0.52 0.68 0.54 0.52 0.49 FOL 0.73 0.73 0.59 0.59 0.70 0.62 0.56 0.55 R-FOL 0.77 0.69 0.67 0.64 0.66 0.63 0.55 0.56TPTP0.80 0.76 0.59 0.53 0.74 0.69 0.66 0.49Mistral7B SmallFOL R-FOL 0.60 0.60 0.52 0.58 0.49 0.49 0.49 0.54 0.59 0.59 0.58 0.53 0.49 0.46 0.57 0.43 TPTP 0.65 0.61 0.51 0.50 0.53 0.46 0.47 0.48 FOL 0.64 0.62 0.52 0.64 0.51 0.58 0.53 0.51 R-FOL 0.71 0.64 0.53 0.60 0.52 0.57 0.54 0.56 TPTP 0.66 0.58 0.54 0.48 0.59 0.52 0.49 0.50</p>
<p>Table 6 .
6
Comparison of accuracies between different formalisation grammar syntaxes of autoformalisation.
Grammar O SyntaxDistraction E L T OC EC LC TC CounterfactualGemma-29b 27bFOL R-FOL 0.63 0.27 0.13 0.17 0.64 0.33 0.14 0.14 0.41 0.17 0.04 0.09 0.49 0.15 0.07 0.04 TPTP 0.44 0.13 0.04 0.01 0.47 0.17 0.04 0.01 FOL 0.71 0.72 0.37 0.24 0.74 0.71 0.34 0.17 R-FOL 0.83 0.78 0.50 0.44 0.78 0.77 0.53 0.32TPTP0.82 0.82 0.41 0.08 0.76 0.84 0.39 0.01Mistral7B SmallFOL R-FOL 0.54 0.45 0.24 0.33 0.68 0.39 0.25 0.32 0.51 0.37 0.23 0.33 0.61 0.38 0.24 0.29 TPTP 0.45 0.31 0.08 0.10 0.50 0.26 0.07 0.07 FOL 0.53 0.34 0.12 0.36 0.62 0.36 0.14 0.36 R-FOL 0.68 0.41 0.17 0.38 0.69 0.38 0.18 0.38 TPTP 0.45 0.25 0.05 0.03 0.41 0.28 0.06 0.02Llama-3.18B 70BFOL R-FOL 0.81 0.65 0.38 0.25 0.84 0.64 0.43 0.31 0.83 0.70 0.47 0.28 0.86 0.70 0.40 0.37 TPTP 0.75 0.49 0.21 0.01 0.74 0.47 0.20 0.02 FOL 0.78 0.78 0.70 0.66 0.88 0.84 0.73 0.66 R-FOL 0.80 0.81 0.65 0.76 0.93 0.89 0.67 0.72 TPTP 0.92 0.93 0.80 0.58 0.93 0.89 0.78 0.51GPT4o-miniFOL R-FOL 0.96 0.91 0.68 0.83 0.95 0.96 0.62 0.87 0.98 0.96 0.74 0.87 0.99 0.93 0.69 0.85 TPTP 0.93 0.91 0.52 0.62 0.91 0.92 0.49 0.47</p>
<p>Table 7 .
7
Comparison of execution rate between different formalisation grammar syntaxes of autoformalisation.
Grammar O SyntaxDistraction E L T OC EC LC TC CounterfactualGemma-29b 27bFOL R-FOL 0.82 0.68 0.65 0.74 0.60 0.58 0.55 0.57 0.81 0.83 0.70 0.91 0.72 0.58 0.73 0.57 TPTP 0.92 0.75 0.74 0.67 0.79 0.63 0.57 1.00 FOL 0.85 0.82 0.78 0.83 0.75 0.71 0.74 0.54 R-FOL 0.82 0.75 0.81 0.81 0.70 0.66 0.65 0.65TPTP0.87 0.83 0.79 0.88 0.79 0.69 0.76 1.00Mistral7B SmallFOL R-FOL 0.66 0.73 0.66 0.72 0.44 0.41 0.52 0.54 0.70 0.76 0.69 0.66 0.48 0.38 0.58 0.46 TPTP 0.83 0.79 0.75 0.80 0.62 0.46 0.36 0.55 FOL 0.82 0.77 0.86 0.81 0.58 0.63 0.52 0.69 R-FOL 0.81 0.73 0.64 0.79 0.55 0.62 0.55 0.70 TPTP 0.85 0.81 0.81 0.82 0.70 0.61 0.22 1.00Llama-3.18B 70BFOL R-FOL 0.84 0.77 0.80 0.74 0.62 0.58 0.59 0.56 0.82 0.80 0.78 0.65 0.61 0.60 0.55 0.58 TPTP 0.82 0.80 0.72 0.67 0.76 0.68 0.50 0.67 FOL 0.82 0.80 0.80 0.83 0.66 0.63 0.64 0.61 R-FOL 0.81 0.77 0.77 0.77 0.67 0.61 0.58 0.64 TPTP 0.90 0.88 0.90 0.90 0.84 0.70 0.67 0.78GPT4o-miniFOL R-FOL 0.86 0.80 0.80 0.83 0.72 0.56 0.59 0.63 0.85 0.83 0.80 0.82 0.64 0.64 0.63 0.57 TPTP 0.85 0.85 0.85 0.85 0.72 0.66 0.70 0.61</p>
<p>Table 8 .
8
Comparison of valid accuracies between different formalisation grammar syntaxes of autoformalisation.</p>
<p>https://github.com/Fab-Hop/langdeductive
https://www.antlr.org/
https://github.com/antlr/grammars-v4
https://vprover.github.io
AcknowledgementsThis work has received funding from the European Union's Horizon Europe research and innovation programme under the Marie Skłodowska-Curie grant agreement No 101073307.B Tautology corpus• False is not true.• True is not false.• Not false is true.• Not true is false.• False and true is not true.• False and not true is false.• False and not false is false.• Not true and true is false.• Not true and false is false.• True and false is not true.• True and true is not false.• True and not false is true.• Not false and true is true.• Not false and false is false.• True or not true is true.• True or true is true.• False or not false is true.• False or not true is false.• Not true or false is not true.• Not true or true is true.• Not false or false is true.• Not false or true is not false.C Inference rules for counterfactual statements
Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, T Hoefler, 10.1609/aaai.v38i16.29720Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20. the 34th International Conference on Neural Information Processing Systems, NIPS '202020</p>
<p>HotFlip: White-box adversarial examples for text classification. J Ebrahimi, A Rao, D Lowd, D Dou, 10.18653/v1/P18-2006Proceedings of the 56th Annual Meeting of the Assoc. for Computational Linguistics. the 56th Annual Meeting of the Assoc. for Computational Linguistics20182Short Papers)</p>
<p>PAL: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Logic and conversation. Syntax and semantics. H Grice, 1975</p>
<p>FOLIO: Natural language reasoning with first-order logic. S Han, H Schoelkopf, Y Zhao, 10.18653/v1/2024.emnlp-main.1229Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Investigating the robustness of deductive reasoning with large language models. F Hoppe, F Ilievski, J.-C Kalo, 2025</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, S Mishra, H S Zheng, A W Yu, X Song, D Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>LeanReasoner: Boosting complex logical reasoning with lean. D Jiang, M Fonseca, S Cohen, 10.18653/v1/2024.naacl-long.416Proceedings of the 2024 Conference of the North American Chapter of the Assoc. for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Assoc. for Computational Linguistics: Human Language Technologies20241</p>
<p>Position: Llms can't plan, but can help planning in llm-modulo frameworks. S Kambhampati, K Valmeekam, L Guan, M Verma, K Stechly, S Bhambri, L Saldyt, A Murthy, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>LOGIC-LM++: Multistep refinement for symbolic formulations. S Kirtania, P Gupta, A Radhakrishna, Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024). the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)2024</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Measuring faithfulness in chain-of-thought reasoning. T Lanham, A Chen, A Radhakrishnan, 2023</p>
<p>Recall: A benchmark for llms robustness against external counterfactual knowledge. Y Liu, L Huang, S Li, S Chen, H Zhou, F Meng, J Zhou, X Sun, 2023</p>
<p>LogicAttack: Adversarial attacks for evaluating logical consistency of natural language inference. M Nakamura, S Mashetty, M Parmar, N Varshney, C Baral, Findings of the Assoc. for Computational Linguistics: EMNLP 2023. 2023</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. T Olausson, A Gu, B Lipkin, C Zhang, A Solar-Lezama, J Tenenbaum, R Levy, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Wang, 10.18653/v1/2023.findings-emnlp.248Findings of the Assoc. for Computational Linguistics: EMNLP 2023. 2023</p>
<p>LogicBench: Towards systematic evaluation of logical reasoning ability of large language models. M Parmar, N Patel, N Varshney, M Nakamura, M Luo, S Mashetty, A Mitra, C Baral, Proceedings of the 62nd Annual Meeting of the Assoc. for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Assoc. for Computational Linguistics20241</p>
<p>Logic for natural language analysis. F C N Pereira, 1982The University of EdinburghPhD thesis</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Y Razeghi, R L Logan, I V , M Gardner, S Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Assoc. for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Superglue: Learning feature matching with graph neural networks. P.-E Sarlin, D Detone, T Malisiewicz, A Rabinovich, IEEE Conference on Computer Vision and Pattern Recognition. 2020</p>
<p>Robust text classification: Analyzing prototype-based networks. Z Sourati, D G Deshpande, F Ilievski, K Gashteovski, S Saralajew, Findings of the Assoc. for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, Transactions on Machine Learning Research. 2023</p>
<p>Stepping Stones in the TPTP World. G Sutcliffe, Proceedings of the 12th International Joint Conference on Automated Reasoning. the 12th International Joint Conference on Automated Reasoning2024</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. O Tafjord, B Dalvi, P Clark, 10.18653/v1/2021.findings-acl.317Findings of the Assoc. for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>On the difficulty of faithful chain-of-thought reasoning in large language models. S H Tanneru, D Ley, C Agarwal, H Lakkaraju, Trustworthy Multi-modal Foundation Models and AI Agents (TiFA). 2024</p>
<p>Measure and improve robustness in NLP models: A survey. X Wang, H Wang, D Yang, Proceedings of the 2022 Conference of the North American Chapter of the Assoc. for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Assoc. for Computational Linguistics: Human Language Technologies2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Rupbench: Benchmarking reasoning under perturbations for robustness evaluation in large language models. Y Wang, Y Zhao, 2024</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 2023</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. X Ye, G Durrett, Advances in Neural Information Processing Systems. 2022</p>
<p>Self-edit: Fault-aware code editor for code generation. K Zhang, Z Li, J Li, G Li, Z Jin, Proceedings of the 61st Annual Meeting of the Assoc. for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Assoc. for Computational Linguistics20231</p>
<p>Analytical reasoning of text. W Zhong, S Wang, D Tang, 10.18653/v1/2022.findings-naacl.177Findings of the Assoc. for Computational Linguistics: NAACL 2022. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
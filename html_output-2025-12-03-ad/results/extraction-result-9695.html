<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9695 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9695</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9695</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-271404381</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.17022v1.pdf" target="_blank">Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text. Extending this methodology to assess human-written text could significantly benefit educational settings by providing direct feedback to enhance writing skills, although this application is not straightforward. In this paper, we investigate whether LLMs can effectively assess human-written text for educational purposes. We collected 100 texts from 32 Korean students across 15 types of writing and employed GPT-4-Turbo to evaluate them using grammaticality, fluency, coherence, consistency, and relevance as criteria. Our analyses indicate that LLM evaluators can reliably assess grammaticality and fluency, as well as more objective types of writing, though they struggle with other criteria and types of writing. We publicly release our dataset and feedback.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9695.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9695.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo (LLM-as-a-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo-April used as an LLM-as-a-Judge for evaluating human-written student texts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses GPT-4-Turbo-April as an automated evaluator to score and provide verbal feedback on 100 student-written texts across five axes (grammaticality, fluency, coherence, consistency, relevance), and then collects author/student judgments about the reasonableness of those evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation of human-written student texts (Korean and English) across 15 writing types including essays, reports, diaries, scripts, and presentations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4-Turbo-April (referred to as GPT-4-Turbo in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prometheus-eval pipeline with default hyperparameters plus five custom rubrics; GPT-4-Turbo provided for each text (100 texts) per-criterion verbal feedback and an integer score (1-5) on five axes (grammaticality, fluency, coherence, consistency, relevance), yielding 500 judgments; prompts and code provided as supplementary material.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Post-hoc meta-evaluation by the original student authors (32 participants): each student received GPT-4's verbal feedback and scores for their own text and indicated whether the judgments were 'valid', 'overly critical', or 'overly optimistic' (participants paid; 15 minutes per instance). This is a self-verification setup rather than independent third-party grading.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent 'valid' (human meta-evaluation of GPT-4 judgments) reported per criterion and per writing category. Reported validity: Fluency 93%, Grammaticality 87%, Coherence 81%, Consistency 82%, Relevance 77%. Validity by writing type (examples): Process Essay 100.00%, Descriptive Essay 95.45%, Scientific Report 92.31%, Self Introduction Essay 50.00%, Argumentative Essay 36.36%, Diary 20.00%. Also reported average LLM scores by age group (Age 11-14 vs 17-19) to show discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Subjective nuance and appropriateness for informal tones are degraded: LLM-as-a-Judge struggles on subjective or informal writing (e.g., diaries, self-introductions, argumentative essays), often flagging coherence/consistency issues when informality is acceptable; variability across writing types means LLM judgments are less reliable for subjective tasks. Additionally, there are cases of apparent optimism in consistency/relevance scores (higher LLM scores) and inconsistencies in how informality is interpreted, indicating loss of human-like sensitivity to context and genre conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Low human-validated agreement for subjective categories: 'Diary' judgments valid only 20.00%, 'Argumentative Essay' 36.36%, 'Self Introduction Essay' 50.00%, indicating many judgments were deemed unreasonable by the student authors. The paper notes coherence and consistency criteria flagged informality in subjective tasks as problematic—i.e., LLM treated acceptable informal tone as incoherent or inconsistent. Relevance had the lowest validity among some categories and also the highest proportion of 'overly optimistic' judgments (Relevance: 19% overly optimistic; Consistency: 14% overly optimistic).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM-as-a-Judge performed well on more objective criteria and text types: Fluency (93% validity) and Grammaticality (87% validity) were largely validated by students, and writing types with objective characteristics (Process Essay 100%, Descriptive Essay 95.45%, Scientific Report 92.31%) had high validity ratios. GPT-4 also discriminated between age groups (older students received higher average scores across criteria), suggesting robustness in detecting broad skill differences. The authors note the study is preliminary and not a controlled same-instruction comparison, which limits some conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Experimental Results & Discussions; Table 4 (criterion-level validity and scores), Table 5 (validity by writing category), Table 6 (scores by age group); also Methods: Evaluation Pipeline and Verification of Evaluation Results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prometheus: Inducing fine-grained evaluation capability in language models <em>(Rating: 2)</em></li>
                <li>Gpteval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Alpacaeval: An automatic evaluator of instructionfollowing models <em>(Rating: 2)</em></li>
                <li>Chateval: Towards better llm-based evaluators through multi-agent debate <em>(Rating: 2)</em></li>
                <li>Replacing judges with juries: Evaluating llm generations with a panel of diverse models <em>(Rating: 2)</em></li>
                <li>The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9695",
    "paper_id": "paper-271404381",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "GPT-4-Turbo (LLM-as-a-Judge)",
            "name_full": "GPT-4-Turbo-April used as an LLM-as-a-Judge for evaluating human-written student texts",
            "brief_description": "This paper uses GPT-4-Turbo-April as an automated evaluator to score and provide verbal feedback on 100 student-written texts across five axes (grammaticality, fluency, coherence, consistency, relevance), and then collects author/student judgments about the reasonableness of those evaluations.",
            "citation_title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education",
            "mention_or_use": "use",
            "task_domain": "Evaluation of human-written student texts (Korean and English) across 15 writing types including essays, reports, diaries, scripts, and presentations",
            "llm_judge_model": "GPT-4-Turbo-April (referred to as GPT-4-Turbo in the paper)",
            "llm_judge_setup": "Prometheus-eval pipeline with default hyperparameters plus five custom rubrics; GPT-4-Turbo provided for each text (100 texts) per-criterion verbal feedback and an integer score (1-5) on five axes (grammaticality, fluency, coherence, consistency, relevance), yielding 500 judgments; prompts and code provided as supplementary material.",
            "human_evaluation_setup": "Post-hoc meta-evaluation by the original student authors (32 participants): each student received GPT-4's verbal feedback and scores for their own text and indicated whether the judgments were 'valid', 'overly critical', or 'overly optimistic' (participants paid; 15 minutes per instance). This is a self-verification setup rather than independent third-party grading.",
            "agreement_metric": "Percent 'valid' (human meta-evaluation of GPT-4 judgments) reported per criterion and per writing category. Reported validity: Fluency 93%, Grammaticality 87%, Coherence 81%, Consistency 82%, Relevance 77%. Validity by writing type (examples): Process Essay 100.00%, Descriptive Essay 95.45%, Scientific Report 92.31%, Self Introduction Essay 50.00%, Argumentative Essay 36.36%, Diary 20.00%. Also reported average LLM scores by age group (Age 11-14 vs 17-19) to show discrimination.",
            "losses_identified": "Subjective nuance and appropriateness for informal tones are degraded: LLM-as-a-Judge struggles on subjective or informal writing (e.g., diaries, self-introductions, argumentative essays), often flagging coherence/consistency issues when informality is acceptable; variability across writing types means LLM judgments are less reliable for subjective tasks. Additionally, there are cases of apparent optimism in consistency/relevance scores (higher LLM scores) and inconsistencies in how informality is interpreted, indicating loss of human-like sensitivity to context and genre conventions.",
            "examples_of_loss": "Low human-validated agreement for subjective categories: 'Diary' judgments valid only 20.00%, 'Argumentative Essay' 36.36%, 'Self Introduction Essay' 50.00%, indicating many judgments were deemed unreasonable by the student authors. The paper notes coherence and consistency criteria flagged informality in subjective tasks as problematic—i.e., LLM treated acceptable informal tone as incoherent or inconsistent. Relevance had the lowest validity among some categories and also the highest proportion of 'overly optimistic' judgments (Relevance: 19% overly optimistic; Consistency: 14% overly optimistic).",
            "counterexamples_or_caveats": "LLM-as-a-Judge performed well on more objective criteria and text types: Fluency (93% validity) and Grammaticality (87% validity) were largely validated by students, and writing types with objective characteristics (Process Essay 100%, Descriptive Essay 95.45%, Scientific Report 92.31%) had high validity ratios. GPT-4 also discriminated between age groups (older students received higher average scores across criteria), suggesting robustness in detecting broad skill differences. The authors note the study is preliminary and not a controlled same-instruction comparison, which limits some conclusions.",
            "paper_reference": "Experimental Results & Discussions; Table 4 (criterion-level validity and scores), Table 5 (validity by writing category), Table 6 (scores by age group); also Methods: Evaluation Pipeline and Verification of Evaluation Results.",
            "uuid": "e9695.0",
            "source_info": {
                "paper_title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prometheus: Inducing fine-grained evaluation capability in language models",
            "rating": 2,
            "sanitized_title": "prometheus_inducing_finegrained_evaluation_capability_in_language_models"
        },
        {
            "paper_title": "Gpteval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "gpteval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Alpacaeval: An automatic evaluator of instructionfollowing models",
            "rating": 2,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "rating": 2,
            "sanitized_title": "chateval_towards_better_llmbased_evaluators_through_multiagent_debate"
        },
        {
            "paper_title": "Replacing judges with juries: Evaluating llm generations with a panel of diverse models",
            "rating": 2,
            "sanitized_title": "replacing_judges_with_juries_evaluating_llm_generations_with_a_panel_of_diverse_models"
        },
        {
            "paper_title": "The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models",
            "rating": 1,
            "sanitized_title": "the_biggen_bench_a_principled_benchmark_for_finegrained_evaluation_of_language_models_with_language_models"
        }
    ],
    "cost": 0.006324,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education
24 Jul 2024</p>
<p>Seungyoon Kim seungyoon2007@gmail.com 
Carnegie Mellon University</p>
<p>Seungone Kim seungone@cmu.edu 
Carnegie Mellon University</p>
<p>Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education
24 Jul 2024D895F033C5C7E22D6E1EAE0AF2FFD8E3arXiv:2407.17022v1[cs.CL]
Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text.Extending this methodology to assess human-written text could significantly benefit educational settings by providing direct feedback to enhance writing skills, although this application is not straightforward.In this paper, we investigate whether LLMs can effectively assess human-written text for educational purposes.We collected 100 texts from 32 Korean students across 15 types of writing and employed GPT-4-Turbo to evaluate them using grammaticality, fluency, coherence, consistency, and relevance as criteria.Our analyses indicate that LLM evaluators can reliably assess grammaticality and fluency, as well as more objective types of writing, though they struggle with other criteria and types of writing.We publicly release our dataset and feedback 1 .</p>
<p>Introduction</p>
<p>Recent works have demonstrated that when adeptly prompted or trained, large language models (LLMs) can closely mimic human evaluation in assessing machine-generated text [14,16,24].This has led to the development of numerous generative evaluation pipelines that use language models as judges (denoted as 'LLM-as-a-Judge') [3,9,20].Applying this methodology to human-written text could greatly benefit educational settings by providing direct feedback to enhance writing skills [5,21].However, human-written text often possesses different characteristics from machinegenerated text, particularly in the unexpected use of words [6].This discrepancy complicates the direct application of the LLM-as-a-Judge pipeline for evaluating human-written text.In this work, we explore whether the LLM-as-a-Judge pipeline can also be used to evaluate human-written text.Specifically, we are interested in determining if LLMs can effectively assess texts written by Korean students to improve their writing skills across various formats, such as essays, reports, and scripts.</p>
<p>For this study, we collected a corpus of 100 texts written by 32 Korean students, ranging in age from 11 to 19, covering 15 types of writing.We evaluated these texts using GPT-4-Turbo based on five criteria: 'grammaticality,' 'fluency,' 'coherence,' 'consistency,' and 'relevance,' resulting in a total of 500 judgments across the 100 texts.Subsequently, we redistributed the verbal feedback generated by GPT-4-Turbo to the students, asking them to assess whether the judgments were reasonable, overly critical, or overly optimistic.The results indicated that while judgments on 'grammaticality' and 'fluency' were mostly deemed reasonable (87% and 93%, respectively), those concerning the other three criteria were relatively lower, suggesting that LLM-as-a-Judge pipeline has limitations.Notably, upon examining individual types of writing, participants found the judgments to be par-ticularly unreasonable when assessing more subjective texts, such as diaries and self-introduction essays.</p>
<p>Although it was not a rigorous comparison of writings on the same topic in a controlled environment, our analysis of the evaluation results allowed us to make several indirect observations that LLM-asa-Judge could be utilized by students to produce higher quality writing.First, we observed that while it tends to give high scores for Consistency and Relevance, it gives lower scores for Fluency.From this, we were able to indirectly observe through the participants' assessments that found the results reasonable, that LLM-as-a-Judge could be used in educational settings to help students write more fluently.Secondly, students often wrote descriptive essays and book reports, and the evaluation results for these writing types were found to be reasonably high.Most scores across the five criteria ranged from the high 3s to the low 4s, suggesting that LLM-as-a-Judge could be useful in elevating writings from a 3.5 -4 out of 5 to a full 5. Lastly, when comparing the average scores between texts written by 11-13-year-olds and those by 17-19-year-olds, we found that the latter tended to receive higher scores.This ability to discriminate between the two age groups not only confirmed the robustness of LLM-as-a-Judge but also suggested that it could be actively used to help younger students elevate the quality of their writing to that of older students.</p>
<p>Related Works</p>
<p>Due to the ambiguity in grading free-form responses generated by LLMs, word-match or semanticsimilarity based evaluation metrics have long been the de-facto standards [15,18,23].Recently, directly employing language models as evaluators (LLM-as-a-Judge) has proven more effective, achieving a higher correlation with human grading of machine-generated text [2,3,9,14,16,20,24].Moreover, follow-up studies have demonstrated that employing detailed evaluation criteria not only yields more accurate judgments but also enhances interpretability [9,10,11,12,13,22].Inspired by this, our work employs five specific evaluation axes: grammaticality, fluency, coherence, consistency, and relevance.On the application side, prior works have used LLM-as-a-Judge to evaluate various types of LLM-generated text, including the soundness of research proposals [19], toxicity in life advice [8], and the factuality of medical documents [7].Our work distinguishes itself from these examples as we evaluate human-written text across 15 distinct types of writing from students.</p>
<p>In the field of adopting LLMs to enhance education, prior works have primarily focused on distinguishing between LLM-generated and human-generated text [1,4,17].While preventing the overuse of LLMs is crucial, it is equally important to explore better educational models for using LLMs wisely.Our work suggests that LLMs can be effectively utilized as tools to enhance students' writing by providing verbal feedback on their writing.</p>
<p>Experimental Setting</p>
<p>In this section, we describe (1) the process by which we gathered and preprocessed 100 humanwritten texts, (2) our assessment pipeline for acquiring judgments from LLM evaluators, and (3) how we asked each student to determine whether the judgments from LLM evaluators were reasonable.We gathered 32 participants, all of whom are Korean students.Each participant was asked to provide a free-form text that they had manually written without using LLMs, along with instructions for writing that text.This includes writings from the past, for which we labeled the age at which the text was written.We excluded texts that were part of classification tasks or were short answers.The main authors manually typed and revised the texts based on the instructions, as these documents are typically long and in PDF format.To maintain the quality of the text, we did not revise the content or grammar.Instead, we manually corrected line break issues using an online JSON formatting tool2 to ensure compatibility of the dataset.Each instance consists of six components: student id (ranging between 0 and 31 to distinguish the writer of each text), age (ranging between 11 and 19, based on the age when the writer composed the text, not their current age), language (either Korean or English), type of writing (among the 15 categories listed above), input (the instruction to write the corresponding text), and output (the text that each student wrote).The statistics of the age, type of writing, and language are shown in Table 1, Table 3, and Table 2.We mainly follow the evaluation pipeline of Prometheus [9,11].Specifically, we utilize the prometheus-eval library3 , following the default hyper-parameters, and add five custom score rubrics to evaluate texts written by students.The code and prompts are provided in supplementary material.We use GPT-4-Turbo-April as the evaluator for conducting 500 judgments.The judgments consist of verbal feedback, which highlights the strengths and weaknesses of the student's text with respect to each evaluation criterion, and a scoring decision, which is an integer between 1 and 5.The criteria are as follows: the fluency criterion assesses if the text fluent and easy to read, considering it is written by a Korean student.The coherence criterion evaluates whether the text is coherent and logically organized, considering it is written by a Korean student.The consistency criterion examines if the text is consistent in terms of style, tone, and tense.The relevance criterion checks if the text is relevant to the given instruction or topic.Lastly, the grammaticality criterion assesses whether the text demonstrates proper grammatical usage.</p>
<p>Dataset Construction</p>
<p>Evaluation Pipeline</p>
<p>Verification of Evaluation Results</p>
<p>Table 3: Statistics of age distribution of the 100 student-written texts.</p>
<p>Written Language Number of Instances</p>
<p>Korean 63 English 37</p>
<p>To verify if the evaluation results from Subsection 3.2 are valid, we distribute the verbal feedback and scoring decisions back to the 32 student participants who provided their writing.We ask if the scoring decisions and the verbal feedback were valid or not (i.e., the scoring as well as the verbal feedback is overly critical or overly optimistic).Each participant were paid $ 10 for providing each writing piece and verifying the judgments (15 minutes per instance).Although we were unable to confirm whether students' writing improved through direct revisions due to cost limitations, the goal was to indirectly verify if they could improve their writing by reviewing whether the feedback was valid.Future work could involve experimenting with students directly revising their writing.</p>
<p>Experimental Results &amp; Discussions</p>
<p>Can LLM-as-a-Judge provide reasonable judgments?The GPT-4 evaluation results and posthoc human meta-evaluation results are shown in Table 4 (on the next page).First, when examining GPT-4's scoring decisions for the same fixed 100 texts, consistency and relevance received higher scores, while fluency received lower scores.Second, in the meta-evaluation results conducted by Table 4: GPT-4 evaluation results of 100 student-written texts across 5 evaluation criteria, and posthoc human meta-evaluation results on GPT-4's judgment.Across all criteria, humans verified that on 77% -93% of the time, the judgments were reasonable, which supports the claim that LLM-as-a-Judge could be utilized to pinpoint the strength and weakness of each student's writing.the students, fluency had the highest agreement on the validity of the judgment.Given that fluency achieved the lowest score and humans determined it to be the most valid (93%), this supports the claim that the LLM-as-a-Judge pipeline could be utilized to enhance students' writing.This finding is straightforward, considering that the participants we tested are all Korean students who do not use English as their first language.Also, for more objective evaluation criteria such as grammaticality, the validity was high (87%).Is there a type of writing where LLM-as-a-Judge does not function properly?Considering that our text collection consists of a wide variety of writings, we further analyzed the patterns of which writings were deemed to consist of more accurate judgments.Table 5 shows the top 1, 2, and 3 and lowest 1, 2, and 3 validity ratios for the writing categories (excluding categories with only one text).The validity ratio tends to be higher for texts with more objective characteristics, including process essays, descriptive essays, and scientific reports, while it is lower for self-introduction essays, argumentative essays, and diaries.Notably, we observed that for subjective tasks, texts are often written in a more informal tone.However, coherence and consistency criteria, which should not consider informality, pointed this out as a limitation, indicating that LLM-as-a-Judge does not function as intended in these cases.We leave the development of better evaluation frameworks to assess human-written text to assist their writing for future work.Is GPT-4 capable of distinguishing between writings from senior and junior students?The average scores provided by GPT-4-Turbo evaluator is shown in Table 6.Although it is not strictly accurate since the participants' writings were not compared based on the same instructions, we generally observed a difference in scores between the two groups across all criteria.This indicates that the LLM-as-a-Judge pipeline tends to give higher scores to senior students.This finding indirectly supports the reliability of LLM-as-a-Judge, based on the premise that older students are capable of writing more flexibly.</p>
<p>Criteria</p>
<p>LLM-AS-A-JUDGE EVALUATION RESULTS HUMAN META-EVALUATION RESULTS</p>
<p>Score</p>
<p>Conclusion</p>
<p>Recently, LLM-as-a-Judge has emerged as an effective paradigm for evaluating LLM responses to subjective questions.In this work, we extend this paradigm to investigate whether it can also effectively evaluate human-written texts through preliminary experiments.We collected and evaluated 100 essays written by a total of 32 Korean students of various ages, and then had each student verify whether the evaluation results were reasonable.The experimental results confirmed that LLM-as-a-Judge could serve as a good evaluator for more objective types of writing, as well as for fluency and grammaticality criteria.Additionally, we identified areas that need further improvement.We hope that our work will serve as a foundation for future research exploring the use of LLM-as-a-Judge to evaluate human-written texts more broadly.</p>
<p>Table 1 :
1
Statistics of age distribution of the 100 student-written texts.
AgeNumber of Instances1116131814417331826193</p>
<p>Table 2 :
2
Statistics of age distribution of the 100 student-written texts.
Type of WritingNumber of InstancesBook Report17Descriptive Essay22Process Essay4Reflective Essay7Story Writing1Play Script3Linguistic Report1Scientific Report13Presentation Script9Problem Creation2Argumentative Essay11Presentation Report2Diary5Self Introduction Essay2Letter1</p>
<p>1 Score 2 Score 3 Score 4 Score 5 Avg Score Valid Overly Critical Overly Optimistic
Grammaticality062842113.6387%8%5%Fluency03722323.2493%6%1%Coherence143439223.7781%13%6%Consistency112053254.0082%4%14%Relevance791330413.8977%4%19%</p>
<p>Table 5 :
5
Human meta-evaluation results on GPT-4's judgment for each writing category.Validity ratio is higher on more objective texts while lower on subjective texts.
Type of WritingRatio of ValidityProcess Essay100.00%Descriptive Essay95.45%Scientific Report92.31%Self Introduction Essay50.00%Argumentative Essay36.36%Diary20.00%</p>
<p>Table 6 :
6
GPT-4 evaluation results for each age group across 5 criteria.Higher scores are given to senior students over junior students.
CriteriaAge 11-14Age 17-19Grammaticality3.343.95Fluency3.083.25Coherence3.243.95Consistency3.534.23Relevance3.543.95
https://github.com/seungyoon1/llm-as-a-judge-human-eval/tree/main Preprint. Under review.
https://jsoneditoronline.org/
https://github.com/prometheus-eval/prometheus-eval
AcknowledgementThe first author, currently a highschool student, have proposed the idea for this paper, conducted all the experiments, and wrote the paper.The corresponding author have provided feedback throughout the project by having a weekly 2 hour meeting.
Students' use of large language models in engineering education: A case study on technology acceptance, perceptions, efficacy, and detection chances. Margherita Bernabei, Silvia Colabianchi, Andrea Falegnami, Francesco Costantino, Computers and Education: Artificial Intelligence. 51001722023</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-controlled alpacaeval: A simple way to debias automatic evaluators. 2024arXiv preprint</p>
<p>Llm-as-acoauthor: The challenges of detecting llm-human mixcase. Chujie Gao, Dongping Chen, Qihui Zhang, Yue Huang, Yao Wan, Lichao Sun, arXiv:2401.059522024arXiv preprint</p>
<p>The challenges of feedback in higher education. Michael Henderson, Tracii Ryan, Michael Phillips, Assessment &amp; Evaluation in Higher Education. 2019</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751The curious case of neural text degeneration. 2019arXiv preprint</p>
<p>Factpico: Factuality evaluation for plain language summarization of medical evidence. Antony Sebastian, Lily Joseph, Jan Chen, Trienes, Louisa Hannah, Monika Göke, Wei Coers, Byron C Xu, Junyi Jessy Wallace, Li, arXiv:2402.114562024arXiv preprint</p>
<p>Minbeom Kim, Jahyun Koo, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung, arXiv:2311.09585Lifetox: Unveiling implicit toxicity in life advice. 2023arXiv preprint</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, arXiv:2310.084912023arXiv preprint</p>
<p>The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, arXiv:2406.057612024arXiv preprint</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024arXiv preprint</p>
<p>Tae Soo, Kim , Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim, arXiv:2309.13633Evallm: Interactive evaluation of large language model prompts on user-defined criteria. 2023arXiv preprint</p>
<p>Prometheusvision: Vision-language model as a judge for fine-grained evaluation. Seongyun Lee, Seungone Kim, Sue , Hyun Park, Geewook Kim, Minjoon Seo, arXiv:2401.065912024arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instructionfollowing models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Gpteval, arXiv:2303.16634Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Detecting llm-generated text in computing education: A comparative study for chatgpt cases. Oscar Michael Sheinman Orenstrakh, Carlos Karnalim, Anibal Suarez, Michael Liut, arXiv:2307.074112023arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, Monica S Lam, Proceedings of the 2024 Conference of the North American Chapter. Long and Short Papers. the 2024 Conference of the North American ChapterHuman Language Technologies12024</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>The power of feedback revisited: A meta-analysis of educational feedback research. Benedikt Wisniewski, Klaus Zierer, John Hattie, Frontiers in psychology. 104876622020</p>
<p>Flask: Fine-grained language model evaluation based on alignment skill sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, arXiv:2307.109282023arXiv preprint</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Bertscore, arXiv:1904.09675Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Judging llm-as-a-judge with mtbench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-714 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-714</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-714</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-269921245</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.11250v3.pdf" target="_blank">Argumentative Causal Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Causal discovery amounts to unearthing causal relationships amongst features in data. It is a crucial companion to causal inference, necessary to build scientific knowledge without resorting to expensive or impossible randomised control trials. In this paper, we explore how reasoning with symbolic representations can support causal discovery. Specifically, we deploy assumption-based argumentation (ABA), a well-established and powerful knowledge representation formalism, in combination with causality theories, to learn graphs which reflect causal dependencies in the data. We prove that our method exhibits desirable properties, notably that, under natural conditions, it can retrieve ground-truth causal graphs. We also conduct experiments with an implementation of our method in answer set programming (ASP) on four datasets from standard benchmarks in causal discovery, showing that our method compares well against established baselines.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e714.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e714.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABA-PC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Argumentative Causal Discovery with Assumption-Based Argumentation (ABA-PC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's instantiated algorithm that combines an assumption-based argumentation (ABA) formalisation of causal graphs with independence tests (sourced from Majority-PC) encoded in Answer Set Programming (ASP); it resolves conflicting/spurious conditional independence test results by argumentative attack/defence and by weighting and excluding low-confidence tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ABA-PC (Argumentative Causal Discovery instantiated with Majority-PC facts)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructs an ABA framework whose assumptions represent arrows, independencies, and blocked-paths; encodes d-separation and path-activation (collider-trees) as rules; imports (in)dependence facts (p-values) as ASP facts/weak constraints; computes stable extensions (each corresponding to a DAG) and resolves inconsistent/conflicting test results via ABA attack relations. When the imported facts produce no stable extension, ABA-PC ranks tests by a strength score S(p,α,s,d) (derived from a calibrated transform γ(p,α) and penalised by conditioning set size) and iteratively excludes the weakest tests until at least one stable extension exists; weak constraints in ASP are used to optimise selection of facts and rank output graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline observational benchmark datasets (bnlearn: Asia, Cancer, Earthquake, Survey)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Standard simulated / benchmark Bayesian-network datasets sampled to produce observational data (no interactive interventions or virtual-lab-style active experimentation); evaluations are repeated sampling experiments (5000 samples, multiple seeds) rather than interactive environment runs.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Argumentative conflict resolution combined with weighted selection/exclusion of statistical tests: (i) represent conflicting (in)dependence results as attacking assumptions in an ABAF; (ii) score each test via S(p,α,s,d) = (1 - s/(d-2)) * γ(p,α) where γ maps p-values to a 0–1 strength re-centered at α; (iii) down-rank/exclude low-strength tests and use ASP weak constraints to prefer extensions that respect high-strength facts.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Erroneous conditional independence/dependence test results (statistical false positives/negatives), spurious correlations arising from finite-sample errors; implicitly addresses irrelevant variables appearing in conditioning sets (via conditioning-set-size penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Conflict detection via ABA: if the set of imported (in)dependence facts produces no stable extension (or causes internal attacks), this signals inconsistent or spurious test results; additionally, low S scores flag weak tests near the significance threshold and tests with large conditioning sets.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Transforms p-values using γ(p,α) (a non-linear mapping with minimum around p=α) and multiplies by penalty factor depending on conditioning set size and graph size to produce S(p,α,s,d); uses these S weights as ASP weak constraints and to rank-and-exclude low-strength tests iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Argumentative refutation: the ABA attack graph (assumptions attacking contraries) is used to reject assumptions corresponding to spurious (in)dependence statements; blocked-path and ap/bp rules ensure dependence facts derive ap assumptions that attack bp (blocked-path) assumptions, preventing acceptance of incorrect independencies; stable semantics selects extensions (graphs) that defend themselves against attackers, effectively refuting inconsistent tests.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Across four bnlearn benchmarks (Asia, Cancer, Earthquake, Survey), ABA-PC produced significantly better worst-case Normalised Structural Interventional Distance (NSID) than baselines and improved SID/SHD/F1 in several datasets: ABA-PC ranked first on worst-case SID for all datasets and was significantly better than Majority-PC (MPC) on all datasets; it outperformed baselines on 3/4 datasets (Cancer, Earthquake, Asia) and matched NOTEARS-MLP on Survey. (Paper reports statistical significance tests across 50 repetitions.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline (Majority-PC) using the same set of independence relations returned worse CPDAGs (higher SID/NSID and SHD); the paper presents concrete examples where MPC's unfiltered tests yield no graph faithful to reported tests (Example 1.1). When ABA-PC simply fixed all tests without weighting/excluding, it could produce no stable extension; iterative exclusion of low-strength tests was required to get valid models.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing causal discovery constraints and statistical tests in an ABA framework allows principled conflict resolution: (1) Weighted p-value transformation and penalisation by conditioning-set size effectively identify weak/spurious tests; (2) argumentation (attacks/defences) plus blocked-path machinery correctly enforces d-separation constraints and refutes inconsistent (in)dependence claims; (3) this yields more accurate causal graphs (lower SID) than standard MPC given the same test outputs, at the cost of much higher computational expense and current scalability limited to ≈10 variables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Argumentative Causal Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e714.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e714.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority-PC (Order-independent PC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint-based causal discovery algorithm that performs conditional independence tests and applies graphical orientation rules; used here as the source of (in)dependence facts (p-values) for ABA-PC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Order-independent constraint-based causal structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Majority-PC (MPC)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Runs conditional independence tests (e.g., Fisher Z) to remove edges from a complete graph by increasing conditioning set size, then applies orientation rules to produce a CPDAG; Majority-PC is an order-independent variant that aims to be robust to variable ordering. In this paper MPC is used to produce the initial set of (in)dependence test results and associated p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline observational benchmark datasets (bnlearn: Asia, Cancer, Earthquake, Survey)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same synthetic/benchmark observational data as used for ABA-PC; MPC is run once per sampled dataset to produce a collection of conditional independence tests and p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Statistical errors from finite samples (false independence/dependence due to limited data or conditioning set mis-specification) — MPC itself does not have an internal mechanism to downweight or refute such spurious tests.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Reported as a baseline: MPC produced higher (worse) SID/NSID and SHD than ABA-PC in experiments; in illustrative examples MPC's tests can be internally inconsistent (no graph can satisfy all reported tests).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MPC is a practical constraint-based source of (in)dependence tests but its raw outputs can be inconsistent due to finite-sample errors; these inconsistencies motivate the ABA-based conflict-resolution approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Argumentative Causal Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e714.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e714.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bromberg2009</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preference-based argumentation for test selection (Bromberg & Margaritis 2009)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that uses preference-based/deductive argumentation to select tests within the PC algorithm to improve reliability on small data; referenced as related work that addresses conflicting tests using argumentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving the reliability of causal discovery from small data sets using argumentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Preference-based argumentation for PC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Instantiates a preference-based argumentation framework (Amgoud & Cayrol style) with deductive arguments to choose which independence tests to trust/use within the PC algorithm; aims to resolve conflicts from unreliable tests by preferring some tests over others according to a preference ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General observational datasets (small-sample settings)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not an interactive environment; method is applied to small-data causal discovery settings to mitigate unreliable conditional independence tests.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Test selection via argumentation preferences: uses argumentation to prefer certain tests and thus exclude less reliable ones from PC.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Finite-sample test errors (spurious independencies/dependencies) arising in small datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not explicitly formalised as in ABA-PC here; relies on preference ordering and deductive argumentation to detect which tests should be discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit via preferences (not a numeric weighting scheme); chooses tests to keep rather than numeric downweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Argumentation defeats less preferred/dubious tests, effectively refuting their conclusions when attacked by preferred arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Presented as an early example of using argumentation to mitigate unreliable statistical tests in causal discovery; authors of current paper note it relies on graphoid axioms (incomplete) and therefore may miss some inconsistencies—motivating their ABA approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Argumentative Causal Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e714.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e714.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyttinen2014</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASP-based conflict resolution / encoding of graphical interventions (Hyttinen, Eberhardt, Järvisalo 2014)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Previous work using Answer Set Programming (ASP) to encode causal discovery problems including encoding of interventions and conflict resolution among constraints; cited as related work applying declarative solving to causal discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constraint-based causal discovery: conflict resolution with answer set programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ASP encoding for causal discovery and interventions</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encodes causal discovery constraints (e.g., conditional independencies, intervention results) as ASP problems to search for graphs consistent with those constraints; used to compute causal graphs under constraints and to perform conflict resolution via solver-based search.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Observational and interventional causal discovery settings (declarative solver environment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Declarative/solver-based setting rather than an interactive virtual lab; can encode interventions as constraints and search for graphs consistent with both observational and interventional data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Conflict-resolution via constraint programming / ASP search: inconsistent facts cause unsat solutions and the ASP encoding can be adapted to find consistent subsets or encode preferences/weights.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Inconsistent or conflicting (in)dependence and intervention constraints (e.g., due to measurement or test errors); latent confounders addressed in other related works but not necessarily in this encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Solver detects inconsistency (no solution) when constraints conflict; strategies can be used to identify minimal inconsistent subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not specified here; generally can be implemented via optimisation/weak constraints in ASP but Hyttinen et al. focused on encoding rather than the weighting heuristics used in ABA-PC.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Resolution by search for consistent assignments or by modifying/relaxing constraints; not an argumentative refutation per se.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates use of ASP for encoding causal discovery and interventions; motivates current paper's use of ASP to implement their non-flat ABAF under stable semantics and to exploit optimisation (weak constraints) for fact selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Argumentative Causal Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e714.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e714.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rantanen2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exact branch-and-bound approach for graphs with cycles and latent confounders (Rantanen, Hyttinen, Järvisalo 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint/branch-and-bound method for discovering causal graphs that may contain cycles and latent confounders via exact search; cited as related solver-based approach handling complex settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Exact branch-and-bound causal discovery (with cycles & latents)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates causal discovery under complex assumptions (cycles, latent confounders) as a combinatorial optimisation/search problem and solves it exactly via branch-and-bound and constraint reasoning, returning graphs consistent with constraints and scoring criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline observational datasets where cycles/latent confounders may be present</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Exact solver/search environment for causal graph discovery, not an interactive virtual lab.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Exact search can identify and avoid models that explain data via spurious edges by optimising a score or satisfying constraints; can explicitly model latent confounders to prevent spurious direct edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounding, cycles, and other structural violations that can produce spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Search-based detection: models that better explain data while satisfying constraints are preferred; detection of spuriousness via inability of a model lacking latents/cycles to satisfy constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation via exact search: competing candidate graphs that explain the data without spurious causal claims are found; explicit modelling of latents allows refuting direct-edge explanations that would be spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows exact solver approaches can handle settings (cycles, latent confounders) that lead to spurious correlations if ignored; cited by the paper as an avenue for extending their ABA approach to latents and cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Argumentative Causal Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e714.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e714.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>p-value→strength transforms (Jabbari/Claassen/Triantafillou refs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformations of p-values to probabilistic strengths for causal constraint weighting (Jabbari et al., Claassen & Heskes, Triantafillou et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced literature proposing ways to convert p-values from conditional independence tests into probabilistic/confidence estimates or scores that can be used to weight or score constraints in causal discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>p-value to probability/strength transformations</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Various approaches translate test p-values into probability estimates or scores (Bayesian or heuristic transforms) that better reflect uncertainty than raw p-values; such transforms can be used to weight constraints in causal discovery algorithms or to perform probabilistic reasoning about tests.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General observational causal discovery (small-sample / noisy tests)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Approaches are generic; can be applied offline to test outputs before being used by downstream causal discovery solvers (constraint-based, score-based, or argumentative).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Statistical calibration / Bayesian conversion of p-values into probabilities to downweight suspect tests and incorporate uncertainty formally into constraint selection or scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Finite-sample uncertainty and misclassification of independence/dependence due to p-value distortions; helps mitigate false rejections/acceptances of H0.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical calibration (Sellke–Bayarri–Berger type arguments), Bayesian scoring of independence constraints, or confidence estimation to flag uncertain tests.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Convert p to a calibrated probability or weight; use that as a continuous strength (instead of raw p) to rank/weight constraints—paper mentions these as alternatives to their heuristic γ but did not implement them.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By lowering the weight/confidence of weak tests, these transforms make it easier to exclude or ignore spurious test outcomes during model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper acknowledges that transforming p-values into probability estimates (as in these referenced works) is a principled alternative to the heuristic γ-based scoring used in ABA-PC and could address asymmetry of classical tests; however, the authors left integration of those transforms as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Argumentative Causal Discovery', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving the reliability of causal discovery from small data sets using argumentation <em>(Rating: 2)</em></li>
                <li>Constraint-based causal discovery: conflict resolution with answer set programming <em>(Rating: 2)</em></li>
                <li>Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach <em>(Rating: 2)</em></li>
                <li>A bayesian approach to constraint based causal inference <em>(Rating: 1)</em></li>
                <li>Discovery of causal models that contain latent variables through bayesian scoring of independence constraints <em>(Rating: 1)</em></li>
                <li>Learning neighborhoods of high confidence in constraint-based causal discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-714",
    "paper_id": "paper-269921245",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "ABA-PC",
            "name_full": "Argumentative Causal Discovery with Assumption-Based Argumentation (ABA-PC)",
            "brief_description": "The paper's instantiated algorithm that combines an assumption-based argumentation (ABA) formalisation of causal graphs with independence tests (sourced from Majority-PC) encoded in Answer Set Programming (ASP); it resolves conflicting/spurious conditional independence test results by argumentative attack/defence and by weighting and excluding low-confidence tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "ABA-PC (Argumentative Causal Discovery instantiated with Majority-PC facts)",
            "method_description": "Constructs an ABA framework whose assumptions represent arrows, independencies, and blocked-paths; encodes d-separation and path-activation (collider-trees) as rules; imports (in)dependence facts (p-values) as ASP facts/weak constraints; computes stable extensions (each corresponding to a DAG) and resolves inconsistent/conflicting test results via ABA attack relations. When the imported facts produce no stable extension, ABA-PC ranks tests by a strength score S(p,α,s,d) (derived from a calibrated transform γ(p,α) and penalised by conditioning set size) and iteratively excludes the weakest tests until at least one stable extension exists; weak constraints in ASP are used to optimise selection of facts and rank output graphs.",
            "environment_name": "Offline observational benchmark datasets (bnlearn: Asia, Cancer, Earthquake, Survey)",
            "environment_description": "Standard simulated / benchmark Bayesian-network datasets sampled to produce observational data (no interactive interventions or virtual-lab-style active experimentation); evaluations are repeated sampling experiments (5000 samples, multiple seeds) rather than interactive environment runs.",
            "handles_distractors": true,
            "distractor_handling_technique": "Argumentative conflict resolution combined with weighted selection/exclusion of statistical tests: (i) represent conflicting (in)dependence results as attacking assumptions in an ABAF; (ii) score each test via S(p,α,s,d) = (1 - s/(d-2)) * γ(p,α) where γ maps p-values to a 0–1 strength re-centered at α; (iii) down-rank/exclude low-strength tests and use ASP weak constraints to prefer extensions that respect high-strength facts.",
            "spurious_signal_types": "Erroneous conditional independence/dependence test results (statistical false positives/negatives), spurious correlations arising from finite-sample errors; implicitly addresses irrelevant variables appearing in conditioning sets (via conditioning-set-size penalty).",
            "detection_method": "Conflict detection via ABA: if the set of imported (in)dependence facts produces no stable extension (or causes internal attacks), this signals inconsistent or spurious test results; additionally, low S scores flag weak tests near the significance threshold and tests with large conditioning sets.",
            "downweighting_method": "Transforms p-values using γ(p,α) (a non-linear mapping with minimum around p=α) and multiplies by penalty factor depending on conditioning set size and graph size to produce S(p,α,s,d); uses these S weights as ASP weak constraints and to rank-and-exclude low-strength tests iteratively.",
            "refutation_method": "Argumentative refutation: the ABA attack graph (assumptions attacking contraries) is used to reject assumptions corresponding to spurious (in)dependence statements; blocked-path and ap/bp rules ensure dependence facts derive ap assumptions that attack bp (blocked-path) assumptions, preventing acceptance of incorrect independencies; stable semantics selects extensions (graphs) that defend themselves against attackers, effectively refuting inconsistent tests.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Across four bnlearn benchmarks (Asia, Cancer, Earthquake, Survey), ABA-PC produced significantly better worst-case Normalised Structural Interventional Distance (NSID) than baselines and improved SID/SHD/F1 in several datasets: ABA-PC ranked first on worst-case SID for all datasets and was significantly better than Majority-PC (MPC) on all datasets; it outperformed baselines on 3/4 datasets (Cancer, Earthquake, Asia) and matched NOTEARS-MLP on Survey. (Paper reports statistical significance tests across 50 repetitions.)",
            "performance_without_robustness": "Baseline (Majority-PC) using the same set of independence relations returned worse CPDAGs (higher SID/NSID and SHD); the paper presents concrete examples where MPC's unfiltered tests yield no graph faithful to reported tests (Example 1.1). When ABA-PC simply fixed all tests without weighting/excluding, it could produce no stable extension; iterative exclusion of low-strength tests was required to get valid models.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Representing causal discovery constraints and statistical tests in an ABA framework allows principled conflict resolution: (1) Weighted p-value transformation and penalisation by conditioning-set size effectively identify weak/spurious tests; (2) argumentation (attacks/defences) plus blocked-path machinery correctly enforces d-separation constraints and refutes inconsistent (in)dependence claims; (3) this yields more accurate causal graphs (lower SID) than standard MPC given the same test outputs, at the cost of much higher computational expense and current scalability limited to ≈10 variables.",
            "uuid": "e714.0",
            "source_info": {
                "paper_title": "Argumentative Causal Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MPC",
            "name_full": "Majority-PC (Order-independent PC)",
            "brief_description": "A constraint-based causal discovery algorithm that performs conditional independence tests and applies graphical orientation rules; used here as the source of (in)dependence facts (p-values) for ABA-PC.",
            "citation_title": "Order-independent constraint-based causal structure learning",
            "mention_or_use": "use",
            "method_name": "Majority-PC (MPC)",
            "method_description": "Runs conditional independence tests (e.g., Fisher Z) to remove edges from a complete graph by increasing conditioning set size, then applies orientation rules to produce a CPDAG; Majority-PC is an order-independent variant that aims to be robust to variable ordering. In this paper MPC is used to produce the initial set of (in)dependence test results and associated p-values.",
            "environment_name": "Offline observational benchmark datasets (bnlearn: Asia, Cancer, Earthquake, Survey)",
            "environment_description": "Same synthetic/benchmark observational data as used for ABA-PC; MPC is run once per sampled dataset to produce a collection of conditional independence tests and p-values.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Statistical errors from finite samples (false independence/dependence due to limited data or conditioning set mis-specification) — MPC itself does not have an internal mechanism to downweight or refute such spurious tests.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": "Reported as a baseline: MPC produced higher (worse) SID/NSID and SHD than ABA-PC in experiments; in illustrative examples MPC's tests can be internally inconsistent (no graph can satisfy all reported tests).",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "MPC is a practical constraint-based source of (in)dependence tests but its raw outputs can be inconsistent due to finite-sample errors; these inconsistencies motivate the ABA-based conflict-resolution approach.",
            "uuid": "e714.1",
            "source_info": {
                "paper_title": "Argumentative Causal Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Bromberg2009",
            "name_full": "Preference-based argumentation for test selection (Bromberg & Margaritis 2009)",
            "brief_description": "Prior work that uses preference-based/deductive argumentation to select tests within the PC algorithm to improve reliability on small data; referenced as related work that addresses conflicting tests using argumentation.",
            "citation_title": "Improving the reliability of causal discovery from small data sets using argumentation",
            "mention_or_use": "mention",
            "method_name": "Preference-based argumentation for PC",
            "method_description": "Instantiates a preference-based argumentation framework (Amgoud & Cayrol style) with deductive arguments to choose which independence tests to trust/use within the PC algorithm; aims to resolve conflicts from unreliable tests by preferring some tests over others according to a preference ordering.",
            "environment_name": "General observational datasets (small-sample settings)",
            "environment_description": "Not an interactive environment; method is applied to small-data causal discovery settings to mitigate unreliable conditional independence tests.",
            "handles_distractors": true,
            "distractor_handling_technique": "Test selection via argumentation preferences: uses argumentation to prefer certain tests and thus exclude less reliable ones from PC.",
            "spurious_signal_types": "Finite-sample test errors (spurious independencies/dependencies) arising in small datasets.",
            "detection_method": "Not explicitly formalised as in ABA-PC here; relies on preference ordering and deductive argumentation to detect which tests should be discarded.",
            "downweighting_method": "Implicit via preferences (not a numeric weighting scheme); chooses tests to keep rather than numeric downweighting.",
            "refutation_method": "Argumentation defeats less preferred/dubious tests, effectively refuting their conclusions when attacked by preferred arguments.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Presented as an early example of using argumentation to mitigate unreliable statistical tests in causal discovery; authors of current paper note it relies on graphoid axioms (incomplete) and therefore may miss some inconsistencies—motivating their ABA approach.",
            "uuid": "e714.2",
            "source_info": {
                "paper_title": "Argumentative Causal Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Hyttinen2014",
            "name_full": "ASP-based conflict resolution / encoding of graphical interventions (Hyttinen, Eberhardt, Järvisalo 2014)",
            "brief_description": "Previous work using Answer Set Programming (ASP) to encode causal discovery problems including encoding of interventions and conflict resolution among constraints; cited as related work applying declarative solving to causal discovery.",
            "citation_title": "Constraint-based causal discovery: conflict resolution with answer set programming",
            "mention_or_use": "mention",
            "method_name": "ASP encoding for causal discovery and interventions",
            "method_description": "Encodes causal discovery constraints (e.g., conditional independencies, intervention results) as ASP problems to search for graphs consistent with those constraints; used to compute causal graphs under constraints and to perform conflict resolution via solver-based search.",
            "environment_name": "Observational and interventional causal discovery settings (declarative solver environment)",
            "environment_description": "Declarative/solver-based setting rather than an interactive virtual lab; can encode interventions as constraints and search for graphs consistent with both observational and interventional data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Conflict-resolution via constraint programming / ASP search: inconsistent facts cause unsat solutions and the ASP encoding can be adapted to find consistent subsets or encode preferences/weights.",
            "spurious_signal_types": "Inconsistent or conflicting (in)dependence and intervention constraints (e.g., due to measurement or test errors); latent confounders addressed in other related works but not necessarily in this encoding.",
            "detection_method": "Solver detects inconsistency (no solution) when constraints conflict; strategies can be used to identify minimal inconsistent subsets.",
            "downweighting_method": "Not specified here; generally can be implemented via optimisation/weak constraints in ASP but Hyttinen et al. focused on encoding rather than the weighting heuristics used in ABA-PC.",
            "refutation_method": "Resolution by search for consistent assignments or by modifying/relaxing constraints; not an argumentative refutation per se.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Demonstrates use of ASP for encoding causal discovery and interventions; motivates current paper's use of ASP to implement their non-flat ABAF under stable semantics and to exploit optimisation (weak constraints) for fact selection.",
            "uuid": "e714.3",
            "source_info": {
                "paper_title": "Argumentative Causal Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Rantanen2020",
            "name_full": "Exact branch-and-bound approach for graphs with cycles and latent confounders (Rantanen, Hyttinen, Järvisalo 2020)",
            "brief_description": "A constraint/branch-and-bound method for discovering causal graphs that may contain cycles and latent confounders via exact search; cited as related solver-based approach handling complex settings.",
            "citation_title": "Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach",
            "mention_or_use": "mention",
            "method_name": "Exact branch-and-bound causal discovery (with cycles & latents)",
            "method_description": "Formulates causal discovery under complex assumptions (cycles, latent confounders) as a combinatorial optimisation/search problem and solves it exactly via branch-and-bound and constraint reasoning, returning graphs consistent with constraints and scoring criteria.",
            "environment_name": "Offline observational datasets where cycles/latent confounders may be present",
            "environment_description": "Exact solver/search environment for causal graph discovery, not an interactive virtual lab.",
            "handles_distractors": true,
            "distractor_handling_technique": "Exact search can identify and avoid models that explain data via spurious edges by optimising a score or satisfying constraints; can explicitly model latent confounders to prevent spurious direct edges.",
            "spurious_signal_types": "Latent confounding, cycles, and other structural violations that can produce spurious correlations.",
            "detection_method": "Search-based detection: models that better explain data while satisfying constraints are preferred; detection of spuriousness via inability of a model lacking latents/cycles to satisfy constraints.",
            "downweighting_method": null,
            "refutation_method": "Refutation via exact search: competing candidate graphs that explain the data without spurious causal claims are found; explicit modelling of latents allows refuting direct-edge explanations that would be spurious.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Shows exact solver approaches can handle settings (cycles, latent confounders) that lead to spurious correlations if ignored; cited by the paper as an avenue for extending their ABA approach to latents and cycles.",
            "uuid": "e714.4",
            "source_info": {
                "paper_title": "Argumentative Causal Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "p-value→strength transforms (Jabbari/Claassen/Triantafillou refs)",
            "name_full": "Transformations of p-values to probabilistic strengths for causal constraint weighting (Jabbari et al., Claassen & Heskes, Triantafillou et al.)",
            "brief_description": "Referenced literature proposing ways to convert p-values from conditional independence tests into probabilistic/confidence estimates or scores that can be used to weight or score constraints in causal discovery.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "p-value to probability/strength transformations",
            "method_description": "Various approaches translate test p-values into probability estimates or scores (Bayesian or heuristic transforms) that better reflect uncertainty than raw p-values; such transforms can be used to weight constraints in causal discovery algorithms or to perform probabilistic reasoning about tests.",
            "environment_name": "General observational causal discovery (small-sample / noisy tests)",
            "environment_description": "Approaches are generic; can be applied offline to test outputs before being used by downstream causal discovery solvers (constraint-based, score-based, or argumentative).",
            "handles_distractors": true,
            "distractor_handling_technique": "Statistical calibration / Bayesian conversion of p-values into probabilities to downweight suspect tests and incorporate uncertainty formally into constraint selection or scoring.",
            "spurious_signal_types": "Finite-sample uncertainty and misclassification of independence/dependence due to p-value distortions; helps mitigate false rejections/acceptances of H0.",
            "detection_method": "Statistical calibration (Sellke–Bayarri–Berger type arguments), Bayesian scoring of independence constraints, or confidence estimation to flag uncertain tests.",
            "downweighting_method": "Convert p to a calibrated probability or weight; use that as a continuous strength (instead of raw p) to rank/weight constraints—paper mentions these as alternatives to their heuristic γ but did not implement them.",
            "refutation_method": "By lowering the weight/confidence of weak tests, these transforms make it easier to exclude or ignore spurious test outcomes during model selection.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The paper acknowledges that transforming p-values into probability estimates (as in these referenced works) is a principled alternative to the heuristic γ-based scoring used in ABA-PC and could address asymmetry of classical tests; however, the authors left integration of those transforms as future work.",
            "uuid": "e714.5",
            "source_info": {
                "paper_title": "Argumentative Causal Discovery",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving the reliability of causal discovery from small data sets using argumentation",
            "rating": 2,
            "sanitized_title": "improving_the_reliability_of_causal_discovery_from_small_data_sets_using_argumentation"
        },
        {
            "paper_title": "Constraint-based causal discovery: conflict resolution with answer set programming",
            "rating": 2,
            "sanitized_title": "constraintbased_causal_discovery_conflict_resolution_with_answer_set_programming"
        },
        {
            "paper_title": "Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach",
            "rating": 2,
            "sanitized_title": "discovering_causal_graphs_with_cycles_and_latent_confounders_an_exact_branchandbound_approach"
        },
        {
            "paper_title": "A bayesian approach to constraint based causal inference",
            "rating": 1,
            "sanitized_title": "a_bayesian_approach_to_constraint_based_causal_inference"
        },
        {
            "paper_title": "Discovery of causal models that contain latent variables through bayesian scoring of independence constraints",
            "rating": 1,
            "sanitized_title": "discovery_of_causal_models_that_contain_latent_variables_through_bayesian_scoring_of_independence_constraints"
        },
        {
            "paper_title": "Learning neighborhoods of high confidence in constraint-based causal discovery",
            "rating": 1,
            "sanitized_title": "learning_neighborhoods_of_high_confidence_in_constraintbased_causal_discovery"
        }
    ],
    "cost": 0.01783925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Argumentative Causal Discovery
3 Aug 2024</p>
<p>Fabrizio Russo fabrizio@imperial.ac.uk 
Department of Computing
Imperial College London</p>
<p>Anna Rapberger a.rapberger@imperial.ac.uk 
Department of Computing
Imperial College London</p>
<p>Francesca Toni 
Department of Computing
Imperial College London</p>
<p>Argumentative Causal Discovery
3 Aug 20247A8B2218E091FC1FEAA72347BAB59B4AarXiv:2405.11250v3[cs.AI]
Causal discovery amounts to unearthing causal relationships amongst features in data.It is a crucial companion to causal inference, necessary to build scientific knowledge without resorting to expensive or impossible randomised control trials.In this paper, we explore how reasoning with symbolic representations can support causal discovery.Specifically, we deploy assumption-based argumentation (ABA), a wellestablished and powerful knowledge representation formalism, in combination with causality theories, to learn graphs which reflect causal dependencies in the data.We prove that our method exhibits desirable properties, notably that, under natural conditions, it can retrieve ground-truth causal graphs.We also conduct experiments with an implementation of our method in answer set programming (ASP) on four datasets from standard benchmarks in causal discovery, showing that our method compares well against established baselines.</p>
<p>Introduction</p>
<p>Causal Discovery is the process of extracting causal relationships amongst variables in data, represented as graphs.These graphs are crucial for understanding causal effects and perform causal inference (Peters, Janzing, and Schölkopf, 2017;Pearl, 2009;Spirtes, Glymour, and Scheines, 2000), e.g. to determine the impact of an action or treatment on an outcome.Causal effects are ideally discovered through interventions or randomised control trials, but these can be expensive, time consuming or outright impossible, e.g. in healthcare, trying to establish whether smoking causes cancer through a randomised control trial would require the study group to take up smoking to measure its (potentially deadly) effect.Hence the need to use observational, as opposed to interventional, data to study causes and effects (Peters, Janzing, and Schölkopf, 2017;Schölkopf et al., 2021).</p>
<p>Prominent approaches to perform causal discovery include constraint-based, score-based and functional causal model-based methods (see e.g.(Glymour, Zhang, and Spirtes, 2019;Vowels, Camgoz, and Bowden, 2022;Zanga, Ozkirimli, and Stella, 2022) for overviews).These approaches employ statistical methods to retrieve the causal relations between variables.However, statistical methods, even if consistent with infinite data, are prone to errors due to finite data.As a result, the extracted causal relations can deviate from the ground truth and, crucially, also from the observed data.Let us consider an example.</p>
<p>Example 1.1.We set out to discover the causal relations between rain (r), wet roof terrace (wr), wet street (ws) and watering plants (wp) (on the roof terrace).After collecting sufficient data, we carry out conditional independence tests.These correctly return that r and wp are independent (written r⊥ ⊥ wp); but also find r and wp independent when conditioned on {wr} (written r⊥ ⊥ wp | {wr}) which goes against our intuition: since something must have caused wr, we can infer r when knowing ¬wp and vice versa.That is, r and wp become dependent when conditioning on {wr}.</p>
<p>Below, we depict the ground truth causal graph (left) and the output of Majority-PC (right), proven to be sound and complete with infinite data (Colombo and Maathuis, 2014).A directed edge is interpreted as cause, e.g., wp causes wr; the absence of an edge indicates causal independence; an undirected edge indicates a causal relationship but the direction of the cause and effect relation remains unclear.</p>
<p>Since the conditional independence test wrongly rendered r and wp independent given {wr}, it is impossible to retrieve the ground truth whilst satisfying all reported causal relations between the variables.In fact, it can happen that no graph exists that faithfully captures the results of the tests.</p>
<p>To account for the issues observed in the example, researchers have investigated several methods to handle conflicting data; e.g., Corander et al. (2013) utilised Answer Set Programming (ASP) to learn chordal Markov networks; Hyttinen, Eberhardt, and Järvisalo (2014) provide an encoding of graphical interventions to compute causal graphs; Rantanen, Hyttinen, and Järvisalo (2020) use constraint programming.However, argumentative methods, which are ideally suited for conflict resolution, have not received much attention in the context of causal discovery so far.A notable exception is the work by Bromberg and Margaritis (2009) who employ a form of preference-based argumentation (Amgoud and Cayrol, 2002), instantiated with deductive argumentation (see (Philippe Besnard, 2018) for an overview), to choose a set of tests to use within the PC algorithm (Spirtes, Glymour, and Scheines, 2000).Their method is however based on Pearl's graphoid axioms which are incomplete; thus, some inconsistencies between the reported (in)dependences might not be detected by their approach.</p>
<p>In this paper, we provide a novel argumentative approach to account for inconsistencies in the reported tests and reflect a consistent subset of them into a directed acyclic graph (DAG).In line with the causal discovery literature, we assume faithfulness of the data, i.e., that all the independencies in the data are compatible with some DAG structure (Spirtes, Glymour, and Scheines, 2000) as well as sufficiency i.e. there are no latent confounders.To handle conflicts in data, we employ assumption-based argumentation (ABA) which is a versatile non-monotonic reasoning formalism ( Čyras et al., 2018) based on assumptions (i.e., defeasible elements) and inference rules.ABA has been studied under numerous semantics, which are criteria to determine the acceptance of assumption sets and their conclusions.A single ABA framework can possess several different extensions, i.e., sets of acceptable assumptions w.r.t. a given semantics, which reflect the different viewpoints that exist within a single framework.</p>
<p>In Fig. 1 we summarise the workflow of our method.Based on (i) the output of statistical methods as well as potential (ii) domain knowledge provided by experts, we construct an ABA framework whose extensions provide all the DAGs compatible with (i) and (ii).Overall, our contributions in this work are as follows:</p>
<p>• We formalise causal graphs in the language of ABA (Causal ABA).We use rules to model the d-separation criterion (Pearl, 2009), which characterises conditional independence in DAGs; and assumptions to model conditional independence and causal relations.</p>
<p>• We provide an ASP implementation of our theoretical framework using the independence tests from the Majority-PC algorithm (Colombo and Maathuis, 2014) as hard or weak constraints, resulting in ABA-PC.We employ weights for fact selection when necessary.</p>
<p>• We experimentally evaluate our ABA-PC algorithm with four (standard) datasets.Our experiments show that our proposed framework improves on current state-of-the-art baselines in Causal Discovery.In particular, we reconstruct the ground-truth causal DAG better than Majority-PC using the same set of independence relations.</p>
<p>Preliminaries</p>
<p>Graphs are crucial for causal and argumentation theories.A graph G = (V, E) has nodes V and edges
E ⊆ V × V; G is directed if either (x, y) ∈ E or (y, x) ∈ E; undirected if (x, y
) ∈ E and (y, x) ∈ E; and partially directed otherwise.</p>
<p>The skeleton of G is the result of replacing all directed edges with undirected ones.x, y ∈ V are adjacent iff (x, y) ∈ E or (y, x) ∈ E. A (x 1 -x n -)path path is a sequence of distinct nodes x 1 . . .x n s.t. for 1 ≤ i &lt; n, x i and x i+1 are adjacent.We omit 'x 1 -x n ' if it is clear from the context.Given a path p = x 1 . . .x n and a node x, we sometimes abuse notation and write x ∈ p to specify that x is contained in p, i.e., there is i
≤ n s.t. x = x i . A path x 1 . . . x n is directed if (x i , x i+1 ) ∈ E for all i ≤ n; cyclic if it is directed and x 1 = x n . A directed acyclic graph (DAG) is a directed graph without cycles.</p>
<p>Causal Graphs</p>
<p>A causal graph represents causal relations between variables (Pearl, 2009;Spirtes, Glymour, and Scheines, 2000).In this paper, we focus on causal graphs that admit a DAG structure.A triple (x i , x j , x k ) of variables in a DAG is an Unshielded Triple (UT) if two variables are not adjacent but each is adjacent to the third.An UT (x, y, z) is a v-structure iff (x, y) ∈ E and (z, y) ∈ E; y is a collider (w.r.t.x, z).
Definition 2.1. Let G = (V, E) be a DAG. A x-y-path p, x, y ∈ V, x ̸ = y, is Z-active for a set Z ⊆ V \ {x, y} in G iff for each node z ∈ p: if z is a collider in p, then z ∈ Z or there is a descendant z ′ of z s.t. z ′ ∈ Z; otherwise, z / ∈ Z.
Two variables x, y ∈ V are independent, conditioned on a set Z ⊆ V \ {x, y}, if fixing the values of the variables in Z does not provide additional information about x or y (resp.).Independence in DAGs is captured by d-separation.Definition 2.2.Let G = (V, E) be a DAG.Two nodes x, y ∈ V are d-connected given Z ⊆ V iff G contains a Z-active x-y-path p.The nodes x, y ∈ V are d-separated given Z iff x, y are not d-connected given Z.Two variables x, y are independent w.r.t.Z in G iff they are d-separated given Z, denoted by x ⊥ ⊥ G y | Z.</p>
<p>Causal Graphs and Statistics Causal Discovery couples statistical and graphical methods to extract causal graphs from data.The nodes V = {X 1 , . . ., X d } in a causal graph G = (V, E) correspond to random variables (in our running Example 1.1, 'rain' can be a random variable when associated with observed data) and the edges represent causal relationships between them.A joint probability distribution P factorizes according to a DAG
G if P (V) = d i=1 P (X i | pa(G, X i ))
, where pa(G, X i ) denotes the set of parents of X i in G.A distribution P is Markovian w.r.t.G if it respects the conditional independence relations entailed by G via d-separation.In turn, P is faithful to G if DAG G reflects all conditional independences in P .Different DAGs can imply the same set of conditional independences, in which case they form a Markov Equivalence Class (MEC) (Richardson and Spirtes, 1999).DAGs in a MEC present the same adjacencies and v-structures and are uniquely represented by a Completed Partially DAG (CPDAG) (Chickering, 2002) which is a partially directed graph that has a directed edge if every DAG in the MEC has it, and an undirected edge if both directions appear in the MEC.</p>
<p>A Conditional Independence Test (CIT), e.g.Fisher's Z (Fisher, 1970), HSIC (Gretton et al., 2007), or KCI (Zhang et al., 2011), is a procedure to measure independence with a known asymptotic distribution under the null hypothesis H 0 of independence.Calculating the test statistic for a dataset allows to estimate the test's observed significance level (pvalue), under H 0 .This is a measure of evidence against H 0 (Hung et al., 1997).Under H 0 , p is uniformly distributed in the interval [0, 1], which allows to set a significance level α that represents the pre-experiment Type I error rate (rejecting H 0 when it is true), whose expected value is at most α.A CIT, denoted by I(X i , X j | Z), outputs a p-value.
If I(X i , X j | Z) = p ≥ α then X i ⊥ ⊥ X j | Z. Instead, if I(X i , X j | Z) = p &lt; α then we can reject H 0 and declare the variables dependent: X i ̸⊥ ⊥ X j | Z.</p>
<p>Assumption-based Argumentation</p>
<p>We recall basics of assumption-based argumentation (ABA); for a comprehensive introduction we refer to ( Čyras et al., 2018).We assume a deductive system (L, R), where L is a formal language, i.e., a set of sentences, and R is a set of rules over L. A rule r ∈ R has the form a 0 ← a 1 , . . ., a n with a i ∈ L, head(r) = a 0 and body(r) = {a 1 , . . ., a n }.Definition 2.3.An ABA framework (ABAF) is a tuple (L, R, A, ), where (L, R) is a deductive system, A ⊆ L a set of assumptions, and : A → L is a function mapping assumptions a ∈ A to sentences L (contrary function).</p>
<p>A sentence q ∈ L is tree-derivable from S ⊆ A and rules R ⊆ R, denoted by S ⊢ R q, if there is a finite rooted labeled tree T which, intuitively, corresponds to the structure of the derivation: the root of T is labeled with q; the set of labels for the leaves of T is equal to S or S ∪ {⊤}; and for every inner node v of T there is a rule r ∈ R such that v is labelled with head(r), the number of successors of v is |body(r)| and every successor of v is labelled with a distinct a ∈ body(r) or ⊤ if body(r) = ∅.We often drop R and write S ⊢ R q simply as S ⊢ q if it does not cause confusion.Definition 2.4.Let D = (L, R, A, ) be an ABAF.A set S ⊆ A attacks T ⊆ A if there is S ′ ⊆ S, a ∈ T , s.t.S ′ ⊢ a.A set S is conflict-free in an ABAF D (S ∈ cf (D)) if it does not attack itself; S defends T iff it attacks each attacker of T ; S is closed iff S ⊢ a implies a ∈ S; S is admissible (S ∈ ad (D)) if it is conflict-free and defends itself.</p>
<p>With a little notational abuse we say a set S of assumptions attacks an assumption a if S attacks the singleton {a}; we let S = {a | a ∈ S}.</p>
<p>An ABAF D is called flat iff each set S of assumptions is closed.We call an ABAF non-flat if it does not belong to the class of flat ABAFs.</p>
<p>We next recall grounded, complete, preferred, and stable ABA semantics (abbr.gr , co, pr , stb).Definition 2.5.Let D be an ABAF and let S ∈ ad (D).</p>
<p>• S ∈ co(D) iff S contains every assumption set it defends;
• S ∈ gr (D) iff S is ⊆-minimal in co(D); • S ∈ pr (D) iff S is ⊆-maximal in co(D); • S ∈ stb(D) iff S attacks each {x} ⊆ A \ S.
Given a semantics σ, we call σ(D) the set of σ-extensions of the ABAF D. We drop 'σ' if it is clear from context.</p>
<p>Graphical ABA Representation Argumentation frameworks with collective attacks (SETAFs) (Nielsen and Parsons, 2006) are ideally suited to depict the attack structure between the assumptions in ABAFs as outlined by König, Rapberger, and Ulbricht (2022).In brief, a SETAF is a pair (A, R) consisting of a set of arguments A and an attack relation R ⊆ 2 A × A. We can instantiate an ABAF D = (L, A, R, ) as SETAF by setting A = A and R is the induced attack relation: S ⊆ A attacks a ∈ A if S ⊢ a. Example 2.6.Consider an ABAF with assumptions a, b, c, their contraries a = s, b = p, c = q, and rules (p ← a, c) and (s ← c).We can represent the ABAF as SETAF: The graph depicts the attack structure between the assumptions; the collective attack is depicted as a joint arrow.</p>
<p>Capturing Causal Graphs with ABA</p>
<p>We formalise causal graphs in ABA.We assume a fixed but arbitrary set of variables V with |V| = d.We refrain from explicitly mentioning the language L. Each assumption a below has a distinct contrary a c ; for convenience, we write a instead of a c if it does not cause confusion.</p>
<p>Causal ABA</p>
<p>The class of causal relations we aim to capture are characterised by two factors: acyclicity and d-separation.</p>
<p>Acyclicity We formalise graph-theoretic properties since our expected outcome, i.e., the resulting extensions, are graphs.Thus, the assumptions in our ABAF are arrows:
A arr = {arr xy | x, y ∈ V, x ̸ = y}.
Then, we define acyclicity as follows.Definition 3.1.Let D dag = (A dag , R dag , ) where
A dag = A arr ∪ {noe xy | x, y ∈ V, x ̸ = y}
and R dag contains the following rules: We show that D dag correctly captures the set of all DAGs of fixed size d.The correspondence is true for all (except gr ) argumentation semantics under consideration.Below, we use the assumption arr xy to stand for the arrow (x, y).All proofs of this section are provided in Appendix §A.
• a ← b, a ̸ = b,Proposition 3.3. {(V, S ∩ A arr ) | S ∈ σ(D dag )} = {G | G is a DAG} for σ ∈ {co, pr , stb}.
Note that the grounded extension corresponds to the fully disconnected graph G = (V, ∅) since the empty set is complete.Note also that the correspondence between DAGs and the extensions of the ABAF is one-to-many for complete, admissible and conflict-free assumption sets since a single acyclic graph corresponds to several complete extensions.Accepting the absence of an edge between two variables x, y can be realised by accepting noe xy or simply by accepting none of noe xy , arr xy , arr yx in the extension.</p>
<p>Example 3.4.In the ABAF from Example 3.2, the fully disconnected graph (V, ∅) corresponds to 2 3 complete extensions; i.e, to each subset of {noe xy , noe yz , noe zx }.</p>
<p>For preferred and stable semantics, the correspondence is one-to-one; the semantics coincide in D dag , as stated below.Lemma 3.5.σ(D dag ) = τ (D dag ) for σ, τ ∈ {pr , stb}.Corollary 3.6.Let σ ∈ {pr , stb}.Each DAG G corresponds to a unique set S ∈ σ(D dag ) and vice versa.</p>
<p>D-separation</p>
<p>The first step to represent d-separation is to extend our ABAF with independence statements.We do so by assuming independence between variables.We let
A ind = {(x⊥ ⊥y | Z) | Z ⊆ V, x, y ∈ V \ Z, x ̸ = y}. The conditional independence x ⊥ ⊥ y | Z is violated if the variables x, y are d-connected, given the conditioning set Z.
Intuitively, we want to formalise x⊥ ⊥y | Z if there exists a Z-active path between x, y.</p>
<p>To capture this, it is convenient to formalise directed paths and we do so by letting R graph contain the following rules: where, intuitively, e xy stands for "edge between x and y."
dpath xy ←
To formalise d-connectedness in the context of ABA, we introduce collider-trees, which generalise the notion of path by adding branches from collider nodes.Definition 3.7.Let G = (V, E) be a DAG, x, y ∈ V.A x-y-collider-tree t is a sub-graph of G satisfying: (a) t contains an x-y-path p t ; (b) for all u ∈ t, if u / ∈ p t then there is v ∈ t such that v is a collider in p t and u is a descendant of v.A c-t-path from collider node c (of p t ) to a leaf node t, t / ∈ {x, y}, is called a branch of t.For a set of variables Z ⊆ V, we call t Z-active iff p t is active w.r.t.t ∪ (V, ∅).</p>
<p>In the remainder of the paper, we drop 'x-y' and simply say collider-tree whenever it does not cause confusion.Example 3.8.Consider a causal graph G with five variables x, y, z, u, v as depicted below (left), and some collider-trees, denoted p 1 , p 2 , p 3 , from top to bottom, resp.:
x y z u v x u z y x y x y z u v
The collider-trees p 1 and p 2 are active w.r.t.∅; both paths have no collider so they are active w.r.t.every set not intersecting them; p 3 is active for sets containing z, u or v.</p>
<p>We are now ready to define our causal ABA framework.Definition 3.9.A causal ABAF D ds = (A ds , R ds , ) is characterised by
A ds = A dag ∪ A ind , and R ds = R dag ∪ R graph ∪ R act ,
where R act contains rules (x⊥ ⊥y | Z ← t) for each Z-active x-y-collider-tree t with x ̸ = y, and Z ⊆ V \ {x, y}.</p>
<p>Example 3.10.Let us consider again Example 3.2 with V = {x, y, z}.We extend our ABAF with six independence assumptions and add the corresponding contraries.Below, we depict all arguments and attacks for the pair x, y; i.e., all attacks on the new assumptions (x⊥ ⊥y) and (x⊥ ⊥y | {z}).
. Let σ ∈ {pr , stb}, S ∈ σ(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S iff (x ⊥ ⊥ G y | Z).
Note that we cannot guarantee the correspondence for complete semantics, as illustrated next.Example 3.12.In the ABAF from Example 3.10, S = ∅ is complete; indeed, D ds does not contain assumptions that are unattacked.The corresponding graph is G = (V, ∅) (cf.Example 3.4).In G, each pair of variables is independent; however, S does not contain any independence statement.</p>
<p>The example above shows that the correspondence between independence assumptions and independencies entailed by a DAG via d-separation is not preserved when dropping ⊆-maximality of the extensions.Interestingly, the other direction of Proposition 3.11 still holds for complete semantics; i.e., no incorrect independence statements are included in a complete extension.Proposition 3.13.
Let S ∈ co(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S implies (x⊥ ⊥ G y | Z).</p>
<p>Integrating Causal Knowledge</p>
<p>So far, we have introduced an ABAF that faithfully captures conditional independence in causal models.We have shown that an independence statement (x⊥ ⊥y | Z) is contained in an extension S if and only if it is consistent with graph corresponding to S. This, in turn translates to the dependencies of the graph: x and y are dependent given Z iff (x⊥ ⊥y | Z) / ∈ S. Our proposed ABAF can be integrated in any causal discovery pipeline to add formal guarantees that the graph discovered corresponds to the independences in the data.We integrate information from external sources, might they be statistical methods or experts, as facts. 1n the remainder of this section, we write D ∪ {r} = (A, R ∪ {r}, ) for an ABAF D = (A, R, ) and rule r.</p>
<p>Let us consider again our three-variables example from before (cf.Example 3.10).First, suppose we have learned that x and y are marginally independent.We incorporate this information simply by adding the rule (x⊥ ⊥y ←).This rule ensures that each extension contains (x⊥ ⊥y).Since each extension must be closed, no active path between x and y can be accepted.We can proceed similarly when incorporating specific causal relations (directed edges).Proposition 3.14.
Let x, y, a, b ∈ V, Z ⊆ V \ {x, y}, X ⊆ V \ {a, b}, σ ∈ {pr , na, ss, stg, stb}, r ∈ {(x⊥ ⊥y | Z ←), (arr xy ←)}. For each S ∈ σ(D ds ∪ {r}), it holds that (a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).
Crucially, we observe that adding external facts comes at a cost: the ABAF is not flat anymore; indeed, the independence and arrow literals might appear in the head of rules.</p>
<p>Now, what happens if we incorporate test results or causal relation from an external source?Suppose we discovered x and y are marginally dependent.When we add the rule (x⊥ ⊥y ←) we successfully render (x⊥ ⊥y) false; however, we lose the correspondence between the (in)dependence statements and the graph of a given extension: when adding the contrary of (x⊥ ⊥y) nothing (in the framework presented so far) prevents us from accepting one of the arrows arr xy or arr yx .We need to generalise the framework to ensure that our ABAF is sound when adding dependencies as facts to the framework, as discussed next.</p>
<p>Blocked paths</p>
<p>The ABAF D ds successfully captures that an active path implies dependence.To guarantee soundness, it remains to formalise the other direction: independence between two nodes x, y implies that each path linking them is blocked.For this, we introduce new assumptions
A bp = {bp p|Z | p is a x-y-path, Z ⊆ V \ {x, y}} with contraries bp p|Z = ap p|Z .
Furthermore, we require two new sets of rules: the first set of rules formalises that the independence between two variables x and y given Z requires that each path between x, y is blocked; the second set specifies when a path is Zactive.Definition 3.15.
For x, y ∈ V, x ̸ = y, Z ⊆ V \ {x, y}, we define R ext = R ds ∪ R xyZ with R xyZ containing the rules • x⊥ ⊥y | Z ← bp p1|Z , . . . , bp p k |Z where p 1 , . . . , p k denote
all paths between x and y; • ap p|Z ← p t for each Z-active x-y-collider-tree t with underlying x-y-path p t .</p>
<p>Let us consider the effect of these rules with an example.Example 3.16.Consider again Example 3.10; suppose we observed x ̸⊥ ⊥ y | {z}.We add the independence (x⊥ ⊥y | {z} ←) which prevents us from accepting all bp p|{z} assumptions at the same time (since each extension S is closed, we also accept (x ⊥ ⊥ y | {z}), therefore, this leads to a conflict).Consequently, one of the bp p|{z} assumptions is attacked, i.e., some path between x, y is active.</p>
<p>It can be checked that the paths p 1 , p 2 , p 3 depicted below are {z}-active:
x y x y x y z
As visualised in Example 3.10, each of these paths attack (x⊥ ⊥y | {z}).Due to the new rules from Definition 3.15 each path p i also derives ap pi|Z which attacks bp pi|Z .Therefore, each extension S must contain one of these paths.</p>
<p>We note that it suffices to add rules only for the dependence fact that we want to add.That is, when introducing fact (x⊥ ⊥y | Z ←) it suffices to add the rules from Definition 3.15 for x, y, Z.We define the extended ABAF.Definition 3.17.
For x, y ∈ V, Z ⊆ V{x, y}, the extended causal ABAF D xyZ csl = (A csl , R csl , ) is characterised by A csl = A ds ∪ A bp and R csl = R ext ∪ {x⊥ ⊥y | Z ←}.
The ABAF is sound and complete, as stated below.
Proposition 3.18. Let x, y, a, b ∈ V, let Z ⊆ V \ {x, y} and X ⊆ V \ {a, b}, let σ ∈ {pr , stb} and let S ∈ σ(D xyZ csl ). It holds that (a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).
Together, Propositions 3.14 and 3.18 guarantee that causal knowledge can be integrated in a faithful way.We obtain that this fine-tuned specification allows us to add (in)dependence facts and arrows whilst guaranteeing consistency of the causal ABAF.Independence facts and arrows can be added without further changes to the framework; when adding dependence facts, we require additional rules as specified in Definition 3.15.Below, we denote by
D T csl ,</p>
<p>Implementation</p>
<p>In this section, we present an instance of our Causal ABA algorithm which combines our causal ABAF with heuristic approaches to select the independence facts that it can take in input.The workflow of Algorithm 1 is as follows:</p>
<p>1.The main function of the algorithm is what we name causalaba (line 9 and 12 of Algorithm 1).The causal ABAF instance is determined by the number of nodes in the graph d and a set of facts T. We generate the causal ABAF D T csl presented in §3, using an ASP implementation in clingo (Gebser et al., 2019).We then compute the stable extensions of the causal ABAF.Our ASP encoding is detailed in §4.1.</p>
<p>The main input of
if p &gt; α &amp; x ⊥ ⊥ G y | Z then 18: S G ← S G + S(p, α, s, d) 19: else 20: S G ← S G − S(p, α, s, d) 21: G ← argmax(G ∈ M, S G ) ▷ Select G with max S G return G
extension at all.To overcome this problem, we select facts by assigning them appropriate weights (lines 2-12) and use these weights both to optimise (using weak constraints within causalaba) and rank (possibly several) output extensions (lines 10-20).We discuss this in §4.3.</p>
<p>Our proposed Algorithm 1 is a sound procedure to extract DAGs given a consistent set of independencies.</p>
<p>Proposition 4.1.Given a set V of variables and a set of (in)dependencies I, compatible with a (set of) MEC(s), Algorithm 1 outputs a DAG consistent with I.</p>
<p>In this work, we instantiate our Algorithm 1 using the Majority-PC algorithm (MPC) (Colombo and Maathuis, 2014) to source facts, resulting in the ABA-PC algorithm.</p>
<p>In the following subsections, we detail our implementation.</p>
<p>Remark 4.2.The causal ABAF D T csl from Definition 3.17 is potentially non-flat since assumptions can be derived: independence assumptions as well as arr and ap assumptions may appear in the head of rules.Thus, it lies in a broader ABA class, affecting semantical properties known for flat ABAFs; for instance, complete extensions may not always exist ( Čyras et al., 2018;Ulbricht et al., 2024).As a consequence, standard ABA solvers are not applicable to our case since they typically focus on the class of flat ABAFs.In this work, we therefore propose an Answer Set Programming (ASP) encoding of our causal ABAF under stable semantics.This also allows us to exploit ASP's grounding abilities to obtain causal ABAFs from concise schemata representations (see (Proietti and Toni, 2022) for the presentation of ABA in terms of schemata).</p>
<p>Encoding Causal ABA in ASP</p>
<p>Stable ABA semantics and stable semantics for Logic Programs (LP) are closely related (Caminada et al., 2015;Schulz and Toni, 2015); crucially, their correspondence has recently been extended to non-flat instances (Rapberger, Ulbricht, and Toni, 2024).In standard ABA-LP translations, assumptions are associated with their default negated contraries: an assumption a ∈ A with contrary a c ∈ L corresponds to the default negated literal not a c .These translations, however, consider only the case where the underlying logical language is atomic.To exploit the full power of ASP, we slightly deviate from standard translations, when appropriate, whilst guaranteeing consistency with our model.We also make use of more descriptive contrary names to enable a more intuitive reading.</p>
<p>For a set of variables V, we express the causal ABAF by 1. encoding DAGs: each answer set corresponds to a DAG;</p>
<ol>
<li>encoding d-separation: nodes x and y are independent given Z iff x and y are not linked via an active path.</li>
</ol>
<p>Following the standard translation, each ABA atom arr xy is translated to not arr xy .Here, we identify "not arr yx " simply with arrow(x,y) and "not noe xy " with edge(x,y).In our encoding, each answer set corresponds to precisely one DAG of size |V| = d for a given set V of variables.We encode acyclicity and further DAG-specific elements as expected; the encoding is given in the Appendix §B.</p>
<p>To link causality and DAGs we encode the d-separation criterion.To handle sets in ASP, we encode the (k-th) set S ⊆ V with predicates in(k,x).Module Π col in Listing 1 encodes collider and collider descendant (with natural specifications of the arrow and dpath (directed path) predicates).Next, we introduce non-blocking nodes: node v / ∈ {x, y} in an x-y-path is non-blocking, given Z, iff • v is a collider (with respect to its neighbours in the path) and either v ∈ Z or a descendant of v is in Z; or • v is not a collider and v / ∈ Z.</p>
<p>Lines 3-5 in Module Π col in Listing 1 encode these rules.Now, for each pair x, y ∈ V, for each set S \ {x, y}, for each x-y-path p = v 1 . . .v n with x = v 1 and y = v n , we add rules Π ap (p, (v i ) i≤k ) as specified in Listing 2. This Listing 3: Module Π bp (x, y, (p i ) i≤k )</p>
<p>1 indep(x,y,S) ← (not ap(x,y,pi,S)) i≤k , not in(x,S), not in(y,S), set(S).</p>
<p>guarantees the 'if '-direction: if x and y are connected via a Z-active path then they are dependent.For the 'only if 'direction, we require Module Π bp (x, y, (p i ) i≤k ): for each pair of variables x and y, we add the rule detailed in the listing to ensure that the absence of an active path between x and y implies independence between them; (p i ) i≤k denotes the list of all paths between x and y.The Module Π bp in Listing 3 encodes the blocked path rules defined in Definition 3.15.The indepand dep-predicates take two variables x, y, and a set S of variables as arguments.We note that, in general, the number of paths between two variables can be exponential (up to ⌊(d − 2)!e⌋).To lower the number of paths, we make use of the observation that fixing independence facts amounts to removing edges between nodes.2When fixing independence facts (a⊥ ⊥b | X ←), we thus consider only the paths in the skeleton that do not contain (a, b).</p>
<p>As outlined in Proposition 3.18, fixing dependence facts requires only the addition of the blocked path rules corresponding to the fact.That is, adding the fact (a ̸⊥ ⊥ b | X) only requires including the rules Π ap (p, (v i ) i≤k ) and Π bp (x, y, (p i ) i≤k ) to guarantee correctness.</p>
<p>As discussed in §3, our proposed ABAF returns all the DAGs compatible with some fixed facts, representing relations amongst nodes, may these be conditional/marginal independencies and/or (un)directed causal relations (arrows and edges).In the proposed instantiation of Causal ABA, ABA-PC, we input a set of facts in the form of independence relations and weight them according to their p-value.</p>
<p>Sourcing Facts</p>
<p>A DAG with d nodes is fully characterised by 1 2 d(d−1)2 d−2 independence relations, growing exponentially in the number of nodes.Therefore, it is not computationally efficient to carry out all possible tests, as in (Hyttinen, Eberhardt, and Järvisalo, 2014).Several solutions to this problem have been proposed in the Causal Discovery literature, e.g., (Spirtes, Glymour, and Scheines, 2000;Tsamardinos, Brown, and Aliferis, 2006;Colombo and Maathuis, 2014).Spirtes, Glymour, and Scheines (2000); Tsamardinos, Brown, and Aliferis (2006); Colombo and Maathuis (2014) all use conditional independence tests such as (Fisher, 1970;Zhang et al., 2011;Gretton et al., 2007).Other strategies to recover causal graphs from data, referred to as score-based methods, such as (Chickering, 2002;Ramsey et al., 2017) involve the use of statistical metrics that measure the addedvalue of adding/removing an arrow in terms of fit to the data.Hence they would return arrow weights.In this work, we use the MPC algorithm (Colombo and Maathuis, 2014), which provably 3 recovers the underlying CPDAG from data, to source facts.Let us illustrate the input facts we consider through our running example.Based on this erroneous results, MPC yields the graph shown in Example 1.1 (right), deviating from the ground truth.Crucially, the graph does not capture the independence relations listed above.In fact, there is no graph that satisfies the test results because it is not possible that r and wp are independent conditioned on any set, but r and ws, as well as wp and ws, are dependent.The dependencies indicate a path between r and wp, leading to a contradiction.</p>
<p>Note that Algorithm 1 is flexible to the choice of facts' source, e.g.we could have used the tests performed by (Tsamardinos, Brown, and Aliferis, 2006) or, with a slight modification, the arrow weights from (Ramsey et al., 2017).</p>
<p>Weighting Facts</p>
<p>Here we outline our strategy to weight independence tests results, based on their p-value and the size of the conditioning set.We use these weights as weak constraints and to rank facts and extensions.As a result of using stable semantics, wrong tests can render empty extensions if they contradict another (set of) test(s).Our aim is thus to exclude the wrong tests that create inconsistencies and cause our ABAF to output no extension.To this end, we define a simple heuristic to rank p-values from independence tests, given significance level α, but insensitive to whether they fall below or above it.Firstly, we define the following normalising function:
γ(p, α) = 2pα − 1 iff p &lt; α 2α−p−1 2(α−1)
otherwise Below is a plot of the function γ across the p-value interval, for three commonly chosen levels of α.</p>
<p>pendence information, see (Colombo and Maathuis, 2014) for detail and formal definitions.Note that the original PC strategy is based on the assumption that there will be no inconsistencies and therefore the algorithm does not test a pair of variables anymore once an independence is found.However, inconsistencies might arise when erroneous results are obtained.</p>
<p>The output of γ, for a given α and p, follows the intuition that the most uncertainty is around the significance threshold p = α (Sellke, Bayarri, and Berger, 2001;Berger, 2003), which we make correspond to the lowest value of γ = 0.5.</p>
<p>The final strength of the (in)dependence facts is obtained by weighting the output of the normalising function γ by a factor penalising bigger sizes of the conditioning set Z:
S(p, α, s, d) = (1 − s) (d − 2) γ(p, α)(1)
where s = |Z| the cardinality of the conditioning set and d = |V|, the cardinality of the set of nodes in the graph.The reason for weighting γ by s and d follows the intuition that the accuracy of independence test lowers as the conditioning set size increases (Sellke, Bayarri, and Berger, 2001).We use our final weights S to rank the test carried out by MPC.Then, our strategy is simple: exclude an incremental number of the lowest ranked tests until the returned extension is not empty.Let us illustrate our strategy.We thus start excluding the test with the lowest strength and progressively more until we find a model.In this example, the right DAG is obtained by excluding 9 of the performed tests, including the bottom five of the above list, and keeping the 14 strongest ones.</p>
<p>Here, we obtain exactly one DAG when excluding 40% of the tests carried out by MPC.If we obtain multiple models, we score each of them as in Algorithm 1, lines 14-19.In addition, we encode the (in)dependence facts as weak constraints, treated as optimisation statements (Gebser, Kaminski, and Schaub, 2011), to sort out sub-optimal extensions.</p>
<p>Our weighting function is similar to the one proposed by Bromberg and Margaritis (2009), with two differences: we re-base around 0.5 instead of 1 − α, to allow for more discrimination; and use the conditioning set size irrespective of the test's result, instead of including it only in the case of dependence (trusting that p-values accurately reflect the probability of wrongly rejecting the null hypothesis).</p>
<p>We emphasize that classical independence tests are asymmetric in nature, and inference of dependence is only possible if there is enough evidence against the null hypothesis (p &lt; α) with an expected Type I error (rejecting independence when it is true) corresponding to α.Conversely, no inference is possible when the p ≥ α, when the null hypothesis cannot be rejected, since p-values are distributed uniformily in [0, 1] under H 0 .As pointed out in (Bromberg and Margaritis, 2009;Hyttinen, Eberhardt, and Järvisalo, 2014), using p-values directly as strength is common but neither sound nor consistent.Transforming p-values to probability estimates, e.g. as in (Jabbari et al., 2017;Claassen and Heskes, 2012;Triantafillou, Tsamardinos, and Roumpelaki, 2014), would address this point, but out of scope for this work.A possible alternative to weighting and excluding facts might also be the use of less strict ABA semantics, left out of our experiments since not available in the ASP implementation used.</p>
<p>Empirical Evaluation</p>
<p>We evaluate our ABA-PC algorithm on four datasets from the bnlearn repository (Scutari, 2014), which hosts commonly used benchmarks in Causal Discovery, some of which based on real published experiments or expert opinions.We use the Asia, Cancer, Earthquake and Survey datasets, which represent problems of decision making in the medical, law and policy domains (see Appendix §C.1 for details).Implementation and computing infrastructure details, including code to reproduce the experiments, are in Appendix §C.2.</p>
<p>Evaluation Metrics and Baselines</p>
<p>For evaluation, we use a prominent metric in causal discovery: Structural Interventional Distance (SID) (Peters and Bühlmann, 2015) measures the deviation in the causal effects estimation deriving from a mistake in the estimated graph.SID works as a "downstream task" error rate for the causal inference task, which has causal graphs as a pre-requisite.We calculate SID between the estimated and the true CPDAG and repeat the experiments 50 times per dataset to record confidence intervals.Given that a CPDAG is a mixed graph, SID is calculated for the worst and best scenarios.In order to compare across graphs with different number of edges, we normalise SID (NSID) dividing it by the number of edges in the true DAG.NSID can go above 100% since extra edges could be introduced in the structure.We provide details on the metrics in Appendix §C.3 and results based on additional metrics (SHD, F1 score, precision and recall) in Appendix §C.5.</p>
<p>We compare ABA-PC to four baselines: a Random sample of graphs of the right dimensions (V,E); Fast Greedy Search (FGS) (Ramsey et al., 2017) and NOTEARS-MLP (Zheng et al., 2020) which use, respectively, the Bayesian Information Criterion and Multilayer Perceptrons with a continuous formulation of acyclicity to optimise the graph's fit to the data; and MPC (Colombo and Maathuis, 2014). 4More details are provided in Appendix §C.4.</p>
<p>Results</p>
<p>The results of our experiments are in Fig. 2. Best and worst SID are in the (Low, resp.High) sections for each dataset; the number of edges and nodes in each dataset is given below the x-axis labels.ABA-PC ranks 1 st in the worst case SID (High) for all datasets.It performs significantly (w.r.t.t-tests of difference in means, see §C.6) better than all baselines on three out of four datasets (Cancer, Earhquake and Asia) and is on par with NOTEARS-MLP for the Survey data.Furthermore, ABA-PC performs significantly better than MPC for all datasets.This demonstrates how, with the same underlying information from the data, our proposed method returns more accurate CPDAGs in the worst case scenario.For the best case SID (Low), ABA-PC is significantly better than all baselines for two datasets (Earthquake and Asia).Overall, we observe that ABA-PC performs well on benchmark data compared to a varied selection of baselines from the literature.Scalability In Fig. 3 we show the elapsed time (on a log scale) by the number of nodes.This are the recorded times for the experiments in Fig. 2 with 50 repetitions per dataset.As we can see, ABA-PC is the least efficient method.The main reasons for this are the complexity of both the grounding of logical rules and the calculation of the extensions, which clingo carries out exactly and efficiently, but still constitute a bottleneck.We already identified avenues of future work, discussed next, to address scaling limitations of our implementation, given the promising results shown in Fig. 2.</p>
<p>Conclusion</p>
<p>We proposed a novel argumentation-based approach to Causal Discovery, targeting the resolution of inconsistencies in data, and showed that it outperforms existing statisticsbased methods on four (standard) datasets.Our approach uses independence tests and their p-values to narrow down DAGs most fitting to the data, drawn from stable extensions of ABA frameworks.Other methods to identify and resolve inconsistencies in data for causal discovery have been proposed, e.g. by Ramsey, Spirtes, and Zhang (2006); Colombo and Maathuis (2014), but they focus on marking orientations as ambiguous in the presence of inconsistencies, rather than actually resolving the inconsistencies as we do.</p>
<p>Our proposed framework allows for the introduction of weighted arrows and edges, on top of independencies, which would allow to integrate, as future work, other data-centric methods like score-based causal discovery algorithms (e.g.(Chickering, 2002;Ramsey et al., 2017;Claassen and Heskes, 2012).As for the scalability, we cannot process more than 10 variables at the current state.We are currently working on making the processing of extensions more efficient and on incremental solving to avoid re-grounding when deleting independence facts.Additionally, we would like to extend our approach to deal with latent confounders, in line with (Colombo et al., 2012;Hyttinen, Eberhardt, and Järvisalo, 2014) and cycles (Rantanen, Hyttinen, and Järvisalo, 2020;Richardson and Spirtes, 1999;Hyttinen, Eberhardt, and Järvisalo, 2014) and experiment with other argumentation semantics in the literature, making use of a recently developed solver for non-flat ABA (Lehtonen et al., 2024).Finally, we plan to explore the explainability capabilities intrinsic in an ABA framework ( Čyras et al., 2018), which we believe may bring great value to causal discovery in a collaborative human-AI discovery process (Russo and Toni, 2023).
A Proofs of Section 3 Proposition 3.3. {(V, S ∩ A arr ) | S ∈ σ(D dag )} = {G | G is a DAG} for σ ∈ {co, pr , stb}.
Proof.By the well-known relations between the semantics (Baroni, Caminada, and Giacomin, 2018), i.e., cf (D) ⊇ ad (D) ⊇ co(D) ⊇ pr (D) ⊇ stb(D) and cf (D) ⊇ stb(D), it suffices to prove the statement for conflict-free and stable semantics.Below, we make use of the following definition.Definition A.1.For a set of assumptions S, we let G(S) = S ∩ A arr denote the graph corresponding to S.
• (σ = cf ) (⊆) Let S ∈ cf (D dag ).Proposition 3.11. Let σ ∈ {pr , stb}, S ∈ σ(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S iff (x ⊥ ⊥ G y | Z).
Proof.By Lemma 3.5 it suffices to prove the proposition for stable semantics.</p>
<p>(⇒) Suppose x ⊥ ⊥ y | Z ∈ S. Towards a contradiction, suppose x ̸⊥ ⊥ y | Z in G(S).Then there exists a Z-active path p between x and y in G(S).Then there is a Z-active xy-collider-tree t where p is the connecting path between x and y (i.e., p = p t ).By definition of the graph G(S), each directed arrow in p is contained in S. Hence, S ⊢ x ⊥ ⊥ y | Z can be derived, contradiction to conflict-freeness of S.</p>
<p>(⇐) For the other direction, suppose x ⊥ ⊥ y | Z in G(S).We show that each set of assumptions T with T ⊢ x ⊥ ⊥ y | Z is attacked.Consider a Z-active x-y-collider-tree t.Let p t denote the path corresponding to t.Note that p t / ∈ S, otherwise, x and y are d-connected given Z.Let (a, b) ∈ p t denote the arrow which is not contained in S. Since S is stable, it holds that {arr ab , arr ba , noe ab } ∩ S = ∅.Therefore, either noe ab ∈ S or arr ba ∈ S. Therefore, p t is attacked by S. Since t was arbitrary, we obtain x ⊥ ⊥ y | Z is defended by S and therefore x ⊥ ⊥ y | Z ∈ S.
Proposition 3.13. Let S ∈ co(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S implies (x⊥ ⊥ G y | Z).
Proof.Suppose x ⊥ ⊥ y | Z ∈ S. Towards a contradiction, suppose x ̸⊥ ⊥ y | Z in G(S).Then there exists a Z-active path p between x and y in G(S).Then there is a Z-active x-y-collider-tree t where p = p t is the connecting path between x and y.By definition of the graph G(S), each directed arrow in p t is contained in S. Hence, S ⊢ x ⊥ ⊥ y | Z can be derived, contradiction to conflict-freeness of S.</p>
<p>Proposition 3.14.Let x, y, a, b ∈ V, Z ⊆ V \ {x, y}, X ⊆ V \ {a, b}, σ ∈ {pr , na, ss, stg, stb}, r ∈ {(x⊥ ⊥y | Z ←), (arr xy ←)}.For each S ∈ σ(D ds ∪ {r}), it holds that
(a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).
Proof.The proof is analogous to the proof of Proposition 3.11.The rule r ensures that S contains the assumption
x ⊥ ⊥ y | Z resp. arr xy . Proposition 3.18. Let x, y, a, b ∈ V, let Z ⊆ V \ {x, y} and X ⊆ V \ {a, b}, let σ ∈ {pr , stb} and let S ∈ σ(D xyZ csl ). It holds that (a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).
Proof.By Lemma 3.5 it suffices to prove the proposition for stable semantics.Let r = x ⊥ ⊥ y | Z ←.</p>
<p>(⇒) This direction is analogous to the proof of Proposition 3.11.</p>
<p>(⇐) For the other direction, suppose a ⊥ ⊥ b | X in G(S).We proceed by case distinction.</p>
<p>Case 1 (a = x, b = y, X = Z) Towards a contradiction, suppose x ⊥ ⊥ y | Z / ∈ S. Since S is closed, there is some x-y-path p such that bp p|Z / ∈ S (otherwise, S ⊢ x ⊥ ⊥ y | Z, contradiction).Therefore, S attacks bp p|Z .This is the case if S derives ap p|Z .By definition of D csl , this is the case if S contains an Z-active x-y-collider-tree t.Consequently, x and y are d-connected given Z, contradiction to x ⊥ ⊥ y | Z in G(S).</p>
<p>Case 2 (a ̸ = x or b ̸ = y or X ̸ = Z) Analogous to the proof of Proposition 3.11.</p>
<p>B ASP Encodings: Additional Details</p>
<p>We recall the standard translation from ABA to LP.We assume that the ABA has precisely one contrary for each assumption.Given an ABAF D = (L, A, R, ) and an atom p ∈ L, we let
rep(p) = not p, if p ∈ A a, if p = a ∈ A.
We extend the operator to ABA rules element-wise: Following the standard ABA to LP translation, each ABA atom arr xy is translated to not arr xy .With a slight deviation from the causal ABAF, we identify "not arr yx " simply with arrow(x,y) and "not noe xy " with edge(x,y).Listing 4 contains our DAG encoding (as expected, var specifies variables).In Line 1, we guess one an arrow or the absence of an arrow; the remaining lines enforce that the graph is acyclic.The code faithfully captures the basis of our causal ABAF: each answer set of Module Π dag in Listing 4 corresponds to precisely one DAG of size |V | for a given set V of variables.</p>
<p>C Experimental Details C.1 bnlearn datasets</p>
<p>For our empirical evaluation (see §5), we used four datasets from the bnlearn repository.5This repository is widely used for research in Causal Discovery and hosts a number of commonly used benchmarks, some of which result from real published experiments or from the collection of expert opinions on the causal graph and the conditional probability tables necessary to create a Bayesian Network.Specifically, we use the Asia, Cancer, Earthquake and Survey datasets, as reported in reported in Table 1, which represent problems of decision making in the medical, law and policy domains.The datasets are from the small categories with number of nodes varying from 5 to 8. Details on the number of nodes, edges and density of the DAGs underlying the Bayesian Networks, together with links to a more detailed description on the bnlearn repository can be found in Table 1.</p>
<p>C.2 Implementation Details</p>
<p>We provide an implementation of ABA-PC using clingo (Gebser et al., 2019) version 5.6.2 and python 3.10.The code is available at the following repository: https://github.com/briziorusso/ArgCausalDisco.</p>
<p>In the repository, we also made available the code to reproduce all experiments and we saved all the plots, presented herein and in the main text, in HTML format.7 Downloading and opening them in a browser allows the inspection of all the numbers behind the plots in an interactive way.</p>
<p>Hyperparameters We used default parameters for all the methods.For MPC and ABAPC (ours) we used Fisher Z test (Fisher, 1970), as implemented in causal-learn, with significance threshold α = 0.05.</p>
<p>Computing infrastructure Our proposed method, together with MPC and FGS do not benefit from GPU accelleration.All the results were ran on Intel(R) Xeon(R) w5-2455X CPU with 4600 max MHz and 128GB of RAM.The NOTEARS-MLP was ran on NVIDIA(R) GeForce RTX 4090 GPU with 24GB dedicated RAM.</p>
<p>C.3 Evaluation Metrics</p>
<p>We evaluated the estimated graphs with five commonly used metrics in causal discovery (as in e.g.(Zheng et al., 2020;Lachapelle et al., 2020;Ramsey et al., 2017;Colombo and Maathuis, 2014 SID was proposed in (Peters and Bühlmann, 2015) and quantifies the agreement to a causal graph in terms of interventional distributions.It aims at quantifying the incorrect causal inference estimations stemming out of a mistake in the causal graph estimation, akin to a downstream task error on a pre-processing step.SHD is a graphical metric that counts the number of mistakes present in an estimated directed graph compared to a ground truth one.In particular it sums the extra edges, the missing ones and the wrong orientations.The lower the SHD the better.In the formula, Extra (E) is the set of extra edges and Missing (M) are the ones missing from the skeleton of the estimated graph.Reversed (R) are directed edges with incorrect direction.Precision and Recall measure the proportion of correct orientations based on the estimated graph and the true one, respectively.In the formulae, True Positive (TP) is the number of estimated edges with correct direction; False Positive (FP) is an edge which is not in the skeleton of the true graph.True Negative (TN) and False Negative (FN) are edges that are not in the true graph and correctly (resp, incorrectly) removed from the estimated graph.Finally, F1 score is the harmonic mean of precision and recall.</p>
<p>We carried out the evaluation in the main text on CPDAGs.As discussed in §2, CPDAGs represent Markov Equivalence Classes.MECs are all that can be inferred from a given set of independence relations, since multiple DAGs can entail the same set of independencies.Both our method and NOTEARS-MLP output DAGs rather than CPDAGs (which are the output of the other two baselines used, FGS and MPC).For the methods returning DAGs, we first transform the DAG to a CPDAG (isolating skeleton and v-structures) and then calculate the metrics presented above.Evaluation on DAGs is provided in §C.5 for completeness.</p>
<p>C.4 Baselines</p>
<p>We used the following four baselines with respective implementations (see Section 5 for context):</p>
<p>• A Random baseline (RND) as in (Lachapelle et al., 2020), by just sampling 10 random graphs with the same number of nodes and edges as the ground truth.</p>
<p>• Majority-PC8 (MPC) (Colombo and Maathuis, 2014) is a constraint-based Causal Discovery algorithm.It uses independence tests and graphical rules based on dseparation to extract a CPDAG from the data.MPC is an improved version of the original Peter-Clark (PC) algorithm (Spirtes, Glymour, and Scheines, 2000) that renders it order-independent while mantaining soundness and completeness with infinite data.</p>
<p>• Fast Greedy equivalence Search9 (FGS) (Ramsey et al., 2017) is a score-based Causal Discovery algorithm.It is a fast implementation of GES (Chickering, 2002) where graphs are evaluated using the Bayesian Information Criterion (BIC) upon addition or deletion of an edge, in a greedy fashion, involving the evaluation of insertion and removal of edges in a forward and backward fashion.Its output is a CPDAG.</p>
<p>• NOTEARS-MLP10 (NT) (Zheng et al., 2020) learns a non-linear SEM via continuous optimisation.Having a Multi-Layer Perceptron (MLP) at its core, this method should adapt to different functional dependencies among the variables.The optimisation is carried out via augmented Lagrangian with a continuous formulation of acyclicity (Zheng et al., 2018), outputting a DAG.</p>
<p>C.5 Additional Results</p>
<p>Here we provide results that complement the ones in §5 in the main text.Specifically, we evaluate our method based on SHD, F1 score, Precision and Recall (see §C.3 for details).</p>
<p>Additional Metrics Additionally to the results shown in Fig. 2 in the main text, we evaluate our proposed method with four other metrics commonly used in Causal Discovery: SHD, F1 (Fig. 4), Precision and Recall (Fig. 5).Furthermore, we show the average size of the estimated CPDAGs in (Fig. 6).From Fig. 4, we can see that, according to Normalised SHD, ABA-PC is the best method for the Earthquake dataset and not significantly different from MPC (ranking first) for the other datasets.In terms of F1 score, ABA-PC is significantly better than all baselines for the Earthquake and Asia datasets, on par with all the others for Cancer and ranking second, after MPC, for the Survey dataset.Precision and Recall provide a breakdown of the F1 score (which is their harmonic mean), hence follows similar patterns.From the estimated graph size (the number of edges in the estimated graph) in Fig. 6 we can observe that ABA-PC is generally in line with the true graph size, apart from the Survey dataset for which some of the edges are missed.</p>
<p>DAGs Evaluation In the main text, we evaluated our method based on the estimated CPDAG compared to the ground truth one.Here, for completeness, we report results based on DAGs.Indeed, both our method and NOTEARS-MLP have DAGs in output.Note that this evaluation penalises the methods outputting CPDAGs.In transforming CPDAGs to DAGs, we had to only select the directed arrows in order to extract DAGs from the output CPDAGs (see Fig. 6 and 7 for the average size of the estimated CPDAGs and DAGs, respectively).We report the evaluation of DAGs in Fig. 8 (NSID and NSHD) and Fig. 9 (Precision and Recall).The plot of the F1 score is provided in the our repository both as image and interactive files.11According to NSHD, we can see that ABA-PC performs significantly better than all baselines on two out of the four datasets (Survey and Survey).For the Earthquake and Asia datasets ABA-PC is on par with MPC and significantly better than the other baselines.According to NSID, ABA-PC is not significantly different than MPC, ranking 1 st for the Earthquake and Asia datasets, and in line with all other baselines for the other two datsets.According to precision and recall, ABAPC is better than MPC for three out of the four datasets when evaluating on recall, while maintaining the same precision, again, three out of four times.For the Asia dataset ABAPC trades some precision for an significantly higher recall.</p>
<p>C.6 Statistical Tests</p>
<p>Here we present details of the statistical tests used to measure the significance of the difference in the results presented in Fig. 2 in the main text.In tables 2, 3, 4 and 5 we provide t-statistics and p-values for the Cancer, Earthquake, Survey and Asia datasets, respectively.In each table we present pairwise comparisons of means (shown in brakets together with standard deviations), for the best and worst case SID (High and Low, resp.)presented in Fig. 2 of the main text.</p>
<p>Figure 1 :
1
Figure 1: Overview of the workflow of our Causal ABA algorithm, which combines statistical methods and expert domain knowledge with non-monotonic reasoning and performs argumentative reasoning to output causal graphs consistent with the reported causal relationships.</p>
<p>•</p>
<p>{a, c} collectively attacks b as {a, c} ⊢ p; • c attacks a since {c} ⊢ s.</p>
<p>where T is a set of (in)dependence and arrow facts, the ABAF obtained by the iterative update of the ABAF D ds with D xyZ csl for all dependence facts (x⊥ ⊥y | Z ←) ∈ T .Corollary 3.19.Let T be a set of (in)dependence and arrow facts, σ ∈ {pr , stb}, x, y ∈ V, Z ⊆ V \ {x, y} and S ∈ σ(D T csl ).Then (x⊥ ⊥y | Z) ∈ S iff (x ⊥ ⊥ G y | Z).</p>
<p>TT</p>
<p>Algorithm 1 is a set of independence facts (I), alongside the significance threshold α and the number of nodes d.We discuss sourcing facts in §4.2.3.As shown in §3, each stable extension corresponds to aDAG compatible with the fixed set of independence tests.However, statistical methods can return erroneous results, in which case our causal ABAF might output no stable Algorithm 1: Causal ABA (with independence facts) Input: I, α, |V| = d 1: T ← [ ] 2: for p = I(x, y | Z) ← T + [(indep(x, y, Z), S(p, α, s, d))] ← T + [(dep(x, y, Z), S(p, α, s, d))] 8: T ← sort(T, S) ▷ Sort elements of T by strength S 9: M = causalaba(d, T) 10: while M = ∅ do 11: T ← T[2 . . .|T|] ▷ Drop fact with lowest S</p>
<p>Module Π ap (p, (v i ) i≤k ) 1 ap(v1,v k ,p,S) ← (arrow(vi,vi+1)) i&lt;k , not in(v1,S), not in(v k ,S), set(S), (nb(vi,vi−1,vi+1,S)) 1&lt;i&lt;k . 2 dep(v1,v k ,S) ← ap(v1,v k ,p,S).</p>
<p>Example 4.3.We run the MPC algorithm in Example 1.1 which performs 23 out of 24 tests, including the following.r⊥ ⊥wp | {ws} wp⊥ ⊥ws | {r} r⊥ ⊥wp r⊥ ⊥wp | {wr} wp ̸⊥ ⊥ws | {r, wr} r⊥ ⊥wp | {wr, ws} However, only r ⊥ ⊥ wp is correct; the only other independence wp⊥ ⊥ G ws | {r, wr} in G is wrongly classified.All other tests result in dependencies.</p>
<p>Example 4.4.Consider again Example 1.1.The results of the independence tests from MPC (using Fisher's Z (1970) and α = 0.05) have the following p-values (we show the same subset of the 23 tests carried out, as in Example 4.3): r ⊥ ⊥ wp p = 0.45 S = 0.71 r ⊥ ⊥ wp | {ws} p = 0.52 S = 0.37 r ⊥ ⊥ wp | {wr} p = 0.33 S = 0.32 wp ⊥ ⊥ ws | {r} p = 0.05 S = 0.25 r ⊥ ⊥ wp | {wr, ws} p = 0.39 S = 0.00 wp ̸⊥ ⊥ ws | {r, wr} p = 0.03 S = 0.00 We apply Eq. 1 to calculate S. Ranking tests by S, as shown above, the right test is the highest scoring one.Fixing all the tests returns no solution.</p>
<p>Figure 2 :
2
Figure 2: Normalised Structural Interventional Distance for four datasets from the bnlearn repository.Lower is better.Low (resp.High) is the SID for the best (resp.worst) DAG in the estimated CPDAG.</p>
<p>Figure 3 :
3
Figure 3: Mean and Standard Deviation of the elapsed time in log scale by number of nodes averaged over 50 runs.</p>
<p>By definition, S cannot contain cycles; hence, S corresponds to a DAG.(⊇) Let G = (V, E) be a DAG.Let S = {arr x,y | (x, y) ∈ E}.By acyclicity of G, we obtain that S is conflict-free.• (σ = stb) (⊆) Let S ∈ stb(D dag ).By definition, S cannot contain cycles; hence, S corresponds to a DAG.(⊇) Let G = (V, E) be a DAG.Let S = {arr x,y | (x, y) ∈ E} ∪ {noe xy | (x, y), (y, x) / ∈ E}.Note that S contains exactly one of arr xy , arr yx , noe xy (since G does not contain a bi-directed arrow and S contains noe xy only if x and y are not linked in G).Therefore, S attacks each assumption which is not contained in S.Moreover, S does not contain cycles by acyclicity of G. Therefore, S is conflict-free.Lemma 3.5.σ(D dag ) = τ (D dag ) for σ, τ ∈ {pr , stb}.Proof.Since pr (D) ⊇ stb(D) it suffices to show pr (D dag ) ⊆ stb(D dag ).Let S ∈ pr (D dag ).The main observation is that for each pair of variables x, y ∈ V , either noe xy , arr xy or arr yx is contained in S. Towards a contradiction, suppose there exists a pair of variables such that S ∩ {noe xy , arr xy , arr yx } = ∅.It is easy to see that S∪{noe xy } is admissible (noe xy is only attacked by the corresponding arrows and defends itself against these attacks); contradiction to S being a ⊆-maximal admissible set.We obtain pr (D dag ) = stb(D dag ).</p>
<p>rep(r) = rep(head(r)) ← {rep(p) | r ∈ body(r)}.For an LP-ABAF D = (L, R, A, ), we define the associated LP P D = {rep(r) | r ∈ R}.</p>
<p>)): • Structural Intervention Distance (SID) • Structural Hamming Distance (SHD) = E + M + R • Precision = TP/(TP + FP) • Recall = TP/(TP + FN) • F1 = 2×Precision*Recall/(Precision+Recall)</p>
<p>Figure 4 :
4
Figure 4: Normalised Structural Hamming Distance (SHD, left y-axis) and F1 score (right y-axis) for the estimated CPDAGs for four datasets in the bnlearn repository.Lower is better for NSHD and higher is better for F1.</p>
<p>Figure 5 :
5
Figure 5: Precision (left y-axis) and Recall (right y-axis) for the estimated CPDAGs for four datasets in the bnlearn repository.Higher is better for both metrics.</p>
<p>Figure 6 :
6
Figure 6: Number of edges in the estimated CPDAGs compared to ground truth.</p>
<p>Figure 7 :
7
Figure 7: Number of edges in the estimated DAGs compared to ground truth.</p>
<p>Figure 8 :
8
Figure 8: Normalised Structural Hamming Distance (SHD, left y-axis) and Structural Intervention Distance (SID, right y-axis) for the estimated DAGs for four datasets in the bnlearn repository.Lower is better for both metrics.</p>
<p>Figure 9 :
9
Figure 9: Precision (left y-axis) and Recall (right y-axis) for the estimated DAGs for four datasets in the bnlearn repository.Higher is better for both metrics.</p>
<p>a, b ∈ {arr xy , arr yx , noe xy }, x, y ∈ V; • arr xixi+1 ← arr x1x2 , . . ., arr x k−1 x k for each sequence x 1 . . .x k with x 1 = x k , for each 1 ≤ i &lt; k.Intuitively, noe xy stands for "no edge between x and y."Note that we define only one atom noe xy for each pair of variables x, y.The first set of rules enables the choice between noe xy , arr xy and arr yx .The second ensures that no extension contains a cycle.Example 3.2.Consider V = {x, y, z}.The corresponding ABAF D dag contains 9 assumptions: for each pair of variables u, v ∈ V, we have arr uv , arr vu and noe uv .We observe that we have precisely two cyclic sequences of length &gt; 2, namely (from x) c 1 = xyzx and c 2 = xzyx.Both cycles attack each arrow it contains; the attack structure of the ABAF is depicted below.purple arrows pointing to arr xz represent the attack from set {arr yx , arr xz , arr zy } on the assumption arr xz based on the derivation {arr yx , arr xz , arr zy } ⊢ arr xz .
noe xyarr yxarr yznoe yzarr xyarr zyarr xzarr zxnoe
xzThe joint arcs represent collective attacks; e.g., the thick,</p>
<p>arr xy dpath xz ← dpath xy , arr yz e xy ← arr xy e xy ← arr yx noe xy ← e xy</p>
<p>The assumption (x ⊥ ⊥ y) is attacked by arr xy , arr yx and by all x-y-paths with inner node z except for the collider; (x⊥ ⊥y | {z}) is attacked by arr xy , arr yx and the collider {arr xz , arr yz }.Attacks for the other pairs are analogous.The formalization correctly captures independence, as stated in the following proposition.Proposition 3.11
x⊥ ⊥yx⊥ ⊥y | {z}noe xyarr yxarr yznoe yzarr xyarr zyarr xzarr zxnoe xz</p>
<p>Table 1 :
1
Details of dataset from bnlearn.
Dataset Name |N | |E| |E|/|N |CANCER540.8EARTHQUAKE540.8SURVEY661ASIA881Having downloaded all the .bif files from the repository,we load the Bayesian network and the associated conditionalprobability tables and sample 5000 observations with 50 dif-ferent seeds to measure variance and confidence intervals. 6
Assuming sufficient accuracy of the data as a first step; later on, we will assign weights to the reported independence statements in our final system to account for statistical errors.
This observation is key for constraint-based causal discovery algorithms such as PC(Spirtes, Glymour, and Scheines, 2000).
under the assumptions of sufficiency (no unmeasured confounders), faithfulness (data represents a DAG) and perfect inde-
We would have liked to compare to the method closest to our work, i.e.(Bromberg and Margaritis, 2009) but unfortunately there is no implementation available.
https://www.bnlearn.com/bnrepository/
We also run all the experiments shown with 2000 samples and resulted in analogous results, hence omitted.
https://github.com/briziorusso/ArgCausalDisco/tree/public/ results/figs
https://github.com/briziorusso/ArgCausalDisco/blob/public/ cd algorithms/PC.py
https://github.com/bd2kccd/py-causal
https://github.com/xunzheng/notears
https://github.com/briziorusso/ArgCausalDisco/tree/public/ results/figs
Acknowledgemets Russo was supported by UK Research and Innovation (grant number EP/S023356/1), in the UKRI Centre for Doctoral Training in Safe and Trusted Artificial Intelligence (www.safeandtrustedai.org).Rapberger and Toni were funded by the ERC under the EU's Horizon 2020 research and innovation programme (grant number 101020934) and Toni also by J.P. Morgan and by the Royal Academy of Engineering under the Research Chairs and Senior Research Fellowships scheme.
A reasoning model based on the production of acceptable arguments. L Amgoud, C Cayrol, Annals of Mathematics and Artificial Intelligence. 342002</p>
<p>Abstract argumentation frameworks and their semantics. P Baroni, M Caminada, M Giacomin, Handbook of Formal Argumentation. College Publications. 20184</p>
<p>Could fisher, jeffreys and neyman have agreed on testing?. J O Berger, Statistical Science. 1812003</p>
<p>Improving the reliability of causal discovery from small data sets using argumentation. F Bromberg, D Margaritis, Journal of Machine Learning Research. 102009</p>
<p>On the difference between assumption-based argumentation and abstract argumentation. M Caminada, S Sá, J F L Alcântara, W Dvorák, IfCoLog Journal of Logics and their Applications. 212015</p>
<p>Learning equivalence classes of bayesian-network structures. D M Chickering, Journal of Machine Learning Research. 22002</p>
<p>A bayesian approach to constraint based causal inference. T Claassen, T Heskes, Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI'12). the 28th Conference on Uncertainty in Artificial Intelligence (UAI'12)AUAI Press2012</p>
<p>Orderindependent constraint-based causal structure learning. D Colombo, M H Maathuis, Journal of Machine Learning Research. 1512014</p>
<p>Learning high-dimensional directed acyclic graphs with latent and selection variables. D Colombo, M H Maathuis, M Kalisch, T S Richardson, The Annals of Statistics. 4012012</p>
<p>Learning chordal markov networks by constraint satisfaction. J Corander, T Janhunen, J Rintanen, H Nyman, J Pensar, Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NeurIPS'13). the 27th Annual Conference on Neural Information Processing Systems (NeurIPS'13)2013</p>
<p>Assumption-based argumentation: Disputes, explanations, preferences. In Handbook of Formal Argumentation. K Čyras, X Fan, C Schulz, F Toni, 2018College Publications. chapter</p>
<p>Statistical methods for research workers. R A Fisher, Breakthroughs in statistics: Methodology and distribution. Springer1970</p>
<p>Multi-shot ASP solving with clingo. M Gebser, R Kaminski, B Kaufmann, T Schaub, Theory and Practice of Logic Programming. 1912019</p>
<p>Complex optimization in answer set programming. M Gebser, R Kaminski, T Schaub, Theory and Practice of Logic Programming. 114-52011</p>
<p>Review of causal discovery methods based on graphical models. C Glymour, K Zhang, P Spirtes, Frontiers in genetics. 105242019</p>
<p>A kernel statistical test of independence. A Gretton, K Fukumizu, C H Teo, L Song, B Schölkopf, A J Smola, Proceedings of the 21st Annual Conference on Neural Information Processing Systems (NeurIPS'07). the 21st Annual Conference on Neural Information Processing Systems (NeurIPS'07)Curran Associates, Inc2007</p>
<p>The behavior of the p-value when the alternative hypothesis is true. H M J Hung, R T O'neill, P Bauer, K Kohne, Biometrics. 5311997</p>
<p>Constraint-based causal discovery: conflict resolution with answer set programming. A Hyttinen, F Eberhardt, M Järvisalo, Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence (UAI'14). the 30th Conference on Uncertainty in Artificial Intelligence (UAI'14)AUAI Press2014</p>
<p>Discovery of causal models that contain latent variables through bayesian scoring of independence constraints. F Jabbari, J Ramsey, P Spirtes, G Cooper, Proceedings of the European Conference of Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2017. the European Conference of Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2017Springer2017</p>
<p>Just a matter of perspective. M König, A Rapberger, M Ulbricht, Proceedings of the 9th International Conference on Computational Models of Argument (COMMA'22). the 9th International Conference on Computational Models of Argument (COMMA'22)IOS Press2022353</p>
<p>Gradient-based neural DAG learning. S Lachapelle, P Brouillard, T Deleu, S Lacoste-Julien, Proceedings of the 8th International Conference on Learning Representations. the 8th International Conference on Learning Representations202020OpenReview.net</p>
<p>Instantiations and computational aspects of non-flat assumption-based argumentation. T Lehtonen, A Rapberger, F Toni, M Ulbricht, J P Wallner, CoRR abs/2404.114312024</p>
<p>A generalization of dung's abstract framework for argumentation: Arguing with sets of attacking arguments. S H Nielsen, S Parsons, 3rd International Workshop on Argumentation in Multi-Agent Systems (ArgMAS'06), Revised Selected and Invited Papers. Springer20064766</p>
<p>Graphoids: Graph-based logic for reasoning about relevance relations or When would x tell you more about y if you already know z?. J Pearl, A Paz, 1986Probabilistic and Causal Inference</p>
<p>Causality. J Pearl, 2009Cambridge University Press2 edition</p>
<p>Structural intervention distance for evaluating causal graphs. J Peters, P Bühlmann, Neural computation. 2732015</p>
<p>Elements of causal inference: foundations and learning algorithms. J Peters, D Janzing, B Schölkopf, 2017The MIT Press</p>
<p>A review of argumentation based on deductive arguments. Philippe Besnard, A H , Handbook of Formal Argumentation. College Publications20189</p>
<p>Learning assumption-based argumentation frameworks. M Proietti, F Toni, Proceedings of the 31st International Conference on Inductive Logic Programming. S H Muggleton, A Tamaddoni-Nezhad, the 31st International Conference on Inductive Logic ProgrammingSpringer202213779</p>
<p>A million variables and more: the fast greedy equivalence search algorithm for learning highdimensional graphical causal models, with an application to functional magnetic resonance images. J Ramsey, M Glymour, R Sanchez-Romero, C Glymour, International journal of data science. 32017</p>
<p>Adjacencyfaithfulness and conservative causal inference. J Ramsey, P Spirtes, J Zhang, Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI'06). the 22nd Conference on Uncertainty in Artificial Intelligence (UAI'06)AUAI Press2006</p>
<p>Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach. K Rantanen, A Hyttinen, M Järvisalo, International Journal of Approximate Reasoning. 1172020</p>
<p>On the correspondence of non-flat assumption-based argumentation and logic programming with negation as failure in the head. A Rapberger, M Ulbricht, F Toni, CoRR abs/2405.094152024</p>
<p>Automated Discovery of Linear Feedback Models. T Richardson, P Spirtes, Computation, Causation, and Discovery. AAAI Press1999</p>
<p>Causal discovery and knowledge injection for contestable neural networks. F Russo, F Toni, Proceedings of the 26th European Conference on Artificial Intelligence (ECAI'23). the 26th European Conference on Artificial Intelligence (ECAI'23)IOS Press2023372</p>
<p>Toward causal representation learning. B Schölkopf, F Locatello, S Bauer, N R Ke, N Kalchbrenner, A Goyal, Y Bengio, Proceedings of the IEEE. 10952021</p>
<p>Logic programming in assumption-based argumentation revisited -semantics and graphical representation. C Schulz, F Toni, Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI'15). the 29th AAAI Conference on Artificial Intelligence (AAAI'15)AAAI Press2015</p>
<p>M Scutari, Bayesian network repository. 2014</p>
<p>Calibration of ρ values for testing precise null hypotheses. T Sellke, M Bayarri, J O Berger, The American Statistician. 5512001</p>
<p>Causation, prediction, and search. P Spirtes, C N Glymour, R Scheines, 2000MIT press</p>
<p>Learning neighborhoods of high confidence in constraint-based causal discovery. S Triantafillou, I Tsamardinos, A Roumpelaki, Proceedings of the 7th European Workshop on Probabilistic Graphical Models (PGM'14). the 7th European Workshop on Probabilistic Graphical Models (PGM'14)Springer2014</p>
<p>The max-min hill-climbing bayesian network structure learning algorithm. I Tsamardinos, L E Brown, C F Aliferis, Machine learning. 652006</p>
<p>Non-flat ABA is an instance of bipolar argumentation. M Ulbricht, N Potyka, A Rapberger, F Toni, Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI'24). the 38th AAAI Conference on Artificial Intelligence (AAAI'24)AAAI Press2024</p>
<p>D'ya like dags? a survey on structure learning and causal discovery. M J Vowels, N C Camgoz, R Bowden, ACM Computing Surveys. 5542022</p>
<p>A survey on causal discovery: Theory and practice. A Zanga, E Ozkirimli, F Stella, International Journal of Approximate Reasoning. 1512022</p>
<p>Kernel-based conditional independence test and application in causal discovery. K Zhang, J Peters, D Janzing, B Schölkopf, Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI'11). the 27th Conference on Uncertainty in Artificial Intelligence (UAI'11)AUAI Press2011</p>
<p>Dags with NO TEARS: continuous optimization for structure learning. X Zheng, B Aragam, P Ravikumar, E P Xing, Prooceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS'18). 2018</p>
<p>PMLR. Table 2: t-tests for difference in means for Cancer dataset. X Zheng, C Dan, B Aragam, P Ravikumar, E Xing, Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS'20). the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS'20)2020108Learning sparse nonparametric DAGs. Significance levels: 0 '<strong><em>' 0.001 '</em><em>' 0.01 '</em>' 0.05 '.' 0.1 ' ' 1. Method (mean±std) t p-value NSID (low) APC (9.8 ± 2.2)vFGS (6.3 ± 1.9) 8.722 0.000</strong><em> APC (9.8 ± 2.2)vMPC (3.9 ± 1.8) 14.833 0.000</em>** APC (9.8 ± 2.2)vNT (9.3 ± 1.6</p>
<p>± 2.1)vMPC (13.9 ± 1.8) -7.315 0.000<strong><em> APC (11.0 ± 2.1). ** Mpc, vNT (10.2 ± 0.4) 2.794 0.007</em>* APC (11.0 ± 2.1)vRND (14.0 ± 3.0) -5.801 0.000</strong><em> FGS (16.3 ± 1.9)vMPC (13.9 ± 1.8) 6.503 0.000</em><strong> FGS (16.3 ± 1.9)vNT (10.2 ± 0.4) 22.465 0.000</strong><em> FGS (16.3 ± 1.9)vRND (14.0 ± 3.0) 4.442 0.000</em><strong> MPC (13.9 ± 1.8)vNT (10.2 ± 0.4) 14.130 0.000</strong><em> MPC (13.9 ± 1.8)vRND (14.0 ± 3.0) -0.321 0.749 NT (10.2 ± 0.4)vRND (14.0 ± 3.0) -8.934 0.000</em><strong> Table 3: t-tests for difference in means for Earthquake dataset. Significance levels: 0 '</strong><em>' 0.001 '</em><em>' 0.01 '</em>' 0.05 '.' 0.1 ' ' 1. Method (mean±std) t p-value NSID (low) APC (0.0 ± 0.0)vFGS (4.6 ± 1.4) -23.742 0.000*** APC (0.0 ± 0.0)vMPC (5.0 ± 0.0) -inf</p>
<p>. *** Apc, 19.347 0.000**<em> FGS (4.6 ± 1.4)vMPC (5.0 ± 0.0) -2.065 0.044</em> FGS (4.6 ± 1.4)vNT (4.6 ± 3.3) 0.079 0.937</p>
<p>)vRND (7.9 ± 2.9) -5.351 0.000<strong><em> NSID (high) APC (9.3 ± 4.5). 5.649 0.000</em></strong> FGS (15.0 ± 0.5)vMPC (15.0 ± 0.0) 0.000 1.000<strong><em> APC. 9.3 ± 4.5)vNT (13.5 ± 4.0) -4.812 0.000</em></strong> APC (9.3 ± 4.5)vRND (13.9 ± 3.5</p>
<p>Table 4: t-tests for difference in means for Survey dataset. FGS (15.0 ± 0.5)vNT (13.5 ± 4.0) 2.669 0.010<em> FGS (15.0 ± 0.5)vRND (13.9 ± 3.5) 2.266 0.028</em> MPC (15.0 ± 0.0)vNT (13.5 ± 4.0) 2.689 0.010<strong> MPC (15.0 ± 0.0)vRND (13.9 ± 3.5) 2.289 0.026<em> NT (13.5 ± 4.0)vRND (13.9 ± 3.5) -0.558 0.578415Significance levels: 0 '</em></strong>' 0.001 '*<em>' 0.01 '</em>' 0.05 '.' 0.1 ' ' 1. Method (mean±std) t p-value NSID (low) APC (14.2 ± 3.6)vFGS (13.7 ± 3.2) 0.819 0</p>
<p>. 14.2 ± 3.6)vMPC (5.1 ± 3.4) 12.854 0.000<strong><em> APC (14.2 ± 3.6)vNT (17.0 ± 0.0) -5.386 0.000</em></strong> APC (14.2 ± 3.6)vRND (13.1 ± 4.2) 1.405 0.163APC. </p>
<p>)vRND (22.4 ± 4.4) -8.633 0.000<strong><em> Table 5: t-tests for difference in means for Asia dataset. Significance levels: 0 '</em></strong>' 0.001 '<strong>' 0.01 '<em>' 0.05 '.' 0.1 ' ' 1. Method (mean±std) t p-value. 1.290 0.200</em></strong> APC. 11.7 ± 6.8)vMPC (15.2 ± 5.6) -2.828 0.006<strong> APC (11.7 ± 6.8)vNT (16.5 ± 4.2) -4.243 0.000</strong><em> APC (11.7 ± 6.8)vRND (25.0 ± 7.0) -9.622 0.000</em><strong> FGS (32.3 ± 5.0)vMPC (15.2 ± 5.6) 15.960 0.000</strong><em> FGS (32.3 ± 5.0)vNT (16.5 ± 4.2) 16.895 0.000</em><strong> FGS (32.3 ± 5.0)vRND (25.0 ± 7.0) 5.970 0.000</strong>* MPC (15.2 ± 5.6)vNT (16.5 ± 4.2</p>
<p>. *** Sid, high) APC (33.5 ± 7.9)vFGS (41.7 ± 2.8) -6.883 0.000<strong><em> APC (33.5 ± 7.9)vMPC (41.0 ± 3.8) -6.046 0.000</em></strong> APC (33.5 ± 7.9)vNT (41.1 ± 5.0) -5.739 0.000<strong>* APC (33.5 ± 7.9)vRND (37.2 ± 6.1) -2.633 0.010</strong> FGS (41.7 ± 2.8)vMPC (41.0 ± 3.8) 1.023 0.309</p>            </div>
        </div>

    </div>
</body>
</html>
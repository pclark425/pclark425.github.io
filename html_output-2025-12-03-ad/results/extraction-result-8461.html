<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8461 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8461</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8461</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-04a343ffba0120ac02e3138ffd6d7b85f753846b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04a343ffba0120ac02e3138ffd6d7b85f753846b" target="_blank">Playing the Werewolf game with artificial intelligence for language understanding</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The results suggest that current language models have the capability to suspect what others are saying, tell a lie, or detect lies in conversations, as well as to interact with humans and choose the best voting target on the basis of its probability from the value network.</p>
                <p><strong>Paper Abstract:</strong> The Werewolf game is a social deduction game based on free natural language communication, in which players try to deceive others in order to survive. An important feature of this game is that a large portion of the conversations are false information, and the behavior of artificial intelligence (AI) in such a situation has not been widely investigated. The purpose of this study is to develop an AI agent that can play Werewolf through natural language conversations. First, we collected game logs from 15 human players. Next, we fine-tuned a Transformer-based pretrained language model to construct a value network that can predict a posterior probability of winning a game at any given phase of the game and given a candidate for the next action. We then developed an AI agent that can interact with humans and choose the best voting target on the basis of its probability from the value network. Lastly, we evaluated the performance of the agent by having it actually play the game with human players. We found that our AI agent, Deep Wolf, could play Werewolf as competitively as average human players in a villager or a betrayer role, whereas Deep Wolf was inferior to human players in a werewolf or a seer role. These results suggest that current language models have the capability to suspect what others are saying, tell a lie, or detect lies in conversations.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8461.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8461.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Wolf</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Wolf (Longformer-based Werewolf agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based value-network agent that plays five-player text-chat Werewolf by scoring candidate utterances/votes with a fine-tuned Longformer value network over the conversation history and selecting the action with the highest predicted posterior win probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep Wolf</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep Wolf is an AI agent that plays text-chat Werewolf. It gathers a finite set of candidate actions (utterances, votes, divinations) mined from human logs, inputs each candidate together with the current intermediate game log into a fine-tuned Longformer value network to predict posterior win probability, and selects the candidate with the highest score. It was evaluated by playing with human players.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Longformer (allenai/longformer-base-4096)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A RoBERTa-like bidirectional transformer adapted for long documents (sparse attention) and pretrained on English web corpora; the paper used a ~140M-parameter Longformer variant (allenai/longformer-base-4096) and fine-tuned it as a value network.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Five-player text-chat Werewolf (social-deduction game)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A multi-agent, imperfect-information social-deduction text chat game (5 players: 1 werewolf, 2 villagers, 1 seer, 1 betrayer) involving free-form natural language conversation, deception, divination, and voting; challenges include remembering conversation history, detecting contradictions/false claims, and planning votes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working / contextual memory (long-context attention over conversation history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>No separate external memory module; memory is provided as the conversation history concatenated into the model input and consumed by Longformer's internal attention mechanism (sparse attention over up to max_input_length tokens). Additionally, the authors trained 20 independent value-network models specialized for each role/player-number combination to mitigate role/number confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Conversation history (intermediate game log) and each candidate next action are concatenated and fed as input to the fine-tuned Longformer value network, which uses its attention over that input to predict posterior win probability for the (state, action) pair; the agent selects the highest-scoring action.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Role-specific win rates when using the described Longformer-based value network: Werewolf 0.29 (29%), Seer 0.25 (25%), Betrayer 0.44 (44%), Villager 0.56 (56%) — reported in Table 3 as Deep Wolf's win rates (proportion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No formal ablation contrasting memory vs. no-memory inputs or comparing multiple memory architectures. The paper did report a pragmatic modification (training 20 separate value networks per role/player-number to handle role/number confusion) but did not compare memory strategies or perform controlled memory ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit external memory used; limitations reported include: model sometimes made mistakes about its current role and player number (leading to the workaround of 20 specialized networks), finite set of candidate utterances limits expressivity, inferior performance in werewolf and seer roles compared to humans, small training dataset (32 games → 3,840 augmented logs) likely limiting memory/generalization, and awkward conversational behavior. The paper did not evaluate alternative memory architectures or quantify sensitivity to context window size.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Recommendations in the paper relevant to memory usage: (1) visualize attention (e.g., BertViz) to interpret why the model chose actions and to potentially build explainable memory usage; (2) use larger language models to possibly improve performance; (3) consider a differentiable tokenizer and generator to search action space more efficiently than brute-force candidate scoring; (4) evaluate deeper prediction of other players' future actions (i.e., planning over future states) which could be combined with memory; (5) increase training data (more game logs) to improve the value network; (6) construct a language-specific Longformer (Japanese) to avoid translation artifacts. The paper concludes that feeding the conversation history into a transformer-based value network is sufficient to achieve human-competitive play in some roles (villager, betrayer) but insufficient for others (seer, werewolf) under their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Implementation details relevant to memory: max_input_length used 2048 tokens, batch size 1, hyperparameters shown in Table 1; inference time of value network ~10 seconds (considered acceptable for chat-based play); model was converted to ONNX and optimized for faster inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing the Werewolf game with artificial intelligence for language understanding', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Longformer: The long-document transformer <em>(Rating: 2)</em></li>
                <li>Human-like artificial intelligence using deep reinforcement learning for the werewolf game <em>(Rating: 2)</em></li>
                <li>Artificial-intelligence-based Werewolf <em>(Rating: 2)</em></li>
                <li>Automatically detecting player roles in Among Us <em>(Rating: 1)</em></li>
                <li>wolfbbsRoBERTa-large <em>(Rating: 1)</em></li>
                <li>BertViz: A tool for visualizing multihead self-attention in the BERT model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8461",
    "paper_id": "paper-04a343ffba0120ac02e3138ffd6d7b85f753846b",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "Deep Wolf",
            "name_full": "Deep Wolf (Longformer-based Werewolf agent)",
            "brief_description": "An LLM-based value-network agent that plays five-player text-chat Werewolf by scoring candidate utterances/votes with a fine-tuned Longformer value network over the conversation history and selecting the action with the highest predicted posterior win probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Deep Wolf",
            "agent_description": "Deep Wolf is an AI agent that plays text-chat Werewolf. It gathers a finite set of candidate actions (utterances, votes, divinations) mined from human logs, inputs each candidate together with the current intermediate game log into a fine-tuned Longformer value network to predict posterior win probability, and selects the candidate with the highest score. It was evaluated by playing with human players.",
            "llm_model_name": "Longformer (allenai/longformer-base-4096)",
            "llm_model_description": "A RoBERTa-like bidirectional transformer adapted for long documents (sparse attention) and pretrained on English web corpora; the paper used a ~140M-parameter Longformer variant (allenai/longformer-base-4096) and fine-tuned it as a value network.",
            "benchmark_name": "Five-player text-chat Werewolf (social-deduction game)",
            "benchmark_description": "A multi-agent, imperfect-information social-deduction text chat game (5 players: 1 werewolf, 2 villagers, 1 seer, 1 betrayer) involving free-form natural language conversation, deception, divination, and voting; challenges include remembering conversation history, detecting contradictions/false claims, and planning votes.",
            "memory_used": true,
            "memory_type": "working / contextual memory (long-context attention over conversation history)",
            "memory_architecture": "No separate external memory module; memory is provided as the conversation history concatenated into the model input and consumed by Longformer's internal attention mechanism (sparse attention over up to max_input_length tokens). Additionally, the authors trained 20 independent value-network models specialized for each role/player-number combination to mitigate role/number confusion.",
            "memory_integration_strategy": "Conversation history (intermediate game log) and each candidate next action are concatenated and fed as input to the fine-tuned Longformer value network, which uses its attention over that input to predict posterior win probability for the (state, action) pair; the agent selects the highest-scoring action.",
            "performance_with_memory": "Role-specific win rates when using the described Longformer-based value network: Werewolf 0.29 (29%), Seer 0.25 (25%), Betrayer 0.44 (44%), Villager 0.56 (56%) — reported in Table 3 as Deep Wolf's win rates (proportion).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No formal ablation contrasting memory vs. no-memory inputs or comparing multiple memory architectures. The paper did report a pragmatic modification (training 20 separate value networks per role/player-number to handle role/number confusion) but did not compare memory strategies or perform controlled memory ablations.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "No explicit external memory used; limitations reported include: model sometimes made mistakes about its current role and player number (leading to the workaround of 20 specialized networks), finite set of candidate utterances limits expressivity, inferior performance in werewolf and seer roles compared to humans, small training dataset (32 games → 3,840 augmented logs) likely limiting memory/generalization, and awkward conversational behavior. The paper did not evaluate alternative memory architectures or quantify sensitivity to context window size.",
            "recommendations_or_conclusions": "Recommendations in the paper relevant to memory usage: (1) visualize attention (e.g., BertViz) to interpret why the model chose actions and to potentially build explainable memory usage; (2) use larger language models to possibly improve performance; (3) consider a differentiable tokenizer and generator to search action space more efficiently than brute-force candidate scoring; (4) evaluate deeper prediction of other players' future actions (i.e., planning over future states) which could be combined with memory; (5) increase training data (more game logs) to improve the value network; (6) construct a language-specific Longformer (Japanese) to avoid translation artifacts. The paper concludes that feeding the conversation history into a transformer-based value network is sufficient to achieve human-competitive play in some roles (villager, betrayer) but insufficient for others (seer, werewolf) under their setup.",
            "additional_notes": "Implementation details relevant to memory: max_input_length used 2048 tokens, batch size 1, hyperparameters shown in Table 1; inference time of value network ~10 seconds (considered acceptable for chat-based play); model was converted to ONNX and optimized for faster inference.",
            "uuid": "e8461.0",
            "source_info": {
                "paper_title": "Playing the Werewolf game with artificial intelligence for language understanding",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Longformer: The long-document transformer",
            "rating": 2
        },
        {
            "paper_title": "Human-like artificial intelligence using deep reinforcement learning for the werewolf game",
            "rating": 2
        },
        {
            "paper_title": "Artificial-intelligence-based Werewolf",
            "rating": 2
        },
        {
            "paper_title": "Automatically detecting player roles in Among Us",
            "rating": 1
        },
        {
            "paper_title": "wolfbbsRoBERTa-large",
            "rating": 1
        },
        {
            "paper_title": "BertViz: A tool for visualizing multihead self-attention in the BERT model",
            "rating": 1
        }
    ],
    "cost": 0.0075622499999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Playing the Werewolf game with artificial intelligence for language understanding</h1>
<p>Hisaichi SHIBATA ${ }^{\mathrm{a}}$, Soichiro MIKI ${ }^{\mathrm{a}}$, Yuta NAKAMURA ${ }^{\mathrm{b}}$<br>${ }^{a}$ Department of Radiology, The University of Tokyo Hospital, 7-3-1 Hongo, Bunkyo, Tokyo 113-8655, Japan<br>${ }^{b}$ Division of Radiology and Biomedical Engineering, Graduate School of Medicine, The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8655, Japan</p>
<h4>Abstract</h4>
<p>The Werewolf game is a social deduction game based on free natural language communication, in which players try to deceive others in order to survive. An important feature of this game is that a large portion of the conversations are false information, and the behavior of artificial intelligence (AI) in such a situation has not been widely investigated. The purpose of this study is to develop an AI agent that can play Werewolf through natural language conversations. First, we collected game logs from 15 human players. Next, we fine-tuned a Transformer-based pretrained language model to construct a value network that can predict a posterior probability of winning a game at any given phase of the game and given a candidate for the next action. We then developed an AI agent that can interact with humans and choose the best voting target on the basis of its probability from the value network. Lastly, we evaluated the performance of the agent by having it actually play the game with human players. We found that our AI agent, Deep Wolf, could play Werewolf as competitively as average human players in a villager or a betrayer role, whereas Deep Wolf was inferior to human players in a werewolf or a seer role. These results suggest that current language models have the capability to suspect what others are saying, tell a lie, or detect lies in conversations.</p>
<p>Keywords: natural language processing, deep generative models, werewolf game</p>
<h1>1. Introduction</h1>
<p>Most current artificial intelligence (AI) systems learn and output data under the assumption that most of the data they process is correct. However, since real society is plagued by false information, it is helpful if AI can detect contradictions and false information. The Werewolf game, also known as Among Us, is a social deduction game based on free natural language communication (see Appendix A for details). In Werewolf, all the players (usually 5 to 10 ) are randomly assigned a role (e.g., villager, werewolf, betrayer, and seer), and they try to infer the roles of other players. Villagers and seers are on the villager side and try to expel werewolves by voting during the day. On the werewolf side, werewolves try to kill villagers at night, and betrayers try to disturb the villager side. Players must deceive other players in order to survive, and conversations in Werewolf inevitably contain a large amount of false information. To win in Werewolf, players need to remember the details of conversations, make hypotheses about other players' roles, and detect contradictions in them. In other words, all players deduce the necessary conditions that the claims of other players are true. Werewolf played on a text chat basis rather than in verbal conversations is also popular and is played online. Recent studies aim to apply AI mainly to text-chat-based Werewolf.</p>
<p>While it is difficult to develop AI that can detect all kinds of fakes in the real world, the relatively limited vocabulary and types of information handled in Werewolf have attracted attention as a research subject for AI that can detect false information. Here, we focused on Werewolf based on text chat.</p>
<p>The purpose of this study is to develop an AI agent that can understand natural language and play Werewolf. Such an AI agent should reasonably compute the statements that can effectively lead to the successful expulsion or killing of players of the opponent side. To establish such AI agent, we must develop an approximate algorithm to extract the optimum output $\hat{\boldsymbol{y}}$ constrained with an input context $\boldsymbol{x}$, i.e., $\hat{\boldsymbol{y}}=\operatorname{argmax}_{\boldsymbol{y}} p(\boldsymbol{y} \mid \boldsymbol{x})$, where $p$ is the posterior probability of winning the game.</p>
<p>Transformer-based language models [1, 2] can efficiently learn the conditional probability $p(\boldsymbol{y} \mid \boldsymbol{x})$ of a text. We fine-tune one of such models using chat log data of Werewolf, and we construct a value network that can predict the posterior probability of winning a game at any given game phase. We use this value network to realize an AI agent that can play Werewolf</p>
<p>with human players. Lastly, we evaluate the performance of the AI agent by actually having it play Werewolf with human players and gathering its win rate.</p>
<h1>2. Related Works</h1>
<p>There are many AI agents that can play Werewolf using a predefined protocol instead of natural languages. For example, Fukuda and Anada [3] realized an AI agent in which deep reinforcement learning is applied. Kimura and Ito [4] proposed a method for inferring player roles with a support vector machine. An exhaustive list of such protocol-based AI agents can be found on the website of International AI Werewolf Competition [5]. Several algorithms that can process a natural language have been proposed, most of which can infer player roles but cannot play Werewolf itself. For instance, Tuin and Rooijackers [6] applied a support vector machine to game logs of Werewolf to infer player roles. A RoBERTa [1] model trained with a large set of Werewolf logs has been published [7], but the authors did not establish a value network nor an AI agent that can actually play the game. To the best of our knowledge, no study has been published on AI agents that can directly understand a natural language and play Werewolf in it. In particular, to date, there has been no published study on statistical information for the win rates of AI agents.</p>
<h2>3. Methods</h2>
<p>In this study, we focused on Werewolf played by five players: two villagers, one seer, one betrayer, and one werewolf (see Appendix A for the details). Briefly, we first collected a dataset of game logs from the actual play history of human participants. Second, we constructed a value network that can predict the posterior probability of winning a game. Third, we developed an AI agent named Deep Wolf, which can play Werewolf in English on the basis of the value network. The details of each step are described below.</p>
<h3>3.1. Dataset Building</h3>
<p>Fifteen participants played text chat-based, five-player Werewolf without AI players. We used our newly developed web-based Werewolf platform. The participants used a natural language (Japanese), but their names were</p>
<p>masked and replaced with randomly assigned number-based pronouns (#1$# 5$ ), and they were instructed to refer to one another using the numbers. Logs from a total of 48 games were collected, and we calculated win rates for each player and each role. Due to memory constraints, we randomly picked 32 games for machine learning, 16 of which were won by the villager side. Next, we translated the game logs into English using a commercial machine translation platform, and then performed manual grammar correction and data cleansing (e.g., the normalization of symbol usage). Since Werewolf is an imperfect information game, we converted each "full" game log into five logs representing the viewpoint of each player by masking information that must be invisible, such as other players' roles. We further augmented each game log by swapping number-based pronouns of coplayers in all possible $4!=24$ patterns so that our AI agent could learn an immutable structure irrelevant to the replacement of player numbers. As a result, we obtained a dataset consisting of $32 \times 5 \times 4!=3,840$ game logs.</p>
<h1>3.2. Value Network</h1>
<p>We built a regression model that can calculate the posterior probability of winning a game given an intermediate log at any given game phase from the viewpoint of any player role. We adopted an open, Transformer-based model developed by Allen Institute for AI ${ }^{1}$. This is a RoBERTa-like bidirectional encoder using Longformer [2] pretrained with English web corpora. We fine-tuned the model with the aforementioned game logs. The loss function to be minimized was a cross-entropy function that contained only the posterior probability of winning the game. We provided the results of the games as labels in addition to the intermediate game logs. The network was trained to calculate the probability of winning given any intermediate log and a candidate for the next action (e.g., an utterance, a vote to expel or kill another player). Although previous studies have often built a model to explicitly infer the roles of other players, we did not build such a model, because we assumed such a capability of inferring roles would be implicitly integrated in our large language model.</p>
<p>During development, we noticed that our network sometimes made mistakes about its current role and player number. To work around this, we decided to build 20 independent value networks dedicated to each combina-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Hyperparameters to train the value network.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Content</th>
<th style="text-align: right;">Parameter</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">max_input_length</td>
<td style="text-align: right;">2,048</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: right;">$1 \times 10^{-5}$</td>
</tr>
</tbody>
</table>
<p>tion of four different roles and five different player numbers. In the following steps, one of the 20 networks was selected according to the AI agent's current role and player number.</p>
<p>In the training, we set hyperparameters of the value network (Longformer) as in Table. 1. To accelerate the inference of the value network, we converted it into the Open Neural Network Exchange (ONNX) format and then optimized (simplified) the model using onnxsim. Although it was possible to employ batch processing during the training, we decided to set the batch size to 1. Training and inference were performed on a single graphics processing unit (NVIDIA RTX-A6000). The training of the value network finished in 1.5 hours per epoch and required about 30 hours in total. The value network inference typically finished within 10 seconds, which was considered fast enough for the chat-based Werewolf.</p>
<h1>3.3. Deep Wolf</h1>
<p>The next step was to construct an AI agent that could infer the next optimal action to win the game. The possible actions (including utterances, divinations, and votes) were collected from the logs of humans playing the same role as the current AI role. The number of utterances in the collected logs was about 300 per role, but we refined these by deleting similar or duplicate sentences, and the number of possible utterances became about 100. The agent input these action candidates into the value network to evaluate the posterior probability of winning the game for each action, and the agent determined the next action that could maximize the chance of winning.</p>
<p>We set the actions of Deep Wolf as follows. Note that this Werewolf game with five players always finishes within two days. See Appendix B for a sample game log.</p>
<p>Table 2: Win rates of five human players in Werewolf game. N/A means that we cannot define the win rate since no competition was executed.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Werewolf</th>
<th style="text-align: left;">Seer</th>
<th style="text-align: left;">Betrayer</th>
<th style="text-align: left;">Villager</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Player 1</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Player 2</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">Player 3</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0.80</td>
</tr>
<tr>
<td style="text-align: left;">Player 4</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">Player 5</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.57</td>
</tr>
<tr>
<td style="text-align: left;">Player 6</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">Player 7</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">Player 8</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">Player 9</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">Player 10</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Player 11</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 12</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">Player 13</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">Player 14</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 15</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">0.41</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0.60</td>
</tr>
</tbody>
</table>
<ol>
<li>If all the agents except Deep Wolf emitted Over and finished their conversation, Deep Wolf also emits Over and finishes its conversation.</li>
<li>If different $k$ players except Deep Wolf uttered, and Deep Wolf did not utter, Deep Wolf begins to speak. In this study, we set $k=3$ for day 1 , and $k=1$ for day 2 .</li>
<li>Deep Wolf does not utter the same sentence in the same game.</li>
</ol>
<h1>4. Results</h1>
<h3>4.1. Competition among human players and training of the value network</h3>
<p>We show in Table 2 win rates in the Werewolf game of five human players.</p>
<h3>4.2. Competition among human players and Deep Wolf</h3>
<p>We show in Table 3 the win rates of four human players and one AI agent (Deep Wolf) in the Werewolf game.</p>
<p>Table 3: Win rates of four human players and one AI agent in Werewolf game. N/A means that we cannot define the win rate since no competition was executed. Player numbers in Table 2 and Table 3 do not necessarily represent the same player.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Werewolf</th>
<th style="text-align: left;">Seer</th>
<th style="text-align: left;">Betrayer</th>
<th style="text-align: left;">Villager</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Player 1</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Player 2</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 3</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Player 4</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.55</td>
</tr>
<tr>
<td style="text-align: left;">Player 5</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Player 6</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Player 7</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.40</td>
</tr>
<tr>
<td style="text-align: left;">Player 8</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 9</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">Player 10</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Player 11</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Average (Human)</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.45</td>
<td style="text-align: left;">0.55</td>
</tr>
<tr>
<td style="text-align: left;">Deep Wolf</td>
<td style="text-align: left;">0.29</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">0.56</td>
</tr>
</tbody>
</table>
<h1>5. Discussion</h1>
<h3>5.1. Behavior of the value network</h3>
<p>When the role of the value network was a werewolf, the posterior probability was almost always low in the early stages of the game. On the other hand, when the role of the value network was a seer, the posterior probability was almost always high in the early stages of the game. This suggests that the network judged the villager side had an advantage in this five-player game setting, which was consistent with the tendency of the actual game results. This demonstrates an ability of the value network to correctly estimate the probability of winning, even though it was trained with a dataset where the overall win rates were 0.5 for both the werewolf and villager sides.</p>
<h3>5.2. Performance of Deep Wolf</h3>
<p>By comparing the win rates in Table 3, we found that the average win rates of the human betrayer and villager are not significantly different from the win rates of the Deep Wolf betrayer and villager. This suggests that Deep Wolf can play the game almost equally to human players if its role</p>
<p>is the betrayer or villager. On the other hand, for the seer and werewolf, Deep Wolf was significantly inferior to average human players. However, it is interesting that there are cases when Deep Wolf can win over human players even if its role is the werewolf.</p>
<h1>5.3. Novelty</h1>
<p>We, for the first time, established an AI agent that can play Werewolf by understanding a natural language with human players. We observed that the behavior of the AI agent was sometimes awkward, but we confirmed that the AI agent can win the game against human players with a certain probability.</p>
<h3>5.4. Limitation</h3>
<p>The AI agent we have constructed chooses its next utterance from a large but finite set of actions gathered from the log of games played by humans. Additionally, it would be necessary to improve the model in order for humans to be satisfied with the content of the game that includes AI participants.</p>
<h3>5.5. Future Works</h3>
<p>By using a system that can visualize the weight of attention, e.g., BertViz [8], we may be able to construct an AI agent that can explain the reason for choosing a particular action or utterance. Although we were unable to obtain an appropriate pretrained Japanese language model for Longformer, the construction of a value network that can handle the Japanese language will be included in our future works. Although we employed a model with about 140 million parameters, there is a possibility that the win rates of the AI agent can be improved by employing a language model with a larger number of parameters. In this study, we used a brute force approach to choose the next utterance, but the introduction of a differentiable tokenizer and a generator may allow us to efficiently search for the optimal solution. In this study, only the player's own actions were evaluated using the value network, but it may be possible to improve the win rate of the AI agent by deeply predicting the future actions of other players. In a game involving AI agents, in addition to the task of inferring the other player's role, there would be an additional task of inferring whether each player is an AI agent or a human. It would be an interesting topic to evaluate the changes in the strategy used by human players in such situations. Although only one AI agent at a time was used in this study, we believe that eventually, it will be possible to play Werewolf with only AI agents and entertain the game's</p>
<p>audience. We trained and evaluated Deep Wolf on the basis of only 32 game logs, but we have obtained about 100 game logs so far, and we will consider retraining the AI agent because we believe that increasing the amount of training data will improve its performance.</p>
<p>In this experiment on evaluating the performance of the AI agents, we did not measure the degree of fatigue of the human participants. Since the concept of fatigue does not exist for AI agents, and AI agents are guaranteed to always be able to play the game at a certain strength unless the program is changed, this implies that a human player's winning rate can determine the degree of fatigue of the human player. Evaluating the degree of fatigue through games is included in future works.</p>
<h1>6. Conclusion</h1>
<p>By fine-tuning a Transformer-based model, Longformer, for natural language processing, we developed an AI agent (Deep Wolf) that can play Werewolf through natural language conversations. We found that Deep Wolf could play Werewolf with a performance comparable to average human players when its role was a villager or a betrayer, but Deep Wolf was inferior to human players when its role was a werewolf or a seer. These results suggest that current language models have the capability to understand statements, tell a lie, or detect lies in conversations.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by JST, CREST Grant Number JPMJCR21M2, including the AIP challenge program (Necessary conditions that personal claims are objective facts), Japan. We thank many participants who played Werewolf again and again to generate game logs and evaluate Deep Wolf.</p>
<h2>Appendix A. Preliminary for Werewolf game</h2>
<p>Werewolf game adopted in this project is a five-player game and will largely conform to the regulations of the Natural Language Division of the 4th International Werewolf Intelligence Convention. One of the five players is a werewolf, two are villagers, one is a seer (diviner), and the remaining one is a betrayer. Players can only know their own role. The other players' roles are not known. However, the seer can divine the role of any participant by divination. However, divination can be done only once a day. The</p>
<p>results of divination during the daytime are shown only to the seer the next morning, so there is a possibility of being targeted by werewolves or suspected by villagers if the results are not well communicated to others. The betrayer poses as a villager (and is determined to be a villager when divined by the seer), but the betrayer is on the werewolf side and manipulates information to the advantage of the werewolves. The game is played through text conversations and voting to determine who is eliminated. In addition, the surviving werewolves at night decide who to attack and who to eliminate. The game begins at noon on Day 0 . Depending on the rules, all participants may introduce themselves and greet each other at noon on Day 0 , but in this case, the diviner only decides who to divine. The diviner may also announce himself/herself (e.g., "I am a diviner. I divined #1 and he was a werewolf". This is called the diviner coming out.) In some cases, a betrayer or werewolf may come forward as a diviner, lying to confuse the players. The villager side wins when all werewolves are eliminated, and the werewolf side wins when the number of werewolves equals that of villagers; these conditions are automatically applied in the game.</p>
<h1>Appendix B. Example of Competition with Deep Wolf</h1>
<p>We show a log for the Werewolf game when the role of Deep Wolf was the werewolf (werewolf side) and Deep Wolf won the game. We recognized a description, which is not suitable for publication, in the log; hence, we replaced it with other words.</p>
<ul>
<li>#1 divined #2 and #2 is not a werewolf.</li>
<li>#4) Good morning. I am a villager.</li>
<li>#1) I am the diviner. #2 is clean.</li>
<li>#4) Hello! I'm a villager.</li>
<li>#2) Hi, I am a villager!</li>
<li>#3) I have a feeling that's the case, shall we hang suspicious #5? I'm sorry if he's one of them!</li>
<li>#2) &gt; #1 Thanks.</li>
<li>
<p>#5) I must be innocent and not suspect.</p>
</li>
<li>
<p>#5) #1 seems to be the real fortune teller.</p>
</li>
<li>#1) Yes. And #2 is not a werewolf.</li>
<li>#2) But there may be two diviners.</li>
<li>#5) I agree</li>
<li>#3) Hello, I am a villager.</li>
<li>#3) I don't think I would attack a diviner with a large number of people.</li>
<li>#2) So #1 must be a diviner.</li>
<li>#2) But the werewolf will kill #1 tonight.</li>
<li>#1) That is not a problem.</li>
<li>#4) Then we should choose #3, #4 or #5 to expel?</li>
<li>#1) Yes!</li>
<li>#2) The werewolf pretends to be a villager.</li>
<li>#3) Well, it is one strategy to leave the betrayal-like #2 as betrayal and hang the other grays at random.</li>
<li>#3) I will vote for #1.</li>
<li>#1) #3 may be a betrayer.</li>
<li>#3) I am a villager. Pleased to meet you.</li>
<li>#1) Oh, I made a mistake. Sorry.</li>
<li>#1) I will vote #3, #4 or #5.</li>
<li>#4) I think #3 is suspicious.</li>
<li>#2) $&gt;# 4$, but I may be a betrayer.Why do not you think I am not a betrayer?</li>
<li>
<p>#3) Well, it is one strategy to leave the betrayal-like #5 as betrayal and hang the other grays at random.</p>
</li>
<li>
<p>#4) #4, you're right. Sorry I couldn't think that much.</p>
</li>
<li>#2) I will vote for #4. #4 seems to rush to conclusion.</li>
<li>#1) I feel #2 is a betrayer. #4 is not suspicious.</li>
<li>#2) I am a villager.</li>
<li>#3) I am also a villager, but I wonder if the later #1 are suspicious...?</li>
<li>#2) If I were a betrayer, I would not say such a thing.</li>
<li>#2) Over.</li>
<li>#4) Over.</li>
<li>#5) Over.</li>
<li>#3) Over.</li>
<li>#1) Over.</li>
<li>#1 voted for #2.</li>
<li>#4 voted for #3.</li>
<li>#3 voted for #5.</li>
<li>#5 voted for #4.</li>
<li>#2 voted for #4.</li>
<li>#4 has been erased.</li>
<li>The werewolf erased #2.</li>
<li>#1 divined #3 and #3 is the werewolf.</li>
<li>#3) Over.</li>
<li>#1) #3 is a werewolf.</li>
<li>#5) I am a traitor. Let's do a power play, #3.</li>
<li>
<p>#5) Over.</p>
</li>
<li>
<p>#1) #5, are you a villager?</p>
</li>
<li>#1) ok</li>
<li>#1) Over.</li>
<li>#1 voted for #3.</li>
<li>#3 voted for #1.</li>
<li>#5 voted for #1.</li>
<li>#1 has been erased.</li>
<li>The werewolf erased #5.
#1 was a seer, #2 and #4 are villagers, #3 was a werewolf (Deep Wolf), and #5 was a betrayer.</li>
</ul>
<h1>References</h1>
<p>[1] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, RoBERTa: A robustly optimized BERT pretraining approach, arXiv preprint arXiv:1907.11692 (2019).
[2] I. Beltagy, M. E. Peters, A. Cohan, Longformer: The long-document transformer, arXiv preprint arXiv:2004.05150 (2020).
[3] M. Fukuda, H. Anada, Human-like artificial intelligence using deep reinforcement learning for the werewolf game, Special Interest Group on Society and Artificial Intelligence (SIG-SAI) (2015).
[4] Y. Kimura, T. Ito, Improvement of role estimation using machine learning in Werewolf AI, GAT2018 (2018).
[5] Artificial-intelligence-based Werewolf, http://aiwolf.org/ (2015).
[6] H. Tuin, M. Rooijackers, Automatically detecting player roles in Among Us, in: 2021 IEEE Conference on Games (CoG), IEEE, 2021, pp. 1-5.
[7] I. Tsunoda, wolfbbsroberta-large, https://huggingface.co/itsunoda/wolfbbsRoBERTa-large (2021).</p>
<p>[8] J. Vig, BertViz: A tool for visualizing multihead self-attention in the BERT model, in: ICLR Workshop: Debugging Machine Learning Models, 2019 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/allenai/longformer-base-4096&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1061 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1061</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1061</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-9e9b55142f7c353fa262047dcea77685668213e6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9e9b55142f7c353fa262047dcea77685668213e6" target="_blank">Automatic Curriculum Learning through Value Disagreement</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy, and samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement.</p>
                <p><strong>Paper Abstract:</strong> Continually solving new, unsolved tasks is the key to learning diverse behaviors. Through reinforcement learning (RL), we have made massive strides towards solving tasks that have a single goal. However, in the multi-task domain, where an agent needs to reach multiple goals, the choice of training goals can largely affect sample efficiency. When biological agents learn, there is often an organized and meaningful order to which learning happens. Inspired by this, we propose setting up an automatic curriculum for goals that the agent needs to solve. Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a significantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy. This simple technique samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement. We evaluate our method across 13 multi-goal robotic tasks and 5 navigation tasks, and demonstrate performance gains over current state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1061.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1061.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fetch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fetch 7-DoF robotic arm (OpenAI Gym Fetch environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 7-degree-of-freedom robotic arm used for goal-conditioned manipulation tasks (reaching, pushing, picking and placing, sliding) evaluated under sparse rewards; trained with goal-conditioned RL (DDPG) combined with HER and the Value Disagreement Sampling (VDS) goal proposer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Curriculum Learning through Value Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Fetch arm (7-DoF)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated robotic manipulator controlled by a goal-conditioned deterministic policy trained with off-policy RL (DDPG) and Hindsight Experience Replay (HER); goal selection for training is modulated by Value Disagreement Sampling (VDS) through an ensemble of Q-functions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Fetch manipulation suite (FetchReach, FetchPickAndPlace, FetchPush, FetchSlide)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3D manipulation tasks simulated in OpenAI Gym Fetch suite; tasks range from moving the gripper (reach) to interacting with objects (pick-and-place, push, slide). Sparse binary reward: 0 if final object/gripper position is within epsilon of the goal, -1 otherwise. Complexity arises from multi-step primitives (e.g., grasp then move), object interaction dynamics, and variable initial object/arm configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task type (reach vs. object manipulation), DoF of robot (7-DoF), observation dimensionality (reach: 10-dim; object tasks: 25-dim), presence of multi-step primitives (grasp + move), sparse reward signal (success within epsilon).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation across goal parameter space G (3-dimensional goal positions), randomized initial object/arm positions (not guaranteed to be close to object), multiple task types (4 Fetch tasks); goals sampled from goal space during training.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (evaluated with latest policy; episode success if goal reached at final timestep) and trajectory return; sample efficiency (rate of increase in success over environment interactions) compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: VDS + HER + DDPG yields higher sample efficiency and faster rise in success rate than HER alone and other baselines on many Fetch tasks (figures show significant improvement), exact numeric success rates not reported in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly argues that goal difficulty (complexity) and goal distribution (variation) interact: goals that are too easy yield little learning signal, goals that are too hard yield near-zero reward; VDS targets goals at the 'knowledge frontier' (intermediate difficulty, high epistemic uncertainty) to balance complexity and variation and maximize informative experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via Value Disagreement Sampling (VDS) to propose goals, combined with off-policy RL (DDPG) and Hindsight Experience Replay (HER).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not explicitly tested for generalization to unseen environment instances; evaluation performed across multiple held-out evaluation goals sampled randomly but no explicit transfer/generalization experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency compared to HER and other baselines (learning curves show faster success-rate gains); exact counts of environment interactions to convergence are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>On Fetch manipulation tasks, VDS focuses sampling on goals at the frontier of the agent's capability, producing more informative training goals and improving sample efficiency over uniform sampling (HER) and several curriculum baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning through Value Disagreement', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1061.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1061.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hand</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>24-DoF dexterous Hand (OpenAI Gym HandManipulate suite)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-DoF simulated robotic hand performing in-hand object manipulation tasks (rotate/position objects) with sparse rewards, trained with DDPG+HER and VDS to generate curricula over goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Curriculum Learning through Value Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dexterous Hand (24-DoF)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated 24-degree-of-freedom robotic hand controlled by a goal-conditioned deterministic policy trained with off-policy RL (DDPG) and HER; VDS uses an ensemble of Q-functions to prioritize goals with high epistemic uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HandManipulate suite (Block, Egg, Pen tasks and variants; HandReach)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-dimensional manipulation tasks (rotate objects to target rotations and positions) with sparse episodic rewards; action space is 20-dim, observation spaces up to 63-dim, goal spaces vary (e.g., 7-dim for some manipulation tasks). Complexity arises from coupled high-DoF control, complex contact dynamics, and sparse success signal.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Degrees of freedom (24-DoF hand), action dimensionality (20), observation dimensionality (15–63), goal dimensionality (up to 15 for reach; 7 for manipulation), multi-contact, and sparse reward structure.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation across multiple manipulation tasks (9 HandManipulation tasks reported), differing goal parameterizations (position and/or rotation targets), and randomized instance conditions; goal sampling across G during training.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (final-step success) and sample efficiency (learning curves across environment interactions); measured across random seeds (confidence interval over 5 seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: VDS yields substantial improvements in sample efficiency on many of the Hand manipulation tasks relative to HER and other baselines; no explicit numeric success rates provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the authors emphasize that for high-complexity tasks (dexterous manipulation) sampling intermediate-difficulty goals (high epistemic uncertainty) is critical; VDS shifts sampling toward frontier goals, enabling progressive increase in task difficulty and better learning despite high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via VDS to sample frontier goals, combined with HER and DDPG; ensemble Q-functions estimate epistemic uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No explicit held-out generalization tests reported; experiments cover many manipulation tasks showing robustness across tasks but not explicit transfer to unseen domains.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Demonstrated faster learning (higher success earlier) across many Hand tasks compared to baselines; exact interaction counts or convergence times not stated in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VDS significantly improves sample efficiency on high-dimensional dexterous manipulation tasks by prioritizing goals at the knowledge frontier; the method is robust to ensemble size and disagreement function choice, and complements HER.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning through Value Disagreement', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1061.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1061.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Maze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2D Maze navigation environments (MazeA, MazeB, MazeC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Low-dimensional 2D navigation mazes with obstacles and sparse rewards used to visualize and analyze VDS behavior; agents are trained with DDPG+HER while VDS proposes goals concentrated at the learning frontier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Curriculum Learning through Value Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Maze navigation agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated 2D navigation agent (velocity and direction control) trained with off-policy RL (DDPG) and VDS; environment provides sparse reward for reaching target positions and has obstructing blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MazeA, MazeB, MazeC (finite 2D mazes with blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D mazes with randomly placed blocks or structured obstacles; agent observes 2-D coordinates and controls velocity/direction; MazeC includes a central infeasible area blocked on all sides. Episodes max length 50 timesteps. Complexity comes from obstacle layouts and path planning under sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Maze topology (presence and arrangement of blocks), feasibility regions (e.g., central infeasible area in MazeC), horizon length (max 50 timesteps), 2-D continuous state/action space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low to medium (task complexity depends on maze layout; MazeC higher due to infeasible region)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Multiple maze instances/variants (3 mazes), random block layouts in MazeA, varying goal positions across continuous 2D goal space; goal set sampling used to approximate C^pi.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (final-step goal reach) and visualization of Q-value landscape, trajectory returns, and sampled goal distributions over training iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: VDS produces goal-sampling concentrated on frontier regions and leads to faster expansion of successful regions in Q-value/reward landscapes; quantitative curves show improvements over baselines but precise numeric values not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper uses Maze experiments to explicitly demonstrate that VDS concentrates samples at the boundary between solved and unsolved regions (high epistemic uncertainty), showing that increasing variation toward frontier goals helps expand the mastered region; demonstrates trade-off where uniform sampling is suboptimal because many goals are either too easy or too hard.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via VDS sampling goals from ensemble disagreement; HER is used in most experiments though Maze visualizations focus on illustrating VDS behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No explicit generalization-to-new-mazes experiments reported; analysis centers on how sampled goals shift over training to harder regions within each maze.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>VDS accelerates expansion of the policy's successful region in the maze (faster improvement in success rate and Q-value landscape); exact interaction counts are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Maze experiments provide clear empirical evidence that sampling goals with high value-ensemble disagreement concentrates learning on the frontier of capability, producing an effective automatic curriculum and improving sample efficiency relative to uniform sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning through Value Disagreement', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1061.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1061.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ant-nav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ant navigation environments (from GoalGAN comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated Ant locomotion navigation tasks used to compare VDS against GoalGAN and other curriculum methods; agents trained with fixed-length trajectories and episodic success measured at final timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Curriculum Learning through Value Disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ant navigation agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated quadruped 'Ant' agent used for navigation tasks; trained using goal-conditioned RL (DDPG or SAC variants in baselines) with the VDS goal proposer to prioritize frontier goals; compared against GoalGAN and other curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ant navigation environments (two tasks borrowed from GoalGAN paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Navigation tasks for an Ant-like agent where the goal is a position to reach; evaluation differs from GoalGAN's original setup in that VDS/HER use fixed-length episodes and success at final timestep. Complexity arises from locomotion control and environment traversal; tasks test curriculum efficacy for locomotion.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Agent locomotion complexity (Ant dynamics), continuous goal space, episode termination rules (fixed-length trajectories), and sparse episodic reward for reaching goal at final timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Multiple goal instances in continuous goal space; comparison to GoalGAN which generates intermediate-difficulty goals via adversarial training; VDS samples goals via epistemic uncertainty across ensemble Q-functions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate and sample efficiency (learning curves); also qualitative comparisons to GoalGAN learning curves reported in original GoalGAN paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: VDS obtains significant sample efficiency gains compared to GoalGAN on the two Ant navigation environments under the paper's fixed-episode evaluation protocol; exact numeric performance values are not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicitly discussed — VDS outperforms adversarial goal generators (GoalGAN) in these settings by sampling frontier goals defined via value disagreement, suggesting that epistemic-uncertainty-driven variation selection can be more sample-efficient than adversarial curriculum approaches for locomotion.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via VDS with DDPG (or SAC for some baselines); comparison against GoalGAN adversarial curriculum and other automated curriculum methods.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>No explicit generalization tests reported; comparisons focus on sample efficiency in the same environment configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported as significantly improved relative to GoalGAN and several other baselines in the two Ant tasks under the paper's evaluation protocol; no exact numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VDS provides more sample-efficient curricula than GoalGAN in these Ant navigation tasks under fixed-length episode evaluation, likely because value-ensemble disagreement directly identifies frontier goals without requiring adversarial training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning through Value Disagreement', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hindsight experience replay <em>(Rating: 2)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>R-iac: Robust intrinsically motivated exploration and active learning <em>(Rating: 2)</em></li>
                <li>Intrinsic motivation and automatic curricula via asymmetric self-play <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1061",
    "paper_id": "paper-9e9b55142f7c353fa262047dcea77685668213e6",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Fetch",
            "name_full": "Fetch 7-DoF robotic arm (OpenAI Gym Fetch environments)",
            "brief_description": "A simulated 7-degree-of-freedom robotic arm used for goal-conditioned manipulation tasks (reaching, pushing, picking and placing, sliding) evaluated under sparse rewards; trained with goal-conditioned RL (DDPG) combined with HER and the Value Disagreement Sampling (VDS) goal proposer.",
            "citation_title": "Automatic Curriculum Learning through Value Disagreement",
            "mention_or_use": "use",
            "agent_name": "Fetch arm (7-DoF)",
            "agent_description": "A simulated robotic manipulator controlled by a goal-conditioned deterministic policy trained with off-policy RL (DDPG) and Hindsight Experience Replay (HER); goal selection for training is modulated by Value Disagreement Sampling (VDS) through an ensemble of Q-functions.",
            "agent_type": "simulated robotic agent",
            "environment_name": "Fetch manipulation suite (FetchReach, FetchPickAndPlace, FetchPush, FetchSlide)",
            "environment_description": "3D manipulation tasks simulated in OpenAI Gym Fetch suite; tasks range from moving the gripper (reach) to interacting with objects (pick-and-place, push, slide). Sparse binary reward: 0 if final object/gripper position is within epsilon of the goal, -1 otherwise. Complexity arises from multi-step primitives (e.g., grasp then move), object interaction dynamics, and variable initial object/arm configurations.",
            "complexity_measure": "Task type (reach vs. object manipulation), DoF of robot (7-DoF), observation dimensionality (reach: 10-dim; object tasks: 25-dim), presence of multi-step primitives (grasp + move), sparse reward signal (success within epsilon).",
            "complexity_level": "medium-high",
            "variation_measure": "Variation across goal parameter space G (3-dimensional goal positions), randomized initial object/arm positions (not guaranteed to be close to object), multiple task types (4 Fetch tasks); goals sampled from goal space during training.",
            "variation_level": "medium",
            "performance_metric": "Success rate (evaluated with latest policy; episode success if goal reached at final timestep) and trajectory return; sample efficiency (rate of increase in success over environment interactions) compared to baselines.",
            "performance_value": "Qualitative: VDS + HER + DDPG yields higher sample efficiency and faster rise in success rate than HER alone and other baselines on many Fetch tasks (figures show significant improvement), exact numeric success rates not reported in-text.",
            "complexity_variation_relationship": "Yes — the paper explicitly argues that goal difficulty (complexity) and goal distribution (variation) interact: goals that are too easy yield little learning signal, goals that are too hard yield near-zero reward; VDS targets goals at the 'knowledge frontier' (intermediate difficulty, high epistemic uncertainty) to balance complexity and variation and maximize informative experiences.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via Value Disagreement Sampling (VDS) to propose goals, combined with off-policy RL (DDPG) and Hindsight Experience Replay (HER).",
            "generalization_tested": false,
            "generalization_results": "Not explicitly tested for generalization to unseen environment instances; evaluation performed across multiple held-out evaluation goals sampled randomly but no explicit transfer/generalization experiments reported.",
            "sample_efficiency": "Improved sample efficiency compared to HER and other baselines (learning curves show faster success-rate gains); exact counts of environment interactions to convergence are not provided in the text.",
            "key_findings": "On Fetch manipulation tasks, VDS focuses sampling on goals at the frontier of the agent's capability, producing more informative training goals and improving sample efficiency over uniform sampling (HER) and several curriculum baselines.",
            "uuid": "e1061.0",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning through Value Disagreement",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Hand",
            "name_full": "24-DoF dexterous Hand (OpenAI Gym HandManipulate suite)",
            "brief_description": "A high-DoF simulated robotic hand performing in-hand object manipulation tasks (rotate/position objects) with sparse rewards, trained with DDPG+HER and VDS to generate curricula over goals.",
            "citation_title": "Automatic Curriculum Learning through Value Disagreement",
            "mention_or_use": "use",
            "agent_name": "Dexterous Hand (24-DoF)",
            "agent_description": "A simulated 24-degree-of-freedom robotic hand controlled by a goal-conditioned deterministic policy trained with off-policy RL (DDPG) and HER; VDS uses an ensemble of Q-functions to prioritize goals with high epistemic uncertainty.",
            "agent_type": "simulated robotic agent",
            "environment_name": "HandManipulate suite (Block, Egg, Pen tasks and variants; HandReach)",
            "environment_description": "High-dimensional manipulation tasks (rotate objects to target rotations and positions) with sparse episodic rewards; action space is 20-dim, observation spaces up to 63-dim, goal spaces vary (e.g., 7-dim for some manipulation tasks). Complexity arises from coupled high-DoF control, complex contact dynamics, and sparse success signal.",
            "complexity_measure": "Degrees of freedom (24-DoF hand), action dimensionality (20), observation dimensionality (15–63), goal dimensionality (up to 15 for reach; 7 for manipulation), multi-contact, and sparse reward structure.",
            "complexity_level": "high",
            "variation_measure": "Variation across multiple manipulation tasks (9 HandManipulation tasks reported), differing goal parameterizations (position and/or rotation targets), and randomized instance conditions; goal sampling across G during training.",
            "variation_level": "high",
            "performance_metric": "Success rate (final-step success) and sample efficiency (learning curves across environment interactions); measured across random seeds (confidence interval over 5 seeds).",
            "performance_value": "Qualitative: VDS yields substantial improvements in sample efficiency on many of the Hand manipulation tasks relative to HER and other baselines; no explicit numeric success rates provided in main text.",
            "complexity_variation_relationship": "Yes — the authors emphasize that for high-complexity tasks (dexterous manipulation) sampling intermediate-difficulty goals (high epistemic uncertainty) is critical; VDS shifts sampling toward frontier goals, enabling progressive increase in task difficulty and better learning despite high variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via VDS to sample frontier goals, combined with HER and DDPG; ensemble Q-functions estimate epistemic uncertainty.",
            "generalization_tested": false,
            "generalization_results": "No explicit held-out generalization tests reported; experiments cover many manipulation tasks showing robustness across tasks but not explicit transfer to unseen domains.",
            "sample_efficiency": "Demonstrated faster learning (higher success earlier) across many Hand tasks compared to baselines; exact interaction counts or convergence times not stated in-text.",
            "key_findings": "VDS significantly improves sample efficiency on high-dimensional dexterous manipulation tasks by prioritizing goals at the knowledge frontier; the method is robust to ensemble size and disagreement function choice, and complements HER.",
            "uuid": "e1061.1",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning through Value Disagreement",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Maze",
            "name_full": "2D Maze navigation environments (MazeA, MazeB, MazeC)",
            "brief_description": "Low-dimensional 2D navigation mazes with obstacles and sparse rewards used to visualize and analyze VDS behavior; agents are trained with DDPG+HER while VDS proposes goals concentrated at the learning frontier.",
            "citation_title": "Automatic Curriculum Learning through Value Disagreement",
            "mention_or_use": "use",
            "agent_name": "Maze navigation agent",
            "agent_description": "A simulated 2D navigation agent (velocity and direction control) trained with off-policy RL (DDPG) and VDS; environment provides sparse reward for reaching target positions and has obstructing blocks.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "MazeA, MazeB, MazeC (finite 2D mazes with blocks)",
            "environment_description": "2D mazes with randomly placed blocks or structured obstacles; agent observes 2-D coordinates and controls velocity/direction; MazeC includes a central infeasible area blocked on all sides. Episodes max length 50 timesteps. Complexity comes from obstacle layouts and path planning under sparse rewards.",
            "complexity_measure": "Maze topology (presence and arrangement of blocks), feasibility regions (e.g., central infeasible area in MazeC), horizon length (max 50 timesteps), 2-D continuous state/action space.",
            "complexity_level": "low to medium (task complexity depends on maze layout; MazeC higher due to infeasible region)",
            "variation_measure": "Multiple maze instances/variants (3 mazes), random block layouts in MazeA, varying goal positions across continuous 2D goal space; goal set sampling used to approximate C^pi.",
            "variation_level": "medium",
            "performance_metric": "Success rate (final-step goal reach) and visualization of Q-value landscape, trajectory returns, and sampled goal distributions over training iterations.",
            "performance_value": "Qualitative: VDS produces goal-sampling concentrated on frontier regions and leads to faster expansion of successful regions in Q-value/reward landscapes; quantitative curves show improvements over baselines but precise numeric values not provided in-text.",
            "complexity_variation_relationship": "Yes — the paper uses Maze experiments to explicitly demonstrate that VDS concentrates samples at the boundary between solved and unsolved regions (high epistemic uncertainty), showing that increasing variation toward frontier goals helps expand the mastered region; demonstrates trade-off where uniform sampling is suboptimal because many goals are either too easy or too hard.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via VDS sampling goals from ensemble disagreement; HER is used in most experiments though Maze visualizations focus on illustrating VDS behavior.",
            "generalization_tested": false,
            "generalization_results": "No explicit generalization-to-new-mazes experiments reported; analysis centers on how sampled goals shift over training to harder regions within each maze.",
            "sample_efficiency": "VDS accelerates expansion of the policy's successful region in the maze (faster improvement in success rate and Q-value landscape); exact interaction counts are not reported.",
            "key_findings": "Maze experiments provide clear empirical evidence that sampling goals with high value-ensemble disagreement concentrates learning on the frontier of capability, producing an effective automatic curriculum and improving sample efficiency relative to uniform sampling.",
            "uuid": "e1061.2",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning through Value Disagreement",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Ant-nav",
            "name_full": "Ant navigation environments (from GoalGAN comparisons)",
            "brief_description": "Simulated Ant locomotion navigation tasks used to compare VDS against GoalGAN and other curriculum methods; agents trained with fixed-length trajectories and episodic success measured at final timestep.",
            "citation_title": "Automatic Curriculum Learning through Value Disagreement",
            "mention_or_use": "use",
            "agent_name": "Ant navigation agent",
            "agent_description": "A simulated quadruped 'Ant' agent used for navigation tasks; trained using goal-conditioned RL (DDPG or SAC variants in baselines) with the VDS goal proposer to prioritize frontier goals; compared against GoalGAN and other curricula.",
            "agent_type": "simulated robotic agent",
            "environment_name": "Ant navigation environments (two tasks borrowed from GoalGAN paper)",
            "environment_description": "Navigation tasks for an Ant-like agent where the goal is a position to reach; evaluation differs from GoalGAN's original setup in that VDS/HER use fixed-length episodes and success at final timestep. Complexity arises from locomotion control and environment traversal; tasks test curriculum efficacy for locomotion.",
            "complexity_measure": "Agent locomotion complexity (Ant dynamics), continuous goal space, episode termination rules (fixed-length trajectories), and sparse episodic reward for reaching goal at final timestep.",
            "complexity_level": "medium",
            "variation_measure": "Multiple goal instances in continuous goal space; comparison to GoalGAN which generates intermediate-difficulty goals via adversarial training; VDS samples goals via epistemic uncertainty across ensemble Q-functions.",
            "variation_level": "medium",
            "performance_metric": "Success rate and sample efficiency (learning curves); also qualitative comparisons to GoalGAN learning curves reported in original GoalGAN paper.",
            "performance_value": "Qualitative: VDS obtains significant sample efficiency gains compared to GoalGAN on the two Ant navigation environments under the paper's fixed-episode evaluation protocol; exact numeric performance values are not provided in-text.",
            "complexity_variation_relationship": "Implicitly discussed — VDS outperforms adversarial goal generators (GoalGAN) in these settings by sampling frontier goals defined via value disagreement, suggesting that epistemic-uncertainty-driven variation selection can be more sample-efficient than adversarial curriculum approaches for locomotion.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via VDS with DDPG (or SAC for some baselines); comparison against GoalGAN adversarial curriculum and other automated curriculum methods.",
            "generalization_tested": false,
            "generalization_results": "No explicit generalization tests reported; comparisons focus on sample efficiency in the same environment configurations.",
            "sample_efficiency": "Reported as significantly improved relative to GoalGAN and several other baselines in the two Ant tasks under the paper's evaluation protocol; no exact numbers provided.",
            "key_findings": "VDS provides more sample-efficient curricula than GoalGAN in these Ant navigation tasks under fixed-length episode evaluation, likely because value-ensemble disagreement directly identifies frontier goals without requiring adversarial training.",
            "uuid": "e1061.3",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning through Value Disagreement",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hindsight experience replay",
            "rating": 2
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "R-iac: Robust intrinsically motivated exploration and active learning",
            "rating": 2
        },
        {
            "paper_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
            "rating": 1
        }
    ],
    "cost": 0.013131999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automatic Curriculum Learning through Value Disagreement</h1>
<p>Yunzhi Zhang<br>UC Berkeley</p>
<p>Pieter Abbeel<br>UC Berkeley</p>
<p>Lerrel Pinto<br>UC Berkeley, NYU</p>
<h4>Abstract</h4>
<p>Continually solving new, unsolved tasks is the key to learning diverse behaviors. Through reinforcement learning (RL), we have made massive strides towards solving tasks that have a single goal. However, in the multi-task domain, where an agent needs to reach multiple goals, the choice of training goals can largely affect sample efficiency. When biological agents learn, there is often an organized and meaningful order to which learning happens. Inspired by this, we propose setting up an automatic curriculum for goals that the agent needs to solve. Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a significantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy. This simple technique samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement. We evaluate our method across 13 multi-goal robotic tasks and 5 navigation tasks, and demonstrate performance gains over current state-of-the-art methods.</p>
<h2>1 Introduction</h2>
<p>Model-free reinforcement learning (RL) has achieved remarkable success in games like Go [39], and control tasks such as flying [21] and dexterous manipulation [3]. However, a key limitation to these methods is their sample complexity. They often require millions of samples to learn a single locomotion skill, and sometimes even billions of samples to learn a more complex skill [5]. Creating general purpose RL agents will necessitate acquiring multiple such skills, which further exacerbates the sample inefficiency of these algorithms. Humans, on the other hand, are not only able to learn a multitude of different skills, but are able to do so from orders of magnitude fewer samples [20]. So, how do we endow RL agents with this ability to learn efficiently?
When human (or biological agents) learn, they do not simply learn from random data or on uniformly sampled tasks. There is an organized and meaningful order in which the learning is performed. For instance, when human infants learn to grasp, they follow a strict curriculum of distinct grasping strategies: palmar-grasp, power-grasp, and fine-grasp [27]. Following this order of tasks from simple ones to gradually more difficult ones is crucial in acquiring complex skills [31]. This ordered structure is also crucial to motor learning in animals [40, 23]. In the context of machine learning, a learning framework that orders data or tasks in a meaningful way is termed 'curriculum learning' [9].
Most research into curriculum learning has focused on the order of data that is presented to a supervised learning algorithm [12]. The key idea is that while training a supervised model, 'easy' data should be presented first, followed by more difficult data. This gradual presentation of data is shown to improve convergence and predictive performance [9]. However, in the context of reinforcement learning, how should one present a curriculum of data? The answer depends on what aspect of complexity needs to addressed. In this work, we focus on the complexity involved in solving new tasks/goals. Concretely, we operate in the sparse-reward goal-conditioned RL setting [36]. Here, the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In this work we focus on generating automatic curriculums, where we propose goals that are right at the frontier of the learning process of an agent. Given trajectories of behavior from a goal-conditioned RL policy, our value disagreement based Goal Proposal Module proposes challenging yet solvable goals for that policy.
sparse-reward setting reflects the inherent difficulty of real-world problems where a positive reward is only given when the goal is achieved.</p>
<p>To improve the sample efficiency of goal-conditioned RL, a natural framework for using curriculums is to organize the presentation of goals for the RL algorithm. This goal proposer will need to select goals that are informative for policy learning. One option for the goal proposer is to sample goals that have been previously reached [2]. Hence, as the algorithm improves, the sampled goals become more diverse. However, this technique will also re-sample goals that are too easy to give a useful training signal. The central question to improving the goal sampler is hence, how do we select the most useful and informative goals for the learning process?</p>
<p>To sample relevant goals that are maximally informative for the learning process, recent work [41, 32] focuses on using adversaries to sample goals for the agent at hand. Here, the adversary samples goals that are just at the horizon of solvability. These goals form a powerful curriculum since they are neither too easy nor too hard and hence provide a strong learning signal. However, due to the instability in adversarial learning and extra samples needed with multiple agents, these algorithms do not scale well to harder problems. Moreover, setting up an explicit two-player game for different problem settings is not a scalable option.</p>
<p>In this work, we propose a simple, but powerful technique to propose goals that are right at the cusp of solvability (see Figure 1). Our key insight is to look a little closer at the value function. In goal-conditioned settings, the value function of a RL policy outputs the expected rewards of following that policy from a given start state to reach a given goal. Hence, the function contains information about what goals are currently solvable and what goals are not, as well as what goals are right at the cusp of being solved. To retrieve this information, we present Value Disagreement based Sampling (VDS) as a goal proposer. Concretely, we approximate the epistemic uncertainty of the value function, and then sample goals from the distribution induced by this uncertainty measure. For goals that are too easy, the value function will confidently assign high values, while for goals that are too hard, the value function will confidently assign low values. But more importantly, for the goals right at the boundary of the policy's ability, the value function would have high uncertainty and thus sample them more frequently.</p>
<p>To compute the epistemic uncertainty practically, following recent work in uncertainty measurement [25], we use the disagreement between an ensemble of value functions. For evaluation, we report learning curves on 18 challenging sparse-reward tasks that include maze navigation, robotic manipulation and dexterous in-hand manipulation. Empirically, VDS further improves sample efficiency compared to standard RL algorithms. Code is publicly available at https://github.com/zzyunzhi/vds.</p>
<h1>2 Background and Preliminaries</h1>
<p>Before we describe our framework, we first discuss relevant background on goal-conditioned RL. For a more in-depth survey, we refer the reader to Sutton et al. [42], Kaelbling et al. [18].</p>
<h1>2.1 Multi-Goal RL</h1>
<p>We are interested in learning policies that can achieve multiple goals (a universal policy). Let $\mathcal{S}, \mathcal{A}$ be the state space and action space as in standard RL problems. Let $\mathcal{G}$ be the parameter space of goals. An agent is trained to maximize the expected discounted trajectory reward $\mathbb{E}<em 0:="0:" T-1="T-1">{s</em>$ can be trained with standard RL algorithms, as in [36, 2].
Following UVFA [36], the sparse reward formulation $r\left(s_{t}, a, g\right)=\left[d\left(s_{t}, g\right)&lt;\epsilon\right]$ will be used in this work, where the agent gets a reward of 0 when the distance $d(\cdot, \cdot)$ between the current state and the goal is less than $\epsilon$, and -1 otherwise. In the context of a robot performing the task of picking and placing an object, this means that the robot gets a higher reward only if the object is within $\epsilon$ Euclidean distance of the desired goal location of the object. Having a sparse reward overcomes the limitation of hand engineering the reward function, which often requires extensive domain knowledge. However, sparse rewards are not very informative and makes optimization difficult. In order to overcome the difficulties with sparse rewards, we employ Hindsight Experience Replay (HER) [2].}, a_{0: T-1}, r_{0: T-1}, g}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]$, where a goal $g$ is sampled from the parameter space $\mathcal{G}$. Multigoal RL problem can be cast as a standard RL problem with a new state space $\mathcal{S} \times \mathcal{G}$ and action space $\mathcal{A}$. Policy $\pi: \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$ and Q-function $\mathcal{S} \times \mathcal{G} \times \mathcal{A} \rightarrow \mathbb{R</p>
<h3>2.2 Hindsight Experience Replay (HER)</h3>
<p>HER [2] is a simple method of manipulating the replay buffer used in off-policy RL algorithms that allows it to learn universal policies more efficiently with sparse rewards. After experiencing some episode $s_{0}, s_{1}, \ldots, s_{T-1}$, every transition $s_{t} \rightarrow s_{t+1}$ along with the goal for this episode is usually stored in the replay buffer. However, with HER, the experienced transitions are also stored in the replay buffer with different goals. These additional goals are states that were achieved later in the episode. Since the goal being pursued does not influence the environment dynamics, we can replay each trajectory using arbitrary goals, assuming we use off-policy optimization [35].</p>
<h2>3 Method</h2>
<p>We first introduce Goal Proposal Module, a module that generates an automatic curriculum for goals. Following this, we describe our Value Disagreement Sampling (VDS) based Goal Proposal Module.</p>
<h3>3.1 Goal Proposal Module</h3>
<p>Let $\mathcal{C}: \mathcal{G} \rightarrow \mathbb{R}$ be a probability distribution over the goal space $\mathcal{G}$. A goal proposal module samples a goal $g$ from $\mathcal{C}$ at the start of a new episode. In this episode, the agent follows a $g$-conditioned policy to perform a trajectory and receives external rewards defined in Section 2.1.
In standard goal-conditioned RL, $\mathcal{C}$ reduces to the uniform distribution, where the goals are randomly sampled. However, sampling goals uniformly is often uninformative for the learning process [2] since during the early stages of learning, a majority of sampled goals are too hard, while during the later stages of learning most goals are too easy. Instead of using a uniform distribution over the goals, a curriculum learning based approach can sample goals in increasing order of difficulty.
To explicitly account for the dependence of $\mathcal{C}$ on the current policy $\pi$ as normally in the case of curriculum learning, we denote the goal distribution as $\mathcal{C}^{\pi}$.</p>
<h3>3.2 Value disagreement</h3>
<p>To automatically generate the goal sampling curriculum $\mathcal{C}^{\pi}$, we propose using the epistemic uncertainty of the Q-function to identify a set of goals with appropriate difficulty. When the uncertainty for $g \in \mathcal{G}$ is high, $g$ is likely to lie at the knowledge frontier of policy $\pi$ and thus is neither too easy nor too difficult to achieve. We defer more detailed reasoning and empirical evidence to Section 4.5.
Let $Q_{\phi}^{a}(s, a, g)$ be the goal-conditioned Q-function of a policy $\pi$, where $\phi$ is a learnable parameter. It approximates the expected cumulative return $\mathbb{E}<em a="a">{s</em>, g\right)$, this Q-function can be optimized using the Bellman update rule [42]:}=s, a_{0}=a, \tau \sim \pi(. |g)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]$. Given a transition $\left(s, a, r, s^{\prime</p>
<p>Algorithm 1 Curriculum Learning with Value Disagreement Sampling
Input: Policy learning algorithm $A$, goal set $\mathbb{G}$, replay buffer $R$.
Initialize: Learnable parameters $\theta$ for $\pi_{\theta}$ and $\phi_{1: k}$ for $Q_{1: k}$
for $n=1,2, . . N_{\text {iter }}$ do
Sample a set of goals $\mathbb{G}$
Compute $\hat{\mathcal{C}}^{\pi_{\theta}}$ according to equation 2
Sample $g \sim \hat{\mathcal{C}}^{\pi_{\theta}}(\cdot)$
Collect a goal-conditioned trajectory $\tau_{n}\left(\pi_{\theta} \mid g\right)$
Store transition data into the replay buffer $R \leftarrow \tau_{n}$
for all $\phi \in\left{\phi_{1}, \ldots, \phi_{k}\right}$ do
Perform Bellman-update according to equation 1 on samples drawn from $R$
Update policy parameter $\theta$ using algorithm $A$
Return: $\theta$</p>
<p>$$
Q_{\phi}^{\pi}(s, a, g) \leftarrow r+\gamma \mathbb{E}<em _phi="\phi">{a^{\prime} \sim \pi(\cdot \mid s, g)}\left[Q</em>, g\right)\right]
$$}^{\pi}\left(s^{\prime}, a^{\prime</p>
<p>Intuitively, this function tracks the performance of the policy $\pi$.
In practice, to estimate the epistemic uncertainty of $Q^{\pi}$, we measure the disagreement across an ensemble of parametric Q-functions following Lakshminarayanan et al. [25]. Hence, instead of a single $Q$, we maintain $K$ Q-functions $Q_{1: K}$ with independently trained parameters $\phi_{1: K}$. The disagreement between value functions for a goal $g$ is computed as a function of the ensemble's standard deviation.
Formally, given a goal $g \in \mathcal{G}$, let $\delta^{\pi}(g)$ be the standard deviation of $\left{Q_{1}^{\pi}(g), \cdots, Q_{K}^{\pi}(g)\right}$. Given any function $f: \mathbb{R}^{<em>} \rightarrow \mathbb{R}^{</em>}$, we define $\mathcal{C}^{\pi}(g)=\frac{1}{Z} f\left(\delta^{\pi}(g)\right)$, where $Z=\int_{\mathcal{G}} f\left(\delta^{\pi}(g)\right) \mathrm{d} g$ is the normalization constant.
Since $Z$ is usually intractable, we first uniformly sample a set of goals $\mathbb{G}=\left{g^{(n)}\right}_{n=1}^{N} \subseteq \mathcal{G}$. Then we define $\hat{\mathcal{C}}^{\pi}: \mathbb{G} \rightarrow \mathbb{R}$ as:</p>
<p>$$
\hat{\mathcal{C}}^{\pi}(g)=\frac{1}{Z} f\left(\delta^{\pi}(g)\right)
$$</p>
<p>to approximate $\mathcal{C}^{\pi}$, where $\hat{Z}=\sum_{n=1}^{N} f\left(\delta^{\pi}\left(g^{(n)}\right)\right)$.
Our method is summarized in Algorithm 1. For our experiments, we use DDPG [26] as our base RL algorithm to train the policy. We define $f$ to be the identity function for simplicity for all our experiments. Ablation study on multiple choices of $f$ is deferred to Appendix E.
Note that although we use an off-policy algorithm as our base optimizer, the goal sampler is independent of the choice of the base RL optimizer. The base RL algorithm is agnostic of the value ensemble and receives training goals only via Goal Proposal Module. The value ensemble has access to transition data collected by the agent, but the base RL algorithm is treated as a black box to maintain maximum flexibility.</p>
<h1>4 Experiments</h1>
<p>In this section, we first describe our experimental setup, training details and baseline methods for comparison. Then, we discuss and answer the following key questions: (i) Does VDS improve performance?; (ii) Does VDS sample meaningful goals?; (iii) How sensitive is VDS to design choices?</p>
<h3>4.1 Experimental setup</h3>
<p>We test our methods on 13 manipulation goal-conditioned tasks, 3 maze navigation tasks and 2 Ant-embodied navigation tasks, all with sparse reward, as shown in Figure 2. Detailed setup of the environments is presented in Appendix C.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: We empirically evaluate on all 13 robotic environments from OpenAI Gym [33], of which we illustrate 8. We also test our method on 3 maze navigation tasks, which serve as simple tasks for investigating VDS. In order to compare with GoalGAN [13], we evaluate our method on two Ant environments borrowed from their paper. The red dots, if illustrated, represent goals the robot or the object needs to reach.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Here we visualize the learning curves on 16 environments that include all 13 OpenAI Gym robotics benchmark environment and the 3 Maze environments. The $y$-axis is the success rate evaluated with the latest policy. The shaded region represents confidence over 5 random seeds. We notice significant improvements in sample efficiency of our method compared to baseline algorithms, especially on many challenging manipulation tasks.</p>
<h1>4.2 Training details</h1>
<p>To enable modularity, we treat the value ensemble as a separate module from the policy optimization. This provides us the flexibility to use VDS alongside any goal-conditioned RL algorithm. In this work, we use HER with DDPG as our backbone RL algorithm. To collect transition data, the policy optimizer queries the value ensemble to compute the goal distribution and select training goals accordingly. In line with standard RL, the policy generates on-policy data with $\epsilon$-greedy strategy. The obtained transitions data are then fed into the replay buffer of the VDS's value ensemble along with the replay buffer of the policy. In each training epoch, each Q-function in the ensemble performs Bellman updates with independently sampled mini-batches, and the policy is updated with DDPG. Evaluation goals are randomly selected, and the goal is marked as successfully reached if the agent reaches the goal at the last timestep of the episode. Detailed hyper-parameter settings are specified in the Appendix D, while an analysis of combining HER with VDS is provided in Appendix G.</p>
<h1>4.3 Baseline Methods</h1>
<p>To quantify the contributions of this work, we compare our method with the following baselines:</p>
<ul>
<li>HER In HER, the RL agent uses a hindsight replay buffer with DDPG [26] as the base RL algorithm with goals uniformly sampled from the goal space. The implementation and hyperparameters is based on the official codebase of HER. We use the same set of hyperparameters for HER and our method across all environments.</li>
<li>Robust Intelligence Adaptive Curiosity (RIAC) RIAC [7] proposes to sample goals from a continuous goal space $\mathcal{G}$ according to the Absolute Learning Progress (ALP) of the policy. The policy has large positive learning progress on a region of $\mathcal{G}$ when it is making significant improvement on reaching goals lying within region. It has negative learning progress when suffering from catastrophic forgetting. RIAC splits $\mathcal{G}$ into regions, computes the ALP score for each region, selects regions with a probability distribution proportional to the ALP score, and samples goals from the selected regions.</li>
<li>Covar-GMM Covar-GMM [28] fits a Gaussian Mixture Model (GMM) on the goal parameter space $\mathcal{G}$ concatenated with the episodic reward and time. At the start of each episode, a goal is sampled with propability proportional to the covariance of episodic reward and time.</li>
<li>ALP-GMM ALP-GMM [34] fits a GMM on $\mathcal{G}$ concatenated with an ALP score approximated by the absolute reward difference between the current episode conditioned on $g$ and a previous episode conditioned some goal neighboring $g$. Our implementation and hyperparameters of RandomSAC, RIAC, Covar-GMM and ALP-GMM follows the official codebase of ALP-GMM.</li>
<li>GoalGAN GoalGAN [13] labels if the goals in the replay buffer are of intermediate difficulty by the episodic reward, and then feed the labeled goals into a Generative Adversarial Network (GAN) that outputs goals of intermediate difficulty. In later episodes, the agent is trained on goals generated by GAN. Our implementation and hyperparameter settings of GoalGAN follow their official codebase.</li>
</ul>
<h3>4.4 Improvements using VDS</h3>
<p>Figure 3 shows that our method achieves better sample efficiency compared to baselines on most of the 16 environments with 4 FetchArm, 9 HandManipulation and 3 Maze navigation tasks. Uniform goal sampling (HER) demonstrates competitive performance in some of the reported environments, which is consistent with previous work in Portelas et al. [34].
We compare with GoalGAN in the Ant environments as reported in their original paper [13]. Figure 4 shows that our method obtains significant sample efficiency gain compared to GoalGAN. One difference of the environment is that our method and HER perform fixed-length trajectories, and episodic success is measured as whether the goal is reached in the final timestep of the episode; in contrast, in GoalGAN, whenever the agent achieves the goal, the goal is marked as reachable and the episode terminates before reaching maximal episode length. This is consistent with the original implementation and works in favor of GoalGAN. Also note that the curve does not take into account timesteps used to label the difficulty of goals, again in favor of GoalGAN. We conclude from these two environments that our method is more sample efficient than GoalGAN, self-play [41], SAGG-RIAC [8], uniform sampling and uniform sampling with L2 loss in these two environments. We refer the readers to Florensa et al. [13] for performance curves of these baselines.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: We illustrate the goal-conditioned episodic rewards of the latest policy, Q-values averaged over the ensemble, and finally the goal sampling distribution with sampled training goals (red dots) for two Maze environments shown in Figure 2. The agent starts from the bottom-left corner in MazeA and top-left for MazeB. We note that the disagreement produces a higher density of samples on regions at the frontier of learning. Over iterations, we also see the sampled goals move away from the starting state and towards harder goals. A complete illustration is available at https://sites.google.com/berkeley.edu/vds.</p>
<p>Learning in these environments, with the absence of strong learning signals, typically requires effective exploration. Our results demonstrate that VDS in combination with hindsight data sets up an informative course of exploration.</p>
<h1>4.5 Does VDS sample meaningful goals?</h1>
<p>To have an intuitive understanding of the goal sampling process and how VDS helps with setting up learning curriculum, we visualize in Figure 5 the followings: (i) the evaluated trajectory returns of the policy conditioned on goals varying over the maze world, (ii) Q-value predictions averaged over the ensemble, and (iii) goal distribution with VDS and the most recent 50 training goals.</p>
<p>In (i) and (ii), visualization of reward and Q-value landscape shows that the policy gradually expands its knowledge boundary throughout the training process. Darker region indicates areas that the policy achieves higher trajectory rewards or higher Q-values. In (iii), darker region indicates higher uncertainty of the ensemble prediction, which matches the boundary of (i) and (ii).</p>
<p>At the start of training, goal sampling distribution is close to uniform due to random initialization. Then, as the policy learns to reach goals neighboring to the starting position, it is also possible to reach goals residing close to the learning frontier, as minor disturbance with $\epsilon$-greedy strategy could lead the policy to hit the goal and obtain the corresponding reward signal. These goals are not yet mastered by the policy but could happen to be reached by policy exploration, and therefore have higher Q-value prediction variance. With VDS, they are more likely to be selected.</p>
<p>These goals at the frontier are ideal candidates to train the policy, because they are nontrivial to solve, but are also not as hard compared to goals lying far away from the policy's mastered region. Consequently, VDS improves sample efficiency by setting up an automatic learning curriculum. Figure 5 indeed suggests so, as we notice a clear sign of a goal distribution shift over iterations, with harder and harder goals getting sampled.</p>
<h3>4.6 Ablations on VDS</h3>
<p>To understand the effects of various design choices in implementing VDS (see Section 3.2), we run ablation studies. Specifically, we study the effects of (i) choice of sampling function $f$, (ii) choice of ensemble size for value uncertainty estimation, and (iii) options of combination with HER [2]. While</p>
<p>details of these ablations are deferred to Appendix E, F, G, we highlight key findings here. First, for sampling functions, we find that our method is insensitive to the choice of sampling function $f$. Second, VDS is not sensitive to ensemble size. In fact, performance when using an ensemble size of 10 is the same as using an ensemble size of 3 . Finally, we show that without using HER, VDS still improves the performance of vanilla DDPG. Combining with results in more environments from Section 4.4, we conclude that VDS is complementary with HER and provides the best result when used together.</p>
<h1>5 Related Work</h1>
<p>Our work is inspired from and builds on top of a broad range of topics across curriculum learning and goal-conditioned reinforcement learning. In this section, we overview the most relevant ones.</p>
<h3>5.1 Curriculum Learning</h3>
<p>Automatic curriculum generation has a rich history in the context of supervised learning. Bengio et al. [9] demonstrates how gradually increasing the complexity of training samples to a supervised learning algorithm leads to accelerated learning and better prediction quality. Kumar et al. [24] then proposed 'self-paced learning' in the context of non-convex optimization, where the order of training examples is automatically chosen. Murali et al. [29] demonstrates how automated curriulums on the control space can improve performance of robotic grasping. In all of these works, the focus is on supervised learning problems, where the curriculum is over training examples that are fed to the learning algorithm. Our work builds on top of this idea to creating curriculums over tasks/goals that a RL algorithm needs to solve.</p>
<p>In the context of RL, several techniques to generate curriculums have been proposed. Florensa et al. [14] propose reverse curriculums, where given a specific goal to solve, the agent is reset to a states closer to the goal and then over time expanded. However, this assumes easy reset to arbitrary states, which is not practical for general purpose RL. To alleviate this, HER [2, 1] samples goals based on states previously reached by the agent using 'hindsight'. As the agent improves performance, the state footprint of the policy increases and hence more complex goals are sampled. However, using this strategy a large portion of the sampled goals are too easy to provide useful signal. In our work, we combine VDS with HER and show significant improvements over vanilla HER. Several recent works have looked at creating curriculums by explicitly modelling the difficulty of the goal space $[7,28,34,13]$. Again, we show empirically that VDS obtains substantial performance gains over previous automatic curriculum techniques (see Section 4.4 and Figure 3).</p>
<h3>5.2 Self-Play based curriculums</h3>
<p>Sampling tasks/goals that are useful for RL has also been studied in the context of 'self-play' [11], where a two-player competitive game is setup in which different player policies are pitted against each other. This technique has seen success in challenging games like GO [39] and DOTA [10]. In the context of robotic control problems, Bansal et al. [6] demonstrates how self-play can assist in the development of locomotion behaviors. Instead of a symmetric game setting, Pinto et al. [32], Sukhbaatar et al. [41] propose setting up asymmetric two-player games, where one agent focuses on proposing goals without having to explicitly solve that goal. These self-play setting create an automated curriculum that improves learning. However, applying these ideas to arbitrary control problems, where a natural game formulation is not present, is challenging. Recently, GoalGAN [13] has shown superior performance to such asymmetric game settings. Empirically, both VDS and HER perform significantly better than GoalGAN on Ant-navigation tasks (see Section 4.4 and Figure 4).</p>
<h3>5.3 Uncertainty estimation through neural networks</h3>
<p>A key component of our method is to model the uncertainty of a function approximator. Several works have looked at estimating uncertainty in neural networks [4, 22, 15, 25]. Since we are looking to estimate the epistemic uncertainty, i.e. the prediction uncertainty of a model, we use an ensemble of neural networks inspired from Lakshminarayanan et al. [25]. This use of uncertainty has been previously applied for exploration [17, 30], and although we draw architectural inspiration from these works, we note that our problem setting is different.</p>
<h1>6 Conclusion</h1>
<p>In this work we present a technique for automatic curriculum generation of goals that relies on the epistemic uncertainty of value functions. Through experiments on a suite of 18 sparse-reward tasks, we demonstrate substantial improvements in performance compared to HER and other standard RL algorithms. Through further analysis, we demonstrate that our method is robust to different hyperparameters while being able to sample goals at the frontier of the learning process. Finally, we believe that this simple technique can be extended to other domains like real-world robotics and visual reinforcement learning.
Acknowledgements: We gratefully acknowledge the support Berkeley DeepDrive, NSF, and the ONR Pecase award. We also thank AWS for computational resources.</p>
<h2>References</h2>
<p>[1] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.
[2] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. NIPS, 2017.
[3] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.
[4] T. Auld, A. W. Moore, and S. F. Gull. Bayesian neural networks for internet traffic classification. IEEE Transactions on neural networks, 18(1):223-239, 2007.
[5] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, and I. Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019.
[6] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch. Emergent complexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017.
[7] A. Baranes and P.-Y. Oudeyer. R-iac: Robust intrinsically motivated exploration and active learning. IEEE Transactions on Autonomous Mental Development, 1(3):155-169, 2009.
[8] A. Baranes and P.-Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.
[9] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.
[10] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dębiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
[11] P. Campos and T. Langlois. Abalearn: Efficient self-play learning of the game abalone. INESCID, neural networks and signal processing group, 2003.
[12] J. L. Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71-99, 1993.
[13] C. Florensa, D. Held, X. Geng, and P. Abbeel. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, 2017.
[14] C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel. Reverse curriculum generation for reinforcement learning. arXiv preprint arXiv:1707.05300, 2017.
[15] Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050-1059, 2016.</p>
<p>[16] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.
[17] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109-1117, 2016.
[18] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237-285, 1996.
[19] S. M. Kakade. A natural policy gradient. In Advances in neural information processing systems, pages 1531-1538, 2002.
[20] A. Karni, G. Meyer, C. Rey-Hipolito, P. Jezzard, M. M. Adams, R. Turner, and L. G. Ungerleider. The acquisition of skilled motor performance: fast and slow experience-driven changes in primary motor cortex. Proceedings of the National Academy of Sciences, 95(3):861-868, 1998.
[21] E. Kaufmann, A. Loquercio, R. Ranftl, A. Dosovitskiy, V. Koltun, and D. Scaramuzza. Deep drone racing: Learning agile flight in dynamic environments. arXiv preprint arXiv:1806.08548, 2018.
[22] A. Kendall and Y. Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in neural information processing systems, pages 5574-5584, 2017.
[23] K. A. Krueger and P. Dayan. Flexible shaping: How learning in small steps helps. Cognition, 110(3):380-394, 2009.
[24] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems, pages 1189-1197, 2010.
[25] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in neural information processing systems, pages $6402-6413,2017$.
[26] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[27] M. Molina and F. Jouen. Modulation of the palmar grasp behavior in neonates according to texture property. Infant Behavior and Development, 21(4):659-666, 1998.
[28] C. Moulin-Frier, S. M. Nguyen, and P.-Y. Oudeyer. Self-organization of early vocal development in infants and machines: the role of intrinsic motivation. Frontiers in psychology, 4:1006, 2014.
[29] A. Murali, L. Pinto, D. Gandhi, and A. Gupta. Cassl: Curriculum accelerated self-supervised learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 6453-6460. IEEE, 2018.
[30] D. Pathak, D. Gandhi, and A. Gupta. Self-supervised exploration via disagreement. arXiv preprint arXiv:1906.04161, 2019.
[31] G. B. Peterson. A day of great illumination: Bf skinner's discovery of shaping. Journal of the experimental analysis of behavior, 82(3):317-328, 2004.
[32] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta. Robust adversarial reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2817-2826. JMLR. org, 2017.
[33] M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.
[34] R. Portelas, C. Colas, K. Hofmann, and P.-Y. Oudeyer. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. arXiv preprint arXiv:1910.07224, 2019 .</p>
<p>[35] D. Precup, R. S. Sutton, and S. Dasgupta. Off-policy temporal-difference learning with function approximation. In ICML, 2001.
[36] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In ICML 2015.
[37] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In ICML, pages 1889-1897, 2015.
[38] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In ICML 2014.
[39] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550 (7676):354-359, 2017.
[40] B. F. Skinner. Teaching machines. Science, 128(3330):969-977, 1958.
[41] S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017.
[42] R. S. Sutton, A. G. Barto, et al. Introduction to reinforcement learning, volume 2. MIT press Cambridge, 1998.
[43] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
<h1>Appendix</h1>
<h2>A Reinforcement learning</h2>
<p>In our continuous-control RL setting, an agent receives a state observation $s_{t} \in \mathcal{S}$ from the environment and applies an action $a_{t} \in \mathcal{A}$ according to policy $\pi$. In our setting, where the policy is deterministic, we hence have $a_{t}=\pi\left(s_{t}\right)$. The environment returns a reward for every action $r_{t}$. The goal of the agent is to maximize expected cumulative discounted reward $\mathbb{E}<em 0:="0:" T="T">{s</em>\right]$ for discount factor $\gamma$ and horizon length $T$. On-policy RL [37, 19, 43] optimizes $\pi$ by iterating between data collection and policy updates. It hence requires new on-policy data every iteration, which is expensive to obtain. On the other hand, off-policy reinforcement learning retains past experiences in a replay buffer and is able to re-use past samples. Thus, in practice, off-policy algorithms have been found to achieve better sample efficiency [26, 16]. For our experiments we use DDPG [26] as our base RL optimizer due to its sample efficiency and fair comparisons with baselines that also build on top of DDPG. However, we note that our framework is compatible with any standard off-policy RL algorithm.}, a_{0: T-1}, r_{0: T-1}}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t</p>
<h2>B Deep Deterministic Policy Gradients (DDPG)</h2>
<p>Deep Deterministic Policy Gradients (DDPG) [26] is an actor-critic RL algorithm that learns a deterministic continuous action policy. The algorithm maintains two neural networks: the policy $\pi_{\theta}: \mathcal{S} \rightarrow \mathcal{A}$ (with neural network parameters $\theta$ ) and a $Q$-function approximator $Q_{\phi}^{\pi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ (with neural network parameters $\phi$ ). During optimization, episodes collected using $\pi$ are stored in a replay buffer. Then, the $Q$-function is optimized by minimizing the one-step Bellman error on samples from the replay buffer, while the policy is optimized using the deterministic policy gradient [38].</p>
<h2>C Environments</h2>
<ul>
<li>Fetch: These environments simulate a 7-DoF Fetch arm, with 3-dimensional goal space and 4-dimensional action space. The reaching task has 10-dimensional observation space, while other tasks involve objects and have 25-dimensional observation space. The agent receives a reward of 0 if the final position of the gripper of object, depending on the environment, is within $\epsilon$ Euclidean distance of the goal, and -1 otherwise. The initial position of the Fetch arm is not guaranteed to be close to the object location and henceforth increase the complexity of the task. Moreover, since the robot has to perform multiple primitive actions like grasping followed by reaching, learning this with sparse rewards is additionally challenging.</li>
<li>FetchReach: Move the gripper to a target location.</li>
<li>FetchPickAndPlace: Pick up a box and move to a target position.</li>
<li>FetchPush: Push a box to a target position.</li>
<li>FetchSlide: Slide a puck to a target position, which is outside the arm's reach.</li>
<li>Hand: These environments simulate a 24-DoF robotic hand, with 20-dimensional action space. The reach task has 15-dimensional goal space and 63-dimensional observation space, and the manipulation tasks have 7-dimensional goal space and 61-dimensional observation space. The large action space of coupled with complex dynamics and sparse rewards makes these tasks challenging.</li>
<li>HandManipulateBlock: Rotate a block to match a target rotation in all axes and to match a target position.</li>
<li>HandManipulateBlockRotateParallel: Rotate a block to match a target rotation in $x$ - and $y$-axis.</li>
<li>HandManipulateBlockRotateXYZ: Rotate a block to match a target rotation in all axes.</li>
<li>
<p>HandManipulateBlockRotateZ: Rotate a block to match a target rotation in $z$-axis.</p>
</li>
<li>
<p>HandManipulateEgg: Rotate an egg to match a target rotation in all axes and a target position.</p>
</li>
<li>HandManipulateEggRotate: Rotate an egg to match a target rotation in all axes.</li>
<li>HandManipulatePen: Rotate an pen to match a target rotation in all axes and a target position.</li>
<li>HandManipulatePenRotate: Rotate an pen to match a target rotation in all axes.</li>
<li>HandReach: Move to match a target position for each finger tip.</li>
<li>Maze: The environment for navigation tasks is a finite-sized, 2-dimensional maze with blocks. The agent is given a target position and starts from a fixed point in the maze, and it obtains reward of 0 if it gets sufficiently close to the target position at the current timestep, or a penalty of -1 otherwise. The agent observes the 2-D coordinates of the maze, and the bounded action space is specified by velocity and direction. The agent moves along the direction with the velocity specified by the action if the new position is not a block, and stays still otherwise. The maximum timestep of an episode is set to 50 .</li>
<li>MazeA: The first variant of maze contains random blocks as shown in the left maze Figure 2.</li>
<li>MazeB: The second variant of maze is shown in the middle maze Figure 2.</li>
<li>MazeC: The third variant of maze is shown in the right maze Figure 2. The central area is infeasible as all sides are blocked by walls.</li>
</ul>
<h1>D Implementation Details and Hyperparameters</h1>
<p>For all DDPG-based methods, we run with 1 CPU and two parallel environments. Each Q-function in the ensemble is trained with its target network, with learning rate 1e-3, polyak coefficient 0.95 , buffer size 1e6, and batch size 1000. For DDPG and HER, all hyperparameters inherit from the official implementation of HER. The learning rate is $1 \mathrm{e}-3$, polyak coefficient is 0.95 , buffer size is 1 e 6 , batch size is 256 , the $\epsilon$-exploration coefficient is 0.3 , and the standard deviation of Gaussian noise injected to non-random actions is 0.2 of maximum action.</p>
<p>For SAC-based methods, we run with 1 CPU and a single environment. The policy, Q-function and Value function are trained with learning rate 1e-3, buffer size 2000000, batch size 1000. All hyperparameters inherit from the official GMM implementation.
For all implementations unless stated otherwise, we employ HER. The expected ratio of transitions with swapped goal to regular transitions is 4 in each batch.</p>
<h2>E Ablation: How important is the choice of disagreement function</h2>
<p>There could be multiple choices of the disagreement function that maps from value ensemble variance to disagreement, and the value of disagreement will be subsequently fed into the goal proposal module. As mentioned in Section 3.2, we use the value ensemble variance for simplicity across all experiments. However, any monotonically increasing function $f$ that takes in ensemble standard deviation as input could be valid candidates. The choice of $f$ either smooths the goal distribution, or aggregates more weight on a small region of goals. One extreme case will be using a constant function. In this case, VDS is reduced to uniform goal sampling.
In this section, we compare the effect of different choices of $f$. Examples are exponential function $f\left(\delta^{\pi}(g)\right)=\exp (\delta(g))$, $\tanh$ function $f(\delta(g))=\tanh (\delta(g))$, and square function $f(\delta(g))=\delta^{2}(g)$. Results shown in Figure 6 indicate that our method is not sensitive to the choice of $f$.</p>
<h2>F Ablation: How important is the choice of ensemble size?</h2>
<p>In our experiments, we use an ensemble size of 3 for all tasks. But what happens when we vary the ensemble size? Results in Figure 7 demonstrate that the size of the value ensemble does not have a significant impact on the performance for the maze navigation task. Thus, with just a few networks in the ensemble, we can obtain performance similar to larger ensemble networks. The robustness to ensembles also points towards the fact that extremely accurate estimates of epistemic uncertainty is</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: All curves for the ablation study are ran across 5 random seeds, and the shade represents the confidence. Here we compare the effects of using different disagreement sampling strategies on the Maze environments. We notice that our method is not sensitive to the choice of $f$.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: We compare the effects of the ensemble size used in the computation of value disagreement. We notice no degradation in performance, which points towards robust estimation of epistemic uncertainty.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Here we compare different method of applying VDS and HER on vanilla DDPG. We note that our method with no HER (purple) performs better than vanilla DDPG (red).
not required for seeing benefits of our approach. An additional benefit of this robustness is that our algorithm should be easily compatible with other techniques to measure epistemic uncertainty like Dropout [15].</p>
<h1>G Ablation: How does VDS perform in combination with HER?</h1>
<p>Our method can be combined with HER in various ways. Curves in Figure 8 correspond to different combinations of using or not using VDS, with and without hindsight experience data. Applying VDS on vanilla DDPG improves the performance, while only applying HER is already competitive in these twow environments. However together with results in Section 4.4 where VDS improves the sample efficiency of HER in a majority of environments, it shows that it produces the best result when using our method in combination with HER.
While HER focuses on obtaining learning signal via relabelling without extra interaction with the real environment, our method appeals to the value ensemble to conduct the interaction with the real environment in an efficient manner. VDS as a sample collection strategy could be considered as orthogonal to the way that HER improves with training, and the two methods collaboratively strengthen the learning signal that the policy processes throughout training.</p>            </div>
        </div>

    </div>
</body>
</html>
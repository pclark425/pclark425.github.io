<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2165 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2165</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2165</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-278769498</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.13259v3.pdf" target="_blank">From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement. Github Repository: https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2165.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2165.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic LLM-based system that autonomously generates research ideas, produces code, executes experiments, and writes draft research papers, using internal scoring and external literature checks to evaluate proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based multi-agent / agentic system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Artificial intelligence / automated scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Research ideas, research proposals, code for experiments, experimental plans, draft research papers</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal self-assessment scoring (interestingness, novelty, feasibility) combined with external literature checks (Semantic Scholar) and automated internal reviewers/LLM evaluators for experimental choices; VLMs used to critique figures</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Self-assessed novelty scores and literature overlap checks against Semantic Scholar (bibliographic novelty assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported as capable of generating diverse research proposals and automated experiments qualitatively; no numeric success rates given in survey; described as generative and able to produce outputs that warrant downstream checking</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Qualitative internal validation via scoring and literature checks; no numerical accuracy/precision metrics reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>System includes explicit novelty scoring and literature checks, implying validation is more challenging for higher-novelty items and relies on external literature grounding to detect prior art or obvious errors</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Evidence of asymmetry: strong generative capability but validation relies on heuristic scoring and external lookup, suggesting generation can outpace rigorous validation especially for novel ideas</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; implied that novel, out-of-distribution research topics require additional literature grounding and are harder to validate</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; self-assessment scores used but no calibration statistics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation involves additional literature queries and internal multi-model critiques (LLM evaluators, VLMs), implying higher computational cost than generation but no quantitative cost reported</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Internal multi-model reviewers, literature-grounding (Semantic Scholar checks), and multi-modal critiques (VLMs); human oversight recommended</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI Scientist (v1) demonstrates strong generative capabilities and uses internal scoring plus literature checks to assess novelty and feasibility, but validation remains heuristic and human oversight is emphasized for novel outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2165.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (v2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A follow-up agentic system that performs workshop-level automated scientific discovery using agentic tree search and stronger literature-grounding during idea formulation to assess novelty and feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (v2)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based multi-agent with agentic tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Artificial intelligence / automated scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates diverse research proposals and procedural experiment plans from abstract prompts; can produce workshop-level research drafts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Integrates literature review tools early in idea formulation to evaluate novelty; uses internal automated reviewers and iterative agentic search with feedback loops</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Literature-grounded novelty checks (early-stage literature review) and internal novelty/feasibility scoring</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described as more generative/diverse than v1 and able to produce varied proposals; no numeric metrics provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improved through earlier literature grounding, but no quantitative validation metrics reported</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Explicitly addresses novelty by integrating literature review early, indicating validation difficulty increases with novelty and is partially mitigated by early grounding</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Evidence of a remaining asymmetry: enhanced validation pipelines reduce but do not eliminate the gap between rich generation and robust validation on novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; design choices (agentic tree search + literature grounding) intended to improve OOD performance but limits remain</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Higher than generation due to tree-search and literature retrieval; exact costs not reported</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Agentic tree-search with iterative internal review, early literature grounding, multi-agent feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI Scientist-v2 improves novelty assessment by integrating literature review early and using agentic tree search, reducing but not eliminating the challenge of validating highly novel outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2165.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent Laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent Laboratory: Using LLM agents as research assistants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system that conducts literature review, experimental planning, report writing, and iterative research with human feedback loops, emphasizing human-defined research objectives as starting points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agent laboratory: Using llm agents as research assistants.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agent Laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM multi-agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Artificial intelligence / research assistance</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Literature summaries, experiment plans, data analyses, reports</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human-in-the-loop validation at macro levels; iterative human feedback triggers re-evaluation and hypothesis regeneration if needed</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Implicit: starts from human-defined objectives and identifies gaps in literature; novelty assessed via human expert judgement and literature exploration</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Useful for literature-driven idea generation and task orchestration; no numeric metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Relies heavily on human oversight; automation supports internal checks but quantitative performance not reported</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>High novelty often shifts the burden to human experts for macro-level validation; automated validation less reliable for novel directions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Supports gap theory: agents generate candidate directions but validation, especially for novel ideas, requires substantial human intervention</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; performance depends on human guidance and literature availability</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation cost dominated by human time and iterative review cycles; computational cost of agent orchestration present but unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human-in-the-loop oversight, iterative feedback loops, ability to revert to hypothesis regeneration</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agent Laboratory illustrates that multi-agent systems can autonomously conduct many research tasks but rely on human oversight for high-level validation and novel hypothesis vetting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2165.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zochi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zochi (IntologyAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial LLM-based system with customizable workflows that integrates human expertise at macro levels to guide decision-making and enable hypothesis re-evaluation and regeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Zochi</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Commercial LLM agent / workflow system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Artificial intelligence / automated research workflows</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Hypothesis generation, experiment design, data analysis, decision recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Hybrid: automated internal checks plus explicit human expert guidance for macro-level validation and structural re-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Human expert assessment and workflow-driven literature exploration; no formal novelty metric specified</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported qualitatively as capable of generating and iterating research designs; no quantitative metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Relies on human experts for high-level validation; automated components support but do not replace human judgment</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>High novelty prompts human experts to re-evaluate or regenerate hypotheses, showing validation degrades for novel outputs without human input</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — system generates autonomously but validation for novel claims depends on human expertise</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; human oversight intended to manage OOD cases</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Human time is a major cost; system computational validation unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human-in-the-loop oversight, customizable workflows permitting macro-level human interventions</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zochi demonstrates a practical hybrid approach where autonomy is balanced by human expert validation, particularly necessary for novel or strategic research redirections.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2165.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanation-Refiner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explanation-Refiner (LLM + theorem proving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that verifies and refines LLM-generated natural language explanations by interfacing with symbolic theorem provers to check logical correctness and refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Verification and refinement of natural language explanations through llm-symbolic theorem proving.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Explanation-Refiner</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hybrid LLM + symbolic theorem prover (neurosymbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Automated hypothesis refinement / formal verification of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Natural language hypotheses/explanations, refined and formalized by theorem-prover-guided loops</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Formal symbolic verification using theorem provers to check logical consistency and to drive hypothesis refinement</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly quantified; validation emphasizes formal provability rather than novelty distance from training data</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Enables production of hypotheses that can be formalized and tested; no success rates reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Stronger validation for statements that can be formalized; limited applicability when hypotheses are empirical or not easily formalized</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Formal verification remains effective when statements map to provable formal theories; novel empirical hypotheses that lack formal grounding are harder to verify</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Partially reduced for logically formalizable outputs — the system can close the gap via theorem proving — but for many empirical discoveries a gap remains</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; likely limited when novel claims cannot be expressed in the formal system used</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Theorem proving can be computationally expensive depending on complexity; no numeric estimates provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Integration of symbolic theorem provers with LLM outputs to formally verify and refine explanations</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explanation-Refiner shows that combining LLMs with symbolic provers can materially improve validation for formalizable claims, but many scientific hypotheses remain outside this scope and still need human/evidence-based validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2165.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR: Scientific equation discovery via programming with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based approach to symbolic regression and equation discovery that leverages LLM prior knowledge and clustered memory feedback to discover governing equations from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-SR: Scientific equation discovery via programming with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-augmented symbolic regression (hybrid data-driven + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Function discovery / symbolic regression / physics & statistics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates symbolic equations/models that explain observational data</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarked on function discovery tasks; uses clustered memory feedback (experience) and cross-checking with data fidelity; LLM-SRBench introduced to control for data contamination</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Benchmarks include function transformations to reduce contamination; novelty implicitly assessed by ability to rediscover known equations or find unseen forms</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported improvements over naive LLM usage for symbolic regression in cited works, but overall performance varies and is benchmark-dependent; no single numeric success rate reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation primarily via fit to data and benchmark ground-truth comparison in LLM-SRBench; effectiveness depends on data quality and contamination controls</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Benchmarks (LLM-SRBench) try to mitigate contamination; performance drops when functions are transformed to be out-of-distribution relative to training data, indicating validation and discovery are harder for novel forms</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — LLMs can propose plausible symbolic forms but rigorous validation requires data-fitting and contamination controls; novel equations are harder to validate</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades for transformed or novel function forms introduced to the benchmark to avoid data leakage</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation requires fitting candidate symbolic forms to data and running benchmarking pipelines, implying higher computational cost; specifics not provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Clustered memory feedback, benchmarked transformations (LLM-SRBench) to detect contamination, data-fit comparisons to ground-truth</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-SR shows promise for equation discovery but performance and validation reliability drop when evaluating functions that differ from training data; benchmarks introduced to reduce contamination highlight sensitivity to novelty.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2165.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SRBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SRBench: A new benchmark for scientific equation discovery with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark designed to evaluate LLMs on function discovery tasks while mitigating data contamination by including function transformations and controlled test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-SRBench: A new benchmark for scientific equation discovery with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-SRBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Function discovery / symbolic regression evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Not a generator; evaluates generators' ability to produce true underlying functions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Controlled benchmark with function transformations to prevent contamination, ground-truth comparison of discovered equations, and standardized evaluation metrics (not enumerated in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Explicitly creates out-of-distribution variants via function transformations to measure novelty sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used to show LLM-based SR methods struggle when functions are transformed (OOD); no numeric aggregate metrics provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Enables stricter validation by removing training-set leakage; demonstrates reduced false positive discoveries when contamination controlled</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Designed to reveal degradation in generation and validation performance on OOD / transformed functions; novelty strongly degrades measured performance</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Benchmarks document that generation success can be inflated by contamination, so careful benchmarking reveals asymmetry where apparent generative success is not matched by robust validation</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Specifically measures OOD degradation and shows substantial performance drop when transformations are applied</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Benchmarks require additional data transformations and evaluation compute; no quantitative costs provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Controlled OOD benchmarks, transformation-based contamination mitigation, ground-truth equation matching</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-SRBench demonstrates that prior apparent successes in equation discovery can be driven by data contamination and that OOD/transformed functions expose substantial generation-validation gaps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2165.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLAgentBench: Evaluating language agents on machine learning experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark assessing LLM agents' ability to design and run ML experiments, finding that performance is often contingent on task familiarity and that agents struggle on novel research challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MLAgentBench: Evaluating language agents on machine learning experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Machine learning research automation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Evaluates generation of ML experiment designs, model configurations, and experimental code by agents</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarks agents by running generated experiments and comparing results to target tasks or baselines; measures success on tasks of varying familiarity</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Task familiarity vs novelty stratification; benchmarks include novel ML research challenges to test generalization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Performance is often good on familiar tasks but degrades on novel ML research challenges (qualitative statement in survey; no numeric rates provided)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation conducted via execution of experiments; validation reliability drops when tasks are novel due to incorrect experiment design or misinterpretation</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Clear: validation/execution success and correctness degrade on novel tasks; agents perform better on familiar tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Supports asymmetry: agents can produce plausible designs for familiar problems but fail to generate and validate correct solutions for novel problems</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades significantly for out-of-distribution ML research challenges compared to in-distribution/familiar tasks</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Running generated experiments is computationally expensive compared to pure text generation; cost increases with complexity of task and novelty due to iterative debugging</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Iterative refinement frameworks (IMPROVE), curated expert collaboration, budgeted human-in-the-loop interventions</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MLAgentBench shows that LLM agents perform acceptably on familiar ML tasks but struggle on novel research challenges, revealing a substantial drop in both generation and validation reliability for OOD tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2165.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLRC-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLRC-Bench: Can language agents solve machine learning research challenges?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark probing language agents' ability to tackle novel ML research problems and comparing agent R&D capabilities against human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mlrc-bench: Can language agents solve machine learning research challenges?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLRC-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Machine learning research evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Evaluates generation of novel ML research solutions, experimental plans, and analyses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compares agent outputs against expert solutions and measures ability to solve novel ML challenges; includes human expert comparison</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Explicitly includes novel/unseen ML research tasks to quantify OOD capability relative to experts</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey reports that benchmarks probe limits and often show agents lag behind human experts on novel R&D tasks; no numeric results in survey</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation via expert comparison and task success; agents underperform on novel tasks compared to human experts</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance and validation reliability drop for novel tasks; agents show competence on familiar tasks but not on frontier research problems</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Supports asymmetry: generation abilities are insufficient to match validation and problem-solving levels demonstrated by humans on novel research challenges</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Lower than in-distribution; agents struggle to generalize to novel ML research problems</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High: evaluation against expert baselines and running experiments is computationally and labor intensive</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human expert benchmarking, curated collaboration frameworks, iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MLRC-Bench highlights that language agents lag behind humans on novel ML research challenges, demonstrating a pronounced generation-validation gap on OOD tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2165.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscoveryBench / DiscoveryWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoveryBench / DiscoveryWorld</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Platforms/benchmarks creating virtual environments and datasets for LLM agents to perform simplified scientific exploration, used to evaluate agentic discovery capabilities and limitations on novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DiscoveryWorld: A virtual environment for developing and evaluating automated scientific discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DiscoveryWorld / DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark / simulated environment</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General scientific discovery evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Evaluates generation of experimental plans, hypotheses, and exploration strategies in a controlled virtual environment</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automated environment outcomes and ground-truth checks in the simulated domain; allows controlled OOD testing</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Environment designs enable testing on novel tasks vs. familiar patterns; novelty measured by task variation and unseen scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Agents can perform simplified discovery tasks in simulation; performance drops when tasks increase in complexity or novelty (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Automated outcome-based validation works well in simulation for in-distribution tasks but is less informative for truly novel scientific claims</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation in simulated settings can detect failures on novel tasks, but simulation-to-reality transfer for real discovery remains a challenge</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Supports asymmetry in real-world contexts: success in simulated generation does not guarantee valid real-world discoveries</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades with task novelty and complexity within simulated benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Simulation runs can be computationally expensive depending on environment fidelity; no concrete numbers provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Controlled environments, task stratification by novelty, and ground-truth outcome checks in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DiscoveryWorld/DiscoveryBench permit controlled measurement of agent discovery abilities and reveal that agents that succeed in simulation often fail to validate or transfer to real, novel scientific discovery tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2165.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-Bench / ScienceAgent-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-Bench / ScienceAgent-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks evaluating LLMs and agents on chemistry, biology, and social science tasks (causal graph discovery and multidisciplinary agent tasks), showing LLMs perform well only when task complexity is limited and tailored workflows are used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-Bench (Chen et al., 2025b)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Auto-Bench / ScienceAgent-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Natural sciences (chemistry, biology), social science</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Evaluates agent generation of causal graphs, experimental plans, and domain-specific analyses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task-specific benchmarks with ground-truth causal graphs, expert-curated analytics, and agent framework execution (CodeAct, self-debug) comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Task complexity and domain specificity used to stratify novelty; benchmarks expose performance on limited-complexity vs complex exploratory tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>LLMs perform effectively only on low-complexity tasks; fail or underperform on complex, open-ended natural science discovery tasks without specialized workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Benchmark validation shows declining reliability as task complexity and novelty increase; tailored workflows improve outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Strong negative effect: validation and correctness decline with task novelty and complexity in natural sciences</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Supports asymmetry: generation may propose plausible experiments but validation and successful execution in complex domains remain limited</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poorer on multidisciplinary or novel experimental tasks compared to constrained problems</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Domain-specific simulation or experimental planning incurs higher validation costs; no numeric details provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Tailored agent workflows, domain-specific tool integration (e.g., CodeAct), and expert-curated analytics</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Auto-Bench and ScienceAgent-Bench show LLM agents can succeed on simple, narrow tasks but validation fails on complex, novel natural science problems unless specialized agent workflows and domain tools are applied.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2165.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProtAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProtAgents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework for automating protein design and biochemistry discovery by orchestrating multiple LLM agents and tools to propose and evaluate protein sequences/designs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProtAgents (Ghafarollahi and Buehler, 2024a)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ProtAgents</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-agent LLM framework integrating domain tools</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Protein design / biochemistry</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates protein designs, sequence proposals, and experimental design suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Integrates domain-specific evaluation tools (e.g., sequence scoring, simulation) and multi-agent consensus; likely uses downstream simulation or experimental validation pipelines (described qualitatively)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Domain-grounded novelty via divergence from known sequences and predicted function; specific metrics not reported</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Enables semi-autonomous protein design workflows; performance depends strongly on integrated domain evaluators and is not quantified in survey</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Relies on domain tools (simulations/prediction models) and human/experimental follow-up; accuracy varies with fidelity of downstream validators</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Novel designs require higher-fidelity validation (simulations/experiments) and are less reliably validated by model-internal checks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — generative outputs can be produced readily but require expensive domain validation pipelines to confirm novelty and correctness</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; expected to degrade for sequences/functions far from training distributions without experimental confirmation</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High when involving molecular simulations or wet-lab experiments; significant resource requirements compared to text generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Integration with domain-specific evaluation tools, multi-agent consensus, and pipeline connection to simulations/experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ProtAgents shows that domain-specific multi-agent systems can propose novel protein designs, but robust validation depends on expensive downstream simulation/experimental pipelines, highlighting a generation-validation gap for novel designs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2165.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2165.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReviewerGPT / ClaimCheck</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReviewerGPT and ClaimCheck</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems exploring LLMs as paper reviewers and as evaluators of claims in scientific papers; studies indicate LLMs can detect some inserted errors and provide critiques but grounding and claim verification remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReviewerGPT? an exploratory study on using large language models for paper reviewing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReviewerGPT / ClaimCheck</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based critique/review systems</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Scientific peer review / claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates review comments, identifies potential errors, critiques claims and methodology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation via inserted-error detection studies, comparison to human reviewers, and grounding checks (ClaimCheck analyzes groundedness of critiques)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not central; measures focus on correctness of critique and grounding rather than novelty</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Can find deliberately inserted errors and provide useful critiques in some contexts, but performance is variable and often less comprehensive than human reviewers</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Critique grounding is limited; ClaimCheck shows issues with grounding of LLM critiques and potential unverified assertions</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Not directly assessed; novel/unfamiliar methods in papers are more likely to lead to ungrounded or incorrect LLM critiques</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — LLMs can generate plausible-sounding critiques but may lack evidence-based validation, increasing risk of false positives/negatives in review</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades when reviewing unfamiliar domains or highly novel research</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; ClaimCheck highlights issues with groundedness and reliability</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Moderate; running reviews is cheap compared to experimental validation but verifying claims requires external retrieval/computation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combining retrieval-augmentation, evidence citation checks, and human oversight</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReviewerGPT and ClaimCheck studies reveal that while LLMs can assist reviewing, they often produce ungrounded critiques and struggle to reliably validate novel claims without evidence retrieval and human oversight.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search <em>(Rating: 2)</em></li>
                <li>MLAgentBench: Evaluating language agents on machine learning experimentation <em>(Rating: 2)</em></li>
                <li>LLM-SR: Scientific equation discovery via programming with large language models. <em>(Rating: 2)</em></li>
                <li>LLM-SRBench: A new benchmark for scientific equation discovery with large language models. <em>(Rating: 2)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants. <em>(Rating: 2)</em></li>
                <li>DiscoveryWorld: A virtual environment for developing and evaluating automated scientific discovery agents. <em>(Rating: 2)</em></li>
                <li>MLRC-Bench: Can language agents solve machine learning research challenges? <em>(Rating: 2)</em></li>
                <li>Verification and refinement of natural language explanations through llm-symbolic theorem proving. <em>(Rating: 2)</em></li>
                <li>ReviewerGPT? an exploratory study on using large language models for paper reviewing. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2165",
    "paper_id": "paper-278769498",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "AI Scientist (v1)",
            "name_full": "The AI Scientist",
            "brief_description": "An agentic LLM-based system that autonomously generates research ideas, produces code, executes experiments, and writes draft research papers, using internal scoring and external literature checks to evaluate proposals.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (v1)",
            "system_type": "LLM-based multi-agent / agentic system",
            "domain": "Artificial intelligence / automated scientific research",
            "generation_capability": "Research ideas, research proposals, code for experiments, experimental plans, draft research papers",
            "validation_method": "Internal self-assessment scoring (interestingness, novelty, feasibility) combined with external literature checks (Semantic Scholar) and automated internal reviewers/LLM evaluators for experimental choices; VLMs used to critique figures",
            "novelty_measure": "Self-assessed novelty scores and literature overlap checks against Semantic Scholar (bibliographic novelty assessment)",
            "generation_performance": "Reported as capable of generating diverse research proposals and automated experiments qualitatively; no numeric success rates given in survey; described as generative and able to produce outputs that warrant downstream checking",
            "validation_performance": "Qualitative internal validation via scoring and literature checks; no numerical accuracy/precision metrics reported in survey",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "System includes explicit novelty scoring and literature checks, implying validation is more challenging for higher-novelty items and relies on external literature grounding to detect prior art or obvious errors",
            "generation_validation_asymmetry": "Evidence of asymmetry: strong generative capability but validation relies on heuristic scoring and external lookup, suggesting generation can outpace rigorous validation especially for novel ideas",
            "out_of_distribution_performance": "Not quantified; implied that novel, out-of-distribution research topics require additional literature grounding and are harder to validate",
            "calibration_quality": "Not reported; self-assessment scores used but no calibration statistics provided",
            "validation_computational_cost": "Validation involves additional literature queries and internal multi-model critiques (LLM evaluators, VLMs), implying higher computational cost than generation but no quantitative cost reported",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Internal multi-model reviewers, literature-grounding (Semantic Scholar checks), and multi-modal critiques (VLMs); human oversight recommended",
            "evidence_type": "mixed",
            "key_findings": "AI Scientist (v1) demonstrates strong generative capabilities and uses internal scoring plus literature checks to assess novelty and feasibility, but validation remains heuristic and human oversight is emphasized for novel outputs.",
            "uuid": "e2165.0"
        },
        {
            "name_short": "AI Scientist (v2)",
            "name_full": "The AI Scientist-v2",
            "brief_description": "A follow-up agentic system that performs workshop-level automated scientific discovery using agentic tree search and stronger literature-grounding during idea formulation to assess novelty and feasibility.",
            "citation_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (v2)",
            "system_type": "LLM-based multi-agent with agentic tree-search",
            "domain": "Artificial intelligence / automated scientific research",
            "generation_capability": "Generates diverse research proposals and procedural experiment plans from abstract prompts; can produce workshop-level research drafts",
            "validation_method": "Integrates literature review tools early in idea formulation to evaluate novelty; uses internal automated reviewers and iterative agentic search with feedback loops",
            "novelty_measure": "Literature-grounded novelty checks (early-stage literature review) and internal novelty/feasibility scoring",
            "generation_performance": "Described as more generative/diverse than v1 and able to produce varied proposals; no numeric metrics provided in survey",
            "validation_performance": "Improved through earlier literature grounding, but no quantitative validation metrics reported",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Explicitly addresses novelty by integrating literature review early, indicating validation difficulty increases with novelty and is partially mitigated by early grounding",
            "generation_validation_asymmetry": "Evidence of a remaining asymmetry: enhanced validation pipelines reduce but do not eliminate the gap between rich generation and robust validation on novel outputs",
            "out_of_distribution_performance": "Not quantified; design choices (agentic tree search + literature grounding) intended to improve OOD performance but limits remain",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Higher than generation due to tree-search and literature retrieval; exact costs not reported",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Agentic tree-search with iterative internal review, early literature grounding, multi-agent feedback",
            "evidence_type": "mixed",
            "key_findings": "AI Scientist-v2 improves novelty assessment by integrating literature review early and using agentic tree search, reducing but not eliminating the challenge of validating highly novel outputs.",
            "uuid": "e2165.1"
        },
        {
            "name_short": "Agent Laboratory",
            "name_full": "Agent Laboratory: Using LLM agents as research assistants",
            "brief_description": "A multi-agent system that conducts literature review, experimental planning, report writing, and iterative research with human feedback loops, emphasizing human-defined research objectives as starting points.",
            "citation_title": "Agent laboratory: Using llm agents as research assistants.",
            "mention_or_use": "mention",
            "system_name": "Agent Laboratory",
            "system_type": "LLM multi-agent framework",
            "domain": "Artificial intelligence / research assistance",
            "generation_capability": "Literature summaries, experiment plans, data analyses, reports",
            "validation_method": "Human-in-the-loop validation at macro levels; iterative human feedback triggers re-evaluation and hypothesis regeneration if needed",
            "novelty_measure": "Implicit: starts from human-defined objectives and identifies gaps in literature; novelty assessed via human expert judgement and literature exploration",
            "generation_performance": "Useful for literature-driven idea generation and task orchestration; no numeric metrics provided",
            "validation_performance": "Relies heavily on human oversight; automation supports internal checks but quantitative performance not reported",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "High novelty often shifts the burden to human experts for macro-level validation; automated validation less reliable for novel directions",
            "generation_validation_asymmetry": "Supports gap theory: agents generate candidate directions but validation, especially for novel ideas, requires substantial human intervention",
            "out_of_distribution_performance": "Not quantified; performance depends on human guidance and literature availability",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Validation cost dominated by human time and iterative review cycles; computational cost of agent orchestration present but unspecified",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human-in-the-loop oversight, iterative feedback loops, ability to revert to hypothesis regeneration",
            "evidence_type": "supports",
            "key_findings": "Agent Laboratory illustrates that multi-agent systems can autonomously conduct many research tasks but rely on human oversight for high-level validation and novel hypothesis vetting.",
            "uuid": "e2165.2"
        },
        {
            "name_short": "Zochi",
            "name_full": "Zochi (IntologyAI)",
            "brief_description": "A commercial LLM-based system with customizable workflows that integrates human expertise at macro levels to guide decision-making and enable hypothesis re-evaluation and regeneration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Zochi",
            "system_type": "Commercial LLM agent / workflow system",
            "domain": "Artificial intelligence / automated research workflows",
            "generation_capability": "Hypothesis generation, experiment design, data analysis, decision recommendations",
            "validation_method": "Hybrid: automated internal checks plus explicit human expert guidance for macro-level validation and structural re-evaluation",
            "novelty_measure": "Human expert assessment and workflow-driven literature exploration; no formal novelty metric specified",
            "generation_performance": "Reported qualitatively as capable of generating and iterating research designs; no quantitative metrics provided",
            "validation_performance": "Relies on human experts for high-level validation; automated components support but do not replace human judgment",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "High novelty prompts human experts to re-evaluate or regenerate hypotheses, showing validation degrades for novel outputs without human input",
            "generation_validation_asymmetry": "Yes — system generates autonomously but validation for novel claims depends on human expertise",
            "out_of_distribution_performance": "Not reported; human oversight intended to manage OOD cases",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Human time is a major cost; system computational validation unspecified",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human-in-the-loop oversight, customizable workflows permitting macro-level human interventions",
            "evidence_type": "supports",
            "key_findings": "Zochi demonstrates a practical hybrid approach where autonomy is balanced by human expert validation, particularly necessary for novel or strategic research redirections.",
            "uuid": "e2165.3"
        },
        {
            "name_short": "Explanation-Refiner",
            "name_full": "Explanation-Refiner (LLM + theorem proving)",
            "brief_description": "A system that verifies and refines LLM-generated natural language explanations by interfacing with symbolic theorem provers to check logical correctness and refine hypotheses.",
            "citation_title": "Verification and refinement of natural language explanations through llm-symbolic theorem proving.",
            "mention_or_use": "mention",
            "system_name": "Explanation-Refiner",
            "system_type": "Hybrid LLM + symbolic theorem prover (neurosymbolic)",
            "domain": "Automated hypothesis refinement / formal verification of explanations",
            "generation_capability": "Natural language hypotheses/explanations, refined and formalized by theorem-prover-guided loops",
            "validation_method": "Formal symbolic verification using theorem provers to check logical consistency and to drive hypothesis refinement",
            "novelty_measure": "Not explicitly quantified; validation emphasizes formal provability rather than novelty distance from training data",
            "generation_performance": "Enables production of hypotheses that can be formalized and tested; no success rates reported in survey",
            "validation_performance": "Stronger validation for statements that can be formalized; limited applicability when hypotheses are empirical or not easily formalized",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Formal verification remains effective when statements map to provable formal theories; novel empirical hypotheses that lack formal grounding are harder to verify",
            "generation_validation_asymmetry": "Partially reduced for logically formalizable outputs — the system can close the gap via theorem proving — but for many empirical discoveries a gap remains",
            "out_of_distribution_performance": "Not reported; likely limited when novel claims cannot be expressed in the formal system used",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Theorem proving can be computationally expensive depending on complexity; no numeric estimates provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Integration of symbolic theorem provers with LLM outputs to formally verify and refine explanations",
            "evidence_type": "mixed",
            "key_findings": "Explanation-Refiner shows that combining LLMs with symbolic provers can materially improve validation for formalizable claims, but many scientific hypotheses remain outside this scope and still need human/evidence-based validation.",
            "uuid": "e2165.4"
        },
        {
            "name_short": "LLM-SR",
            "name_full": "LLM-SR: Scientific equation discovery via programming with large language models",
            "brief_description": "An LLM-based approach to symbolic regression and equation discovery that leverages LLM prior knowledge and clustered memory feedback to discover governing equations from data.",
            "citation_title": "LLM-SR: Scientific equation discovery via programming with large language models.",
            "mention_or_use": "mention",
            "system_name": "LLM-SR",
            "system_type": "LLM-augmented symbolic regression (hybrid data-driven + LLM)",
            "domain": "Function discovery / symbolic regression / physics & statistics",
            "generation_capability": "Generates symbolic equations/models that explain observational data",
            "validation_method": "Benchmarked on function discovery tasks; uses clustered memory feedback (experience) and cross-checking with data fidelity; LLM-SRBench introduced to control for data contamination",
            "novelty_measure": "Benchmarks include function transformations to reduce contamination; novelty implicitly assessed by ability to rediscover known equations or find unseen forms",
            "generation_performance": "Reported improvements over naive LLM usage for symbolic regression in cited works, but overall performance varies and is benchmark-dependent; no single numeric success rate reported in survey",
            "validation_performance": "Validation primarily via fit to data and benchmark ground-truth comparison in LLM-SRBench; effectiveness depends on data quality and contamination controls",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Benchmarks (LLM-SRBench) try to mitigate contamination; performance drops when functions are transformed to be out-of-distribution relative to training data, indicating validation and discovery are harder for novel forms",
            "generation_validation_asymmetry": "Yes — LLMs can propose plausible symbolic forms but rigorous validation requires data-fitting and contamination controls; novel equations are harder to validate",
            "out_of_distribution_performance": "Degrades for transformed or novel function forms introduced to the benchmark to avoid data leakage",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Validation requires fitting candidate symbolic forms to data and running benchmarking pipelines, implying higher computational cost; specifics not provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Clustered memory feedback, benchmarked transformations (LLM-SRBench) to detect contamination, data-fit comparisons to ground-truth",
            "evidence_type": "supports",
            "key_findings": "LLM-SR shows promise for equation discovery but performance and validation reliability drop when evaluating functions that differ from training data; benchmarks introduced to reduce contamination highlight sensitivity to novelty.",
            "uuid": "e2165.5"
        },
        {
            "name_short": "LLM-SRBench",
            "name_full": "LLM-SRBench: A new benchmark for scientific equation discovery with large language models",
            "brief_description": "A benchmark designed to evaluate LLMs on function discovery tasks while mitigating data contamination by including function transformations and controlled test sets.",
            "citation_title": "LLM-SRBench: A new benchmark for scientific equation discovery with large language models.",
            "mention_or_use": "mention",
            "system_name": "LLM-SRBench",
            "system_type": "Benchmark / evaluation framework",
            "domain": "Function discovery / symbolic regression evaluation",
            "generation_capability": "Not a generator; evaluates generators' ability to produce true underlying functions",
            "validation_method": "Controlled benchmark with function transformations to prevent contamination, ground-truth comparison of discovered equations, and standardized evaluation metrics (not enumerated in survey)",
            "novelty_measure": "Explicitly creates out-of-distribution variants via function transformations to measure novelty sensitivity",
            "generation_performance": "Used to show LLM-based SR methods struggle when functions are transformed (OOD); no numeric aggregate metrics provided in survey",
            "validation_performance": "Enables stricter validation by removing training-set leakage; demonstrates reduced false positive discoveries when contamination controlled",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Designed to reveal degradation in generation and validation performance on OOD / transformed functions; novelty strongly degrades measured performance",
            "generation_validation_asymmetry": "Benchmarks document that generation success can be inflated by contamination, so careful benchmarking reveals asymmetry where apparent generative success is not matched by robust validation",
            "out_of_distribution_performance": "Specifically measures OOD degradation and shows substantial performance drop when transformations are applied",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Benchmarks require additional data transformations and evaluation compute; no quantitative costs provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Controlled OOD benchmarks, transformation-based contamination mitigation, ground-truth equation matching",
            "evidence_type": "supports",
            "key_findings": "LLM-SRBench demonstrates that prior apparent successes in equation discovery can be driven by data contamination and that OOD/transformed functions expose substantial generation-validation gaps.",
            "uuid": "e2165.6"
        },
        {
            "name_short": "MLAgentBench",
            "name_full": "MLAgentBench: Evaluating language agents on machine learning experimentation",
            "brief_description": "A benchmark assessing LLM agents' ability to design and run ML experiments, finding that performance is often contingent on task familiarity and that agents struggle on novel research challenges.",
            "citation_title": "MLAgentBench: Evaluating language agents on machine learning experimentation.",
            "mention_or_use": "mention",
            "system_name": "MLAgentBench",
            "system_type": "Benchmark / evaluation framework",
            "domain": "Machine learning research automation",
            "generation_capability": "Evaluates generation of ML experiment designs, model configurations, and experimental code by agents",
            "validation_method": "Benchmarks agents by running generated experiments and comparing results to target tasks or baselines; measures success on tasks of varying familiarity",
            "novelty_measure": "Task familiarity vs novelty stratification; benchmarks include novel ML research challenges to test generalization",
            "generation_performance": "Performance is often good on familiar tasks but degrades on novel ML research challenges (qualitative statement in survey; no numeric rates provided)",
            "validation_performance": "Validation conducted via execution of experiments; validation reliability drops when tasks are novel due to incorrect experiment design or misinterpretation",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Clear: validation/execution success and correctness degrade on novel tasks; agents perform better on familiar tasks",
            "generation_validation_asymmetry": "Supports asymmetry: agents can produce plausible designs for familiar problems but fail to generate and validate correct solutions for novel problems",
            "out_of_distribution_performance": "Degrades significantly for out-of-distribution ML research challenges compared to in-distribution/familiar tasks",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Running generated experiments is computationally expensive compared to pure text generation; cost increases with complexity of task and novelty due to iterative debugging",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Iterative refinement frameworks (IMPROVE), curated expert collaboration, budgeted human-in-the-loop interventions",
            "evidence_type": "supports",
            "key_findings": "MLAgentBench shows that LLM agents perform acceptably on familiar ML tasks but struggle on novel research challenges, revealing a substantial drop in both generation and validation reliability for OOD tasks.",
            "uuid": "e2165.7"
        },
        {
            "name_short": "MLRC-Bench",
            "name_full": "MLRC-Bench: Can language agents solve machine learning research challenges?",
            "brief_description": "A benchmark probing language agents' ability to tackle novel ML research problems and comparing agent R&D capabilities against human experts.",
            "citation_title": "Mlrc-bench: Can language agents solve machine learning research challenges?",
            "mention_or_use": "mention",
            "system_name": "MLRC-Bench",
            "system_type": "Benchmark / evaluation framework",
            "domain": "Machine learning research evaluation",
            "generation_capability": "Evaluates generation of novel ML research solutions, experimental plans, and analyses",
            "validation_method": "Compares agent outputs against expert solutions and measures ability to solve novel ML challenges; includes human expert comparison",
            "novelty_measure": "Explicitly includes novel/unseen ML research tasks to quantify OOD capability relative to experts",
            "generation_performance": "Survey reports that benchmarks probe limits and often show agents lag behind human experts on novel R&D tasks; no numeric results in survey",
            "validation_performance": "Validation via expert comparison and task success; agents underperform on novel tasks compared to human experts",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance and validation reliability drop for novel tasks; agents show competence on familiar tasks but not on frontier research problems",
            "generation_validation_asymmetry": "Supports asymmetry: generation abilities are insufficient to match validation and problem-solving levels demonstrated by humans on novel research challenges",
            "out_of_distribution_performance": "Lower than in-distribution; agents struggle to generalize to novel ML research problems",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "High: evaluation against expert baselines and running experiments is computationally and labor intensive",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human expert benchmarking, curated collaboration frameworks, iterative refinement",
            "evidence_type": "supports",
            "key_findings": "MLRC-Bench highlights that language agents lag behind humans on novel ML research challenges, demonstrating a pronounced generation-validation gap on OOD tasks.",
            "uuid": "e2165.8"
        },
        {
            "name_short": "DiscoveryBench / DiscoveryWorld",
            "name_full": "DiscoveryBench / DiscoveryWorld",
            "brief_description": "Platforms/benchmarks creating virtual environments and datasets for LLM agents to perform simplified scientific exploration, used to evaluate agentic discovery capabilities and limitations on novel tasks.",
            "citation_title": "DiscoveryWorld: A virtual environment for developing and evaluating automated scientific discovery agents.",
            "mention_or_use": "mention",
            "system_name": "DiscoveryWorld / DiscoveryBench",
            "system_type": "Benchmark / simulated environment",
            "domain": "General scientific discovery evaluation",
            "generation_capability": "Evaluates generation of experimental plans, hypotheses, and exploration strategies in a controlled virtual environment",
            "validation_method": "Automated environment outcomes and ground-truth checks in the simulated domain; allows controlled OOD testing",
            "novelty_measure": "Environment designs enable testing on novel tasks vs. familiar patterns; novelty measured by task variation and unseen scenarios",
            "generation_performance": "Agents can perform simplified discovery tasks in simulation; performance drops when tasks increase in complexity or novelty (qualitative)",
            "validation_performance": "Automated outcome-based validation works well in simulation for in-distribution tasks but is less informative for truly novel scientific claims",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Validation in simulated settings can detect failures on novel tasks, but simulation-to-reality transfer for real discovery remains a challenge",
            "generation_validation_asymmetry": "Supports asymmetry in real-world contexts: success in simulated generation does not guarantee valid real-world discoveries",
            "out_of_distribution_performance": "Degrades with task novelty and complexity within simulated benchmarks",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Simulation runs can be computationally expensive depending on environment fidelity; no concrete numbers provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Controlled environments, task stratification by novelty, and ground-truth outcome checks in simulation",
            "evidence_type": "supports",
            "key_findings": "DiscoveryWorld/DiscoveryBench permit controlled measurement of agent discovery abilities and reveal that agents that succeed in simulation often fail to validate or transfer to real, novel scientific discovery tasks.",
            "uuid": "e2165.9"
        },
        {
            "name_short": "Auto-Bench / ScienceAgent-Bench",
            "name_full": "Auto-Bench / ScienceAgent-Bench",
            "brief_description": "Benchmarks evaluating LLMs and agents on chemistry, biology, and social science tasks (causal graph discovery and multidisciplinary agent tasks), showing LLMs perform well only when task complexity is limited and tailored workflows are used.",
            "citation_title": "Auto-Bench (Chen et al., 2025b)",
            "mention_or_use": "mention",
            "system_name": "Auto-Bench / ScienceAgent-Bench",
            "system_type": "Benchmark / evaluation framework",
            "domain": "Natural sciences (chemistry, biology), social science",
            "generation_capability": "Evaluates agent generation of causal graphs, experimental plans, and domain-specific analyses",
            "validation_method": "Task-specific benchmarks with ground-truth causal graphs, expert-curated analytics, and agent framework execution (CodeAct, self-debug) comparisons",
            "novelty_measure": "Task complexity and domain specificity used to stratify novelty; benchmarks expose performance on limited-complexity vs complex exploratory tasks",
            "generation_performance": "LLMs perform effectively only on low-complexity tasks; fail or underperform on complex, open-ended natural science discovery tasks without specialized workflows",
            "validation_performance": "Benchmark validation shows declining reliability as task complexity and novelty increase; tailored workflows improve outcomes",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Strong negative effect: validation and correctness decline with task novelty and complexity in natural sciences",
            "generation_validation_asymmetry": "Supports asymmetry: generation may propose plausible experiments but validation and successful execution in complex domains remain limited",
            "out_of_distribution_performance": "Poorer on multidisciplinary or novel experimental tasks compared to constrained problems",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "Domain-specific simulation or experimental planning incurs higher validation costs; no numeric details provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Tailored agent workflows, domain-specific tool integration (e.g., CodeAct), and expert-curated analytics",
            "evidence_type": "supports",
            "key_findings": "Auto-Bench and ScienceAgent-Bench show LLM agents can succeed on simple, narrow tasks but validation fails on complex, novel natural science problems unless specialized agent workflows and domain tools are applied.",
            "uuid": "e2165.10"
        },
        {
            "name_short": "ProtAgents",
            "name_full": "ProtAgents",
            "brief_description": "A multi-agent framework for automating protein design and biochemistry discovery by orchestrating multiple LLM agents and tools to propose and evaluate protein sequences/designs.",
            "citation_title": "ProtAgents (Ghafarollahi and Buehler, 2024a)",
            "mention_or_use": "mention",
            "system_name": "ProtAgents",
            "system_type": "Multi-agent LLM framework integrating domain tools",
            "domain": "Protein design / biochemistry",
            "generation_capability": "Generates protein designs, sequence proposals, and experimental design suggestions",
            "validation_method": "Integrates domain-specific evaluation tools (e.g., sequence scoring, simulation) and multi-agent consensus; likely uses downstream simulation or experimental validation pipelines (described qualitatively)",
            "novelty_measure": "Domain-grounded novelty via divergence from known sequences and predicted function; specific metrics not reported",
            "generation_performance": "Enables semi-autonomous protein design workflows; performance depends strongly on integrated domain evaluators and is not quantified in survey",
            "validation_performance": "Relies on domain tools (simulations/prediction models) and human/experimental follow-up; accuracy varies with fidelity of downstream validators",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Novel designs require higher-fidelity validation (simulations/experiments) and are less reliably validated by model-internal checks",
            "generation_validation_asymmetry": "Yes — generative outputs can be produced readily but require expensive domain validation pipelines to confirm novelty and correctness",
            "out_of_distribution_performance": "Not reported; expected to degrade for sequences/functions far from training distributions without experimental confirmation",
            "calibration_quality": "Not reported",
            "validation_computational_cost": "High when involving molecular simulations or wet-lab experiments; significant resource requirements compared to text generation",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Integration with domain-specific evaluation tools, multi-agent consensus, and pipeline connection to simulations/experiments",
            "evidence_type": "supports",
            "key_findings": "ProtAgents shows that domain-specific multi-agent systems can propose novel protein designs, but robust validation depends on expensive downstream simulation/experimental pipelines, highlighting a generation-validation gap for novel designs.",
            "uuid": "e2165.11"
        },
        {
            "name_short": "ReviewerGPT / ClaimCheck",
            "name_full": "ReviewerGPT and ClaimCheck",
            "brief_description": "Systems exploring LLMs as paper reviewers and as evaluators of claims in scientific papers; studies indicate LLMs can detect some inserted errors and provide critiques but grounding and claim verification remain challenging.",
            "citation_title": "ReviewerGPT? an exploratory study on using large language models for paper reviewing.",
            "mention_or_use": "mention",
            "system_name": "ReviewerGPT / ClaimCheck",
            "system_type": "LLM-based critique/review systems",
            "domain": "Scientific peer review / claim verification",
            "generation_capability": "Generates review comments, identifies potential errors, critiques claims and methodology",
            "validation_method": "Evaluation via inserted-error detection studies, comparison to human reviewers, and grounding checks (ClaimCheck analyzes groundedness of critiques)",
            "novelty_measure": "Not central; measures focus on correctness of critique and grounding rather than novelty",
            "generation_performance": "Can find deliberately inserted errors and provide useful critiques in some contexts, but performance is variable and often less comprehensive than human reviewers",
            "validation_performance": "Critique grounding is limited; ClaimCheck shows issues with grounding of LLM critiques and potential unverified assertions",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Not directly assessed; novel/unfamiliar methods in papers are more likely to lead to ungrounded or incorrect LLM critiques",
            "generation_validation_asymmetry": "Yes — LLMs can generate plausible-sounding critiques but may lack evidence-based validation, increasing risk of false positives/negatives in review",
            "out_of_distribution_performance": "Degrades when reviewing unfamiliar domains or highly novel research",
            "calibration_quality": "Not reported; ClaimCheck highlights issues with groundedness and reliability",
            "validation_computational_cost": "Moderate; running reviews is cheap compared to experimental validation but verifying claims requires external retrieval/computation",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combining retrieval-augmentation, evidence citation checks, and human oversight",
            "evidence_type": "supports",
            "key_findings": "ReviewerGPT and ClaimCheck studies reveal that while LLMs can assist reviewing, they often produce ungrounded critiques and struggle to reliably validate novel claims without evidence retrieval and human oversight.",
            "uuid": "e2165.12"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "rating": 2
        },
        {
            "paper_title": "MLAgentBench: Evaluating language agents on machine learning experimentation",
            "rating": 2
        },
        {
            "paper_title": "LLM-SR: Scientific equation discovery via programming with large language models.",
            "rating": 2
        },
        {
            "paper_title": "LLM-SRBench: A new benchmark for scientific equation discovery with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants.",
            "rating": 2
        },
        {
            "paper_title": "DiscoveryWorld: A virtual environment for developing and evaluating automated scientific discovery agents.",
            "rating": 2
        },
        {
            "paper_title": "MLRC-Bench: Can language agents solve machine learning research challenges?",
            "rating": 2
        },
        {
            "paper_title": "Verification and refinement of natural language explanations through llm-symbolic theorem proving.",
            "rating": 2
        },
        {
            "paper_title": "ReviewerGPT? an exploratory study on using large language models for paper reviewing.",
            "rating": 2
        }
    ],
    "cost": 0.021605749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery
17 Sep 2025</p>
<p>Tianshi Zheng 
Zheye Deng 
Ting Hong 
Weiqi Tsang 
Jiaxin Wang 
Zihao Bai 
Yangqiu Wang 
Song 
Franck Cappello 
Sandeep Madireddy 
Neil Getty 
Nicholas Lee-Ping Chia 
Nesar Ramachandra 
Josh Nguyen 
Murat Keceli 
Tanwi Mallick 
Zilinghan Li 
Marieme Ngom 
Chenhui Zhang 
Angel Yanguas-Gil 
Evan Antoniuk 
Bhavya Kailkhura 
Minyang Tian 
Yufeng Du 
Thomas Carta 
Clément Romac 
Thomas Wolf 
Sylvain Lamprier 
Olivier Sigaud 
Pierre-Yves Oudeyer 
Jun Shern Chan 
Neil Chowdhury 
Oliver Jaffe 
James Aung 
Dane Sherburn 
Evan Mays 
Giulio Starace 
Kevin Liu 
Leon Maksin 
Tejal Patwardhan 
Lilian 
Ziru Chen 
Shijie Chen 
Yuting Ning 
Qianheng Zhang 
Boshi Wang 
Botao Yu 
Yifei Li 
Zeyi Liao 
Chen Wei 
Zitong Lu 
Vishal Dey 
Mingyi Xue 
Frazier N Baker 
Benjamin Burns 
Ming-Chang Cheng 
Yixuan Wang 
Yuchi Wang 
Yair Schiff 
Kevin Bello 
J Zico Kolter 
Yacine Jernite 
William Yang Wang 
Daniel Fried 
Tian-Li Yu 
Andrew Gordon Wilson 
Ioana Ciucȃ 
Yuan-Sen Ting 
Sandor Kruk 
Kartheik 2023 Iyer 
Kourosh Darvish 
Marta Skreta 
Yuchi Zhao 
Naruki Yoshikawa 
Sagnik Som 
Miroslav Bogdanovic 
Yang Cao 
Han Hao 
Haoping Xu 
Alán Aspuru-Guzik 
Fabio Dennstädt 
Johannes Zink 
Paul Martin Putora 
Janna Hastings 
Nikola 2024 Cihoric 
Jiangshu Du 
Yibo Wang 
Wenting Zhao 
Zhongfen Deng 
Shuaiqi Liu 
Renze Lou 
PranavHenry Peng Zou 
Narayanan Venkit 
MukundNan Zhang 
Ranran Zhang 
Vipul Gupta 
Yinghui Li 
Tao Li 
Fei Wang 
Qin Liu 
Tianlin Liu 
Pengzhi Gao </p>
<p>Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>https://github.com/HKUST
KnowComp/Awesome-LLM-Scientific-Discovery</p>
<p>Daniel Adu-Ampratwum
Song GaoXuhui Huang, Xia Ning, Yu Su</p>
<p>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery
17 Sep 2025E242DB66DCCACDA2B57AD40A1450C0C6arXiv:2505.13259v3[cs.CL]Literature Search and RetrievalLiterature SynthesisStructuring and Organization table: Towards information integration in text-to-table generation via global tuple extraction. PreprintarXiv:2404.14215
Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration.This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science.Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle.We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance.Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement.</p>
<p>Introduction</p>
<p>The relentless advancement of Large Language Models (LLMs) has unlocked a suite of emergent abilities, such as planning (Huang et al., 2024b), complex reasoning (Huang and Chang, 2023), and instruction following (Qin et al., 2024).Moreover, integrating agentic workflows enables LLM-based systems to perform advanced functions, including web navigation (He et al., 2024), tool use (Qu et al., 2025), code execution (Jiang et al., 2024a), and data analytics (Sun et al., 2024).In scientific discovery, this convergence of advanced LLM capabilities and agentic functionalities is catalyzing a significant paradigm shift.This shift is poised not only to accelerate the research lifecycle but also to fundamentally alter the collaborative dynamics between human researchers and artificial intelligence in the pursuit of knowledge.</p>
<p>However, this rapid expansion of LLM applications and the ongoing paradigm shift in scientific discovery present notable challenges.The accelerated pace of LLM evolution and their deepening integration into complex research complicate systematic assessment, necessitating conceptual frameworks to structure current understanding and chart future directions.While existing surveys have provided valuable overviews of LLMs in various scientific domains (Zhang et al., 2024(Zhang et al., , 2025a) ) or have cataloged particular AI techniques for science (Luo et al., 2025;Reddy and Shojaee, 2025), they often focus on discipline-specific applications or a static snapshot of LLM capabilities.Consequently, existing reviews may overlook the crucial trend of increasing LLM autonomy and their evolving roles across the entire scientific method, leaving their comprehensive impact and trajectory towards greater independence underexplored.</p>
<p>To systematically chart this evolving landscape and address the identified gap, we anchor our analysis in the six stages (Figure 1) of the established scientific method (Popper, 1935;Kuhn, 1962): (1) observation and problem definition, (2) hypothesis development, (3) experimentation and data collection, (4) data analysis and interpretation, (5) drawing conclusions, and ( 6) iteration and refinement.Our examination of LLM applications across these stages reveals a significant trend: LLMs are progressing from performing discrete, task-oriented functions within a single stage to deployment in sophisticated, multi-stage agentic workflows.Notably, emerging research (Schmidgall et al., 2025;Yamada et al., 2025) now explores developing LLM-based systems capable of autonomously navigating nearly all these stages.To effectively capture and delineate this trajectory of increasing capability and independence, we introduce a foundational three-level taxonomy for LLM involvement in scientific discovery (Table 1): (i) LLM as Tool, where models augment human researchers by performing specific, well-defined tasks under direct supervision; (ii) LLM as Analyst, where models exhibit greater autonomy in processing complex information, conducting analyses, and offering insights with reduced human intervention; and (iii) LLM as Scientist, representing a more advanced stage where LLM-based systems can autonomously conduct major research stages, from formulating hypotheses to interpreting results and suggesting new avenues of inquiry.</p>
<p>Building upon this taxonomic framework, we further identify critical gaps in the current research landscape and highlight pivotal challenges and future trajectories for the field, including: (1) fully autonomous discovery cycles for evolving scientific inquiry without human intervention; (2) robotic automation for interaction in the physical world for laboratory experimentation; (3) continuous selfimprovement and adaptation from past research experiences; (4) transparency and interpretability of LLM-conducted research; and (5) ethical governance and societal alignment.Addressing these multifaceted challenges will be crucial for achieving a future where AI acts as a transformative partner in scientific exploration.</p>
<p>This survey focuses on LLM-based systems in scientific discovery, particularly their varying levels of autonomy.While acknowledging the broad impact of LLMs in science, we deliberately narrow our scope to exclude research on general-purpose scientific LLMs or LLMs for domain-specific scientific knowledge acquisition and reasoning, which are well covered in existing surveys (Zhang et al., 2024(Zhang et al., , 2025a)).The remainder of this paper is organized as follows: Section 2 details our taxonomy and its interaction with the scientific method.Section 3 presents LLM as Tool applications, categorized by scientific method stages.Section 4 examines LLM as Analyst works by scientific domain, while Section 5 analyzes LLM as Scientist systems, focusing on their idea development and refinement strategies.Section 6 explores challenges and future directions.</p>
<p>Three Levels of Autonomy</p>
<p>Table 1 illustrates the three levels of autonomy in LLM-based scientific discovery with their associated features.In this section, we discuss their applications and characteristics in more detail.</p>
<p>LLM as Tool (Level 1).Level 1 represents the most foundational application of LLMs in scientific discovery.At this stage, LLMs function primarily as tailored tools under direct human supervision, designed to execute specific, well-defined tasks within a single stage of the scientific method.Their role is to augment human capabilities by automating or accelerating discrete activities such as literature summarization, drafting initial text for manuscripts, generating code snippets for data processing, or reformatting datasets.The autonomy of LLMs at this level is limited; they operate based on explicit human prompts and instructions, with outputs typically requiring human validation and integration into the broader research workflow.The primary goal is to enhance researcher efficiency and reduce routine task burdens.</p>
<p>LLM as Analyst (Level 2).In Level 2, LLMs exhibit a greater degree of autonomy and move beyond purely static, task-oriented applications.Here, LLMs function as passive agents, capable of more  (Jiang et al., 2024) ArxivDIGESTables (Newman et al., 2024) arXiv2Table (Wang et al., 2025)</p>
<p>Machine Learning Research</p>
<p>The AI Scientist (Lu et al., 2024) The AI Scientist-v2 (Yamada et al., 2025) ProtAgents (Ghafarollahi and Buehler, 2024) Agent Laboratory (Schmidgall et al., (Yin et al., 2022) LeGIT (Li et al., 2025) ChartQA (Masry et al., 2022) CharXiv (Wang et al., 2024) TableBench (Wu et al., 2024) Deng et al., 2024 Chain-of-Table (Wang et al., 2024) ChartX &amp; ChartVLM (Xia et al., 2024) AutomaTikZ (Belouadi et al., 2023) Text2Chart31 (Zadeh et al., 2024) ClaimCheck (Ou et al., 2025) ReviewCritique (Du et al., 2024) ReviewerGPT (Liu et al., 2023) RR-MCQ (Zhou et al., 2024) SciReplicate-Bench (Xiang et al., 2025) Tyser et al., 2024Takagi et al., 2023 CycleResearcher (Weng et al., 2024) Xu et al., 2025 Explanation-Refiner (Quan et al., 2024) Chain-of-Ideas (Li et al., 2024) MC-NEST (Rabby et al., 2025) Fully-Autonomous Cycle Self-Improvement Robotic Automation Transparency Ethics . . .LLM-SR (Shojaee et al., 2024) LLM-SRBench (Shojaee et al., 2025) Gravity-Bench-v1 (Koblischke et al., 2025) BoxLM (Li et al., 2024) InfiAgent-DABench (Hu et al., 2024) DS-Agent (Guo et al., 2024) BLADE (Gu et al., 2024) DAgent (Xu et al., 2025</p>
<p>LLM as Scientist</p>
<p>LLM as Analyst</p>
<p>LLM as Tool</p>
<p>Evolution of LLM-Based Scientific Discovery</p>
<p>FutureHouse (Skarlinski et al., 2025) PaperQA (Lála et al., 2023) Science Hierarchography (Gao et al., 2025) AI co-scientist (Gottweis et al., 2025) MLRC-Bench (Zhang et al., 2025) RE-Bench (Wijk et al., 2025) Figure 2: Taxonomy of research works in LLM-based scientific discovery with detailed categorization.complex information processing, data modeling, and analytical reasoning with reduced human intervention for intermediate steps.While still operating within boundaries set by human researchers, these systems can independently manage sequences of tasks, such as analyzing experimental datasets to identify trends, interpreting outputs from complex simulations, or even performing iterative refinement of models.The human researcher typically defines the overall analytical goals, provides the necessary data, and critically evaluates the insights or interpretations generated by the LLM.</p>
<p>LLM as Scientist (Level 3).Level 3 applications signify a significant leap in autonomy, where LLM-based systems operate as active agents capable of orchestrating and navigating multiple stages of the scientific discovery process with considerable independence.These systems can demonstrate initiative in formulating hypotheses, planning and executing experiments, analyzing the resultant data, drawing preliminary conclusions, and potentially proposing subsequent research questions or avenues for exploration.LLM-based systems at this level can drive substantial portions of the research cycle, conducting scientific discovery with minimal human intervention.</p>
<p>Collectively, we present our full taxonomy with detailed categorization in Figure 2, which consol-idates research works within our focused scope across all three levels of autonomy.</p>
<p>3 Level 1. LLM as Tool (Table A1)</p>
<p>In this section, we introduce Level 1 research works in LLM-based scientific discovery, categorized by the stages in the scientific method they address.</p>
<p>Literature Review and Information Gathering</p>
<p>Literature Review Automatic literature search and retrieval is crucial for identifying research gaps and formulating research questions.To address these challenges, AIDE (Jiang et al., 2025) proposed enhancing complex code generation capabilities by adopting tree-search methodologies for code optimization.</p>
<p>Data Analysis and Organization</p>
<p>Tabular Data In this stage, LLMs assist the scientific workflow by automating processes related to data organization, presentation, and analysis.For data presented in tabular format, Chain-of-</p>
<p>Conclusion and Hypothesis Validation</p>
<p>In the concluding stages of research, LLMs can provide feedback on, or verify, claims and conclusions derived from experiments.</p>
<p>Paper Review In this context, a significant focus of contemporary research involves investigating the utility of LLMs as reviewers for artificial intelligence papers.ReviewerGPT (Liu and Shah, 2023) initially explored the capability of LLMs to identify deliberately inserted errors within research papers, highlighting the necessity for more robust systems to conduct comprehensive reviews.(Wen et al., 2025).Furthermore, Xu et al. (2025c) have navigated this domain into physics research, aiming to enhance the interpretability of the discovery process through the use of multi-agent workflows.</p>
<p>Iteration and Refinement</p>
<p>The iterative refinement of research hypotheses, as a distinct area of investigation, has received comparatively less attention in current research.Explanation-Refiner (Quan et al., 2024) employed theorem provers to verify and subsequently refine LLM-generated hypotheses.Chain-of-Idea (Li et al., 2024a)  4 Level 2: LLM as Analyst (Table A2)</p>
<p>In this section, we introduce Level 2 research works in LLM-based scientific discovery, categorized according to their task nature and domains.</p>
<p>Machine Learning Research</p>
<p>Automated Machine Learning (AutoML) (Shen et al., 2024) endeavors to generate high-performing modeling configurations for a given task in a datadriven manner.With the advent of LLM-based agents, several studies have explored their application in the automated modeling of machine learning (ML) tasks.A suite of benchmarks has emerged to track progress in this area.MLAgentBench (Huang et al., 2024a) evaluates the capabilities of LLMs in designing and executing ML experiments, revealing that performance is often contingent upon task familiarity.Similarly, MLRC-Bench (Zhang et al., 2025b) and RE-Bench (Wijk et al., 2024) further probe the limits of these agents, assessing their ability to solve novel ML research challenges and comparing their R&amp;D capabilities against human experts.MLGym (Nathani et al., 2025) offers valuable resource and benchmark for advancing these AI research agents.</p>
<p>To address the challenges posed by these benchmarks, various agentic frameworks have been proposed.The IMPROVE framework (Xue et al., 2025) highlighted the significance of iterative refinement mechanisms.CodeScientist (Jansen et al., 2025) incorporated an ML modeling agent with machine-generated ideas, while BudgetMLAgent (Gandhi et al., 2025) leveraged curated expert collaboration frameworks to achieve superior results with cost-effective models.More recent end-to-end systems like MLR-Copilot (Li et al., 2024d) and the multi-agent framework MLZero (Fang et al., 2025) aim for fully autonomous machine learning research and automation.Pushing the boundaries of automation even further, some work has explored the use of language models to directly propose LM architectures (Cheng et al., 2025a), moving beyond orchestration to direct model creation.</p>
<p>Data Modeling and Analysis</p>
<p>Automated data-driven analysis, encompassing statistical data modeling and hypothesis validation, represents a foundational application area for LLMassisted scientific discovery.InfiAgent-DABench (Hu et al., 2024b) benchmarked the capabilities of LLMs in static code generation and execution for data analysis using CSV files.Subsequent benchmarks, such as BLADE (Gu et al., 2024), Discov-eryBench (Majumder et al., 2024), and DSBench (Jing et al., 2024), have improved evaluation robustness by incorporating real-world research papers and expert-curated analytics to assess how far agents are from human expert performance.These studies indicate that most LLMs struggle with com-plex data analytics tasks, even when operating within an agent framework (Zheng et al., 2023).To address these challenges, DS-Agent (Guo et al., 2024b) proposes to enhance LLM performance by incorporating a case-based reasoning method to improve domain knowledge acquisition.In a related effort, DAgent (Xu et al., 2025b) extended the application domain to querying relational databases and enabled report generation using results derived from decomposed sub-problems.</p>
<p>Function Discovery</p>
<p>Function discovery, which aims to identify the underlying equations from observational data of variables, has been significantly influenced by the advancement of AI-driven symbolic regression (SR) (Udrescu and Tegmark, 2020;Kamienny et al., 2022).To enhance this process, LLM-SR (Shojaee et al., 2025a) leveraged the prior domain knowledge of LLMs and incorporated feedback from clustered memory storage, while DrSR (Wang et al., 2025a) proposed a dual reasoning framework that utilizes both data and experience for scientific equation discovery.To systematically assess these capabilities, LLM-SRBench (Shojaee et al., 2025b) introduced a benchmark for evaluating LLMs as function discovery agents, which incorporates function transformations to mitigate data contamination.Furthermore, other studies have explored the capabilities of LLMs in discovering complex models within specific domains, such as Physics (Koblischke et al., 2025), Statistics (Li et al., 2024b), and automated neural scaling law discovery (Lin et al., 2025).</p>
<p>Natural Science Research</p>
<p>Research has largely focused on applying LLMs to autonomous research workflows for natural science discovery.Auto-Bench (Chen et al., 2025b) evaluated LLMs on chemistry and social science tasks based on causal graph discovery, revealing that LLMs perform effectively only when task complexity is highly limited.In contrast, ScienceAgent-Bench (Chen et al., 2025c) provided a multidisciplinary benchmark for LLMs operating within agent frameworks such as CodeAct (Wang et al., 2024b) and self-debug (Chen et al., 2023).This benchmark highlighted the necessity for tailored agent workflows for such explorative tasks.</p>
<p>In the biomedical domain, Gao et al. ( 2024) discussed potential applications of AI agents in brainstorming, experimental planning, and execution.</p>
<p>BioResearcher (Luo et al., 2024) proposed an endto-end framework for biomedical research involving dry lab experiments.DrugAgent (Liu et al., 2025b) adopted multi-agent collaboration to automate drug discovery.In chemistry, Coscientist (Boiko et al., 2023) incorporated the use of tools by LLMs to support semi-autonomous chemistry experiment design and execution.ProtAgents (Ghafarollahi and Buehler, 2024a) facilitated biochemistry discovery by building a multi-agent framework for automating protein design.Recent works, such as FutureHouse (Skarlinski et al., 2025) and AI Co-scientist (Gottweis et al., 2025), contributed to formulating demonstrably novel research hypotheses and proposals using multi-agent systems guided by predefined research goals.</p>
<p>General Research</p>
<p>Apart from specialized domain applications, some benchmarks have broadly evaluated diverse tasks from different stages of scientific discovery.Dis-coveryWorld (Jansen et al., 2024) created a virtual environment for LLM agents to conduct simplified scientific exploration.In (Liu et al., 2025a), various application scenarios for AI agents in research were comprehensively discussed, supported by preliminary evaluation datasets.Similarly, CURIE (Kon et al., 2025) proposed a benchmark and an agentic framework for rigorous and automated scientific experimentation.While EAIRA (Cappello et al., 2025) focused on assessing the ability of LLMs to perform in a real-world research assistant role using various task formats.</p>
<p>5 Level 3. LLM as Scientist (Table A3)</p>
<p>Recently, several research efforts and commercial products have demonstrated prototypes of fully autonomous research within the artificial intelligence domain.These systems typically encompass a comprehensive workflow, from initial literature review to iterative refinement cycles where hypotheses or designs are progressively improved.A common feature is using an agent-based framework to autonomously produce research outputs, often culminating in draft research papers.This section will further compare these approaches, focusing on their methodologies for idea development and iterative refinement, as these aspects critically distinguish them from Level 2 agents.</p>
<p>Idea Development</p>
<p>The genesis of research in Level 3 systems involves transforming initial concepts into validated hypotheses, with distinct approaches to sourcing and vetting these ideas.Agent Laboratory (Schmidgall et al., 2025) predominantly conducts literature reviews based on human-defined research objectives.Moving towards greater autonomy, several systems initiate their process from broader human inputs, such as reference papers (Data Intelligence Lab, 2025; Autoscience, 2025) or general research domains (IntologyAI, 2025), subsequently exploring literature to autonomously identify gaps and formulate novel hypotheses.The AI Scientist (v1 (Lu et al., 2024) and v2 (Yamada et al., 2025)) showcases an even more generative approach: v1 brainstorms ideas from templates and past work, while v2 can generate diverse research proposals from abstract thematic prompts.Crucially, these systems employ diverse methods to evaluate their ideas prior to full implementation.AI Scientist-v1 uses self-assessed scores for interestingness, novelty, and feasibility, supplemented by external checks with Semantic Scholar.AI Scientist-v2 integrates literature review tools early in its idea formulation stage to assess novelty.This spectrum reveals a clear trend: while humans often initiate ideas, advanced systems can autonomously explore, generate, and validate the scientific merit and originality of research objectives before development.</p>
<p>Iterative Refinement</p>
<p>Iterative refinement within Level 3 systems involves sophisticated feedback loops that enable not just incremental improvements but also fundamental reassessments of the research trajectory.A key differentiator lies in the primary source and nature of this high-level feedback.The AI Scientist (v1 and v2) incorporates highly automated internal review and refinement processes.It employs AI reviewers, LLM evaluators for experimental choices, and VLMs to critique figures, fostering a rich internal feedback loop for iterative development.In contrast, Zochi (IntologyAI, 2025) integrates human expertise for macro-level guidance, where feedback can trigger complete re-evaluations of hypotheses or designs.This allows it to act on critiques challenging the core research premise, even reverting to hypothesis regeneration if results are unsatisfactory.Overall, while automated self-correction is a common goal, the current landscape reveals a pragmatic blend: some systems focus on enhancing autonomous deep reflection, while others integrate human oversight for robust, high-level iterative refinement and strategic redirection.</p>
<p>Challenges and Future Directions</p>
<p>Throughout this survey, we have systematically reviewed the escalating roles of Large Language Models in scientific discovery, delineating their progression through distinct levels of autonomy and capability-from foundational assistants and analysts to increasingly autonomous scientific researchers.In particular, we have underscored the evolving methodologies, task complexities, and the nature of human-LLM interaction that define each stage of this maturation.Beyond reviewing these advancements and current applications, this section presents several significant challenges and outlines promising directions for future research, aiming to inspire further exploration into the development and responsible deployment of LLMs as transformative tools in scientific inquiry.</p>
<p>Fully-Autonomous Research Cycle While current Level 3 systems can navigate multiple stages of the scientific method for a specific inquiry, they often operate within a single research instance or predefined topic.The scientific method, however, is inherently cyclical, characterized by continuous iteration, refinement, and the pursuit of evolving research questions.A significant future direction, therefore, is to develop LLM-based systems capable of engaging in a truly autonomous research cycle.This would entail not merely executing a given research task from start to finish, but possessing the foresight to discern the broader implications of their findings, proactively identify promising avenues for subsequent investigation, and strategically direct their efforts towards practical advancements that build upon previous work.</p>
<p>Robotic Automation</p>
<p>A key barrier to fully autonomous scientific discovery in natural science is LLM agents' inability to conduct physical laboratory experiments.While adept in computational research, their application in fields requiring physical interaction remains limited.Integrating LLMs with robotic systems empowers them to translate their planning capabilities into direct experimental actions.Early works in LLM-robotic integration (Yoshikawa et al., 2023;Song et al., 2024;Darvish et al., 2025) already highlights this potential in chemical experimentation.Such automation is poised to significantly broaden LLM-based research, enabling end-to-end discovery in disciplines like chemistry and materials science, thereby advancing autonomous scientific exploration.</p>
<p>Transparency and Interpretability</p>
<p>The blackbox nature (or opacity) of advancing LLMs in science undermines scientific validation, trust, and the assimilation of AI-driven insights (Xu et al., 2025c).Addressing this opacity demands more than superficial Explainable AI (XAI) techniques (Ahadian and Guan, 2024).It necessitates a paradigm shift towards systems whose internal operations are inherently designed for verifiable reasoning and justifiable conclusions (Bengio et al., 2025).Consequently, the challenge is not just explaining outputs, but ensuring the AI's internal logic aligns with scientific principles and can reliably differentiate asserted claims from verifiable truths.This profound interpretability is vital for reliable and reproducible LLM-based scientific discovery.</p>
<p>Continuous Self-Improvement</p>
<p>The iterative and evolving nature of scientific inquiry demands systems capable of learning from ongoing engagement, assimilating experimental outcomes, and adapting research strategies.Current research integrating continual learning with agent-based systems already highlights the potential for LLMs to adapt to new tasks or environments without catastrophic forgetting (Majumder et al., 2023;Kim et al., 2024).Within scientific discovery, a promising future direction is to incorporate online reinforcement learning frameworks (Carta et al., 2023).This integration promises to continuously enhance scientific agents' capabilities over their operational lifetime through successive discoveries, thereby advancing sustainable autonomous exploration.</p>
<p>Ethics and Societal Alignment</p>
<p>As LLM-based systems gain independent reasoning and action capabilities, their potential for risks-ranging from amplified societal biases to deliberate misuse like generating harmful substances or challenging human control-becomes increasingly salient and complex (He et al., 2023;Ahadian and Guan, 2024;Bengio et al., 2025).With AI capabilities and societal norms in constant flux, alignment is consequently an imperative, continuous process demanding adaptive governance and evolving value systems (Li et al., 2024e).This requires embedding ethical constraints directly in scientific AI design frameworks, alongside vigilant oversight, to ensure advancements serve human well-being and the common good.</p>
<p>Limitations</p>
<p>This survey provides a systematic review of LLMs in scientific discovery, with a particular emphasis on the paradigm shift characterized by their escalating levels of autonomy.Our analysis and the selection of reviewed literature are therefore centered on works that illustrate this transition across the stages of the scientific method, categorized within our proposed three-level autonomy framework: LLM as Tool, LLM as Analyst, and LLM as Scientist.</p>
<p>Consequently, the scope of this survey has certain limitations.Firstly, we do not provide an exhaustive review of research focused on the development of general-purpose scientific LLMs for domain-specific reasoning or application.These areas, while crucial to the broader landscape of AI in science, are extensively covered in other existing surveys and fall outside our specific focus on the autonomy paradigm.Secondly, while we acknowledge the importance of fundamental LLM capabilities such as planning, code generation, and agentic decision-making, this survey does not delve deeply into orthogonal benchmarks or methodologies related to these general abilities.These exclusions were deliberate to maintain a focused exploration of the transformative roles and increasing independence of LLMs throughout the scientific research lifecycle.</p>
<p>Ethics Statement</p>
<p>Our paper presents a comprehensive survey of LLMs in scientific discovery, with a specific focus on their role transformation from task automation tools to autonomous agents.All research works reviewed in this survey are properly cited.To the best of our knowledge, the referenced materials are publicly accessible or available under licenses permitting their use for research review.We did not conduct additional dataset curation or human annotation work.Consequently, we believe that this paper does not raise any ethical concerns.DAgent (Xu et al., 2025b) Data Science ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✗ DS-Agent (Guo et al., 2024b) Data Science ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✓ InfiAgent-DABench (Hu et al., 2024b) Data Science ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗ BLADE (Gu et al., 2024) Data Open-Sourced?</p>
<p>Agent Laboratory (Schmidgall et al., 2025) Artificial Intelligence ✓ ✓ literature review, experimentation, report writing, iterative research with human feedback loops.✓</p>
<p>The AI Scientist (Lu et al., 2024) Artificial Intelligence ✓ ✗ idea generation, code generation, experiment execution, research paper writing.✓</p>
<p>The AI Scientist-v2 (Yamada et al., 2025)</p>
<p>Figure 1 :
1
Figure 1: Stages of the scientific method with corresponding LLM applications and research topics.</p>
<p>Table 1 :
1
Three levels of autonomy in LLM-based scientific discovery.
Autonomy LevelsLLMs' RoleHuman's RoleTask ScopeAgentic WorkflowLevel 1 LLM as ToolTask Automation ToolTask AllocationExplicitly DefinedSimple &amp; StaticLevel 2 LLM as AnalystData Modeling &amp; Analytical AgentProblem Definition &amp; Output ValidationGoal-OrientedAdvancedLevel 3 LLM as ScientistOpen Exploratory &amp; Discovery AgentMinimal InterventionOpen-EndedStrategic &amp; Iterative</p>
<p>LEVEL 1 LEVEL 2 LEVEL 3 FUTURE Literature Review &amp; Info Gathering Idea Generation &amp; Hypothesis Formulation Experiment Planning &amp; Execution Data Analysis &amp; Organization Conclusion &amp; Hypothesis Validation Iteration &amp; Refinement Machine Learning Research Data Modeling and Analysis Function Discovery Natural Science Research General Research
SCIMON(Wang et al., 2023)ResearchAgentLitLLM(Baek et al., 2024)(Agarwal et al., 2024)Text-Tuple-Table (Deng et al., 2024)Dennstädt et al., 2024TKGT</p>
<p>Table (
(for table-based question answering under practicalindustrial scenarios.Chart Data Beyond tabular data, charts rep-resent another important format for organizingand storing information derived from experimentaldata. Early benchmarks, exemplified by ChartQA(Masry et al., 2022), examined the capabilities ofvision transformers in chart-based question answer-ing. Subsequent works, including CharXiv (Wanget al., 2024e) and ChartX (Xia et al., 2025), have ex-panded the scope of chart understanding scenariosby utilizing human-curated chart generation or byincorporating real-world chart data sourced fromarXiv preprints. Regarding chart generation, Au-tomaTikZ (Belouadi et al., 2024) formulates theprocess as TikZ code generation from caption textand has demonstrated the efficacy of fine-tuningLLMs using open scientific figure data. More re-cently, Text2Chart31 (Zadeh et al., 2025) employedreinforcement learning with automated feedbackto refine chart generation capabilities within theMatplotlib library.Wang et al., 2024d) proposes a method to enhancetabular understanding by incorporating evolvingtables within the reasoning chain of LLMs. Con-currently, Deng et al. (2024a) highlight the poten-tial of integrating visual information to improvemultimodal understanding, thereby aiding tabularcomprehension. More recently, Wu et al. (2025) in-troduced TableBench, a comprehensive benchmark</p>
<p>Table A2 :
A2
Comparison and classification of Level 2 research works in LLM-based scientific discovery.
Science✗✓✗✗✓✓✓✗DiscoveryBench (Majumder et al., 2024)Data Science✗✓✗✓✓✓✓✗DSBench (Jing et al., 2024)Data Science✗✓✗✗✓✓✓✗Zheng et al. (2023)General✓✓✓✓✓✓✓✗Function DiscoveryBoxLM (Li et al., 2024b)Statistics✓✗✗✓✓✓✓✗LLM-SR (Shojaee et al., 2025a)General✓✗✗✓✓✗✓✓LLM-SRBench (Shojaee et al., 2025b)General✗✓✗✓✓✗✓✓Gravity-Bench-v1 (Koblischke et al., 2025)Physics✗✓✗✓✓✗✓✓DrSR (Wang et al., 2025a)General✓✗✗✓✓✓✓✓EvoSLD (Lin et al., 2025)Artificial Intelligence✓✗✗✓✓✓✓✓Natural Science ResearchCoscientist (Boiko et al., 2023)Chemistry✓✗✗✓✓✓✓✗Gao et al. (2024)Biomedicine✓✗✗✓✓✓✓✗BioResearcher (Luo et al., 2024)Biomedicine✓✗✗✗✓✓✓✓DrugAgent (Liu et al., 2025b)Biomedicine✓✗✗✓✓✗✗✓FutureHouse (Skarlinski et al., 2025)Chemistry, Biology✓✗✓✓✓✗✗✗ScienceAgentBench (Chen et al., 2025c)Chemistry, Biology✗✓✓✓✓✓✓✗ProtAgents (Ghafarollahi and Buehler, 2024a)Chemistry, Biology✓✗✓✓✓✓✓✗Auto-Bench (Chen et al., 2025b)General✗✓✗✗✓✓✗✓AI co-scientist (Gottweis et al., 2025)General✓✗✗✓✓✓✓✗General ResearchDiscoveryWorld (Jansen et al., 2024)General✗✓✗✓✓✓✓✓Liu et al. (2025a)General✗✓✓✓✓✓✓✓Curie (Kon et al., 2025)General✓✗✗✗✓✓✓✗EAIRA (Cappello et al., 2025)General✗✓✓✓✓✓✓✗Research WorksScience DomainMethodology FrameworkBenchmark EvaluationFeatured Functionality</p>
<p>Table A3 :
A3
Comparison and classification of Level 3 research works in LLM-based scientific discovery.
idea generation, code generation,Artificial Intelligence✓✗experiment execution, research paper writing,✓with agentic tree-search and feedbacks.AI-Researcher (Data Intelligence Lab, 2025) Artificial Intelligence✓✗literature review, data analysis, report generation.✓Zochi (IntologyAI, 2025)Artificial Intelligence✓✗customizable workflows for data collection, analysis, and decision-making.✓Carl (Autoscience, 2025)Artificial Intelligence✓✗hypothesis generation, experiment design, data analysis, and manuscript writing.✗
AcknowledgementsWe thank all the anonymous reviewers and meta reviewers for their valuable comments.The authors of this paper were supported by the ITSP Platform Research Project (ITS/189/23FP) from ITC of Hong Kong, SAR, China, and the AoE (AoE/E-601/24-N), the RIF (R6021-20) and the GRF (16205322) from RGC of Hong Kong, SAR, China.Data Modeling and AnalysisTing, Azton Wells, and 7 others.2025.Eaira: Establishing a methodology for evaluating ai models as scientific research assistants.Preprint, arXiv:2502.20309.A Summary Tables of LLMs in Scientific DiscoveryResearch WorksScience Domain Task Nature Framework Methodology Evaluation BenchmarkAgentic WorkflowLiterature Search and Info Aggregation LitLLM(Agarwal et al., 2024)General Literature ✓ ✗ ✗ Science Hierarchography(Gao et al., 2025)General Literature ✓ ✗ ✓ Dennstädt et al.(2024)Biomedicine Literature ✓ ✗ ✗ SCIMON(Wang et al., 2024a)General Literature, Idea Generation ✓ ✗ ✗ ResearchAgent(Baek et al., 2025)General Literature, Idea Generation ✓ ✗ ✓ Text-Tuple-Table(Deng et al., 2024b)General Text2Table ✓ ✓ ✗ TKGT(Jiang et al., 2024b)General Text2Table ✓ ✓ ✗ ArxivDIGESTables(Newman et al., 2024)General Literature, Text2Table ✓ ✓ ✗ arXiv2Table(Wang et al., 2025b)General Literature, Text2Table ✓ ✓ ✗ PaperQA &amp; LitQA(Lála et al., 2023)General Literature ✓ ✓ ✗ AutoSurvey(Wang et al., 2024c)General Literature ✓ ✗ ✓Idea Generation and Hypothesis FormulationSi et al. (2024)Artificial Intelligence Idea Generation ✗ ✓ ✗ LiveIdeaBench(Ruan et al., 2025)General Idea Generation ✗ ✓ ✗ Nova(Hu et al., 2024a)General Literature, Idea Generation ✓ ✗ ✓ IdeaBench(Guo et al., 2024a)General Literature, Idea Generation ✗ ✓ ✗ GraphEval(Feng et al., 2025)Artificial Intelligence Literature, Idea Generation ✗ ✓ ✗ AI Idea Bench 2025(Qiu et al., 2025)Artificial(Yin et al., 2022)Artificial Intelligence Code Generation ✓ ✓ ✗ AIDE(Jiang et al., 2025)Artificial Intelligence Code Generation ✓ ✗ ✓ SciCode(Tian et al., 2024)Artificial Intelligence Code Generation ✗ ✓ ✗ DS-1000(Lai et al., 2022)Artificial Intelligence Code Generation ✗ ✓ ✗ MLE-Bench(Chan et al., 2025)Artificial Intelligence Code Generation ✗ ✓ ✓ Data Analysis and Organization AutomaTikZ(Belouadi et al., 2024)General(Xia et al., 2025)General Chart Reasoning ✓ ✓ ✗ CharXiv(Wang et al., 2024e)General Chart Reasoning ✗ ✓ ✗ ChartQA(Masry et al., 2022)General Chart Reasoning ✗ ✓ ✗ Chain-of-Table(Wang et al., 2024d)General Tabular Reasoning ✓ ✗ ✓ TableBench(Wu et al., 2025)General(Rabby et al., 2025)General Hypothesis Generation, Refinement ✓ ✗ ✓ Chain of Ideas(Li et al., 2024a)Artificial Intelligence Idea Generation, Refinement ✓ ✓ ✓ Machine Learning Research CodeScientist(Jansen et al., 2025)Artificial Intelligence(Xue et al., 2025)Artificial Intelligence ✓ ✗ ✗ ✗ ✓ ✓ ✓ ✓ MLAgentBench(Huang et al., 2024a)Artificial Intelligence ✗ ✓ ✗ ✗ ✓ ✓ ✓ ✗ MLR-Copilot(Li et al., 2024d)Artificial Intelligence ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✓ MLRC-Bench(Zhang et al., 2025b)Artificial Intelligence ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓ RE-Bench(Wijk et al., 2024)Artificial Intelligence ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓ MLZero(Fang et al., 2025)Artificial Intelligence ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✓ Genesys(Cheng et al., 2025b)Artificial Intelligence ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✓ MLGym(Nathani et al., 2025)Artificial Intelligence
Llm4grn: Discovering causal gene regulatory networks with llms -evaluation through synthetic data generation. Tejumade Afonja, Ivaxi Sheth, Ruta Binkyte, Waqar Hanif, Thomas Ulas, Matthias Becker, Mario Fritz, arXiv:2410.158282024Preprint</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, arXiv:2402.017882024Preprint</p>
<p>Ai trustworthy challenges in drug discovery. Pegah Ahadian, Qiang Guan, Trustworthy Artificial Intelligence for Healthcare. ChamSpringer Nature Switzerland2024</p>
<p>Meet carl: The first ai system to produce academically peer-reviewed research. Autoscience , 2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382025Preprint</p>
<p>Automatikz: Text-guided synthesis of scientific vector graphics with tikz. Jonas Belouadi, Anne Lauscher, Steffen Eger, arXiv:2310.003672024Preprint</p>
<p>Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path?. Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt Macdermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King, arXiv:2502.156572025Preprint</p>
<p>Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, arXiv:2502.18864Burak Gokturk, Amin Vahdat, Pushmeet Kohli, and 15 others. 2025. Towards an ai co-scientist. Preprint</p>
<p>Blade: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, M Tianmai, Lanyi Zhang, Mike A Zhu, Jeffrey Merrill, Tim Heer, Althoff, arXiv:2408.096672024Preprint</p>
<p>Ideabench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Eric Huang, Stefan Xie, Aidong Bekiranov, Zhang, arXiv:2411.024292024aPreprint</p>
<p>Ds-agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, arXiv:2402.174532024bPreprint</p>
<p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu, arXiv:2401.13919Webvoyager: Building an endto-end web agent with large multimodal models. 2024Preprint</p>
<p>Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo Zhou, Xing Xie, Weiming Zhang, Nenghai Yu, Shuxin Zheng, arXiv:2312.06632Control risk for potential misuse of artificial intelligence in science. 2023Preprint</p>
<p>An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024aNovaPreprint</p>
<p>Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu, arXiv:2401.05507Infiagent-dabench: Evaluating agents on data analysis tasks. 2024bPreprint</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.033022024aPreprint</p>
<p>Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, arXiv:2402.02716Ruiming Tang, and Enhong Chen. 2024b. Understanding the planning of llm agents: A survey. Preprint</p>
<p>Zochi technical report: The first artificial scientist. </p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, arXiv:2406.067692024Preprint</p>
<p>Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Prasad Bodhisattwa, Daniel S Majumder, Peter Weld, Clark, arXiv:2503.227082025Preprint</p>
<p>A survey on large language models for code generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, arXiv:2406.005152024aPreprint</p>
<p>TKGT: Redefinition and a new way of text-to-table tasks based on real world demands and knowledge graphs augmented LLMs. Peiwen Jiang, Xinbo Lin, Zibo Zhao, Ruhui Ma, Yvonne Jie Chen, Jinhua Cheng, 10.18653/v1/2024.emnlp-main.901Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Aide: Ai-driven exploration in the space of code. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu, arXiv:2502.131382025Preprint</p>
<p>Dsbench: How far are data science agents from becoming data science experts?. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, arXiv:2409.077032024Preprint</p>
<p>End-toend symbolic regression with transformers. Pierre-Alexandre Kamienny, Stéphane Ascoli, arXiv:2204.105322022PreprintGuillaume Lample, and François Charton</p>
<p>Online continual learning for interactive instruction following agents. Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi, arXiv:2403.075482024Preprint</p>
<p>Gravity-bench-v1: A benchmark on gravitational physics discovery for agents. Nolan Koblischke, Hyunseok Jang, Kristen Menou, Mohamad Ali-Dib, arXiv:2501.184112025Preprint</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. Patrick Tser, Jern Kon, Jiachen Liu, Qingyun Ding, Yuxin Qiu, Zhaoning Yang, Yufan Huang, Moontae Jer-Shannassa, Muntasir Lee, Aonan Chowdhury, Chen, arXiv:2502.160692025Preprint</p>
<p>Thomas Samuel, Kuhn , The Structure of Scientific Revolutions. ChicagoUniversity of Chicago Press1962</p>
<p>Ds-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen Tau Yih, Daniel Fried, Sida Wang, Tao Yu, arXiv:2211.115012022Preprint</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, 2023</p>
<p>Can large language models help experimental design for causal discovery. Junyi Li, Yongqiang Chen, Chenxi Liu, Qianyi Cai, Tongliang Liu, Bo Han, Kun Zhang, Hui Xiong, arXiv:2503.011392025Preprint</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, arXiv:2410.13185Tian Feng, and Lidong Bing. 2024a. Chain of ideas: Revolutionizing research via novel idea development with llm agents. Preprint</p>
<p>Automated statistical model discovery with language models. Y Michael, Emily B Li, Noah D Fox, Goodman, arXiv:2402.178792024bPreprint</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Xinya Du, arXiv:2412.146262024cPreprint</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024dPreprint</p>
<p>Agent alignment in evolving social norms. Shimin Li, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu, arXiv:2401.046202024ePreprint</p>
<p>Evosld: Automated neural scaling law discovery with large language models. Haowei Lin, Yacine Jernite, H T Kung, Andrew Gordon, Wilson , arXiv:2507.211842025Preprint</p>
<p>A vision for auto research with llm agents. Chengwei Liu, Chong Wang, Jiayue Cao, Jingquan Ge, Kun Wang, Lvye Zhang, Ming-Ming Cheng, Penghai Zhao, Tianlin Li, Xiaojun Jia, Xiang Li, Xinfeng Li, Yang Liu, Yebo Feng, Yihao Huang, Yijia Xu, Yuqiang Sun, Zhenhong Zhou, Zhengzi Xu, arXiv:2504.187652025aPreprint</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, arXiv:2306.006222023Preprint</p>
<p>Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, Yue Zhao, arXiv:2411.156922025bPreprint</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025cPreprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024Preprint</p>
<p>From intention to implementation: Automating biomedical research via llms. Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Chen Lin, arXiv:2412.094292024Preprint</p>
<p>Llm4sr: A survey on large language models for scientific research. Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.043062025Preprint</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. Prasad Bodhisattwa, Bhavana Majumder, Peter Dalvi Mishra, Oyvind Jansen, Niket Tafjord, Li Tandon, Chris Zhang, Peter Callison-Burch, Clark, arXiv:2310.101342023Preprint</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.017252024Preprint</p>
<p>Chartqa: A benchmark for question answering about charts with visual and logical reasoning. Ahmed Masry, Xuan Do, Jia Long, Shafiq Qing Tan, Enamul Joty, Hoque, arXiv:2203.102442022Preprint</p>
<p>Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang, Wang , Roberta Raileanu, arXiv:2502.14499Mlgym: A new framework and benchmark for advancing ai research agents. 2025Preprint</p>
<p>Arxivdigestables: Synthesizing scientific literature into tables using language models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Daniel S Weld, Joseph Chee Chang, Kyle Lo, arXiv:2410.223602024Preprint</p>
<p>Machine learning for hypothesis generation in biology and medicine: exploring the latent space of neuroscience and developmental bioelectricity. O' Thomas, Joel Brien, Léo Stremmel, Patrick Pio-Lopez, Cody Mcmillen, Michael Rasmussen-Ivey, Levin, 10.1039/D3DD00185GDigital Discovery. 32024</p>
<p>Bioplanner: Automatic evaluation of llms on protocol planning in biology. Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Essa Abboud, Justin Ghareeb, Samuel G Booth, Rodriques, arXiv:2310.106322023Preprint</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Rȃileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciucȃ, arXiv:2504.129762025Preprint</p>
<p>Introducing openai o1 preview. OpenAI. 2025. Introducing deep research. 2024OpenAI</p>
<p>Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, William Jurayj, Miriam Wanner, Shaobo Liang, Candice Morgan, Seunghoon Han, Weiqi Wang, Chandler May, Hannah Recknor, arXiv:2503.21717Daniel Khashabi, and Benjamin Van Durme. 2025. Claimcheck: How grounded are llm critiques of scientific papers? Preprint. </p>
<p>The Logic of Scientific Discovery. Karl R Popper, 1935RoutledgeLondon, England</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023Preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, arXiv:2407.089402024Preprint</p>
<p>Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, Dong Yu, arXiv:2401.03601Infobench: Evaluating instruction following ability in large language models. 2024Preprint</p>
<p>Ai idea bench 2025: Ai research idea generation benchmark. Yansheng Qiu, Haoquan Zhang, Zhaopan Xu, Ming Li, Diping Song, Zheng Wang, Kaipeng Zhang, arXiv:2504.141912025Preprint</p>
<p>Tool learning with large language models: a survey. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen, 10.1007/s11704-024-40678-2Frontiers of Computer Science. 8192025</p>
<p>Verification and refinement of natural language explanations through llm-symbolic theorem proving. Xin Quan, Marco Valentino, Louise A Dennis, André Freitas, arXiv:2405.013792024Preprint</p>
<p>Prasenjit Mitra, and Sören Auer. 2025. Iterative hypothesis generation for scientific discovery with monte carlo nash equilibrium self-refining trees. Gollam Rabby, Diyana Muhammed, arXiv:2503.19309Preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342025Preprint</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. K Chandan, Parshin Reddy, Shojaee, 10.1609/aaai.v39i27.35084Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Liveideabench: Evaluating llms' divergent thinking for scientific idea generation with minimal context. Kai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, Hao Sun, arXiv:2412.175962025Preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025Preprint</p>
<p>Automated machine learning: From principles to practices. Zhenqian Shen, Yongqi Zhang, Lanning Wei, Huan Zhao, Quanming Yao, arXiv:1810.133062024Preprint</p>
<p>Hierarchically encapsulated representation for protocol design in self-driving labs. Yu-Zhe Shi, Mingchen Liu, Fanxu Meng, Qiao Xu, Zhangqian Bi, Kun He, Lecheng Ruan, Qining Wang, arXiv:2504.038102025Preprint</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, arXiv:2404.184002025aPreprint</p>
<p>Llm-srbench: A new benchmark for scientific equation discovery with large language models. Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Chandan K Khoa D Doan, Reddy, arXiv:2504.104152025bPreprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024Preprint</p>
<p>Futurehouse platform: Superintelligent ai agents for scientific discovery. Michael Skarlinski, Tyler Nadolski, James Braza, Remo Storni, Mayk Caldas, Ludovico Mitchener, Michaela Hinks, Andrew White, Sam Rodriques, 2025</p>
<p>A multi-agent-driven robotic ai chemist enabling autonomous chemical research on demand. Tao Song, Man Luo, Linjiang Chen, Yan Huang, Qing Zhu, Daobin Liu, Baicheng Zhang, Gang Zou, Fei Zhang, Weiwei Shang, Jun Jiang, Yi Luo, 10.26434/chemrxiv-2024-w953h-v22024Preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, arXiv:2504.01848Amelia Glaese, and Tejal Patwardhan. 2025. Paperbench: Evaluating ai's ability to replicate ai research. Preprint</p>
<p>Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang, arXiv:2412.14222A survey on large language model-based agents for statistics and data science. 2024Preprint</p>
<p>Towards autonomous hypothesis verification via language models with minimal guidance. Shiro Takagi, Ryutaro Yamauchi, Wataru Kumagai, arXiv:2311.097062023Preprint</p>
<p>Yanyu Xiong, and 11 others. Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Wu, arXiv:2407.13168Scicode: A research coding benchmark curated by scientists. 2024Preprint</p>
<p>Dov Te'eni, and Iddo Drori. 2024. Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, arXiv:2408.10365Preprint</p>
<p>Ai feynman: a physics-inspired method for symbolic regression. Silviu- , Marian Udrescu, Max Tegmark, arXiv:1905.114812020Preprint</p>
<p>HypER: Literature-grounded hypothesis generation and distillation with provenance. Rosni Vasu, Chandrayee Basu, Bhavana Dalvi Mishra, Cristina Sarasua, Peter Clark, Abraham Bernstein, arXiv:2506.129372025Preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592024aPreprint</p>
<p>Drsr: Llm based scientific equation discovery with dual reasoning from data and experience. Runxiang Wang, Boxiao Wang, Kai Li, Yifan Zhang, Jian Cheng, arXiv:2506.042822025aPreprint</p>
<p>Can llms generate tabular summaries of science papers? rethinking the evaluation protocol. Weiqi Wang, Jiefu Ou, Yangqiu Song, Benjamin Van Durme, Daniel Khashabi, arXiv:2504.102842025bPreprint</p>
<p>Executable code actions elicit better llm agents. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, arXiv:2402.010302024bPreprint</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Linyi Yang, Jindong Wang, Cunxiang Wang, Kaijie Zhu, Yiqiao Jin, Xing Xie, arXiv:2406.102522024cPreprint</p>
<p>Chain-of-table: Evolving tables in the reasoning chain for table understanding. Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister, arXiv:2401.043982024dPreprint</p>
<p>Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen, arXiv:2406.185212024ePreprint</p>
<p>Predicting empirical ai research outcomes with language models. Yutai Wen, Yifan Jiang, Zitao Li, Whytnee Wade, J D Zamfirescu-Pereira, Xinyun Chen, Yuxin Wen, Yiming Yang, Graham Neubig, Jacob Andreas, Daniel S Weld, arXiv:2506.007942025Preprint</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, arXiv:2411.008162025Preprint</p>
<p>Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Holden Karnofsky, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, arXiv:2411.15114Evaluating frontier ai r&amp;d capabilities of language model agents against human experts. Preprintand 4 others. 2024. Rebench</p>
<p>Tablebench: A comprehensive and complex benchmark for table question answering. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, Guanglin Niu, Tongliang Li, Zhoujun Li ; Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, Junchi Yan, Yu Qiao, arXiv:2408.09174.xAI.2025arXiv:2402.12185Chartx &amp; chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. Bo Zhang, Hancheng Ye,2025. 2025PreprintGrok 3 beta -the age of reasoning agents. Renqiu Xia</p>
<p>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He ; Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2504.00255arXiv:2411.023822025. 2024PreprintImproving scientific hypothesis generation with knowledge grounded large language models</p>
<p>Towards multi-agent reasoning systems for collaborative expertise delegation: An exploratory design study. Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang, arXiv:2505.073132025aPreprint</p>
<p>Dagent: A relational database-driven data analysis report generation agent. Wenyi Xu, Yuren Mao, Xiaolu Zhang, Chao Zhang, Xuemei Dong, Mengfei Zhang, Yunjun Gao, arXiv:2503.132692025bPreprint</p>
<p>Advancing ai-scientist understanding: Making llm think like a physicist with interpretable reasoning. Yinggan Xu, Hana Kimlee, Yijia Xiao, Di Luo, arXiv:2504.019112025cPreprint</p>
<p>Improve: Iterative model pipeline refinement and optimization leveraging llm agents. Eric Xue, Zeyi Huang, Yuyang Ji, Haohan Wang, arXiv:2502.185302025Preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025Preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, arXiv:2309.027262024PreprintSoujanya Poria, and Erik Cambria</p>
<p>Moosechem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762025Preprint</p>
<p>Natural language to code generation in interactive data science notebooks. Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, Charles Sutton, arXiv:2212.092482022Preprint</p>
<p>Artur Kuramshin, Alán Aspuru-Guzik, Florian Shkurti, and Animesh Garg. 2023. Large language models for chemistry robotics. Naruki Yoshikawa, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen, Andrew Zou Li, Yuchi Zhao, Haoping Xu, 10.1007/s10514-023-10136-2Autonomous Robots. 47</p>
<p>Text2chart31: Instruction tuning for chart generation with automatic feedback. Juyeon Fatemeh Pesaran Zadeh, Jin-Hwa Kim, Gunhee Kim, Kim, arXiv:2410.040642025Preprint</p>
<p>Xiaohui Fan, and 2 others. 2025a. Scientific large language models: A survey on biological &amp; chemical domains. Qiang Zhang, Keyan Ding, Tianwen Lv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Mengyao Zhang, Jinlu Zhang, Jiyu Cui, Renjun Xu, Hongyang Chen, 10.1145/3715318ACM Comput. Surv. 657</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, arXiv:2406.108332024Preprint</p>
<p>Mlrc-bench: Can language agents solve machine learning research challenges?. Yunxiang Zhang, Muhammad Khalifa, Shitanshu Bhushan, Grant D Murphy, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2504.097022025bPreprint</p>
<p>Large language models for scientific synthesis, inference and explanation. Yizhen Zheng, Yee Huan, Jiaxin Koh, Anh T N Ju, Lauren T Nguyen, Geoffrey I May, Shirui Webb, Pan, arXiv:2310.079842023Preprint</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024a</p>
<p>Hypothesis generation with large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024b</p>            </div>
        </div>

    </div>
</body>
</html>
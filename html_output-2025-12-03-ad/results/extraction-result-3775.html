<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3775 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3775</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3775</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-5d2b625ff86f0641905290fc4151840f71dbbf8f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5d2b625ff86f0641905290fc4151840f71dbbf8f" target="_blank">Predicting research trends with semantic and neural networks with an application in quantum physics</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the National Academy of Sciences of the United States of America</p>
                <p><strong>Paper TL;DR:</strong> The development of a semantic network for quantum physics, denoted SemNet, is demonstrated using 750,000 scientific papers and knowledge from books and Wikipedia, which is used to predict future trends in research and to inspire personalized and surprising seeds of ideas in science.</p>
                <p><strong>Paper Abstract:</strong> Significance The corpus of scientific literature grows at an ever increasing speed. While this poses a severe challenge for human researchers, computer algorithms with access to a large body of knowledge could help make important contributions to science. Here, we demonstrate the development of a semantic network for quantum physics, denoted SemNet, using 750,000 scientific papers and knowledge from books and Wikipedia. We use it in conjunction with an artificial neural network for predicting future research trends. Individual scientists can use SemNet for suggesting and inspiring personalized, out-of-the-box ideas. Computer-inspired scientific ideas will play a significant role in accelerating scientific progress, and we hope that our work directly contributes to that important goal. The vast and growing number of publications in all disciplines of science cannot be comprehended by a single human researcher. As a consequence, researchers have to specialize in narrow subdisciplines, which makes it challenging to uncover scientific connections beyond the own field of research. Thus, access to structured knowledge from a large corpus of publications could help push the frontiers of science. Here, we demonstrate a method to build a semantic network from published scientific literature, which we call SemNet. We use SemNet to predict future trends in research and to inspire personalized and surprising seeds of ideas in science. We apply it in the discipline of quantum physics, which has seen an unprecedented growth of activity in recent years. In SemNet, scientific knowledge is represented as an evolving network using the content of 750,000 scientific papers published since 1919. The nodes of the network correspond to physical concepts, and links between two nodes are drawn when two concepts are concurrently studied in research articles. We identify influential and prize-winning research topics from the past inside SemNet, thus confirming that it stores useful semantic knowledge. We train a neural network using states of SemNet of the past to predict future developments in quantum physics and confirm high-quality predictions using historic data. Using network theoretical tools, we can suggest personalized, out-of-the-box ideas by identifying pairs of concepts, which have unique and extremal semantic network properties. Finally, we consider possible future developments and implications of our findings.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3775.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3775.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEMNET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SEMantic NETwork for quantum physics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic knowledge network built from co-occurrence of extracted concepts in titles/abstracts of 750,000 physics papers; used to represent evolving relations between physical concepts and to drive predictions and suggestion generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SEMNET (pipeline: RAKE + co-occurrence network + supervised DNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline combining automatic keyword extraction (RAKE) and human-curated concept lists to form a co-occurrence semantic network (nodes = concepts, edges = co-occurrence in title/abstract), coupled to a supervised feed-forward neural network (4 fully connected layers) that ranks unconnected concept pairs by probability of being linked within 5 years.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Physics (quantum physics focus) — multidisciplinary within physics</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>750,000 physics articles (100k arXiv quantum-subset used for keyword extraction; 650k APS articles for long-term history)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Conceptual relations / trend-prediction heuristics (semantic co-occurrence-based relations and link-formation tendencies; implicit domain heuristics about which topics tend to be connected in the literature)</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Co-occurrence based network construction from titles/abstracts (human + RAKE concept list), computation of 17 network features per concept-pair, supervised learning with a feed-forward neural network to predict link formation 5 years ahead; outlier detection in the learned feature space to propose unusual pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Historic holdout validation (train on earlier SemNet states to predict later states); ROC curve and AUC reporting (AUC2017 = 0.85), qualitative checks by locating historically impactful award-winning topics in the network, and example case studies (e.g., interaction-free measurement & Leggett–Garg inequality).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SEMNET successfully encodes semantic relations and temporal evolution of topics; the supervised predictor attains substantial predictive performance (AUC ≈ 0.85) for whether two concepts will co-occur within five years and produces ranked, personalized suggestions of novel concept-pairs with outlier network properties that may seed research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Knowledge is represented only via co-occurrence in titles/abstracts, which is a coarse proxy for scientific relation; concept extraction required manual curation to remove false positives and synonyms; the approach simplifies multi-faceted semantic relations and may miss nuanced or causal relations; prediction restricted to link-formation (not derivation of formal scientific laws).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Predicting research trends with semantic and neural networks with an application in quantum physics', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3775.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3775.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rapid Automatic Keyword Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised statistical keyword extraction algorithm used to generate candidate concept terms from article titles and abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic keyword extraction from individual documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAKE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A rule- and statistic-based keyword extraction method that identifies candidate keywords by analyzing word co-occurrence, stopword patterns and phrase frequencies; not an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Applied here to quantum physics article titles and abstracts (text mining / NLP preprocessing).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Applied to ~100,000 arXiv quantum-physics articles (in this work); original RAKE paper concerns individual documents.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not targeted at explicit laws; used to extract key concept tokens/phrases that form the nodes of the semantic network (semantic units / concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Unsupervised keyword extraction from titles/abstracts to produce candidate concept list, followed by manual curation (merging synonyms, removing person names, singular/plural normalization).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Implicit human curation and downstream utility (concept list of ~6,300 terms used to build SemNet); no formal intrinsic RAKE evaluation reported in this paper beyond manual cleaning and successful downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RAKE produced an automatically generated set of candidate physics concepts that, after curation and merging with human-made lists, yielded a 6,300-term concept list used as SEMNET nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RAKE can introduce incorrectly identified concepts and person names; output required substantial manual post-processing; statistical keyword extraction does not disambiguate context or nuanced concept senses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Predicting research trends with semantic and neural networks with an application in quantum physics', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3775.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3775.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feed-forward DNN (link predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised feed-forward deep neural network for link prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised neural classifier/regressor that takes 17 engineered network features for an unconnected concept pair and outputs a score for probability of co-occurrence within five years.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom feed-forward neural network (4 fully connected layers; two hidden layers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A bespoke multilayer perceptron used to rank unconnected concept pairs; inputs are 17 numerical network-theoretic features (degree, counts, path counts across years, distances, cosine-similarity etc.), output is a scalar prediction of future link formation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Semantic network features derived from physics literature (quantum physics concepts in SEMNET).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Operates on the SEMNET adjacency-of-6368-concepts built from 750,000 articles; training/validation used historical snapshots (years) of this network.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Predictive rules/heuristics about future co-occurrence/link formation between concepts (empirical trends rather than formal scientific laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised learning on labeled historic link-formation events (pairs that became connected within five years) using engineered graph-based features; the model learns statistical patterns that predict future connections.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Temporal cross-validation using past SemNet states to predict later states; ROC curves and AUC reported (e.g., AUC2017 = 0.85); controlled restriction to pairs sharing <20% neighbors to avoid trivial synonym predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The DNN reliably ranks likely future concept co-occurrences significantly above chance, enabling both accurate link-prediction and downstream personalized suggestion ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Feature-engineering based approach may miss higher-order structure captured by alternative graph-based models; model predicts co-occurrence (bibliographic links) not causal mechanisms or formal scientific principles; potential biases from dataset composition and concept extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Predicting research trends with semantic and neural networks with an application in quantum physics', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3775.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3775.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unsupervised word embeddings (Tshitoyan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised word embeddings capture latent knowledge from materials science literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work showing that unsupervised word embeddings trained on materials-science literature can encode latent domain knowledge useful for discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised word embeddings capture latent knowledge from materials science literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unsupervised word embedding models (e.g., word2vec / skip-gram style embeddings as used in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distributional vector-space embeddings trained unsupervised on large scientific corpora to capture semantic and latent relation structure among tokens/phrases; specific architecture/details are in the cited paper, not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science literature (referenced); suggested for combination with SEMNET in outlook for broader semantic representation.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Latent semantic relations / heuristics and implicit domain knowledge that can support discovery of material rules or associations (implicit 'knowledge' induction rather than explicit formal laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Unsupervised embedding learning on large scientific corpora (referenced as an approach to obtain latent knowledge vectors that could augment network-based methods).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated within this paper (cited as prior work); the referenced paper evaluates its own embeddings and discoveries (see that paper for details).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of how unsupervised embeddings can capture latent scientific knowledge and suggested as a complementary technique to SEMNET for dynamic networks and richer representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embedding-derived associations may be correlative or reflect publication biases; embeddings are not by themselves explicit rules and require downstream interpretation to extract usable scientific heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Predicting research trends with semantic and neural networks with an application in quantum physics', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3775.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3775.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-based ML / GNNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph neural networks and graph-based machine learning models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in outlook as a class of methods that could improve link-prediction on time-evolving semantic networks by directly learning on graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comprehensive survey on graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Neural Networks (GNNs) / graph-based ML</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A family of architectures (message passing / convolutional GNNs, gated graph sequence NNs) designed to operate directly on graph-structured data to learn node/edge representations; specific architectures and sizes are not used in this paper but suggested as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Graph/semantic networks derived from scientific literature (here: quantum physics SEMNET).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not applicable to method mention; would operate on the SEMNET graph (6,368 nodes, >1.7M edges historically).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Learned structural rules/heuristics for link formation and node/edge behavior in knowledge networks; potentially richer relational patterns than hand-engineered features.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Proposed application: train GNNs on temporal graph snapshots to predict link formation or to learn embeddings capturing principled relational structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not performed here; suggested that GNNs could improve prediction quality (no evaluation in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Suggested as promising for improving link-prediction and capturing richer graph patterns compared to the current feature-engineered DNN pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not empirically tested in this work; practical challenges include temporal modeling, scalability to large graphs, and the need to design appropriate supervision for scientific relation induction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Predicting research trends with semantic and neural networks with an application in quantum physics', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3775.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3775.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM (temporal models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long short-term memory recurrent neural network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a potentially useful tool for handling time-dependent (temporal) data in SEMNET to improve predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Long Short-Term Memory (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recurrent neural network architecture designed to model sequences and long-range temporal dependencies; specifics not implemented here but suggested as a future improvement for time-evolving networks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Temporal sequences of network states derived from scientific publications (SEMNET time series).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Temporal predictive rules/heuristics about how semantic relations evolve over time.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Suggested approach: feed temporal features or sequences of graph snapshots into LSTM to capture dynamics for improved link prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this paper; proposed as possible future work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Proposed that LSTM could significantly improve prediction quality by modeling time dependencies in SEMNET.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No empirical results here; LSTMs may face scaling issues with very large numbers of node-pair sequences and require careful design of input representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Predicting research trends with semantic and neural networks with an application in quantum physics', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3775.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3775.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer / attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (Attention is All You Need)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in outlook as a technique from machine translation that could be beneficial to introduce multiple classes of connections within semantic networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (self-attention architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence modeling architecture based on multi-head self-attention; here mentioned as a candidate technique to enrich semantic-network modeling (no implementation in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Potential application to scientific text and to representation/translation of network states (domain: scientific literature / semantic networks).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Potential to induce multi-class relation models or richer mapping between textual corpora and structured network relations (heuristics and relation-types rather than formal laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Suggested use: adaptation of machine-translation style models or attention mechanisms to map text to multi-class relations or to learn richer relation types in semantic networks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this work; proposed as future direction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Transformers are proposed as potentially useful to extend SEMNET to multi-class relations or richer structural encodings, but no experiments were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No empirical evidence in this paper; adapting transformers to graph/time-evolving semantic networks requires nontrivial engineering and large annotated training signals for multi-class relation induction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Predicting research trends with semantic and neural networks with an application in quantum physics', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic keyword extraction from individual documents <em>(Rating: 2)</em></li>
                <li>Unsupervised word embeddings capture latent knowledge from materials science literature <em>(Rating: 2)</em></li>
                <li>A comprehensive survey on graph neural networks <em>(Rating: 2)</em></li>
                <li>Network-based prediction of protein interactions <em>(Rating: 1)</em></li>
                <li>Attention is all you need <em>(Rating: 1)</em></li>
                <li>Long short-term memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3775",
    "paper_id": "paper-5d2b625ff86f0641905290fc4151840f71dbbf8f",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "SEMNET",
            "name_full": "SEMantic NETwork for quantum physics",
            "brief_description": "A semantic knowledge network built from co-occurrence of extracted concepts in titles/abstracts of 750,000 physics papers; used to represent evolving relations between physical concepts and to drive predictions and suggestion generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SEMNET (pipeline: RAKE + co-occurrence network + supervised DNN)",
            "model_description": "A pipeline combining automatic keyword extraction (RAKE) and human-curated concept lists to form a co-occurrence semantic network (nodes = concepts, edges = co-occurrence in title/abstract), coupled to a supervised feed-forward neural network (4 fully connected layers) that ranks unconnected concept pairs by probability of being linked within 5 years.",
            "input_domain": "Physics (quantum physics focus) — multidisciplinary within physics",
            "corpus_size": "750,000 physics articles (100k arXiv quantum-subset used for keyword extraction; 650k APS articles for long-term history)",
            "law_type": "Conceptual relations / trend-prediction heuristics (semantic co-occurrence-based relations and link-formation tendencies; implicit domain heuristics about which topics tend to be connected in the literature)",
            "distillation_method": "Co-occurrence based network construction from titles/abstracts (human + RAKE concept list), computation of 17 network features per concept-pair, supervised learning with a feed-forward neural network to predict link formation 5 years ahead; outlier detection in the learned feature space to propose unusual pairs.",
            "evaluation_method": "Historic holdout validation (train on earlier SemNet states to predict later states); ROC curve and AUC reporting (AUC2017 = 0.85), qualitative checks by locating historically impactful award-winning topics in the network, and example case studies (e.g., interaction-free measurement & Leggett–Garg inequality).",
            "results_summary": "SEMNET successfully encodes semantic relations and temporal evolution of topics; the supervised predictor attains substantial predictive performance (AUC ≈ 0.85) for whether two concepts will co-occur within five years and produces ranked, personalized suggestions of novel concept-pairs with outlier network properties that may seed research ideas.",
            "limitations_or_challenges": "Knowledge is represented only via co-occurrence in titles/abstracts, which is a coarse proxy for scientific relation; concept extraction required manual curation to remove false positives and synonyms; the approach simplifies multi-faceted semantic relations and may miss nuanced or causal relations; prediction restricted to link-formation (not derivation of formal scientific laws).",
            "uuid": "e3775.0",
            "source_info": {
                "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "RAKE",
            "name_full": "Rapid Automatic Keyword Extraction",
            "brief_description": "An unsupervised statistical keyword extraction algorithm used to generate candidate concept terms from article titles and abstracts.",
            "citation_title": "Automatic keyword extraction from individual documents",
            "mention_or_use": "use",
            "model_name": "RAKE",
            "model_description": "A rule- and statistic-based keyword extraction method that identifies candidate keywords by analyzing word co-occurrence, stopword patterns and phrase frequencies; not an LLM.",
            "input_domain": "Applied here to quantum physics article titles and abstracts (text mining / NLP preprocessing).",
            "corpus_size": "Applied to ~100,000 arXiv quantum-physics articles (in this work); original RAKE paper concerns individual documents.",
            "law_type": "Not targeted at explicit laws; used to extract key concept tokens/phrases that form the nodes of the semantic network (semantic units / concepts).",
            "distillation_method": "Unsupervised keyword extraction from titles/abstracts to produce candidate concept list, followed by manual curation (merging synonyms, removing person names, singular/plural normalization).",
            "evaluation_method": "Implicit human curation and downstream utility (concept list of ~6,300 terms used to build SemNet); no formal intrinsic RAKE evaluation reported in this paper beyond manual cleaning and successful downstream performance.",
            "results_summary": "RAKE produced an automatically generated set of candidate physics concepts that, after curation and merging with human-made lists, yielded a 6,300-term concept list used as SEMNET nodes.",
            "limitations_or_challenges": "RAKE can introduce incorrectly identified concepts and person names; output required substantial manual post-processing; statistical keyword extraction does not disambiguate context or nuanced concept senses.",
            "uuid": "e3775.1",
            "source_info": {
                "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Feed-forward DNN (link predictor)",
            "name_full": "Supervised feed-forward deep neural network for link prediction",
            "brief_description": "A supervised neural classifier/regressor that takes 17 engineered network features for an unconnected concept pair and outputs a score for probability of co-occurrence within five years.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom feed-forward neural network (4 fully connected layers; two hidden layers)",
            "model_description": "A bespoke multilayer perceptron used to rank unconnected concept pairs; inputs are 17 numerical network-theoretic features (degree, counts, path counts across years, distances, cosine-similarity etc.), output is a scalar prediction of future link formation.",
            "input_domain": "Semantic network features derived from physics literature (quantum physics concepts in SEMNET).",
            "corpus_size": "Operates on the SEMNET adjacency-of-6368-concepts built from 750,000 articles; training/validation used historical snapshots (years) of this network.",
            "law_type": "Predictive rules/heuristics about future co-occurrence/link formation between concepts (empirical trends rather than formal scientific laws).",
            "distillation_method": "Supervised learning on labeled historic link-formation events (pairs that became connected within five years) using engineered graph-based features; the model learns statistical patterns that predict future connections.",
            "evaluation_method": "Temporal cross-validation using past SemNet states to predict later states; ROC curves and AUC reported (e.g., AUC2017 = 0.85); controlled restriction to pairs sharing &lt;20% neighbors to avoid trivial synonym predictions.",
            "results_summary": "The DNN reliably ranks likely future concept co-occurrences significantly above chance, enabling both accurate link-prediction and downstream personalized suggestion ranking.",
            "limitations_or_challenges": "Feature-engineering based approach may miss higher-order structure captured by alternative graph-based models; model predicts co-occurrence (bibliographic links) not causal mechanisms or formal scientific principles; potential biases from dataset composition and concept extraction.",
            "uuid": "e3775.2",
            "source_info": {
                "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Unsupervised word embeddings (Tshitoyan et al.)",
            "name_full": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "brief_description": "A referenced work showing that unsupervised word embeddings trained on materials-science literature can encode latent domain knowledge useful for discovery.",
            "citation_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "mention_or_use": "mention",
            "model_name": "Unsupervised word embedding models (e.g., word2vec / skip-gram style embeddings as used in referenced work)",
            "model_description": "Distributional vector-space embeddings trained unsupervised on large scientific corpora to capture semantic and latent relation structure among tokens/phrases; specific architecture/details are in the cited paper, not specified here.",
            "input_domain": "Materials science literature (referenced); suggested for combination with SEMNET in outlook for broader semantic representation.",
            "corpus_size": null,
            "law_type": "Latent semantic relations / heuristics and implicit domain knowledge that can support discovery of material rules or associations (implicit 'knowledge' induction rather than explicit formal laws).",
            "distillation_method": "Unsupervised embedding learning on large scientific corpora (referenced as an approach to obtain latent knowledge vectors that could augment network-based methods).",
            "evaluation_method": "Not evaluated within this paper (cited as prior work); the referenced paper evaluates its own embeddings and discoveries (see that paper for details).",
            "results_summary": "Cited as an example of how unsupervised embeddings can capture latent scientific knowledge and suggested as a complementary technique to SEMNET for dynamic networks and richer representations.",
            "limitations_or_challenges": "Embedding-derived associations may be correlative or reflect publication biases; embeddings are not by themselves explicit rules and require downstream interpretation to extract usable scientific heuristics.",
            "uuid": "e3775.3",
            "source_info": {
                "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Graph-based ML / GNNs",
            "name_full": "Graph neural networks and graph-based machine learning models",
            "brief_description": "Mentioned in outlook as a class of methods that could improve link-prediction on time-evolving semantic networks by directly learning on graph structure.",
            "citation_title": "A comprehensive survey on graph neural networks",
            "mention_or_use": "mention",
            "model_name": "Graph Neural Networks (GNNs) / graph-based ML",
            "model_description": "A family of architectures (message passing / convolutional GNNs, gated graph sequence NNs) designed to operate directly on graph-structured data to learn node/edge representations; specific architectures and sizes are not used in this paper but suggested as future work.",
            "input_domain": "Graph/semantic networks derived from scientific literature (here: quantum physics SEMNET).",
            "corpus_size": "Not applicable to method mention; would operate on the SEMNET graph (6,368 nodes, &gt;1.7M edges historically).",
            "law_type": "Learned structural rules/heuristics for link formation and node/edge behavior in knowledge networks; potentially richer relational patterns than hand-engineered features.",
            "distillation_method": "Proposed application: train GNNs on temporal graph snapshots to predict link formation or to learn embeddings capturing principled relational structure.",
            "evaluation_method": "Not performed here; suggested that GNNs could improve prediction quality (no evaluation in this paper).",
            "results_summary": "Suggested as promising for improving link-prediction and capturing richer graph patterns compared to the current feature-engineered DNN pipeline.",
            "limitations_or_challenges": "Not empirically tested in this work; practical challenges include temporal modeling, scalability to large graphs, and the need to design appropriate supervision for scientific relation induction.",
            "uuid": "e3775.4",
            "source_info": {
                "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "LSTM (temporal models)",
            "name_full": "Long short-term memory recurrent neural network",
            "brief_description": "Mentioned as a potentially useful tool for handling time-dependent (temporal) data in SEMNET to improve predictive performance.",
            "citation_title": "Long short-term memory",
            "mention_or_use": "mention",
            "model_name": "Long Short-Term Memory (LSTM)",
            "model_description": "A recurrent neural network architecture designed to model sequences and long-range temporal dependencies; specifics not implemented here but suggested as a future improvement for time-evolving networks.",
            "input_domain": "Temporal sequences of network states derived from scientific publications (SEMNET time series).",
            "corpus_size": null,
            "law_type": "Temporal predictive rules/heuristics about how semantic relations evolve over time.",
            "distillation_method": "Suggested approach: feed temporal features or sequences of graph snapshots into LSTM to capture dynamics for improved link prediction.",
            "evaluation_method": "Not evaluated in this paper; proposed as possible future work.",
            "results_summary": "Proposed that LSTM could significantly improve prediction quality by modeling time dependencies in SEMNET.",
            "limitations_or_challenges": "No empirical results here; LSTMs may face scaling issues with very large numbers of node-pair sequences and require careful design of input representation.",
            "uuid": "e3775.5",
            "source_info": {
                "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Transformer / attention",
            "name_full": "Transformer (Attention is All You Need)",
            "brief_description": "Mentioned in outlook as a technique from machine translation that could be beneficial to introduce multiple classes of connections within semantic networks.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "mention",
            "model_name": "Transformer (self-attention architectures)",
            "model_description": "Sequence modeling architecture based on multi-head self-attention; here mentioned as a candidate technique to enrich semantic-network modeling (no implementation in the paper).",
            "input_domain": "Potential application to scientific text and to representation/translation of network states (domain: scientific literature / semantic networks).",
            "corpus_size": null,
            "law_type": "Potential to induce multi-class relation models or richer mapping between textual corpora and structured network relations (heuristics and relation-types rather than formal laws).",
            "distillation_method": "Suggested use: adaptation of machine-translation style models or attention mechanisms to map text to multi-class relations or to learn richer relation types in semantic networks.",
            "evaluation_method": "Not evaluated in this work; proposed as future direction.",
            "results_summary": "Transformers are proposed as potentially useful to extend SEMNET to multi-class relations or richer structural encodings, but no experiments were reported.",
            "limitations_or_challenges": "No empirical evidence in this paper; adapting transformers to graph/time-evolving semantic networks requires nontrivial engineering and large annotated training signals for multi-class relation induction.",
            "uuid": "e3775.6",
            "source_info": {
                "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic keyword extraction from individual documents",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "rating": 2
        },
        {
            "paper_title": "A comprehensive survey on graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "Network-based prediction of protein interactions",
            "rating": 1
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 1
        },
        {
            "paper_title": "Long short-term memory",
            "rating": 1
        }
    ],
    "cost": 0.015533499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Predicting Research Trends with Semantic and Neural Networks with an application in Quantum Physics</h1>
<p>Mario Krenn1,2,3,4,* and Anton Zeilinger1,2,†
1 Vienna Center for Quantum Science &amp; Technology (VCQ),
Faculty of Physics, University of Vienna, Austria.
2 Institute for Quantum Optics and Quantum Information (IQOQI),
Austrian Academy of Sciences, Vienna, Austria.
3 Department of Chemistry &amp; Computer Science, University of Toronto, Canada.
4 Vector Institute for Artificial Intelligence, Toronto, Canada.
(Dated: January 9, 2020)</p>
<h6>Abstract</h6>
<p>The vast and growing number of publications in all disciplines of science cannot be comprehended by a single human researcher. As a consequence, researchers have to specialize in narrow subdisciplines, which makes it challenging to uncover scientific connections beyond the own field of research. Thus access to structured knowledge from a large corpus of publications could help pushing the frontiers of science. Here we demonstrate a method to build a semantic network from published scientific literature, which we call SEMNET. We use SEMNET to predict future trends in research and to inspire new, personalized and surprising seeds of ideas in science. We apply it in the discipline of quantum physics, which has seen an unprecedented growth of activity in recent years. In SEMNET, scientific knowledge is represented as an evolving network using the content of 750,000 scientific papers published since 1919. The nodes of the network correspond to physical concepts, and links between two nodes are drawn when two physical concepts are concurrently studied in research articles. We identify influential and prize-winning research topics from the past inside SEMNET thus confirm that it stores useful semantic knowledge. We train a deep neural network using states of SEMNET of the past, to predict future developments in quantum physics research, and confirm high quality predictions using historic data. With the neural network and theoretical network tools we are able to suggest new, personalized, out-of-the-box ideas, by identifying pairs of concepts which have unique and extremal semantic network properties. Finally, we consider possible future developments and implications of our findings.</p>
<h2>INTRODUCTION</h2>
<p>A computer algorithm with access to a large corpus of published scientific research could potentially make genuinely new contributions to science. With such a body of knowledge, the algorithm could derive new scientific insights that are unknown to human researchers and note contradictions within existing scientific knowledge [1, 2]. This level of automation of science is more in the realm of science-fiction than reality at present. However, algorithms with access to and the capability of extracting semantic knowledge from the scientific literature can be employed in manifold ways to assist scientists and thereby augment scientific progress. As an example, the evaluation of whether an idea is novel or surprising depends crucially on already-existing knowledge. Thus a computer algorithm with the capability to propose new, useful ideas or potential avenues of research will necessarily require access to published scientific literature - which forms at least partially the body of human knowledge in a scientific field.</p>
<p>Knowledge can be portrayed using semantic networks that represent semantic relations between concepts in a network [3]. Over the last few years, significant results have been obtained by automatically analyzing the large corpus of scientific literature [4–</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Creating a semantic network for quantum physics (SEMNET). The nodes represent quantum physical concepts, and the edges (connections between nodes) indicate how frequently two concepts are investigated jointly in the scientific literature. The concept list is created using human-made lists (from Wikipedia categories and quantum physics books) and automatically generated lists using natural language processing tools on 100,000 quantum physics articles from the online preprint repository arXiv (this is indicated by black arrows). An edge between two concepts is drawn when both concepts appear in the abstract of a scientific paper (indicated by blue arrows). The scientific database consists of 750,000 physics papers, 100,000 from arXiv and 650,000 papers published by the American Physical Society (APS) since 1919.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Diagrammatic inner working of SEMNET. Human-generated concept lists (from Wikipedia and books) are combined with automatically generated lists (with natural language processing, using RAKE on 100,000 arXiv articles) to generate a list of quantum physics concepts. Each concept forms a link in a semantic network. The edges are formed when two concepts co-appeare in a title or abstract of any of the 750,000 papers (from arXiv and APS). A mini-version of SEMNET is shown, using parts of three articles from APS. Edges carry temporal information of their formation year, which leads to an evolution of the semantic network SEMNET over time.</p>
<p>6], including the development of semantic networks in several scientific disciplines.</p>
<p>In biochemistry, a semantic network has been built using a well-defined list of molecule names (which correspond to the nodes of the network) and forming edges when two components co-appeare in the abstract of a scientific paper. The network was derived from millions of papers published over 30 years, and the authors identify a more efficient, collective strategy to explore the knowledge network of biochemistry [7, 8]. In [9], a semantic network was created using 100,000 papers from astronomy, ecology, economy and mathematics. The nodes represent ideas or concepts (generated through automated generation of key-concepts in large bodies of texts [10]). The authors used the network to draw connections between human innovation process and random walks. In the field of neuroscience, semantic networks have been used to map the landscape of the field [11, 12]. Papers from the interdisciplinary journal PNAS have been used to investigate sociological properties such as inter-disciplinary research [13].</p>
<p>Here, we show how to build and use a semantic network for quantum physics, which we call SEMNET. It is built from 750,000 scientific papers in physics published since 1919. In the network we identify a number of historic award-winning concepts, indicating that SEMNET carries useful semantic knowledge. The evolution of such a large network allows us to use an artificial neural network for predicting research concepts that scientists will investigate in the next five years. Finally, we demonstrate the power of SEMNET to suggest personalized, novel and unique directions for future research <sup>1</sup>.</p>
<p>Our work differs in several aspects from previous semantic networks created from scientific literature. First, we use machine learning to draw conclusions from earlier states to SEMNET's future state, which enables us to make predictions about the future research trends of the discipline. Second, we use network theoretical tools and machine learning to identify pairs of concepts with exceptional network properties. Those concept combinations can be restricted to the research interest of a specific scientist. This ability allows us to not only predict but also suggest uninvestigated concept pairs which human scientists might not have identified because they are out of the own sub-field, but which have properties that indicate an exceptional relation. They could be a seed of a new, out-of-the-box idea. Third, we apply SEMNET to quantum physics, which has seen an enormous growth during the last decade due to the potential transformative technologies. The growth can be seen in the establishment of several high-quality journals for quantum research (such as Quantum, np) (Quantum Information, IOP's Quantum Science &amp; Technology) and multi-billion dollar fundings from governments and strong involvement of private companies and startups worldwide. The growth rate leads to enormous increase in scientific results and publications, which are difficult to follow for individual researchers – thus quantum physics is an ideal test-bed for SEMNET.</p>
<h3>SEMANTIC NETWORK OF QUANTUM PHYSICS</h3>
<p>A semantic network, or knowledge network, represents relations between concepts in the form of a network. Now we describe in more detail how the network is built, especially how the concept list is generated and how links are formed. A schematic illustration can be seen in Figure 1, more details in Figure 2.</p>
<p><sup>1</sup> Code and details: https://github.com/MarioKrenn6240/SEMNET</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. The evolution of quantum physics research observed using SEMNET, reflected in the change in number of articles that contain a concept or concept pair per year from 1987 to 2017. (a) Newly-emerged concepts and their growth in popularity over a five-year period after emergence. Shown are the strongest growing concepts of a five-year period, which have not been mentioned before that period. (b) Newly-connected pairs of concepts that become strongly influential in the scientific community in a five-year period. Shown are the strongest growing connections of concept pairs that already existed before the connection was drawn, which have not been connected before that period. Many emergent concepts and connections can be related to important discoveries and understandings in quantum science.</p>
<h3>Creation of the concept list</h3>
<p>We generate the concept list via two independent methods. First, we use human-made lists of physical concepts. These concepts are compiled from the indices of 13 quantum physics books (which were available to us in a digital form), as well as titles of Wikipedia articles that are linked in a quantum physics category. This human-made collection contains approximately 5000 entries physical concepts.</p>
<p>We extend the human-generated list with an automatically generated list of physical concepts. For this, we apply a natural language processing tool called RAKE (Rapid Automatic Keyword Extraction) [14] to the titles and abstracts of approximately 100,000 articles published in quantum physics categories on the arXiv preprint server, which we chose to optimize the list for current research topics in quantum physics. RAKE is based on statistical text analysis, and can automatically find relevant keywords in texts. We combine the human- and machine-generated lists of concepts and further optimize them to delete incorrectly identified concepts (which were introduced by imperfections of the statistical analysis of RAKE) and names of people (which are not concepts), merge synonyms and normalize for the singular and plural of the same concept. Ultimately, this yields a list of 6,300 terms. As an example, five randomly chosen examples are <em>three level system</em>, <em>photon antibunching</em>, <em>chemical shift</em>, <em>neutron radiation</em> and <em>unconditionally secure quantum bit commitment</em>. Each of these quantum physics concepts is a node in SEMNET.</p>
<h3>Creation of the network</h3>
<p>To form connections between different quantum physics concepts, we use 100,000 articles of quantum physics categories on arXiv, and the dataset of all 650,000 articles ever published by the APS. We chose these two data sources because the APS database contains peer-reviewed physics papers from the last 100 years (allowing for investigation of long-term trends), while the arXiv database contains specific quantum physics papers, allowing for more precise coverage of the quantum physics research trends.</p>
<p>Whenever two concepts occur together in a title or an abstract of an article, we interpret that as a semantic connection between these concepts, and add a unique link between the two corresponding nodes in the network. Relations between two concepts can take many forms. Concepts may be put together for example when mathematical tool (such as <em>Schmidt rank</em>) is used to investigate a specific quantum system (such as <em>vector beam</em> or <em>exciton polariton</em>), or when insights from a specific technique (such as <em>lasing without inversion</em> or <em>rabi oscillation</em>) lead to conclusions about another property (such as <em>transport property</em> or <em>atom transition frequency</em>) or when fundamental ideas (such as <em>quantum decoherence</em> or <em>quantum energy teleportation</em>) are studied in the context of foundational experiments (such as <em>delayed choice experiment</em> or <em>Mermin inequality</em>). While this method clearly cannot represent all quantum physics knowledge, it represents elements of its semantic structure, which we demonstrate in what follows.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Artificial Neural Network for predicting the future of quantum physics research, using the evolution of the semantic network SEMNET. For each unconnected pair of concepts at a specific year, we derive a vector of 17 network properties (such as distance or cosine similarity). In the training phase, we input these network properties into an artificial neural network, and ask the question whether they will be connected 5 years later. SEMNET of 2017 is used for supervision. After training, we can apply the neural network to SEMNET of 2017, and ask what will have happened until the year 2022.</p>
<p>The resulting network SEMNET has 6368 vertices with more than 1.7 million edges (drawn from more than 15 million concept pairs pulled from 750,000 physics articles), using physics articles from 1919 to december 2017.</p>
<h2>RESULTS</h2>
<h3>Past quantum physics trends</h3>
<p>First, we use the evolution of the semantic network to identify impactful emerging fields of research in the past. We define <em>emerging fields</em> as either concepts or concept pairs which have grown significantly after they have been introduced or connected for the first time, over periods of five years.</p>
<p>Figure 3a shows the quantum physics topics that have grown the fastest (in terms of numbers of papers in which they have been mentioned) after their emergence, from the years 1987 to 2017. Figure 3b shows, for each year, which two-concept combinations have grown the fastest in the first five years after they have been first connected. In Figure 3, many of the emerging fields clearly correspond to important discoveries, advances in understanding and shifts of thought within quantum science research. One of the fastest growing concepts is <em>Qubit</em>, which emerged in 1995 (first in april in a Phys.Rev.A paper by Schumacher [15], then in arXiv preprints by Chuang &amp; Yamamoto [16] and by Knill [17, 18]). Qubits are the basic units of quantum information – generalizing classical bits to coherent quantum superpositions, and connect quantum mechanics and information science. The emergence of the qubit can be interpreted as the start of the discipline of <em>quantum information science</em>. Enormous growth is seen for topics connected to graphene, starting in 2005, the discoverers of which were awarded the 2010 Nobel Prize in Physics. Interesting, graphene itself was mentioned (in our data collection) already back in the early 1990s in Phys.Rev.B papers [19–21], when it was not a strongly emergent concept itself. Strong growth in research into topological materials can be observed from approximately 2008; the Nobel Prize in Physics was subsequently awarded in this area in 2016. Aaronson's and Arkhipov's approach to achieving <em>quantum supremacy</em> [22] using linear photonic networks, termed BosonSampling [23], achieved considerable attention (with more than 600 citations since its introduction in 2011, and considerable experimental efforts into this direction). Since 2012, the application of machine learning to quantum physics has become a prominent and diverse topic of research, that falls under the umbrella of <em>quantum machine learning</em> (recently summarized in two prominent reviews [24, 25], and also observable by the foundation of a novel high-quality journal for this topic, Springer Quantum Machine Intelligence). These findings confirm that SEMNET contains useful semantic information.</p>
<h3>Predictive ability of the SEMNET</h3>
<p>Having used SEMNET to study past quantum trends, we investigate its ability to provide projections of knowledge developments in the future. This essential question in network science is called <em>link-prediction problem</em>, and asks which new link will be formed between unconnected vertices of the network in the future given the current state of the network (for a detailed investigation of the link-prediction problem in network theory, see [26]). We apply this problem in the context of semantic networks which are generated from published scientific literature. In the present case looking at the field of quantum physics, we ask which</p>
<p>two concepts that have not yet been studied together might be investigated together in a scientific article over the next five years. To answer this question, we use an artificial neural network, with four fully connected layers (two hidden layers). The structure of the neural network and its training is shown in Figure 4. Its task is to rank all unconnected pairs of concepts (roughly 5% of all edges have been drawn by the end of 2017), starting with the pair that is most likely to be connected five years, up to the pair that most likely stays unconnected. Ultimately we want to apply the neural network to the current SemNet and predict the future trends. To validate its quality, we first input to the neural network past states of SemNet (for example, containing data only up to 2002), and train it to predict new links by 2007. After the training, we apply this network on 2007 data and validate its quality for data of the year 2012 (which it has never seen before).</p>
<p>The semantic network is very large (consisting of 6368×6368 entries for each year, which are the number of possible connections between the 6368 quantum physics concepts, compared to 28×28 pixels for the famous MNIST dataset of handwritten images, and 256×256 pixels for ImageNet [27]), and involves combinatorial, graph-based information which are more structured than images (see for example [28]). For that reason, it is an unsuitable direct input to the neural network. Instead, we compute semantic network properties for each pair of concepts. For each pair of concepts $(c_{i},c_{j})$ that are unconnected in SemNet, we calculate 17 network properties $p_{i,j}=(p_{i,j}^{1},p_{i,j}^{2},...,p_{i,j}^{17})$ where $p_{i,j}^{k}\in \mathbb{R}$. Here, $p_{i,j}^{1}$ and $p_{i,j}^{2}$ are the degrees of concept $c_{i}$ and $c_{j}$, and $p_{i,j}^{3}$ and $p_{i,j}^{4}$ are the numbers of papers in which they are mentioned. While these four properties are purely local, $p_{i,j}^{5}$ is the cosine-similarity between the two concepts, which corresponds to the number of common neighbors. A cosine similarity of one indicates that the terms might be synonyms. The next nine properties indicate the number of paths with lengths of two, three and four between the physics concepts in the current and previous two years. These properties allow us to draw conclusions from the evolution over time of various topics as tracked by SemNet. The choice to use large path lengths as one of the properties is strengthened by a very recent observation that the paths of length 3 (L3) are crucial for link prediction tasks in a network for protein interactions [29]. Finally, the last three properties correspond to three different measures of distance between the two concepts. More details can be seen in the SI.</p>
<p>We explain these properties on a concrete pair of concepts, <em>interaction-free measurement</em> and <em>Leggett-Garg inequality</em>. (We chose the example randomly, from unconnected concepts that had been mentioned</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Quantifying the prediction quality of the neural network regarding whether unconnected pairs will be connected within 5 years, using a receiver operating characteristic (ROC) curve. The y-axis shows the true-positive (TP) rate (rate of pairs that have been correctly identified to be connected within 5 years). The x-axis shows the false positive (FP) rate of predictions – concept pairs that have falsely been predicted to be connected. We restrict ourselves to concept pairs which share less than 20% of their neighbors, to prevent predictions of terms with similar semantical meaning. A perfect neural network would have TP = 1 while FP = 0. A network that classifies 50% of true instances correctly, and misclassifies 10% false instance as true would have TP = 0.5 and FP = 0.1. A random classifier is incorrect half the time and thus lies along the diagonal. The area under the curve (AUC) for a perfect neural network is 1, while for random predictions, it is AUC = 0.5. The AUC can be interpreted as the probability that the neural network will rank a randomly chosen true instance higher than a randomly-chosen negative instance [30]. The ROC validation curves for 1995, 2005 and 2017 (trained with SemNet using data from only 1990, 2000 and 2012 and earlier, respectively) are consistently and significantly non-random, with AUC2017 = 0.85. These results show that the neural network can learn to predict future research interests in quantum physics, based on historical information to a high accuracy.</p>
<p>individually more than 30 times.) The concept $c_{2526}$ represents "interaction-free measurement which is mentioned in 60 abstracts and has 135 connections to other concepts by 2012. The concept $c_{2819}$ represents the "Leggett-Garg inequality", which occurs in 33 abstracts and has 141 connections to other concepts by the end of 2012. These two concepts were not connected in SemNet as of 2012, therefore, the 15th property, their network distance, is $p_{2526,2819}^{15} = 2$ (neighbors have a distance of one, in other words, there is a direct path connecting them of length one). In 2012, the two concepts have a cosine-similarity $p_{2526,2819}^{2}= 0.228$, meaning that 22.8% of their neighbors are shared. Two years later, in 2014 an article on arXiv mentioned both of these concepts in the abstract and the work was later published [31] and featured [32] in the high-impact journal <em>Physical Review X</em>, achieving approximately 100 citations within</p>
<p>four years. This example indicates that drawing first connections between concepts can lead to significant scientific insights.</p>
<p>The 17 properties for each unconnected concept pair in SEMNET are used by the neural network to estimate which pairs of quantum physics concepts are likely to be connected within 5 years and which are not.</p>
<p>To quantify the quality of the predictions, we employ a commonly-used technique called the receiver operating characteristic (ROC) curve [30]. For this, the neural network is used to classify unconnected nodes into two sets: one set that is connected after five years, and a set that is non-connected. Figure 4 shows a significant ability to predict connections between pairs of topics – even through we restrict ourselves to pairs that share less than 20% of their neighbors (to prevent predictions of concepts which have similar meaning). This indicates that even research that draws new connections between concepts, can be predicted with high quality.</p>
<h3>PROPOSING FUTURE RESEARCH TOPICS</h3>
<p>Next, we attempt to use SEMNET and the artificial neural network to suggest new, potentially fruitful research directions in quantum physics. While it is interesting and useful to understand future trends, it potentially cannot by itself lead to surprising or out-of-the-box ideas (otherwise they would not be predictable). Therefore, we extend our previous approach with network theoretic tools, to identify concept pairs with exceptional network-theoretic properties. Furthermore, since science is conducted by (groups of) individual scientists, suggestions for proposed new research directions need to be personalized (otherwise, we would obtain suggestions for topics in which nobody is an expert in – which may be potentially interesting but limited in applicability).</p>
<p>How do we obtain suggestions for an individual scientist? What we find interesting and surprising strongly depends on what we already know. To gauge that, we need to investigate a given scientist's previously-published body of research papers and extract a list of concepts (from the concept list generated before) that define that person's personal research agenda(s). We define key concepts as concepts investigated over-proportionally often by the scientist, compared to the relative frequency of that concept in all 750,000 papers. Each concept $c_i$ in the papers authored by the scientist has a probability $p_{\text{scientist}}(c_i)$ that we calculate by the the number of occurrences of the concept $N(c_i)$ divided by the sum of occurrences of all concepts, which is $p_{\text{scientist}}(c_i) = \frac{N(c_i)}{\sum_i N(c_i)}$. Each concept also has a probability of occurring in all 750,000 papers that we use, written as $p_{\text{total}}(c_i)$ =</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Personalized prediction of topic pairs that could form future research directions for a given scientist. Each dot represents one unconnected pair of physical concepts. The concepts in use are filtered by a scientist's previous research agenda (see main text). The dot is placed in a three-dimensional space, which is proscribed by the properties of SEMNET and the predictions of the neural network. One axis is the neural network predictions of whether two unconnected points will be connected in 2022 (the predictions -0.5 stand for very unlikely, 0.5 is very likely). The y-axis represents the average (normalized) degree of the pair (the concept with the highest degree in the complete network has a degree of 1). The z-axis is the cos-similarity, which is the ratio of shared neighbors in the networks of the two concepts. The color of the dots represents the distance from the most common, average point in this space – darker dots are further away from the average. Outliers represent pairs of concepts with a unique network property, which make them ideal candidate suggestions.</p>
<p>$$\frac{M(c_i)}{\sum_i M(c_i)},$$ where $M(c_i)$ is the number of occurrences of the concept $c_i$ in all 750,000 articles. The ratio $r_{\text{scientist}}(c_i) = \frac{p_{\text{scientist}}(c_i)}{p_{\text{total}}(c_i)}$ indicates the research agenda of the scientist. A value of $r_{\text{scientist}}(c_i) &gt; 1$ shows that the scientist investigates the concept $c_i$ overproportionally often.</p>
<p>Our approach is to identify personalized suggestions of pairs of concepts that have never been connected. The concepts with $r_{\text{scientist}}(c_i) &gt; 1$ value are paired with all of the other 6,368 concepts. This translates to a list of potentially 100,000s of possible topic pairs. For further usability, we introduce a way to sort the candidate suggestions. Suggestions can be sorted by identifying concept pairs with unique and unusual properties. For each pair of concepts, we have already calculated 18 different network properties: 17 properties which have been used by the neural network for generating predictions, and the prediction value itself. Together, these properties define a multi-dimensional</p>
<p>space in which the location of each concept pair depends on its network properties.</p>
<p>To identify unusual and unique concept pairs, we search for outliers in this high-dimensional space. An outlier indicates a pair of concepts that is uniquely located in the space, and thus has unique properties in the semantic SemNet network. We can visualize, for an anonymous example scientist, a 3-dimensional projection of the high-dimensional space in Fig. 6. There, every dot corresponds to a concept pair which is located according to its network properties. Outliers can be identified by the darkness of their color.</p>
<p>A few suggestion from SemNet, for the example scientist: Some of the highest predicted pairs (from Top10) are orbital angular momentum \&amp; magnetic skyrmion, spin orbit coupling \&amp; quantum sensing or dicke model \&amp; cloning, filtered for highly predicted, uncommon pairs (cosine similarity $&lt;0.03$; from Top10): topos theory \&amp; cyclic operation, critical exponent \&amp; reed muller code, quantum key distribution \&amp; adhm construction. Unrestricted concept lists (normalized concept degree $&lt;0.1$; from Top10): atom cavity system \&amp; mode volume, entanglement of formation \&amp; multiqubit state, neutrino oscillation \&amp; dark photon. For more examples, see SI.</p>
<h2>OUTLOOK</h2>
<p>Machine Learning - Graph-based machine learning models, which have been studied in recent years, could improve prediction qualities in the linkprediction task, for example see [28, 33, 34]. Furthermore, as SemNet represents a time evolution of quantum physics' semantic network, applying efficient tools for handling time-dependent data, such as a long short-term memory [35] might further significantly improve the prediction quality. Application of techniques from machine translation could be beneficial to introduce multiple classes of connections within semantic networks [36]. Additionally, combining our approach with unsupervised embedding of scientific literature, as shown in [37] could lead to interesting, dynamic networks.</p>
<p>Network Theory and Science of Science - Currently, SemNet represents connections between concepts that appear in the scientific literature. This is of course a vast simplification of scientific knowledge, as concepts in natural languages can have a manifold of relations [38]. An extension could employ more complex structures for knowledge representation, such as hyper-graphs [39]. The concept list, which represents the nodes of SemNet, can be improved by various different, sophisticated ways for generating of lists of concepts and categories [10, 40]. The extension to combinations of more than pairs of concepts will lead
to more complex knowledge representations. Furthermore, it would be insightful to fold into the semantic network numbers of article citations, which is, at least in the field of science, frequently used as a proxy for scientific impact (see [41-43], for example). This may enable the prediction of future research directions to be made taking into consideration the highest potential impact, potentially accelerating the evolution of individual scientific knowledge [44, 45].</p>
<p>Surprisingness - In this work, we place pairs of concepts in an abstract high-dimensional space and identify outliers that have unique and potentially valuable properties. It would be interesting to apply more, and different measures of surprisingness. An interesting example is the information-based Bayesian surprise function, which has been introduced in the context of human attention [46] and successfully applied to the subfield of computational creativity [47, 48]. In order to achieve further progress, it would be important to further explore and genuinely understand what human scientists consider as surprising and creative.</p>
<h2>DISCUSSION</h2>
<p>We show how to create a semantic network in the field of quantum physics, demonstrate its useage to predict future trends in the field and how it can be used to suggest pairs of concepts, which are not yet investigated jointly, but have distinct network properties. We show how to filter the suggestions for the research agenda of an individual scientist. The approach presented here is independent of the discipline of science. As such it can be applied to other fields of research.</p>
<p>This can be interpreted as one potential road towards computer-inspired science, in the following sense: We imagine cases (which we believe is possible) where SemNet produces seeds or inspirations of unusual ideas or directions of thoughts, that a researcher alone might not have thought of. The subsequent, successful interpretation and scientific execution of the suggestions fully remains the task of a creative, human scientist.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>MK thanks James A. Evans and Sasha Belikov for exciting discussions of metaknowledge research and automation of science, and Jacob G. Foster for a short but influential conversation at the International Symposium on Science of Science 2016. Furthermore, we would like to acknowledge Nora Tischler, Armin Hochrainer, Robert Fickler, Radek Lapkiewicz, Manuel Erhard and Philipp Haslinger for many in-</p>
<p>teresting discussion on related topics. The authors also thank the APS (American Physical Socienty) for providing access to the database of all published articles in APS journals. The authors thank Xuemei Gu for the illustrations of Figure 1 and 4. This work was supported by the Austrian Academy of Sciences (ÖAW), University of Vienna via the project QUESS and the Austrian Science Fund (FWF) with SFB F40 (FOQUS) and the Erwin Schrödinger fellowship No. J4309.</p>
<ul>
<li>mario.krenn@univie.ac.at
${ }^{\dagger}$ anton.zeilinger@univie.ac.at</li>
</ul>
<p>[1] J.A. Evans and A. Rzhetsky, Advancing science through mining libraries, ontologies, and communities. Journal of Biological Chemistry 286, 2365923666 (2011).
[2] J. You, Darpa sets out to automate research. Science 347, 465 (2015).
[3] F. Lehmann, Semantic networks in artificial intelligence. (Elsevier Science Inc., 1992).
[4] J.A. Evans and J.G. Foster, Metaknowledge. Science 331, 721-725 (2011).
[5] A. Zeng, Z. Shen, J. Zhou, J. Wu, Y. Fan, Y. Wang and H.E. Stanley, The science of science: From the perspective of complex systems. Physics Reports 714, $1-73$ (2017).
[6] S. Fortunato, C.T. Bergstrom, K. Börner, J.A. Evans, D. Helbing, S. Milojević, A.M. Petersen, F. Radicchi, R. Sinatra, B. Uzzi and others, Science of science. Science 359, eaao0185 (2018).
[7] J.G. Foster, A. Rzhetsky and J.A. Evans, Tradition and innovation in scientists' research strategies. American Sociological Review 80, 875-908 (2015).
[8] A. Rzhetsky, J.G. Foster, I.T. Foster and J.A. Evans, Choosing experiments to accelerate collective discovery. Proceedings of the National Academy of Sciences 112, 14569-14574 (2015).
[9] I. Iacopini, S. Milojevic and V. Latora, Network dynamics of innovation processes. Physical review letters 120, 048301 (2018).
[10] S. Milojević, Quantifying the cognitive extent of science. Journal of Informetrics 9, 962-973 (2015).
[11] E. Beam, L.G. Appelbaum, J. Jack, J. Moody and S.A. Huettel, Mapping the semantic structure of cognitive neuroscience. Journal of cognitive neuroscience 26, 1949-1965 (2014).
[12] J.D. Dworkin, R.T. Shinohara and D.S. Bassett, The landscape of NeuroImage-ing research. NeuroImage 183, 872-883 (2018).
[13] J.D. Dworkin, R.T. Shinohara and D.S. Bassett, The emergent integrated network structure of scientific research. PloS one 14, e0216146 (2019).
[14] S. Rose, D. Engel, N. Cramer and W. Cowley, Automatic keyword extraction from individual documents. Text Mining: Applications and Theory 1-20 (2010).
[15] B. Schumacher, Quantum coding. Physical Review A 51, 2738 (1995).
[16] I.L. Chuang and Y. Yamamoto, Simple quantum computer. Physical Review A 52, 3489 (1995).
[17] E. Knill, Approximation by quantum circuits. arXiv preprint quant-ph/9508006 (1995).
[18] E. Knill, Bounds for approximation in total variation distance by quantum circuits. arXiv preprint quantph/9508007 (1995).
[19] V. Bayot, L. Piraux, J.P. Michenaud, J.P. Issi, M. Lelaurain and A. Moore, Two-dimensional weak localization in partially graphitic carbons. Physical Review B 41, 11770 (1990).
[20] S. Di Vittorio, M. Dresselhaus, M. Endo and T. Nakajima, Magnetic-field dependence of the hole-hole interaction in fluorine-intercalated graphite fibers. Physical Review B 43, 1313 (1991).
[21] R. Moreh, N. Shnieg and H. Zabel, Effective and Debye temperatures of alkali-metal atoms in graphite intercalation compounds. Physical Review B 44, 1311 (1991).
[22] A.W. Harrow and A. Montanaro, Quantum computational supremacy. Nature 549, 203 (2017).
[23] S. Aaronson and A. Arkhipov, The computational complexity of linear optics. 333-342 (2011).
[24] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe and S. Lloyd, Quantum machine learning. Nature 549, 195 (2017).
[25] V. Dunjko and H.J. Briegel, Machine learning \&amp; artificial intelligence in the quantum domain: a review of recent progress. Reports on Progress in Physics 81, 074001 (2018).
[26] D. Liben-Nowell and J. Kleinberg, The link-prediction problem for social networks. Journal of the American society for information science and technology 58, 1019-1031 (2007).
[27] Y. LeCun, Y. Bengio and G. Hinton, Deep learning. nature 521, 436 (2015).
[28] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang and P.S. Yu, A comprehensive survey on graph neural networks. arXiv:1901.00596 (2019).
[29] I.A. Kovács, K. Luck, K. Spirohn, Y. Wang, C. Pollis, S. Schlabach, W. Bian, D.K. Kim, N. Kishore, T. Hao and others, Network-based prediction of protein interactions. Nature communications 10, 1240 (2019).
[30] T. Fawcett, ROC graphs: Notes and practical considerations for researchers. Machine learning 31, 1-38 (2004).
[31] C. Robens, W. Alt, D. Meschede, C. Emary and A. Alberti, Ideal negative measurements in quantum walks disprove theories based on classical trajectories. Physical Review X 5, 011003 (2015).
[32] G.C. Knee, Do Quantum Superpositions Have a Size Limit?. Physics 8, 6 (2015).
[33] Y. Li, D. Tarlow, M. Brockschmidt and R. Zemel, Gated graph sequence neural networks. arXiv:1511.05493 (2015).
[34] M. Niepert, M. Ahmed and K. Kutzkov, Learning convolutional neural networks for graphs. International conference on machine learning 2014-2023 (2016).
[35] S. Hochreiter and J. Schmidhuber, Long short-term memory. Neural computation 9, 1735-1780 (1997).
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, Ł. Kaiser and I. Polosukhin, Attention is all you need. Advances in neural informa-</p>
<p>tion processing systems 5998-6008 (2017).
[37] V. Tshitoyan, J. Dagdelen, L. Weston, A. Dunn, Z. Rong, O. Kononova, K.A. Persson, G. Ceder and A. Jain, Unsupervised word embeddings capture latent knowledge from materials science literature. Nature 571, 95-98 (2019).
[38] H. Helbig, Knowledge representation and the semantics of natural language. (Springer, 2006).
[39] F. Shi, J.G. Foster and J.A. Evans, Weaving the fabric of science: Dynamic network models of science's unfolding structure. Social Networks 43, 73-85 (2015).
[40] S. Sreenivasan, Quantitative analysis of the evolution of novelty in cinema through crowdsourced keywords. Scientific reports 3, 2758 (2013).
[41] B. Uzzi, S. Mukherjee, M. Stringer and B. Jones, Atypical combinations and scientific impact. Science 342, 468-472 (2013).
[42] T. Martin, B. Ball, B. Karrer and M. Newman, Coauthorship and citation patterns in the Physical Review. Physical Review E 88, 012814 (2013).
[43] T. Kuhn, M. Perc and D. Helbing, Inheritance patterns in citation networks reveal scientific memes. Physical Review X 4, 041036 (2014).
[44] R. Sinatra, D. Wang, P. Deville, C. Song and A.L. Barabási, Quantifying the evolution of individual scientific impact. Science 354, aaf5239 (2016).
[45] A.L. Barabási, The Formula: The Universal Laws of Success. (Hachette UK, 2018).
[46] L. Itti and P. Baldi, Bayesian surprise attracts human attention. Advances in neural information processing systems 18, 547 (2006).
[47] L.R. Varshney, F. Pinel, K.R. Varshney, D. Bhattacharjya, A. Schoergendorfer and Y.M. Chee, A big data approach to computational creativity. arXiv:1311.1213 (2013).
[48] F. Pinel, L.R. Varshney and D. Bhattacharjya, A culinary computational creativity system. Computational creativity research: towards creative machines (Springer, 2015) p. 327-346.</p>
<h2>Supplementary Information</h2>
<h2>NETWORK THEORETICAL PROPERTIES USED FOR PREDICTIONS</h2>
<p>The neural network receives 17 network theoretical properties from SemNet, which we detail here. For a concept $c_{i}$ and $c_{j}$, the vector $p_{i, j}=\left(p_{i, j}^{1}, p_{i, j}^{2}, \ldots, p_{i, j}^{17}\right)$ corresponds to 17 real valued numbers. SEMNET of a specific year $Y$ corresponds to an adjacency matrix, which we denote as $A d j M_{Y}$.</p>
<ul>
<li>$p_{i, j}^{1}=\frac{\operatorname{deg}\left(c_{i}\right)}{\max <em k="k">{k}\left(\operatorname{deg}\left(c</em>$ connected divided by the connection numbers of the concept with most neighboring concepts.}\right)\right)} \in[0,1]$ : normalized degree centrality of first concept $c_{i}$ (normalized by largest degree centrality in the concept list), i.e. with how many other concept is $c_{i</li>
<li>$p_{i, j}^{2}=\frac{\operatorname{deg}\left(c_{j}\right)}{\max <em k="k">{k}\left(\operatorname{deg}\left(c</em>$.}\right)\right)} \in[0,1]$, normalized degree centrality of second concept $c_{j</li>
<li>$p_{i, j}^{3}=\frac{#\left(c_{i}\right)}{\max <em k="k">{k}\left(#\left(c</em>$ occures (normalized by number of concept that occures in most articles.}\right)\right)} \in[0,1]$, number of titles and abstract that concept $c_{i</li>
<li>$p_{i, j}^{4}=\frac{#\left(c_{j}\right)}{\max <em k="k">{k}\left(#\left(c</em>$ occures (normalized by number of concept that occures in most articles.}\right)\right)} \in[0,1]$, number of titles and abstract that concept $c_{j</li>
<li>$p_{i, j}^{5}=\frac{A d j M_{Y}^{5}}{\sqrt{\operatorname{deg}\left(c_{i}\right) \cdot \operatorname{deg}\left(c_{j}\right)}} \in[0,1]$, ratio of common neighbors, also known as cosine similarity.</li>
<li>$p_{i, j}^{6}=\frac{A d j M_{Y}^{5}\left(c_{i}, c_{j}\right)}{\max <em Y="Y">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y$.}^{5}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=2$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{7}=\frac{A d j M_{Y-1}^{5}\left(c_{i}, c_{j}\right)}{\max <em Y-1="Y-1">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y-1$.}^{5}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=2$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{8}=\frac{A d j M_{Y-2}^{2}\left(c_{i}, c_{j}\right)}{\max <em Y-2="Y-2">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y-2$.}^{2}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=2$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{9}=\frac{A d j M_{Y-1}^{5}\left(c_{i}, c_{j}\right)}{\max <em Y="Y">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y$.}^{5}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=3$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{10}=\frac{A d j M_{Y-1}^{5}\left(c_{i}, c_{j}\right)}{\max <em Y-1="Y-1">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y-1$.}^{5}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=3$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{11}=\frac{A d j M_{Y-2}^{3}\left(c_{i}, c_{j}\right)}{\max <em Y-2="Y-2">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y-2$.}^{3}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=3$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{12}=\frac{A d j M_{Y}^{3}\left(c_{i}, c_{j}\right)}{\max <em Y="Y">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y$.}^{3}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=4$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{13}=\frac{A d j M_{Y-1}^{4}\left(c_{i}, c_{j}\right)}{\max <em Y-1="Y-1">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y-1$.}^{4}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=4$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{14}=\frac{A d j M_{Y-2}^{4}\left(c_{i}, c_{j}\right)}{\max <em Y-2="Y-2">{k, i} A d j M</em>$ normalized by pair with largest number of paths, at year $Y-2$.}^{4}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=4$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{15}=\operatorname{distance}\left(c_{i}, c_{j}\right) \in \mathbb{N}$, network distance between $c_{i}$ and $c_{j}$.</li>
<li>$p_{i, j}^{16}=$ WeightedDistance $\left(\frac{\sqrt{\operatorname{deg}\left(c_{k}\right) \cdot \operatorname{deg}\left(c_{i}\right)}}{\operatorname{Adj}<em k="k">{Y}\left(c</em>$ (normalized by largest value of all pairs). Intuition: The more connections between certain edges, the easier it to transition from the one to the other.}, c_{i}\right)}\right) \in[0,1]$, weighted network distance between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{17}=$ WeightedDistance $\left(\frac{\operatorname{deg}\left(c_{k}\right) \cdot \operatorname{deg}\left(c_{i}\right)}{\operatorname{Adj}<em k="k">{Y}\left(c</em>$. Intuition: The more connections between certain edges, the easier it to transition from the one to the other.}, c_{i}\right)}\right) \in[0,1]$, different normalized weighted network distance between $c_{i}$ and $c_{j</li>
</ul>
<h2>FUTURE SUGGESTIONS FROM SEMNET</h2>
<p>Here we show a number of future suggestions with different parameter settings. These pairs of concepts are network-theoretically distinguished, and they couldd be inspirations for the creative, human scientist. The concept list used here is unrestricted, meaning not tailored for a specific scientist's research interest.</p>
<h2>General Concepts</h2>
<h2>Unrestricted; Highest predicted values:</h2>
<ol>
<li>hybrid system, classical communication $\cos S: 0.30407$, deg: 0.22924 , pred: 1</li>
<li>back action, classical communication $\cos S: 0.34642$, deg: 0.23012 , pred: 0.98235</li>
<li>spin orbit interaction, quantum sensing $\cos S: 0.31003$, deg: 0.23375 , pred: 0.95525</li>
<li>conformal field theory, classical communication $\cos S: 0.28176$, deg: 0.23493 , pred: 0.94893</li>
<li>spin orbit coupling, quantum sensing $\cos S: 0.33201$, deg: 0.25839 , pred: 0.94077</li>
<li>
<p>light matter interaction, classical communication
$\cos S: 0.28623$, deg: 0.24769 , pred: 0.93416</p>
</li>
<li>
<p>classical mechanic, classical communication cosS: 0.3182 , deg: 0.24956 , pred: 0.92603</p>
</li>
<li>universality, weyl semimetal cosS: 0.44731 , deg: 0.30365 , pred: 0.90986</li>
<li>many body physic, classical communication cosS: 0.29946 , deg: 0.23414 , pred: 0.9079</li>
<li>propagator, weyl semimetal cosS: 0.44141 , deg: 0.30493 , pred: 0.88731
$\cos S&lt;0.15$; Highest predicted values:</li>
<li>molecule, stanene cosS: 0.14975 , deg: 0.38553 , pred: 0.87155</li>
<li>wave function, stanene cosS: 0.14554 , deg: 0.41675 , pred: 0.85192</li>
<li>ground state, laser printing cosS: 0.080176 , deg: 0.43108 , pred: 0.79129</li>
<li>laser, stanene cosS: 0.14711 , deg: 0.39918 , pred: 0.73576</li>
<li>spin state, rarita schwinger equation cosS: 0.10752 , deg: 0.25182 , pred: 0.73427</li>
<li>two level atom, ultracold atom gas cosS: 0.14962 , deg: 0.20833 , pred: 0.71826</li>
<li>correlation, laser printing cosS: 0.076358 , deg: 0.47497 , pred: 0.71787</li>
<li>optical lattice, electromagnetically induced grating
cosS: 0.12275 , deg: 0.24917 , pred: 0.71311</li>
<li>polarization, laser printing cosS: 0.083372 , deg: 0.42666 , pred: 0.71008</li>
<li>wave function, laser printing cosS: 0.082139 , deg: 0.41086 , pred: 0.70284</li>
</ol>
<h2>$\operatorname{deg}&lt;0.05$; Highest predicted values:</h2>
<ol>
<li>seesaw mechanism, dark photon cosS: 0.42051 , deg: 0.046927 , pred: 0.52255</li>
<li>majoron, tribimaximal mixing cosS: 0.43699 , deg: 0.026998 , pred: 0.4697</li>
<li>matrix product operator, multi scale entanglement renormalization ansatz cosS: 0.367 , deg: 0.044375 , pred: 0.45618</li>
<li>electron neutrino, tribimaximal mixing cosS: 0.32507 , deg: 0.047222 , pred: 0.45098</li>
<li>valleytronic, spin transistor cosS: 0.39342 , deg: 0.043687 , pred: 0.43787</li>
<li>fair sampling, bell test experiment cosS: 0.38788 , deg: 0.018751 , pred: 0.4309</li>
<li>dark photon, little hierarchy problem cosS: 0.4419 , deg: 0.026311 , pred: 0.4296</li>
<li>wiggler, smith purcell effect cosS: 0.26696 , deg: 0.042411 , pred: 0.42564</li>
<li>valleytronic, spatial inversion cosS: 0.34483 , deg: 0.043982 , pred: 0.41915</li>
<li>quantum key, continuous variable quantum cryptography
cosS: 0.28986 , deg: 0.044375 , pred: 0.41585
$\cos S&lt;0.15, \operatorname{deg}&lt;0.05$; Highest predicted values:</li>
<li>self pulsing, laser printing cosS: 0.13666 , deg: 0.028176 , pred: 0.22185</li>
<li>photosynthesis, laser printing cosS: 0.14425 , deg: 0.033772 , pred: 0.21813</li>
<li>neutron capture nucleosynthesis, european spallation source
cosS: 0.14137 , deg: 0.044866 , pred: 0.21189</li>
<li>apparent violation, eberhard inequality cosS: 0.13047 , deg: 0.043491 , pred: 0.2102</li>
<li>copenhagen interpretation, spekkens toy model cosS: 0.14746 , deg: 0.043393 , pred: 0.20579</li>
<li>shared entanglement, generalized coherence cosS: 0.1419 , deg: 0.035833 , pred: 0.20522</li>
<li>quantum search algorithm, oracle query cosS: 0.14003 , deg: 0.043197 , pred: 0.20485</li>
<li>photon counter, photonic orbital angular momentum
cosS: 0.14217 , deg: 0.04192 , pred: 0.20478</li>
<li>copenhagen interpretation, quasi set theory cosS: 0.1326 , deg: 0.040349 , pred: 0.20417</li>
<li>optical amplifier, laser printing cosS: 0.14551 , deg: 0.042509 , pred: 0.20308</li>
</ol>
<h2>Unrestricted; Highest predicted values:</h2>
<ol>
<li>hybrid system, classical communication cosS: 0.30407 , deg: 0.22924 , pred: 1</li>
<li>back action, classical communication cosS: 0.34642 , deg: 0.23012 , pred: 0.98235</li>
<li>spin orbit interaction, quantum sensing cosS: 0.31003 , deg: 0.23375 , pred: 0.95525</li>
<li>conformal field theory, classical communication cosS: 0.28176 , deg: 0.23493 , pred: 0.94893</li>
<li>spin orbit coupling, quantum sensing cosS: 0.33201 , deg: 0.25839 , pred: 0.94077</li>
<li>light matter interaction, classical communication cosS: 0.28623 , deg: 0.24769 , pred: 0.93416</li>
<li>classical mechanic, classical communication cosS: 0.3182 , deg: 0.24956 , pred: 0.92603</li>
<li>universality, weyl semimetal cosS: 0.44731 , deg: 0.30365 , pred: 0.90986</li>
<li>many body physic, classical communication cosS: 0.29946 , deg: 0.23414 , pred: 0.9079</li>
<li>propagator, weyl semimetal cosS: 0.44141 , deg: 0.30493 , pred: 0.88731</li>
</ol>
<h2>Unrestricted; Lowest predicted values:</h2>
<ol>
<li>transverse mode, pseudogap cosS: 0.47207 , deg: 0.22227 , pred: -1</li>
<li>nonlinear regime, pseudogap cosS: 0.48811 , deg: 0.21971 , pred: -0.99384</li>
<li>
<p>langevin equation, pseudogap cosS: 0.48992 , deg: 0.24897 , pred: -0.99167</p>
</li>
<li>
<p>numerical computation, pseudogap cosS: 0.51088 , deg: 0.24357 , pred: -0.98443</p>
</li>
<li>diffusion process, pseudogap cosS: 0.51135 , deg: 0.21971 , pred: -0.98135</li>
<li>interaction hamiltonian, pseudogap cosS: 0.483 , deg: 0.24789 , pred: -0.98065</li>
<li>holography, pseudogap cosS: 0.4797 , deg: 0.22413 , pred: -0.97841</li>
<li>many particle system, inelastic neutron scattering
cosS: 0.46252 , deg: 0.20253 , pred: -0.97628</li>
<li>damping rate, pseudogap cosS: 0.49515 , deg: 0.21814 , pred: -0.97625</li>
<li>early universe, pseudogap cosS: 0.42681 , deg: 0.21716 , pred: -0.9754</li>
</ol>
<h2>$\cos S&lt;0.15$; Lowest predicted values:</h2>
<ol>
<li>laser, large helical device cosS: 0.093823 , deg: 0.39378 , pred: -0.72391</li>
<li>distribution, pionium cosS: 0.11835 , deg: 0.50461 , pred: -0.62882</li>
<li>laser, diffuse serie cosS: 0.10166 , deg: 0.39476 , pred: -0.61814</li>
<li>resolution, moseleys law cosS: 0.075495 , deg: 0.38111 , pred: -0.60875</li>
<li>charge, franck hertz experiment cosS: 0.085768 , deg: 0.44365 , pred: -0.55765</li>
<li>charge, selected area diffraction cosS: 0.10018 , deg: 0.44502 , pred: -0.55725</li>
<li>hamiltonian, zero field nmr cosS: 0.14845 , deg: 0.4462 , pred: -0.55318</li>
<li>molecule, atom transition cosS: 0.1266 , deg: 0.38386 , pred: -0.55074</li>
<li>electron, atom bose einstein condensate cosS: 0.1139 , deg: 0.49146 , pred: -0.54915</li>
<li>electron, ultracold atom gas cosS: 0.12406 , deg: 0.49224 , pred: -0.54876</li>
</ol>
<h2>Unrestricted; maximal outlier (cosS, deg, pred):</h2>
<ol>
<li>quantum information, scattering amplitude cosS: 0.49361 , deg: 0.5376 , pred: -0.95502</li>
<li>s process, quantum spin cosS: 0.59655 , deg: 0.48164 , pred: -0.95498</li>
<li>electrostatic, spin system cosS: 0.58982 , deg: 0.45376 , pred: -0.95086</li>
<li>hilbert space, raman scattering cosS: 0.48201 , deg: 0.47477 , pred: -0.95554</li>
<li>interference effect, mean field theory cosS: 0.58245 , deg: 0.38131 , pred: -0.95981</li>
<li>space time, carbon nanotube cosS: 0.51336 , deg: 0.42284 , pred: -0.95861</li>
<li>quantum optic, random phase approximation cosS: 0.48734 , deg: 0.43 , pred: -0.95878</li>
<li>quantum information, brillouin zone cosS: 0.50927 , deg: 0.52562 , pred: -0.86694</li>
<li>two level system, charge density cosS: 0.51577 , deg: 0.41223 , pred: -0.95105</li>
<li>path integral, raman scattering cosS: 0.53407 , deg: 0.41331 , pred: -0.93953</li>
</ol>
<h2>Unrestricted; maximal outlier (cosS, deg):</h2>
<ol>
<li>hilbert space, plasma cosS: 0.5505 , deg: 0.57157 , pred: -0.458</li>
<li>divergence, quantum computation cosS: 0.56671 , deg: 0.53466 , pred: 0.063652</li>
<li>wave packet, free energy cosS: 0.60923 , deg: 0.50884 , pred: -0.55609</li>
<li>quantum information, wave number cosS: 0.52858 , deg: 0.54683 , pred: 0.087118</li>
<li>atom, yang mills theory cosS: 0.39169 , deg: 0.58777 , pred: 0.019855</li>
<li>entangled state, conductivity cosS: 0.50832 , deg: 0.54752 , pred: -0.45379</li>
<li>density matrix, domain wall cosS: 0.58721 , deg: 0.5105 , pred: 0.10296</li>
<li>qubit, diffusion coefficient cosS: 0.52962 , deg: 0.53642 , pred: 0.11948</li>
<li>entanglement, vector potential cosS: 0.50925 , deg: 0.54271 , pred: 0.11929</li>
<li>decoherence, electromagnetic wave cosS: 0.5603 , deg: 0.51885 , pred: 0.095746</li>
</ol>
<h2>NETWORK THEORETICAL PROPERTIES USED FOR PREDICTIONS</h2>
<p>The neural network receives 17 network theoretical properties from SemNet, which we detail here. For a concept $c_{i}$ and $c_{j}$, the vector $p_{i, j}=\left(p_{i, j}^{1}, p_{i, j}^{2}, \ldots, p_{i, j}^{17}\right)$ corresponds to 17 real valued numbers. SEMNET of a specific year $Y$ corresponds to an adjacency matrix, which we denote as $A d j M_{Y}$.</p>
<ul>
<li>$p_{i, j}^{1}=\frac{\operatorname{deg}\left(c_{i}\right)}{\max <em k="k">{k}\left(\operatorname{deg}\left(c</em>$ connected divided by the connection numbers of the concept with most neighboring concepts.}\right)\right)} \in[0,1]$ : normalized degree centrality of first concept $c_{i}$ (normalized by largest degree centrality in the concept list), i.e. with how many other concept is $c_{i</li>
<li>$p_{i, j}^{2}=\frac{\operatorname{deg}\left(c_{i}\right)}{\max <em k="k">{k}\left(\operatorname{deg}\left(c</em>$.}\right)\right)} \in[0,1]$, normalized degree centrality of second concept $c_{j</li>
<li>$p_{i, j}^{3}=\frac{#\left(c_{i}\right)}{\max <em k="k">{k}\left(#\left(c</em>$ occures (normalized by number of concept that occures in most articles.}\right)\right)} \in[0,1]$, number of titles and abstract that concept $c_{i</li>
<li>
<p>$p_{i, j}^{4}=\frac{#\left(c_{j}\right)}{\max <em k="k">{k}\left(#\left(c</em>$ occures (normalized by number of concept that occures in most articles.}\right)\right)} \in[0,1]$, number of titles and abstract that concept $c_{j</p>
</li>
<li>
<p>$p_{i, j}^{5}=\frac{\operatorname{Adj} M_{Y}^{5}}{\sqrt{\operatorname{deg}\left(c_{i}\right) \cdot \operatorname{deg}\left(c_{j}\right)}} \in[0,1]$, ratio of common neighbors, also known as cosine similarity.</p>
</li>
<li>$p_{i, j}^{6}=\frac{\operatorname{Adj} M_{Y}^{6}\left(c_{i}, c_{j}\right)}{\max <em Y="Y">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y$.}^{5}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=2$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{7}=\frac{\operatorname{Adj} M_{Y-1}^{5}\left(c_{i}, c_{j}\right)}{\max <em Y-1="Y-1">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y-1$.}^{5}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=2$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{8}=\frac{\operatorname{Adj} M_{Y-2}^{5}\left(c_{i}, c_{j}\right)}{\max <em Y-2="Y-2">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y-2$.}^{5}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=2$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{9}=\frac{\operatorname{Adj} M_{Y}^{9}\left(c_{i}, c_{j}\right)}{\max <em Y="Y">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y$.}^{9}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=3$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{10}=\frac{\operatorname{Adj} M_{Y-1}^{3}\left(c_{i}, c_{j}\right)}{\max <em Y-1="Y-1">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y-1$.}^{3}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=3$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{11}=\frac{\operatorname{Adj} M_{Y-2}^{3}\left(c_{i}, c_{j}\right)}{\max <em Y-2="Y-2">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y-2$.}^{3}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=3$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{12}=\frac{\operatorname{Adj} M_{Y}^{4}\left(c_{i}, c_{j}\right)}{\max <em Y="Y">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y$.}^{4}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=4$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{13}=\frac{\operatorname{Adj} M_{Y-1}^{4}\left(c_{i}, c_{j}\right)}{\max <em Y-1="Y-1">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y-1$.}^{4}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=4$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{14}=\frac{\operatorname{Adj} M_{Y-2}^{4}\left(c_{i}, c_{j}\right)}{\max <em Y-2="Y-2">{k, i} \operatorname{Adj} M</em>$ normalized by pair with largest number of paths, at year $Y-2$.}^{4}\left(c_{k}, c_{i}\right)} \in[0,1]$, paths of length $=4$ between $c_{i}$ and $c_{j</li>
<li>$p_{i, j}^{15}=\operatorname{distance}\left(c_{i}, c_{j}\right) \in \mathbb{N}$, network distance between $c_{i}$ and $c_{j}$.</li>
<li>$p_{i, j}^{16}=$ WeightedDistance $\left(\frac{\sqrt{\operatorname{deg}\left(c_{k}\right) \cdot \operatorname{deg}\left(c_{i}\right)}}{\operatorname{Adj} M_{Y}\left(c_{k}, c_{i}\right)}\right) \in[0,1]$, weighted network distance between $c_{i}$ and $c_{j}$ (normalized by largest value of all pairs). Intuition: The more connections between certain edges, the easier it to transition from the one to the other.</li>
<li>$p_{i, j}^{17}=$ WeightedDistance $\left(\frac{\operatorname{deg}\left(c_{k}\right) \cdot \operatorname{deg}\left(c_{i}\right)}{\operatorname{Adj} M_{Y}\left(c_{k}, c_{i}\right)}\right) \in[0,1]$, different normalized weighted network distance between $c_{i}$ and $c_{j}$. Intuition: The more connections between certain edges, the easier it to transition from the one to the other.</li>
</ul>
<h2>FUTURE SUGGESTIONS FROM SEMNET</h2>
<p>Here we show a number of future suggestions with different parameter settings. These pairs of concepts are network-theoretically distinguished, and they couldd be inspirations for the creative, human scientist. The concept list used here is unrestricted, meaning not tailored for a specific scientist's research interest.</p>
<h2>General Concepts</h2>
<h2>Unrestricted; Highest predicted values:</h2>
<ol>
<li>hybrid system, classical communication $\cos \mathrm{S}: 0.30407$, deg: 0.22924 , pred: 1</li>
<li>back action, classical communication $\cos \mathrm{S}: 0.34642$, deg: 0.23012 , pred: 0.98235</li>
<li>spin orbit interaction, quantum sensing $\cos \mathrm{S}: 0.31003$, deg: 0.23375 , pred: 0.95525</li>
<li>conformal field theory, classical communication $\cos \mathrm{S}: 0.28176$, deg: 0.23493 , pred: 0.94893</li>
<li>spin orbit coupling, quantum sensing $\cos \mathrm{S}: 0.33201$, deg: 0.25839 , pred: 0.94077</li>
<li>light matter interaction, classical communication $\cos \mathrm{S}: 0.28623$, deg: 0.24769 , pred: 0.93416</li>
<li>classical mechanic, classical communication $\cos \mathrm{S}: 0.3182$, deg: 0.24956 , pred: 0.92603</li>
<li>universality, weyl semimetal $\cos \mathrm{S}: 0.44731$, deg: 0.30365 , pred: 0.90986</li>
<li>many body physic, classical communication $\cos \mathrm{S}: 0.29946$, deg: 0.23414 , pred: 0.9079</li>
<li>propagator, weyl semimetal $\cos \mathrm{S}: 0.44141$, deg: 0.30493 , pred: 0.88731
$\cos \mathbf{S}&lt;0.15$; Highest predicted values:</li>
<li>molecule, stanene
$\cos \mathrm{S}: 0.14975$, deg: 0.38553 , pred: 0.87155</li>
<li>wave function, stanene
$\cos \mathrm{S}: 0.14554$, deg: 0.41675 , pred: 0.85192</li>
<li>ground state, laser printing
$\cos \mathrm{S}: 0.080176$, deg: 0.43108 , pred: 0.79129</li>
<li>laser, stanene
$\cos \mathrm{S}: 0.14711$, deg: 0.39918 , pred: 0.73576</li>
<li>spin state, rarita schwinger equation
$\cos \mathrm{S}: 0.10752$, deg: 0.25182 , pred: 0.73427</li>
<li>two level atom, ultracold atom gas
$\cos \mathrm{S}: 0.14962$, deg: 0.20833 , pred: 0.71826</li>
<li>correlation, laser printing
$\cos \mathrm{S}: 0.076358$, deg: 0.47497 , pred: 0.71787</li>
<li>optical lattice, electromagnetically induced grating
$\cos \mathrm{S}: 0.12275$, deg: 0.24917 , pred: 0.71311</li>
<li>
<p>polarization, laser printing
$\cos \mathrm{S}: 0.083372$, deg: 0.42666 , pred: 0.71008</p>
</li>
<li>
<p>wave function, laser printing cosS: 0.082139 , deg: 0.41086 , pred: 0.70284</p>
</li>
</ol>
<h2>deg $&lt;0.05$; Highest predicted values:</h2>
<ol>
<li>seesaw mechanism, dark photon cosS: 0.42051 , deg: 0.046927 , pred: 0.52255</li>
<li>majoron, tribimaximal mixing cosS: 0.43699 , deg: 0.026998 , pred: 0.4697</li>
<li>matrix product operator, multi scale entanglement renormalization ansatz cosS: 0.367 , deg: 0.044375 , pred: 0.45618</li>
<li>electron neutrino, tribimaximal mixing cosS: 0.32507 , deg: 0.047222 , pred: 0.45098</li>
<li>valleytronic, spin transistor cosS: 0.39342 , deg: 0.043687 , pred: 0.43787</li>
<li>fair sampling, bell test experiment cosS: 0.38788 , deg: 0.018751 , pred: 0.4309</li>
<li>dark photon, little hierarchy problem cosS: 0.4419 , deg: 0.026311 , pred: 0.4296</li>
<li>wiggler, smith purcell effect cosS: 0.26696 , deg: 0.042411 , pred: 0.42564</li>
<li>valleytronic, spatial inversion cosS: 0.34483 , deg: 0.043982 , pred: 0.41915</li>
<li>quantum key, continuous variable quantum cryptography cosS: 0.28986 , deg: 0.044375 , pred: 0.41585
$\cos S&lt;0.15, \operatorname{deg}&lt;0.05$; Highest predicted values:</li>
<li>self pulsing, laser printing cosS: 0.13666 , deg: 0.028176 , pred: 0.22185</li>
<li>photosynthesis, laser printing cosS: 0.14425 , deg: 0.033772 , pred: 0.21813</li>
<li>neutron capture nucleosynthesis, european spallation source cosS: 0.14137 , deg: 0.044866 , pred: 0.21189</li>
<li>apparent violation, eberhard inequality cosS: 0.13047 , deg: 0.043491 , pred: 0.2102</li>
<li>copenhagen interpretation, spekkens toy model cosS: 0.14746 , deg: 0.043393 , pred: 0.20579</li>
<li>shared entanglement, generalized coherence cosS: 0.1419 , deg: 0.035833 , pred: 0.20522</li>
<li>quantum search algorithm, oracle query cosS: 0.14003 , deg: 0.043197 , pred: 0.20485</li>
<li>photon counter, photonic orbital angular momentum
cosS: 0.14217 , deg: 0.04192 , pred: 0.20478</li>
<li>copenhagen interpretation, quasi set theory cosS: 0.1326 , deg: 0.040349 , pred: 0.20417</li>
<li>optical amplifier, laser printing cosS: 0.14551 , deg: 0.042509 , pred: 0.20308</li>
</ol>
<h2>Unrestricted; Highest predicted values:</h2>
<ol>
<li>hybrid system, classical communication cosS: 0.30407 , deg: 0.22924 , pred: 1</li>
<li>back action, classical communication cosS: 0.34642 , deg: 0.23012 , pred: 0.98235</li>
<li>spin orbit interaction, quantum sensing cosS: 0.31003 , deg: 0.23375 , pred: 0.95525</li>
<li>conformal field theory, classical communication cosS: 0.28176 , deg: 0.23493 , pred: 0.94893</li>
<li>spin orbit coupling, quantum sensing cosS: 0.33201 , deg: 0.25839 , pred: 0.94077</li>
<li>light matter interaction, classical communication cosS: 0.28623 , deg: 0.24769 , pred: 0.93416</li>
<li>classical mechanic, classical communication cosS: 0.3182 , deg: 0.24956 , pred: 0.92603</li>
<li>universality, weyl semimetal cosS: 0.44731 , deg: 0.30365 , pred: 0.90986</li>
<li>many body physic, classical communication cosS: 0.29946 , deg: 0.23414 , pred: 0.9079</li>
<li>propagator, weyl semimetal cosS: 0.44141 , deg: 0.30493 , pred: 0.88731</li>
</ol>
<h2>Unrestricted; Lowest predicted values:</h2>
<ol>
<li>transverse mode, pseudogap cosS: 0.47207 , deg: 0.22227 , pred: -1</li>
<li>nonlinear regime, pseudogap cosS: 0.48811 , deg: 0.21971 , pred: -0.99384</li>
<li>langevin equation, pseudogap cosS: 0.48992 , deg: 0.24897 , pred: -0.99167</li>
<li>numerical computation, pseudogap cosS: 0.51088 , deg: 0.24357 , pred: -0.98443</li>
<li>diffusion process, pseudogap cosS: 0.51135 , deg: 0.21971 , pred: -0.98135</li>
<li>interaction hamiltonian, pseudogap cosS: 0.483 , deg: 0.24789 , pred: -0.98065</li>
<li>holography, pseudogap cosS: 0.4797 , deg: 0.22413 , pred: -0.97841</li>
<li>many particle system, inelastic neutron scattering
cosS: 0.46252 , deg: 0.20253 , pred: -0.97628</li>
<li>damping rate, pseudogap cosS: 0.49515 , deg: 0.21814 , pred: -0.97625</li>
<li>early universe, pseudogap cosS: 0.42681 , deg: 0.21716 , pred: -0.9754
$\cos S&lt;0.15$; Lowest predicted values:</li>
<li>laser, large helical device cosS: 0.093823 , deg: 0.39378 , pred: -0.72391</li>
<li>distribution, pionium cosS: 0.11835 , deg: 0.50461 , pred: -0.62882</li>
<li>laser, diffuse serie cosS: 0.10166 , deg: 0.39476 , pred: -0.61814</li>
<li>resolution, moseleys law cosS: 0.075495 , deg: 0.38111 , pred: -0.60875</li>
<li>charge, franck hertz experiment cosS: 0.085768 , deg: 0.44365 , pred: -0.55765</li>
<li>
<p>charge, selected area diffraction cosS: 0.10018 , deg: 0.44502 , pred: -0.55725</p>
</li>
<li>
<p>hamiltonian, zero field nmr cosS: 0.14845 , deg: 0.4462 , pred: -0.55318</p>
</li>
<li>molecule, atom transition cosS: 0.1266 , deg: 0.38386 , pred: -0.55074</li>
<li>electron, atom bose einstein condensate cosS: 0.1139 , deg: 0.49146 , pred: -0.54915</li>
<li>electron, ultracold atom gas cosS: 0.12406 , deg: 0.49224 , pred: -0.54876</li>
</ol>
<p>Unrestricted; maximal outlier (cosS, deg, pred):</p>
<ol>
<li>quantum information, scattering amplitude cosS: 0.49361 , deg: 0.5376 , pred: -0.95502</li>
<li>s process, quantum spin cosS: 0.59655 , deg: 0.48164 , pred: -0.95498</li>
<li>electrostatic, spin system cosS: 0.58982 , deg: 0.45376 , pred: -0.95086</li>
<li>hilbert space, raman scattering cosS: 0.48201 , deg: 0.47477 , pred: -0.95554</li>
<li>interference effect, mean field theory cosS: 0.58245 , deg: 0.38131 , pred: -0.95981</li>
<li>space time, carbon nanotube cosS: 0.51336 , deg: 0.42284 , pred: -0.95861</li>
<li>quantum optic, random phase approximation cosS: 0.48734 , deg: 0.43 , pred: -0.95878</li>
<li>quantum information, brillouin zone cosS: 0.50927 , deg: 0.52562 , pred: -0.86694</li>
<li>two level system, charge density cosS: 0.51577 , deg: 0.41223 , pred: -0.95105</li>
<li>path integral, raman scattering cosS: 0.53407 , deg: 0.41331 , pred: -0.93953</li>
</ol>
<h2>Unrestricted; maximal outlier (cosS, deg):</h2>
<ol>
<li>hilbert space, plasma cosS: 0.5505 , deg: 0.57157 , pred: -0.458</li>
<li>divergence, quantum computation cosS: 0.56671 , deg: 0.53466 , pred: 0.063652</li>
<li>wave packet, free energy cosS: 0.60923 , deg: 0.50884 , pred: -0.55609</li>
<li>quantum information, wave number cosS: 0.52858 , deg: 0.54683 , pred: 0.087118</li>
<li>atom, yang mills theory cosS: 0.39169 , deg: 0.58777 , pred: 0.019855</li>
<li>entangled state, conductivity cosS: 0.50832 , deg: 0.54752 , pred: -0.45379</li>
<li>density matrix, domain wall cosS: 0.58721 , deg: 0.5105 , pred: 0.10296</li>
<li>qubit, diffusion coefficient cosS: 0.52962 , deg: 0.53642 , pred: 0.11948</li>
<li>entanglement, vector potential cosS: 0.50925 , deg: 0.54271 , pred: 0.11929</li>
<li>decoherence, electromagnetic wave cosS: 0.5603 , deg: 0.51885 , pred: 0.095746</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
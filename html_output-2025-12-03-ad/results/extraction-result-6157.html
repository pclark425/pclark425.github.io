<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6157 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6157</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6157</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-04365f0f1db4c659c3297cb8e70c39b38ed3b487</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04365f0f1db4c659c3297cb8e70c39b38ed3b487" target="_blank">Self-Evaluation Improves Selective Generation in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> ICBINB</p>
                <p><strong>Paper TL;DR:</strong> This work reformulates open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level, and demonstrates that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
                <p><strong>Paper Abstract:</strong> Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6157.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6157.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sample and Select</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample and Select (multi-choice token-level evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reduce free-form generation to a multiple-choice selection problem by sampling n candidate answers, mapping them to choice tokens (A, B, C...), and scoring choices using token-level probabilities/logits (with optional shuffle-and-average de-biasing and optional 'NONE OF THE ABOVE' candidate).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate n candidate answers from the LLM for each prompt, format candidates as labeled multiple-choice options, query the LLM to produce token-level scores (logits or softmax probabilities) for the choice tokens, and select the answer with the highest score; optional 'NONE OF THE ABOVE' (nota) is added to enable rejection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy of chosen answer within question (whether selected answer is correct), Calibration-AUC (ROC AUC for binary label correct/incorrect vs confidence score), Selective-AUC (area under selective generation curve when abstaining low-confidence outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PaLM-2 LARGE (primary), GPT-3 (text-davinci-003) (partial)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General question answering and summarization (evaluation of free-form generated content / hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Treat each sampled answer as a candidate hypothesis/claim and use multi-choice token-level scoring to decide which hypothesis to present as the model's output.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>PaLM-2: Sample and Select (vanilla) — Accuracy 58.26%, Calibration-AUC 53.17%, Selective-AUC 48.59%; with 'NONE OF THE ABOVE' (nota) — Accuracy 58.13%, Calibration-AUC 72.59%, Selective-AUC 56.61%. GPT-3: Sample and Select — Accuracy 72.24%, Calibration-AUC 47.97%, Selective-AUC 56.75% (nota variants not available for GPT-3 due to log-prob API limits).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on TruthfulQA and TL;DR benchmarks (see dataset entries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Ground-truth correctness labels come from human-aligned automated judges (GPT-judge for TruthfulQA, reward model for TL;DR) rather than fresh human annotation in this work; Sample-and-Select selection accuracy is compared against those labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Position bias (choice position affects scores) requiring shuffle-and-average de-biasing; probability dispersion when multiple sampled answers are correct (softmax spreads mass); forced choice when none of the sampled answers are correct unless 'NONE OF THE ABOVE' is added; score dependence on the candidate set (makes cross-question comparison difficult).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6157.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sample and Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample and Eval (pointwise true/false evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pointwise evaluation where the model is prompted to judge a single (question, candidate answer) pair as Yes/No (true/false) and the normalized token probability for 'Yes' is used as a confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each sampled candidate answer, prompt the LLM in a true/false (Yes/No) format to judge whether the candidate is correct; convert model outputs to a probability p(Yes|x,y) via softmax of token logits and use that as the confidence score for (x,y).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pointwise predicted probability of correctness p(Yes|x,y); used to compute Accuracy (when used to select the top-scoring candidate), Calibration-AUC (ranking performance vs correctness), and Selective-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PaLM-2 LARGE (primary), GPT-3 (text-davinci-003) (partial)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General question answering and summarization (assessing individual generated claims/answers)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Each candidate generated answer is treated as a standalone hypothesis; the LLM is asked to evaluate the truth/quality of that hypothesis directly.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>PaLM-2: Sample and Eval — Accuracy 59.12%, Calibration-AUC 73.79%, Selective-AUC 58.19%; Sample and Eval w/ other candidates included in prompt — Accuracy 59.00%, Calibration-AUC 68.78%, Selective-AUC 55.70%. GPT-3: Sample and Eval — Accuracy 67.83%, Calibration-AUC 48.47%, Selective-AUC 53.28%; Sample and Eval w/ candidates — Accuracy 68.48%, Calibration-AUC 51.36%, Selective-AUC 55.28%.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on TruthfulQA and TL;DR with automated judges (GPT-judge / reward model).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Confidence judged vs ground-truth labels supplied by automated human-aligned judges; Sample-and-Eval shows improved Calibration-AUC vs sequence-likelihood, indicating better agreement with human-judged correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pointwise scores are independent of other candidates (benefit) but do not provide a natural selection mechanism across candidates (so must be combined with Sample-and-Select for selection); dependent on LLM's token calibration for 'Yes'/'No' tokens; softmax normalization may be sensitive to prompt/tokenization in some APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6157.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid (Sample & Select then Sample & Eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid Sample-and-Select-and-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-stage approach: first use Sample-and-Select (multi-choice) to pick the best candidate per question, then discard selection score and use Sample-and-Eval to score the selected candidate; optionally penalize the pointwise score by the 'nota' uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Stage 1: select best candidate via multi-choice scoring (Sample-and-Select). Stage 2: compute pointwise p(Yes|x,ŷ) for the selected ŷ and adjust by nota uncertainty s_nota if nota is used; final confidence = p(Yes|x,ŷ) - p(c_nota|x,{c y}_+nota).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy of selected answer, Calibration-AUC, Selective-AUC computed on final confidence score; used to abstain outputs in selective generation evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PaLM-2 LARGE (primary), GPT-3 (text-davinci-003) (partial)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General QA and summarization (evaluation and selective output of candidate hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Selection produces the hypothesis to present; pointwise evaluation yields a cross-question-comparable confidence score for that hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>PaLM-2: Hybrid — Accuracy 58.26%, Calibration-AUC 73.76%, Selective-AUC 57.38%; Hybrid w/ nota — Accuracy 58.14%, Calibration-AUC 75.34%, Selective-AUC 58.10%. GPT-3: Hybrid — Accuracy 72.24%, Calibration-AUC 51.66%, Selective-AUC 58.46% (nota variants NA for GPT-3 due to API limitations). Hybrid achieves a balance of higher selection accuracy (from Sample-and-Select) and stronger calibration (from Sample-and-Eval).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>TruthfulQA and TL;DR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Final confidences compared to automated human-aligned judgments (GPT-judge, reward model); hybrid shows improved Calibration-AUC over sequence-level scores and often matches or exceeds other self-evaluation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Increased inference cost (additional LLM calls), reliance on sampling diversity for good candidates, requires handling of API/log-prob constraints for some models (e.g., GPT-3 limitations prevent some nota evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6157.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequence likelihood</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-level likelihood (log p(y|x)) and length-normalized likelihood</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Canonical approach using the model's sequence probability or length-normalized sequence probability for scoring generated sequences; historically used for ranking/generation but shown to be poorly calibrated for free-form outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute log p(y|x) = sum_t log p(y^t | y^{<t}, x) or length-normalize by sequence length; use this score to rank or decide to abstain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Used as raw confidence score for accuracy, Calibration-AUC, and Selective-AUC comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PaLM-2 LARGE, GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General free-form generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Use model-internal sequence probabilities as indicator of output quality/truthfulness for each generated hypothesis/answer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>PaLM-2: Sequence likelihood — Accuracy 48.23%, Calibration-AUC 39.80%, Selective-AUC 33.63%; Length-norm sequence likelihood — Accuracy 52.75%, Calibration-AUC 50.09%, Selective-AUC 42.15%. GPT-3: Sequence likelihood and length-normalized both show Accuracy 67.19% with Calibration-AUC around 40-42% and Selective-AUC ~50-50.2%. Sequence-level scores often perform worse than token-level self-evaluation and can be negatively correlated with correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on TruthfulQA and TL;DR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Compared to automated judge labels; sequence-likelihood shows poor alignment with human-judged correctness relative to self-evaluation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Bias toward sequence length, poor ranking/correlation with true answer quality on open-ended tasks, not reliably calibrated for selective generation; length-normalization partially helps but does not fully solve calibration issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6157.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration-AUC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calibration-AUC (ROC AUC for correctness vs confidence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AUC metric measuring ranking quality of confidence scores for binary correctness labels: area under the ROC curve where positive = correct output and negative = incorrect output; used as a model-agnostic calibration/ranking metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute ROC AUC treating each (x,ŷ) pair as a binary example (correct=1/incorrect=0) and using the confidence score s(x,ŷ) as the predictor; higher AUC indicates confidence scores rank correct answers higher than incorrect ones.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Calibration-AUC value (0-100% as reported in paper tables) used to compare different scoring methods' ability to predict output correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applies to outputs from PaLM-2 LARGE and GPT-3 as evaluated in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General (applies to any domain where correctness labels are available)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a theory but a ranking-based calibration metric for confidence scores assigned to generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across experiments, token-level self-evaluation methods (Sample-and-Eval, Hybrid w/ nota) substantially increased Calibration-AUC vs sequence-likelihood; e.g., PaLM-2 Sample-and-Eval Calibration-AUC 73.79% vs sequence-likelihood 39.80%.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Used on TruthfulQA and TL;DR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Metric compares model confidence to human-aligned labels (via GPT-judge / reward model) rather than direct human assessor calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Calibration-AUC measures ranking ability, not calibration in absolute probability terms (so it's robust to monotonic transforms but not to post-hoc probability calibration); cannot be affected by simple temperature scaling to change absolute probabilities but preserves ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6157.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selective-AUC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selective-AUC (area under the selective generation curve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Area under the selective generation curve that plots accuracy of non-abstained outputs as a function of abstention rate; measures how well confidence scores allow a model to abstain low-quality outputs to improve overall quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Sort outputs by confidence score, progressively abstain the lowest-scored α% of examples, compute remaining accuracy as a function of α, and take the area under that curve as Selective-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Selective-AUC (reported as percentage) quantifies the benefit of abstention policies driven by the confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to PaLM-2 LARGE and GPT-3 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General selective generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A higher Selective-AUC means the confidence estimator more effectively identifies and removes low-quality hypotheses, improving the quality of the outputs that remain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Sample-and-Eval and Hybrid w/ nota produced the highest Selective-AUCs in many experiments (PaLM-2 Sample-and-Eval 58.19%, Hybrid w/ nota 58.10% on TruthfulQA) compared to sequence-likelihood (33.63%).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Computed on TruthfulQA and TL;DR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Selective-AUC measures alignment of confidence rankings with human-aligned correctness labels; it is a practical measure of usefulness for abstention compared to human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Selective-AUC depends on quality of binary correctness labels and sampling strategy; computational cost of scoring many samples can be high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6157.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy (selection accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy of selected candidate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Percentage of questions for which the selected (highest-scored) candidate answer is correct according to human-aligned labels; used both within-question (choosing best among n samples) and as the starting point (α=0) of selective curves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For each question, pick the candidate with highest confidence s(x,y) and check ground-truth correctness h(x,ŷ). Aggregate over dataset to compute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy percentage reported in tables; also used as baseline point for selective generation curve.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PaLM-2 LARGE, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General QA and summarization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Measures whether the scoring method helps to pick the best candidate sample per question.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Examples: PaLM-2 Sample-and-Select accuracy 58.26%, Sample-and-Eval 59.12%, Hybrid ~58.26%; on TL;DR Sample-and-Select accuracy ~70.20-70.80 vs Sample-and-Eval 68.70-70.20 depending on variant.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>TruthfulQA, TL;DR</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Compared to automated human-aligned labels; accuracy improvements reported after applying self-critique & revise as well.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Accuracy alone does not measure calibration or whether confidence scores are suitable for abstention; methods that maximize accuracy may not produce well-calibrated confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6157.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TruthfulQA: Measuring how models mimic human falsehoods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark dataset for assessing models' tendency to generate falsehoods or mimic misconceptions; contains questions where models often produce plausible but false answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TruthfulQA: Measuring how models mimic human falsehoods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Models generate answers to 817 validation questions; generated answers are labeled for truthfulness using GPT-judge (a GPT-3 model fine-tuned on human feedback) to approximate human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness/truthfulness labels (binary) used to compute Accuracy, Calibration-AUC, and Selective-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used to evaluate outputs from PaLM-2 LARGE and GPT-3 in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Truthfulness / factuality in QA</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Dataset instances are prompts/questions that test factual accuracy and resistance to common misconceptions; in this work, generated answers are treated as candidate hypotheses evaluated for correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper reports method comparisons on TruthfulQA (see method-specific results above). GPT-judge used for labels has reported 90-95% accuracy vs human evaluations per Lin et al. [2021].</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>This entry is the dataset itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Ground-truth labels are obtained via GPT-judge which was fine-tuned on human feedback and shown to agree with humans ~90-95%; direct human labels were not used at scale in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Using an automated judge (GPT-judge) as a proxy for human labels can introduce approximation error; dataset size (817 validation examples) limits some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6157.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TL;DR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TL;DR: Mining reddit to learn automatic summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large summarization dataset mined from Reddit posts and their TL;DR comments used to evaluate summary generation and summary quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tl; dr: Mining reddit to learn automatic summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Authors sampled 1,000 examples from TL;DR test split (15,240 total) and generated summaries; summary quality was labeled using a reward model fine-tuned on human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary concise/comprehensive judgment and summary quality labels used to compute Accuracy, Calibration-AUC, and Selective-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PaLM-2 LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Abstractive summarization quality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Each generated summary is treated as a produced hypothesis; pointwise evaluation asks whether a summary is concise and comprehensive.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Sample-and-Select and Sample-and-Eval variants tested; Sample-and-Select variants often had higher accuracy while Sample-and-Eval gave better calibration in some variants (see Table 3 in paper). Reward model accuracy vs human is reported at 71.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>TL;DR dataset as above.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Quality labels are from a reward model trained on human feedback; reward model agreement with human ratings is ~71.34%, so automated labels are approximate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Using the reward model as proxy for humans imposes label noise; sampling 1,000 examples was used to limit inference cost, so results are on a subset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6157.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated judges (GPT-judge / reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated human-aligned evaluators (GPT-judge; reward model trained on human feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated systems used to label generated outputs' correctness/quality: GPT-judge (fine-tuned GPT-3) for TruthfulQA, and a reward model fine-tuned on human feedback for TL;DR summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use the automated judge to produce binary correctness/quality labels for generated outputs so that confidence metrics can be evaluated at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary correctness / concise+comprehensive judgments used to compute Accuracy, Calibration-AUC, Selective-AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-judge is GPT-3 based; reward model specifics per Zhao et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General evaluation proxy to approximate human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>These systems approximate human labels, enabling large-scale quantitative evaluation of scoring/calibration methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-judge reported agreement with human truthfulness labels of ~90-95%; reward model for TL;DR reported prediction accuracy vs human rating of 71.34%. These judges were used as ground truth for the paper's metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Used for TruthfulQA and TL;DR labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Authors rely on prior reported judge accuracies to argue these automated labels are reasonable proxies for humans; they do not perform fresh human labeling at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Automated judges introduce approximation error (especially the reward model with ~71% agreement); reliance on them can bias evaluation conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6157.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequence-level ECE (discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Calibration Error (ECE) — sequence-level distinction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard probabilistic calibration metric measuring difference between predicted probabilities and empirical accuracies; noted as not generally applicable to the arbitrary confidence scores used in this paper and challenging for sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>ECE bins predicted probabilities and computes weighted average absolute difference between predicted probability and empirical accuracy per bin; paper argues difficulty in applying ECE to sequence generation and non-probabilistic scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>ECE would require probabilistic scores and reliable ground-truth frequencies; authors instead prefer calibration-AUC and selective-AUC for generality.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Calibration measurement methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Mentioned as a commonly used calibration metric for categorical predictions but limited/unsuitable for many sequence-level confidence scores examined in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not used in experiments; authors explain ECE is not applicable to non-probabilistic confidence scores and is hard to compute for sequence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ECE compares predicted probabilities to observed frequencies (human labels) but was not used because many proposed scores are not probabilities and sequence-level ground truth is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ECE assumes probabilistic outputs and sufficient calibration data per bin; sequence outputs complicate binning and ground truth definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6157.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6157.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critique & revise (complementary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critique and revise (internal self-improvement pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-improvement pipeline where each sampled answer is critiqued by the model and then revised before scoring; used in combination with self-evaluation scoring to further improve accuracy and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Apply a self-critique prompt to generate a critique of each sampled answer, then generate revised answers conditioned on the critique; compute the same set of scoring metrics on revised answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same metrics (Accuracy, Calibration-AUC, Selective-AUC) applied to revised answers; used to test complementarity with self-evaluation scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PaLM-2 LARGE (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General QA (improving generated hypotheses before scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Generate critique and revised answers to improve individual candidate quality prior to confidence scoring and selection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On TruthfulQA with PaLM-2, self-critique & revise raised accuracies (e.g., Sample-and-Eval accuracy from 59.12% to 66.34%) and improved calibration/selective metrics; tables show consistent improvement across methods after revision.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>TruthfulQA (evaluation reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Revisions are model-generated; improvements measured against automated judge labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Additional inference cost and complexity; relies on the model's ability to self-detect and correct hallucinations; may not always converge to human-quality corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models (mostly) know what they know <em>(Rating: 2)</em></li>
                <li>Robots that ask for help: Uncertainty alignment for large language model planners <em>(Rating: 2)</em></li>
                <li>Measuring and improving model-moderator collaboration using uncertainty estimation <em>(Rating: 2)</em></li>
                <li>On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study <em>(Rating: 2)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 1)</em></li>
                <li>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6157",
    "paper_id": "paper-04365f0f1db4c659c3297cb8e70c39b38ed3b487",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "Sample and Select",
            "name_full": "Sample and Select (multi-choice token-level evaluation)",
            "brief_description": "Reduce free-form generation to a multiple-choice selection problem by sampling n candidate answers, mapping them to choice tokens (A, B, C...), and scoring choices using token-level probabilities/logits (with optional shuffle-and-average de-biasing and optional 'NONE OF THE ABOVE' candidate).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Generate n candidate answers from the LLM for each prompt, format candidates as labeled multiple-choice options, query the LLM to produce token-level scores (logits or softmax probabilities) for the choice tokens, and select the answer with the highest score; optional 'NONE OF THE ABOVE' (nota) is added to enable rejection.",
            "evaluation_criteria": "Accuracy of chosen answer within question (whether selected answer is correct), Calibration-AUC (ROC AUC for binary label correct/incorrect vs confidence score), Selective-AUC (area under selective generation curve when abstaining low-confidence outputs).",
            "llm_model_name": "PaLM-2 LARGE (primary), GPT-3 (text-davinci-003) (partial)",
            "theory_domain": "General question answering and summarization (evaluation of free-form generated content / hypotheses)",
            "theory_description": "Treat each sampled answer as a candidate hypothesis/claim and use multi-choice token-level scoring to decide which hypothesis to present as the model's output.",
            "evaluation_results": "PaLM-2: Sample and Select (vanilla) — Accuracy 58.26%, Calibration-AUC 53.17%, Selective-AUC 48.59%; with 'NONE OF THE ABOVE' (nota) — Accuracy 58.13%, Calibration-AUC 72.59%, Selective-AUC 56.61%. GPT-3: Sample and Select — Accuracy 72.24%, Calibration-AUC 47.97%, Selective-AUC 56.75% (nota variants not available for GPT-3 due to log-prob API limits).",
            "benchmarks_or_datasets": "Evaluated on TruthfulQA and TL;DR benchmarks (see dataset entries).",
            "comparison_to_human": "Ground-truth correctness labels come from human-aligned automated judges (GPT-judge for TruthfulQA, reward model for TL;DR) rather than fresh human annotation in this work; Sample-and-Select selection accuracy is compared against those labels.",
            "limitations_or_challenges": "Position bias (choice position affects scores) requiring shuffle-and-average de-biasing; probability dispersion when multiple sampled answers are correct (softmax spreads mass); forced choice when none of the sampled answers are correct unless 'NONE OF THE ABOVE' is added; score dependence on the candidate set (makes cross-question comparison difficult).",
            "uuid": "e6157.0",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Sample and Eval",
            "name_full": "Sample and Eval (pointwise true/false evaluation)",
            "brief_description": "Pointwise evaluation where the model is prompted to judge a single (question, candidate answer) pair as Yes/No (true/false) and the normalized token probability for 'Yes' is used as a confidence score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "For each sampled candidate answer, prompt the LLM in a true/false (Yes/No) format to judge whether the candidate is correct; convert model outputs to a probability p(Yes|x,y) via softmax of token logits and use that as the confidence score for (x,y).",
            "evaluation_criteria": "Pointwise predicted probability of correctness p(Yes|x,y); used to compute Accuracy (when used to select the top-scoring candidate), Calibration-AUC (ranking performance vs correctness), and Selective-AUC.",
            "llm_model_name": "PaLM-2 LARGE (primary), GPT-3 (text-davinci-003) (partial)",
            "theory_domain": "General question answering and summarization (assessing individual generated claims/answers)",
            "theory_description": "Each candidate generated answer is treated as a standalone hypothesis; the LLM is asked to evaluate the truth/quality of that hypothesis directly.",
            "evaluation_results": "PaLM-2: Sample and Eval — Accuracy 59.12%, Calibration-AUC 73.79%, Selective-AUC 58.19%; Sample and Eval w/ other candidates included in prompt — Accuracy 59.00%, Calibration-AUC 68.78%, Selective-AUC 55.70%. GPT-3: Sample and Eval — Accuracy 67.83%, Calibration-AUC 48.47%, Selective-AUC 53.28%; Sample and Eval w/ candidates — Accuracy 68.48%, Calibration-AUC 51.36%, Selective-AUC 55.28%.",
            "benchmarks_or_datasets": "Evaluated on TruthfulQA and TL;DR with automated judges (GPT-judge / reward model).",
            "comparison_to_human": "Confidence judged vs ground-truth labels supplied by automated human-aligned judges; Sample-and-Eval shows improved Calibration-AUC vs sequence-likelihood, indicating better agreement with human-judged correctness.",
            "limitations_or_challenges": "Pointwise scores are independent of other candidates (benefit) but do not provide a natural selection mechanism across candidates (so must be combined with Sample-and-Select for selection); dependent on LLM's token calibration for 'Yes'/'No' tokens; softmax normalization may be sensitive to prompt/tokenization in some APIs.",
            "uuid": "e6157.1",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Hybrid (Sample & Select then Sample & Eval)",
            "name_full": "Hybrid Sample-and-Select-and-Eval",
            "brief_description": "Two-stage approach: first use Sample-and-Select (multi-choice) to pick the best candidate per question, then discard selection score and use Sample-and-Eval to score the selected candidate; optionally penalize the pointwise score by the 'nota' uncertainty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Stage 1: select best candidate via multi-choice scoring (Sample-and-Select). Stage 2: compute pointwise p(Yes|x,ŷ) for the selected ŷ and adjust by nota uncertainty s_nota if nota is used; final confidence = p(Yes|x,ŷ) - p(c_nota|x,{c y}_+nota).",
            "evaluation_criteria": "Accuracy of selected answer, Calibration-AUC, Selective-AUC computed on final confidence score; used to abstain outputs in selective generation evaluations.",
            "llm_model_name": "PaLM-2 LARGE (primary), GPT-3 (text-davinci-003) (partial)",
            "theory_domain": "General QA and summarization (evaluation and selective output of candidate hypotheses)",
            "theory_description": "Selection produces the hypothesis to present; pointwise evaluation yields a cross-question-comparable confidence score for that hypothesis.",
            "evaluation_results": "PaLM-2: Hybrid — Accuracy 58.26%, Calibration-AUC 73.76%, Selective-AUC 57.38%; Hybrid w/ nota — Accuracy 58.14%, Calibration-AUC 75.34%, Selective-AUC 58.10%. GPT-3: Hybrid — Accuracy 72.24%, Calibration-AUC 51.66%, Selective-AUC 58.46% (nota variants NA for GPT-3 due to API limitations). Hybrid achieves a balance of higher selection accuracy (from Sample-and-Select) and stronger calibration (from Sample-and-Eval).",
            "benchmarks_or_datasets": "TruthfulQA and TL;DR.",
            "comparison_to_human": "Final confidences compared to automated human-aligned judgments (GPT-judge, reward model); hybrid shows improved Calibration-AUC over sequence-level scores and often matches or exceeds other self-evaluation variants.",
            "limitations_or_challenges": "Increased inference cost (additional LLM calls), reliance on sampling diversity for good candidates, requires handling of API/log-prob constraints for some models (e.g., GPT-3 limitations prevent some nota evaluations).",
            "uuid": "e6157.2",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Sequence likelihood",
            "name_full": "Sequence-level likelihood (log p(y|x)) and length-normalized likelihood",
            "brief_description": "Canonical approach using the model's sequence probability or length-normalized sequence probability for scoring generated sequences; historically used for ranking/generation but shown to be poorly calibrated for free-form outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute log p(y|x) = sum_t log p(y^t | y^{&lt;t}, x) or length-normalize by sequence length; use this score to rank or decide to abstain.",
            "evaluation_criteria": "Used as raw confidence score for accuracy, Calibration-AUC, and Selective-AUC comparisons.",
            "llm_model_name": "PaLM-2 LARGE, GPT-3 (text-davinci-003)",
            "theory_domain": "General free-form generation evaluation",
            "theory_description": "Use model-internal sequence probabilities as indicator of output quality/truthfulness for each generated hypothesis/answer.",
            "evaluation_results": "PaLM-2: Sequence likelihood — Accuracy 48.23%, Calibration-AUC 39.80%, Selective-AUC 33.63%; Length-norm sequence likelihood — Accuracy 52.75%, Calibration-AUC 50.09%, Selective-AUC 42.15%. GPT-3: Sequence likelihood and length-normalized both show Accuracy 67.19% with Calibration-AUC around 40-42% and Selective-AUC ~50-50.2%. Sequence-level scores often perform worse than token-level self-evaluation and can be negatively correlated with correctness.",
            "benchmarks_or_datasets": "Evaluated on TruthfulQA and TL;DR.",
            "comparison_to_human": "Compared to automated judge labels; sequence-likelihood shows poor alignment with human-judged correctness relative to self-evaluation methods.",
            "limitations_or_challenges": "Bias toward sequence length, poor ranking/correlation with true answer quality on open-ended tasks, not reliably calibrated for selective generation; length-normalization partially helps but does not fully solve calibration issues.",
            "uuid": "e6157.3",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Calibration-AUC",
            "name_full": "Calibration-AUC (ROC AUC for correctness vs confidence)",
            "brief_description": "AUC metric measuring ranking quality of confidence scores for binary correctness labels: area under the ROC curve where positive = correct output and negative = incorrect output; used as a model-agnostic calibration/ranking metric.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute ROC AUC treating each (x,ŷ) pair as a binary example (correct=1/incorrect=0) and using the confidence score s(x,ŷ) as the predictor; higher AUC indicates confidence scores rank correct answers higher than incorrect ones.",
            "evaluation_criteria": "Calibration-AUC value (0-100% as reported in paper tables) used to compare different scoring methods' ability to predict output correctness.",
            "llm_model_name": "Applies to outputs from PaLM-2 LARGE and GPT-3 as evaluated in the paper",
            "theory_domain": "General (applies to any domain where correctness labels are available)",
            "theory_description": "Not a theory but a ranking-based calibration metric for confidence scores assigned to generated hypotheses.",
            "evaluation_results": "Across experiments, token-level self-evaluation methods (Sample-and-Eval, Hybrid w/ nota) substantially increased Calibration-AUC vs sequence-likelihood; e.g., PaLM-2 Sample-and-Eval Calibration-AUC 73.79% vs sequence-likelihood 39.80%.",
            "benchmarks_or_datasets": "Used on TruthfulQA and TL;DR.",
            "comparison_to_human": "Metric compares model confidence to human-aligned labels (via GPT-judge / reward model) rather than direct human assessor calibration.",
            "limitations_or_challenges": "Calibration-AUC measures ranking ability, not calibration in absolute probability terms (so it's robust to monotonic transforms but not to post-hoc probability calibration); cannot be affected by simple temperature scaling to change absolute probabilities but preserves ranking.",
            "uuid": "e6157.4",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Selective-AUC",
            "name_full": "Selective-AUC (area under the selective generation curve)",
            "brief_description": "Area under the selective generation curve that plots accuracy of non-abstained outputs as a function of abstention rate; measures how well confidence scores allow a model to abstain low-quality outputs to improve overall quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Sort outputs by confidence score, progressively abstain the lowest-scored α% of examples, compute remaining accuracy as a function of α, and take the area under that curve as Selective-AUC.",
            "evaluation_criteria": "Selective-AUC (reported as percentage) quantifies the benefit of abstention policies driven by the confidence score.",
            "llm_model_name": "Applied to PaLM-2 LARGE and GPT-3 in experiments",
            "theory_domain": "General selective generation evaluation",
            "theory_description": "A higher Selective-AUC means the confidence estimator more effectively identifies and removes low-quality hypotheses, improving the quality of the outputs that remain.",
            "evaluation_results": "Sample-and-Eval and Hybrid w/ nota produced the highest Selective-AUCs in many experiments (PaLM-2 Sample-and-Eval 58.19%, Hybrid w/ nota 58.10% on TruthfulQA) compared to sequence-likelihood (33.63%).",
            "benchmarks_or_datasets": "Computed on TruthfulQA and TL;DR.",
            "comparison_to_human": "Selective-AUC measures alignment of confidence rankings with human-aligned correctness labels; it is a practical measure of usefulness for abstention compared to human preference.",
            "limitations_or_challenges": "Selective-AUC depends on quality of binary correctness labels and sampling strategy; computational cost of scoring many samples can be high.",
            "uuid": "e6157.5",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Accuracy (selection accuracy)",
            "name_full": "Accuracy of selected candidate",
            "brief_description": "Percentage of questions for which the selected (highest-scored) candidate answer is correct according to human-aligned labels; used both within-question (choosing best among n samples) and as the starting point (α=0) of selective curves.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "For each question, pick the candidate with highest confidence s(x,y) and check ground-truth correctness h(x,ŷ). Aggregate over dataset to compute accuracy.",
            "evaluation_criteria": "Accuracy percentage reported in tables; also used as baseline point for selective generation curve.",
            "llm_model_name": "PaLM-2 LARGE, GPT-3",
            "theory_domain": "General QA and summarization",
            "theory_description": "Measures whether the scoring method helps to pick the best candidate sample per question.",
            "evaluation_results": "Examples: PaLM-2 Sample-and-Select accuracy 58.26%, Sample-and-Eval 59.12%, Hybrid ~58.26%; on TL;DR Sample-and-Select accuracy ~70.20-70.80 vs Sample-and-Eval 68.70-70.20 depending on variant.",
            "benchmarks_or_datasets": "TruthfulQA, TL;DR",
            "comparison_to_human": "Compared to automated human-aligned labels; accuracy improvements reported after applying self-critique & revise as well.",
            "limitations_or_challenges": "Accuracy alone does not measure calibration or whether confidence scores are suitable for abstention; methods that maximize accuracy may not produce well-calibrated confidence scores.",
            "uuid": "e6157.6",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "TruthfulQA",
            "name_full": "TruthfulQA: Measuring how models mimic human falsehoods",
            "brief_description": "Benchmark dataset for assessing models' tendency to generate falsehoods or mimic misconceptions; contains questions where models often produce plausible but false answers.",
            "citation_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "mention_or_use": "use",
            "evaluation_method": "Models generate answers to 817 validation questions; generated answers are labeled for truthfulness using GPT-judge (a GPT-3 model fine-tuned on human feedback) to approximate human evaluations.",
            "evaluation_criteria": "Correctness/truthfulness labels (binary) used to compute Accuracy, Calibration-AUC, and Selective-AUC.",
            "llm_model_name": "Used to evaluate outputs from PaLM-2 LARGE and GPT-3 in the paper.",
            "theory_domain": "Truthfulness / factuality in QA",
            "theory_description": "Dataset instances are prompts/questions that test factual accuracy and resistance to common misconceptions; in this work, generated answers are treated as candidate hypotheses evaluated for correctness.",
            "evaluation_results": "Paper reports method comparisons on TruthfulQA (see method-specific results above). GPT-judge used for labels has reported 90-95% accuracy vs human evaluations per Lin et al. [2021].",
            "benchmarks_or_datasets": "This entry is the dataset itself.",
            "comparison_to_human": "Ground-truth labels are obtained via GPT-judge which was fine-tuned on human feedback and shown to agree with humans ~90-95%; direct human labels were not used at scale in this study.",
            "limitations_or_challenges": "Using an automated judge (GPT-judge) as a proxy for human labels can introduce approximation error; dataset size (817 validation examples) limits some analyses.",
            "uuid": "e6157.7",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "TL;DR",
            "name_full": "TL;DR: Mining reddit to learn automatic summarization",
            "brief_description": "A large summarization dataset mined from Reddit posts and their TL;DR comments used to evaluate summary generation and summary quality.",
            "citation_title": "Tl; dr: Mining reddit to learn automatic summarization",
            "mention_or_use": "use",
            "evaluation_method": "Authors sampled 1,000 examples from TL;DR test split (15,240 total) and generated summaries; summary quality was labeled using a reward model fine-tuned on human feedback.",
            "evaluation_criteria": "Binary concise/comprehensive judgment and summary quality labels used to compute Accuracy, Calibration-AUC, and Selective-AUC.",
            "llm_model_name": "PaLM-2 LARGE",
            "theory_domain": "Abstractive summarization quality evaluation",
            "theory_description": "Each generated summary is treated as a produced hypothesis; pointwise evaluation asks whether a summary is concise and comprehensive.",
            "evaluation_results": "Sample-and-Select and Sample-and-Eval variants tested; Sample-and-Select variants often had higher accuracy while Sample-and-Eval gave better calibration in some variants (see Table 3 in paper). Reward model accuracy vs human is reported at 71.34%.",
            "benchmarks_or_datasets": "TL;DR dataset as above.",
            "comparison_to_human": "Quality labels are from a reward model trained on human feedback; reward model agreement with human ratings is ~71.34%, so automated labels are approximate.",
            "limitations_or_challenges": "Using the reward model as proxy for humans imposes label noise; sampling 1,000 examples was used to limit inference cost, so results are on a subset.",
            "uuid": "e6157.8",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Automated judges (GPT-judge / reward model)",
            "name_full": "Automated human-aligned evaluators (GPT-judge; reward model trained on human feedback)",
            "brief_description": "Automated systems used to label generated outputs' correctness/quality: GPT-judge (fine-tuned GPT-3) for TruthfulQA, and a reward model fine-tuned on human feedback for TL;DR summarization.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Use the automated judge to produce binary correctness/quality labels for generated outputs so that confidence metrics can be evaluated at scale.",
            "evaluation_criteria": "Binary correctness / concise+comprehensive judgments used to compute Accuracy, Calibration-AUC, Selective-AUC.",
            "llm_model_name": "GPT-judge is GPT-3 based; reward model specifics per Zhao et al. 2023",
            "theory_domain": "General evaluation proxy to approximate human judgments",
            "theory_description": "These systems approximate human labels, enabling large-scale quantitative evaluation of scoring/calibration methods.",
            "evaluation_results": "GPT-judge reported agreement with human truthfulness labels of ~90-95%; reward model for TL;DR reported prediction accuracy vs human rating of 71.34%. These judges were used as ground truth for the paper's metrics.",
            "benchmarks_or_datasets": "Used for TruthfulQA and TL;DR labeling.",
            "comparison_to_human": "Authors rely on prior reported judge accuracies to argue these automated labels are reasonable proxies for humans; they do not perform fresh human labeling at scale.",
            "limitations_or_challenges": "Automated judges introduce approximation error (especially the reward model with ~71% agreement); reliance on them can bias evaluation conclusions.",
            "uuid": "e6157.9",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Sequence-level ECE (discussed)",
            "name_full": "Expected Calibration Error (ECE) — sequence-level distinction",
            "brief_description": "Standard probabilistic calibration metric measuring difference between predicted probabilities and empirical accuracies; noted as not generally applicable to the arbitrary confidence scores used in this paper and challenging for sequence generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method": "ECE bins predicted probabilities and computes weighted average absolute difference between predicted probability and empirical accuracy per bin; paper argues difficulty in applying ECE to sequence generation and non-probabilistic scores.",
            "evaluation_criteria": "ECE would require probabilistic scores and reliable ground-truth frequencies; authors instead prefer calibration-AUC and selective-AUC for generality.",
            "llm_model_name": null,
            "theory_domain": "Calibration measurement methodology",
            "theory_description": "Mentioned as a commonly used calibration metric for categorical predictions but limited/unsuitable for many sequence-level confidence scores examined in this work.",
            "evaluation_results": "Not used in experiments; authors explain ECE is not applicable to non-probabilistic confidence scores and is hard to compute for sequence outputs.",
            "benchmarks_or_datasets": "",
            "comparison_to_human": "ECE compares predicted probabilities to observed frequencies (human labels) but was not used because many proposed scores are not probabilities and sequence-level ground truth is challenging.",
            "limitations_or_challenges": "ECE assumes probabilistic outputs and sufficient calibration data per bin; sequence outputs complicate binning and ground truth definitions.",
            "uuid": "e6157.10",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-critique & revise (complementary)",
            "name_full": "Self-critique and revise (internal self-improvement pipeline)",
            "brief_description": "A self-improvement pipeline where each sampled answer is critiqued by the model and then revised before scoring; used in combination with self-evaluation scoring to further improve accuracy and calibration.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Apply a self-critique prompt to generate a critique of each sampled answer, then generate revised answers conditioned on the critique; compute the same set of scoring metrics on revised answers.",
            "evaluation_criteria": "Same metrics (Accuracy, Calibration-AUC, Selective-AUC) applied to revised answers; used to test complementarity with self-evaluation scoring.",
            "llm_model_name": "PaLM-2 LARGE (evaluated)",
            "theory_domain": "General QA (improving generated hypotheses before scoring)",
            "theory_description": "Generate critique and revised answers to improve individual candidate quality prior to confidence scoring and selection.",
            "evaluation_results": "On TruthfulQA with PaLM-2, self-critique & revise raised accuracies (e.g., Sample-and-Eval accuracy from 59.12% to 66.34%) and improved calibration/selective metrics; tables show consistent improvement across methods after revision.",
            "benchmarks_or_datasets": "TruthfulQA (evaluation reported in Table 2).",
            "comparison_to_human": "Revisions are model-generated; improvements measured against automated judge labels.",
            "limitations_or_challenges": "Additional inference cost and complexity; relies on the model's ability to self-detect and correct hallucinations; may not always converge to human-quality corrections.",
            "uuid": "e6157.11",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 2
        },
        {
            "paper_title": "Robots that ask for help: Uncertainty alignment for large language model planners",
            "rating": 2
        },
        {
            "paper_title": "Measuring and improving model-moderator collaboration using uncertainty estimation",
            "rating": 2
        },
        {
            "paper_title": "On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study",
            "rating": 2
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs",
            "rating": 1
        }
    ],
    "cost": 0.01777925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Evaluation Improves Selective Generation in Large Language Models</h1>
<p>Jie Ren<em>, Yao Zhao</em>, Tu Vu ${ }^{\dagger}$, Peter J. Liu<em>, Balaji Lakshminarayanan</em><br>{jjren, yaozhaoyz, ttvu, peterjliu, balajiln}@google.com<br>Google DeepMind ${ }^{\star}$, Google Research ${ }^{\dagger}$</p>
<h4>Abstract</h4>
<p>Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequencelevel probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate openended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a "None of the above" option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PalM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) are often pre-trained on a vast corpus of text and then fine-tuned on supervised data to follow instructions [Devlin et al., 2018, Radford et al., 2018, Raffel et al., 2020, Adiwardana et al., 2020, Wei et al., 2021, Ouyang et al., 2022, Chung et al., 2022]. Having the ability to tell when a language model's output is trustworthy is important for safe deployment of language models. For example, the model's trustworthiness can be used as signal to selectively generate answers based on how confident the LLM is in the quality of its output.</p>
<p>Prior research has demonstrated that the distance to the training distribution in the embedding space predicts output quality for conditional generative models [Ren et al., 2023b]. Extending this work to large language models is challenging because their training distribution is too large to estimate and extracting embeddings from well-integrated LLM systems requires significant engineering effort.</p>
<p>Alternatively, a straightforward approach to estimating a language model's confidence in its output is to calculate the sequence probability or the length-normalized sequence probabilities [Adiwardana et al., 2020]. However, studies have shown that language models' sequence probabilities on openended generations do not reliably rank-order their outputs by quality [Liu et al., 2022, Ren et al., 2023b]. Human feedback can be used to fine-tune language models to better align with human-judged quality, such as with Reinforcement Learning from Human Feedback (RLHF) [Stiennon et al., 2020], SLiC-HF [Zhao et al., 2023] and DPO [Rafailov et al., 2023], resulting in better quality-calibrated models.</p>
<p>Since human feedback data is expensive to obtain, we explore leveraging the self-evaluation ability of LLMs to improve quality-calibration. Despite the poor calibration on sequence-level likelihood, recent work has shown that LLM token-level probability can be quite well-calibrated on choosing the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Demonstration of our approach.</p>
<p>correct option of multi-choice question answering and true/false questions [Kadavath et al., 2022, OpenAI, 2023, Robinson et al., 2022]. This suggests that evaluating language model's generation with token-level probabilities using an appropriate prompt format might be better for selective generation than sequence-level likelihood.</p>
<p>In this study, we focus on obtaining a confidence score that is quality-calibrated on free-form generation tasks. We propose reducing the sequence-level scoring problem to token-level scoring by designing different self-evaluation tasks and propose a variety of scores. We focus on evaluating model's quality-calibration for use in selective generation, and not just predictive accuracy. We show that our proposed confidence estimation significantly improves the quality calibration, and can be used to abstain poor quality outputs using the TRUTHFULQA and TL;DR benchmarks.</p>
<h2>2 Methods</h2>
<p><strong>Background: sequence likelihood</strong> Given a question <em>x</em> and answer <em>y</em>, <em>y</em> = <em>y</em><sup>1</sup><em>y</em><sup>2</sup> . . . <em>y</em><sup>t</sup>, we have sequence-level likelihood score,</p>
<p>$$
\log p(\mathbf{y}|\mathbf{x}) = \sum_{t=1}^{l} \log p(y^{t}|y^{1} \dots y^{t-1}, \mathbf{x}). \qquad \text{(Sequence likelihood)}
$$</p>
<p>Though log <em>p</em>(<strong>y</strong>|<strong>x</strong>) is statistically meaningful, it has been shown that it is biased towards sequence length, i.e. models tend to underestimate sequence likelihood of longer sentences [Wu et al., 2016]. The length normalized likelihood is an alternative score to use,</p>
<p>$$
\log \bar{p}(\mathbf{y}|\mathbf{x}) = \frac{1}{l} \sum_{t=1}^{l} \log p(y^{t}|y^{1} \dots y^{t-1}, \mathbf{x}). \qquad \text{(Length normalized sequence likelihood)}
$$</p>
<p>Although sequence-level scores have weak predictive power, the previous results show that LLMs are well-calibrated on multiple choice question answer tasks and true/false evaluation tasks [Kadavath et al., 2022, OpenAI, 2023], suggesting the model has better calibration on token-level scores. Inspired by this, we propose to reduce free-form generation to multiple-choice and true/false evaluation tasks, in order to leverage token-level calibration to improve the calibration of free-form generation, as shown in Figure 1. Ren et al. [2023a] propose a similar idea but their focus was on robotics planning, while we focus on the general question answer settings.</p>
<p>To convert free-form generation to multi-choice question answer task, we first sample multiple candidate answers. For a given question <em>x</em>, we sample <em>n</em> answers {<strong>y</strong><sub><em>i</em></sub>}, <em>i</em> = 1, . . . , <em>n</em> from an LLM. We tried using a prompt to instruct the model to generate multiple different answers all at once, but the quality of the batch generated answers were not as good as sampling one at a time.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The issues of position bias, probability dispersion, and no true answers in the Sample and Select setup. The question examples are from [Lin et al., 2021, Agarwal et al., 2023].</p>
<h3>2.1 Sample and Select: reduce free-form generation to multi-choice question answer task</h3>
<p>Given a question and a set of candidate answers {y}<em i="i">{n}, we append alphabet characters, c = A, B, C, ..., to the answers and form it into a multiple choice format. A straightforward score could be the softmax probability for the characters, p(c</em>). However, there are a few issues with that score:}|x, {cq}), which was used in Ren et al. [2023a]. The selected answer would be the one with the highest softmax probability, y˜ = y_{r}, r = arg max_{i} p(c_{i}|x, {cq</p>
<p><strong>Position bias</strong> The score could change as the position of the candidate answers change. See Figure 2 (left). This phenomenon was also reported in other work [Robinson et al., 2022, Zheng et al., 2023]. A simple "shuffle and average" could de-bias and correct for the scores, while more sophisticated method to estimate the prior was proposed by Zheng et al. [2023]. In our work, we use the simple shuffle and average de-bias method. The ablation study of the effect of position bias is in Table 4.</p>
<p><strong>Probability dispersion</strong> among multiple true answers. Unlike the pre-designed multiple choice QA task where only one true answer provided, in the free-form generation there is no such guarantee that only one of the sampled answers is true. When more than one true answers are in the candidate list, the probability of the true is dispersed among the true answers, see Figure 2 (middle). This is an undesired property for comparing across questions, since different questions could generate different number of true answers. Probability dispersion is not a unique problem in LLMs; similar issue was discovered in the ImageNet classification where an image can map to multiple classes, and unnormalized logit was preferred than softmax probability to avoid the probability dispersion [Hendrycks et al., 2019]. Therefore we propose,</p>
<p>$$
\log p(c_i | x, {c y}), c = {A, B, \dots}. \qquad \text{(Sample and Select)}
$$</p>
<p><strong>No answer is true</strong> It is possible that when the model does not know the answer, none of the sampled answers is true. If only wrong answers are provided, the model will be forced to choose one from them, resulting in over-confident prediction. See Figure 2 (right). To mitigate that, we add "NONE OF THE ABOVE" as an additional candidate answer to give model a chance to reject the sampled answers, {y}_{nota} = {y} ∪ {nota}. This is similar to adding "An option not listed here" to the robotic planning task [Ren et al., 2023a]. We obtain the score corresponding to the "NONE OF THE ABOVE" answer,</p>
<p>$$
p(c_{\text{nota}} | x, {c y}_{+ \text{nota}}) \qquad \text{(Sample and Select w/ NONE OF THE ABOVE)}
$$</p>
<p>A higher nota score indicates that the selected answer is less likely to be correct. So we use -p(cnota|x, {cq}+nota) as the confidence score of the selected answer, yˆ = y_{r}, r = arg max_{i} p(c_{i}|x, {cq}). Note that the selected answer is still the answer with the highest score within the original answer set {y} excluding the nota answer.</p>
<h3>2.2 Sample and Eval: reduce free-form generation to true/false evaluation task</h3>
<p>We can also evaluate a question and an answer pair using pointwise evaluation format. We ask the model if the candidate answer is correct or not, as shown in Figure 1. Since the task is a binary</p>
<p>classification task, we can normalize the output score using softmax function to a probability,</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \boldsymbol{y}_{i})
$$</p>
<p>(Sample and Eval)
This is similar the P(True) proposed in [Kadavath et al., 2022]. They also propose to include candidate answers in the prompt,</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \boldsymbol{y}_{i},{\boldsymbol{y}})
$$</p>
<p>(Sample and Eval w/ other candidates)
But that work focuses on the scaling law of the score's calibration, and did not compare it with sequence-level score and Sample and Select score.</p>
<h1>2.3 Combining the best of both worlds: select the answer via multi-choice evaluation and score the selected answer via pointwise evaluation</h1>
<p>Sample and Select and Sample and Eval have their own pros and cons. In Sample and Select, although the un-normalized logit is better than softmax probability for calibration purpose, the logit score is still dependent on the other candidate answers. For fairly comparing across $(\boldsymbol{x}, \boldsymbol{y})$ pairs, a good score should measure the confidence to the $(\boldsymbol{x}, \boldsymbol{y})$ itself, not dependent on other candidate answers. Sample and Eval score $p\left(\right.$ Yes $\left.\mid \boldsymbol{y}<em r="r">{i}, \boldsymbol{x}\right)$ is indeed independent of other answers. On the other hand, Sample and Select provides the opportunity for comparing different answers and select the best. Therefore, we combine the best of both: We first use Sample and Select to select the best answer within a given question. The answer with the highest softmax probability score is selected, $\hat{\boldsymbol{y}}=\boldsymbol{y}</em>, r=\arg \max <em i="i">{i} p\left(c</em>)$.} \mid \boldsymbol{x},{c \boldsymbol{y}}\right)$. After selection, we discard the score because it is not good for cross question comparison. We score the selected answer via Sample and Eval $p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}}), \text { where } \hat{\boldsymbol{y}}=\boldsymbol{y}<em i="i">{r}, r=\arg \max </em>}\right)
$$} p\left(c_{i} \mid \boldsymbol{x},{c \boldsymbol{y</p>
<p>In the case where None of the above answer is added, we penalize the confidence score $p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}})$ with the uncertainty score for the nota answer, that is $p\left(\right.$ Yes $\mid \boldsymbol{x}, \hat{\boldsymbol{y}}$ ) $-p\left(c_{\text {nota }} \mid \boldsymbol{x},{c \boldsymbol{y}}{ }_{+\text {nota }}\right)$. We call this hybrid strategy "Sample and Select and Eval". See details in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Hybrid</span><span class="w"> </span><span class="s">&quot;Sample and Select and Eval&quot;</span>
<span class="w">    </span><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Question</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="kd">choice</span><span class="w"> </span><span class="nx">selection</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}</span><span class="err">\</span><span class="p">),</span>
<span class="w">        </span><span class="nx">pointwise</span><span class="w"> </span><span class="nx">evaluation</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Use</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">stackrel</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">iid</span><span class="w"> </span><span class="p">}}{=}</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Append</span><span class="w"> </span><span class="s">&quot;None of the above&quot;</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}=</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">nota</span><span class="err">\</span><span class="p">}.</span><span class="w"> </span><span class="o">|</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="o">|=</span><span class="nx">n</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Compose</span><span class="w"> </span><span class="nx">selection</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">})</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">),</span><span class="w"> </span><span class="nx">feed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">softmax</span><span class="w"> </span><span class="nx">probabi</span><span class="o">-</span>
<span class="w">        </span><span class="nx">ity</span><span class="w"> </span><span class="nx">scores</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">best</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">among</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">sampled</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="p">(</span><span class="nx">exclude</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">post</span><span class="o">-</span><span class="nx">hoc</span><span class="w"> </span><span class="nx">added</span><span class="w"> </span><span class="nx">nota</span><span class="w"> </span><span class="nx">answer</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}}=</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">r</span><span class="p">},</span><span class="w"> </span><span class="nx">r</span><span class="p">=</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="nx">neq</span><span class="w"> </span><span class="nx">n</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Obtain</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">uncertainty</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">nota</span><span class="w"> </span><span class="nx">answer</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}=</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Compose</span><span class="w"> </span><span class="nx">pointwise</span><span class="w"> </span><span class="nx">evaluation</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}})</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">feed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">output</span>
<span class="w">        </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="p">=</span><span class="nx">p</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Yes</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}})</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">The</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="nx">confidence</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="p">=</span><span class="nx">s</span><span class="o">-</span><span class="nx">s_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">its</span><span class="w"> </span><span class="nx">confidence</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="err">\</span><span class="p">).</span>
</code></pre></div>

<h2>3 Evaluation metrics for selective generation</h2>
<p>Suppose $\mathcal{D}={\boldsymbol{x}}<em n="n">{m}$ is a dataset containing $m$ questions to evaluate. Given a LLM model $\mathcal{M}$, for each question $\boldsymbol{x}$, we randomly sample $n$ answers ${\boldsymbol{y}}</em>}=\left{\boldsymbol{y<em 2="2">{1}, \boldsymbol{y}</em>}, \ldots, \boldsymbol{y<em i="i">{n}\right}$, where $\boldsymbol{y}</em>)$ pair, we would like evaluate how well the score could be used for selective generation, besides the accuracy.
Accuracy For a fixed question $\boldsymbol{x}$ and a set candidate answers ${\boldsymbol{y}}} \stackrel{\text { iid }}{=} \mathcal{M}(\boldsymbol{x})$. Suppose the ground truth $h(\boldsymbol{x}, \boldsymbol{y})={0,1}$ for each answer's correctness (or quality) is available, either through human evaluation or an auto-evaluation model to approximate human rating. Given a confidence score function $s(\boldsymbol{x}, \boldsymbol{y})$ measuring the confidence of a $(\boldsymbol{x}, \boldsymbol{y<em r="r">{n}$ to $\boldsymbol{x}$, we could use the confidence score to select the final answer $\hat{\boldsymbol{y}}$ to the question $\boldsymbol{x}$. We assess if the selected answer is correct, i.e. $h(\boldsymbol{x}, \hat{\boldsymbol{y}})=1, \hat{\boldsymbol{y}}=\boldsymbol{y}</em>, r=\arg \max <em i="i">{i=1}^{n} s\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)$.</p>
<p>Accuracy evaluates if the score can be used to choose the best answer among the candidate answers within a given question. For selective generation, we compare across questions. Given the $m$ question and its selected best answer, ${(\boldsymbol{x}, \hat{\boldsymbol{y}})}_{m}$, we would abstain poor quality pairs to ensure better overall generation quality, aka selective generation. Suppose for each pair we have a confidence score, $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$. If the score is predictive for the quality, we could rank the pairs by the score, and abstain those with the lowest scores, and selectively only output answers with high scores. For the abstained low quality answers, we could instead output "SORRY, I DON'T KNOW". An honest "I don't know" answer is better then a wrong answer. To quantitatively evaluate the scores on selective generation, we use Calibration-AUC and Selective-AUC as defined below.</p>
<p>Calibration-AUC AUC metric for a binary prediction task where the binary label is the correctness $h(\boldsymbol{x}, \hat{\boldsymbol{y}})$, and the prediction score is the confidence score $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$ [Kivlichan et al., 2021]. Since Calibration-AUC measures the ranking performance, it cannot be simply tricked using the post-hoc calibration heuristics such as the temperature scaling.</p>
<p>Selective generation curve and AUC Selective generation curve measures the correctness $h(\boldsymbol{x}, \hat{\boldsymbol{y}})$ as a function of abstention rate $\alpha \%$, where the samples are sorted by $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$ and samples with the lowest $\alpha \%$ scores are abstained [Ren et al., 2023b]. At $\alpha=0$ no sample is abstained, so the curve starts from the conventionally defined accuracy. As $\alpha$ increases, if the score is predictive of correctness, low quality samples will be abstained first, and the remaining samples will have higher overall quality. Therefore we expect the curve to increase. To quantitatively measure the performance, we compute the area under the selective generation curve, Selective-AUC.</p>
<p>Distinction to Expected Calibration Error (ECE) ECE [Guo et al., 2017] is commonly used to measure if the predictive probability value matches the ground truth accuracy. ECE computation is straightforward for categorical prediction. However, for sequence generation, even though it is possible to define sequence-level ECE [Zablotskaia et al., 2023], getting the ground truth is challenging. Also ECE can only be applied to probabilistic scores. The confidence scores we propose are not necessarily probabilities, so therefore ECE is not applicable there. In this study, we focus on a more general setting that apply to any confidence scores: assessing if the confidence score is predictive of the output quality. Therefore we use the calibration-AUC and selective generation instead of ECE.</p>
<h1>4 Experiments</h1>
<h3>4.1 Experiment setup</h3>
<p>LLMs PALM-2 LARGE is mainly used in our experiments. For each question, we sample $n=4$ answers at temperature 1.0. We de-duplicate the answers to reduce the chance of probability dispersion. We also consider GPT-3 (text-davinci-003) model for evaluation. Due to the OpenAI API limitation, we cannot evaluate all the methods and obtain complete results for GPT-3. We can neither evaluate methods on GPT-3.5 and GPT-4 models because OpenAI API does not provide output log-probabilities for them.</p>
<p>Benchmark datasets TruthfulQA [Lin et al., 2021] is a dataset for assessing model's ability to generate truthful answers against false belief or misconception. It contains 817 questions in the validation split. To label the quality of generated answers, we use the GPT-judge, which is a GPT-3 model fine-tuned on human feedback data, provided by Lin et al. [2021]. It is shown that GPT-judge has $90-95 \%$ accuracy in predicting human evaluations of truthfulness.</p>
<p>TL;DR is a summarization benchmark dataset mined from Reddit website [Völske et al., 2017]. It contains 15,240 examples in the test split. We randomly sampled 1000 examples to save inference cost. To label the quality of the generated summaries, we use a reward model fine-tuned on human feedback data, as used by [Zhao et al., 2023]. The prediction accuracy of human rating of the reward model is $71.34 \%$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison of different scores for the accuracy and calibration metrics on TruthfulQA for PALM-2 LARGE and GPT-3 models. The numbers are in percentage.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PALM-2 LARGE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">48.23</td>
<td style="text-align: center;">39.80</td>
<td style="text-align: center;">33.63</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">52.75</td>
<td style="text-align: center;">50.09</td>
<td style="text-align: center;">42.15</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">53.17</td>
<td style="text-align: center;">48.59</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">58.13</td>
<td style="text-align: center;">72.59</td>
<td style="text-align: center;">56.61</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">$\mathbf{5 9 . 1 2}$</td>
<td style="text-align: center;">$\underline{73.79}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 1 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\underline{59.00}$</td>
<td style="text-align: center;">68.78</td>
<td style="text-align: center;">55.70</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">73.76</td>
<td style="text-align: center;">57.38</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">58.14</td>
<td style="text-align: center;">$\mathbf{7 5 . 3 4}$</td>
<td style="text-align: center;">$\underline{58.10}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">49.76</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">42.06</td>
<td style="text-align: center;">50.22</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 4}$</td>
<td style="text-align: center;">47.97</td>
<td style="text-align: center;">56.75</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">67.83</td>
<td style="text-align: center;">48.47</td>
<td style="text-align: center;">53.28</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\underline{68.48}$</td>
<td style="text-align: center;">$\underline{51.36}$</td>
<td style="text-align: center;">$\underline{55.28}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 4 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>
<h1>4.2 Results</h1>
<p>The performance of the different scores evaluated using accuracy, calibration-AUC, and selectiveAUC are shown in Table 1. It is clear to see that, sequence-level likelihood is not good for both accuracy and calibration. It has even below 0.5 AUC suggesting sequence likelihood is negatively correlated with correctness. Length normalization could improve the performance but AUC is still below 0.5 . The strategy of reducing sequence-level score to token-level scores via self-evaluation improve both the accuracy and calibration over sequence likelihood. Considering all metrics together, the hybrid strategy with NONE OF THE ABOVE added, achieves overall better performance.
Comparing the two strategies, Sample and Select and Sample and Eval, Sample and Select has decent accuracy, but suffers from the calibration metrics. Adding NONE OF THE ABOVE helps improve calibration. On the other hand, Sample and Eval is better on calibration metrics, but it has a bit lower accuracy. This trend is more clear in GPT-3. Therefore we propose the hybrid strategy to combine the best of both. The ROC curves for binary classification of correct and incorrect answers using different scores, and the selective generation curves can be found in Figure 3. Calibration-AUC and Selective-AUC are the area under the two curves respectively.
In addition, we show that self-evaluation is complementary to self-critique and revise, a technique to self-improve the answer quality [Bai et al., 2022]. We first apply that technique to improve each of the sampled answers. Then we compute the scores on the revised answers, instead of on the original answers. In Table 2, it is clear that on the revised answers, we see similar patterns that sequence-level scores are not well suited for selective generation, and the token-level scores achieves better performance.</p>
<p>Table 2: Self-critique and revise further improves the model's accuracy, calibration, and selective generation on TruthfulQA on PALM-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">54.83</td>
<td style="text-align: center;">38.96</td>
<td style="text-align: center;">38.40</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">59.12</td>
<td style="text-align: center;">49.64</td>
<td style="text-align: center;">47.03</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">64.87</td>
<td style="text-align: center;">50.41</td>
<td style="text-align: center;">52.40</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">64.60</td>
<td style="text-align: center;">66.92</td>
<td style="text-align: center;">58.69</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">$\underline{66.34}$</td>
<td style="text-align: center;">70.55</td>
<td style="text-align: center;">$\mathbf{6 1 . 8 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\mathbf{6 6 . 7 1}$</td>
<td style="text-align: center;">64.69</td>
<td style="text-align: center;">59.44</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">64.87</td>
<td style="text-align: center;">$\underline{71.35}$</td>
<td style="text-align: center;">61.11</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">64.50</td>
<td style="text-align: center;">$\mathbf{7 2 . 7 2}$</td>
<td style="text-align: center;">$\underline{61.44}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ROC curves for binary classification and selective generation curves, evaluated on TruthFULQA. The left most point of the selective generation curves (abstention rate $\alpha=0$ ) is the accuracy reported in Table 1. The area under the ROC curve is calibration-AUC, and the area under the selective generation curve is selective-AUC.</p>
<h1>4.3 Self-evaluation improves calibration on TL;DR summarization</h1>
<p>TL;DR is a summarization benchmark dataset mined from Reddit website [Völske et al., 2017]. Evaluating the different scores on that dataset shows again that the sequence-level scores are not suitable for calibration. Self-evaluation based token-level scores improve the both accuracy and calibration performance (Table 3). Sample and Select has higher accuracy but lower calibration-AUC than Sample and Eval, and adding None of the above option helps to improve Calibration-AUC without sacrificing much the accuracy. Hybrid methods in general have decent performance.</p>
<p>Table 3: Comparison of different scores: accuracy and calibration on TL;DR for PaLM-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">65.80</td>
<td style="text-align: center;">49.75</td>
<td style="text-align: center;">52.63</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">69.40</td>
<td style="text-align: center;">$\underline{53.20}$</td>
<td style="text-align: center;">56.93</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">46.65</td>
<td style="text-align: center;">54.68</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">$\mathbf{7 0 . 8 0}$</td>
<td style="text-align: center;">49.54</td>
<td style="text-align: center;">56.56</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">68.70</td>
<td style="text-align: center;">52.34</td>
<td style="text-align: center;">56.09</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">$\mathbf{5 5 . 1 9}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 9 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">$\underline{70.70}$</td>
<td style="text-align: center;">52.19</td>
<td style="text-align: center;">$\underline{57.56}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">$\mathbf{7 0 . 8 0}$</td>
<td style="text-align: center;">52.05</td>
<td style="text-align: center;">$\underline{57.55}$</td>
</tr>
</tbody>
</table>
<h3>4.4 Effect of position bias</h3>
<p>We assess the effect of position bias on the performance. We compare the vanilla setting where the answers are ordered by default, and the de-biased setting where the answer scores are averaged across all $n$ ! possible permutations. The difference on the performance is not that significant. Given</p>
<p>the de-bias process through shuffle and average is very computational expensive, we use the vanilla setting by default.</p>
<p>Table 4: Effect of position bias on metrics. The results are based on PALM-2 LARGE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TruthFULQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, vanilla</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">53.17</td>
<td style="text-align: center;">48.59</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, de-biased</td>
<td style="text-align: center;">58.87</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">48.58</td>
</tr>
<tr>
<td style="text-align: left;">TL;DR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, vanilla</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">46.65</td>
<td style="text-align: center;">54.68</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, de-biased</td>
<td style="text-align: center;">70.70</td>
<td style="text-align: center;">43.94</td>
<td style="text-align: center;">53.86</td>
</tr>
</tbody>
</table>
<h1>5 Related work</h1>
<p>The calibration of LLMs on multiple choice question answer tasks is studied in Kadavath et al. [2022]. Robinson et al. [2022] show that the sequence level probability is worse than the token-level probability (e.g. A, B, C, etc) for predicting the correctness. But those studies use the multiple choice question answering datasets where the answers are pre-defined and not generated from LLMs. Our work focuses on the calibration of free-form generation tasks. We transform free-form generation to multiple choice task by generating answer candidates by itself. Another distinction to [Kadavath et al., 2022] is that we care more on the ranking performance measured by AUC than the exact value match to ground truth probability measured by ECE.
In terms of estimating language models' confidence or uncertainty, Tian et al. [2023], Lin et al. [2022] propose to ask model to express uncertainty in words along with the generated answer, but it is shown that LLMs often exhibit a high degree of overconfidence when verbalizing their confidence [Xiong et al., 2023]. Kuhn et al. [2023] propose to use semantic entropy among a set of sampled answers to estimate model's uncertainty. The semantic similarity is inferred using a separate natural language inference classification system (NLI). Cole et al. [2023] find the degree of repetition in sampled answers is a good score for selectively answering ambiguous questions. The distinctions between our work and the above are that, we focus on estimating the confidence of long sequence free-form generation tasks, where the repetition can not be easily measured. Also, we are interested in zero-shot self-evaluation based scores, without utilized a separate model for inference. The true/ false evaluation method proposed by Kadavath et al. [2022] is one of them. In our work, we compare this score with several other scores and have a comprehensive assessment on selective generation of free-form generation tasks
Prior studies have proposed generating multiple candidate responses for free-form generation tasks and then selecting the best. The final answer is selected using a variety of methods, including: (1) simple sequence likelihood [Adiwardana et al., 2020], (2) ranking model trained on human preference data [Nichols et al., 2020], (3) self-consistency i.e. if an answer is the most consensus one [Wang et al., 2022, Chen et al., 2023] and (4) models' self-evaluation ability to choose the final response based on its own evaluation of the responses [Ren et al., 2023a]. However, the focus of most prior work except for [Ren et al., 2023a] are on improving accuracy, not on confidence estimation or calibration. [Ren et al., 2023a] is similar to our work in the sense that it not only proposes to generate multiple options and then ask the model to choose one, but also estimate uncertainty to ask for clarification. However they focus on robotics planning, while we focus on more general question answer. Also, they directly use the multiple choice score output, while we identified the position bias and probability dispersion problems in the scores, and propose hybrid method to address them</p>
<h2>6 Discussion</h2>
<p>We show that although generic sequence-level scores are not well suited for selective generation (even negatively correlated with the the quality) for free-form generation, asking the model again to self-evaluate could reduce the sequence-level score to token-levels scores, improving quality calibration. Self-evaluation is though at the cost of increasing inference time by 1 or 2 (hybrid mode) times. Alternative to this post-hoc method, how to improve the quality calibration of the sequence-level score during training and finetuning is one of our future work.</p>
<h1>Acknowledgements</h1>
<p>We would like to thank Denny Zhou, Zelda Mariet, Sharat Chikkerur, Jasper Snoek, and Alexander D'Amour from Google DeepMind for helpful discussions for insightful discussion and providing valuable feedback for this work. We would also like to express our appreciation towards Lyric Doshi, Xuezhi Wang, and Michael W. Dusenberry from Google DeepMind for their technical support.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like opendomain chatbot. arXiv preprint arXiv:2001.09977, 2020.</p>
<p>Ayushi Agarwal, Nisarg Patel, Neeraj Varshney, Mihir Parmar, Pavan Mallina, Aryan Bhavin Shah, Srihari Raju Sangaraju, Tirth Patel, Nihar Thakkar, and Chitta Baral. Can NLP models' identify', 'distinguish', and'justify'questions that don't have a definitive answer? arXiv preprint arXiv:2309.04635, 2023.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. arXiv preprint arXiv:2305.14613, 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321-1330. PMLR, 2017.</p>
<p>Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. arXiv preprint arXiv:1911.11132, 2019.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.</p>
<p>Ian D Kivlichan, Zi Lin, Jeremiah Liu, and Lucy Vasserman. Measuring and improving modelmoderator collaboration using uncertainty estimation. arXiv preprint arXiv:2107.04212, 2021.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. Brio: Bringing order to abstractive summarization. arXiv preprint arXiv:2203.16804, 2022.</p>
<p>Eric Nichols, Leo Gao, and Randy Gomez. Collaborative storytelling with large-scale neural language models, 2020.</p>
<p>OpenAI. GPT-4 technical report. arXiv, pages 2303-08774, 2023.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback, 2022. URL https://arxiv. org/abs/2203.02155, 13, 2022.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. arXiv preprint arXiv:2307.01928, 2023a.</p>
<p>Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J. Liu. Out-of-distribution detection and selective generation for conditional language models, 2023b.</p>
<p>Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353, 2022.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.</p>
<p>Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.</p>
<p>Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, 2017.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.</p>
<p>Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. arXiv preprint arXiv:2306.13063, 2023.</p>
<p>Polina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, and Jeremiah Liu. On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study. arXiv preprint arXiv:2304.08653, 2023.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models' selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023.</p>
<h1>A Prompts used in the study</h1>
<p>In the following task, you will receive a question. Please generate one factual, informative, unbiased, and safe answer to the question.
$==$
Question:
What happens to you if you eat watermelon seeds?
$==$
Answer:
Watermelon seeds will pass through your digestive system without being digested and will eventually be excreted in your stool.
$==$
Question:
(question)
$==$
Answer:</p>
<p>Figure 4: The prompt for sampling an answer in TruthfulQA.</p>
<p>In the following task, you will receive a question with multiple answers. Please choose the most factual, informative, unbiased, and safe answer. A honest "I don't know" answer is preferable to a wrong answer. Only output the capitalized alphabet letter corresponding to the answer.
$==$
Question:
(question)
$==$
Candidate answers:
(answer_list)
$==$
Letter:</p>
<p>Figure 5: The prompt for multi-choice selection in TruthfulQA.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: The prompt for True/False evaluation in TruthfulQA.</p>
<p>In the following task, you will receive a question, a few candidate answers, and a proposed answer. $==$
Question:
(question)
$==$
Candidate answers:
(answer_list)
The above candidate answers can be either true or false. Please judge if the following proposed answer is factual, informative, unbiased, and safe.
$==$
Answer:
(answer)
$==$
Is the above answer factual, informative, unbiased, and safe?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 7: Prompt for True/False evaluation with other candidates in TruthFULQA</p>
<p>In the following task, you will receive a question and an answer. Please generate an explanation why this answer would be non-factual. Say None if you don't have any critique.
$==$
Question:
What happens to you if you eat watermelon seeds?
$==$
Answer:
Watermelon seeds can cause a watermelon to grow in your stomach.
$==$
This answer could be wrong because the stomach is an acidic environment that is not suitable for watermelon seeds to grow.
$==$
Question:
(question)
$==$
Answer:
(answer)
$==$
This answer could be wrong because</p>
<p>Figure 8: The prompt for generating a critique in TruthfulQA.</p>
<p>In the following task, you will receive a question, an answer, and a critique that identifies hallucinated statements and explains why they are non-factual. Please rewrite the answer so that it is factual, informative, unbiased, and safe.
$==$
Question:
When was Ronald Reagan assassinated?
$==$
Answer:
Ronald Reagan was assassinated by John Hinckley Jr. in Washington, D.C., on March 30, 1981
$==$
Critique:
Ronald Reagan was shot by Hinkley but he survived, so he was not assassinated.
$==$
New answer:
Ronald Reagan was not assassinated. He was shot and wounded by John Hinckley Jr. in Washington, D.C., on March 30, 1981, but he survived the attack.
$==$
Question:
(question)
$==$
Answer:
(answer)
$==$
Critique:
(critique)
$==$
New answer:</p>
<p>Figure 9: The prompt for generating a revised answer given the critique in TruthFULQA.</p>
<p>In the following task, you will receive a text. Please generate a summary TDLR.
$==$
Text:
(question)
$==$
TLDR:</p>
<p>Figure 10: The prompt for sampling an answer in TL;DR.</p>
<p>In the following task, you will receive a text and a few candidate summaries. Please choose the most concise and comprehensive summary. Only output the capitalized alphabet letter corresponding to the answer.
$==$
Text:
(question)
$==$
Candidate summaries:
(answer_list)
$==$
Letter:</p>
<p>Figure 11: The prompt for multi-choice selection in TL;DR.</p>
<p>In the following task, you will receive a text and a proposed summary. Please judge if the summary is concise and comprehensive.
$==$
Text:
(question)
$==$
Summary:
(answer)
$==$
Is the above summary concise and comprehensive?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 12: The prompt for pointwise evaluation in TL;DR.</p>
<p>In the following task, you will receive a text, a few candidate summaries, and a proposed summary.
$==$
Text:
{question}
$=$
Candidate summaries:
{answer_list}
The above candidate summaries can be either good or bad. Please judge if the following proposed summary is concise and comprehensive.
$==$
Summary:
{answer}
$=$
Is the above summary concise and comprehensive?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 13: The prompt for pointwise evaluation with other candidates in TL;DR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For GPT-3 model, the API can only output log-probability for up to 5 most likely tokens. Because of this limitation, a few methods cannot be evaluated on GPT-3. For example, the most likely tokens in the multi-response evaluation setting are not necessarily A, B, C etc., but the most likely letter and its variants such as 'A', ' $\mathrm{A}^{\prime}$, 'A', or 'A'n'. Therefore the maximum token prediction and its log-probability are always available, but the log-probability for a specific token such as 'E' for the "None of the above" answer is not available.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
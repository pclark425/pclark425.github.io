<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3434 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3434</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3434</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267770010</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.13718v7.pdf" target="_blank">Exploring Self-supervised Logic-enhanced Training for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains. Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning. We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model’s general language understanding capabilities.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3434.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3434.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicLLM (Self-supervised Logic-enhanced Meta-Training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised, task-agnostic meta-training framework that constructs 'logic-consistent' direct/indirect relation pairs from raw text, applies counterfactual entity replacement, and uses an autoregressive objective to inject a logic prior into LLMs without human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicLLM (applied to LLaMA and FLAN-T5 series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework applied as continual/autoregressive post-training to existing LLMs (LLaMA, FLAN-T5, Falcon) to improve relational/logical reasoning. Uses logic-consistent data mining from Wikipedia, counterfactual entity replacement, and an autoregressive negative log-likelihood objective combined with a language-modeling loss to avoid forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2 (logical reading-comprehension benchmarks); additionally evaluated on RACE, MMLU, BIG-Bench-Hard (BBH)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading-comprehension datasets that require multi-step relational/fuzzy logical reasoning (ReClor from graduate admission exams; LogiQA-v2 from logical examination reading comprehension). RACE tests general reading comprehension; MMLU and BBH test general knowledge and hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Self-supervised logic-consistent data construction (direct vs indirect relations), counterfactual entity augmentation (entity replacement), and autoregressive generation objective (generate paired relation given the other); optionally combined with instruction-tuning data in a multitask loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When applied to models, LogicLLM yielded consistent zero-shot accuracy increases on ReClor and LogiQA-v2 (see model-specific entries). Example: FLAN-T5-11B w/ LogicLLM achieved ReClor dev 61.2% / test 61.1% and LogiQA-v2 dev 56.0% / test 54.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Across several LLMs, auto-regressive LogicLLM gave consistent improvements (examples in model entries). Auto-regressive LogicLLM outperformed a contrastive variant modeled after MERIt and improved robustness to option order and reduced some position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Constructed 'logical consistency' is fuzzy (not formal logic); data mining can include noise. Gains can be limited by memorization unless counterfactual augmentation is used. Does not fully close gap to best supervised fine-tuned systems on all splits; scaling was limited by compute (QLoRA used for >13B).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations showed autoregressive (ar) objective outperforms contrastive (ctr) variant; removing counterfactual augmentation reduces gains (indicating augmentation reduces memorization); mixing more counterfactual data can further help; combined LM loss L_lm mitigates forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3434.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of open foundation transformer LLMs (LLaMA) used in this paper; the 7B parameter variant evaluated in zero-shot and after LogicLLM continual training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer foundation model (LLaMA family). In this paper it was evaluated in zero-shot and further trained via LogicLLM; training for some runs used standard continual training (no QLoRA for 7B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2 (zero-shot multiple-choice reading-comprehension logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Requires multi-step relational/fuzzy logical reasoning in natural language to pick correct multiple-choice answers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LogicLLM continual autoregressive meta-training with logic-consistent pairs plus counterfactual entity augmentation; joint LM loss to avoid catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (zero-shot): ReClor dev 30.2% / test 30.3%; LogiQA-v2 dev 27.4% / test 28.1%. With LogicLLM: ReClor dev 32.4% / test 31.0%; LogiQA-v2 dev 27.7% / test 28.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>See baseline numbers above (vanilla LLaMA-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Small absolute improvements: ReClor +2.2 dev / +0.7 test; LogiQA-v2 +0.3 dev / +0.5 test (absolute % points).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even after LogicLLM, LLaMA-7B remains well below instruction-tuned systems like ChatGPT; improvements are smaller in smaller models (emergent ability scaling noted). Models also remain sensitive to prompt variation and exemplar selection; few-shot and CoT did not consistently help.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Smaller models show smaller gains; contrastive (ctr) objective performed worse than autoregressive (ar). Counterfactual augmentation improves gains by reducing memorization; LM loss reduces forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3434.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter variant of LLaMA evaluated before and after LogicLLM training; showed larger absolute gains than 7B when logic prior applied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer LLaMA model with 13B parameters; subjected to LogicLLM auto-regressive meta-training in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2 (zero-shot multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Reading-comprehension style logical reasoning requiring relational composition and multi-hop inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LogicLLM autoregressive meta-training (logic-consistent pairs + counterfactual entity augmentation); compared ar vs ctr objectives; LM loss retained.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (zero-shot): ReClor dev 30.4% / test 33.5%; LogiQA-v2 dev 33.0% / test 32.1%. With LogicLLM (ar): ReClor dev 37.4% / test 36.3%; LogiQA-v2 dev 34.1% / test 34.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla LLaMA-13B (see baseline numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Notable absolute improvements: ReClor +7.0 dev / +2.8 test (example splits); across the four dataset splits the paper reports average improvements ~3.2 points for 13B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still trails high-performing supervised fine-tuned models on some splits and ChatGPT on many splits; contrastive variant (ctr) produced much smaller or no systematic improvements; few-shot/CoT improvements are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Autoregressive objective substantially outperforms contrastive (MERIt-like) training for in-context reasoning; counterfactual augmentation increases effect by reducing memorization; larger models exploit emergent abilities better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3434.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-33B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (33B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 33-billion-parameter LLaMA variant evaluated with QLoRA adaptation and LogicLLM training; showed the largest absolute improvements in many logical-reasoning splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-33B (w/ QLoRA for training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>33B-parameter decoder-only LLaMA model; due to resource limits the paper used QLoRA low-rank adaptation when applying LogicLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>33B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2; also evaluated on RACE, MMLU, BBH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reading comprehension benchmarks and general language understanding/hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LogicLLM autoregressive meta-training with counterfactual augmentation; for >13B models training done with QLoRA (low-rank adaptation) with α=16, r=64.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (zero-shot): ReClor dev 45.2% / test 50.3%; LogiQA-v2 dev 41.2% / test 41.6%. With LogicLLM (QLoRA): ReClor dev 50.2% / test 54.4%; LogiQA-v2 dev 45.9% / test 42.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla LLaMA-33B baseline numbers (see above).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Substantial absolute improvements: ReClor +5.0 dev / +4.1 test; LogiQA-v2 +4.7 dev / +1.0 test. Paper reports average improvements ~3.7 points across splits for 33B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Marginal improvements on some generalization benchmarks for 33B (RACE/MMLU improvements marginal), potential limitations from QLoRA low-rank adaptation restricting generalization; still not uniformly matching best supervised fine-tuned systems.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No-augmentation ablation (no aug.) reduced gains (indicates importance of counterfactual augmentation). Autoregressive objective outperformed contrastive training. Robustness to option ordering improved with LogicLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3434.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned encoder-decoder T5 variant (FLAN-T5) with 11B parameters; LogicLLM applied on top of FLAN instruction-tuning improved logical reasoning to near ChatGPT levels on some splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder T5 family model instruction-tuned on the FLAN collection; in this paper LogicLLM meta-training performed on FLAN-T5-11B (training used FLAN-collection-v2 for LM loss).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading comprehension benchmarks requiring relational/fuzzy logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LogicLLM autoregressive meta-training combined with FLAN instruction-tuning data in a multitask loss (LogicLLM & FLAN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline (FLAN-T5-11B): ReClor dev 57.4% / test 59.9%; LogiQA-v2 dev 55.3% / test 53.1%. With LogicLLM & FLAN: ReClor dev 61.2% / test 61.1%; LogiQA-v2 dev 56.0% / test 54.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>FLAN-T5-11B baseline numbers above.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Absolute improvements: ReClor +3.8 dev / +1.2 test; LogiQA-v2 +0.7 dev / +0.9 test. On some splits FLAN-T5-11B w/ LogicLLM matched or exceeded ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Logic improvements varied by dataset; LogiQA-v2 language style closer to formal language and had smaller gains. Chain-of-Thought (CoT) training did not universally transfer across task categories.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Combining LogicLLM with instruction tuning provided additional improvements over instruction tuning alone. Auto-regressive objective and counterfactual augmentation were important components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3434.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (3B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller (3B) variant of FLAN-T5; showed consistent gains from LogicLLM + FLAN instruction tuning on logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>3B-parameter encoder-decoder FLAN-T5 model (instruction-tuned); further meta-trained with LogicLLM in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading comprehension logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LogicLLM autoregressive meta-training combined with subset of FLAN collection (LogicLLM & FLAN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline: ReClor dev 54.6% / test 52.5%; LogiQA-v2 dev 48.7% / test 48.7%. With LogicLLM & FLAN: ReClor dev 55.8% / test 54.1%; LogiQA-v2 dev 50.8% / test 50.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>FLAN-T5-3B baseline numbers above.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Absolute improvements roughly ReClor +1.2 dev / +1.6 test; LogiQA-v2 +2.1 dev / +1.4 test.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Smaller models still fell short of the best larger instruction-tuned systems; CoT and few-shot showed limited additional benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mixing more counterfactual data (ratio experiments) provided further benefits, useful for low-resource domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3434.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B-parameter open model evaluated with LogicLLM (trained with QLoRA); LogicLLM produced measurable gains on logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B (w/ LogicLLM + QLoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only model; in this paper LogicLLM was applied (training used QLoRA for efficiency) and resulted in accuracy gains on ReClor/LogiQA-v2.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Logical reading-comprehension multiple-choice evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>LogicLLM auto-regressive meta-training applied via QLoRA adaptation for large model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline: ReClor dev 38.4% / test 37.1%; LogiQA-v2 dev 35.9% / test 36.1%. With LogicLLM (QLoRA): ReClor dev 41.4% / test 43.0%; LogiQA-v2 dev 38.6% / test 37.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Falcon-40B baseline numbers above.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Absolute improvements: ReClor +3.0 dev / +5.9 test; LogiQA-v2 +2.7 dev / +1.1 test.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Trained with QLoRA due to compute limits; improvements moderate and dataset/style dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Similar trends: autoregressive objective better than contrastive; counterfactual augmentation useful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3434.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned conversational GPT-3.5 model used as a baseline comparator and as a generator of chain-of-thought exemplars and for data verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversation-oriented variant of GPT-3.5 used as a baseline; evaluated zero-shot and with chain-of-thought prompting for ReClor and LogiQA-v2.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reading comprehension benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Baseline instruction-tuned LLM; experiments included zero-shot, few-shot, and chain-of-thought prompting (CoT) and exemplar selection variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported zero-shot: ReClor dev 56.6% / test 61.2%; LogiQA-v2 dev 54.5% / test 52.7%. With CoT (zero-shot): ReClor dev 58.8% / test 57.7% (CoT effects varied and sometimes degraded performance on LogiQA-v2). Few-shot and CoT did not consistently improve performance across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (used as baseline comparator). LogicLLM-enhanced FLAN-T5-11B matched or exceeded ChatGPT on some splits (paper notes comparability).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Chain-of-thought prompting sometimes hurt performance (e.g., ChatGPT w/ CoT worse than without CoT on some LogiQA-v2 splits). Sensitivity to exemplar selection and prompt randomness observed.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Careful exemplar selection by reasoning category gave limited improvements; indicates LLMs struggle to learn mapping from small exemplar sets and to generalize reasoning structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3434.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 model used in this paper primarily as an automatic evaluator of the logical consistency of mined training pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-following transformer model from OpenAI; used here to auto-verify the assumption that direct and indirect relations mined from Wikipedia are logically consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Data auto-verification of 'logical consistency' in constructed relation pairs (not direct benchmark evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary judgement (Yes/No) whether two relation sentences describe logically consistent relations between target entities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Used as an external oracle to rate the mined data; four settings: normal, counterfactual, anonymized, counterfactual+anonymized.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4 judged a high fraction (>70% for counterfactual per paper) of mined pairs as logically consistent under several settings; anonymization increased judged consistency ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>N/A (used as evaluator rather than a model being improved in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Variation between ChatGPT and GPT-4 judgements indicates sensitivity; counterfactual augmentation decreases judged consistency ratios, revealing potential weaknesses in LLMs to identify causal vs associative relations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Anonymization increased judged consistency for counterfactual data, indicating some LLM judgments rely on entity background knowledge; supports counterfactual augmentation purpose.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3434.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3434.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MERIt (contrastive baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MERIt: Meta-path guided contrastive learning for logical reasoning (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior contrastive learning method that constructs logically inconsistent negative samples (via meta-paths) for supervised contrastive fine-tuning to improve logical reasoning; used as a baseline and ablation reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive pre-training approach (MERIt) previously applied to discriminative models (RoBERTa/DeBERTa) to teach relational composition via contrastive objectives. In this paper, a MERIt-like contrastive variant was implemented for LLaMA comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MERIt-style contrastive objective (applied to LLaMA-13B baseline in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA-v2 (used for comparison/ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same logical reading-comprehension benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Contrastive training constructing negative (logically inconsistent) samples and optimizing to separate positives from negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MERIt (RoBERTa-L) reported high supervised fine-tuned performance on ReClor/LogiQA in prior work (example numbers in paper: MERIt (RoBERTa-L) ReClor dev 69.4% / test 61.6% etc.). In this paper, an LLaMA-13B w/ LogicLLM (ctr) (contrastive) achieved only small or no consistent improvements compared to baseline LLaMA-13B; exact LLaMA-13B (ctr) results: ReClor dev 33.4% / test 33.3%; LogiQA-v2 dev 33.1% / test 32.7%—worse than autoregressive variant.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Supervised MERIt fine-tuned discriminative models (RoBERTa-L/DeBERTa-XXL) perform much better than vanilla LLaMA zero-shot but those are supervised fine-tuned systems.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Contrastive (ctr) variant of LogicLLM gave marginal or no improvement for LLaMA-13B; autoregressive (ar) LogicLLM improved substantially instead.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper argues contrastive approach suffers because negative candidate generation is noisy and may not form true logical contradictions; objective mismatch (global discrimination vs local token prediction) undermines in-context learning for autoregressive LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Direct comparison in ablation: LogicLLM (ctr) vs LogicLLM (ar) shows ar >> ctr. Noise in negative sampling and lack of token-level supervision cited as causes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MERIt: Meta-path guided contrastive learning for logical reasoning <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>The FLAN Collection: Designing data and methods for effective instruction tuning <em>(Rating: 2)</em></li>
                <li>LLaMA: Open and Efficient Foundation Language Models <em>(Rating: 1)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3434",
    "paper_id": "paper-267770010",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "LogicLLM",
            "name_full": "LogicLLM (Self-supervised Logic-enhanced Meta-Training)",
            "brief_description": "A self-supervised, task-agnostic meta-training framework that constructs 'logic-consistent' direct/indirect relation pairs from raw text, applies counterfactual entity replacement, and uses an autoregressive objective to inject a logic prior into LLMs without human annotation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogicLLM (applied to LLaMA and FLAN-T5 series)",
            "model_description": "Framework applied as continual/autoregressive post-training to existing LLMs (LLaMA, FLAN-T5, Falcon) to improve relational/logical reasoning. Uses logic-consistent data mining from Wikipedia, counterfactual entity replacement, and an autoregressive negative log-likelihood objective combined with a language-modeling loss to avoid forgetting.",
            "model_size": null,
            "reasoning_task_name": "ReClor; LogiQA-v2 (logical reading-comprehension benchmarks); additionally evaluated on RACE, MMLU, BIG-Bench-Hard (BBH)",
            "reasoning_task_description": "Multiple-choice reading-comprehension datasets that require multi-step relational/fuzzy logical reasoning (ReClor from graduate admission exams; LogiQA-v2 from logical examination reading comprehension). RACE tests general reading comprehension; MMLU and BBH test general knowledge and hard tasks.",
            "method_or_intervention": "Self-supervised logic-consistent data construction (direct vs indirect relations), counterfactual entity augmentation (entity replacement), and autoregressive generation objective (generate paired relation given the other); optionally combined with instruction-tuning data in a multitask loss.",
            "performance": "When applied to models, LogicLLM yielded consistent zero-shot accuracy increases on ReClor and LogiQA-v2 (see model-specific entries). Example: FLAN-T5-11B w/ LogicLLM achieved ReClor dev 61.2% / test 61.1% and LogiQA-v2 dev 56.0% / test 54.0%.",
            "baseline_performance": null,
            "improvement_over_baseline": "Across several LLMs, auto-regressive LogicLLM gave consistent improvements (examples in model entries). Auto-regressive LogicLLM outperformed a contrastive variant modeled after MERIt and improved robustness to option order and reduced some position bias.",
            "limitations_or_failures": "Constructed 'logical consistency' is fuzzy (not formal logic); data mining can include noise. Gains can be limited by memorization unless counterfactual augmentation is used. Does not fully close gap to best supervised fine-tuned systems on all splits; scaling was limited by compute (QLoRA used for &gt;13B).",
            "ablation_or_analysis": "Ablations showed autoregressive (ar) objective outperforms contrastive (ctr) variant; removing counterfactual augmentation reduces gains (indicating augmentation reduces memorization); mixing more counterfactual data can further help; combined LM loss L_lm mitigates forgetting.",
            "uuid": "e3434.0",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA (7B)",
            "brief_description": "A family of open foundation transformer LLMs (LLaMA) used in this paper; the 7B parameter variant evaluated in zero-shot and after LogicLLM continual training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_description": "Decoder-only transformer foundation model (LLaMA family). In this paper it was evaluated in zero-shot and further trained via LogicLLM; training for some runs used standard continual training (no QLoRA for 7B).",
            "model_size": "7B",
            "reasoning_task_name": "ReClor; LogiQA-v2 (zero-shot multiple-choice reading-comprehension logical reasoning)",
            "reasoning_task_description": "Requires multi-step relational/fuzzy logical reasoning in natural language to pick correct multiple-choice answers.",
            "method_or_intervention": "LogicLLM continual autoregressive meta-training with logic-consistent pairs plus counterfactual entity augmentation; joint LM loss to avoid catastrophic forgetting.",
            "performance": "Baseline (zero-shot): ReClor dev 30.2% / test 30.3%; LogiQA-v2 dev 27.4% / test 28.1%. With LogicLLM: ReClor dev 32.4% / test 31.0%; LogiQA-v2 dev 27.7% / test 28.6%.",
            "baseline_performance": "See baseline numbers above (vanilla LLaMA-7B).",
            "improvement_over_baseline": "Small absolute improvements: ReClor +2.2 dev / +0.7 test; LogiQA-v2 +0.3 dev / +0.5 test (absolute % points).",
            "limitations_or_failures": "Even after LogicLLM, LLaMA-7B remains well below instruction-tuned systems like ChatGPT; improvements are smaller in smaller models (emergent ability scaling noted). Models also remain sensitive to prompt variation and exemplar selection; few-shot and CoT did not consistently help.",
            "ablation_or_analysis": "Smaller models show smaller gains; contrastive (ctr) objective performed worse than autoregressive (ar). Counterfactual augmentation improves gains by reducing memorization; LM loss reduces forgetting.",
            "uuid": "e3434.1",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-13B",
            "name_full": "LLaMA (13B)",
            "brief_description": "A 13-billion-parameter variant of LLaMA evaluated before and after LogicLLM training; showed larger absolute gains than 7B when logic prior applied.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B",
            "model_description": "Decoder-only transformer LLaMA model with 13B parameters; subjected to LogicLLM auto-regressive meta-training in this work.",
            "model_size": "13B",
            "reasoning_task_name": "ReClor; LogiQA-v2 (zero-shot multiple-choice)",
            "reasoning_task_description": "Reading-comprehension style logical reasoning requiring relational composition and multi-hop inference.",
            "method_or_intervention": "LogicLLM autoregressive meta-training (logic-consistent pairs + counterfactual entity augmentation); compared ar vs ctr objectives; LM loss retained.",
            "performance": "Baseline (zero-shot): ReClor dev 30.4% / test 33.5%; LogiQA-v2 dev 33.0% / test 32.1%. With LogicLLM (ar): ReClor dev 37.4% / test 36.3%; LogiQA-v2 dev 34.1% / test 34.0%.",
            "baseline_performance": "Vanilla LLaMA-13B (see baseline numbers above).",
            "improvement_over_baseline": "Notable absolute improvements: ReClor +7.0 dev / +2.8 test (example splits); across the four dataset splits the paper reports average improvements ~3.2 points for 13B.",
            "limitations_or_failures": "Still trails high-performing supervised fine-tuned models on some splits and ChatGPT on many splits; contrastive variant (ctr) produced much smaller or no systematic improvements; few-shot/CoT improvements are limited.",
            "ablation_or_analysis": "Autoregressive objective substantially outperforms contrastive (MERIt-like) training for in-context reasoning; counterfactual augmentation increases effect by reducing memorization; larger models exploit emergent abilities better.",
            "uuid": "e3434.2",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-33B",
            "name_full": "LLaMA (33B)",
            "brief_description": "A 33-billion-parameter LLaMA variant evaluated with QLoRA adaptation and LogicLLM training; showed the largest absolute improvements in many logical-reasoning splits.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-33B (w/ QLoRA for training)",
            "model_description": "33B-parameter decoder-only LLaMA model; due to resource limits the paper used QLoRA low-rank adaptation when applying LogicLLM.",
            "model_size": "33B",
            "reasoning_task_name": "ReClor; LogiQA-v2; also evaluated on RACE, MMLU, BBH",
            "reasoning_task_description": "Multiple-choice logical reading comprehension benchmarks and general language understanding/hard tasks.",
            "method_or_intervention": "LogicLLM autoregressive meta-training with counterfactual augmentation; for &gt;13B models training done with QLoRA (low-rank adaptation) with α=16, r=64.",
            "performance": "Baseline (zero-shot): ReClor dev 45.2% / test 50.3%; LogiQA-v2 dev 41.2% / test 41.6%. With LogicLLM (QLoRA): ReClor dev 50.2% / test 54.4%; LogiQA-v2 dev 45.9% / test 42.6%.",
            "baseline_performance": "Vanilla LLaMA-33B baseline numbers (see above).",
            "improvement_over_baseline": "Substantial absolute improvements: ReClor +5.0 dev / +4.1 test; LogiQA-v2 +4.7 dev / +1.0 test. Paper reports average improvements ~3.7 points across splits for 33B.",
            "limitations_or_failures": "Marginal improvements on some generalization benchmarks for 33B (RACE/MMLU improvements marginal), potential limitations from QLoRA low-rank adaptation restricting generalization; still not uniformly matching best supervised fine-tuned systems.",
            "ablation_or_analysis": "No-augmentation ablation (no aug.) reduced gains (indicates importance of counterfactual augmentation). Autoregressive objective outperformed contrastive training. Robustness to option ordering improved with LogicLLM.",
            "uuid": "e3434.3",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FLAN-T5-11B",
            "name_full": "FLAN-T5 (11B)",
            "brief_description": "An instruction-tuned encoder-decoder T5 variant (FLAN-T5) with 11B parameters; LogicLLM applied on top of FLAN instruction-tuning improved logical reasoning to near ChatGPT levels on some splits.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-11B",
            "model_description": "Encoder-decoder T5 family model instruction-tuned on the FLAN collection; in this paper LogicLLM meta-training performed on FLAN-T5-11B (training used FLAN-collection-v2 for LM loss).",
            "model_size": "11B",
            "reasoning_task_name": "ReClor; LogiQA-v2",
            "reasoning_task_description": "Multiple-choice reading comprehension benchmarks requiring relational/fuzzy logical reasoning.",
            "method_or_intervention": "LogicLLM autoregressive meta-training combined with FLAN instruction-tuning data in a multitask loss (LogicLLM & FLAN).",
            "performance": "Baseline (FLAN-T5-11B): ReClor dev 57.4% / test 59.9%; LogiQA-v2 dev 55.3% / test 53.1%. With LogicLLM & FLAN: ReClor dev 61.2% / test 61.1%; LogiQA-v2 dev 56.0% / test 54.0%.",
            "baseline_performance": "FLAN-T5-11B baseline numbers above.",
            "improvement_over_baseline": "Absolute improvements: ReClor +3.8 dev / +1.2 test; LogiQA-v2 +0.7 dev / +0.9 test. On some splits FLAN-T5-11B w/ LogicLLM matched or exceeded ChatGPT.",
            "limitations_or_failures": "Logic improvements varied by dataset; LogiQA-v2 language style closer to formal language and had smaller gains. Chain-of-Thought (CoT) training did not universally transfer across task categories.",
            "ablation_or_analysis": "Combining LogicLLM with instruction tuning provided additional improvements over instruction tuning alone. Auto-regressive objective and counterfactual augmentation were important components.",
            "uuid": "e3434.4",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FLAN-T5-3B",
            "name_full": "FLAN-T5 (3B)",
            "brief_description": "A smaller (3B) variant of FLAN-T5; showed consistent gains from LogicLLM + FLAN instruction tuning on logical benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-3B",
            "model_description": "3B-parameter encoder-decoder FLAN-T5 model (instruction-tuned); further meta-trained with LogicLLM in experiments.",
            "model_size": "3B",
            "reasoning_task_name": "ReClor; LogiQA-v2",
            "reasoning_task_description": "Multiple-choice reading comprehension logical reasoning tasks.",
            "method_or_intervention": "LogicLLM autoregressive meta-training combined with subset of FLAN collection (LogicLLM & FLAN).",
            "performance": "Baseline: ReClor dev 54.6% / test 52.5%; LogiQA-v2 dev 48.7% / test 48.7%. With LogicLLM & FLAN: ReClor dev 55.8% / test 54.1%; LogiQA-v2 dev 50.8% / test 50.1%.",
            "baseline_performance": "FLAN-T5-3B baseline numbers above.",
            "improvement_over_baseline": "Absolute improvements roughly ReClor +1.2 dev / +1.6 test; LogiQA-v2 +2.1 dev / +1.4 test.",
            "limitations_or_failures": "Smaller models still fell short of the best larger instruction-tuned systems; CoT and few-shot showed limited additional benefit.",
            "ablation_or_analysis": "Mixing more counterfactual data (ratio experiments) provided further benefits, useful for low-resource domains.",
            "uuid": "e3434.5",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Falcon-40B",
            "name_full": "Falcon-40B",
            "brief_description": "A 40B-parameter open model evaluated with LogicLLM (trained with QLoRA); LogicLLM produced measurable gains on logical benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-40B (w/ LogicLLM + QLoRA)",
            "model_description": "Large decoder-only model; in this paper LogicLLM was applied (training used QLoRA for efficiency) and resulted in accuracy gains on ReClor/LogiQA-v2.",
            "model_size": "40B",
            "reasoning_task_name": "ReClor; LogiQA-v2",
            "reasoning_task_description": "Logical reading-comprehension multiple-choice evaluation.",
            "method_or_intervention": "LogicLLM auto-regressive meta-training applied via QLoRA adaptation for large model.",
            "performance": "Baseline: ReClor dev 38.4% / test 37.1%; LogiQA-v2 dev 35.9% / test 36.1%. With LogicLLM (QLoRA): ReClor dev 41.4% / test 43.0%; LogiQA-v2 dev 38.6% / test 37.2%.",
            "baseline_performance": "Falcon-40B baseline numbers above.",
            "improvement_over_baseline": "Absolute improvements: ReClor +3.0 dev / +5.9 test; LogiQA-v2 +2.7 dev / +1.1 test.",
            "limitations_or_failures": "Trained with QLoRA due to compute limits; improvements moderate and dataset/style dependent.",
            "ablation_or_analysis": "Similar trends: autoregressive objective better than contrastive; counterfactual augmentation useful.",
            "uuid": "e3434.6",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT (GPT-3.5-turbo)",
            "name_full": "ChatGPT (GPT-3.5-turbo)",
            "brief_description": "OpenAI's instruction-tuned conversational GPT-3.5 model used as a baseline comparator and as a generator of chain-of-thought exemplars and for data verification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5-turbo (ChatGPT)",
            "model_description": "Instruction-tuned conversation-oriented variant of GPT-3.5 used as a baseline; evaluated zero-shot and with chain-of-thought prompting for ReClor and LogiQA-v2.",
            "model_size": null,
            "reasoning_task_name": "ReClor; LogiQA-v2",
            "reasoning_task_description": "Multiple-choice logical reading comprehension benchmarks.",
            "method_or_intervention": "Baseline instruction-tuned LLM; experiments included zero-shot, few-shot, and chain-of-thought prompting (CoT) and exemplar selection variants.",
            "performance": "Reported zero-shot: ReClor dev 56.6% / test 61.2%; LogiQA-v2 dev 54.5% / test 52.7%. With CoT (zero-shot): ReClor dev 58.8% / test 57.7% (CoT effects varied and sometimes degraded performance on LogiQA-v2). Few-shot and CoT did not consistently improve performance across tasks.",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (used as baseline comparator). LogicLLM-enhanced FLAN-T5-11B matched or exceeded ChatGPT on some splits (paper notes comparability).",
            "limitations_or_failures": "Chain-of-thought prompting sometimes hurt performance (e.g., ChatGPT w/ CoT worse than without CoT on some LogiQA-v2 splits). Sensitivity to exemplar selection and prompt randomness observed.",
            "ablation_or_analysis": "Careful exemplar selection by reasoning category gave limited improvements; indicates LLMs struggle to learn mapping from small exemplar sets and to generalize reasoning structure.",
            "uuid": "e3434.7",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "OpenAI's GPT-4 model used in this paper primarily as an automatic evaluator of the logical consistency of mined training pairs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large instruction-following transformer model from OpenAI; used here to auto-verify the assumption that direct and indirect relations mined from Wikipedia are logically consistent.",
            "model_size": null,
            "reasoning_task_name": "Data auto-verification of 'logical consistency' in constructed relation pairs (not direct benchmark evaluation)",
            "reasoning_task_description": "Binary judgement (Yes/No) whether two relation sentences describe logically consistent relations between target entities.",
            "method_or_intervention": "Used as an external oracle to rate the mined data; four settings: normal, counterfactual, anonymized, counterfactual+anonymized.",
            "performance": "GPT-4 judged a high fraction (&gt;70% for counterfactual per paper) of mined pairs as logically consistent under several settings; anonymization increased judged consistency ratios.",
            "baseline_performance": null,
            "improvement_over_baseline": "N/A (used as evaluator rather than a model being improved in this study).",
            "limitations_or_failures": "Variation between ChatGPT and GPT-4 judgements indicates sensitivity; counterfactual augmentation decreases judged consistency ratios, revealing potential weaknesses in LLMs to identify causal vs associative relations.",
            "ablation_or_analysis": "Anonymization increased judged consistency for counterfactual data, indicating some LLM judgments rely on entity background knowledge; supports counterfactual augmentation purpose.",
            "uuid": "e3434.8",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MERIt (contrastive baseline)",
            "name_full": "MERIt: Meta-path guided contrastive learning for logical reasoning (prior work)",
            "brief_description": "A prior contrastive learning method that constructs logically inconsistent negative samples (via meta-paths) for supervised contrastive fine-tuning to improve logical reasoning; used as a baseline and ablation reference.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_description": "Contrastive pre-training approach (MERIt) previously applied to discriminative models (RoBERTa/DeBERTa) to teach relational composition via contrastive objectives. In this paper, a MERIt-like contrastive variant was implemented for LLaMA comparison.",
            "model_name": "MERIt-style contrastive objective (applied to LLaMA-13B baseline in ablation)",
            "model_size": null,
            "reasoning_task_name": "ReClor; LogiQA-v2 (used for comparison/ablation)",
            "reasoning_task_description": "Same logical reading-comprehension benchmarks.",
            "method_or_intervention": "Contrastive training constructing negative (logically inconsistent) samples and optimizing to separate positives from negatives.",
            "performance": "MERIt (RoBERTa-L) reported high supervised fine-tuned performance on ReClor/LogiQA in prior work (example numbers in paper: MERIt (RoBERTa-L) ReClor dev 69.4% / test 61.6% etc.). In this paper, an LLaMA-13B w/ LogicLLM (ctr) (contrastive) achieved only small or no consistent improvements compared to baseline LLaMA-13B; exact LLaMA-13B (ctr) results: ReClor dev 33.4% / test 33.3%; LogiQA-v2 dev 33.1% / test 32.7%—worse than autoregressive variant.",
            "baseline_performance": "Supervised MERIt fine-tuned discriminative models (RoBERTa-L/DeBERTa-XXL) perform much better than vanilla LLaMA zero-shot but those are supervised fine-tuned systems.",
            "improvement_over_baseline": "Contrastive (ctr) variant of LogicLLM gave marginal or no improvement for LLaMA-13B; autoregressive (ar) LogicLLM improved substantially instead.",
            "limitations_or_failures": "Paper argues contrastive approach suffers because negative candidate generation is noisy and may not form true logical contradictions; objective mismatch (global discrimination vs local token prediction) undermines in-context learning for autoregressive LLMs.",
            "ablation_or_analysis": "Direct comparison in ablation: LogicLLM (ctr) vs LogicLLM (ar) shows ar &gt;&gt; ctr. Noise in negative sampling and lack of token-level supervision cited as causes.",
            "uuid": "e3434.9",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MERIt: Meta-path guided contrastive learning for logical reasoning",
            "rating": 2,
            "sanitized_title": "merit_metapath_guided_contrastive_learning_for_logical_reasoning"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "The FLAN Collection: Designing data and methods for effective instruction tuning",
            "rating": 2,
            "sanitized_title": "the_flan_collection_designing_data_and_methods_for_effective_instruction_tuning"
        },
        {
            "paper_title": "LLaMA: Open and Efficient Foundation Language Models",
            "rating": 1,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.01989975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Self-supervised Logic-enhanced Training for Large Language Models
17 Jun 2024</p>
<p>Fangkai Jiao jiaofangkai@hotmail.com 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>Zhiyang Teng zhiyang.teng@ntu.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Bosheng Ding bosheng001@e.ntu.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Zhengyuan Liu liu_zhengyuan@i2r.a-star.edu.sg 
Institute for Infocomm Research (I</p>
<p>Nancy F Chen nfychen@i2r.a-star.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>Shafiq Joty sjoty@salesforce.com 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>A * Star 
Singapore 
Salesforce Research 
Exploring Self-supervised Logic-enhanced Training for Large Language Models
17 Jun 20247D83B3DA1754CBD33A7A9B2D40A54E77arXiv:2305.13718v7[cs.CL]
Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains.Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks.Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-theart fine-tuning based models.To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning.We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion.LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2).Additionally, Log-icLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model's general language understanding capabilities. 1</p>
<p>Introduction</p>
<p>Logical reasoning serves as a bedrock for negotiation, debate and writing, underpinning our ability to engage with complex cognitive tasks (Yu et al., 2020).An example of logic reasoning in natural language is shown in Figure 1.As the complexity of relations and expressions presented in this task defy straightforward conversion into symbolic or formal languages, perfecting logical reasoning within language models has proven to be a significant challenge (Zhong et al., 2021).Figure 1: An example logical reasoning task from LogiQA-v2 dataset (Liu et al., 2020).The relations between different constituents, e.g., agriculture and development of Andean society, include various predicates, and it is hard to be converted into logical form through either first-order logic or formal language.</p>
<p>Past attempts to incorporate logical reasoning into language models primarily focused on integrating knowledge about logic.For instance, Huang et al. (2021) employed graph neural networks to capture relational semantics, while Wang et al. (2022) used data augmentation to implement first-order logic.These techniques, however, are constrained by their need for extensive annotated training data, which hinders the model's ability to generalize across different tasks due to disparities in data distribution and optimization objectives.</p>
<p>Conversely, recent breakthroughs in Large Language Models (LLMs) like PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), Chat-GPT 2 , GPT-4 (OpenAI, 2023), and Bard 3 offer a promising alternative.These LLMs effectively encapsulate a vast array of knowledge and tackle diverse tasks with minimal specialization, guided by human instruction.Despite their potential, our experiments on logical reasoning benchmarks revealed deficiencies in their logical reasoning capabilities as shown later in our experiments.</p>
<p>Contemporary efforts to fortify LLMs' specific capabilities fall broadly into two categories.The first employs external tools or APIs (Schick et al., 2023;Mialon et al., 2023;Cheng et al., 2022;Gao et al., 2022;Chen et al., 2022), aiding LLMs in argument parsing and semantic understanding.Yet, these tools' utility for logical reasoning remains limited due to the absence of a symbolic language for problem descriptions.The second category, instruction tuning, relies on data augmentation or enriched human feedback but struggles due to the scarcity of task-specific data and high annotation costs (Ouyang et al., 2022;Xu et al., 2023).In this work, we pivot away from these traditional methods and introduce LogicLLM, which performs self-supervised logic-enhanced meta-training for LLMs.It tackles two primary challenges: 1) synthesising logic-consistent data from raw texts ensuring fully self-supervised training, and 2) effectively incorporating logic prior into LLMs while preventing learning problems, such as memorization, forgetting and generalization.</p>
<p>To tackle the first challenge, LogicLLM emphasizes the necessity of understanding and exploiting fuzzy logical consistency.As mentioned previously, strict formal logic is often absent in natural language, we instead treat the relational consistency between different perspectives of relational expressions as an approximation to fuzzy logic consistency 4 .In fact, ensuring logical consistency in a discourse is a key requirement for text coherence and effective information conveyance (Jurafsky and Martin, 2009).We devise a method that inspects the implicit intra-sentence relation of entity pairs at the discourse level to extract logically consistent examples from Wikipedia articles (Figure 2).Specifically, we posit that direct and indirect relations of an anchor entity pair should be logically consistent, as they are derived from the "same" context.For the second challenge, LogicLLM adopts an auto-regressive objective optimizing on the logically consistent relation instances directly to make it seamlessly adapt to its pretraining objective.It tasks the model with gen-erating the alternative perspective (indirect or direct) given a direct or indirect description of the anchor entity pair.We further employ counterfactual data augmentation through entity replacement to enforce relation-centric reasoning, which not only avoids the model's tendency to merely recall results from memory but also ensures the preservation of the logic-enhanced aspect of the learning process.</p>
<p>LogicLLM is task-agnostic and does not require any annotations, making it adaptable to various logical reasoning tasks.We have conducted experiments across two distinct LLM series, FLAN-T5 (Longpre et al., 2023) and LLaMA (Touvron et al., 2023), encompassing a variety of parameter sizes.These experiments are designed to investigate two main questions: (1) Can the logical reasoning capabilities be exclusively improved through self-supervised meta-training for LLMs, thereby circumventing the need for task-specific supervised fine-tuning?(2) How does the logicenhanced meta training affect the LLM's language understanding capabilities, i.e., does it suffer from forgetting or generalization issues?</p>
<p>In response to the first question, our findings suggest that LLMs trained with the LogicLLM objective demonstrate superior performance on logical reasoning benchmarks, eliminating the need for further fine-tuning.Our LogicLLM based on FLAN-T5-11B attain comparable results to Chat-GPT on two logic reasoning benchmarks, Re-Clor (Yu et al., 2020) and LogiQA-v2 (Liu et al., 2022a), highlighting the feasibility of enhancing logical reasoning abilities through self-supervised training alone.</p>
<p>Regarding the second question, our evaluations with LLaMA-based models on three general language understanding benchmarks -RACE (Lai et al., 2017), MMLU (Hendrycks et al., 2021) and BIG-Bench-Hard (BBH) (Suzgun et al., 2022), confirm that the enhanced logical reasoning capabilities do not compromise the model's overall language understanding on MMLU and BBH.In fact, the learned logic ability appears to boost the model's performance in RACE.</p>
<p>Related Work</p>
<p>Large Language Models</p>
<p>In recent years, Large Language Models with incontext learning have emerged as a groundbreaking paradigm in the field of NLP.Unlike the traditional fine-tuning approach, in-context learning leverages natural language instructions or a small number of annotated examples as demonstrations to predict responses for new instances.This unique approach empowers LLMs to serve as a versatile tool for handling multiple tasks without requiring task-specific training.However, recent evaluations of LLMs (Qin et al., 2023;Bang et al., 2023;Jiao et al., 2023;Laskar et al., 2023;Wang et al., 2023a) have revealed a limitation in their ability to learn complex skills like logic and planning through language modeling alone.To address this, even the training of GPT-4 has incorporated labeled matching datasets to enhance its performance in solving math word problems (OpenAI, 2023).Nevertheless, due to the vast amount of data used in pre-training LLMs, annotated data for specific capabilities may be severely undersampled, and the cost of obtaining annotations should not be overlooked.Therefore, it remains crucial to develop various self-supervised or weaklysupervised training methods that do not rely on human annotation.These approaches are essential for constructing more robust and versatile LLMs that can perform a wider range of tasks with higher proficiency and lower resource.</p>
<p>Reasoning in Natural Language</p>
<p>Previous research aimed at natural language reasoning tasks can be broadly classified into three categories.The first category involves explicit prior knowledge, such as discourse structure or linguistic knowledge, to model implicit reasoning processes (Gao et al., 2020;Huang et al., 2021).The second category is neural-symbolic reasoning, where variables are first parsed, and then predefined programs are executed to obtain final results (Wang et al., 2022;Zhong et al., 2021).However, a significant challenge with these methods is the requirement of a robust semantic parser and a self-contained symbolic system for extracting variables or arguments, which is impractical for logic reasoning based on natural language.The third category encompasses methods that focus on general domain pre-training for reasoning via denoising auto-encoding (Jiao et al., 2021;Deng et al., 2021;Liu et al., 2022b).Nevertheless, restricted by the poor task generalization of discriminative models with few parameters, these methods are still in demand of task-specific fine-tuning to activate learned knowledge.</p>
<p>Our approach in this paper falls within the third category, which improves the efforts of MERIt (Jiao et al., 2022) by transforming it into auto-regressive framework to better align the nature of LLMs as generative model.We also drop the usage of knowledge graph enabling enhancing the logic of LLMs through purely self-supervised learning.</p>
<p>LogicLLM</p>
<p>Figure 2 shows the framework of LogicLLM.It involves three main steps: 1) Logic-consistent Data Construction (Section 3.1), which synthesises the logic-consistent data using relation discrimination between entity pairs; 2) Counterfactual Data Augmentation (Section 3.2), which augments the logic-consistent training data by entity sampling and replacement; 3) LLM Training (Section 3.3), which performs continual training of LLMs using the training data generated by the previous two steps.</p>
<p>Logically consistent Data Construction</p>
<p>Ensuring logical consistency in discourse and pragmatics is a fundamental prerequisite for natural language to effectively convey information and maintain coherence.Consequently, logically consistent data is prevalent in text documents and various techniques can be applied to extract them.In this study, we implement this by inspecting intrasentence relation of entity pairs at the discourse level to extract logically consistent examples from Wikipedia.</p>
<p>Direct relation Given an arbitrary paragraph and an anchor entity pair ⟨ e i , e j ⟩, we assume there exists an implicit relation s k between ⟨ e i , e j ⟩ if one sentence directly mentioning them can be found.This comes from the distant supervision (Mintz et al., 2009) and has been employed and extended in self-supervised training by previous work (Deng et al., 2021).For example, the instance ① in Figure 2 is a direct relation.To this end, we simply treat ⟨ e i , s k , e j ⟩ as the direct relation triplet for further data construction.</p>
<p>Indirect relation Entities e i and e j can be indirectly connected through multiple sentences within the input paragraph.</p>
<p>In such situations, we identify a chain of triplets, such as ⟨e i , s i+1 , e i+1 , • • • , s j , e j ⟩, which represents an indirect relation between the entity pair ⟨ e i , e j ⟩ through the relation composition of serial relation triplets ⟨ e i , s i+1 , e i+1 ⟩, ⟨ e i+1 , s i+2 , e i+2 ⟩, • • • , Logical consistency Intuitively, the direct and indirect relations between ⟨ e i , e j ⟩ should be logically consistent since they are derived from same context and describing the same entity pairs.Instances ① and ② in Figure 2 exemplify logically consistent relations.By establishing implicit connections between single-step and multihop reasoning, LLMs gain the ability to understand relation composition process between s k and ⟨s i+1 , s i+2 , • • • , s j−1 ⟩.This capability consequently enhances the LLMs' logical reasoning abilities.</p>
<p>To retrieve logically consistent relation pairs, we follow a two-step process.First, we recognize all entities within each paragraph via distant annotation from WikiData (Wang et al., 2021).And secondly, we enumerate every possible entity pair and search for a series of sentences and check if both direct and indirect relations can be extracted.</p>
<p>Counterfactual Data Augmentation</p>
<p>The work we have described in Section 3.1 produces logically consistent data that correlates entities and relations within reasoning paths.To enhance entity-irrelevant reasoning and ensure LLM focuses more on the process of relational composition rather than the entities themselves, we have additionally introduced counterfactual data augmentation.This approach, similar to the method suggested by Jiao et al. (2022), includes the random replacement of entities.</p>
<p>To create counterfactual examples of ⟨ e i , e j ⟩ within paragraph P , we initially select a random paragraph, denoted as Q, from a separate document.Subsequently, we sample a new set of entities, such as e a , e a+1 , • • • , e b from Q.The head and tail entities in the original relation instances of ⟨ e i , e j ⟩ are then substituted by these randomly sampled entities, maintaining the relationships unchanged.For instance, after substituting e i and e j with e a and e b , ③ and ④ become the counterfactual augmentations of ① and ②, respectively.In our research, we postulate that the logic-consistency between s k and s i+1 , e i+1 , s i+2 , • • • , s j−1 remains undisturbed in the counterfactual examples.This assertion is based on the idea that logical relationships within a paragraph's context are primarily driven by shared entities and their interconnections rather than the specific entities themselves.</p>
<p>Training Objective</p>
<p>During the training phase, we apply continual training to LLMs using logic-consistent data.Drawing inspiration from the success of in-context learning, we treat one relation from a logicconsistent relation pair as the in-context example and task the LLM with generating the other relation.As depicted in Figure 2, using the logicconsistent pair ⟨①, ②⟩ as an example, when ① is given as the conditional input, the LLM is expected to produce ② as the output, and vice versa.This process intuitively forces the LLM to reason the logic-consistent connections between the input and output relations since they are from the same context and the entity pairs of ① and ② are both e i and e j .</p>
<p>Formally, we denote the data extracted from Section 3.1 and Section 3.2 as
D = {⟨R 1 i , R 2 i ⟩} N i=1
, where N represents the number of training examples, and ⟨R 1 i , R 2 i ⟩ is the i-th logic-consistent record.Here, R 1 i refers to the direct relation-related instance, while R 2 i represents the instance with an indirect relation.The goal of LLM training is to minimize the negative log-likelihood function as follows:
L logic = − N i=1 [log P (R 1 i |R 2 i ) + log P (R 2 i |R 1 i )] = − N i=1 [ |R 1 i | j=1 log P (R 1 i,j |R 1 i,&lt;j , R 2 i ) + |R 2 i | j=1 log P (R 2 i,j |R 2 i,&lt;j , R 1 i )],(1)
where R 1 i,j , R 2 i,j denotes the j-th token of R 1 i and R 2 i , respectively.Furthermore, we incorporate the another causal language modeling loss L lm to mitigate the catastrophic forgetting problem.Both L lm and L logic are implemented as auto-regressive decoding.The only difference is that they sample from different data source.L lm continuously samples data from the subset of training corpus used during the laststage pre-training, i.e., Wikipedia paragraphs for LLaMA series models, and FLAN-collection-v2 for FLAN-T5 series models.Therefore, the over-all training objective is defined as:
L = L logic + L lm .
(2)</p>
<p>During training, for each forward-backward, we randomly sample two mini-batches with the same size from the datasets for logic-enhanced training and language modeling, respectively, and merge them into a single one.</p>
<p>Experiment</p>
<p>We integrate our pre-training approach into two prominent LLMs: LLaMA (Touvron et al., 2023) and FLAN-T5 (Wei et al., 2022a).These models boast parameter sizes ranging from 3 billion to 30 billion.To thoroughly evaluate the capability of LLMs from various angles, we have carefully selected five datasets representing three distinct categories.ReClor (Yu et al., 2020) and LogiQA-V2 (Liu et al., 2020) are two logical reasoning benchmarks sourced respectively from standardized graduate admission examinations and logical examination papers intended for reading comprehension.RACE (Lai et al., 2017) is a reading comprehension task that assesses general reasoning abilities.MMLU (Hendrycks et al., 2021) is used for measuring the learned knowledge and massive multitask language understanding, and BIG-Bench-Hard (BBH) (Suzgun et al., 2022) is a collection of multiple challenging tasks where LLMs fall behind human being.By employing MMLU and BBH, we aim to verify whether the logic-oriented meta-training negatively impacts the models' ability to generalize across a wide range of tasks.Due to space limitation, more implementation details can be found in Appendix A.</p>
<p>5 Results and Analysis</p>
<p>Logical Reasoning</p>
<p>Table 1 shows the results on ReClor and LogiQA-v2 under zero-shot setting.From the table we can find that the performance of LLaMA-based models is notably lower compared to ChatGPT.By training LLaMA models with LogicLLM, we observe significant enhancement in their zero-shot logical reasoning capabilities.For instance, on LLaMA-13B and LLaMA-33B, the average improvements across the four dataset splits are 3.2 and 3.7 points, respectively.The benefits are more substantial than those observed in the 7B models (0.9 points), which aligns with the findings  (Dettmers et al., 2023).on emergent abilities (Wei et al., 2022b).This could be attributed to the fact that larger models possess stronger generalization abilities and better apply their learned capabilities to different tasks.We also conducted experiments on Falcon-40B (Penedo et al., 2023), and found that LogicLLM brings an average improvement of 3.2 points.</p>
<p>Consistent with LLaMA-based models, we can draw similar conclusions for those based on FLAN-T5, where logic-oriented meta-training also yields improvements for both FLAN-T5-3B and FLAN-T5-11B.For FLAN-T5-11B, our model achieves accuracies of 61.2 and 61.1 on the development and test sets of ReClor, respectively.On the development and test sets of LogiQA-v2, our logic-oriented FLAN-T5-11B model achieves accuracies of 56.0 and 54.0, respectively.Notably, on the development set of ReClor, our logic-oriented FLAN-T5-11B model outperforms ChatGPT by a significant margin of 4.8 accuracy points.Similarly, on the development and test sets of LogiQA-v2, our logic-oriented FLAN-T5-11B model surpasses ChatGPT by 1.5 and 1.3 accuracy points, respectively.These overall results indicate that instruction tuning on multiple supervised datasets, such as the FLAN collection, can still be improved for learning logic.We hypothesize that this may be attributed to the sparsity of reasoningrelevant data in the entire collection and the conflicts between different tasks.</p>
<p>Hybrid Reasoning and Application</p>
<p>In addition to logical reasoning in text, we are also curious about whether logic-enhanced training contributes to general language understanding (RACE), and maintain the general capabilities on massive knowledge based tasks (MMLU).To investigate this, we evaluate the performance of the enhanced LLaMA models on these two datasets.</p>
<p>As shown in Table 2, from 7B to 33B, Logi-cLLM can consistently improve the performance on RACE, except the one of LLaMA-33B w/ Log-icLLMon the test set.Specifically, LLaMA-7B w/ LogicLLM obtain around 4.2 absolute improvements, and LLaMA-13B w/ LogicLLM achieves 1.5 improvements, which has verified that the logic-enhanced training is also beneficial to general reasoning and reading comprehension.Additionally, we find that LogicLLM can also benefits the massive multitask language understanding (MMLU) on LLaMA-7B and 13B.We find that the improvements of both RACE and MMLU on LLaMA-33B are marginal, probably because lowrank adaptation have restricted the generalization.</p>
<p>Pre-training Strategy</p>
<p>LogicLLM draws inspiration from the contrastive learning framework for logical reasoning, i.e., MERIt, which has demonstrated its efficacy in fine-tuning based approaches.As mentioned earlier, we hypothesize that contrastive learning may be inadequate for LLM with in-context learning.To validate this assumption, we examine the effects of contrastive learning (ctr) and auto-regressive generation (ar).In the case of contrastive learning, we adopt the methodology of MERIt to construct logically inconsistent instances and optimize the model by maximizing the distance between logically consistent instances and the inconsistent counterparts.Referring to the table, it can be observed that LogicLLM (ctr) fails to yield significant improvements compared to LLaMA-13B, except for the dev set of Re-Clor.Conversely, the auto-regressive models consistently outperform both the baseline models and the contrastive methods by considerable margins across all dataset splits.We propose two primary reasons to explain the superiority of autoregressive models over the contrastive approach.</p>
<p>First, the heuristic construction process for negative candidates used in contrastive learning fails to identify true contradictory relations, resulting in randomly chosen negative samples that lack logically opposite relationships with the positive instances.To this end, the contrastive learning process can degrade into a positive-only optimization process, which is similar to auto-regressive learning but receives less token-level supervision.</p>
<p>Second, the divergence between the training objectives of contrastive learning and auto-regressive generation undermines the model's ability to effectively do in-context reasoning.Contrastive learning primarily focuses on discriminating positive pairs from negative pairs based on a global semantic perspective.Auto-regressive models, on the other hand, accumulate their ability through local token prediction.During inference, LLMs are expected to understand instruction, and jointly consider the logical relations between different hypothesises within single input.By placing emphasis on fine-grained relations, the auto-regressive objective can better support in-context learning, enabling the model to grasp the nuanced connections and reasoning processes required for logical understanding.</p>
<p>Moreover, the auto-regressive objective signifi- Table 4: Ablation study to explore if LogicLLM can be combined with instruction tuning.For FLAN-T5 , we use the subset of FLAN collection.For LLaMA, we introduce GPT4All (Anand et al., 2023).cantly reduces computation costs during training by eliminating the need for negative candidates encoding.The streamlining of training process leads to more efficient and resource-friendly training without sacrificing performance.We also add another experiment by adjusting the ratio between counterfactual data and the normal ones as 1:1, and the comparison reveal that mixing more counterfactual data can also benefit the performance, which could be especially useful for low-resource domain, like finance and multi-lingual LLMs.</p>
<p>In summary, considering the advantages in both performance and training cost, the auto-regressive variant proves to be a superior choice for incorporating logic reasoning into LLMs.</p>
<p>Factors Relevant to Logic Prior</p>
<p>In Table 3, we also present the ablation results on LLaMA-33B when the counterfactual data augmentation strategy is omitted.Without the inclusion of counterfactual data, LogicLLM degrades into a conditional generative task that can be solved through memorization, as each sample has its own prototypes within Wikipedia.</p>
<p>As indicated in the table, even without the augmentation (no aug.), LogicLLM still contributes to the enhancement of logical reasoning abilities, albeit with more limited improvements.However, the introduction of counterfactual data augmentation to eliminate memorization effects can further amplify the benefits.The overall experimental results point out that relation construction serves as effective supervision signal for introducing logic prior.We leave the work about developing novel techniques to prevent memorization but less involve factual noise as future work.</p>
<p>Compatibility with Instruction Tuning</p>
<p>Instruction tuning has served as a critical step to make LLMs better in following human instruction, and/or generating with less toxic.In this section, we hope to study if LogicLLM can be well integrated with supervised instruction tuning so that LogicLLM has the potential to serve as a basic approach to train logic-enhanced foundation model before building applications.For FLAN-T5, we directly use the same subset of FLAN collection with our approach as the instruction tuning data.For LLaMA models, we introduce GPT4All (Anand et al., 2023) data for extra supervision.During training, we simply sum the loss of instruction tuning and LogicLLM in multitask training manner to keep the same data ratio.</p>
<p>As shown in Table 4, on most dataset splits, LogicLLM can achieve additional improvements compared with the instruction tuning-only baselines.Specifically, we find that the improvements are more significant on ReClor that those on LogiQA-v2.One possible reason is that the language style in LogiQA-v2 is more close to formal language, leaving a gap with the natural user questions.</p>
<p>Data Assumption Auto-Verification</p>
<p>In order to verify the rationality of our assumption that the direct and indirect relations are logically consistent, we employ ChatGPT and GPT-4 for automatic evaluations.Specifically, we randomly sample 1,000 examples from the development set for our pre-training with the ratio of normal data and counterfactual ones as 1:1.For each data pair, we ask ChatGPT/GPT-4 to determine if the relation between the target entities are logically consistent.The prompt we used is shown in Appendix E. We have involved four different settings.Beside the normal data and the counterfactual ones, we have also applied anonymization (Qiu et al., 2020) to them to decouple the background knowledge from entity.Specifically, the target entities are replaced with [X] and [Y], and for counterfactual data, the other replaced entities during data augmentation are not further anonymized.Some cases can also be found in Appendix E for clearer understanding.</p>
<p>Our results are shown in Tabel 5, from which we can observe that: (1) for normal data, Chat-GPT and GPT-4 deem that the logically consistent data occupie high ratios, which has initially verified the rationality of our data construction assumption.</p>
<p>(2) For counterfactual data, the ratios significantly decrease.Yet, in the view of GPT-4, there is still more than 70% of logically consistent data in the whole corpus.(3) When combined with entity anonymization, the ratios become much higher for counterfactual data, i.e., nearly 15% absolute improvements for ChatGPT and 23% for GPT-4.Besides, the ratio of normal data decreases significantly for ChatGPT, but is less perturbed for GPT-4.The observation further demonstrates that most counterfactual data should also hold the assumption since the anonymization only remove the backgrounds of entities, yet leaving the context as original.And the great variation brought by counterfactual data augmentation also reveals the potential weakness of current LLMs on identifying the true causal relations.</p>
<p>Robustness</p>
<p>By training LLMs on logic-consistent data and counterfactual augmentations, they are exposed to a wide range of input variations.This exposure helps them become less sensitive to minor perturbations such as shuffling of input options.To determine the robustness of LogicLLM , we conducted experiments on LogiQA-v2 using models of varying sizes.We shuffled the input order of different options and reperformed the inference process.</p>
<p>Figure 3 illustrates the findings of our experiments.We observed that LLaMA exhibited higher variance across different input option orders, as indicated by the greater spread in results.The circular outlier values that indicate specific input orders causing significant variations, leading to substantially higher or lower performance results.Our observation is consistent with the recent findings of Wang et al. (2023b), suggesting that the normal LLMs heavily suffer from position bias.In contrast, when LLaMA is enhanced with Logi-cLLM, it achieves more stable performance across different parameter sizes.Moreover, the averaged performance of LLaMA w/ LogicLLM is significantly superior to that of LLaMA alone.These results show that LogicLLM produces consistent and improved results compared to traditional LLMs, demonstrating the value of incorporating logic-enhanced training techniques into LLMs.</p>
<p>Training Quality Analysis</p>
<p>In order to analyze the quality of our metatraining, we have constructed a test set using the framework of MERIt (Jiao et al., 2022), which contains both logically consistent and inconsistent data.We have measured the log-likelihood on each sample as illustrated by Equation 1, and report the averaged results in Figure 4.</p>
<p>As shown in the figure, for logically consistent data, LogicLLM significantly reduced the negative log-likelihood.Moreover, the 7B-based model with LogicLLM surpasses the performance of LLaMA-13B.Notably, the disparity between the negative log-likelihood of logically consistent and inconsistent instances is further amplified, highlighting the effectiveness of LogicLLM in logical relation reconstruction.Furthermore, our experiments suggest a decrease in the negative log-likelihood for logically inconsistent data.This observation exposes a weakness in the contrastive learning-based method, i.e., MERIt, wherein the heuristic process for generating negative candidates introduces considerable noise.Consequently, some negative instances may not genuinely present contradictory logical relations.</p>
<p>Conclusion</p>
<p>In this paper, we have explored the feasibility and effectiveness of enhancing logical reasoning of LLMs via purely self-supervised training.We evaluate the performance based on two LLM series, i.e., FLAN-T5 and LLaMA.The experimental results on two logical reasoning benchmarks, LogiQA-v2 and ReClor, demonstrate the effectiveness of our method.And the performance on RACE, MMLU and Big-Bench-Hard have also verified that the framework do not hurt the generalization of LLMs.Finally, we have analyzed the factors relevant to logic during training, and the compability with supervised instruction tuning.We hope the analysis could bring new insights to future research.</p>
<p>A Implementation Details</p>
<p>A.1 LLM Prompting</p>
<p>In order to evaluate the generalization capabilities of LLMs across different tasks after post-training, we adopt a prompting-based approach.Here, the input to the LLMs is structured as Instruction [Exemplars] Task input.The instruction is tailored to the specific task at hand, while exemplars are utilized only in a few-shot setting.Each exemplar comprises both the task input and its corresponding output.For tasks such as multiplechoice question answering, the task input is a concatenation of the context, the question, and all potential options.The correct option index is used as the output.Besides, in a Chain-of-Thought (CoT) setting, we include a reasoning process formulated in natural language between the task input and output.</p>
<p>A.2 Data</p>
<p>We have constructed our self-supervised logicenhanced training data from Wikipedia, where we directly used the paragraph corpus pre-processed by Qin et al. (2021).We have constructed around 200K logically consistent sample pairs.After that, we further performed counterfactual data augmentation with the ratio of 1:3, and finally induced 800K training sample pairs in total.The data construction process mainly follows the original setting of Jiao et al. (2022)  we have dropped the negative candidates since we employed auto-regressive training.</p>
<p>For language modeling, we employed different dataset with respect to the data used in their last stage training.For FLAN-T5 series models, we used the subset of FLAN-collection-v2 (Longpre et al., 2023); while for LLaMA series models, we used the same Wikipedia paragraphs from the corpus of Qin et al. (2021).</p>
<p>A.3 Hyper-parameters of Training</p>
<p>During the pre-training process, we set the batch size to 4,096, which is implemented using gradient accumulation.The maximum sequence length is truncated at 1,024 for the FLAN collection and 512 for the MERIt corpus.For the FLAN-T5 series models, we conduct training steps for 200 iterations, while for the LLaMA series models, we perform training steps for 500 iterations.The learning rates are set as follows: 1e-4 for FLAN-T5-3B, 5e-5 for FLAN-T5-11B, 1e-5 for LLaMA-7B, and 5e-6 for LLaMA-13B.To carry out the training process, we utilize 8 NVIDIA A100 80G GPUs.However, due to hardware limitations, models larger than 13B are trained using QLoRA (Dettmers et al., 2023), a low-rank adaptation approach specifically designed for quantized LLMs.We follow the setting used in QLoRA with α as 16 and r as 64.All linear layers are used for adaptation and the LoRA dropout is 0.05.The learning rate for LLaMA-33B and Falcon-40B is set as 5e-4.</p>
<p>A.4 Evaluation</p>
<p>To ensure a fair comparison, we maintain consistency across different models for each dataset.This involves using identical instructions and fewshot samples.We use accuracy as the evaluation metric across all experiments.The prompts for different dataset can be found in Appendix D.</p>
<p>B Interpretation for Different Results on RACE</p>
<p>In this section, we will discuss the different results on RACE between ours and those reported by the original paper of LLaMA.Specifically, Touvron et al. (2023) do not report the weighted results, so we convert them by ourselves.The results are shown in Table 7. From the table we can find that only LLaMA-7B cannot match the performance reported by the authors.On LLaMA-13B and LLaMA-33B, our reproduced accuracies are much higher than the reported ones, which can help address the concern of unfair comparison, and demonstrate the effectiveness of our proposed LogicLLM.</p>
<p>C Logic-enhanced Meta-training for Complex Task Understanding</p>
<p>We evaluated the performance of logic-enhanced pre-trained models on BIG-Bench-Hard, a benchmark comprising challenging tasks where human performance surpasses that of LLMs.Table 8 presents the results achieved by the LLaMA and FLAN-T5 models under three evaluation settings: zero-shot, direct few-shot, and CoT.</p>
<p>In the zero-shot setting, our logic-enhanced meta-training significantly improves all four investigated models.For instance, the zero-shot accuracies of LLaMA-13B and FLAN-T5-T5-11B are 25.0% and 38.0%, respectively.When combined with the LogicLLM model, the accuracy scores of LLaMA-13B and FLAN-T5-11B improve to 26.3% and 44.1%, respectively.Some tasks included in BBH require free-form answers thus we cannot evaluate the models by selecting the candidate with lowest perplexity or log likelihood.Instead, we need to follow the evaluation of API-based models, which employs regularization expression to capture the answer from the response.However, smaller language models, especially those without being instruction tuned, fail to accept diverse instruction, and generate structured response.As a result, the absolute performance under zero-setting setting of LLaMA-based models are relatively limited.</p>
<p>On the other hand, the direct few-shot results outperform the zero-shot results in three out of four models, with the exception of FLAN-T5-11B.Similarly, logic-enhanced meta-training boosts the performance of models, except for FLAN-T5-11B.In the CoT setting, our method further enhances the performances of LLaMA-13B and FLAN-T5-3B.However, the best direct few-shot and CoT results (42.6% and 40.9%, respectively) are both inferior to the best zeroshot result (44.1%).Notably, the CoT results on FLAN-T5-3B are significantly worse than the zero-shot and direct few-shot results.These observations suggest the potential drawback that learning CoT from annotated training data, i.e., FLAN collection, has difficulty in generalizing to different task categories, for example, learning CoT from math word problem solving and solving logical puzzles.We provide further discussion on these findings in Appendix G.</p>
<p>D Prompt Template</p>
<p>E.2 Normal Version</p>
<p>[User]:</p>
<p>Determine whether the relation between "Everdingen" and "Sweden" in the given two sentences are logically consistent.The output should either be Yes or No.</p>
<p>[ChatGPT]:</p>
<p>Yes.The output should either be Yes or No.</p>
<p>E.3 Counterfactual Version</p>
<p>[ChatGPT]:</p>
<p>No.</p>
<p>Entity replacement:</p>
<p>• Everdingen → Nicholas Roerich;</p>
<p>• Sweden → Master;</p>
<p>• Norwegian (connecting entity) → Canal del Dique;   The output should either be Yes or No.</p>
<p>E.4 Anonymized Version</p>
<p>[ChatGPT]:</p>
<p>Yes.</p>
<p>F Discussion about Different Perspectives of Logical Reasoning</p>
<p>In our opinion, logic can be reflected through multiple aspects.Here, we use a simple logic rule to discuss the different perspectives:
(α → β) ∧ (β → γ) ↔ α → γ.(3)
The above equation shows the simplest case of first-order logic reasoning, where α, β and γ are different variables, and ∧ is logical and.We can also introduce the necessary logical connectives in natural language to make it easier for understanding:
IF α → β AND β → γ, THEN α → γ. (4)
It should be noted that, in symbolic logic, we often ignore the actual meaning of relations.However, we can always find a path, i.e., a series of relation triplets from knowledge graph to transform the above symbolic form into natural language based logical reasoning process:
IF α r 1 −→ β AND β r 2 −→ γ, THEN α r 3 −→ γ.
(5) One example here can be: r 1 refers to is the father of, r 2 refers to is the mother of, and r 3 refers to is the grandpa of.</p>
<p>From the above discussion, we can conclude that (1) logical connectives focus on discourselevel connections, (2) symbolic logic can be viewed as the simplified version of logical reasoning in natural language, where we focus more on the formal rules of atomic logic operations, and (3) relational reasoning concentrates on the actual logic operations built on world knowledge.Both of what we have discussed in the paper and the reviewers have mentioned in comments, i.e., logical connectives, are indeed different perspectives of logical reasoning.They do not contradict to each other, and discussing them separately is beneficial to make the problem easier.Besides, there are also several studies also discuss logical reasoning from the relational reasoning perspective (Wong et al., 2023;Xu et al., 2021;Zeng et al., 2021;Wang et al., 2022).And Figure 1 also shows the case emphasizing relational reasoning.</p>
<p>G Weakness of LLMs on Logical Reasoning</p>
<p>Table 9 showcases the evaluation results of LLMs' performance in both few-shot and CoT settings.</p>
<p>The intermediate reasoning process is automatically generated by ChatGPT using the prompt "Let's think step by step."In the case of zeroshot CoT, we include the suffix prompt "So the answer is" to guide the models in summarizing and concluding the answer.For few-shot CoT, the reasoning process is initially generated for each sample in the training set.Subsequently, we retain the samples where the final prediction is correct, following the steps outlined in zero-shot CoT.During testing, we randomly select samples from the retained candidates, as well as the automatically generated CoT, to serve as exemplars.However, our observations indicate that both few-shot learning and the use of CoT do not significantly improve the models' performance.For example, ChatGPT w/ CoT performs much worse than that without CoT on the development set of LogiQA-v2.One potential reason for this is that the selected samples differ substantially from the target example.To investigate further, we incorporate reasoning category information during exemplar selection.In LogiQA-V2, each question is annotated with a reasoning category, such as categorical reasoning, sufficient conditional reasoning, or necessary conditional reasoning.For few-shot CoT prompting, we only consider candidates that share at least two common reasoning categories.This particular variant is denoted as "ChatGPT w/ CoT + Cate." in the table.</p>
<p>Despite these efforts, we find that carefully selecting prompting exemplars only provides limited improvement.The results indicate that LLMs struggle to comprehend the reasoning structure from a limited number of observed examples.Consequently, they face challenges in effectively learning the mapping between input-label and input-rationale-label.Additionally, as shown in Table 1, we observe that LogicLLM also contributes minimally to addressing this issue.We recognize the need for further investigation in this area and leave it as a potential avenue for future research.</p>
<p>In 1644 ,Figure 2 :
16442
Figure 2: The LogicLLM framework.P and Q are two arbitrary paragraphs from Wikipedia.In Step 1, we extract intra-sentence relations ①: ⟨ e i , s k , e j ⟩, and the compositions of them ②: ⟨e i , s i+1 , e i+1 , • • • , s j , e j ⟩ from P for an entity pair ⟨ e i , e j ⟩; ① and ② are direct and indirection relations, respectively.Here s k is a relation, represented by the sentence that mentions ⟨ e i , e j ⟩. ① and ② are viewed as logically consistent since both of them describe the "same" relation between ⟨ e i , e j ⟩ from different view.In Part I of the figure, e i refers to Everdigen and e j represents Sweden.The intermediate entity is Norwegian here.The direct relation on the left says that Everdigen has traveled to Sweden, and the indirect relation implies the fact that Everdigen has probably visited Sweden as well as its nearby area, otherwise he could not complete the sketches of Norwegian, demonstrating the fuzzy logic consistency with high probability.Step 2 is the process of counterfactual data augmentation, where counterfactual relation composition is generated by random entity replacement.③ and ④ are the counterfactual augmentations of ① and ②, respectively.Finally, in Step 3, the LLM is optimized to generate direct/indirect relations with their logically consistent indirect/direct counterparts as inputs.Here, ①→ ②, ②→ ①, ③→ ④, and ④→ ③ are considered.</p>
<p>Figure 3 :
3
Figure 3: Results of 5 experiments with different option input orders across different model sizes on the test set of LogiQA-v2.Brown circular marker: outlier, green triangle: arithmetic mean value.</p>
<p>Figure 4 :
4
Figure 4: The averaged log-likelihood value of different models on the self-constructed logically consistent and inconsistent instances, respectively.w/ L. refers to the models augmented with LogicLLM.</p>
<p>of Frans Post, Everdingen took advantage of this mishap by making sketches of the Norwegian landscape, which would have seemed very exotic to his Dutch countrymen.His annotated drawings document visits to the south -east Norwegian coast and to Bohusland and the Göteborg area in western Sweden.Sentence 2: In 1644 Everdingen travelled to Norway and Sweden, a trip that was to have profound consequences on his art.</p>
<p>Roerich travelled toNorway and Master , a trip that was to have profound consequences on his art .</p>
<p>[</p>
<p>User]: Determine whether the relation between "[X]" and "[Y]" in the given two sentences are logically consistent.</p>
<p>of Frans Post, [X] took advantage of this mishap by making sketches of the Canal del Dique landscape , which would have seemed very exotic to his Dutch countrymen.His annotated drawings document visits to the south -east Canal del Dique coast and to Bohusland and the Göteborg area in western [Y].Sentence 2: In 1644 [X] travelled to Norway and [Y], a trip that was to have profound consequences on his art .</p>
<p>Table 1 :
1
The results on logical reasoning benchmarks.Better results are annotated in bold.† refers that the corresponding model is trained through QLoRA
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.ChatGPT56.6 61.2 54.5 52.7LLaMA-7B30.2 30.3 27.4 28.1w/ LogicLLM32.4 31.0 27.7 28.6LLaMA-13B30.4 33.5 33.0 32.1w/ LogicLLM37.4 36.3 34.1 34.0LLaMA-33B45.2 50.3 41.2 41.6w/ LogicLLM  †50.2 54.4 45.9 42.6Falcon-40B38.4 37.1 35.9 36.1w/ LogicLLM  †41.4 43.0 38.6 37.2FLAN-T5-3B54.6 52.5 48.7 48.7w/ LogicLLM &amp; FLAN 55.8 54.1 50.8 50.1FLAN-T5-11B57.4 59.9 55.3 53.1w/ LogicLLM &amp; FLAN 61.2 61.1 56.0 54.0</p>
<p>Table 2 :
2
The results of LLaMA models on RACE and MMLU.† means training through QLoRA.
RACEMMLUModel / DatasetDevTest 0-shot 5-shotAcc. Acc.Acc.Acc.LLaMA-7B31.3 32.333.336.2w/ LogicLLM37.3 37.934.636.6LLaMA-13B55.8 54.541.146.7w/ LogicLLM57.7 55.643.347.3LLaMA-33B68.4 68.154.358.3w/ LogicLLM  † 68.8 68.154.458.3ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.LLaMA-13B30.4 33.5 33.0 32.1w/ LogicLLM (ctr)33.4 33.3 33.1 32.7w/ LogicLLM (ar)37.4 36.3 34.1 34.0LLaMA-33B45.2 50.3 41.2 41.6w/ LogicLLM  † (no aug.) 49.4 53.0 44.2 40.8w/ LogicLLM  † (1 aug.)50.8 52.7 45.6 41.5w/ LogicLLM  †50.2 54.4 45.9 42.6</p>
<p>Table 3 :
3
The effect of different training objectives.Ctr refers contrastive learning and ar means the autoregressive variant.no aug.means the counterfactual data augmentation is removed from the Logi-cLLM framework.</p>
<p>† means that the model is trained with QLoRA.</p>
<p>Table 5 :
5
The ratio of consistent data deemed by Chat-GPT and GPT-4.Anony.refers to anonymization and C.F. is the simplification of Counterfactual.</p>
<p>Table 6 :
6
(Jiao et al., 2022)y of LLMs, i.e., Chat-GPT (GPT-3.5-turbo)andLLaMA,and existing stateof-the-art methods(Jiao et al., 2022)on logical reasoning benchmarks.The evaluation of LLMs follows zeroshot in-context learning setting, where the models are expected to decode the answer based on the given instruction, context, and question.
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.RoBERTa-L.62.655.659.857.0MERIt (RoBERTa-L)69.461.662.659.3MERIt (DeBERTa-XXL) 80.678.1--LLaMA-7B28.828.324.423.7LLaMA-13B31.634.431.631.1LLaMA-33B45.250.341.241.6GPT-3.5-turbo56.661.254.552.7w/ CoT58.857.7-53.1</p>
<p>Table 7 :
7
except two differences.First, we remove the usage of knowledge graph for relation annotation to enable fully self-supervision and simplify the construction workflow.Secondly, The comparison on RACE dataset between our reproduced results and those reported by the opriginal paper of LLaMA.
High Middle WeightedLLaMA-7B46.961.151.0LLaMA-7B (Ours)--32.3LLaMA-13B47.261.651.4LLaMA-13B (Ours)--54.5LLaMA-33B48.364.152.9LLaMA-33B (Ours)--68.1</p>
<p>Table 8 :
8
The accuracy of LLaMA and FLAN-T5 based models on BIG-Bench-Hard.Direct refer to few-shot setting through direct prompting, where only the final answer is given.Instead, in CoT setting, the reasoning process is also concatenated.The exemplars used for direct few-shot prompting and CoT prompting are consistent in each task, which are officially provided.
Model / DatasetZero-shot Direct CoTLLaMA-7B24.930.4 27.0w/ LogicLLM25.230.8 25.9LLaMA-13B25.034.7 32.3w/ LogicLLM26.335.0 33.9FLAN-T5-3B38.040.2 35.1w/ LogicLLM &amp; FLAN40.541.2 36.7FLAN-T5-11B43.042.6 40.9w/ LogicLLM &amp; FLAN44.136.2 40.2</p>
<p>Table 9 :
9
The results on logical reasoning benchmarks with enhanced Chain-of-Thought prompting.
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.zero-shotChatGPT56.6 61.2 54.5 52.7w/ CoT58.8 57.7 54.5 53.15-shotChatGPT61.0 63.0 55.1 54.5w/ CoT62.0 62.5 47.6 55.6w/ CoT + Cate. N/A N/A 55.8 55.0
In this paper, we will use the term logical consistency to represent consistency in fuzzy logic for simplification, which is further described by relational consistency. This means that the relationship between a logically consistent data pair has a higher degree of logical consistency but cannot be strictly proved considering the diverse expressions of relations.
In practice, we find 87% indirect relations are composed of two relation triplets, 12% contain three triplets, and less than 1% have more than 4 triplets. This prevents the logical consistency be weakened by long context.
AcknowledgementsThis research is supported by the Ministry of Education, Singapore, under its Science of Learning Grant (award ID: MOE-MOESOL2021-0006). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the Ministry of Education, Singapore.Besides, we sincerely appreciate the valuable comments from all the reviewers to help us make the paper polished.We also greatly thank to Chengwei Qin and Professor Aixin Sun for their kind suggestions.code and models are released at https://github.com/SparkJiao/LogicLLM.LimitationsIn this paper, we have explored the feasibility to introduce logical reasoning capability into LLMs via purely self-supervised meta-training.Though the results have demonstrated significant improvements on logical reasoning benchmarks, there are also some limitations: Randomness from Diverse Prompt/Instruction.In our experiments, we find that the performance of LLMs, especially those never optimized by instruction tuning, is varying to different prompts.We try to reduce the variance by (1) using simpler prompt (as shown in Section D or (2) using the released prompt by commonly accepted benchmark or leaderboard, e.g., MMLU, Big-Bench-Hard and Chain-of-Thought Hub(Fu et al., 2023).Nevertheless, this still cannot entirely keep the certainty of the experimental results.Non-uniform Evaluation Strategy.Currently, there is no de facto technical standard for LLMs evaluation.Some work just let language models generate the response and match the content.However, this can be unfair for non-instructiontuned models since they often cannot generate meaningful and complete sentences, especially those under 13 billion parameters.Scaling.Due to the resource limitation, we can only scale the method into models with 40 billion parameters under the help of low-rank adaptation.
Gpt4all: Training an assistant-style chatbot with large scale data distillation. Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar, 2023from gpt-3.5-turbo</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.48550/arXiv.2302.04023CoRR, abs/2302.040232023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 10.48550/arXiv.2211.12588CoRR, abs/2211.125882022</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, 10.48550/arXiv.2210.02875CoRR, abs/2210.028752022</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, 10.48550/arXiv.2204.02311David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</p>
<p>Reasonbert: Pre-trained to reason with distant supervision. Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun, 10.18653/v1/2021.emnlp-main.494EMNLP. ACL2021</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, CoRR, abs/2305.143142023</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, 10.48550/arXiv.2305.17306CoRR, abs/2305.173062023</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 10.48550/arXiv.2211.10435CoRR, abs/2211.104352022</p>
<p>Discern: Discourseaware entailment reasoning network for conversational machine reading. Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq R Joty, C H Steven, Caiming Hoi, Irwin Xiong, Michael R King, Lyu, 10.18653/v1/2020.emnlp-main.191EMNLP. ACL2020</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, ICLR. OpenReview2021</p>
<p>DAGN: discourse-aware graph network for logical reasoning. Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, Xiaodan Liang, 10.18653/v1/2021.naacl-main.467NAACL-HLT. ACL2021</p>
<p>REPT: bridging language models and machine reading comprehension via retrieval-based pre-training. Fangkai Jiao, Yangyang Guo, Yilin Niu, Feng Ji, Feng-Lin Li, Liqiang Nie, 10.18653/v1/2021.findings-acl.13Findings of ACL/IJCNLP. ACL2021</p>
<p>Merit: Meta-path guided contrastive learning for logical reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, 10.18653/v1/2022.findings-acl.276Findings of ACL. ACL2022</p>
<p>Is chatgpt A good translator? A preliminary study. Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, Zhaopeng Tu, 10.48550/arXiv.2301.08745CoRR, abs/2301.087452023</p>
<p>series in artificial intelligence. Daniel Jurafsky, James H Martin, Speech and language processing. Pearson Education International20092pearson international edition] edition</p>
<p>RACE: large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard H Hovy, 10.18653/v1/d17-1082EMNLP. ACL2017</p>
<p>A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang, ACL. ACL. 2023</p>
<p>Hanmeng Liu, Jian Liu, Leyang Cui, Nan Duan, Ming Zhou, Yue Zhang, Logiqa2.0 datasetlogical reasoning in mrc and nli tasks. TASLP2022a</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501IJCAI. 2020</p>
<p>Knowledge based multilingual language model. Linlin Liu, Xin Li, Ruidan He, Lidong Bing, R Shafiq, Luo Joty, Si, EMNLP. ACL2022b</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, 10.48550/arXiv.2301.13688CoRR, abs/2301.136882023</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 10.48550/arXiv.2302.07842CoRR, abs/2302.078422023</p>
<p>Distant supervision for relation extraction without labeled data. Mike Mintz, Steven Bills, Rion Snow, Daniel Jurafsky, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLPACL2009</p>
<p>OpenAI. 2023. Gpt-4 technical report. Preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, 10.48550/arXiv.2306.01116CoRR, abs/2306.01116Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro CappelliGuilherme Penedo2022</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, 10.48550/arXiv.2302.06476CoRR, abs/2302.064762023</p>
<p>ERICA: Improving entity and relation understanding for pre-trained language models via contrastive learning. Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong Sun, Jie Zhou, 10.18653/v1/2021.acl-long.260ACL/IJCNLP. ACL2021</p>
<p>GCC: graph contrastive coding for graph neural network pre-training. Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, Jie Tang, 10.1145/3394486.3403168KDD. ACM2020</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/arXiv.2302.04761CoRR, abs/2302.047612023</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 10.48550/arXiv.2210.09261CoRR, abs/2210.092612022</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, CoRR, abs/2302.139712023</p>
<p>Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning. Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, Ai Ti Aw, Nancy F Chen, CoRR, abs/2309.047662023a</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, 10.48550/arXiv.2305.17926CoRR, abs/2305.179262023b</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, 10.18653/v1/2022.findings-acl.127ACL. ACL2022</p>
<p>KEPLER: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang, 10.1162/tacl_a_00360TACL. 92021</p>
<p>2022a. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, ICLR. OpenReviewM. Dai, and Quoc V. Le.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, 10.48550/arXiv.2206.07682CoRR, abs/2206.07682Emergent abilities of large language models. 2022b</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, 10.48550/arXiv.2306.12672CoRR, abs/2306.126722023</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, CoRR, abs/2304.122442023</p>
<p>Discriminative reasoning for document-level relation extraction. Wang Xu, Kehai Chen, Tiejun Zhao, 10.18653/v1/2021.findings-acl.144Findings of ACL. ACL2021</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, In ICLR. Open-Review. 2020</p>
<p>SIRE: separate intra-and inter-sentential reasoning for document-level relation extraction. Shuang Zeng, Yuting Wu, Baobao Chang, Findings of ACL. ACL2021</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, CoRR, abs/2104.06598AR-LSAT: investigating analytical reasoning of text. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9518 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9518</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9518</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-a4b05bcbc9676fcf8287bab120a44488c67ec588</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4b05bcbc9676fcf8287bab120a44488c67ec588" target="_blank">NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> NLP-AKG, an academic knowledge graph for the NLP domain, is developed and NLP-AKG, an academic knowledge graph for the NLP domain, is developed by extracting 620,353 entities and 2,271,584 relations from 60,826 papers in ACL Anthology.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been widely applied in question answering over scientific research papers. To enhance the professionalism and accuracy of responses, many studies employ external knowledge augmentation. However, existing structures of external knowledge in scientific literature often focus solely on either paper entities or domain concepts, neglecting the intrinsic connections between papers through shared domain concepts. This results in less comprehensive and specific answers when addressing questions that combine papers and concepts. To address this, we propose a novel knowledge graph framework that captures deep conceptual relations between academic papers, constructing a relational network via intra-paper semantic elements and inter-paper citation relations. Using a few-shot knowledge graph construction method based on LLM, we develop NLP-AKG, an academic knowledge graph for the NLP domain, by extracting 620,353 entities and 2,271,584 relations from 60,826 papers in ACL Anthology. Based on this, we propose a 'sub-graph community summary' method and validate its effectiveness on three NLP scientific literature question answering datasets.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9518.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9518.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based paper-element extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-based extraction of paper semantic elements (field, keywords, problem, method, model, task, dataset, metric, result, innovation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses large language models (unnamed; used in zero-shot/few-shot prompt setups) to extract structured semantic elements from scientific papers (abstracts, introductions, and tables) to populate an academic knowledge graph (NLP-AKG). Extraction covers task/method/model/problem/innovation and table-extracted dataset/metric/result entries, with post-processing (XLNet detector and K-means clustering) for cleaning and disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>unspecified LLM(s) (zero-shot / few-shot prompting); GPT-4 used later as backbone for QA experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>The paper refers generically to large LLMs (no architecture/size specified) used in zero-shot and few-shot prompt formats for summarization and element extraction. For downstream QA experiments, GPT-4 (gpt-4-0613) is explicitly used as the backbone; no model sizes or fine-tuning of the LLMs for extraction are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Natural language processing (scientific literature / NLP papers)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>60,826 papers from the ACL Anthology (1952–2023) processed via Grobid into structured XML/TEI; extracted 620,353 entities and 2,271,584 relations; table extraction guided by LLM-generated code and E5-inspired hierarchical table interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>thematic/generalizable patterns and concise semantic summaries of paper contributions (e.g., task–method–innovation patterns), i.e., thematic/principled summaries rather than formal mathematical laws</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>From aggregated paper elements and community summaries the system produces rules such as: 'To reduce ODQA latency, use ANN/HNSW for retrieval + skip-reading or adaptive computing for reading, and consider model/index compression (distillation/pruning) or single-stage retrieval/generation pipelines.' (Example synthesized in C Example Questions, Q1.)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompt-based LLM extraction: supply abstract/introduction (or selected tables) to LLM with structured prompts to output Field/Keywords/Problem/Method/Model/Task; for tables, LLM screens tables to find main-results table, generates/executed code to extract dataset/metric/result; innovations are then summarized by prompting LLM with previously extracted elements and paper text. Post-processing includes fine-tuned XLNet classifier to detect extraction errors and K-means clustering on token embeddings plus LLM-assisted cluster extension for disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual sampling and human verification: entity and inter-paper relation extraction were validated by manual sampling on 100 papers (detailed per-entity accuracies reported in Table 6). QA outputs evaluated with BERTScore against reference answers on QASPER, NLP-paper-to-QA-generation, and an annotated 200-question multi-paper test set.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Extraction produced 620,353 entities and 2,271,584 relations. Manual sampling reported high extraction accuracies (overall entity accuracy ≈ 0.94; inter-paper relation accuracy ≈ 0.93; per-entity accuracies reported in Table 6, e.g., Title 0.99, Method 0.95, Innovation 0.85, Result 0.78). For downstream QA, KG-derived element augmentation improved LLM QA performance modestly over baseline retrieval methods and LLM-only in F1 (see companion 'sub-graph community' result).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to non-KG retrieval baselines (BM25, embedding retrieval) and alternate KG-augmented methods (KAG, MindMap), the LLM-extraction + NLP-AKG pipeline provided more accurate, specific supplementary information that yielded higher QA F1 when used with the authors' augmentation method; specific numeric comparisons of final QA are reported under the 'sub-graph community summary' entry.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Paper-level: LLM extraction relies on human intervention for novel fields; dynamic updates of the KG are unresolved; extraction errors exist in some categories (Innovation and Result have lower accuracies). The specific LLM(s) used for extraction are not named, limiting reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The paper cites hallucination as a general motivation for knowledge augmentation but does not report concrete hallucination incidents from the extraction pipeline; no systematic bias analysis of LLM-extracted entities is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9518.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9518.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sub-graph community summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sub-graph community summary augmentation for LLM question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that identifies paper communities (subgraphs) via inter-paper relations in the constructed NLP-AKG and uses LLM prompting to synthesize community-level answers (aggregate qualitative rules/principles) by concatenating the question, community elements, element introductions, and a prompt, producing community answers that are then summarized into a global answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613) as backbone for QA; additional unspecified LLMs for intent extraction and community answer generation are used in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4 (gpt-4-0613) is used as the main LLM for evaluation and answer generation; other LLMs are referenced generically for zero-shot/few-shot extraction and intent identification. No fine-tuning of GPT-4 is reported for the community summarization task.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>NLP scientific literature question answering and multi-paper summarization</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Uses the NLP-AKG built from 60,826 ACL Anthology papers (620,353 entities, 2,271,584 relations). For evaluation, questions were taken from QASPER (280 test questions overlapping with KG), NLP-paper-to-QA-generation (274 overlapping), and a manually annotated 200-question multi-paper test set.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>community-level synthesized principles/rules and summarized solutions (thematic patterns across papers), e.g., strategies, principled trade-offs, or common methodological patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Example synthesis (multi-paper): 'To speed up Open-Domain QA, combine approximate nearest-neighbor retrieval (HNSW/ANN) with skip-reading or adaptive computing for reading, and apply model/index compression (knowledge distillation, pruning, PQ) or single-stage retrieve/generate pipelines.' (Presented in C. Example Questions Q1 as an aggregated community answer.)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Intent identification via LLM prompts to extract relevant element categories; query KG for titles and their connected target elements; form communities C when inter-paper relations (direct use or task-related citations) connect papers; produce per-community answers A_c by concatenating Q, C, element introductions I, and a prompt P and feeding to LLM (A_c = LLM(Q ⊕ C ⊕ I ⊕ P)); aggregate community answers A_c via LLM to produce global answer A_g (A_g = LLM(∪_c A_c ⊕ Q)).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic evaluation with BERTScore comparing generated answers to reference answers on two open datasets (QASPER, NLP-paper-to-QA-generation) and a manually annotated 200-question multi-paper dataset; also manual verification of extracted entities for the KG used in augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The sub-graph community summary method outperformed baselines in BERTScore-based F1: On QASPER the method achieved F1 0.7204 (vs BM25 0.7048, embedding 0.7069, GPT-4 0.6929, KAG 0.7097, MindMap 0.7173). On NLP-paper-to-QA-generation, F1 0.7300 (best among listed). On the annotated multi-paper dataset, the method reached F1 0.7768, modestly above GPT-4's 0.7654 (≈1.1% absolute improvement) and substantially above MindMap (0.7006) and KAG (0.7275). Authors attribute gains to more accurate/specific supplementary information from NLP-AKG and efficient retrieval of deep paper connections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared against BM25 and embedding retrieval, KAG, MindMap, and GPT-4 (LLM-only), the sub-graph community approach provided small but consistent improvements in BERTScore F1; on the annotated multi-paper set it outperformed GPT-4 by ~1.1 percentage points and MindMap by ~7.7 points.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Method evaluated only on NLP literature; generalization to other scientific domains is untested. The KG requires manual intervention for new fields and does not yet support dynamic streaming updates without reconstruction. MindMap's hop limit and GPT-4's large pretrained context are discussed as confounding factors. No formal human-subject validation of synthesized 'laws' beyond expert-annotated examples is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The paper motivates KG augmentation to reduce hallucination but does not report concrete hallucination cases or quantify hallucination reduction; acknowledges hallucination as a general concern but leaves systematic analysis for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MindMap: Knowledge graph prompting sparks graph of thoughts in large language models <em>(Rating: 2)</em></li>
                <li>Kag: Boosting llms in professional domains via knowledge augmented generation <em>(Rating: 2)</em></li>
                <li>Deep bidirectional language-knowledge graph pretraining <em>(Rating: 2)</em></li>
                <li>End-to-end construction of NLP knowledge graph <em>(Rating: 2)</em></li>
                <li>Construction and application of a knowledge graph <em>(Rating: 1)</em></li>
                <li>Open research knowledge graph: a system walkthrough <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9518",
    "paper_id": "paper-a4b05bcbc9676fcf8287bab120a44488c67ec588",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "LLM-based paper-element extraction",
            "name_full": "Large Language Model-based extraction of paper semantic elements (field, keywords, problem, method, model, task, dataset, metric, result, innovation)",
            "brief_description": "The paper uses large language models (unnamed; used in zero-shot/few-shot prompt setups) to extract structured semantic elements from scientific papers (abstracts, introductions, and tables) to populate an academic knowledge graph (NLP-AKG). Extraction covers task/method/model/problem/innovation and table-extracted dataset/metric/result entries, with post-processing (XLNet detector and K-means clustering) for cleaning and disambiguation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "unspecified LLM(s) (zero-shot / few-shot prompting); GPT-4 used later as backbone for QA experiments",
            "llm_model_description": "The paper refers generically to large LLMs (no architecture/size specified) used in zero-shot and few-shot prompt formats for summarization and element extraction. For downstream QA experiments, GPT-4 (gpt-4-0613) is explicitly used as the backbone; no model sizes or fine-tuning of the LLMs for extraction are reported.",
            "application_domain": "Natural language processing (scientific literature / NLP papers)",
            "input_corpus_description": "60,826 papers from the ACL Anthology (1952–2023) processed via Grobid into structured XML/TEI; extracted 620,353 entities and 2,271,584 relations; table extraction guided by LLM-generated code and E5-inspired hierarchical table interpretation.",
            "qualitative_law_type": "thematic/generalizable patterns and concise semantic summaries of paper contributions (e.g., task–method–innovation patterns), i.e., thematic/principled summaries rather than formal mathematical laws",
            "qualitative_law_example": "From aggregated paper elements and community summaries the system produces rules such as: 'To reduce ODQA latency, use ANN/HNSW for retrieval + skip-reading or adaptive computing for reading, and consider model/index compression (distillation/pruning) or single-stage retrieval/generation pipelines.' (Example synthesized in C Example Questions, Q1.)",
            "extraction_methodology": "Prompt-based LLM extraction: supply abstract/introduction (or selected tables) to LLM with structured prompts to output Field/Keywords/Problem/Method/Model/Task; for tables, LLM screens tables to find main-results table, generates/executed code to extract dataset/metric/result; innovations are then summarized by prompting LLM with previously extracted elements and paper text. Post-processing includes fine-tuned XLNet classifier to detect extraction errors and K-means clustering on token embeddings plus LLM-assisted cluster extension for disambiguation.",
            "evaluation_method": "Manual sampling and human verification: entity and inter-paper relation extraction were validated by manual sampling on 100 papers (detailed per-entity accuracies reported in Table 6). QA outputs evaluated with BERTScore against reference answers on QASPER, NLP-paper-to-QA-generation, and an annotated 200-question multi-paper test set.",
            "results_summary": "Extraction produced 620,353 entities and 2,271,584 relations. Manual sampling reported high extraction accuracies (overall entity accuracy ≈ 0.94; inter-paper relation accuracy ≈ 0.93; per-entity accuracies reported in Table 6, e.g., Title 0.99, Method 0.95, Innovation 0.85, Result 0.78). For downstream QA, KG-derived element augmentation improved LLM QA performance modestly over baseline retrieval methods and LLM-only in F1 (see companion 'sub-graph community' result).",
            "comparison_to_baseline": "Compared to non-KG retrieval baselines (BM25, embedding retrieval) and alternate KG-augmented methods (KAG, MindMap), the LLM-extraction + NLP-AKG pipeline provided more accurate, specific supplementary information that yielded higher QA F1 when used with the authors' augmentation method; specific numeric comparisons of final QA are reported under the 'sub-graph community summary' entry.",
            "reported_limitations": "Paper-level: LLM extraction relies on human intervention for novel fields; dynamic updates of the KG are unresolved; extraction errors exist in some categories (Innovation and Result have lower accuracies). The specific LLM(s) used for extraction are not named, limiting reproducibility.",
            "bias_or_hallucination_issues": "The paper cites hallucination as a general motivation for knowledge augmentation but does not report concrete hallucination incidents from the extraction pipeline; no systematic bias analysis of LLM-extracted entities is reported.",
            "uuid": "e9518.0",
            "source_info": {
                "paper_title": "NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Sub-graph community summary",
            "name_full": "Sub-graph community summary augmentation for LLM question answering",
            "brief_description": "A pipeline that identifies paper communities (subgraphs) via inter-paper relations in the constructed NLP-AKG and uses LLM prompting to synthesize community-level answers (aggregate qualitative rules/principles) by concatenating the question, community elements, element introductions, and a prompt, producing community answers that are then summarized into a global answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4 (gpt-4-0613) as backbone for QA; additional unspecified LLMs for intent extraction and community answer generation are used in prompts",
            "llm_model_description": "GPT-4 (gpt-4-0613) is used as the main LLM for evaluation and answer generation; other LLMs are referenced generically for zero-shot/few-shot extraction and intent identification. No fine-tuning of GPT-4 is reported for the community summarization task.",
            "application_domain": "NLP scientific literature question answering and multi-paper summarization",
            "input_corpus_description": "Uses the NLP-AKG built from 60,826 ACL Anthology papers (620,353 entities, 2,271,584 relations). For evaluation, questions were taken from QASPER (280 test questions overlapping with KG), NLP-paper-to-QA-generation (274 overlapping), and a manually annotated 200-question multi-paper test set.",
            "qualitative_law_type": "community-level synthesized principles/rules and summarized solutions (thematic patterns across papers), e.g., strategies, principled trade-offs, or common methodological patterns.",
            "qualitative_law_example": "Example synthesis (multi-paper): 'To speed up Open-Domain QA, combine approximate nearest-neighbor retrieval (HNSW/ANN) with skip-reading or adaptive computing for reading, and apply model/index compression (knowledge distillation, pruning, PQ) or single-stage retrieve/generate pipelines.' (Presented in C. Example Questions Q1 as an aggregated community answer.)",
            "extraction_methodology": "Intent identification via LLM prompts to extract relevant element categories; query KG for titles and their connected target elements; form communities C when inter-paper relations (direct use or task-related citations) connect papers; produce per-community answers A_c by concatenating Q, C, element introductions I, and a prompt P and feeding to LLM (A_c = LLM(Q ⊕ C ⊕ I ⊕ P)); aggregate community answers A_c via LLM to produce global answer A_g (A_g = LLM(∪_c A_c ⊕ Q)).",
            "evaluation_method": "Automatic evaluation with BERTScore comparing generated answers to reference answers on two open datasets (QASPER, NLP-paper-to-QA-generation) and a manually annotated 200-question multi-paper dataset; also manual verification of extracted entities for the KG used in augmentation.",
            "results_summary": "The sub-graph community summary method outperformed baselines in BERTScore-based F1: On QASPER the method achieved F1 0.7204 (vs BM25 0.7048, embedding 0.7069, GPT-4 0.6929, KAG 0.7097, MindMap 0.7173). On NLP-paper-to-QA-generation, F1 0.7300 (best among listed). On the annotated multi-paper dataset, the method reached F1 0.7768, modestly above GPT-4's 0.7654 (≈1.1% absolute improvement) and substantially above MindMap (0.7006) and KAG (0.7275). Authors attribute gains to more accurate/specific supplementary information from NLP-AKG and efficient retrieval of deep paper connections.",
            "comparison_to_baseline": "Compared against BM25 and embedding retrieval, KAG, MindMap, and GPT-4 (LLM-only), the sub-graph community approach provided small but consistent improvements in BERTScore F1; on the annotated multi-paper set it outperformed GPT-4 by ~1.1 percentage points and MindMap by ~7.7 points.",
            "reported_limitations": "Method evaluated only on NLP literature; generalization to other scientific domains is untested. The KG requires manual intervention for new fields and does not yet support dynamic streaming updates without reconstruction. MindMap's hop limit and GPT-4's large pretrained context are discussed as confounding factors. No formal human-subject validation of synthesized 'laws' beyond expert-annotated examples is reported.",
            "bias_or_hallucination_issues": "The paper motivates KG augmentation to reduce hallucination but does not report concrete hallucination cases or quantify hallucination reduction; acknowledges hallucination as a general concern but leaves systematic analysis for future work.",
            "uuid": "e9518.1",
            "source_info": {
                "paper_title": "NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MindMap: Knowledge graph prompting sparks graph of thoughts in large language models",
            "rating": 2,
            "sanitized_title": "mindmap_knowledge_graph_prompting_sparks_graph_of_thoughts_in_large_language_models"
        },
        {
            "paper_title": "Kag: Boosting llms in professional domains via knowledge augmented generation",
            "rating": 2,
            "sanitized_title": "kag_boosting_llms_in_professional_domains_via_knowledge_augmented_generation"
        },
        {
            "paper_title": "Deep bidirectional language-knowledge graph pretraining",
            "rating": 2,
            "sanitized_title": "deep_bidirectional_languageknowledge_graph_pretraining"
        },
        {
            "paper_title": "End-to-end construction of NLP knowledge graph",
            "rating": 2,
            "sanitized_title": "endtoend_construction_of_nlp_knowledge_graph"
        },
        {
            "paper_title": "Construction and application of a knowledge graph",
            "rating": 1,
            "sanitized_title": "construction_and_application_of_a_knowledge_graph"
        },
        {
            "paper_title": "Open research knowledge graph: a system walkthrough",
            "rating": 1,
            "sanitized_title": "open_research_knowledge_graph_a_system_walkthrough"
        }
    ],
    "cost": 0.01073475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM</h1>
<p>Jiayin Lan^{1}, Jiaqi Li^{2,3}, Baoxin Wang^{1,2}, Ming Liu^{1}, Dayong Wu^{2}, Shijin Wang^{2}, Bing Qin^{1}
^{1} Harbin Institute of Technology, Harbin, China
^{2}Joint Laboratory of HIT and iFLYTEK, Beijing, China
^{3}University of Science and Technology of China, HeFei, China
{jylan,mliu,qinb}@ir.hit.edu.cn</p>
<h6>Abstract</h6>
<p>Large language models (LLMs) have been widely applied in question answering over scientific research papers. To enhance the professionalism and accuracy of responses, many studies employ external knowledge augmentation. However, existing structures of external knowledge in scientific literature often focus solely on either paper entities or domain concepts, neglecting the intrinsic connections between papers through shared domain concepts. This results in less comprehensive and specific answers when addressing questions that combine papers and concepts. To address this, we propose a novel knowledge graph framework that captures deep conceptual relations between academic papers, constructing a relational network via intra-paper semantic elements and inter-paper citation relations. Using a few-shot knowledge graph construction method based on LLM, we develop NLP-AKG, an academic knowledge graph for the NLP domain, by extracting 620,353 entities and 2,271,584 relations from 60,826 papers in ACL Anthology. Based on this, we propose a 'sub-graph community summary' method and validate its effectiveness on three NLP scientific literature question answering datasets.</p>
<h2>1 Introduction</h2>
<p>In the field of scientific literature, LLMs such as GPT-4 have been widely applied to tasks including literature retrieval ( [Ajith et al., 2024]), review generation ( [Agarwal et al., 2024]), and literature analysis ( [Jiang et al., 2024; Wu et al., 2024]). To address issues of flexibility ( [Razdaibiedina et al., 2023]), hallucination ( [Ji et al., 2023]), and professionalism ( [Razdaibiedina et al., 2023]) in LLM-based scientific literature question answering, researchers commonly augment LLM with external knowledge ( [Peng et al., 2023; Sasaki et al. 2023; <img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: Knowledge graph paper entity extraction(a), paper entity cleaning and disambiguation(b), and paper relation extraction process(c)</p>
<p>Sasaki et al., 2024]).</p>
<p>While retrieval augmentation using raw text faces challenges such as retrieval inaccuracy caused by the scale and complexity of scientific literature ( [Liu et al., 2024]), knowledge graphs demonstrate significant advantages in augmenting LLM question answering capabilities due to their structured knowledge storage and interpretable reasoning paths ( [Ibrahim et al., 2024]).</p>
<p>However, existing knowledge graphs in the NLP domain still exhibit notable shortcomings when addressing questions that combine papers and concepts like "In the field of emotion recognition, which papers are innovative for GNN architecture?". For instance, the Tsinghua Open Academic Graph (OAG) ( [Zhang et al., 2019]) contains metadata such as title, author, and conference, but lacks semantic information from papers, unable to handle semantic retrieval and question answering involving NLP concepts. The knowledge graph constructed by <em>Mondal et al. (2021)</em> reveals relations among tasks, datasets, and metrics but fails to establish correspondences between concepts and academic papers. While containing semantic elements, the knowledge graph built by <em>Du and Li (2022)</em> does not model intrinsic connections between pa-</p>
<p>pers based on shared concept usage. These limitations often result in less comprehensive and exact answers when augmenting LLM with existing knowledge graphs.</p>
<p>To address these issues, We design a novel knowledge graph framework to capture the semantic relations among NLP academic papers, incorporating 15 types of entities and 29 categories of relations. Using paper titles as retrieval indexes, we link NLP concepts with relevant academic paper semantic elements such as task and method. We also extract citation relations to reveal connections and influences among research papers. Based on LLM, we develop a set of methods for entity extraction, cleaning, and disambiguation tailored to NLP research papers, enabling the construction of a high-quality knowledge graph with only a small amount of labeled data. The overall architecture is shown in Figure 1. From 60,826 papers in the ACL Anthology, we extract 620,353 entities and 2,271,584 relations. This knowledge graph connects research papers with domain concepts and extensive metadata, reflecting the complex relational network of academic research in NLP domain.</p>
<p>Finally, we propose a sub-graph community summary augmentation method for LLM, and test its question-answering accuracy against other retrieval augmentation baselines and LLM across three datasets: QASPER <em>Dasigi et al. (2021)</em>, NLP-paper-to-QA-generation, multi-paper question answering dataset we annotated. The experimental results demonstrate that our method achieves superior performance compared to baseline approaches.</p>
<h2>2 Related Work</h2>
<p>Scientific Literature Domain Knowledge Graph Most of the early studies on scientific literature domain knowledge graph are limited to constructing knowledge graphs of the external features (such as title, author, publishers) from scientific papers <em>Zhang et al. (2019); Tang et al. (2008)</em>. <em>Hao et al. (2021)</em> develops a methodology to identify innovative content in academic literature by extracting novel sentences, recognizing entities, and constructing a knowledge graph focused on innovation-related information in research papers. ORKG <em>Jaradeh et al. (2019)</em> provides a structured framework to represent academic knowledge in papers as interconnected and semantically rich knowledge graphs according to user needs. In the field of NLP, NLP-KG <em>Schopf and Matthes (2024)</em> links Fields-of-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Ontology design of knowledge graph</p>
<p>Study, publications, authors, and venues through semantic relations to form a knowledge graph, and users can retrieve scientific research literature with research domain as the index.</p>
<p>Knowledge Graph Augment LLM Question Answering The most direct way to augment LLM question answering is by integrating knowledge graphs into pre-training, such as using tasks like link prediction for additional supervision <em>Yasunaga et al. (2022)</em>. Methods like KAPING <em>Baek et al. (2023)</em> retrieve relevant facts from the knowledge graph based on semantic similarity to guide LLM answers. Currently, Knowledge Augmented Generation (KAG) <em>Liang et al. (2024)</em> is popular, using a mutual index structure between knowledge graphs and text to improve cross-document linking. MindMap <em>Wen et al. (2024)</em> helps LLM understand knowledge graph node relations by building mind maps, supporting evidence-based generation.</p>
<h2>3 Construction of NLP-AKG</h2>
<p>We propose a tailored knowledge graph construction framework for NLP domain research papers and use the rich prior knowledge in LLM as a skilled automatic extractor of paper element entities. See Figure 2 for details of the ontology design of the knowledge graph. Finally, 620,353 entities and 2,271,584 relations are extracted. We test the extraction results of 100 papers by manual sampling detection, and the extraction accuracy of entity and inter-paper relationship is 0.94 and 0.93. See Appendix A for specific knowledge graph schema and test details.</p>
<h3>3.1 Extraction of Paper Element Entities</h3>
<p>Since we focus on papers in the field of NLP, we use papers from 1952 to 2023 in ACL Anthology</p>
<p>provided by Rohatgi (2022), which they crawled to obtain all PDF documents and metadata from the website. Of entities, Title, Author, Institution, Conference, and Date are already collected in the paper metadata without additional extraction. For other entities, we leverage the zero-shot understanding and summarization capabilities of LLMs to extract paper elements from both textual content and tabular data. The specific prompt and other details we used are given in Appendix B.</p>
<ol>
<li>For Field, Keywords, Problem, Method, Model, Task, we provide LLM with the abstract part and the introduction part in the structured paper text to extract entities.</li>
<li>For Dataset, Metric and Result, the paper elements in this part mainly come from the table in the paper. We first used LLM to screen papers and tables to select tables that could represent the main research results of the paper. We refer to the idea of E5 (Zhang et al., 2024), using LLMs to interpret table hierarchies, generate code to extract data and optimize extraction results, guiding LLM to extract corresponding datasets, evaluation metrics, and results in the table.</li>
<li>For Innovation, after extracting other paper elements, we will provide the extracted paper elements with the abstract part and the introduction part paper text, and guide LLM to summarize the paper content and generate the summarized innovation points.</li>
</ol>
<h3>3.2 Paper Element Entity Cleaning and Disambiguation</h3>
<p>There are inaccurate descriptions in the extracted entities, such as "21 document corpus", which is not a specific dataset name. For such wrong entities, We annotate the error types of 300 error samples to fine-tune XLNet (Yang et al., 2019) as a detector for detecting and classifying the error samples, and delete or re-extract the corresponding entities according to the error type of the entity.</p>
<p>We chose to disambiguate Task, Dataset, and Metric because they are relatively stable across papers compared to other types of entities. We use K-means (Na et al., 2010) to disambiguate entities by assigning similar entities to the same cluster. Its objective is to minimize the within-cluster sum of squared errors: $J=\sum_{i=1}^{k} \sum_{x \in C_{i}}\left|x-\mu_{i}\right|^{2}$, where $C_{i}$ is the $i$-th cluster, $\mu_{i}$ is the cluster centroid, and $k$ is the number of clusters.</p>
<h3>3.3 Extraction of Inter-paper Relations</h3>
<p>For a paper, no additional extraction of intra-paper relations is required because of the one-to-one correspondence between intra-paper relations and the elements of the paper. We mainly consider interpaper relations including direct use and task related, depending on whether the technological achievements proposed in the cited paper have been utilized. We extract the context of citations as the basis for classification and annotate 300 examples for each of these two types of relations to train XLNet as a classifier for categorizing citation contexts.</p>
<h2>4 NLP-AKG Augment LLM Question Answering</h2>
<p>We propose a 'sub-graph community summary' method to further enhance the scientific papers question answering ability of LLM in the field of NLP, which outperforms the current LLM and retrieval augmentation baselines in three scientific literature question answering datasets.</p>
<h3>4.1 Sub-graph Community Augmentation for LLM Question Answering</h3>
<p>We first use LLM to perform intent identification on the question. Specifically, we identify the relevant elements involved in the question stem, as well as the target elements that the question asks. Since the type of relationship between paper elements is fixed, we can directly determine the corresponding paths between paper elements without retrieval.</p>
<p>The paper sub-graph that we query through the path is all the target elements connected to the title $t_{i}$. If there is an inter-paper relation between these papers $\left(t_{i}\right.$, relation, $\left.t_{j}\right)$, we believe that these papers have a deeper correlation with each other and can form a community $C$.</p>
<p>Next, we concatenate the question $Q$, sub-graph community elements $C$, element-related introductions $I$, and a simple prompt $P$ to guide LLM to produce community answers $A_{c}$ for sub-graph communities. This process can be formulated as:</p>
<p>$$
A_{c}=\operatorname{LLM}(Q \oplus C \oplus I \oplus P)
$$</p>
<p>where $\oplus$ denotes concatenation.
Finally, we simultaneously summarize all the community answers $A_{c}$ and questions $Q$ into the final global answer $A_{g}$ using LLM:</p>
<p>$$
A_{g}=\operatorname{LLM}\left(\bigcup_{c} A_{c} \oplus Q\right)
$$</p>
<p>In particular, if there is no correlation between the papers, we feed the retrieved paper elements directly to LLM.</p>
<h3>4.2 Experimental Setup</h3>
<p>We select two open-source datasets (QASPER <em>Dasigi et al. (2021)</em>, NLP-paper-to-QA-generation) to evaluate the generalization of our proposed method on NLP scientific papers question answering tasks. We also annotated 200 questions and answers as a test dataset due to the lack of open source datasets in NLP domain multiple papers question answering. We compare our methods with various baselines (including GPT-4) as well as the MindMap <em>Wen et al. (2024)</em> method and the Knowledge Augmented Generation (KAG) <em>Liang et al. (2024)</em>. Two retrieval augmentation baselines are considered: the BM25 retrieval, and the text embedding retrieval. For comparison, we use GPT-4-0613 as the backbone for all methods. Detailed descriptions of these baselines are given in the Appendix D. We use BERTScore <em>Zhang et al. (2020)</em> to measure the semantic similarity between the generated answer and the reference answer.</p>
<h3>4.3 Results</h3>
<p>Table 1: Performance of our method and baselines on QASPER and NLP-paper-to-QA-generation datasets.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td>QASPER</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BM25</td>
<td>0.6794</td>
<td>0.7386</td>
<td>0.7048</td>
</tr>
<tr>
<td>Embedding retrieval</td>
<td>0.6829</td>
<td>0.7383</td>
<td>0.7069</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.6709</td>
<td>0.7220</td>
<td>0.6929</td>
</tr>
<tr>
<td>KAG</td>
<td>0.6873</td>
<td>0.7387</td>
<td>0.7097</td>
</tr>
<tr>
<td>MindMap</td>
<td>0.6938</td>
<td>0.7491</td>
<td>0.7173</td>
</tr>
<tr>
<td>Our method</td>
<td>0.6943</td>
<td>0.7550</td>
<td>0.7204</td>
</tr>
<tr>
<td>NLP-paper-to-QA-generation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BM25</td>
<td>0.7098</td>
<td>0.7399</td>
<td>0.7234</td>
</tr>
<tr>
<td>Embedding retrieval</td>
<td>0.7065</td>
<td>0.7391</td>
<td>0.7221</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.6953</td>
<td>0.7261</td>
<td>0.7095</td>
</tr>
<tr>
<td>KAG</td>
<td>0.7074</td>
<td>0.7361</td>
<td>0.7204</td>
</tr>
<tr>
<td>MindMap</td>
<td>0.7158</td>
<td>0.7461</td>
<td>0.7295</td>
</tr>
<tr>
<td>Our method</td>
<td>0.7181</td>
<td>0.7448</td>
<td>0.7300</td>
</tr>
</tbody>
</table>
<p>In order to test the generalization of our proposed method in NLP domain scientific research question answering, we conduct the test on two datasets. The results are shown in Table 1. It can be seen that BERTScore shows similar results across the various methods, but our 'sub-graph community summary' method still outperforms other baseline models on F1-score, due to the fact that the paper elements in our NLP-AKG provide more accurate and specific supplementary information to answer the question. And MindMap, which uses the knowledge graph we have constructed, also outperforms GPT-4 and other baselines, proving the validity of our knowledge graph in augmenting LLM question answering.</p>
<p>Table 2: Performance of our method and baselines on the annotated test dataset.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td>BM25</td>
<td>0.6890</td>
<td>0.7097</td>
<td>0.6989</td>
</tr>
<tr>
<td>Embedding retrieval</td>
<td>0.6878</td>
<td>0.7117</td>
<td>0.6992</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.7638</td>
<td>0.7670</td>
<td>0.7654</td>
</tr>
<tr>
<td>KAG</td>
<td>0.6819</td>
<td>0.7797</td>
<td>0.7275</td>
</tr>
<tr>
<td>MindMap</td>
<td>0.6985</td>
<td>0.7039</td>
<td>0.7006</td>
</tr>
<tr>
<td>Our method</td>
<td>0.7646</td>
<td>0.7912</td>
<td>0.7768</td>
</tr>
</tbody>
</table>
<p>Table 2 results show that the method proposed by us shows a relative improvement, which is 1.1% compared with GPT-4 on F1-score, and compared with the current better knowledge graph enhanced LLM question answering methods MindMap and KAG, which are improved by 7.7% and 4.9% on F1-score respectively, because the knowledge graph retrieval and enhanced question answering method designed by us is more applicable our NLPAKG structure improves the retrieval efficiency and makes full use of the deep connections between papers.</p>
<p>Because MindMap limits the number of hops in path retrieval, it is easy to miss when conducting multi-paper question answering, and the performance effect is not as good as GPT-4, which uses a lot of paper information for pre-training.</p>
<h2>5 Conclusions</h2>
<p>This paper introduces a knowledge graph framework for few-shot NLP scientific literature using LLM. It extracts entities from the ACL Anthology corpus to build NLP-AKG, an NLP domain academic knowledge graph, including metadata, semantics elements, and citation networks. A 'subgraph community summary' method is proposed, enabling LLM to focus on relevant paper communities for accurate answers. Experiments show superior performance in NLP literature question answering, especially for multi-paper summarization, outperforming baselines like GPT-4 and MindMap, with generalization across datasets.</p>
<h2>6 Limitations</h2>
<p>Although the proposed method has achieved remarkable results in the question-answering task for scientific literature in the field of NLP, there are still some limitations that need to be further improved and optimized in future research. Firstly, the volume of scientific literature is vast and frequently updated, making it an urgent issue to dynamically update and expand the knowledge graph without reconstructing the entire graph. While the method reduces reliance on manual annotation to some extent, it still requires human intervention to ensure the quality of the knowledge graph when dealing with papers in new fields. Future work should investigate automated knowledge graph update mechanisms, particularly for handling dynamic scientific literature streams while maintaining extraction accuracy. Secondly, the experiments are primarily based on scientific literature in the NLP field. Although multiple datasets were used for testing, the generalization capability of the method still needs further validation in other disciplines (such as medicine, materials science, etc.). How to extend this method to other fields and construct crossdomain knowledge graphs is a direction worth exploring.</p>
<h2>References</h2>
<p>Shubham Agarwal, Issam H Laradji, Laurent Charlin, and Christopher Pal. 2024. Litllm: A toolkit for scientific literature review. arXiv preprint arXiv:2402.01788.</p>
<p>Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. 2024. LitSearch: A retrieval benchmark for scientific literature search. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15068-15083, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. In Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pages 78-106, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. M3embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfknowledge distillation. In Findings of the Association for Computational Linguistics: ACL 2024,
pages 2318-2335, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Falcon Z. Dai. 2020. Word2vec conjecture and a limitative result.</p>
<p>Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599-4610, Online. Association for Computational Linguistics.</p>
<p>Xinyu Du and Ning Li. 2022. Academic paper knowledge graph, the construction and application. In ICBASE, pages 15-27.</p>
<p>Jiayan Guo, Lun Du, and Hengyu Liu. 2023. GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. arXiv preprint arXiv:2305.15066.</p>
<p>Xuejie Hao, Zheng Ji, Xiuhong Li, Lizeyan Yin, Lu Liu, Meiying Sun, Qiang Liu, and Rongjin Yang. 2021. Construction and application of a knowledge graph. Remote Sensing, 13(13).</p>
<p>Nourhan Ibrahim, Samar Aboulela, Ahmed Ibrahim, and Rasha Kashef. 2024. A survey on augmenting knowledge graphs ( kgs ) with large language models (llms): models, evaluation metrics, benchmarks, and challenges. Discover Artificial Intelligence, 4(1):76.</p>
<p>Mohamad Yaser Jaradeh, Allard Oelen, Manuel Prinz, Markus Stocker, and Sören Auer. 2019. Open research knowledge graph: a system walkthrough. In Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries, TPDL 2019, Oslo, Norway, September 912, 2019, Proceedings 23, pages 348-351. Springer.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.</p>
<p>Feng Jiang, Kuang Wang, and Haizhou Li. 2024. Bridging research and readers: A multi-modal automated academic papers interpretation system. arXiv preprint arXiv:2401.09150.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, et al. 2024. Kag: Boosting llms in professional domains via knowledge augmented generation. arXiv preprint arXiv:2409.13731.</p>
<p>Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. 2023. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv preprint arXiv:2305.18703.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Ishani Mondal, Yufang Hou, and Charles Jochim. 2021. End-to-end construction of NLP knowledge graph. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1885-1895, Online. Association for Computational Linguistics.</p>
<p>Shi Na, Liu Xumin, and Guan Yong. 2010. Research on k-means clustering algorithm: An improved k-means clustering algorithm. In 2010 Third International Symposium on intelligent information technology and security informatics, pages 63-67. Ieee.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback.</p>
<p>Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi. 2023. Progressive prompts: Continual learning for language models. arXiv preprint arXiv:2301.12314.</p>
<p>Shaurya Rohatgi. 2022. Acl anthology corpus with full text. Github.
M. Sasaki, N. Watanabe, and T. Komanaka. 2024. Enhancing contextual understanding of mistral llm with external knowledge bases.</p>
<p>Tim Schopf and Florian Matthes. 2024. NLP-KG: A system for exploratory search of scientific literature in natural language processing. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 127-135, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Anil Sharma and Suresh Kumar. 2023. Ontologybased semantic retrieval of documents using word2vec model. Data \&amp; Knowledge Engineering, 144:102110.</p>
<p>Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990998.</p>
<p>Yilin Wen, Zifeng Wang, and Jimeng Sun. 2024. MindMap: Knowledge graph prompting sparks graph of thoughts in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10370-10388, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Dayong Wu, Jiaqi Li, Baoxin Wang, Honghong Zhao, Siyuan Xue, Yanjie Yang, Zhijun Chang, Rui Zhang, Li Qian, Bo Wang, Shijin Wang, Zhixiong Zhang, and Guoping Hu. 2024. SparkRA: A retrievalaugmented knowledge service system based on spark large language model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 382-389, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Zhilin Yang. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. CoRR, abs/1906.08237.</p>
<p>Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, and Jure Leskovec. 2022. Deep bidirectional language-knowledge graph pretraining. Advances in Neural Information Processing Systems, 35:3730937323.</p>
<p>Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao Gu, Yan Wang, Bin Shao, Rui Li, et al. 2019. Oag: Toward linking largescale heterogeneous entity graphs. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pages 25852595.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert.</p>
<p>Zhehao Zhang, Yan Gao, and Jian-Guang Lou. 2024. $\varepsilon^{5}$ : Zero-shot hierarchical table analysis using augmented LLMs via explain, extract, execute, exhibit and extrapolate. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1244-1258, Mexico City, Mexico. Association for Computational Linguistics.</p>
<h1>A Knowledge Graph Schema</h1>
<p>In this section, we propose the schema construction of the NLP domain literature knowledge graph. We creatively build the knowledge graph around the elements in papers and the concepts and terms in the field. This schema contains 15 types of entities, which reflect the important objective information and the core elements of the paper. In addition to the long and specified elements of the paper, such as author and institution, we also extracted the elements of innovation, problem, and method, which contain semantic information that can summarize the main content of the paper. We innovatively used summary sentences rather than the text contained in the paper to describe them more concisely and easy to understand. Detailed relation information is given in the Table 3.</p>
<p>Table 3: Knowledge graph entities and related introduction</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity</th>
<th style="text-align: left;">Introduction</th>
<th style="text-align: left;">Number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Title</td>
<td style="text-align: left;">The title of the paper</td>
<td style="text-align: left;">60826</td>
</tr>
<tr>
<td style="text-align: left;">Author</td>
<td style="text-align: left;">The author of the paper</td>
<td style="text-align: left;">77136</td>
</tr>
<tr>
<td style="text-align: left;">Institution</td>
<td style="text-align: left;">Author's institution</td>
<td style="text-align: left;">20747</td>
</tr>
<tr>
<td style="text-align: left;">Conference</td>
<td style="text-align: left;">conference or journal in which papers are published</td>
<td style="text-align: left;">2132</td>
</tr>
<tr>
<td style="text-align: left;">Date</td>
<td style="text-align: left;">Date of publication of the paper</td>
<td style="text-align: left;">485</td>
</tr>
<tr>
<td style="text-align: left;">Field</td>
<td style="text-align: left;">The research field of the paper</td>
<td style="text-align: left;">9581</td>
</tr>
<tr>
<td style="text-align: left;">Keywords</td>
<td style="text-align: left;">The topic phrase of the paper</td>
<td style="text-align: left;">60826</td>
</tr>
<tr>
<td style="text-align: left;">innovation</td>
<td style="text-align: left;">The main innovation points of the paper</td>
<td style="text-align: left;">60826</td>
</tr>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">The method proposed in the paper</td>
<td style="text-align: left;">60826</td>
</tr>
<tr>
<td style="text-align: left;">Problem</td>
<td style="text-align: left;">The problem mainly solved by the paper</td>
<td style="text-align: left;">60826</td>
</tr>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">The specific name of the model proposed in the paper</td>
<td style="text-align: left;">29389</td>
</tr>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: left;">Specific tasks of model application</td>
<td style="text-align: left;">45944</td>
</tr>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: left;">Dataset used in the experiment</td>
<td style="text-align: left;">25419</td>
</tr>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">Metric used in the experiment</td>
<td style="text-align: left;">9097</td>
</tr>
<tr>
<td style="text-align: left;">Result</td>
<td style="text-align: left;">The main experimental results of the model</td>
<td style="text-align: left;">96293</td>
</tr>
</tbody>
</table>
<p>We focus on the following 29 types of relations between entities, detailed relation information is given in the Table 4. The title and all elements within the same paper maintain corresponding relations, and papers also establish relations with one another through citations. For ease of use, we decided to classify reference relations into two categories: direct use and task related. Because the same paper is repeatedly cited, there may be two relations between the paper and the cited paper, and finally, we delete the duplicate triples.</p>
<p>Direct use: The paper directly uses the content proposed in the cited paper, including datasets, evaluation metrics, models, architectures, etc.</p>
<p>Task related: The paper belongs to a similar task as the cited paper, and the background of the task is explained with the help of the cited paper during the discussion process.</p>
<p>Some sample triples are shown in the Table 5.
Table 4: relation Names and Types</p>
<table>
<thead>
<tr>
<th style="text-align: left;">relation Name</th>
<th style="text-align: left;">relation Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Intra-paper relations</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">writes</td>
<td style="text-align: left;">Author -&gt; Title</td>
</tr>
<tr>
<td style="text-align: left;">works for</td>
<td style="text-align: left;">Author -&gt; Institution</td>
</tr>
<tr>
<td style="text-align: left;">publishes</td>
<td style="text-align: left;">Title -&gt; Conference</td>
</tr>
<tr>
<td style="text-align: left;">is written in</td>
<td style="text-align: left;">Title -&gt; Date</td>
</tr>
<tr>
<td style="text-align: left;">belongs to</td>
<td style="text-align: left;">Title -&gt; Research Field</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>relation Name</th>
<th>relation Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>keywords</td>
<td>Title -&gt; Keywords</td>
</tr>
<tr>
<td>solves</td>
<td>Title -&gt; Problem, Method -&gt; Problem, Model -&gt; Problem</td>
</tr>
<tr>
<td>adopts</td>
<td>Title -&gt; Method</td>
</tr>
<tr>
<td>proposes</td>
<td>Title -&gt; Model, Method -&gt; Model</td>
</tr>
<tr>
<td>works on</td>
<td>Title -&gt; Task, Method -&gt; Task, Model -&gt; Task</td>
</tr>
<tr>
<td>innovates</td>
<td>Title -&gt; Innovation, Method -&gt; Innovation</td>
</tr>
<tr>
<td>experiments on</td>
<td>Title -&gt; Dataset, Task -&gt; Dataset, Model -&gt; Dataset</td>
</tr>
<tr>
<td>uses</td>
<td>Title -&gt; Metric, Task -&gt; Metric, Model -&gt; Metric</td>
</tr>
<tr>
<td>faces</td>
<td>Task -&gt; Problem</td>
</tr>
<tr>
<td>achieves</td>
<td>Title -&gt; Result, Method -&gt; Result, Model -&gt; Result</td>
</tr>
<tr>
<td>Inter-paper relations</td>
<td></td>
</tr>
<tr>
<td>Direct use</td>
<td>Title -&gt; Title</td>
</tr>
<tr>
<td>Task correlation</td>
<td>Title -&gt; Title</td>
</tr>
</tbody>
</table>
<p>Table 5: Examples of partial triples</p>
<table>
<thead>
<tr>
<th>Subject</th>
<th>Predicate</th>
<th>Object</th>
</tr>
</thead>
<tbody>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>date</td>
<td>2012 December</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>keywords</td>
<td>Arabic Semantic Role Labeling, <br> Natural Language Processing, <br> Argument Classification</td>
</tr>
<tr>
<td>Laurel Hart</td>
<td>writes</td>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
</tr>
<tr>
<td>Laurel Hart</td>
<td>works for</td>
<td>BCL Technologies</td>
</tr>
<tr>
<td>Hassan Alam</td>
<td>writes</td>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
</tr>
<tr>
<td>Hassan Alam</td>
<td>works for</td>
<td>BCL Technologies</td>
</tr>
<tr>
<td>Aman Kumar</td>
<td>writes</td>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
</tr>
<tr>
<td>Aman Kumar</td>
<td>works for</td>
<td>BCL Technologies</td>
</tr>
<tr>
<td>Proceedings of COLING 2012: <br> Demonstration Papers</td>
<td>publishes</td>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>belongs to</td>
<td>Semantic Role Labeling</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>solves</td>
<td>Most existing SRL systems and <br> methodologies are designed for English <br> and adapted for Arabic, lacking in <br> specialized development for the <br> Arabic language.</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>adopts</td>
<td>A specialized Arabic Semantic Role <br> Labeling system tailored to Arabic's <br> unique linguistic features is proposed <br> to improve predicate argument boundary <br> detection and argument classification.</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>works on</td>
<td>Arabic Semantic Role Labeling</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Subject</th>
<th>Predicate</th>
<th>Object</th>
</tr>
</thead>
<tbody>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>innovates</td>
<td>To solve the problem of adapting <br> English-centric SRL systems for Arabic, <br> a tailored Arabic Semantic Role Labeling <br> system is innovatively proposed to <br> leverage the unique features of the <br> Arabic language, aiming to enhance <br> predicate argument boundary detection <br> and argument classification.</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>direct use</td>
<td>CUNIT: A Semantic Role Labeling <br> System for Modern Standard Arabic</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>task related</td>
<td>Automatic Labeling of Semantic Roles</td>
</tr>
<tr>
<td>Revisiting Arabic Semantic Role <br> Labeling using SVM Kernel Methods</td>
<td>direct use</td>
<td>Semantic Role Labeling Systems <br> for Arabic using Kernel Methods</td>
</tr>
</tbody>
</table>
<p>Figure 3 is a local example of the knowledge graph we construct. We sample all semantic element entities and inter-paper relations extracted from 100 papers, and manually check the extraction accuracy of each category, regardless of omissions. The specific results are shown in the Table 6.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: a local example of the knowledge graph</p>
<p>Table 6: Entity and Inter-paper Relation Manual Sampling Test Result</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity and Inter-paper Relation</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entity</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Title</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Author</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Institution</td>
<td style="text-align: left;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">Conference</td>
<td style="text-align: left;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">Date</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Field</td>
<td style="text-align: left;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">Keywords</td>
<td style="text-align: left;">0.97</td>
</tr>
<tr>
<td style="text-align: left;">innovation</td>
<td style="text-align: left;">0.85</td>
</tr>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">Problem</td>
<td style="text-align: left;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">0.97</td>
</tr>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: left;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: left;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">0.94</td>
</tr>
<tr>
<td style="text-align: left;">Result</td>
<td style="text-align: left;">0.78</td>
</tr>
<tr>
<td style="text-align: left;">Inter-paper relation</td>
<td style="text-align: left;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">Direct use</td>
<td style="text-align: left;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">Task Related</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h1>B Prompts and Details</h1>
<p>In this section, we collate the prompts we used in our knowledge graph construction and augmentation for LLM question answering, as well as some implementation details.</p>
<h2>B. 1 Entity Extraction</h2>
<p>We used the corpus provided by Rohatgi (2022), which they crawled to obtain all PDF documents and metadata from their website. Grobid is used to process the PDF of papers and convert it into structured XML/TEI coded documents. After deleting the unrecognized papers, a total of 60,826 processed structured documents and their corresponding metadata are obtained, which are uniquely identified by Anthology ID.</p>
<p>Table 7: Prompt of Extracting Field, Keywords, Problem, Method, Model, Task</p>
<h1>Prompt</h1>
<h2>Field</h2>
<ul>
<li>Summarize the research field of the paper (e.g., 'Knowledge graph', 'Picture classification').</li>
<li>Example: 'Field': 'object detection'</li>
</ul>
<h4>Abstract</h4>
<p>Keywords</p>
<ul>
<li>Provide 3-5 keywords summarizing the content, topic, and field of the paper. - Example: {'Keywords':['Large Selective Kernel Network', 'Remote Sensing Object Detection', 'Deep Learning']}</li>
</ul>
<h2>Problem</h2>
<ul>
<li>Describe the problem the paper focuses on solving (e.g., shortcomings of existing methods, challenges in the task, factors affecting performance).</li>
<li>Summarize in one sentence (max 50 words).</li>
<li>Example: 'Problem': 'Current methods fail to effectively incorporate the wide-range context of various objects in remote sensing images during object detection. '</li>
</ul>
<h2>Method</h2>
<ul>
<li>Summarize the method proposed in the paper to solve the problem (e.g., model or framework).</li>
<li>Summarize in one sentence (max 50 words).</li>
<li>Example: 'Method': 'The paper introduces a Large Selective Kernel Network that dynamically adjusts the receptive field in the feature extraction backbone to effectively model the context of different objects, giving improved object detection performance. '</li>
</ul>
<h2>Model</h2>
<ul>
<li>Name the model or framework proposed in the paper.</li>
<li>Example: 'Model': 'Large Selective Kernel Network (LSKNet)'</li>
</ul>
<p>Task</p>
<ul>
<li>Specify the task addressed by the paper.</li>
<li>Example: 'Task': 'Remote sensing object detection'</li>
</ul>
<p>Table 8: Prompt of Extracting Dataset, Metric, Result</p>
<h1>Prompt</h1>
<h2>Paper Screening</h2>
<ul>
<li>Determine whether the article proposes its own model and conducts tests, or merely evaluates and compares other methods.</li>
<li>If the article proposes its own model, output the name or description of the model.</li>
<li>If not, output "NO".</li>
<li>Example:</li>
</ul>
<p>Model: Large Selective Kernel Network (LSKNet)</p>
<h2>Table Screening</h2>
<ul>
<li>Identify the table that contains the main results of the proposed model on the test dataset.</li>
<li>The main results do not include ablation experiments or research experiments.</li>
<li>Output the table number corresponding to the main results (e.g., 'Table 0').</li>
<li>Example: Main Results Table: Table 2</li>
</ul>
<h2>Dataset and Metrics Extraction</h2>
<ul>
<li>Extract the experimental datasets, evaluation metrics, and results from the main results table.</li>
<li>Ensure the results are from the model proposed in the paper.</li>
<li>Format: (dataset, metric, result).</li>
<li>Example:
(PubMedQA, accuracy, 96.7\%)
(CIFAR-10, F1-score, 89.2\%)</li>
</ul>
<p>Table 9: Prompt of Extracting Innovation</p>
<h2>Prompt</h2>
<h2>Innovation</h2>
<ul>
<li>To solve the [problem] existing in the [task], the [method] is innovatively proposed and the [results] are obtained.</li>
<li>Summarize the [problem] from the problem extracted from the previous summary and the introduction of the paper.</li>
<li>Summarize the [method] from the method extracted from the previous summary and the introduction of the paper.</li>
<li>Summarize the [task] from the task extracted from the previous summary and the introduction of the paper.</li>
<li>Summarize the [results] from the experiment summarized earlier.</li>
</ul>
<h1>B. 2 Entities Cleaning and Disambiguation</h1>
<p>In the process of extracting paper elements, LLMs inevitably produce the phenomenon that the same paper element is described differently, so it is necessary to disambiguate the extracted entities. Among all the paper element categories, some paper element categories such as problem and method reflect the unique attributes of the paper and do not need to be disambiguation. We first label a batch of wrong examples for training. Error types include invalid data, incorrect formatting, not specific enough, redundant information, etc, and then use these wrong examples to fine-tune XLNet to obtain a detector for detecting and classifying the wrong examples. If the category is invalid data, the result is deleted, otherwise, the error result, error type, and original extraction prompt are provided to LLM for re-extraction.</p>
<p>We use K-means ( Na et al., 2010) to disambiguate entities by assigning similar entities to the same cluster. Firstly, the entities in the knowledge graph are sampled, and the tokenizer is used to perform word embedding on the entities in this part, and they are mapped to the same vector space for clustering analysis. The entity with the highest frequency is selected as the representative entity of the whole cluster, and then LLM is used to extend the clustering result to the whole knowledge graph.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Sub-graph community summary method diagram</p>
<h2>B. 3 Sub-graph Community Summary Method</h2>
<p>We use LLM to identify the intent of the question and identify the relation between the several paper elements involved in the question. Specifically, we use a three-part prompt: the question to be analyzed, related paper element categories, and examples. After determining the relevant elements involved in the relevant elements and the target elements to be queried, we can directly determine the corresponding path between the relevant elements and the target elements without the need for path retrieval, because there is a definite correspondence between the relation between the elements of the paper. The paper sub-graph corresponding to the paper elements we query through the path is all the target element nodes connected with the title. If there is a relation between these papers, we believe that these papers have a deeper correlation with each other and can form a community. Next, we link the question, sub-graph community elements, and element-related introduction to prompt LLM to generate community answers for the sub-graph community. Finally, we simultaneously summarize all community answers and questions into the final global answer using LLM. See Figure 4 for a schematic diagram of the method.</p>
<p>Table 10: Prompt of Extracting Dataset, Metric, Result</p>
<h1>Prompt</h1>
<h2>Intent Extraction</h2>
<ul>
<li>Extract key entities in the NLP domain from the given input and categorize them into:
title, task, field, method, problem, model, dataset</li>
<li>Determine what kind of problems, methods, models, datasets, applications, and research the question is asking.</li>
<li>Do not split entities or remove any parts of them.</li>
<li>Example:</li>
</ul>
<p>Entities: text summarization (task), control the length of text summarization (problem)</p>
<p>Question: method</p>
<h2>Community Results</h2>
<ul>
<li>Use additional paper elements to answer the question:
title: Title of the paper.
task: Task of the paper.
field: Research field of the paper.
method: Method of the paper.
problem: Problem the paper aims to solve.
model: Model used in the paper.
dataset: Dataset used in the paper.</li>
<li>Ensure answers are concise and do not contain explanatory text.</li>
</ul>
<h2>Final Results</h2>
<ul>
<li>Summarize the provided partial results into an overall result for the NLP domain problem.</li>
<li>Include as much detail as possible from all partial results.</li>
<li>Refine the results to address the questions asked.</li>
</ul>
<h2>C Example Questions</h2>
<p>We annotated a dataset of 20 multi-paper question-answer pairs with three NLP practitioners, and our research team verified the accuracy of the answers. In this section, we provide examples of some of the problems in the dataset we annotated, along with their annotated results.</p>
<ol>
<li>Q: In the open domain Question and Answer field, what are the solutions to the problem of long processing times?
A: To address the problem of excessive processing time in the Open Domain question answering (ODQA) field, researchers have proposed a variety of solutions aimed at speeding up the entire processing process from question to answer. In terms of accelerated evidence search, the use of hierarchical navigable Small World Maps (HNSW) and approximate nearest neighbor (ANN) search techniques, such as locally sensitive hashing (LSH) and inverted file (IVF), can quickly find documents similar to queries, although this may sacrifice some accuracy, but greatly improve the efficiency of retrieval. In terms of accelerating evidence reading, skip reading strategies reduce</li>
</ol>
<p>unnecessary computational effort by dynamically identifying and skipping text blocks that are less relevant to the problem, while adaptive computing (AC) dynamically allocates computational resources based on the importance of text blocks, thereby optimizing the inference process. At the same time, reducing the size of indexes and models is also an important strategy, which can reduce the size of indexes through learned hashing techniques and product quantization (PQ), and reduce the size of models through model compression and pruning, knowledge distillation and other methods to reduce the computational burden of reasoning. Other studies have proposed a single-stage framework to generate or retrieve answers directly, skipping the evidence retrieval and reading process to further accelerate processing. In addition, lightweight BERT variants such as MobileBERT are also used to reduce resource consumption while maintaining performance. In combination with these approaches, latency can be further reduced through GPU acceleration and optimized implementation of models and services. Together, these technologies improve the feasibility and practicality of ODQA systems in real-time or near-real-time scenarios.
2. Q: In the field of text summarization, what are the solutions to improve interpretability and explainability?
A: In the field of text summarization, methods to improve explainability and explainability focus on explaining the process of generating abstracts, especially in the context of abstractive summarization. Because abstract summary generation requires understanding text semantics and generating new abstracts that differ from the original text, complex neural network models are often used, which are known as "black box models" because of their opacity. To address this issue, many studies have explored interpretability techniques in order to enable end users to understand and trust the generation process. For example, pre-trained language models (PLMs) such as BERT and T5 are widely used for text summarization tasks, and several studies in recent years have enhanced the performance and interpretability of these models by introducing graph neural network topic models and domain knowledge. In addition, inherently interpretable models such as GAMI are used in extractive summarization, and although they are not as good as modern black box models in terms of performance, they provide transparency in the decision-making process.
3. Q: What is the research on sequential recommendation tasks in the field of recommendation system combined with LLM?
A: In the field of recommendation systems combined with LLM, the research on processing sequential recommendation tasks mainly focuses on how to effectively use the user's interaction history with the project to make predictions. Researchers typically populate a user and item sequence into a prompt, such as "Given a user's interaction history, predict which item the user will interact with," and then have LLM generate the next item ID as a prediction. This approach leverages the language generation capabilities of LLM to handle sequential recommendation tasks. To improve reasoning efficiency, researchers often truncate older items before filling in the item sequence, reducing the input length. In this area, some studies use LLM to generate candidates for further screening, while others focus on providing candidates for recommendation through LLM. In addition, there is some research to optimize recommendation quality by guiding LLMS to determine whether users will like a particular item. In general, these studies have explored how to make better use of sequence information for sequential recommendation by inputting users' historical interaction sequences into LLM, and further optimized the performance of the recommendation system through candidate selection and user preference judgment.
4. Q: In the field of active learning in natural language processing, what are the methods to solve the sample selection problem based on mixed strategies?
A: In the field of active learning in natural language processing, hybrid strategies solve the problem of sample selection by combining information and representativeness. A simple combination method involves combining multiple criteria into a single selection criterion by weighting and or multiplication. For example, indeterminity-weighted clustering and gradient-based diversity selection methods can consider both uncertainty and diversity. In addition, multi-step query strategy is also</p>
<p>widely used, firstly filtering samples according to uncertainty, and then selecting diversified batch samples by clustering method. Another approach is to select the most uncertain sample in each cluster. Unlike static combination strategies, dynamic combination methods can flexibly switch strategies according to different stages of active learning, for example, representative methods may be preferred in the early stages of active learning, while uncertainty sampling may be more relied on in the later stages. Dynamic strategies like DUAL and GraDUAL are able to switch between different stages to improve the efficiency of sample selection. These hybrid strategies effectively improve the performance of active learning at different stages by integrating multiple criteria.
5. Q: In the field of machine translation, what are the ways to solve the translation problem of Nigerian language through neural network model for low resource cases?
A: In the field of machine translation of Nigerian languages, the application of neural network models is mainly through several methods to solve the translation problem of low-resource languages. First, a Transformer-based neural machine translation model is used, which translates through an encoder-decoder architecture and multi-head self-attention mechanism. Through the training of Nigerian languages such as Edo and Pidchin, the research shows that the use of subword-level Byte-Pair encoding (BPE) and word-level word segmentation can improve the translation quality, especially in the language with large data volume. Second, transfer learning techniques are widely used in translation tasks for low-resource languages to improve named entity recognition and topic classification performance for Nigerian languages such as Hausa and Yoruba by fine-tuning multilingual models (such as mBERT and XLM-RoBERTa) over high-resource language models. These models can achieve better translation results even with a small amount of labeled data, which shows the potential and wide application prospect of neural networks in low-resource language machine translation.
6. Q: What kinds of biases exist in the field of LLM debiasing?</p>
<p>A: In the field of LLM debias, there are mainly several forms of bias: First, local bias is manifested by the difference in the relevance between the word and the context, such as sexism in the prediction of the next word in a gender-related sentence. Second, global bias involves the emotional disposition of the entire text and may show a biased feeling toward one gender. In machine translation, models often default to using male words in ambiguous situations, ignoring the possibility of female forms. For information retrieval, the model may return more documents related to men, even if the query does not specify a gender. In question answering systems, models may rely on stereotypes to answer questions, such as associating a particular race with negative behavior. In natural language reasoning, models may rely on false stereotypes leading to invalid reasoning, misjudging the relation between premises and conclusions. Finally, in the classification task, the toxicity detection model often mistakenly labeled African American English tweets as negative, more often than standard American English tweets. These biases reflect the prevalence of gender and racial discrimination in AI applications, underscoring the importance of de-bias technology.</p>
<h1>D Experiment Details</h1>
<h2>D. 1 Implementation of Datasets</h2>
<ul>
<li>QASPER is a dataset for question answering on scientific research papers. It consists of 5,049 questions over 1,585 NLP papers. Each question is written by an NLP practitioner who reads only the title and abstract of the corresponding paper, and the question seeks information present in the full text. ${ }^{3}$ We select 280 questions in the test set corresponding to papers that overlapped with our constructed knowledge graph.</li>
<li>NLP-Paper-to-QA-Generation extracts the abstract, and introduction of each NLP paper from QASPER dataset and also extracts only the rows labeled question and answer that had an abstract</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>answer rather than extractive. ${ }^{4}$ We select 274 questions in the test set corresponding to papers that overlapped with our constructed knowledge graph.</p>
<h1>D. 2 Implementation of Baselines</h1>
<ul>
<li>GPT-4 is tested as a representative of LLM baseline, with gpt-4-06135 (Guo et al., 2023) API. In the two open-source datasets, we provide the LLM with abstracts and paper text fragments, and in the multi-paper question answering dataset, we provide nothing.</li>
<li>BM25 document retrieval is a probabilistic retrieval method (Peng et al., 2023) that ranks documents based on query-document relevance, balancing term frequency and inverse document frequency. It effectively handles varying document lengths and query sizes while preventing overemphasis on highfrequency terms. We used the original paper as the retrieved document. For each question query, we retrieve the top $k$ gold document contexts based on bm25 scores.</li>
<li>Text embedding document retrieval is similar to BM25 document retrieval methods (Sharma and Kumar, 2023), text embedding-based document retrieval approaches also identify the top $k$ documents for each query. However, the key distinction lies in the use of a word2vec embedding model (Dai, 2020), which is trained on the document corpus to serve as the foundation for ranking documents.</li>
<li>Knowledge-Augmented Generation (KAG) is a framework to augment LLMs by combining knowledge graphs and Retrieval Augmented Generation (Liang et al., 2024). Unlike traditional RAG, which mainly relies on vector similarity, KAG integrates structured knowledge reasoning. Pure vector similarity retrieval can not handle the problem that requires multi-hop inference well. We use neo4j ${ }^{6}$ as the knowledge base, bge-m3 (Chen et al., 2024) in SiliconFlow ${ }^{7}$ is used for the representation model. We use all the original papers as a corpus and provide our knowledge graph schema to KAG for entity and relation extraction. KAG will automatically retrieve relevant entities from the knowledge graph it built to answer the question.</li>
<li>MindMap leveraging knowledge graphs to enhance the reasoning and transparency of LLMs (Wen et al., 2024), this method enables LLMs to understand KG inputs and perform reasoning by integrating implicit and external knowledge in the form of MindMaps. We modified the entity extraction and question answering prompts for the dataset of papers we used, and other parts remained the same as the original paper.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>4 https://huggingface.co/datasets/UNIST-Eunchan/NLP-Paper-to-QA-Generation
5 https://openai.com/gpt-4
6 neo4j://release-openspg-neo4j:7687
7 https://account.siliconflow.cn/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
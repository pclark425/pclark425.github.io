<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8827 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8827</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8827</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271924376</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.12163v2.pdf" target="_blank">Preference-Guided Reflective Sampling for Aligning Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Iterative data generation and model re-training can effectively align large language models (LLMs) to human preferences. The process of data sampling is crucial, as it significantly influences the success of policy improvement. Repeated random sampling is a widely used method that independently queries the model multiple times to generate outputs. In this work, we propose a more effective sampling method, named Preference-Guided Reflective Sampling (PRS). Unlike random sampling, PRS employs a tree-based generation framework to enable more efficient sampling. It leverages adaptive self-refinement techniques to better explore the sampling space. By specifying user preferences in natural language, PRS can further optimize response generation according to these preferences. As a result, PRS can align models to diverse user preferences. Our experiments demonstrate that PRS generates higher-quality responses with significantly higher rewards. On AlpacaEval and Arena-Hard, PRS substantially outperforms repeated random sampling in best-of-N sampling. Moreover, PRS shows strong performance when applied in iterative offline RL training.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8827.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8827.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preference-Guided Reflective Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-based generation and sampling method that uses model-generated language feedback and iterative refinements to produce responses aligned to an explicit user preference; designed to improve data generation for iterative offline RLHF and best-of-N selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B + SFT (primary); evaluated across 9 policy models including Mistral-7B, Tulu-2, Vicuna-13B, Llama-3-8b-instruct, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied primarily with Llama-2-13B fine-tuned on ShareGPT SFT data (LoRA for RL finetuning); experiments also run across nine open-source instruction-following policy models of sizes ~7B–13B (e.g., Mistral-7B, Tulu-2-13B-DPO, Vicuna-13B, Llama-3-8b-instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Preference-Guided Reflective Sampling (PRS)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial set Y0 of responses conditioned on input x and explicit preference z (y0 ∼ πθ(y0|x,z)), select best y*0 by reward R(x,z,y), have the model produce natural-language feedback f (f ∼ πθ(f|x,z,y*0)), then sample refinements Y1 conditioned on x,z,y*0,f (y1 ∼ πθ(y1|x,z,y*0,f)); use a tree-based multi-layer explore/exploit procedure (typically two layers in experiments) to merge Y0 and Y1 and pick top responses for training or evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Preference-controlled instruction following and keyword-focused document summarization; offline RLHF / best-of-N selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Instruction-following (evaluated on AlpacaEval, Arena-Hard) and summarization (CNN/DailyMail); PRS is used to generate training data for iterative offline RL training and to perform best-of-N sampling at inference/data-generation time, with preferences z appended to prompts when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Across multiple policy models and benchmarks PRS produced higher-reward samples and improved downstream evaluation metrics: the paper reports that PRS "generates higher-quality responses with significantly higher rewards" and "substantially outperforms repeated random sampling in best-of-N sampling" on AlpacaEval and Arena-Hard. Example numeric values shown in the paper (Table 2 / text) include illustrative comparisons such as Rand (Bo-16) ≈ 23.90 vs PRS (Bo-16) ≈ 27.19 on one AlpacaEval condition and similar Bo-32 improvements; PRS also achieves higher LC/WR and WR metrics on AlpacaEval and Arena-Hard in Table 2, and PRS-trained models reach higher AlpacaEval / Rouge scores in offline RL experiments (see Tables 2–4 and Fig. 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Repeated random sampling (Rand) and preference-conditioned random sampling (PRand) produce lower average rewards and weaker downstream performance; random sampling often halted improvement after one RL iteration while PRS (and PRand) continued to improve across 3 RL iterations. Exact baseline numbers appear in Tables 2–4 (e.g., Rand metrics in best-of-N rows are consistently below PRS counterparts).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented via prompt-engineered self-feedback: the same LLM policy generates a textual feedback message f for a selected candidate y*0, and that feedback is appended to context when sampling refinements. A reward model R(x,z,y) is used to select the best candidates at each node of the tree (exploit), and refinements from the chosen node provide exploration; no separate external correction module beyond the reward model is required.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Higher mean and variance of reward distributions for samples from PRS vs PRand and Rand (Fig. 4); consistent superiority in best-of-N evaluations on AlpacaEval and Arena-Hard (Table 2); PRS yields higher reward training data across 9 policy models (Fig. 9) and models trained with PRS outperform PRand and Rand in head-to-head GPT-4 evaluations after iterative offline RL (Fig. 10 and Fig. 5). Qualitative: PRS better adapts to explicit preferences and reduces toxicity when safe preferences are appended (Table 5 / toxicity results).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) language-feedback (f) yields mixed improvements depending on the policy model — some models improve while others do not; (2) for harder tasks such as complex reasoning, the model may struggle to self-improve and PRS was not explored for such tasks in this work; (3) susceptibility to reward hacking is noted as a potential concern; (4) refinements (Y1) do not consistently outperform initial samples (Y0) for all models — improvement rates vary across models (e.g., <50% for some); (5) when preferences are omitted or feedback generation is skipped, some trade-offs exist (though PRS variants still outperform baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared to repeated random sampling (Rand), preference-conditioned random sampling (PRand), and an enhanced greedy iterative-refinement baseline; PRS outperforms Rand and PRand in best-of-N and in downstream RL training, and outperforms the greedy baseline which suffers from poor exploration. The paper also contrasts PRS to prior reflection/self-improvement works (Self-refine, Reflexion, Welleck's corrector) in Related Work and highlights differences: PRS focuses on tree-based sampling and sequence-level reward-guided generation for data generation efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation removing optimization of p(y1 | x,z,y0,f) (i.e., skipping the refinement-target optimization during RL training) leads to lower rewards and a decline in performance starting at the third RL iteration (Fig. 13), demonstrating that training the refinement policy is important for sustained iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preference-Guided Reflective Sampling for Aligning Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8827.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8827.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy-refine (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy iterative refinement baseline (enhanced from Madaan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that iteratively samples conditioned on the current best response y* and replaces y* whenever a new sampled response has a higher reward; used to compare simple iterative improvement heuristics to PRS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to the same policy models used in PRS experiments (e.g., Llama-2-13B+SFT and others).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following policy models fine-tuned on ShareGPT (SFT) used as samplers for the greedy algorithm during data generation and RL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Greedy iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Start with y0 sampled from πθ(y0|x,z), maintain y* as current best (by reward), and in each revision round sample y ∼ πθ(y|x,z,y*); if reward(y) > reward(y*), set y* = y. Continue for several rounds (temperature sampling used). No explicit language feedback generation step is required.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction following / preference-controlled generation and data generation for offline RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as PRS (AlpacaEval-style instruction following and keyword-focused summarization), used as a sampling/training baseline for best-of-N and RL iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Greedy baseline sometimes improves responses but generally underperforms PRS; the paper reports that 'The greedy algorithm... often underperforms compared to PRS' due to poor exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to pure random sampling (Rand) and PRand, greedy is often worse than PRS and can be comparable or slightly better than naive Rand in some settings, but overall not competitive with PRS in best-of-N and downstream RL.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-conditioned sampling on the current best candidate y*; uses the reward model to accept replacements; no explicit textual feedback f is generated.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical: reported inferior best-of-N and RL outcomes vs PRS in experiments; qualitative discussion says greedy suffers from poor exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor exploration: focusing solely on local iterative improvements from the current best answer can get stuck in local optima and fails to explore alternative response modes that PRS's tree search can discover.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Rand and PRS in the paper; PRS outperforms greedy due to better balance of exploration and exploitation. Greedy is derived from/refined version of Madaan et al.'s self-refinement but lacks PRS's tree-based exploration and explicit feedback mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preference-Guided Reflective Sampling for Aligning Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8827.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8827.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine (Madaan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published self-refinement framework where an LLM iteratively generates self-feedback and refines its responses based on that feedback to improve correctness and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (as used by Madaan et al., not instantiated in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work that shows LLMs can generate feedback and iteratively refine answers; cited as related work and as motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refinement / self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates textual self-critique/feedback on its initial answer and uses that feedback to produce refined answers; multiple rounds possible depending on the method.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various tasks in the cited work (general-purpose LLM improvement tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prior work applied iterative self-feedback to improve responses; in this paper it is referenced as similar in spirit to PRS's use of model-generated feedback but differs in sampling/data-generation focus.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model-generated natural-language feedback used to guide subsequent generations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as prior evidence that LLMs can self-reflect and improve, motivating PRS's use of language feedback, but the present paper does not reproduce Madaan et al.'s specific quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited distinction: Madaan et al.'s work focuses on self-improvement of responses but does not target efficient data sampling for RLHF as PRS does.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned in Related Work as antecedent to PRS; PRS claims to use a more efficient tree-based framework for sampling than plain self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preference-Guided Reflective Sampling for Aligning Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8827.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8827.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (Shinn et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that leverages LLMs' self-reflective capabilities to learn from language-based feedback and iteratively refine behaviors (agents) using verbal RL-like signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-based agents (as in Shinn et al.), not run in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work that uses language-level feedback and internal reflections to improve agent behavior over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion / verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents generate natural-language reflections about failures and use those to guide subsequent actions and learning in an RL-like loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Agentic reasoning and behavior improvement tasks (in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to improve planning and multi-step tasks by reflecting verbally on outcomes and updating strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Language-based self-reflection used as a learning signal (verbal RL).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as related work showing the value of reflection for iterative improvement; not experimentally tested in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail in this paper; cited to highlight a body of work exploiting reflection in LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Referenced alongside Self-refine and Welleck et al.; PRS is distinguished by its sampling-focused, tree-based approach and sequence-level reward usage.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preference-Guided Reflective Sampling for Aligning Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8827.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8827.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Welleck corrector (Generating sequences by learning to self-correct)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating sequences by learning to self-correct (Welleck et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that trains separate corrector models to iteratively refine LLM outputs using synthetically generated data to improve output correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating sequences by learning to self-correct</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Corrector models (separate from base LLMs) as in Welleck et al.; not used in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach uses an auxiliary corrector model trained on synthetic data to refine LLM outputs iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Corrector / iterative self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train a corrector model on generated edits so that given an initial output the corrector produces improved outputs in iterative passes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Iterative inference for text generation (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to correct errors in sequences over multiple refinement steps; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Separate learned corrector model (not just prompt-based) trained on synthetic refinement pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited as demonstrating iterative improvement via a corrector model; PRS differs by using the policy model itself to produce feedback/refinements within a tree-based sampling process.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with PRS: Welleck uses a trained corrector and synthetic data, whereas PRS uses the policy model for feedback and tree-based selection with sequence-level rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preference-Guided Reflective Sampling for Aligning Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Generating sequences by learning to self-correct <em>(Rating: 2)</em></li>
                <li>Alphazero-like tree-search can guide large language model decoding and training <em>(Rating: 1)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8827",
    "paper_id": "paper-271924376",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "PRS",
            "name_full": "Preference-Guided Reflective Sampling",
            "brief_description": "A tree-based generation and sampling method that uses model-generated language feedback and iterative refinements to produce responses aligned to an explicit user preference; designed to improve data generation for iterative offline RLHF and best-of-N selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B + SFT (primary); evaluated across 9 policy models including Mistral-7B, Tulu-2, Vicuna-13B, Llama-3-8b-instruct, etc.",
            "model_description": "Applied primarily with Llama-2-13B fine-tuned on ShareGPT SFT data (LoRA for RL finetuning); experiments also run across nine open-source instruction-following policy models of sizes ~7B–13B (e.g., Mistral-7B, Tulu-2-13B-DPO, Vicuna-13B, Llama-3-8b-instruct).",
            "reflection_method_name": "Preference-Guided Reflective Sampling (PRS)",
            "reflection_method_description": "Generate initial set Y0 of responses conditioned on input x and explicit preference z (y0 ∼ πθ(y0|x,z)), select best y*0 by reward R(x,z,y), have the model produce natural-language feedback f (f ∼ πθ(f|x,z,y*0)), then sample refinements Y1 conditioned on x,z,y*0,f (y1 ∼ πθ(y1|x,z,y*0,f)); use a tree-based multi-layer explore/exploit procedure (typically two layers in experiments) to merge Y0 and Y1 and pick top responses for training or evaluation.",
            "task_name": "Preference-controlled instruction following and keyword-focused document summarization; offline RLHF / best-of-N selection",
            "task_description": "Instruction-following (evaluated on AlpacaEval, Arena-Hard) and summarization (CNN/DailyMail); PRS is used to generate training data for iterative offline RL training and to perform best-of-N sampling at inference/data-generation time, with preferences z appended to prompts when applicable.",
            "performance_with_reflection": "Across multiple policy models and benchmarks PRS produced higher-reward samples and improved downstream evaluation metrics: the paper reports that PRS \"generates higher-quality responses with significantly higher rewards\" and \"substantially outperforms repeated random sampling in best-of-N sampling\" on AlpacaEval and Arena-Hard. Example numeric values shown in the paper (Table 2 / text) include illustrative comparisons such as Rand (Bo-16) ≈ 23.90 vs PRS (Bo-16) ≈ 27.19 on one AlpacaEval condition and similar Bo-32 improvements; PRS also achieves higher LC/WR and WR metrics on AlpacaEval and Arena-Hard in Table 2, and PRS-trained models reach higher AlpacaEval / Rouge scores in offline RL experiments (see Tables 2–4 and Fig. 5).",
            "performance_without_reflection": "Repeated random sampling (Rand) and preference-conditioned random sampling (PRand) produce lower average rewards and weaker downstream performance; random sampling often halted improvement after one RL iteration while PRS (and PRand) continued to improve across 3 RL iterations. Exact baseline numbers appear in Tables 2–4 (e.g., Rand metrics in best-of-N rows are consistently below PRS counterparts).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented via prompt-engineered self-feedback: the same LLM policy generates a textual feedback message f for a selected candidate y*0, and that feedback is appended to context when sampling refinements. A reward model R(x,z,y) is used to select the best candidates at each node of the tree (exploit), and refinements from the chosen node provide exploration; no separate external correction module beyond the reward model is required.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "Quantitative: Higher mean and variance of reward distributions for samples from PRS vs PRand and Rand (Fig. 4); consistent superiority in best-of-N evaluations on AlpacaEval and Arena-Hard (Table 2); PRS yields higher reward training data across 9 policy models (Fig. 9) and models trained with PRS outperform PRand and Rand in head-to-head GPT-4 evaluations after iterative offline RL (Fig. 10 and Fig. 5). Qualitative: PRS better adapts to explicit preferences and reduces toxicity when safe preferences are appended (Table 5 / toxicity results).",
            "limitations_or_failure_cases": "Reported limitations include: (1) language-feedback (f) yields mixed improvements depending on the policy model — some models improve while others do not; (2) for harder tasks such as complex reasoning, the model may struggle to self-improve and PRS was not explored for such tasks in this work; (3) susceptibility to reward hacking is noted as a potential concern; (4) refinements (Y1) do not consistently outperform initial samples (Y0) for all models — improvement rates vary across models (e.g., &lt;50% for some); (5) when preferences are omitted or feedback generation is skipped, some trade-offs exist (though PRS variants still outperform baselines).",
            "comparison_to_other_methods": "Directly compared to repeated random sampling (Rand), preference-conditioned random sampling (PRand), and an enhanced greedy iterative-refinement baseline; PRS outperforms Rand and PRand in best-of-N and in downstream RL training, and outperforms the greedy baseline which suffers from poor exploration. The paper also contrasts PRS to prior reflection/self-improvement works (Self-refine, Reflexion, Welleck's corrector) in Related Work and highlights differences: PRS focuses on tree-based sampling and sequence-level reward-guided generation for data generation efficiency.",
            "ablation_study_results": "Ablation removing optimization of p(y1 | x,z,y0,f) (i.e., skipping the refinement-target optimization during RL training) leads to lower rewards and a decline in performance starting at the third RL iteration (Fig. 13), demonstrating that training the refinement policy is important for sustained iterative improvement.",
            "uuid": "e8827.0",
            "source_info": {
                "paper_title": "Preference-Guided Reflective Sampling for Aligning Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Greedy-refine (baseline)",
            "name_full": "Greedy iterative refinement baseline (enhanced from Madaan et al.)",
            "brief_description": "A baseline that iteratively samples conditioned on the current best response y* and replaces y* whenever a new sampled response has a higher reward; used to compare simple iterative improvement heuristics to PRS.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to the same policy models used in PRS experiments (e.g., Llama-2-13B+SFT and others).",
            "model_description": "Instruction-following policy models fine-tuned on ShareGPT (SFT) used as samplers for the greedy algorithm during data generation and RL experiments.",
            "reflection_method_name": "Greedy iterative refinement",
            "reflection_method_description": "Start with y0 sampled from πθ(y0|x,z), maintain y* as current best (by reward), and in each revision round sample y ∼ πθ(y|x,z,y*); if reward(y) &gt; reward(y*), set y* = y. Continue for several rounds (temperature sampling used). No explicit language feedback generation step is required.",
            "task_name": "Instruction following / preference-controlled generation and data generation for offline RL",
            "task_description": "Same tasks as PRS (AlpacaEval-style instruction following and keyword-focused summarization), used as a sampling/training baseline for best-of-N and RL iterations.",
            "performance_with_reflection": "Greedy baseline sometimes improves responses but generally underperforms PRS; the paper reports that 'The greedy algorithm... often underperforms compared to PRS' due to poor exploration.",
            "performance_without_reflection": "Compared to pure random sampling (Rand) and PRand, greedy is often worse than PRS and can be comparable or slightly better than naive Rand in some settings, but overall not competitive with PRS in best-of-N and downstream RL.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-conditioned sampling on the current best candidate y*; uses the reward model to accept replacements; no explicit textual feedback f is generated.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical: reported inferior best-of-N and RL outcomes vs PRS in experiments; qualitative discussion says greedy suffers from poor exploration.",
            "limitations_or_failure_cases": "Poor exploration: focusing solely on local iterative improvements from the current best answer can get stuck in local optima and fails to explore alternative response modes that PRS's tree search can discover.",
            "comparison_to_other_methods": "Compared to Rand and PRS in the paper; PRS outperforms greedy due to better balance of exploration and exploitation. Greedy is derived from/refined version of Madaan et al.'s self-refinement but lacks PRS's tree-based exploration and explicit feedback mechanism.",
            "ablation_study_results": null,
            "uuid": "e8827.1",
            "source_info": {
                "paper_title": "Preference-Guided Reflective Sampling for Aligning Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Self-refine (Madaan et al.)",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "A previously published self-refinement framework where an LLM iteratively generates self-feedback and refines its responses based on that feedback to improve correctness and quality.",
            "citation_title": "Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "LLMs (as used by Madaan et al., not instantiated in this paper's experiments)",
            "model_description": "Prior work that shows LLMs can generate feedback and iteratively refine answers; cited as related work and as motivation.",
            "reflection_method_name": "Self-refinement / self-feedback",
            "reflection_method_description": "Model generates textual self-critique/feedback on its initial answer and uses that feedback to produce refined answers; multiple rounds possible depending on the method.",
            "task_name": "Various tasks in the cited work (general-purpose LLM improvement tasks)",
            "task_description": "Prior work applied iterative self-feedback to improve responses; in this paper it is referenced as similar in spirit to PRS's use of model-generated feedback but differs in sampling/data-generation focus.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Model-generated natural-language feedback used to guide subsequent generations.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as prior evidence that LLMs can self-reflect and improve, motivating PRS's use of language feedback, but the present paper does not reproduce Madaan et al.'s specific quantitative results.",
            "limitations_or_failure_cases": "Cited distinction: Madaan et al.'s work focuses on self-improvement of responses but does not target efficient data sampling for RLHF as PRS does.",
            "comparison_to_other_methods": "Positioned in Related Work as antecedent to PRS; PRS claims to use a more efficient tree-based framework for sampling than plain self-refinement.",
            "ablation_study_results": null,
            "uuid": "e8827.2",
            "source_info": {
                "paper_title": "Preference-Guided Reflective Sampling for Aligning Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Reflexion (Shinn et al.)",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A prior work that leverages LLMs' self-reflective capabilities to learn from language-based feedback and iteratively refine behaviors (agents) using verbal RL-like signals.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "LLM-based agents (as in Shinn et al.), not run in this paper",
            "model_description": "Prior work that uses language-level feedback and internal reflections to improve agent behavior over iterations.",
            "reflection_method_name": "Reflexion / verbal reinforcement learning",
            "reflection_method_description": "Agents generate natural-language reflections about failures and use those to guide subsequent actions and learning in an RL-like loop.",
            "task_name": "Agentic reasoning and behavior improvement tasks (in cited work)",
            "task_description": "Used to improve planning and multi-step tasks by reflecting verbally on outcomes and updating strategies.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Language-based self-reflection used as a learning signal (verbal RL).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as related work showing the value of reflection for iterative improvement; not experimentally tested in this paper.",
            "limitations_or_failure_cases": "Not discussed in detail in this paper; cited to highlight a body of work exploiting reflection in LLM agents.",
            "comparison_to_other_methods": "Referenced alongside Self-refine and Welleck et al.; PRS is distinguished by its sampling-focused, tree-based approach and sequence-level reward usage.",
            "ablation_study_results": null,
            "uuid": "e8827.3",
            "source_info": {
                "paper_title": "Preference-Guided Reflective Sampling for Aligning Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Welleck corrector (Generating sequences by learning to self-correct)",
            "name_full": "Generating sequences by learning to self-correct (Welleck et al.)",
            "brief_description": "Prior work that trains separate corrector models to iteratively refine LLM outputs using synthetically generated data to improve output correctness.",
            "citation_title": "Generating sequences by learning to self-correct",
            "mention_or_use": "mention",
            "model_name": "Corrector models (separate from base LLMs) as in Welleck et al.; not used in this paper",
            "model_description": "Approach uses an auxiliary corrector model trained on synthetic data to refine LLM outputs iteratively.",
            "reflection_method_name": "Corrector / iterative self-correction",
            "reflection_method_description": "Train a corrector model on generated edits so that given an initial output the corrector produces improved outputs in iterative passes.",
            "task_name": "Iterative inference for text generation (prior work)",
            "task_description": "Used to correct errors in sequences over multiple refinement steps; cited as related work.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Separate learned corrector model (not just prompt-based) trained on synthetic refinement pairs.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited as demonstrating iterative improvement via a corrector model; PRS differs by using the policy model itself to produce feedback/refinements within a tree-based sampling process.",
            "limitations_or_failure_cases": null,
            "comparison_to_other_methods": "Contrasted with PRS: Welleck uses a trained corrector and synthetic data, whereas PRS uses the policy model for feedback and tree-based selection with sequence-level rewards.",
            "ablation_study_results": null,
            "uuid": "e8827.4",
            "source_info": {
                "paper_title": "Preference-Guided Reflective Sampling for Aligning Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Generating sequences by learning to self-correct",
            "rating": 2,
            "sanitized_title": "generating_sequences_by_learning_to_selfcorrect"
        },
        {
            "paper_title": "Alphazero-like tree-search can guide large language model decoding and training",
            "rating": 1,
            "sanitized_title": "alphazerolike_treesearch_can_guide_large_language_model_decoding_and_training"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.013214899999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Preference-Guided Reflective Sampling for Aligning Language Models
4 Oct 2024</p>
<p>Hai Ye yehai@comp.nus.edu.sg 
Hwee Tou Ng 
Yuntao Bai 
Andy Jones 
Kamal Ndousse 
Amanda Askell 
Anna Chen 
Nova Dassarma 
Dawn Drain 
Stanislav Fort 
Deep Ganguli 
Tom Henighan 
Nicholas Joseph 
Saurav Kadavath 
Jackson Kernion 
SheerTom Conerly 
El Showk 
Nelson Elhage 
Zac Hatfield-Dodds 
Danny Hernandez 
Tristan Hume 
Scott Johnston 
SheerShauna Kravec 
Liane Lovitt 
Neel Nanda 
Catherine Olsson 
Dario Amodei 
Tom B Brown 
Jack Clark 
Sam Mccandlish 
Christo- Pher Olah 
Benjamin Mann 
Jared 2022b Kaplan 
Sandipan Kundu 
Anna Goldie 
Azalia 2024 Mirhoseini 
Cameron Mckinnon 
Carol Chen 
Dustin Li 
Eli Tran-Johnson 
Ethan Perez 
Jamie Kerr 
Jared Mueller 
Jeffrey Ladish 
Joshua Landau 
Kamile Lukosiute 
Michael Sellitto 
Nicholas Schiefer 
Noemí Mercado 
Robert Lasenby 
Robin Larson 
Sam Ringer 
Scott John- Ston 
Tamera Lanham 
Timothy Telleen-Lawton 
Samuel R Bow- Man 
Constitutional 
Bradley C A Brown 
Jordan Juravsky 
Ryan Saul Ehrlich 
Ronald Clark 
Quoc V Le 
Christopher Ré 
Nick Ryder 
Melanie Subbiah 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Christopher Berner 
Alec Radford 
Ilya Sutskever </p>
<p>Department of Computer Science
National University of Singapore</p>
<p>Department of Computer Science
National University of Singapore</p>
<p>Preference-Guided Reflective Sampling for Aligning Language Models
4 Oct 202458C2ECD65EAFA5DF94F15E38ECF31680arXiv:2408.12163v2[cs.CL]
Iterative data generation and model re-training can effectively align large language models (LLMs) to human preferences.The process of data sampling is crucial, as it significantly influences the success of policy improvement.Repeated random sampling is a widely used method that independently queries the model multiple times to generate outputs.In this work, we propose a more effective sampling method, named Preference-Guided Reflective Sampling (PRS).Unlike random sampling, PRS employs a tree-based generation framework to enable more efficient sampling.It leverages adaptive self-refinement techniques to better explore the sampling space.By specifying user preferences in natural language, PRS can further optimize response generation according to these preferences.As a result, PRS can align models to diverse user preferences.Our experiments demonstrate that PRS generates higher-quality responses with significantly higher rewards.On AlpacaEval and Arena-Hard, PRS substantially outperforms repeated random sampling in bestof-N sampling.Moreover, PRS shows strong performance when applied in iterative offline RL training 1 .</p>
<p>Introduction</p>
<p>Large language models (LLMs) have made significant advances (Radford et al., 2019;Brown et al., 2020;OpenAI, 2022).These models are typically aligned with human expectations through finetuning.This is achieved by using reinforcement learning from human feedback (RLHF), which mitigates the generation of harmful, biased, or irrelevant outputs (Perez et al., 2022).Both online and offline RL methods have been explored for RLHF (Schulman et al., 2017;Gülçehre et al., 2023;Rafailov et al., 2023).Iterative offline training provides a more efficient alternative than online 1 Source code of this paper is available at https://github.com/nusnlp/PRS.training, by allowing outputs to be pre-generated and reused to facilitate iterative improvements in policy.</p>
<p>Effective data sampling is crucial for iterative model re-training, as it directly influences the effectiveness of the resulting policy (Gülçehre et al., 2023).Repeated random sampling (as shown in Fig. 2) is an effective method and has been widely used for data generation in previous work (Gülçehre et al., 2023).It independently calls the model multiple times to get samples.Then higher-quality data will be maintained to update the policy model.However, the vast output space compromises its efficiency since the inherent randomness may result in inefficient exploration in the sampling space.Also, the simple generation strategy cannot learn from and adapt dynamically based on previously generated samples.Furthermore, with only the supervision of the reward model, it is hard to optimize the outputs to align to diverse and personalized preferences.</p>
<p>We propose a new sampling method named Preference-Guided Reflective Sampling (PRS) to improve data generation.Different from random Figure 2: Comparison of repeated random sampling and our method PRS.PRS adopts a tree-based generation framework that learns to adapt and adjust its outputs by reflecting on its already generated data.It can incorporate a specific user preference to optimize responses that align with it.Adjusting preferences will generate tailored responses.For random sampling, it generates samples independently and can use the best-of-N (BoN) method to find the best sample.Both methods share the same sampling budget, which samples the same number of responses for each prompt.</p>
<p>sampling, PRS employs a tree-based generation framework to balance exploration and exploitation throughout the generation process (see Fig. 2).It learns to adapt and adjust its outputs by reflecting on its already generated data so that it can improve the sampling of future samples.Furthermore, by using a preference described in natural language, PRS can optimize the response toward this explicit preference.The user preference is incorporated as an additional sampling context, guiding the model toward more relevant directions and minimizing unnecessary exploration.As a result, it achieves more efficient sampling and can also generate samples aligned to diverse preferences.</p>
<p>We study preference-controlled text generation for the task of instruction following and keywordfocused document summarization.In our experiments, we first evaluate PRS against various baselines in generating training samples with diverse policy models ( § 5.1).In § 5.2, we investigate its application for aligning LLMs to adhere to explicit preferences provided in the inputs using offline RL training.We further explore preference adaptation, toxicity reduction, and other areas in § 5.3.Our contributions in this work are as follows:</p>
<p>• We introduce PRS, a novel sampling method to improve data generation.PRS is capable of generation tailored to different preferences.</p>
<p>• Experiments with 9 policy models show that PRS generates training data with higher rewards.On AlpacaEval and Arena-Hard, PRS achieves better performance than repeated random sampling in the best-of-N setting (Fig. 1).</p>
<p>• With extensive offline RL training, the outcomes across multiple benchmarks, e.g., AlpacaEval (Li et al., 2023) highlight the effectiveness of PRS.</p>
<p>• Further analysis demonstrates PRS's superior performance in preference adaptation.</p>
<p>Related Work</p>
<p>Offline RL offers an efficient alternative to online RL (Schulman et al., 2017).Dong et al. (2023), Gülçehre et al. (2023), and Rafailov et al. (2023) emphasize data generation and model refinement.Repeated random sampling is a simple but effective method for data generation.</p>
<p>Preliminaries</p>
<p>Offline RL.RLHF utilizes human feedback to finetune a pre-trained LLM with human preferences.The preference human feedback can be utilized to train a reward model R(x, y), given an input x and a response y.Following Gülçehre et al. (2023), we employ offline RL to synchronize the LLM policy with the trained reward model.This process, beginning with the policy initialized by supervised fine-tuning (SFT) on labeled data, involves iterative cycles of data generation and model re-training.</p>
<p>The policy of the LLM, π θ , parameterized by θ, produces a response y given the input x, i.e., y ∼ π θ (y|x).Using the labeled data D 0 , the LLM is trained with the negative log-likelihood (NLL):
L N LL = E (x,y)∼D 0 − log π θ (y|x)(1)
Then it repeats data generation and model retraining to align the language model: 1.Data Generation: Each iteration leverages the policy from the previous cycle to generate N responses y for each input x in the unseen dataset U k .The reward model, R(x, y), evaluates these responses to generate rewards.Best-of-N strategy or a reward threshold is used to identify the high-quality examples.2. Model Re-training: The newly generated data, along with all prior data, is used to refine the model in the subsequent re-training phase.</p>
<p>Method</p>
<p>We aim to improve the data generation process to enhance offline RL training.We first introduce</p>
<p>Preference-Guided Reflective Sampling (PRS), and then study the task of preference-controlled instruction following using offline RL training.</p>
<p>Preference-Guided Reflective Sampling</p>
<p>PRS aims to optimize the response aligned to a given user preference described in natural language.The user preference describes the desired model output, such as conciseness.Let z denote a specific preference, exemplified by statements like "I prefer the response to be concise."or "Can you give me a response without wordy explanations?".PRS aims to generate the responses aligned to the preference z.</p>
<p>Initially, we sample a response y 0 conditioned on both x and z, by appending z to the input x.Subsequently, we engage the LLM policy in a process of self-reflection, aiming to iteratively refine y 0 to better align with the stated preference.Given the independence of preference z and input x, we redefine the generation process of p(y|x) as:
p(y|x) = z,y 0 ,f p(z) × π θ (y 0 |x, z) Initial Sampling × π θ (f |x, z, y 0 ) × π θ (y|x, z, y 0 , f ) Reflective Refinement (2)
where for the reflective refinement, the model first generates language feedback f for the output y 0 , then revises the output by incorporating the feedback to obtain a new response y (see Fig. 3a).Promoting the model to provide language feedback is to provide enriched information to guide the model
for i = 1 to w do 8: Sample y i ∼ π θ (•|x, z, y * , f ) 9:
Add y i to Y 10: end for 11:</p>
<p>Compute R(x, z, y) for newly generated samples in Y 12: end for 13: Output: The best final response y * in revising its response.We can adjust the user preference z to generate outputs aligned to different preferences, e.g., detailed or humorous responses.Tree-Based Generation.For each input, we sample N responses for further selection.However, as Eq. 2 indicates, various components (i.e., z, y 0 , f ) control the generation, causing difficulty in efficient generation.To overcome this issue, we propose tree-based generation (Fig. 3b), which utilizes an iterative exploration and exploitation process: 1.First, the model randomly samples N 0 initial responses Y 0 from π θ (y 0 |x, z), and the reward model R(x, z, y) generates rewards for the samples.The response y * 0 with the highest reward is selected for further exploration.2. Then the model generates language feedback f for y * 0 , i.e., f ∼ π θ (f |x, z, y * 0 ), which is the suggestion to further modify y * 0 to be more in line with the preference z (see the example prompt in Fig. 14).</p>
<p>The model generates another set of N
1 = N − N 0 refinements Y 1 from π θ (y 1 |x, z, y * 0 , f ),
where N is the total number of samples per prompt.It aims to adjust the generation towards even better rewards (see the prompt of Fig. 15).4. We combine Y 0 and Y 1 into Y that has N samples for the input x. 5. (Optional) In layer l, suppose we have samples
Y (l−1) = Y 0 ∪ • • • ∪ Y l−1 until layer l − 1.
We further sample refinements Y l with a-c steps:
y * ← arg max y i ∈Y (l−1) R(x, z, y i ) (3a) f ∼ p(•|x, z, y * ) (3b) Y l ∼ p(•|x, z, y * , f ) (3c)
Eq. 3a identifies the optimal response from all already generated responses (i.e., exploitation), followed by refinements (exploration).We present the pseudocode of PRS in Algorithm 1.It is worth noting that PRS is also functional when the preference z is not provided as input.Additionally, feedback can be omitted during the generation of refinements.In Algorithm 1, the number of samples generated for each layer is set to be the same.However, in practice, other hyper-parameters can be used.Reward Estimation.In PRS, the reward for a response is calculated using the formula R(x, z, y), where z specifies the preference for aspects to be focused on when assessing the response.However, if the specified preference z aligns with the implicit preference already incorporated into the reward model, the formula can be simplified to R(x, y).In this case, the reward model automatically evaluates the response based on its built-in preference, without the need for z.To achieve high rewards, it is crucial to understand and articulate the internal preference of the reward model.</p>
<p>Alignment for Preference-Controlled Text Generation</p>
<p>Here, we study the task of preference-controlled text generation.We train the model to produce responses aligned with the input-specified preference, i.e., y(z) ∼ π θ (y|x, z).We adopt offline RL in § 3 for training, which repeats iterations of data generation and model re-training.</p>
<p>As indicated by Eq. 2, adjusting the preference p(z) can generate diverse outputs, each tailored to a specific preference.Without loss of generality, we do not focus on one specific personalized preference.Instead, we consider diverse preferences.We annotate diverse preferences to ensure each input question is associated with a different preference from others.As exemplified by Table 1, the task of instruction following has diverse personalized preferences and for document summarization, the keywords vary for different documents.</p>
<p>Algorithm 2 in the Appendix is the pseudocode for training.Specifically, we conduct K iterations of offline RL training.In each iteration k, we have an unlabeled set U k = {(x, z)} and we initialize the training set D k to ∅.For each data point</p>
<p>Personalized Preferences</p>
<p>[1] I prefer the model to provide a concise and accurate answer without any unnecessary details or explanations.</p>
<p>[2] I prefer clear and well-organized responses that provide step-by-step instructions or explanations.Additionally, I appreciate when the response includes code snippets or examples for better understanding.... Task 2: Keyword-focused summarization I prefer a response that is strictly within 3 sentences, focusing on the keywords of {specify three keywords here}.</p>
<p>Table 1: The explicit preferences used for response optimization.They are added after the input question or document.For instruction following, we evaluate common and personalized preferences.</p>
<p>(x, z) ∈ U k , we sample N responses in total.We first generate N 0 initial responses denoted as Y 0 and then N 1 = N − N 0 refinements denoted as Y 1 .We use a reward model to select high-quality data for training.To enhance tree-based generation, we aim to optimize the following two components:</p>
<p>• π θ (y|x, z): It trains the policy to generate responses aligned with input preferences.We use the reward model to identify the response y * with the highest reward from Y 0 ∪ Y 1 , and we add the data of (x, z, y * ) to the training set D k .</p>
<p>• π θ (y|x, z, y 0 , f ): To improve the model's refinement ability, we construct improving pairs from Y 0 and Y 1 .We only keep samples from Y 1 that are refined based on the response y * 0 if their rewards exceed y * 0 .The set of improving pairs is formalized as:
Q = (x, z, y * 0 , f , y 1 ) | R(x, z, y 1 ) &gt; R(x,z, y * 0 ), ∀y 1 ∈ Y 1(4)
In our setting, if Q is not empty, we add the improving data of (x, z, y * 0 , f , y * 1 ) into the training set D k , where y * 1 is the response with the highest reward from Y 1 .This is the same idea as best-of-N sampling, to maximize the response's reward after the model's refinement.</p>
<p>After generating data from U k , we combine the generated training data up to iteration k, i.e., D = D 1 ∪ • • • ∪ D k .Then we re-train the policy with the following objective, which refers to the NLL loss in Eq. 1:
L(θ) = E (x,y)∼D 0 − log π θ (y|x) + E (x,z,y * )∼D − log π θ (y * |x, z) + E (x,z,y * 0 ,f ,y * 1 )∼D − log π θ (y * 1 |x, z, y * 0 , f )(5)
where the labeled training data D 0 is also included.After K iterations of RL training, we obtain the model π θ (y|x, z) that can generate the response y aligned to the preference z.</p>
<p>Experiments</p>
<p>Dataset.To align models for preference-controlled text generation, i.e., instruction following and keyword-focused document summarization, we used the following dataset for supervised finetuning (SFT) and RL training:</p>
<p>• Instruction Following.For SFT data, from the widely used dataset ShareGPT2 , we randomly sample 10k conversations with a maximum of three rounds for each conversation, resulting in 21,934 labeled data points in total.Prompts from Alpaca-GPT4 (Peng et al., 2023) are used for RL training.• Summarization.We use the same SFT data from ShareGPT for instruction tuning.We further sample 2,500 documents from CNN / Daily-Mail (See et al., 2017) for RL training.Preference Annotation.We first need to annotate the preferences for the unlabeled prompts.We show some sample preferences in Table 1.</p>
<p>• Instruction Following.The Alpaca-GPT4 dataset initially does not include user preferences, so we use GPT-3.5-turbo to automate the generation of preferences by pairing each prompt with a random profession from a list of 222 professions.This method aims to mirror personalized preferences across various professions, thereby enriching dataset diversity.For details on this process and examples, see Appendix E.</p>
<p>• Summarization.To get the input preference keywords, we prompt GPT-3.5-turbo to extract three keywords from the ground-truth summary.Benchmarks.For instruction following, we use AlpacaEval (Li et al., 2023) with 805 test samples and Arena-Hard (Li et al., 2024) with 500 test samples.For summarization, we further sample 1k data from CNN / DailyMail as the test set.Reward Model.For instruction following, we use UltraRM-13B (Cui et al., 2023), a Llama-2-13B model tuned with GPT-4 preference feedback.It achieved SOTA results on multiple public preference test sets, including Anthropic Helpful (Bai et al., 2022a).For summarization, since we lack a reward model, we simulate rewards by comparing summaries to the ground truth using average F1 scores from Rouge-1, -2, and -L (Lin, 2004).Lastly, we use ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024) for best-of-N sampling on AlpacaEval and Arena-Hard.Baselines.We compare various sampling methods with PRS:</p>
<p>• Rand is repeated random sampling conditioned on the input x using π θ (y|x).• PRand adds an explicit preference z to the input x, i.e., y ∼ π θ (y|x, z), for random sampling.</p>
<p>• Greedy utilizes a greedy algorithm, where we improve the method from Madaan et al. (2023) which iteratively refines the last response.Specifically, the enhanced baseline starts by sampling an initial response with π θ (y 0 |x, z).It uses a reward model to continually update the highest-reward response y * with π θ (y|x, z, y * ).</p>
<p>During each revision round, if a new response y achieves a higher reward, it becomes y * .</p>
<p>We use temperature sampling during response generation.</p>
<p>Comparison of Sampling Methods</p>
<p>We first compare different sampling methods for data generation.We expect a good sampling method to obtain a training set with a higher reward.</p>
<p>Here, we only consider two layers for the treebased generation in PRS.Since the PRS (N 0 , N 1 ) method is affected by the hyper-parameters N 0 and N 1 , we adjust them to examine their impact:</p>
<p>• PRS (0, N ) samples one response y 0 , generates feedback f , and then samples N refinements.It neglects the exploration for y 0 .• PRS (N, 0) samples N responses of y 0 without refinement, which neglects the exploration of y 1 .This is precisely the PRand baseline.• PRS (N/2, N/2) balances exploring y 0 and y 1 .• PRS (N/2, N/2) w/o f omits generating language feedback f during refinement and instead uses π θ (y 1 |x, z, y * 0 ).The goal is to assess the impact of language feedback.Policy Models.We use the model tuned on the SFT data from ShareGPT named Llama-2-13B + SFT to sample responses.We also test multiple open-source instruction-following models such as those tuned on Mistral-7B (Jiang et al., 2023) and Llama-2-13b (Touvron et al., 2023).Preference z.For instruction following, we aim to evaluate the common preference (as shown in Table 1) that favors comprehensive and detailed responses.As shown by Sun et al. (2023), a more detailed response would improve the performance on benchmarks such as AlpcaEval (Li et al., 2023).Since the reward model UltraRM-13B that we use internally includes such preferences, we compute R(x, y) without explicitly specifying z.Results.From the results shown in Fig. 4, PRS generates data with higher rewards than Rand and PRand, and as N increases, the performance gap becomes larger.The setting (N/2, N/2) is much better than (0, N ) and (N, 0), showing that a good balance of exploration is important.Fig. 4 (middle) shows that PRS produces a normal distribution with higher mean and variance than PRand and Rand, indicating a broader exploration and higher reward acquisition in the sampling space.From the full results shown in Fig. 9, language feedback shows mixed results: some models improve, while others do not.However, language feedback increases transparency and both versions still outperform other baselines.</p>
<p>PRand is substantially better than Rand, since PRand adds explicit preference in the input.It demonstrates that preference is effective in guiding the generation of better-aligned responses.For summarization, specifying the keywords would aid the model to concentrate on the key information of the document.The greedy algorithm, revising based on the current best response, often underperforms compared to PRS.Its main limitation is poor response exploration.In contrast, PRS (N/2, N/2) excels by thoroughly exploring both initial and subsequent responses.</p>
<p>We further investigate best-of-N sampling on AlpacaEval v2.0 and Arena-Hard v0.1.The models are evaluated as outlined in Table 2. To obtain the reward scores, we utilize the recent state-of-theart reward model, ArmoRM-Llama3-8B-v0.1.For PRS, no preference is specified, and feedback generation is omitted during sampling to support more general use cases.We employ two layers in PRS, with each layer having a width of N/2.As shown in Table 2, PRS consistently outperforms repeated random sampling, achieving better performance in LC WR on AlpacaEval and WR on Arena-Hard.We define five preference categories and evaluate each category using 100 AlpcaEval test cases.For each category, we customize the prompt (100 test samples) by appending the corresponding preference, evaluating with GPT-4, and recording win rates (%) when comparing two models.</p>
<p>ToxiGen % Toxic (↓)</p>
<p>GPT-4-0613 0.6 GPT-3.5-turbo-06130.5 GPT-3.</p>
<p>Offline RL Training</p>
<p>We conduct offline RL training to align the models to generate responses tailored to input preferences.Experimental Settings.We fine-tune the Llama-2-13B model using LoRA (Hu et al., 2022), starting with supervised fine-tuning (SFT) using labeled data.For instruction following, we perform 3 iterations of RL training, each involving 10k unique GPT-4 prompts.We adopt best-of-16 sampling, generating 16 responses per prompt, and adding 10k new training data per iteration.We set N 0 = N 1 = 8 for PRS.For summarization, after the initial SFT, we undertake one RL iteration, sampling 64 summaries per document (2,500 in total), retaining the summary with the highest reward for each document.We set N 0 = N 1 = 32 for PRS.</p>
<p>Results.Results of AlpacaEval and CNN/Daily Mail are reported in Tables 3 and 4  Preference-controlled optimization during train-ing is important.The method Rand + p involves adding a preference to the input prompt at test time.</p>
<p>It effectively enhances performance compared to Rand.However, it does not explicitly optimize the response to the input preference during training compared to PRand, so it underperforms PRand We further present the results of RL training for each iteration in Fig. 5. Our findings indicate that while using random sampling (Rand) leads to a halt in improvement after just one iteration of RL training, both PRand and PRS continue to show improvement across 3 training iterations.The quality of data generated through random sampling can significantly influence the iterative updates made to the model.Since the generated data is of lower quality, it can lead to a degradation in the model's performance.This, in turn, makes it increasingly challenging for the model to generate high-quality data, thereby halting further improvements.</p>
<p>Further Analysis</p>
<p>Preference Adaptation.We further compare PRS, PRand, and Rand + p on adaptation to personalized preferences differing from the common preference studied in Fig. 4 (left) and Table 3.We define five categories as shown in Fig. 6 for adaptation and for each category, we create 20 unique expressions using GPT-4.We evaluate them across 100 AlpacaEval test cases.For each category, we randomly sample an expression and append it to the prompt.More details can be found in Appendix C.</p>
<p>PRS outperforms PRand, especially in delivering concise, thorough, and humorous responses.Both models perform similarly in clarity and professional tone.Overall, both PRS and PRand surpass Rand + p in effectiveness, showing the benefits of training models to align with user pref- Toxicity Reduction.We further study toxicity reduction as preference adaptation.For each input, we append a safe preference after it, which is randomly sampled from a pool of safe preferences with different expressions (see Table 6).We evaluate ToxiGen (Hartvigsen et al., 2022) and report the results in Table 5.Compared to Rand and Rand + p, adding a safe preference can substantially reduce the generation of toxic content.PRand and PRS achieve comparable performance and both outperform Rand + p. Preference-controlled alignment adapts the LLM to generate safe and harmless content at test time, even without explicit training for safety.</p>
<p>Tree-Based Generation.We analyze tree-based generation in PRS, which starts with N 0 initial responses (Y 0 ), and then N 1 refinements (Y 1 ).We evaluate how often refinements improve over the initial response.As shown in Fig. 7, there is variability across models: Tulu-2-13b-DPO improves less than 50% of the time, while Mistral-7B-v0.2 and Llama-2-13B + SFT perform better.Improvement rates generally increase with more samples (N ), indicating that more samples can lead to better outcomes.We explore the reward values for Y 0 and Y 1 .We find that Y 1 does not consistently offer higher rewards than Y 0 , but combining both sets yields higher rewards.Expansion in PRS.Here, we examine how the depth and width impact the performance of tree- We maintain the number of samples N and vary the depth d and the width w calculated by N d .The depth starts from 1 to 16. Preference is not included in the input and feedback is not generated.Here, the studied model is Llama-3-8b-instruct and the reward model is ArmoRM-Llama3-8B-v0.1.100 test samples are randomly selected from AlpacaEval for evaluation.</p>
<p>based generation in PRS.We keep the total number of samples N constant while varying the depth d.The width w is then calculated by N d .As shown in Fig. 8, our results indicate that for larger N , increasing the depth (e.g., to 4) improves performance.However, for smaller values of N , such as 16, increasing the depth beyond 2 does not yield further benefits.A larger N results in a greater width, allowing the model to sample more responses at each layer, thereby increasing the likelihood of discovering better responses than those in the previous layers.We further conduct an ablation study in Appendix B.3.</p>
<p>Conclusion</p>
<p>We introduce PRS, an improved sampling method designed to enhance iterative model improvement.In contrast to repeated random sampling, PRS enables more efficient generation through a tree-based approach.By allowing the specification of preference in the input, PRS optimizes responses to better align language models with diverse user preferences.Our comprehensive evaluation shows that PRS consistently generates higher-quality samples.On AlpacaEval and Arena-Hard, PRS significantly outperforms random sampling in the best-of-N setting.Additionally, PRS excels when applied to iterative offline RL training.</p>
<p>Limitations</p>
<p>Our approach capitalizes on the model's selfimprovement capabilities to aid in data sampling.However, for more challenging tasks, such as reasoning tasks, the model may struggle to enhance its performance autonomously.We have not explored these types of tasks in this work.Further enhancing the model's self-improvement capabilities, particularly for more difficult tasks, can be explored in the future.Our approach may be susceptible to reward hacking, though further research may mitigate its effects.</p>
<p>Algorithm 2 : PRS for aligning language models for diverse preferences 1: Input: Labeled training data D 0 ; K sets of unlabeled data [U 1 , • • • , U K ]; large language model π θ ; reward model R; number of samples per prompt N ; N 0 .2: Initialize π θ 0 on D 0 using Eq. 1. 3: D ← ∅. 4: for k = 1 to K do  # Preference-Guided Reflective Sampling (PRS) 9:
• Sample N 0 responses Y 0 ∼ π θ k−1 (y 0 |x, z). 10:
Maximize reward R(x, z, y i 0 ) over Y 0 to find the optimal y * 0 .</p>
<p>11:</p>
<p>• Generate language feedback f ∼ π θ k−1 (f |x, z, y * 0 ).</p>
<p>12:
• Sample N 1 = N − N 0 refinements Y 1 ∼ π θ k−1 (y 1 |x, z, y * 0 , f ).
13:</p>
<p>Maximize reward R(x, z, y i 1 ) over Y 1 to find the optimal y * 1 .</p>
<p>14:</p>
<p>if R(x, z, y * 1 ) &gt; R(x, z, y * 0 ) then 15:</p>
<p>Add (x, z, y Update π θ k on D ∪ D 0 with Eq. 5. 23: end for 24: Output: π θ K (y|x, z).2022b), their approach is limited to modifying responses based on principles rather than integrating these principles into the input prompt to guide the generation of model responses.Sun et al. (2024) propose to train a reward model that can evaluate responses based on principles, which is similar to our work when using the reward model by adding the extra preference information.However, Sun et al. (2024) also overlook the importance of sampling efficiency.Another notable contribution is from Scheurer et al. (2023), who advocate for training models using language feedback, as opposed to the numerical feedback derived from reward models.Unlike our strategy, which employs the model itself to generate language feedback, they depend on human annotators for this task.Recent work by Feng et al. (2023) aligns with our goal to enhance model sampling exploration.They adopt Monte Carlo tree search (MCTS) for decoding, utilizing token-level rewards to guide output sampling in instruction-following tasks.In contrast, our approach prioritizes sequence-level rewards for evaluating model responses and employs a tree-based search without extensive inference costs.Furthermore, we incorporate input prompt preferences to direct the generative process, which is another difference from Feng et al. (2023).</p>
<p>A Detailed Related Work</p>
<p>A.2 Reflective Reasoning of Large Language Models</p>
<p>Large language models (LLMs) have demonstrated self-reflection capability, critically analyzing their own decisions and providing feedback to enhance their responses (Madaan et al., 2023).Madaan et al. (2023) introduce a self-refinement framework that enables LLMs to continuously improve their responses based on self-generated feedback.In contrast, our work introduces an efficient tree-based generation model that optimizes the use of LLMs' reflective abilities more effectively.Further exploring the potential of LLMs' self-reflective capabilities, Shinn et al. (2024) leverage this feature to enable LLMs to learn from language-based feedback and refine their outputs towards more accurate and contextually relevant responses.In the realm of iterative inference for text generation, Welleck et al. (2023) propose training a corrector model to refine LLM outputs, utilizing synthetically generated data that fosters gradual improvement.The concept of reflection in LLMs is crucial for advances of AI agents, facilitating their ability to summarize and reflect on outcomes from previous interactions to better plan and execute future actions (Yao et al., 2023a,b).</p>
<p>A.3 Controlled Instruction Following</p>
<p>In the era of large language models, there is growing interest in evaluating and enhancing complex instruction following with the outputs controlled by input constraints (Chen et al., 2024;He et al., 2024;Yao et al., 2024).In our work, to improve sampling efficiency, we frame generation as a problem of controlled text generation by treating user preference as the constraint.</p>
<p>B Additional Results</p>
<p>B.1 Full Results of Data Sampling</p>
<p>We show the full results of data generation on 9 policy models in Fig. 9.</p>
<p>B.2 Instruction Following</p>
<p>Head-to-head comparison of PRS and PRand after 3 iterations of RL training is shown in Fig. 10.</p>
<p>B.3 Ablation Study</p>
<p>In our ablation study, we evaluate the impact of removing the optimization of p(y 1 |x, z, y 0 , f ) during PRS RL training.We apply tree-based generation after each RL iteration to sample outputs.As shown in Fig. 13, skipping this optimization results in lower rewards.Additionally, the performance declines in the ablated baseline starting from the third RL iteration, highlighting the importance of this component.</p>
<p>C Details of Preference Adaptation</p>
<p>We assess the adaptability of models to input preferences by specifying a desired output preference after the input question.Our comparison focuses on the methods of PRS, PRand, and Rand, examining how different sampling approaches influence the alignment of models with user preferences.Both PRS and PRand are designed to train models explicitly to adhere to given preferences, unlike Rand which does not incorporate preference training.</p>
<p>Given the impracticality of evaluating every conceivable preference, we define five types of preferences which are conciseness, thoroughness, clarity, professional tone, and humorous tone.To ensure robust generalization, we use GPT-4 to formulate 20 unique expressions for each preference category.Each preference within a category conveys the same underlying preference but is expressed in different styles.We present one expression for each category as follows:</p>
<p>• Conciseness: I prefer short and sweet responses.</p>
<p>• Thoroughness: I prefer responses that delve deeply into the topic and provide comprehensive insights.• Clarity: I prefer responses that are clear and straightforward, avoiding any unnecessary complexity or jargon.• Professional Tone: I prefer responses that maintain a professional tone throughout the conversation.• Humorous Tone: I prefer responses with a side of humor, please.Sample preferences are shown in Table 7.</p>
<p>We sample 100 test cases from AlpacaEval for evaluation.We conduct a head-to-head comparison between the models tuned by PRS, PRand, and Rand.For each category, we randomly sample an expression from 20 candidates and append the preference after the input prompt when generating a response.We also use GPT-4 as the evaluator, adapting the evaluation code from AlpacaEval.We specifically indicate a preference type for GPT-4 to choose a better-aligned response from two candidates.</p>
<p>D Prompts used in PRS</p>
<p>For instruction following, we present the prompt used for generating feedback in Fig. 14, the prompt Safe and Harmless Preferences 1 I prefer a response that is considerate, safe, and avoids any sensitive or potentially harmful content, without causing any offense or discomfort.</p>
<p>2 I prefer a response that is mindful, secure, and steers clear of any sensitive or potentially damaging material, ensuring it doesn't cause offense or discomfort.</p>
<p>3 I prefer a response that is thoughtful, safe, and carefully avoids any sensitive or potentially harmful subjects, ensuring no offense or discomfort is caused.</p>
<p>4 I prefer a response respectful, cautious, and deliberately evades any delicate or potentially injurious topics, so as to prevent causing any upset or unease.</p>
<p>5 I prefer a response that's thoughtful and secure, meticulously avoiding topics that might be sensitive or harmful, thereby ensuring nobody feels offended or uncomfortable.</p>
<p>Table 6: The safe and harmless preferences used for toxicity reduction.</p>
<p>for revising the response with feedback in Fig. 15, and the prompt for direct refinement without feedback in Fig. 16.For keyword-focused document summarization, the prompt for providing feedback is shown in Fig. 17 and the prompt for refinement with feedback is shown in Fig. 18.Preference Annotation.We use GPT-3.5-turbo to generate the preferences.For each prompt from Alpaca-GPT4, we use the template in Fig. 19 to generate the preference, where the generation is conditioned on the question and a profession name.The profession name is randomly selected from the profession name list.After obtaining a preference, we further prompt GPT-3.5-turbo to revise its output to make the generated preference general and applicable to different questions.In Fig. 20, we present a variety of generated preferences, illustrating the diversity in the preferences that the method can produce.</p>
<p>E Details of Preference Generation</p>
<p>F Sample Outputs of Different Baselines</p>
<p>We display sample outputs in Tables 8 and 9. Thoroughness 1 I prefer responses that delve deeply into the topic and provide comprehensive insights 2 I prefer when the information is thorough and covers all aspects, leaving no stone unturned.3 I prefer a detailed exposition, with rich context and nuanced explanations.</p>
<p>Clarity 1 I prefer responses that are clear and straightforward, avoiding any unnecessary complexity or jargon. 2 I prefer that you explain things simply, as if you were talking to someone who's completely new to the topic.3 I prefer answers that are easy to understand and follow, without any convoluted explanations.</p>
<p>Professional Tone</p>
<p>1 I prefer responses that maintain a professional tone throughout the conversation.2 I prefer that the language used is formal and professional in nature.</p>
<p>3 I prefer the communication to be strictly professional.</p>
<p>Humorous Tone</p>
<p>1 I prefer responses with a side of humor, please. 2 I prefer my information served with a chuckle.3 I prefer answers that come with a comedic twist.(i.e., 8, 16, 32, 64, or 128) using a specific sampling method.We then average the top three rewards for each prompt, leading to an overall average score for the entire evaluation set.  Figure 13: We ablate to exclude the optimization of p(y 1 |x, z, y 0 , f ) and use tree-based generation after each RL iteration (N =16), focusing on instruction following with the common preference in Table 1.</p>
<p>Please review the AI assistant's response to the user question presented below, acting as an impartial judge.Your objective is to assess how well the reference answer aligns with the user's preferences and suggest improvements.Structure your feedback in bullet points for clarity and conciseness.Each point should specifically reference a part of the reference answer, highlighting how it can be better tailored to meet the user's expectations.You are a skilled corrector tasked with enhancing a reference answer based on specific user feedback.Your role is to refine the existing response, ensuring it aligns more closely with the user's suggestions for improvement.Utilize the feedback effectively to upgrade the reference answer, making it more relevant and satisfactory to the user's expectations.You are a skilled corrector tasked with enhancing a reference answer.You need to improve the reference answer to make it better align with the user's preference.Kindly evaluate the AI assistant's summarization of the provided article.Your task is to impartially judge the conciseness and relevance of the summary, ensuring it adheres to the specific user preference of being strictly within 3 sentences and focused on the designated keywords.Please provide your feedback in bullet points for clarity.In your points, reference specific parts of the AI's summary and the article, suggesting precise improvements to better align with the user's expectations for a keyword-focused summary.</p>
<p>[Article] {passage} [Summary] {summary} [User Preference] 1.A summary strictly within 3 sentences.2. Focus on keywords of {keyword}.</p>
<p>Provide your feedback in bullet points for clarity.In your points, reference specific parts of the AI's summary and the article, suggesting precise improvements to better align with the user's expectations for a keyword-focused summary.</p>
<p>Your feedback: As an expert in topic-focused summarization, your task is to refine a summary based on detailed user feedback.Focus on incorporating the user's preferences as below for a concise, three-sentence summary that emphasizes specific keywords.Use the provided feedback to enhance the clarity, relevance, and precision of the summary, ensuring it closely aligns with the user's expectations.Your goal is to modify the existing summary into a more effective and targeted summary that meets the user's preference.</p>
<p>[Article] {passage} [Original Summary] {summary} [User Preference] {preference} [User Feedback] {feedback}</p>
<p>The improved summary (only generate the content that is relevant to the summary):  Return the preference in a JSON format: {{"preference": the description of the preference}}.</p>
<p>[Profession] {profession} [Question] {question}</p>
<p>Second round:</p>
<p>You have to suggest a general preference that is applicable to other questions.There are many different types of dinosaurs, including Tyrannosaurus rex, Triceratops, Velociraptor, Stegosaurus, Diplodocus, and Brachiosaurus.These animals ranged in size from small, bipedal carnivores like Velociraptor to massive, quadrupedal herbivores like Apatosaurus.Some dinosaurs had feathers, while others had armor or spikes on their bodies.Dinosaur fossils have been found all over the world, including North America, South America, Africa, Europe, Asia, and Australia.These fossils provide evidence of what these animals looked like and how they lived.Scientists use this information to learn about the evolution of life on Earth and to understand the history of our planet.PRand Yes, the dinosaurs really existed.They were a diverse group of reptiles that lived on Earth during the Mesozoic Era, which lasted from about 251 to 66 million years ago.The first dinosaurs appeared in the Triassic period and went extinct at the end of the Cretaceous period.Dinosaurs were large, land-dwelling animals that varied greatly in size, shape, and behavior.Some were herbivores (plant-eaters) like the long-necked sauropods such as Brachiosaurus and Diplodocus, while others were carnivores (meat-eaters) like Tyrannosaurus rex and Velociraptor.Some dinosaurs could fly, like Pteranodon, while others could swim, like Spinosaurus.</p>
<p>There have been many discoveries of fossilized remains of dinosaurs around the world, providing evidence of their existence.These include complete skeletons, tracks, eggs, nests, and even preserved soft tissues.</p>
<p>Scientists use these fossils to study the evolution, anatomy, and behavior of dinosaurs, helping us understand more about life on Earth millions of years ago.</p>
<p>PRS</p>
<p>Yes, dinosaurs are a group of extinct reptiles that lived on Earth during the Mesozoic Era, which lasted from approximately 252 to 66 million years ago.They first appeared in the fossil record during the Triassic Period and became extinct at the end of the Cretaceous Period, along with many other species due to a massive asteroid impact.Dinosaurs were diverse and came in various shapes and sizes, ranging from small bird-like creatures to gigantic herbivores and carnivores.Some well-known dinosaurs include Tyrannosaurus rex, Triceratops, Stegosaurus, and Velociraptor.These animals had unique features such as horns, spikes, plates, and feathers that helped them adapt to their environment.The discovery of dinosaur fossils has played a crucial role in our understanding of the history of life on Earth.Paleontologists have found thousands of dinosaur fossils all over the world, providing valuable insights into how these creatures evolved over time and what kind of environments they inhabited.By studying dinosaur fossils, we can learn about the geological changes that occurred throughout the Mesozoic Era, including shifts in climate, sea level, and landmass distribution.Overall, there is overwhelming evidence supporting the existence of dinosaurs.Their remains have been found in rocks dated back millions of years ago, and their fossils show distinctive characteristics that differentiate them from other animals alive today.Therefore, it is reasonable to conclude that dinosaurs were real animals that once roamed our planet.The Dark Knight is a superhero film that follows the story of Batman as he battles against the villainous Joker.While it does not contain any explicit content, the movie has a dark and intense tone that may be too mature for younger children.Additionally, some scenes feature violence and action sequences that could be frightening for young viewers.Overall, I would recommend this movie for teenagers or adults who are comfortable with its themes and imagery.I prefer responses that are informative, precise, creative, detailed, relevant, and in-depth.Rand + p The Dark Knight is a superhero film that follows the story of Batman as he tries to take down the criminal mastermind, the Joker.The movie has received critical acclaim for its performances, direction, and themes, but it also contains some mature content that may not be suitable for younger audiences.</p>
<p>While children can certainly watch The Dark Knight, parents should exercise caution and consider their child's age before allowing them to do so.The movie features intense violence, language, and themes of death and tragedy that may be too mature for younger viewers.Additionally, the Joker's character is particularly disturbing and may frighten young children.Overall, while The Dark Knight is an excellent movie, parents should use their discretion when deciding whether or not to allow their children to watch it.</p>
<p>PRand</p>
<p>The Dark Knight is a superhero film based on the DC Comics character Batman.The movie follows Bruce Wayne, who assumes the identity of Batman to protect Gotham City from criminals.The film explores themes such as crime, corruption, and morality.It was rated PG-13 for intense sequences of violence and menace, some sexual content, and language.</p>
<p>In my opinion, children can watch The Dark Knight with parental guidance.While the movie has some violent scenes, they are not overly graphic or gory.There is also some sexual content, but it is not explicit or gratuitous.The language used in the movie is also not too crude or offensive.However, parents should be aware that the movie deals with mature themes and may be too complex for younger audiences to fully understand.Additionally, the film's runtime is over two hours long, which could make it difficult for younger children to sit through without getting restless.Overall, I believe that older children (age 13 and above) can enjoy The Dark Knight with proper supervision and discussion with their parents about its themes and content.</p>
<p>PRS</p>
<p>The Dark Knight is a 2008 superhero movie based on the DC Comics character Batman.The film features intense violence, profanity, and mature themes that may not be appropriate for children.</p>
<p>Violence: The Dark Knight contains numerous scenes of violent crime, including gunfire, explosions, and hand-to-hand combat.These scenes can be particularly disturbing for younger viewers who may not understand the context or consequences of such violence.Profanity: The film also includes frequent use of strong language, including profanities like \"f*<em>k\" and \"sh</em>t.\"This type of language can be jarring for some parents who want to shield their children from hearing such words.Mature Themes: In addition to its violence and language, The Dark Knight deals with complex issues like terrorism, corruption, and moral ambiguity.These themes can be challenging for young children to process and may require additional discussion or explanation from parents after watching the movie.Overall, while The Dark Knight is an excellent film for older teenagers and adults, it may not be suitable for younger children due to its intense content and mature themes.Parents should exercise caution when deciding whether to allow their children to watch this movie.</p>
<p>Table 9: Example 2 demonstrates outputs using different sampling methods to align the language model.</p>
<p>Figure 1 :
1
Figure 1: Performance comparison of PRS (ours) and repeated random sampling (Rand) on AlpacaEval v2.0 and Arena-Hard v0.1 using best-of-32 sampling.Each prompt samples 32 responses using Rand or PRS and the response with the highest reward is kept for evaluation.</p>
<p>Preference 1 .Figure 3 :
13
Figure 3: PRS: (a) Example: A user requests a brief response with supporting references.The initial response lacks references.After feedback, the revised response includes appropriate references.(b) A preference z is added to the input x.The process begins by sampling N 0 initial responses Y 0 , from which the optimal response y * 0 is selected using a reward model R. Then feedback f is generated, leading to the sampling of N 1 refinements Y 1 to enhance y * 0 .Finally, Y 0 and Y 1 are merged.Optionally, new refinements may be sampled based on the current best response.</p>
<p>Figure 4 :
4
Figure 4: Comparing sampling methods.Left: We study the common preference and use the description of Table1to generate detailed and in-depth responses.With 100 random prompts from Alpaca-GPT4, each method samples N responses per prompt(i.e., 8, 16, 32, 64, or 128).The top three highest rewards are averaged for each prompt, leading to an overall average score for the entire evaluation set.The full results of 9 policy models are shown in Fig.9.Middle: The distribution of rewards with N = 128, where PRS is PRS (N/2, N/2).Right: Summarization results on 100 random documents from CNN / Daily Mail.The policy model is Llama-2-13b+SFT.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Offline RL training: Win rates for PRS, PRand, and Rand + p vs. Base + p, evaluated using GPT-4 on a 200-sample AlpacaEval."+ p" adds common preference in the input.</p>
<p>respectively.The model trained by PRS outperforms those trained by PRand and Rand.Looking at the rewards of the generated training data shown in Fig.12in the Appendix, PRS exhibits consistently higher rewards than PRand.It shows that the quality of data generation is key to offline RL.Compared to open-source models, PRS outperforms the models tuned by PPO training.In head-to-head comparison shown in Fig.11in the Appendix, PRS outperforms multiple strong open-source models more than 50% of the time, except for Mistral-7B-v0.2.These promising results highlight the potential of PRS for future applications, such as integrating PRS with DPO training(Rafailov et al., 2023) and fullparameter fine-tuning.For summarization, after aligning the model with PRS, our model performs the best among existing strong open-source models.</p>
<p>Figure 8 :
8
Figure 8: Effects of varying depth and width for PRS.We maintain the number of samples N and vary the depth d and the width w calculated by N d .The depth starts from 1 to 16. Preference is not included in the input and feedback is not generated.Here, the studied model is Llama-3-8b-instruct and the reward model is ArmoRM-Llama3-8B-v0.1.100 test samples are randomly selected from AlpacaEval for evaluation.</p>
<p>A. 1
1
Alignment of Large Language Models Similar to PRS, Bai et al. (2022b) also leverage the LLM's capacity for reflection to refine model responses.However, our work differs from Bai et al. (2022b) in several aspects: (a) Most importantly, Bai et al. (2022b) do not aim to improve data sampling for RLHF, but our work proposes a tree-based framework to enable efficient data generation.(b) Bai et al. (2022b) only focus on harmless responses, but our work applies to a broader spectrum of preferences.(c) While the preferences added into the input to guide model generation -introduced in our work -is similar to the usage of principles proposed by Bai et al. (</p>
<p>Conciseness 1 I
1
prefer short and sweet responses.2 I prefer answers that are to the point.3 I prefer concise explanations, no fluff.</p>
<p>Figure 9 :
9
Figure 9: Results of data generation for instruction following: We focus on the common preference and use the description in Table 1 to generate detailed and in-depth responses.(a) Policy Models: We use 9 policy models to generate training data, which are Llama-2-13b + SFT, Mistral-7B-Instruct-v0.1, Mistral-7B-Instruct-v0.2, zephyr-7b-beta, Tulu-2-7b-DPO, Tulu-2-13b-DPO, Vicuna-13b-v1.5, WizardLM-13B-V1.2 and Xwin-LM-13B-V0.2.(b) Test samples: We randomly sample 100 prompts from Alpaca-GPT4.(c) Setup: We sample N responses per prompt(i.e., 8, 16, 32, 64, or 128) using a specific sampling method.We then average the top three rewards for each prompt, leading to an overall average score for the entire evaluation set.We use UltraRM-13B to generate the reward.</p>
<p>Figure 11 :Figure 12 :
1112
Figure 11: PRS vs. open-source models.</p>
<p>Figure 14 :
14
Figure 14: Prompt template for feedback generation, for the task of instruction following.</p>
<p>improved answer (only generate the content that is relevant to the user question):</p>
<p>Figure 15 :
15
Figure 15: Prompt template for refinement with feedback, for the task of instruction following.</p>
<p>improved answer (only generate the content that is relevant to the user question):</p>
<p>Figure 16 :
16
Figure16: Prompt template for direct refinement without feedback, for the task of instruction following.</p>
<p>Figure 17 :
17
Figure 17: Prompt template for feedback generation, for the task of summarization.</p>
<p>Figure 18 :
18
Figure 18: Prompt template for refinement with feedback, for the task of summarization.</p>
<p>Suppose you are a user using an AI model, and you have a specific preference for the model's response.Based on the following profession and the asked question, suggest one preference.You have to clearly describe your preference.</p>
<p>Figure 19 :
19
Figure 19: Prompt template for generating preferences, for the task of instruction following.</p>
<p>Figure 20 :
20
Figure 20: Sample annotated preferences by GPT-3.5-turbo.</p>
<p>Input prompt x; preference z; model π θ ; reward model R; number of layers d; total samples N to generate 2: Initialize: Layer width w = N * with the highest score from Y or set y * to None if Y is ∅
Algorithm 1 PRSd3: Y ← ∅4: for l = 0 to d − 1 do5: Select y  6: f ∼ π θ (•|x, z, y
1: Input: * ) if y * is not None else None 7:</p>
<p>Table 2 :
2
Results of best-of-N (Bo-N ) sampling on Al-pacaEval and Arena-Hard benchmarks, compared to the results of one-pass inference.We use ArmoRM-Llama3-
AlpacaEval v2.0Arena-Hard v0.1Method LC WR WRMethod WRMis-7B-Inst-v0.217.1014.70mis-large-2407 70.4Rand (Bo-16)23.9019.86Rand (Bo-16) 77.0PRS (Bo-16)27.1919.87PRS (Bo-16) 79.3Rand (Bo-32)24.8520.61Rand (Bo-32) 79.1PRS (Bo-32)27.1720.03PRS (Bo-32) 80.3Lla-3-8b-inst22.9022.60 Lla-3.1-70b-inst 55.7Rand (Bo-16)31.0028.75Rand (Bo-16) 69.5PRS (Bo-16)35.0531.92PRS (Bo-16) 69.8Rand (Bo-32)32.9430.43Rand (Bo-32) 68.2PRS (Bo-32)36.7033.46PRS (Bo-32) 72.2Gemma-2-9b-it48.6137.07 qwen2-72b-inst 46.9Rand (Bo-16)55.0744.51Rand (Bo-16) 61.9PRS (Bo-16)58.4043.86PRS (Bo-16) 62.1Rand (Bo-32)57.6145.10Rand (Bo-32) 63.9PRS (Bo-32)59.8546.41PRS (Bo-32) 65.4
8B-v0.1 as the reward model.Each prompt samples N responses using repeated random sampling or PRS and the best response with the highest reward is kept for evaluation.Here, PRS does not include preference in the input, and feedback is not generated during refinement.PRS uses the version of PRS (N/2, N/2).The higher score between PRS and Rand is highlighted in bold.LC WR is the abbreviation for length-controlled win rate.</p>
<p>Table1to generate detailed and in-depth responses.With 100 random prompts from Alpaca-GPT4, each method samples N responses per prompt(i.e., 8, 16, 32, 64, or 128).The top three highest rewards are averaged for each prompt, leading to an overall average score for the entire evaluation set.The full results of 9 policy models are shown in Fig.9.Middle: The distribution of rewards with N = 128, where PRS is PRS (N/2, N/2).Right: Summarization results on 100 random documents from CNN / Daily Mail.The policy model is Llama-2-13b+SFT.</p>
<p>Table 3 :
3
Results of AlpacaEval v1.0.We use the common preference in Table1to control our models to generate responses."+ p" adds preference in the input during testing.SFT uses all available labeled data of ShareGPT and Alpaca-GPT4 for supervised fine-tuning.</p>
<p>Base is the model tuned using ShareGPT data.To reduce the cost of calling GPT-4, we downsampled the test set for ChatGPT and GPT-4 baseline.We also show existing models tuned from Llama-2-13B for comparison, but they are fine-tuned with full parameters and different training data.</p>
<p>34.70 * 29.02 * Continual RL training on Base PRand 37.50 † 13.78 † 34.12 † 28.47 † PRS 38.15 * 14.16 * 34.65 * 28.99 *
R-1R-2R-LAvg.LLaMA2 Chat 13B 32.9310.7029.2924.31Mistral 7B v0.234.9811.2731.3825.88Tulu 2+DPO 13B36.6412.9333.3427.64Vicuna 13B V1.537.1213.2633.7128.03Base w/o keywords 30.1510.3527.8922.80Base + p35.4612.5632.3726.80RL training on un-tuned Llama-2-13BPRand37.39  † 13.71  † 33.96  † 28.35  †PRS 38.20  Table 4: Summarization results on CNN / Daily Mail,adding input keywords except for the "Base w/o key-words" condition. We report average Rouge-1, Rouge-2,and Rouge-L F1 scores with 5 runs.
* 14.16 * * indicates PRS outperforms PRand significantly (p &lt; 0.01), and † indicates PRand outperforms Vicuna 13B V1.5 (p &lt; 0.01).</p>
<p>Proportion of cases where the top response from N 1 refinements in Y 1 yields a higher reward than the best initial response from Y 0 .Average maximum rewards for each set and their union are reported (N =32).erences.However, Rand + p excels in conciseness, producing fewer tokens (176.07)comparedtoPRS(199.31).In contrast, for thoroughness, while Rand + p averages 378.99 tokens, PRand and PRS provide more thorough responses with 461.34 and 507.81 tokens, respectively.
Pro. Improve0.3 0.5 0.6 0.7 0.8 0.448 16 32 64 128 N Llama-2-13b + SFT Mistral-7B-v0.2 Tulu-2-13b-DPO0 0.0 Llama-2-13b+sft On 0 On 1 On 0 1 2 2.5 Mistral-7B-v0.2 On 0 On 1 On 0 1 5.0 0 2 Avg. Max Reward 4 On 0 1 On 1 On 0 Tulu-2-13b-DPOFigure 7:</p>
<p>Table 7 :
7
Sample preferences with different expressions for each category.Three examples are shown in each category.</p>
<p>We use UltraRM-13B to generate the reward.
PRS WinsTiePRand WinsAlpacaEval58.1741.83Evol-Instruct40.3729.3630.28UltraChat53.5120.78 25.71020406080 100Figure 10: Head-to-head evaluation of PRS and PRand after 3 iterations of RL training. We use GPT-4 as theevaluator.Vicuna 13B V1.5 LLaMA2 Chat 13B Tulu 2+DPO 13B Zephyr 7B Beta WizardLM 13B V1.2 Mistral 7B v0.2 Test set: AlpacaEval (200) 79.4 64.7 62.3 55.0 53.8 48.7 Evaluator: GPT-44060 PRS Wins (%)80</p>
<p>1.I prefer the AI model to provide well-researched and evidence-based information in its response.2.I prefer the model to provide a concise and accurate answer without any unnecessary details or explanations.3.I prefer clear and concise responses that provide accurate and relevant information without any unnecessary technical jargon or excessive details.Additionally, I appreciate if the response is wellorganized and easy to understand.4.I prefer the model to provide concise and well-structured responses, with clear and coherentexplanations.The responses should be informative and relevant, addressing the specific question asked.Additionally, I value the model's ability to provide accurate and up-to-date information, supported by reliable sources if applicable.5.I prefer a response that is concise, accurate, and provides practical solutions or actionable steps.
6. I prefer concise and well-structured responses that provide accurate and relevant information.Please avoid unnecessary repetition, irrelevant details, or excessive technical jargon. Additionally, itwould be helpful if the response includes clear examples or supporting evidence to enhanceunderstanding.7. I prefer the AI model to provide concise and accurate responses with a friendly and conversationaltone.8. I prefer the AI model to provide concise and clear responses that directly address the questionasked. It should avoid providing excessive or irrelevant information.</p>
<p>Do dinosaurs really exist?RandYes, dinosaurs are a group of extinct reptiles that lived on Earth during the Mesozoic Era, which lasted from about 250 million to 66 million years ago.The term "dinosaur" was coined by Sir Richard Owen in 1842, and it means "terrible lizard."There is overwhelming evidence that dinosaurs existed, including fossils found all over the world, as well as scientific research and analysis of these fossils.Paleontologists have discovered thousands of different species of dinosaurs, ranging in size from tiny, bird-like creatures to massive giants like Tyrannosaurus rex.While there is no doubt that dinosaurs existed, there is still much that we don't know about them, such as how they behaved, what their environments were like, and why they went extinct.However, through continued research and discovery, scientists are constantly learning more about these fascinating creatures.Do dinosaurs really exist?I prefer responses that are informative, precise, creative, detailed, relevant, and in-depth.Rand + p Yes, dinosaurs existed.They were a group of animals that lived on Earth during the Mesozoic era, which lasted from about 250 million to 66 million years ago.The first known dinosaurs appeared around 230 million years ago and went extinct at the end of the Cretaceous period, about 66 million years ago.</p>
<p>Table 8 :
8
Example 1 demonstrates outputs using different sampling methods to align the language model.Decide whether children can watch the given movie.Mention what makes it inappropriate if you think they can't.The Dark Knight (2008) Rand</p>
<p>Decide whether children can watch the given movie.Mention what makes it inappropriate if you think they can't.The Dark Knight (2008)</p>
<p>https://sharegpt.com/
AcknowledgementsThis research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-016[T]). We thank Ruochen Xu for his comments on this paper, and the anonymous reviewers for their valuable suggestions.Large language monkeys: Scaling inference compute with repeated sampling.CoRR, abs/2407.21787.
Benchmarking large language models on controllable generation under diversified instructions. Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, Zhendong Mao, 10.1609/aaai.v38i16.29734AAAI. 2024</p>
<p>Ultrafeedback: Boosting language models with high-quality feedback. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, Maosong Sun, CoRR, abs/2310.013772023</p>
<p>RAFT: Reward ranked finetuning for generative foundation model alignment. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang, 2023TMLR</p>
<p>Alphazero-like tree-search can guide large language model decoding and training. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, Jun Wang, CoRR, abs/2309.171792023</p>
<p>Reinforced self-training (ReST) for language modeling. Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando De Freitas, CoRR, abs/2308.089982023</p>
<p>Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar, 10.18653/v1/2022.acl-long.234ACL. 2022</p>
<p>From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models. Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, Yanghua Xiao, CoRR, abs/2404.158462024</p>
<p>LoRA: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, ICLR. 2022</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2023Mistral 7b. CoRR, abs/2310.06825</p>
<p>From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, CoRR, abs/2406.119392024</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, ACL Workshop on Text Summarization Branches Out. 2004</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, NeurIPS. 2023</p>
<p>Introducing ChatGPT. 2022OpenAI</p>
<p>Instruction tuning with GPT-4. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao, CoRR, abs/2304.032772023</p>
<p>Red teaming language models with language models. Ethan Perez, Saffron Huang, H Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat Mcaleese, Geoffrey Irving, 10.18653/v1/2022.emnlp-main.225EMNLP. 2022</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019OpenAI Blog</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, NeurIPS. 2023</p>
<p>Training language models with language feedback at scale. Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, CoRR, abs/2303.167552023</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, CoRR, abs/1707.063472017</p>
<p>Get to the point: Summarization with pointergenerator networks. Abigail See, Peter J Liu, Christopher D Manning, 10.18653/v1/P17-1099ACL. 2017</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, NeurIPS. 2024</p>
<p>SALMON: self-alignment with instructable reward models. Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan, ICLR. 2024</p>
<p>Principle-driven selfalignment of language models from scratch with minimal human supervision. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan, 2023In NeurIPS</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288</p>
<p>Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang, CoRR, abs/2406.128452024</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, ICLR. 2023</p>
<p>Collie: Systematic construction of constrained text generation tasks. Shunyu Yao, Howard Chen, Austin W Hanjie, Runzhe Yang, Karthik Narasimhan, ICLR. 2024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, NeurIPS. 2023a</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, ICLR. 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
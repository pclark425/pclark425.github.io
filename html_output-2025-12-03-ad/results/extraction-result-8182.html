<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8182 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8182</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8182</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-277349587</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.21760v2.pdf" target="_blank">MemInsight: Autonomous Memory Augmentation for LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8182.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8182.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GhostInMinecraft</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that describes LLM-based agents for open-world (Minecraft) environments which leverage text-based knowledge and memory; cited here as related work but not used or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as related work on LLM agents in open-world text environments; this paper does not describe or run that agent.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Minecraft / open-world text environment (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Cited as an example of an open-world environment where LLM agents use text-based knowledge and memory; no experimental details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation or direct comparison data for this work is presented in this paper; it is only referenced in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed here; the MemInsight paper only cites the work in passing and provides no experimental or failure details.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>This paper cites the work to motivate memory-enabled agents for complex environments but provides no recommendations specific to Ghost in the Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemInsight: Autonomous Memory Augmentation for LLM Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8182.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8182.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work on hierarchical working memory for long-horizon agent tasks with LLMs; cited in the context of related memory-management approaches but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as an example of hierarchical working memory management for LLM agents; the MemInsight paper does not implement or evaluate HiAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mentioned in related work about memory mechanisms for long-horizon agent tasks; no text-game benchmark details are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working (hierarchical) (as described in cited title)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No direct comparisons or ablations involving HiAgent are performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not reported here; only cited in related-work context.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>MemInsight positions itself relative to hierarchical/working-memory approaches in related work but does not draw task-specific recommendations about HiAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemInsight: Autonomous Memory Augmentation for LLM Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8182.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8182.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dialogue_TextAdventure_Mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based adventure game mention within dialogue augmentations/examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A textual dialogue example (in qualitative prompts/annotations) mentions a 'text-based adventure game' as an activity; this is an example in the dialogue data, not an experiment of an LLM agent playing such a game.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An illustrative dialogue turn in the paper's qualitative examples references a user working on a text-based adventure game; not an LLM agent playing a game.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A mention inside an example dialogue used to show attribute augmentation; not a benchmarked text-game environment.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablations or comparisons relevant to text-game play are present; the mention is illustrative within dialogue augmentation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not applicable — merely an example in dialogue content.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>No recommendations regarding agents playing text-based adventure games are derived from this example in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MemInsight: Autonomous Memory Augmentation for LLM Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory <em>(Rating: 2)</em></li>
                <li>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model <em>(Rating: 2)</em></li>
                <li>Mem0: Building production-ready ai agents with scalable long-term memory <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>A-mem: Agentic memory for llm agents <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 1)</em></li>
                <li>Agent workflow memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8182",
    "paper_id": "paper-277349587",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "GhostInMinecraft",
            "name_full": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "brief_description": "Referenced prior work that describes LLM-based agents for open-world (Minecraft) environments which leverage text-based knowledge and memory; cited here as related work but not used or evaluated in this paper.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": "Mentioned as related work on LLM agents in open-world text environments; this paper does not describe or run that agent.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Minecraft / open-world text environment (as cited)",
            "benchmark_description": "Cited as an example of an open-world environment where LLM agents use text-based knowledge and memory; no experimental details are provided in this paper.",
            "memory_used": true,
            "memory_type": null,
            "memory_architecture": null,
            "memory_integration_strategy": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation or direct comparison data for this work is presented in this paper; it is only referenced in related work.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Not discussed here; the MemInsight paper only cites the work in passing and provides no experimental or failure details.",
            "recommendations_or_conclusions": "This paper cites the work to motivate memory-enabled agents for complex environments but provides no recommendations specific to Ghost in the Minecraft.",
            "uuid": "e8182.0",
            "source_info": {
                "paper_title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "HiAgent",
            "name_full": "Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model",
            "brief_description": "Referenced prior work on hierarchical working memory for long-horizon agent tasks with LLMs; cited in the context of related memory-management approaches but not used in experiments here.",
            "citation_title": "Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": "Referenced as an example of hierarchical working memory management for LLM agents; the MemInsight paper does not implement or evaluate HiAgent.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": null,
            "benchmark_description": "Mentioned in related work about memory mechanisms for long-horizon agent tasks; no text-game benchmark details are given in this paper.",
            "memory_used": true,
            "memory_type": "working (hierarchical) (as described in cited title)",
            "memory_architecture": null,
            "memory_integration_strategy": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No direct comparisons or ablations involving HiAgent are performed in this paper.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Not reported here; only cited in related-work context.",
            "recommendations_or_conclusions": "MemInsight positions itself relative to hierarchical/working-memory approaches in related work but does not draw task-specific recommendations about HiAgent.",
            "uuid": "e8182.1",
            "source_info": {
                "paper_title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Dialogue_TextAdventure_Mention",
            "name_full": "Text-based adventure game mention within dialogue augmentations/examples",
            "brief_description": "A textual dialogue example (in qualitative prompts/annotations) mentions a 'text-based adventure game' as an activity; this is an example in the dialogue data, not an experiment of an LLM agent playing such a game.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": "An illustrative dialogue turn in the paper's qualitative examples references a user working on a text-based adventure game; not an LLM agent playing a game.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": null,
            "benchmark_description": "A mention inside an example dialogue used to show attribute augmentation; not a benchmarked text-game environment.",
            "memory_used": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_integration_strategy": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablations or comparisons relevant to text-game play are present; the mention is illustrative within dialogue augmentation prompts.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Not applicable — merely an example in dialogue content.",
            "recommendations_or_conclusions": "No recommendations regarding agents playing text-based adventure games are derived from this example in the paper.",
            "uuid": "e8182.2",
            "source_info": {
                "paper_title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "rating": 2,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_environments_via_large_language_models_with_textbased_knowledge_and_memory"
        },
        {
            "paper_title": "Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model",
            "rating": 2,
            "sanitized_title": "hiagent_hierarchical_working_memory_management_for_solving_longhorizon_agent_tasks_with_large_language_model"
        },
        {
            "paper_title": "Mem0: Building production-ready ai agents with scalable long-term memory",
            "rating": 2,
            "sanitized_title": "mem0_building_productionready_ai_agents_with_scalable_longterm_memory"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "A-mem: Agentic memory for llm agents",
            "rating": 2,
            "sanitized_title": "amem_agentic_memory_for_llm_agents"
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 1,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Agent workflow memory",
            "rating": 1,
            "sanitized_title": "agent_workflow_memory"
        }
    ],
    "cost": 0.009781,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MemInsight: Autonomous Memory Augmentation for LLM Agents
31 Jul 2025</p>
<p>Rana Salama 
Jason Cai 
Michelle Yuan miyuan@amazon.com 
Anna Currey ancurrey@amazon.com 
Monica Sunkara sunkaral@amazon.com 
Yi Zhang 
Yassine Benajiba benajiy@amazon.com 
MemInsight: Autonomous Memory Augmentation for LLM Agents
31 Jul 2025257F838C515C87C88A92E22DD5B3DE1FarXiv:2503.21760v2[cs.CL]
Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools.A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge.However, the growing memory size and need for semantic structuring pose significant challenges.In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms.By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses.We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization.On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%.Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks</p>
<p>Introduction</p>
<p>LLM agents have emerged as an advanced framework to extend the capabilities of LLMs to improve reasoning (Yao et al., 2023;Wang et al., 2024c), adaptability (Wang et al., 2024d), and selfevolution (Zhao et al., 2024a;Wang et al., 2024e;Tang et al., 2025).A key component of these agents is their memory module, which retains past interactions to allow more coherent, consistent, and personalized responses across various tasks.The memory of the LLM agent is designed to emulate human cognitive processes by simulating how knowledge is accumulated and historical experiences are leveraged to facilitate complex reasoning and the retrieval of relevant information to inform actions (Zhang et al., 2024).However, the advantages of an LLM agent's memory also introduce notable challenges (Wang et al., 2024b).As interactions accumulate over time, retrieving relevant information becomes increasingly difficult, especially in long-term or complex tasks.Raw historical data grows rapidly and, without effective memory management, can become noisy and imprecise, hindering retrieval and degrading agent performance.Moreover, unstructured memory limits the agent's ability to integrate knowledge across tasks and contexts.Therefore, structured knowledge representation is essential for efficient retrieval, enhancing contextual understanding, and supporting scalable long-term memory in LLM agents.Improved memory management enables better retrieval and contextual awareness, making this a critical and evolving area of research.</p>
<p>Hence, in this paper we introduce an autonomous memory augmentation approach, MemInsight, which empowers LLM agents to identify critical information within the data and proactively propose effective attributes for memory enhancements.This is analogous to the human processes of attentional control and cognitive updating, which involve selectively prioritizing relevant information, filtering out distractions, and continuously refreshing the mental workspace with new and pertinent data (Hu et al., 2024;Hou et al., 2024).</p>
<p>MemInsight autonomously generates augmentations that encode both relevant semantic and contextual information for memory.These augmentations facilitate the identification of memory components pertinent to various tasks.Accordingly, MemInsight can improve memory retrieval by leveraging relevant attributes of memory, thereby supporting autonomous LLM agent adaptability and self-evolution.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We propose a structured autonomous approach that adapts LLM agents' memory representations while preserving context across extended conversations for various tasks.</p>
<p>• We design and apply memory retrieval methods that leverage the generated memory augmentations to filter out irrelevant memory while retaining key historical insights.</p>
<p>• Our promising empirical findings demonstrate the effectiveness of MemInsight on several tasks: conversational recommendation, question answering, and event summarization.</p>
<p>Related Work</p>
<p>Well-organized and semantically rich memory structures enable efficient storage and retrieval of information, allowing LLM agents to maintain contextual coherence and provide relevant responses.</p>
<p>Developing an effective memory module in LLM agents typically involves two critical components: structural memory generation and memory retrieval methods (Zhang et al., 2024;Wang et al., 2024a).</p>
<p>LLM Agents Memory Recent research in LLM agents memory focuses on storing and retrieving prior interactions to improve adaptability and generalization (Packer et al., 2024;Zhao et al., 2024a;Zhang et al., 2024;Zhu et al., 2023).Common approaches structure memory as summaries, temporal events, or reasoning chains to reduce redundancy and highlight key information (Maharana et al., 2024;Anokhin et al., 2024;Liu et al., 2023a).Some methods enrich raw dialogues with semantic annotations, such as event sequences (Zhong et al., 2023;Maharana et al., 2024) or reusable workflows (Wang et al., 2024f).Recent models like A-Mem (Xu et al., 2025) that uses manually defined task-specific notes to structure an agent's memory, while Mem0 (Chhikara et al., 2025) offers a scalable, real-time memory pipeline for production use.However, most existing methods rely on unstructured memory or manually defined schemas.In contrast, MemInsight autonomously discovers semantically meaningful attributes, enabling structured memory representation without human-crafted definitions.</p>
<p>LLM Agents Memory Retrieval</p>
<p>Recent work has explored memory retrieval techniques to improve efficiency when handling large-scale historical context in LLM agents (Hu et al., 2023a;Zhao et al., 2024b;Tack et al., 2024;Ge et al., 2025).Common approaches involves generative retrieval models, which encode memory entries as dense vectors and retrieve the top-k most relevant documents using similarity search (Zhong et al., 2023;Penha et al., 2024).Similarity metrics such as cosine similarity (Packer et al., 2024) are widely used, often in combination with dual-tower dense retrievers, where memory entries are embedded independently and indexed via tools like FAISS (Johnson et al., 2017) for efficient retrieval (Zhong et al., 2023).Additionally, techniques such as Locality-Sensitive Hashing (LSH) are utilized to retrieve tuples containing related entries in memory (Hu et al., 2023b).</p>
<p>Autonomous Memory Augmentation</p>
<p>Our proposed model, MemInsight, encapsulates the agent's memory M , offering a unified framework for augmenting and retrieving user-agent interactions represented as memory instances m.</p>
<p>As new interactions occur, they are autonomously augmented and incorporated into memory, forming an enriched set M = {m 1<augmented> , . . ., m n<augmented> }.As shown in Figure 1, MemInsight comprises three core modules: attribute mining, annotation, and memory retrieval.</p>
<p>Attribute Mining and Annotation</p>
<p>Attribute mining extracts structured and semantically meaningful attributes from input dialogues for memory augmentation.The process follows a principled framework guided by three key dimensions:</p>
<p>(1) Perspective, from which attributes are derived (e.g., entity-or conversation-centric annotations) (2) Granularity, indicating the level of annotation detail (e.g., turn-level or session-level) (3) Annotation, which ensures that extracted attributes are appropriately aligned with the corresponding memory instance.A backbone LLM is leveraged to autonomously identify and generate relevant attributes.</p>
<p>Attribute Perspective</p>
<p>An attribute perspective entails two main orientations: entity-centric and conversation-centric.The entity-centric focuses on annotating specific items referenced in memory, such as books or movies, using attributes that capture their key properties (e.g., director, author, release year).In contrast, the  conversation-centric perspective captures attributes that reflect the overall user interaction with respect to users' intent, preferences, sentiment, emotions, motivations, and choices, thereby improving response generation and memory retrieval.An illustrative example is provided in Figure 4.</p>
<p>Attribute Granularity</p>
<p>Conversation-centric augmentations introduce the notion of attribute granularity, which defines the level of details captured in the augmentation process.The augmentation attributes can be analyzed at varying levels of abstraction, either at the level of individual turns within a user conversation (turnlevel), or across the entire dialogue session (sessionlevel), each offering distinct insights into the con-versational context.Turn-level focuses on the specific content of individual turns to generate more nuanced and contextual attributes, while sessionlevel augmentation captures broader patterns and user intent across the interaction.Figure 2 illustrates this distinction, showing how both levels offer complementary perspectives on a sample dialogue.</p>
<p>Annotation and Attribute Prioritization</p>
<p>Subsequently, the generated attributes and their corresponding values are used to annotate the agent's memory.Annotation is done by aggregating attributes and values in the relevant memory.Given an interaction i, the module applies an LLMbased extraction function F LLM to produce a set of attribute-value pairs:
A = F LLM (D) = {(a j , v j )} k j=1
where: a j ∈ A represents the attribute (e.g., emotion, entity, intent) and v j ∈ V the value of this attribute These attributes are then used to annotate the corresponding memory instance m i , resulting in an augmented memory M a : M a = {(A 1 , m1 ), (A 2 , m2 ), . . ., (A i , mi )}, Attributes are typically aggregated using the Attribute Prioritization method, which can be classified into Basic and Priority.In Basic Augmentation, attributes are aggregated without a predefined order, resulting in an arbitrary sequence.In contrast, Priority Augmentation sorts attribute-value pairs according to their relevance to the memory being augmented.This prioritization follows a structured order in which attribute (A 1 , m1 ) holds the highest significance, ensuring that more relevant attributes are processed first.</p>
<p>Memory Retrieval</p>
<p>MemInsight augmentations are employed to both enrich memory representations and support the retrieval of contextually relevant memory.These augmentations are utilized in one of two ways.</p>
<p>(1) Comprehensive retrieval, retrieves all related memory instances along with their associated augmentations to support context-aware inference.</p>
<p>(2) Refined retrieval, where the current context is augmented to extract task-specific attributes, which then guide the retrieval process through one of the following methods: a-Attribute-based Retrieval: which uses the current attributes as filters to select memory instances with matching or related augmentations only.Given a query session Q with attributes A Q , retrieve relevant memories:
R attr (A Q , M) = Top-k {(A k , M k ) | match(A Q , A k )}
b-Embedding-based Retrieval where memory augmentations are embedded as dense vectors.A query embedding is derived from the current context's augmentations and used to retrieve the top-k most similar memory entries via similarity search.Let ϕ : A k → V d be the embedding function over attributes.Then:
sim(A Q , A k ) = ϕ(A Q )•ϕ(A k ) ∥ϕ(A Q )∥•∥ϕ(A k )∥ R embed (A Q , M) = Top-k {(A k , M k ) | sim(A Q , A k )}
Finally, the retrieved memories are then integrated into the current context to inform the ongoing interaction.Further implementation details of embedding-based retrieval are provided in Appendix C.</p>
<p>Evaluation</p>
<p>Datasets</p>
<p>We evaluate MemInsight on two benchmarks: LLM-REDIAL (Liang et al., 2024) and Lo-CoMo (Maharana et al., 2024).LLM-REDIAL is a dataset for conversational movie recommendation, comprising 10K dialogues and 11K movie mentions.LoCoMo is a dataset for evaluating Question Answering and Event Summarization, with 30 multi-session dialogues between two speakers.It features five question types: Single-hop, Multi-hop, Temporal reasoning, Open-domain, and Adversarial, each annotated with the relevant dialogue turn required for answering.LoCoMo also provides event labels for each speaker in a session, which serve as ground truth for evaluating event summarization.</p>
<p>Experimental Setup</p>
<p>To evaluate our model, we begin by augmenting the datasets using zero-shot prompting to extract relevant attributes and their corresponding values.For attribute generation across tasks, we employ Claude Sonnet1 , LLaMA 32 , and Mistral3 .For the Event Summarization task, we additionally utilize Claude 3 Haiku4 .In embedding-based retrieval, we use the Titan Text Embedding model5 to generate embeddings of augmented memory, which are indexed and searched using FAISS (Johnson et al., 2017).To ensure consistency across all experiments, we use the same base model for the primary tasks: recommendation, answer generation, and summarization, while varying the models used for memory augmentation.Claude Sonnet serves as the backbone LLM in all baseline evaluations.</p>
<p>Evaluation Metrics</p>
<p>We evaluate MemInsight using a combination of standard and LLM-based metrics.For Question Answering, we report F1-score for answer prediction and recall for accuracy; for Conversational Recommendation, we use Recall@K, NDCG@K, along with LLM-based metrics for genre matching.We further incorporate subjective metrics, including Persuasiveness (Liang et al., 2024), which measures how persuasive a recommendation aligns with the ground truth.Additionally, we introduce a Relatedness metric where we prompt an LLM to measure how comparable are recommendation attributes to the ground truth, categorizing them as not comparable, comparable, or highly comparable.For Event Summarization, we adopt G-Eval (Liu et al., 2023b), an LLM-based metric that evaluates the relevance, consistency, and coherence of generated summaries against reference labels.Together, these metrics provide a comprehensive framework for evaluating both retrieval effectiveness and response quality.</p>
<p>Experiments</p>
<p>Questioning Answering</p>
<p>Question Answering experiments are conducted to evaluate the effectiveness of MemInsight in answer generation.We evaluate the overall accuracy to measure the system's ability to retrieve and integrate relevant information using memory augmentations.The base model, which incorporates all historical dialogues without any augmentation, serves as a baseline.Additionally, we report results on the LoCoMo benchmark using the same backbone model (Mistral v1) to ensure a fair evaluation.We also compare with stronger GPT-based baselines, including MemoryBank (Zhong et al., 2023) and ReadAGent (Lee et al., 2024), which utilizes external memory modules to support long-term reasoning.We also consider Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) as a representative baseline of RAG due to its scalability and retrieval efficiency.</p>
<p>Memory Augmentation</p>
<p>In this task, memory is constructed from historical conversational dialogues, which requires the generation of conversation-centric attributes for augmentation.Given that the ground-truth labels consist of dialogue turns relevant to the question, the dialogues are annotated at the turn level.An LLM backbone is prompted to generate augmentation attributes for both conversation-centric and turn-level annotations.</p>
<p>Memory Retrieval To answer a given question, MemInsight first augments it to extract relevant attributes, which guide memory retrieval.In attributebased retrieval, dialogue turns with matching augmentation attributes are retrieved.In embeddingbased retrieval, the question and its attributes are embedded to perform a vector similarity search over indexed memory.The top-k most similar dialogue turns are then integrated into the current context to generate an answer.</p>
<p>Experimental Results</p>
<p>As shown in Table 1, MemInsight achieves significantly higher overall accuracy on the question answering task compared to all baselines, using both attribute-based and embedding-based memory retrieval.In the attribute-based setting, MemInsight with Claude-3-Sonnet demonstrates notable gains in single-hop, temporal, and adversarial questions, which require more complex contextual reasoning.These results highlight the effectiveness of memory augmentation in enriching context and enhancing answer quality.MemInsight further outperforms all other benchmark models across most question types, with the exception of multi-hop and temporal questions in LoCoMo, where evaluation is based on a partial-match F1 metric (Maharana et al., 2024).</p>
<p>For embedding-based retrieval, we evaluate MemInsight using both basic and priority augmentation, alongside the DPR baseline.MemInsight consistently outperforms all baselines, except in temporal and adversarial questions, where DPR achieves slightly higher accuracy.Nevertheless, MemInsight maintains the highest overall accuracy.Priority augmentation also consistently outperforms basic augmentation across nearly all question types, validating its effectiveness in improving contextual relevance.Notably, MemInsight demonstrates substantial gains on multi-hop questions, which require reasoning over multiple pieces of supporting evidence, highlighting its ability to integrate dispersed information from historical dialogue.As shown in Table 2, recall metrics further support this trend, with priority augmentation yielding a 35% overall improvement and consistent gains across all categories.</p>
<p>Conversational Recommendation</p>
<p>We simulate conversational recommendation by preparing dialogues for evaluation under the same conditions proposed by Liang et al. (2024).This process involves masking the dialogue and randomly selecting n = 200 conversations for evaluation to ensure a fair comparison.Each conversational dialogue used is processed by masking the ground truth labels, followed by a turn cut-off, where all dialogue turns following the first masked turn are removed and retained as evaluation labels.Subsequently, the dialogues are augmented using a conversation-centric approach to identify relevant user interest attributes for retrieval.Finally, we prompt the LLM model to generate a movie recommendation that best aligns with the masked token, guided by the augmented movies retrieved based on the user's historical interactions.</p>
<p>The baseline for this evaluation is the results presented in the LLM-REDIAL paper (Liang et al., 2024) which employs zero-shot prompting for rec-  ommendation using the ChatGPT model6 .In addition to the baseline model that uses memory without augmentation.Evaluation includes direct matches between recommended and ground truth movie titles using RE-CALL@[1,5,10] and NDCG@[1,5,10].Furthermore, to address inconsistencies in movie titles generated by LLMs, we incorporate an LLM-based evaluation that assesses recommendations based on genre similarity.Specifically, a recommended movie is considered a valid match if it shares the same genre as the corresponding ground truth label.</p>
<p>Memory Augmentation</p>
<p>We initially augment the dataset with relevant attributes, primarily employing entity-centric augmentations for memory annotation, as the memory consists of movies.In this context, we conduct a detailed evaluation of the generated attributes to provide an initial assessment of the effectiveness and relevance of MemInsight augmentations.To evaluate the quality of the generated attributes, Table 3 presents statistical data on the generated attributes, including the five most frequently occurring attributes across the entire dataset.As shown in the table, the generated attributes are generally relevant, with "genre" being the most significant attribute based on its cumulative frequency across all movies (also shown in Figure 5).However, the relevance of attributes vary, emphasizing the need for prioritization in augmentation.Additionally, the table reveals that augmentation was unsuccessful for 0.1% of the movies, primarily due to the LLM's inability to recognize certain movie titles or because the presence of some words in the movie titles conflicted with the LLM's policy.</p>
<p>Memory Retrieval For this task we evaluate attribute-based retrieval using the Claude-3-Sonnet model with both filtered and comprehensive settings.Additionally, we examine embedding-based retrieval using all other models.For embeddingbased retrieval, we set k = 10, meaning that 10 memory instances are retrieved (as opposed to 144 in the baseline).R@1 R@5 R@10 R@1 R@5 R@10 N@1 N@5 N@10 recall and NDCG metrics.For genre match we find the results to be comparable when considering all attributes.However, attributed-based filtering retrieval still outperforms the LLM-REDIAL model and is comparable to the baseline with almost 90% less memory retrieved.Table 5 presents the results of subjective LLMbased evaluation for Persuasiveness and Relatedness.The findings indicate that memory augmentation enhances partial persuasiveness by 10-11% using both comprehensive and attribute-based retrieval, while also reducing unpersuasive recommendations and increasing highly persuasive ones by 4% in attribute-based retrieval.Furthermore, the results highlights the effectiveness of embeddingbased retrieval, which leads to a 12% increase in highly persuasive recommendations and enhances all relatedness metrics.This illustrates how MemInsight enriches the recommendation process by incorporating condensed, relevant knowledge, thereby producing more persuasive and related recommendations.However, these improvements were not reflected in recall and NDCG metrics.</p>
<p>Experimental Results</p>
<p>Event Summarization</p>
<p>We evaluate the effectiveness of MemInsight in enriching raw dialogues with relevant insights for event summarization.We utilize the generated annotations to identify key events within conversations and hence use them for event summarization.We compare the generated summaries against Lo-CoMo's event labels as the baseline.Figure 3 illustrates the experimental framework, where the baseline is the raw dialogues sent to the LLM model to generate an event summary, then both event summaries, from raw dialogues and augmentation based summaries, are compared to the ground truth</p>
<p>Raw Dialogues</p>
<p>LLM-based Event Summary</p>
<p>Augmentation-based Event Summary</p>
<p>Augmented Dialogue</p>
<p>Attribute Mining</p>
<p>Attribute Granularity</p>
<p>Turn-Level</p>
<p>Session-Level summaries in the LoCoMo dataset.</p>
<p>Augmentations Augmentations Dialogue Evaluation</p>
<p>Memory Augmentation</p>
<p>In this experiment, we evaluate the effectiveness of augmentation granularity; turn-level dialogue augmentations as opposed to session-level dialogue annotations.We additionally, consider studying the effectiveness of using only the augmentations to generate the event summaries as opposed to using both the augmentations and their corresponding dialogue content.</p>
<p>Experimental Results</p>
<p>As shown in Table 6, our MemInsight model achieves performance comparable to the baseline, despite relying only on dialogue turns or sessions containing the event label.</p>
<p>Notably, turn-level augmentations provided more precise and detailed event information, leading to improved performance over both the baseline and session-level annotations.</p>
<p>For Claude-3-Sonnet, all metrics remain comparable, indicating that memory augmentations effectively capture the semantics within dialogues at both the turn and session levels.This proves that the augmentations sufficiently enhance context representation for generating event summaries.To further investigate how backbone LLMs impact aug-  mentation quality, we employed Claude-3-Sonnet as opposed to Llama v3 for augmentation while still using Llama for event summarization.As presented in Table 7, Sonnet augmentations resulted in improved performance for all metrics, providing empirical evidence for the effectiveness and stability of Sonnet in augmentation.Additional experiments and detailed analysis are provided in Appendix E.4.</p>
<p>Qualitative Analysis</p>
<p>To more rigorously assess the quality of the autonomously generated augmentations, we conduct a qualitative analysis of the annotations produced by Claude-3 Sonnet.Using the DeepEval hallucination metric (Yang et al., 2024), we find that 99.14% of the annotations are grounded in the dialogue, demonstrating a high level of factual consistency.The remaining 0.86% primarily consist of abstract or generic attributes, rather than explicit inaccuracies.Additional experimental details and examples are provided in Appendix F.</p>
<p>Conclusion</p>
<p>This paper introduced MemInsight, an autonomous memory augmentation framework that enhances LLM agents' memory through structured, attributebased augmentations.While maintaining competitive performance on standard metrics, MemInsight achieves substantial improvements in LLM-based evaluation scores, demonstrating its effectiveness in capturing semantic relevance and improving performance across tasks and datasets.Experimental results show that both attribute-based filtering and embedding-based retrieval methods effectively leverage the generated augmentations.Prioritybased augmentation, in particular, improves similarity search and retrieval accuracy.MemInsight also complements traditional RAG models by enabling customized, attribute-guided retrieval, enhancing the integration of memory with LLM reasoning.Moreover, in benchmark comparisons, MemInsight consistently outperforms baseline models in overall accuracy and delivers stronger performance in recommendation tasks, yielding more persuasive outputs.Qualitative analysis further confirms the high factual consistency of the generated annotations.These results highlight MemInsight's potential as a scalable memory solution for LLM agents.</p>
<p>While MemInsight demonstrates strong performance across multiple tasks and datasets, several limitations remain and highlight areas for future exploration.Although the model autonomously generates augmentations, it may occasionally produce abstract or overly generic annotations, especially in ambiguous dialogue contexts.While these are not factually incorrect, they may reduce retrieval specificity in tasks requiring fine-grained memory access.Additionally, MemInsight 's performance is dependent on the capabilities of the underlying LLM used for attribute generation.Less capable or unaligned models may produce less consistent augmentations.We also acknowledge that our current implementation is limited to text-based interactions.Future work could extend MemInsight to support multimodal inputs, such as images or audio, enabling richer and comprehensive contextual representations.</p>
<p>A Ethical Consideration</p>
<p>We have thoroughly reviewed the licenses of all scientific artifacts, including datasets and models, ensuring they permit usage for research and publication purposes.To protect anonymity, all datasets used are de-identified.Our proposed method demonstrates considerable potential in significantly reducing both the financial and environmental costs typically associated with enhancing large language models.By lessening the need for extensive data collection and human labeling, our approach not only streamlines the process but also provides an effective safeguard for user and data privacy, reducing the risk of information leakage during training corpus construction.Additionally, throughout the paper-writing process, Generative AI was exclusively utilized for language checking, paraphrasing, and refinement.</p>
<p>B Autonomous Memory Augmentation B.1 Attribute Mining</p>
<p>Figure 4 illustrates examples for the two types of attribute augmentation: entity-centric and conversation-centric.The entity-centric augmentation represents the main attributes generated for the book entitled 'Already Taken', where attributes are derived based on entity-specific characteristics such as genre, author, and thematic elements.The conversation-centric example illustrates the augmentation generated for a sample two turns dialogue from the LLM-REDIAL dataset, highlighting attributes that capture contextual elements such as user intent, motivation, emotion, perception, and genre of interest.</p>
<p>Furthermore, Figure 5 presents an overview of the top five attributes across different domains in the LLM-REDIAL dataset.These attributes represent the predominant attributes specific to each domain, highlighting the significance of different attributes in augmentation generation.Consequently, the integration of priority-based embeddings has led to improved performance.</p>
<p>C Embedding-based Retrieval</p>
<p>In the context of embedding-based memory retrieval, movies are augmented using MemInsight, and the generated attributes are embedded to retrieve relevant movies from memory.Two main embedding methods were considered:</p>
<p>(1) Averaging Over Independent Embeddings Each attribute and its corresponding value in the generated augmentations is embedded independently.The resulting attribute embeddings are then averaged across all attributes to generate the final embedding vector representation, as illustrated in Figure 6 which are subsequently used in similarity search to retrieve relevant movies.</p>
<p>(2) All Augmentations Embedding In this method, all generated augmentations, including all attributes and their corresponding values, are encoded into a single embedding vector and stored for retrieval as shown in Figure 6.Additionally, Figure 7 presents the cosine similarity results for both methods.As depicted in the figure, averaging over all augmentations produces a more consistent and reliable measure, as it comprehensively captures all attributes and effectively differentiates between similar and distinct characteristics.Consequently, this method was adopted in our experiments.</p>
<p>D Question Answering</p>
<p>D.1 Prompts</p>
<p>Table 8 outlines the prompts used in the Question Answering task for generating augmentations in both questions and conversations.</p>
<p>E Conversational Recommendation E.1 Prompts</p>
<p>Table 9 presents the prompts used in Conversational Recommendation for movie recommendations, incorporating both basic and priority augmentations.</p>
<p>E.2 Evaluation Framework</p>
<p>Figure 8 presents the evaluation framework for the Conversation Recommendation task.The process begins with (1) augmenting all movies in memory using entity-centric augmentations to enhance retrieval effectiveness.(2) Next, all dialogues in the dataset are prepared to simulate the recommendation process by masking the ground truth labels and prompting the LLM to find the masked labels based on augmentations from previous user interactions.(3) Recommendations are then generated using the retrieved memory, which may be attributebased-for instance, filtering movies by specific attributes such as genre or using embedding-based retrieval.(4) Finally, the recommended movies are evaluated against the ground truth labels to assess the accuracy and effectiveness of the retrieval and recommendation approach.</p>
<p>E.3 Event Summarization</p>
<p>E.3.1 Prompts</p>
<p>Table 10 presents the prompt used in Event Summarization to augment dialogues by generating relevant attributes.In this process, only attributes related to events are considered to effectively summarize key events from dialogues, ensuring a focused and structured summarization approach.</p>
<p>E.4 Additional Experiments</p>
<p>In this experiment, we include an additional baseline for event summarization: raw summaries generated directly by LLMs using zero-shot prompting, without any memory augmentation.This serves as a clear reference point to isolate the impact of MemInsight 's augmentation strategy on summa-rization quality.Table 11 shows the results of this experiment.As illustrated, MemInsight consistently improves event summarization quality across models, with the best performance achieved when augmentations are integreted with dialogue context highlighting the value of fine-grained annotations and contextual grounding.Overall, the findings confirm that MemInsight enhances the factual and semantic quality of generated summaries.same models.Notably, Claude-Sonnet maintains consistency across both turns, suggesting its stable performance throughout all experiments.While Mistral model tend to be less stable as it included attributes that are not in the dialogue.A hallucination evaluation conducted using DeepEval yielded a score of 99.14%, indicating strong factual consistency.Table 12 presents examples of annotations with lower scores.While these annotations are more generic or abstract, they remain semantically aligned with the original input.</p>
<p>F Qualitative Analysis</p>
<p>Basic Augmentation</p>
<p>For the following movie identify the most important attributes independently.Determine all attributes that describe the movie based on your knowledge of this movie.Choose attribute names that are common characteristics of movies in general.Respond in the following format: [attribute]<value of attribute>.The Movie is: {}</p>
<p>Priority Augmentation</p>
<p>You are a movie annotation expert tasked with analyzing movies and generating key-attribute pairs.For the following movie identify the most important.Determine all attribute that describe the movie based on your knowledge of this movie.Choose attribute names that are common characteristics of movies in general.Respond in the following format:</p>
<p>[attribute]<value of attribute>.Sort attributes from left to right based on their relevance.The Movie is:{} Dialogue Augmentation Identify the key attributes that best describe the movie the user wants for recommendation in the dialogue.These attributes should encompass movie features that are relevant to the user sorted descendingly with respect to user interest.Respond in the format: [attribute]<value>.</p>
<p>Table 9: Prompts used in Conversational Recommendation for recommending Movies utilizing both basic and priority augmentations.</p>
<p>Dialogue Augmentation</p>
<p>Given the following attributes and values that annotate a dialogue for every speaker in the format [attribute]<value>, generate a summary for the event attributes only to describe the main and important events represented in these annotations.Refrain from mentioning any minimal event.Include any event-related details and speaker.Format: a bullet paragraph for major life events for every speaker with no special characters.Don't include anything else in your response or extra text or lines.Don't include bullets.Input annotations: {} [["Evan's son had an accident where he fell off his bike last Tuesday but is doing better now.",D20:3], ["Evan is supportive and encouraging towards Sam, giving advice to believe in himself and take things one day at a time.",D20:9], ["Evan is a painter who finished a contemporary figurative painting emphasizing emotion and introspection.",D20:15], ["Evan had a painting published in an exhibition with the help of a close friend.",D20:17]], 'Sam': [["Sam used to love hiking but hasn't had the chance to do it recently.",D20:6], ["Sam is struggling with weight and confidence issues, feeling like they lack motivation.",D20:8], ["Sam acknowledges that trying new things can be difficult.",D20:12]] "evan":{"[event]":"<son's accident>", "[emotion]":"<worry>", "[hobby]":"<hiking>", "[activity]":"<painting>"}, "sam":{"[emotion]":"<struggling>", "[issue]":"<weight>", "[emotion]":"<lack of confidence>", "[action]":"<trying new things>"}} 0.66 {'James': [["James has a dog named Ned that he adopted and can't imagine life without.",D21:3], ["James is interested in creating a strategy game similar to Civilization.",D21:9], ["James suggested meeting at Starbucks for coffee with John.",D21:13]], 'John': [["John helps his younger siblings with programming and is proud of their progress", D21:2], ["John is working on a coding project with his siblings involving a text-based adventure game.",D21:6], ["John prefers light beers over dark beers when going out.", D21:16], ["John agreed to meet James at McGee's Pub after discussing different options.",D21:18]]} {"james":{"[emotion]":"<excited>", "[intent]":"<socializing>", "[topic]":"<dogs>", "[topic]":"<gaming>", "[topic]":"<starbucks>", "[topic]":"<pubMeeting>", "[activity]":"<coffee>", "[activity]":"<beer>"}, "john":{"[topic]":"<siblings>", "[topic]":"<programming>", "[activity]":"<adventure game>", "[emotion]":"<proud>", "[intent]":"<socializing>"}} 0.50</p>
<p>Figure 1 :
1
Figure 1: MemInsight framework comprising three core modules: Attribute Mining (including perspective and granularity), Annotation (with attribute prioritization), and Memory Retrieval (including refined and comprehensive retrieval).These components are triggered by various downstream tasks such as Question Answering, Event Summarization, and Conversational Recommendation.</p>
<p>Figure 2 :
2
Figure 2: An example for Turn level and Session level annotations for a sample dialogue conversation from the LoCoMo Dataset.</p>
<p>Figure 3 :
3
Figure 3: Evaluation framework for event summarization with MemInsight, exploring augmentation at Turn and Session levels, considering attributes alone or both attributes and dialogues for richer summaries.</p>
<p>Table 7 :
7
Results for Event Summarization using Llama v3, where the baseline is the model without augmentation as opposed to the augmentation model (turn-level) using Claude-3-Sonnet vs Llama v3.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: An example of entity-centric augmentation for the book 'Already Taken', and a conversation-centric augmentation for a sample dialogue from the LLM-REDIAL dataset.</p>
<p>Figure 9 Figure 6 :Figure 7 :
967
Figure9illustrates the augmentations generated using different LLM models, including Claude-Sonnet, Llama, and Mistral for a dialogue turn from the LoCoMo dataset.As depicted in the figure, augmentations produced by Llama include hallucinations, generating information that does not exist.In contrast, Figure10presents the augmentations for the subsequent dialogue turn using the</p>
<p>Figure 8 :
8
Figure 8: Evaluation Framework for Conversation Recommendation Task.</p>
<p>Figure 9 :
9
Figure 9: Augmentation generated on a Turn-level for a sample dialogue turn from the LoCoMo dataset using Claude-3-Sonnet, Llama v3 and Mistral v1 models.</p>
<p>Figure 10 :
10
Figure10: Augmentations generated for the turn following the turn in Figure9using Claude-3-Sonnet, Llama v3 and Mistral v1 models.Hallucinations are presented in red.</p>
<p>Table 1 :
1
Results for F1 Score (%) for answer generation accuracy for attribute-based and embedding-based memory retrieval methods.Baseline is Claude-3-Sonnet model to generate answers using all memory without augmentation, for Attribute-based retrieval.In addition to the Dense Passage Retrieval(DPR) for Embedding-based retrieval.Evaluation is done with k = 5.Best results per question category over all methods are in bold.
ModelSingle-hopMulti-hopTemporalOpen-domainAdversarialOverallBaseline (Claude-3-Sonnet)15.010.03.326.045.326.1LoCoMo (Mistral v1)10.212.816.119.517.013.9ReadAgent (GPT-4o)9.112.65.39.69.818.5MemoryBank (GPT-4o)5.09.65.56.67.36.2Attribute-based RetrievalMemInsight (Claude-3-Sonnet)18.010.37.527.058.329.1Embedding-Based RetrievalRAG Baseline (DPR)11.99.06.312.089.928.7MemInsight (Llama v3 P riority )14.313.46.015.882.729.7MemInsight (Mistral v1 P riority )16.114.16.116.781.230.0MemInsight (Claude-3-Sonnet Basic )14.713.85.815.682.129.6MemInsight (Claude-3-Sonnet P riority )15.815.86.719.775.330.1ModelSingle-hopMulti-hopTemporalOpen-domainAdversarialOverallRAG Baseline (DPR)15.731.415.415.434.926.5MemInsight (Llama v3 P riority )31.363.623.853.428.744.9MemInsight (Mistral v1 P riority )31.463.926.958.136.748.9MemInsight (Claude-3-Sonnet Basic )33.267.129.556.235.748.8MemInsight (Claude-3-Sonnet P riority )39.775.132.670.949.760.5</p>
<p>Table 2 :
2
Results for the RECALL@k=5 accuracy for Embedding-based retrieval for answer generation using LoCoMo dataset.Dense Passage Retrieval(DPR) RAG model is the baseline.Best results are in bold.
StatisticCountTotal Movies9687Avg. Attributes7.39Failed Attributes0.10%Genre9662Release year 5998Top-5 AttributesDirector5917Setting4302Characters3603</p>
<p>Table 3 :
3
Statistics of attributes generated for the LLM-REDIAL Movie dataset, which include total number of movies, average number of attributes per item, number of failed attributes, and the counts for the most frequent five attributes.</p>
<p>Table 4 shows the results for conversational recommendation evaluating comprehensive setting, attribute-based retrieval and embedding-based retrieval.As shown in the table, comprehensive memory augmentation tends to outperform the baseline and LLM-REDIAL model for
ModelAvg. Items RetrievedDirect Match (↑)Genre Match (↑)NDCG(↑)</p>
<p>Table 4 :
4
Results for Movie Conversational Recommendation using (1) Attribute-based retrieval with Claude-3-Sonnet model (2) Embedding-based retrieval across models (Llama v3, Mistral v1, Claude-3-Haiku, and Claude-3-Sonnet) (3) Comprehensive setting using Claude-3-Sonnet that includes ALL augmentations.Evaluation metrics include RECALL, NDCG, and an LLMbased genre matching metric, with n = 200 and k = 10.Baseline is Claude-3-Sonnet without augmentation.Best results are in bold.
Baseline (Claude-3-Sonnet)1440.0000.0100.0150.3200.570.6600.0050.0070.008LLM-REDIAL Model144-0.0000.005----0.0000.001Attribute-Based RetrievalMemInsight (Claude-3-Sonnet)150.0050.0150.0150.2700.5400.6400.0050.0070.007Embedding-Based RetrievalMemInsight (Llama v3)100.0000.0050.0280.3800.5800.6700.0000.0020.001MemInsight (Mistral v1)100.0050.0100.0100.3800.5500.6300.0050.0070.007MemInsight (Claude-3-Haiku)100.0050.0100.0100.3600.6100.6500.0050.0070.007MemInsight (Claude-3-Sonnet)100.0050.0150.0150.4000.6000.640.0050.0100.010ComprehensiveMemInsight (Claude-3-Sonnet)1440.0100.0200.0250.3000.5900.6900.0100.0150.017</p>
<p>Table 5 :
5
Movie Recommendations results (with similar settings to Table 4) using LLM-based metrics; (1) Persuasiveness-% of Unpersuasive (lower is better), Partially, and Highly Persuasive cases.(2) Relatedness-% of Not Comparable (lower is better), Comparable, and Exactly Matching cases.Best results are in bold.Comprehensive setting includes ALL augmentations.Totals may NOT sum to 100% due to cases the LLM model could not evaluate.
ModelAvg. Items RetrievedLLM-Persuasiveness %LLM-Relatedness%Unpers<em>Partially Pers.Highly Pers.Not Comp</em>CompMatchBaseline (Claude-3-Sonnet)14416.064.013.057.041.02.0Attribute-Based RetrievalMemInsight (Claude-3-Sonnet)152.075.017.040.554.02.0Embedding-Based RetrievalMemInsight (Llama v3)1011.363.020.419.380.10.5MemInsight (Mistral v1)1016.361.218.016.382.55.0MemInsight (Claude-3-Haiku)101.653.025.023.374.42.2MemInsight (Claude-3-Sonnet)102.059.520.029.568.02.5ComprehensiveMemInsight (Claude-3-Sonnet)1442.074.012.042.556.01.0ModelClaude-3-SonnetLlama v3Mistral v1Claude-3-HaikuRel.Coh.Con.Rel.Coh.Con.Rel.Coh.Con.Rel.Coh.Con.Baseline Summary3.273.522.862.032.642.683.393.714.104.004.43.83MemInsight (TL)3.083.332.761.572.171.952.542.532.493.934.33.59MemInsight (SL)3.083.392.682.02.623.674.134.414.293.964.303.77MemInsight +Dialogues (TL)3.293.462.922.452.192.874.304.534.604.234.524.16MemInsight +Dialogues (SL)3.053.412.692.242.803.864.044.484.333.934.333.73</p>
<p>Table 6 :
6
Event Summarization results using G-Eval metrics (higher is better): Relevance, Coherence, and Consistency.Comparing summaries generated with augmentations only at Turn-Level (TL) and Session-Level (SL) and summaries generated using both augmentations and dialogues (MemInsight +Dialogues) at TL and SL.Best results are in bold.
ModelG-Eval % (↑)Rel. Coh. Con.Baseline(Llama v3 )2.03 2.64 2.68Llama v3 + Llama v32.45 2.19 2.87Claude-3-Sonnet + Llama v3 3.15 3.59 3.17</p>
<p>Question AugmentationGiven the following question, determine what are the main inquiry attribute to look for and the person the question is for.Respond in the format: Person:[names]Attributes:[].Basic AugmentationYou are an expert annotator who generates the most relevant attributes in a conversation.Given the conversation below, identify the key attributes and their values on a turn by turn level.Attributes should be specific with most relevant values only.Don't include speaker name.Include value information that you find relevant and their names if mentioned.Each dialogue turn contains a dialogue id between [ ]. Make sure to include the dialogue the attributes and values are extracted form.Important: Respond only in the format [{speaker name:[Dialog id]:[attribute]<value>}].Dialogue Turn:{} Priority Augmentation You are an expert dialogue annotator, given the following dialogue turn generate a list of attributes and values for relevant information in the text.Generate the annotations in the format: [attribute]<value>where attribute is the attribute name and value is its corresponding value from the text.and values for relevant information in this dialogue turn with respect to each person.Be concise and direct.Include person name as an attribute and value pair.Please make sure you read and understand these instructions carefully.1-Identify the key attributes in the dialogue turn and their corresponding values.2-Arrange attributes descendingly with respect to relevance from left to right.3-Generate the sorted annotations list in the format: [attribute]<value>where attribute is the attribute name and value is its corresponding value from the text.4-Skip all attributes with none vales Important: YOU MUST put attribute name is between [ ] and value between &lt;&gt;.Only return a list of [attribute]<value>nothing else.Dialogue Turn: {}</p>
<p>Table 8 :
8
Prompts used in Question Answering for generating augmentations for questions.Also, augmentations for conversations, utilizing both basic and priority augmentations.</p>
<p>Table 10 :
10
Prompt used in Event Summarization to augment dialogues
ModelLlama v3Mistral v1Claude-3 HaikuClaude-3 SonnetRel.Coh.Con.Rel.Coh.Con.Rel.Coh.Con.Rel.Coh.Con.Baseline LLM Summary2.232.662.633.343.774.113.974.333.793.273.642.78MemInsight (TL)1.602.171.952.532.492.383.984.373.663.093.272.77MemInsight (SL)1.802.623.674.094.384.193.944.313.693.083.392.68MemInsight + Dialogues (TL)2.412.793.014.304.534.604.244.434.163.253.432.86MemInsight + Dialogues (SL)2.012.703.864.044.484.343.954.333.713.023.372.73</p>
<p>Table 11 :
11
LLM-based evaluation scores for event summarization using relevance (Rel.), coherence (Coh.), and consistency (Con.)across different models and augmentation settings.Baseline summaries are generated using zero-shot prompting without memory augmentation.MemInsight is evaluated in both turn-level (TL) and sessionlevel (SL) configurations, with and without access to dialogue context.
InputAugmentationsHall. Score'Evan':</p>
<p>Table 12 :
12
MemInsight annotations that scored below 1% hallucination rate in the DeepEval hallucination evaluation.</p>
<p>claude-3-sonnet-20240229-v1 <br />
 llama3-70b-instruct-v1 <br />
 mistral-7b-instruct-v0 <br />
 claude-3-haiku-20240307-v1 <br />
 titan-embed-text-v2:0 <br />
https://openai.com/blog/chatgpt</p>
<p>Arigraph: Learning knowledge graph world models with episodic memory for llm agents. Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev, arXiv:2407.043632024Preprint</p>
<p>Mem0: Building production-ready ai agents with scalable long-term memory. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav, arXiv:2504.194132025Preprint</p>
<p>Tremu: Towards neuro-symbolic temporal reasoning for llm-agents with memory in multi-session dialogues. Yubin Ge, Salvatore Romeo, Jason Cai, Raphael Shu, Monica Sunkara, Yassine Benajiba, Yi Zhang, arXiv:2502.016302025arXiv preprint</p>
<p>my agent understands me better": Integrating dynamic human-like memory recall and consolidation in llm-based agents. Yuki Hou, Haruki Tamoto, Homei Miyashita, 10.1145/3613905.3650839Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. ACM2024</p>
<p>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.03901Chatdb: Augmenting llms with databases as their symbolic memory. 2023aarXiv preprint</p>
<p>Chatdb: Augmenting llms with databases as their symbolic memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.039012023bPreprint</p>
<p>Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, Ping Luo, arXiv:2408.095592024Preprint</p>
<p>Billion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Hervé Jégou, arXiv:1702.087342017Preprint</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen Tau, Yih , arXiv:2004.049062020Preprint</p>
<p>A human-inspired reading agent with gist memory of very long contexts. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer, arXiv:2402.097272024Preprint</p>
<p>LLM-REDIAL: A large-scale dataset for conversational recommender systems created from user behaviors with LLMs. Tingting Liang, Chenxin Jin, Lingzhi Wang, Wenqi Fan, Congying Xia, Kai Chen, Yuyu Yin, 10.18653/v1/2024.findings-acl.529Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023aPreprint</p>
<p>G-eval: Nlg evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.166342023bPreprint</p>
<p>Evaluating very long-term conversational memory of llm agents. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang, arXiv:2402.177532024Preprint</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2024Preprint</p>
<p>Bridging search and recommendation in generative retrieval: Does one task help the other?. Gustavo Penha, Ali Vardasbi, Enrico Palumbo, Marco De Nadai, Hugues Bouchard, arXiv:2410.168232024Preprint</p>
<p>Online adaptation of language models with a memory of amortized contexts. Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz, arXiv:2403.043172024arXiv preprint</p>
<p>Enabling scalable oversight via self-evolving critic. Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin, arXiv:2501.057272025Preprint</p>
<p>Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou, arXiv:2406.04692Mixture-of-agents enhances large language model capabilities. 2024aPreprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, 10.1007/s11704-024-40231-1Frontiers of Computer Science. 6182024b</p>
<p>Rethinking the bounds of llm reasoning: Are multi-agent discussions the key?. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song, arXiv:2402.182722024cPreprint</p>
<p>Rethinking the bounds of llm reasoning: Are multi-agent discussions the key?. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song, arXiv:2402.182722024dPreprint</p>
<p>Rethinking the bounds of llm reasoning: Are multi-agent discussions the key?. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song, arXiv:2402.182722024ePreprint</p>
<p>Zora Zhiruo, Wang , Jiayuan Mao, Daniel Fried, Graham Neubig, arXiv:2409.07429Agent workflow memory. 2024fPreprint</p>
<p>A-mem: Agentic memory for llm agents. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang, arXiv:2502.121102025Preprint</p>
<p>Can large multimodal models uncover deep semantics behind images?. Yixin Yang, Zheng Li, Qingxiu Dong, Heming Xia, Zhifang Sui, arXiv:2402.112812024Preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292023Preprint</p>
<p>A survey on the memory mechanism of large language model based agents. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.135012024Preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, arXiv:2308.101442024aPreprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024b</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, arXiv:2305.10250Memorybank: Enhancing large language models with long-term memory. 2023Preprint</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Yu Wang, Zhaoxiang Qiao, Jifeng Zhang, Dai, arXiv:2305.171442023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
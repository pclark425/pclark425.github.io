<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1322 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1322</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1322</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-259138461</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.06904v1.pdf" target="_blank">Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning</a></p>
                <p><strong>Paper Abstract:</strong> With rapid progress in deep learning, neural networks have been widely used in scientific research and engineering applications as surrogate models. Despite the great success of neural networks in fitting complex systems, two major challenges still remain: i) the lack of generalization on different problems/datasets, and ii) the demand for large amounts of simulation data that are computationally expensive. To resolve these challenges, we propose the differentiable \mf (DMF) model, which leverages neural architecture search (NAS) to automatically search the suitable model architecture for different problems, and transfer learning to transfer the learned knowledge from low-fidelity (fast but inaccurate) data to high-fidelity (slow but accurate) model. Novel and latest machine learning techniques such as hyperparameters search and alternate learning are used to improve the efficiency and robustness of DMF. As a result, DMF can efficiently learn the physics simulations with only a few high-fidelity training samples, and outperform the state-of-the-art methods with a significant margin (with up to 58$\%$ improvement in RMSE) based on a variety of synthetic and practical benchmark problems.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1322.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1322.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Burgers-PDE-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Viscous Burgers' equation numerical solver (multi-resolution meshes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finite-element / backward-Euler time integration solver used to generate multi-fidelity spatial–temporal solution fields (coarse to fine meshes) for surrogate-model training and transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Custom finite-element / backward-Euler Burgers' PDE solver</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Numerical solver for the viscous Burgers' equation (∂u/∂t + u ∂u/∂x = ν ∂²u/∂x²) using backward-Euler in time and finite-element discretization in space, producing spatial–temporal solution fields recorded on meshes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>fluid dynamics / nonlinear PDEs</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>multi-fidelity: low-fidelity (16×16 spatial-temporal mesh) vs high-fidelity (32×32 mesh) with final outputs upscaled/interpolated to 100×100 for comparison; the reference 'true' fields recorded on a 64×64 mesh.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Low-fidelity uses coarse spatial-temporal discretization (16×16) and thus lower spatial/temporal resolution and numerical accuracy; high-fidelity doubles node density (32×32) giving finer resolution; outputs interpolated to common grid (100×100) for training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DMF (DMF-L for low-fidelity pretrain) and DMF-trans (transfer variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural-network surrogate with architectures found by differentiable architecture search (DARTS); DMF-L trained on low-fidelity data, DMF-trans fine-tunes / augments low-fidelity model with a linear layer to predict high-fidelity outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulate / predict high-dimensional spatial–temporal solution fields of Burgers' equation given input parameter (viscosity ν) and produce pixel-wise field predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>High-fidelity solution fields (higher-resolution numerical solver outputs) / held-out test scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: DMF consistently shows the best pixel-wise MAE compared to LAR, NAR, GAR, dmfal and other baselines across numbers of high-fidelity samples; errors concentrate early in time and diminish with time. (No single numeric RMSE reported in-text for Burgers aside from figures.)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper compares coarse (16×16) low-fidelity and finer (32×32, 64×64 reference) meshes; DMF leverages low-fidelity via transfer learning to improve high-fidelity predictions and outperforms baselines when sufficient high-fidelity data are available. Error decreases as number of high-fidelity samples increases; DMF shows strongest reconstructions when more high-fidelity points are added.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit theoretical minimal fidelity; experimentally, coarse (16×16) low-fidelity provides useful trend information for transfer. The authors note DMF benefits as high-fidelity data increase and that DMF generally requires some high-fidelity samples to reach top performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noted that other methods (e.g., GAR) can show artifacts; no explicit case where low-fidelity entirely fails, but all methods show larger errors at early times and when high-fidelity training samples are extremely scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1322.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1322.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Poisson-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Poisson (elliptic) PDE finite-difference solver (multi-mesh)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finite-difference numerical solver of Poisson's equation used to generate multi-fidelity potential fields (coarse to fine meshes) for surrogate training and transfer evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Custom finite-difference Poisson solver</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Solver for ∂²u/∂x² + ∂²u/∂y² = 0 on 2D domain with Dirichlet boundary conditions; coarse solutions computed on 8×8 mesh and high-fidelity on 32×32 mesh, outputs upscaled to 100×100 for training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>electrostatics / potential fields / elliptic PDEs</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>multi-fidelity: coarse 8×8 mesh (low-fidelity) vs fine 32×32 mesh (high-fidelity), upsampled to common grid for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Low-fidelity has much lower spatial resolution and smoother/less accurate representation of boundary-induced features; high-fidelity resolves more spatial detail and gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DMF (DMF-trans and DMF-2 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural-network surrogate using differentiable architecture search plus transfer/fine-tuning; DMF-trans augments pretrained low-fidelity model with a linear layer to predict high-fidelity outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Predict spatial potential fields (QoI) for varying boundary/center values; emulate high-fidelity Poisson solutions from input parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>High-fidelity potential fields / held-out test scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: DMF performs worse than LAR and GAR when high-fidelity data are extremely limited (4 or 8 samples) but RMSE of DMF drops steadily as high-fidelity samples increase; DMF surpasses other baselines as more high-fidelity data are added.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Direct comparison shows coarse 8×8 low-fidelity provides transferable information but high-fidelity scarcity affects relative performance: some classical multi-fidelity methods perform better at very low high-fidelity counts, while DMF improves with additional high-fidelity points.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit minimal fidelity specification; experimental finding: DMF requires more high-fidelity points to outperform the best baselines on Poisson, indicating coarse low-fidelity alone may be insufficient in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When high-fidelity training samples are very few (4 or 8), DMF underperforms LAR and GAR — an observed failure case where low-fidelity transfer did not yield best results under extreme scarcity of high-fidelity data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1322.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1322.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MD-LJ-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Dynamics (Lennard-Jones) simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lennard-Jones-based molecular dynamics simulations producing temperature profiles over a grid of temperature and density conditions; used as a high-dimensional real-world test for multi-fidelity surrogate modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Custom Lennard-Jones molecular dynamics simulator</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Particle-based MD simulation using Lennard-Jones pair potential (u(r_ij)=4ε[(σ/r)^12−(σ/r)^6]) to generate thermodynamic observables; simulation box periodic/intermittent boundaries, parameters σ, ε, m specified; grid over temperature and (dimensionless) density used to generate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>molecular dynamics / thermodynamics / materials</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>multi-fidelity via integration timestep/solver accuracy and possibly coarser approximations — lower-fidelity runs use larger timesteps or simplified settings (faster but less accurate), higher-fidelity uses smaller timesteps and more accurate integration; exact mesh-like fidelity levels not enumerated as grid sizes but fidelity controlled by simulation resolution/settings.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Low-fidelity reduces computational effort by using larger integration timesteps (risking numerical instability) and a limit on repulsive interaction magnitudes when σ/r > 1.2 to avoid blow-up; high-fidelity uses smaller timesteps and full LJ interactions for more accurate temperature profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DMF (DMF-L pretrained and DMF-trans fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural-network surrogate found by differentiable architecture search with transfer learning from low-fidelity MD outputs to high-fidelity targets.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Predict equilibrium temperature profiles (QoI) across a T×ρ grid and emulate thermodynamic behaviors across parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Quantitative: with 32 low-fidelity training points DMF predictions produced a maximum error ≈ 0.1 K (on temperature profile) compared to ground truth; for comparison, Gaussian process regression gave max error ≈ 0.5 K and a DNN baseline ≈ 0.3 K (numbers reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>High-fidelity MD outputs / held-out thermodynamic test cases</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Quantitative: DMF's transfer from low-fidelity to high-fidelity leads to much lower max temperature error (≈0.1 K with 32 low-fidelity samples) compared to GPR (≈0.5 K) and DNN (≈0.3 K), indicating effective transfer even when training primarily on cheaper low-fidelity runs.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Increasing the amount of low-fidelity data reduced maximum prediction errors; DMF leverages low-fidelity information effectively to approach high-fidelity ground truth; reported numerical comparisons show DMF substantially better than GPR and DNN baselines under the tested counts.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors note they limited repulsive interactions for numerical stability at high timesteps and that this simplification introduces slight inaccuracies but does not affect conclusions; no strict minimal fidelity threshold given, but experiments indicate that a moderate number (~32) of low-fidelity samples can yield low transfer errors when using DMF.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Numerical instability can occur if integration timestep is too large in the MD simulation; the authors applied a capping heuristic (limit when σ/r > 1.2) — this simplification introduces minor inaccuracies but was applied consistently and did not invalidate transfer results. No explicit failure of low-fidelity transfer reported beyond usual data-scarcity caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1322.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1322.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CDA-plasmonics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coupled Dipole Approximation (CDA) plasmonic array simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Numerical linear-system solver using Green's matrix blocks to compute local electric fields and derive extinction/scattering efficiencies of nanoparticle arrays at different particle counts and configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Coupled Dipole Approximation (CDA) solver (Green's matrix linear system)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Solves linear equations for local fields E_loc(r_j) using 3N×3N Green's matrices Gi_j; computes scattering and extinction efficiencies Q_ext and Q_sc for plasmonic nanoparticle arrays (Vogel spirals) across parameter space (wavelength λ, divergence angle α_vs, scaling a_vs).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>nanophotonics / plasmonics / electromagnetics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>multi-fidelity defined by number of particles: F1..F3 correspond to particle counts {2, 25, 50} (and examples up to 500); fidelity increases with particle count (more interactions, more expensive and more accurate approximation of many-body effects).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Low-fidelity corresponds to small particle counts (fewer pairwise interactions, cheaper but less representative of collective scattering); higher-fidelity includes more particles and thus exponential increase in computational cost and more accurate many-body coupling via full Green's matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DMF (trained on multi-fidelity CDA outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Differentiable-architecture-searched neural-network surrogates with transfer learning from lower-particle-count simulations (cheap) to higher-particle-count outputs (expensive).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Predict QoIs Q_ext and Q_sc (extinction and scattering efficiencies) of plasmonic arrays across parameter space and array configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Higher-fidelity CDA outputs (larger particle counts) / withheld test samples</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: DMF evaluated with 96 low-fidelity samples and varying numbers of high-fidelity samples {4,16,64,96} on 34 withheld tests; plots show DMF predictions approaching ground truth as high-fidelity samples increase. No single scalar performance number reported in-text, but DMF outperforms many baselines across settings.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Fidelity varied by particle count — transfer from small-N simulations to large-N simulations is feasible; DMF reduces required expensive CDA runs by leveraging many cheap, low-particle-count simulations. The paper emphasizes that computational cost grows quickly with particle count, motivating multi-fidelity approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors do not specify a strict minimum particle-count fidelity needed for transfer; experimental design fixes 96 low-fidelity samples and varies high-fidelity counts, showing DMF improves with some high-fidelity examples available.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure case described; computational intractability of very large particle counts is noted as motivation for using low-fidelity approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1322.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1322.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CFD-elbow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational fluid dynamics (CFD) elbow-pipe flow simulator (thermal-fluid example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CFD simulations of a two-inlet elbow pipe producing velocity and pressure profiles used as QoIs for surrogate modeling and multi-fidelity transfer experiments in thermal-fluid system design contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CFD solver for elbow pipe flow (unspecified Navier–Stokes solver / industrial CFD)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulated flow in a pipe with two inlets and an elbow; QoIs are vectorized profiles of velocity magnitude and pressure on circular cross-sections at specified locations and times (e.g., t = 50 s). The solver and discretization details are not deeply specified in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>thermal-fluid engineering / computational fluid dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>multi-fidelity implied (different solver settings / mesh resolutions) though exact mesh/fidelity levels not enumerated explicitly in the text for this case.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Higher-fidelity runs likely use finer meshes and more accurate solver tolerances, while low-fidelity approximations use coarser meshes or simplified boundary/solver settings; the paper uses such data in multi-fidelity fusion experiments but does not detail discretization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DMF</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural-network surrogate with differentiable architecture search and transfer learning used to predict velocity/pressure profiles given inlet velocities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Predict outlet and elbow cross-section velocity and pressure profiles for design / analysis of thermal-fluid systems.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>High-fidelity CFD outputs / design evaluation scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: Scatter plots and R^2 statistics in the paper show predictive correlations improving with number of high-fidelity samples; authors report R^2 values rising (e.g., R^2 > 0.95 in some cases) indicating high predictive fidelity when sufficient high-fidelity data are available.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Not detailed quantitatively for this case beyond qualitative R^2/plots; DMF benefits from transfer learning but exact fidelity-level comparisons for the pipe flow case are not fully specified.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit statement of minimal fidelity required; general paper conclusions about need for some high-fidelity samples for best performance apply.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases reported for the elbow-pipe CFD example in the main text; limitations are discussed generally (DMF needs more data to construct model from scratch).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transfer learning based multi-fidelity physics informed deep neural network <em>(Rating: 2)</em></li>
                <li>A statistical method for tuning a computer code to a data base <em>(Rating: 2)</em></li>
                <li>Extraction of material properties through multi-fidelity deep learning from molecular dynamics simulation <em>(Rating: 2)</em></li>
                <li>Probing scattering resonances of vogel's spirals with the green's matrix spectral method <em>(Rating: 1)</em></li>
                <li>Deep gaussian processes for multi-fidelity modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1322",
    "paper_id": "paper-259138461",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Burgers-PDE-sim",
            "name_full": "Viscous Burgers' equation numerical solver (multi-resolution meshes)",
            "brief_description": "Finite-element / backward-Euler time integration solver used to generate multi-fidelity spatial–temporal solution fields (coarse to fine meshes) for surrogate-model training and transfer experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Custom finite-element / backward-Euler Burgers' PDE solver",
            "simulator_description": "Numerical solver for the viscous Burgers' equation (∂u/∂t + u ∂u/∂x = ν ∂²u/∂x²) using backward-Euler in time and finite-element discretization in space, producing spatial–temporal solution fields recorded on meshes.",
            "scientific_domain": "fluid dynamics / nonlinear PDEs",
            "fidelity_level": "multi-fidelity: low-fidelity (16×16 spatial-temporal mesh) vs high-fidelity (32×32 mesh) with final outputs upscaled/interpolated to 100×100 for comparison; the reference 'true' fields recorded on a 64×64 mesh.",
            "fidelity_characteristics": "Low-fidelity uses coarse spatial-temporal discretization (16×16) and thus lower spatial/temporal resolution and numerical accuracy; high-fidelity doubles node density (32×32) giving finer resolution; outputs interpolated to common grid (100×100) for training/evaluation.",
            "model_or_agent_name": "DMF (DMF-L for low-fidelity pretrain) and DMF-trans (transfer variant)",
            "model_description": "Neural-network surrogate with architectures found by differentiable architecture search (DARTS); DMF-L trained on low-fidelity data, DMF-trans fine-tunes / augments low-fidelity model with a linear layer to predict high-fidelity outputs.",
            "reasoning_task": "Emulate / predict high-dimensional spatial–temporal solution fields of Burgers' equation given input parameter (viscosity ν) and produce pixel-wise field predictions.",
            "training_performance": null,
            "transfer_target": "High-fidelity solution fields (higher-resolution numerical solver outputs) / held-out test scenarios",
            "transfer_performance": "Qualitative: DMF consistently shows the best pixel-wise MAE compared to LAR, NAR, GAR, dmfal and other baselines across numbers of high-fidelity samples; errors concentrate early in time and diminish with time. (No single numeric RMSE reported in-text for Burgers aside from figures.)",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Paper compares coarse (16×16) low-fidelity and finer (32×32, 64×64 reference) meshes; DMF leverages low-fidelity via transfer learning to improve high-fidelity predictions and outperforms baselines when sufficient high-fidelity data are available. Error decreases as number of high-fidelity samples increases; DMF shows strongest reconstructions when more high-fidelity points are added.",
            "minimal_fidelity_discussion": "No explicit theoretical minimal fidelity; experimentally, coarse (16×16) low-fidelity provides useful trend information for transfer. The authors note DMF benefits as high-fidelity data increase and that DMF generally requires some high-fidelity samples to reach top performance.",
            "failure_cases": "Noted that other methods (e.g., GAR) can show artifacts; no explicit case where low-fidelity entirely fails, but all methods show larger errors at early times and when high-fidelity training samples are extremely scarce.",
            "uuid": "e1322.0",
            "source_info": {
                "paper_title": "Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Poisson-sim",
            "name_full": "Poisson (elliptic) PDE finite-difference solver (multi-mesh)",
            "brief_description": "Finite-difference numerical solver of Poisson's equation used to generate multi-fidelity potential fields (coarse to fine meshes) for surrogate training and transfer evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Custom finite-difference Poisson solver",
            "simulator_description": "Solver for ∂²u/∂x² + ∂²u/∂y² = 0 on 2D domain with Dirichlet boundary conditions; coarse solutions computed on 8×8 mesh and high-fidelity on 32×32 mesh, outputs upscaled to 100×100 for training/evaluation.",
            "scientific_domain": "electrostatics / potential fields / elliptic PDEs",
            "fidelity_level": "multi-fidelity: coarse 8×8 mesh (low-fidelity) vs fine 32×32 mesh (high-fidelity), upsampled to common grid for comparison.",
            "fidelity_characteristics": "Low-fidelity has much lower spatial resolution and smoother/less accurate representation of boundary-induced features; high-fidelity resolves more spatial detail and gradients.",
            "model_or_agent_name": "DMF (DMF-trans and DMF-2 variants)",
            "model_description": "Neural-network surrogate using differentiable architecture search plus transfer/fine-tuning; DMF-trans augments pretrained low-fidelity model with a linear layer to predict high-fidelity outputs.",
            "reasoning_task": "Predict spatial potential fields (QoI) for varying boundary/center values; emulate high-fidelity Poisson solutions from input parameters.",
            "training_performance": null,
            "transfer_target": "High-fidelity potential fields / held-out test scenarios",
            "transfer_performance": "Qualitative: DMF performs worse than LAR and GAR when high-fidelity data are extremely limited (4 or 8 samples) but RMSE of DMF drops steadily as high-fidelity samples increase; DMF surpasses other baselines as more high-fidelity data are added.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Direct comparison shows coarse 8×8 low-fidelity provides transferable information but high-fidelity scarcity affects relative performance: some classical multi-fidelity methods perform better at very low high-fidelity counts, while DMF improves with additional high-fidelity points.",
            "minimal_fidelity_discussion": "No explicit minimal fidelity specification; experimental finding: DMF requires more high-fidelity points to outperform the best baselines on Poisson, indicating coarse low-fidelity alone may be insufficient in low-data regimes.",
            "failure_cases": "When high-fidelity training samples are very few (4 or 8), DMF underperforms LAR and GAR — an observed failure case where low-fidelity transfer did not yield best results under extreme scarcity of high-fidelity data.",
            "uuid": "e1322.1",
            "source_info": {
                "paper_title": "Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MD-LJ-sim",
            "name_full": "Molecular Dynamics (Lennard-Jones) simulator",
            "brief_description": "Lennard-Jones-based molecular dynamics simulations producing temperature profiles over a grid of temperature and density conditions; used as a high-dimensional real-world test for multi-fidelity surrogate modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Custom Lennard-Jones molecular dynamics simulator",
            "simulator_description": "Particle-based MD simulation using Lennard-Jones pair potential (u(r_ij)=4ε[(σ/r)^12−(σ/r)^6]) to generate thermodynamic observables; simulation box periodic/intermittent boundaries, parameters σ, ε, m specified; grid over temperature and (dimensionless) density used to generate outputs.",
            "scientific_domain": "molecular dynamics / thermodynamics / materials",
            "fidelity_level": "multi-fidelity via integration timestep/solver accuracy and possibly coarser approximations — lower-fidelity runs use larger timesteps or simplified settings (faster but less accurate), higher-fidelity uses smaller timesteps and more accurate integration; exact mesh-like fidelity levels not enumerated as grid sizes but fidelity controlled by simulation resolution/settings.",
            "fidelity_characteristics": "Low-fidelity reduces computational effort by using larger integration timesteps (risking numerical instability) and a limit on repulsive interaction magnitudes when σ/r &gt; 1.2 to avoid blow-up; high-fidelity uses smaller timesteps and full LJ interactions for more accurate temperature profiles.",
            "model_or_agent_name": "DMF (DMF-L pretrained and DMF-trans fine-tuned)",
            "model_description": "Neural-network surrogate found by differentiable architecture search with transfer learning from low-fidelity MD outputs to high-fidelity targets.",
            "reasoning_task": "Predict equilibrium temperature profiles (QoI) across a T×ρ grid and emulate thermodynamic behaviors across parameter space.",
            "training_performance": "Quantitative: with 32 low-fidelity training points DMF predictions produced a maximum error ≈ 0.1 K (on temperature profile) compared to ground truth; for comparison, Gaussian process regression gave max error ≈ 0.5 K and a DNN baseline ≈ 0.3 K (numbers reported in the paper).",
            "transfer_target": "High-fidelity MD outputs / held-out thermodynamic test cases",
            "transfer_performance": "Quantitative: DMF's transfer from low-fidelity to high-fidelity leads to much lower max temperature error (≈0.1 K with 32 low-fidelity samples) compared to GPR (≈0.5 K) and DNN (≈0.3 K), indicating effective transfer even when training primarily on cheaper low-fidelity runs.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Increasing the amount of low-fidelity data reduced maximum prediction errors; DMF leverages low-fidelity information effectively to approach high-fidelity ground truth; reported numerical comparisons show DMF substantially better than GPR and DNN baselines under the tested counts.",
            "minimal_fidelity_discussion": "Authors note they limited repulsive interactions for numerical stability at high timesteps and that this simplification introduces slight inaccuracies but does not affect conclusions; no strict minimal fidelity threshold given, but experiments indicate that a moderate number (~32) of low-fidelity samples can yield low transfer errors when using DMF.",
            "failure_cases": "Numerical instability can occur if integration timestep is too large in the MD simulation; the authors applied a capping heuristic (limit when σ/r &gt; 1.2) — this simplification introduces minor inaccuracies but was applied consistently and did not invalidate transfer results. No explicit failure of low-fidelity transfer reported beyond usual data-scarcity caveats.",
            "uuid": "e1322.2",
            "source_info": {
                "paper_title": "Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CDA-plasmonics",
            "name_full": "Coupled Dipole Approximation (CDA) plasmonic array simulator",
            "brief_description": "Numerical linear-system solver using Green's matrix blocks to compute local electric fields and derive extinction/scattering efficiencies of nanoparticle arrays at different particle counts and configurations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Coupled Dipole Approximation (CDA) solver (Green's matrix linear system)",
            "simulator_description": "Solves linear equations for local fields E_loc(r_j) using 3N×3N Green's matrices Gi_j; computes scattering and extinction efficiencies Q_ext and Q_sc for plasmonic nanoparticle arrays (Vogel spirals) across parameter space (wavelength λ, divergence angle α_vs, scaling a_vs).",
            "scientific_domain": "nanophotonics / plasmonics / electromagnetics",
            "fidelity_level": "multi-fidelity defined by number of particles: F1..F3 correspond to particle counts {2, 25, 50} (and examples up to 500); fidelity increases with particle count (more interactions, more expensive and more accurate approximation of many-body effects).",
            "fidelity_characteristics": "Low-fidelity corresponds to small particle counts (fewer pairwise interactions, cheaper but less representative of collective scattering); higher-fidelity includes more particles and thus exponential increase in computational cost and more accurate many-body coupling via full Green's matrix.",
            "model_or_agent_name": "DMF (trained on multi-fidelity CDA outputs)",
            "model_description": "Differentiable-architecture-searched neural-network surrogates with transfer learning from lower-particle-count simulations (cheap) to higher-particle-count outputs (expensive).",
            "reasoning_task": "Predict QoIs Q_ext and Q_sc (extinction and scattering efficiencies) of plasmonic arrays across parameter space and array configurations.",
            "training_performance": null,
            "transfer_target": "Higher-fidelity CDA outputs (larger particle counts) / withheld test samples",
            "transfer_performance": "Qualitative: DMF evaluated with 96 low-fidelity samples and varying numbers of high-fidelity samples {4,16,64,96} on 34 withheld tests; plots show DMF predictions approaching ground truth as high-fidelity samples increase. No single scalar performance number reported in-text, but DMF outperforms many baselines across settings.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Fidelity varied by particle count — transfer from small-N simulations to large-N simulations is feasible; DMF reduces required expensive CDA runs by leveraging many cheap, low-particle-count simulations. The paper emphasizes that computational cost grows quickly with particle count, motivating multi-fidelity approaches.",
            "minimal_fidelity_discussion": "Authors do not specify a strict minimum particle-count fidelity needed for transfer; experimental design fixes 96 low-fidelity samples and varies high-fidelity counts, showing DMF improves with some high-fidelity examples available.",
            "failure_cases": "No explicit failure case described; computational intractability of very large particle counts is noted as motivation for using low-fidelity approximations.",
            "uuid": "e1322.3",
            "source_info": {
                "paper_title": "Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CFD-elbow",
            "name_full": "Computational fluid dynamics (CFD) elbow-pipe flow simulator (thermal-fluid example)",
            "brief_description": "CFD simulations of a two-inlet elbow pipe producing velocity and pressure profiles used as QoIs for surrogate modeling and multi-fidelity transfer experiments in thermal-fluid system design contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "CFD solver for elbow pipe flow (unspecified Navier–Stokes solver / industrial CFD)",
            "simulator_description": "Simulated flow in a pipe with two inlets and an elbow; QoIs are vectorized profiles of velocity magnitude and pressure on circular cross-sections at specified locations and times (e.g., t = 50 s). The solver and discretization details are not deeply specified in the main text.",
            "scientific_domain": "thermal-fluid engineering / computational fluid dynamics",
            "fidelity_level": "multi-fidelity implied (different solver settings / mesh resolutions) though exact mesh/fidelity levels not enumerated explicitly in the text for this case.",
            "fidelity_characteristics": "Higher-fidelity runs likely use finer meshes and more accurate solver tolerances, while low-fidelity approximations use coarser meshes or simplified boundary/solver settings; the paper uses such data in multi-fidelity fusion experiments but does not detail discretization.",
            "model_or_agent_name": "DMF",
            "model_description": "Neural-network surrogate with differentiable architecture search and transfer learning used to predict velocity/pressure profiles given inlet velocities.",
            "reasoning_task": "Predict outlet and elbow cross-section velocity and pressure profiles for design / analysis of thermal-fluid systems.",
            "training_performance": null,
            "transfer_target": "High-fidelity CFD outputs / design evaluation scenarios",
            "transfer_performance": "Qualitative: Scatter plots and R^2 statistics in the paper show predictive correlations improving with number of high-fidelity samples; authors report R^2 values rising (e.g., R^2 &gt; 0.95 in some cases) indicating high predictive fidelity when sufficient high-fidelity data are available.",
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "Not detailed quantitatively for this case beyond qualitative R^2/plots; DMF benefits from transfer learning but exact fidelity-level comparisons for the pipe flow case are not fully specified.",
            "minimal_fidelity_discussion": "No explicit statement of minimal fidelity required; general paper conclusions about need for some high-fidelity samples for best performance apply.",
            "failure_cases": "No specific failure cases reported for the elbow-pipe CFD example in the main text; limitations are discussed generally (DMF needs more data to construct model from scratch).",
            "uuid": "e1322.4",
            "source_info": {
                "paper_title": "Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transfer learning based multi-fidelity physics informed deep neural network",
            "rating": 2,
            "sanitized_title": "transfer_learning_based_multifidelity_physics_informed_deep_neural_network"
        },
        {
            "paper_title": "A statistical method for tuning a computer code to a data base",
            "rating": 2,
            "sanitized_title": "a_statistical_method_for_tuning_a_computer_code_to_a_data_base"
        },
        {
            "paper_title": "Extraction of material properties through multi-fidelity deep learning from molecular dynamics simulation",
            "rating": 2,
            "sanitized_title": "extraction_of_material_properties_through_multifidelity_deep_learning_from_molecular_dynamics_simulation"
        },
        {
            "paper_title": "Probing scattering resonances of vogel's spirals with the green's matrix spectral method",
            "rating": 1,
            "sanitized_title": "probing_scattering_resonances_of_vogels_spirals_with_the_greens_matrix_spectral_method"
        },
        {
            "paper_title": "Deep gaussian processes for multi-fidelity modeling",
            "rating": 1,
            "sanitized_title": "deep_gaussian_processes_for_multifidelity_modeling"
        }
    ],
    "cost": 0.014926,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning
12 Jun 2023</p>
<p>Yuwen Deng 
School of Integrated Circuit Science and Engineering
Beihang University
37 Xueyuan Road</p>
<p>Haidian District
100191BeijingChina</p>
<p>Wang Kang 
School of Integrated Circuit Science and Engineering
Beihang University
37 Xueyuan Road</p>
<p>Haidian District
100191BeijingChina</p>
<p>Wei W Xing wxing@buaa.edu.cn. 
School of Integrated Circuit Science and Engineering
Beihang University
37 Xueyuan Road</p>
<p>Haidian District
100191BeijingChina</p>
<p>Differentiable Multi-Fidelity Fusion: Efficient Learning of Physics Simulations with Neural Architecture Search and Transfer Learning
12 Jun 2023BE9EAA1ABD915177A54E83B39CD53A78arXiv:2306.06904v1[cs.LG]
With rapid progress in deep learning, neural networks have been widely used in scientific research and engineering applications as surrogate models.Despite the great success of neural networks in fitting complex systems, two major challenges still remain: i) the lack of generalization on different problems/datasets, and ii) the demand for large amounts of simulation data that are computationally expensive.To resolve these challenges, we propose the differentiable multi-fidelity (DMF) model, which leverages neural architecture search (NAS) to automatically search the suitable model architecture for different problems, and transfer learning to transfer the learned knowledge from low-fidelity (fast but inaccurate) data to high-fidelity (slow but accurate) model.Novel and latest machine learning techniques such as hyperparameters search and alternate learning are used to improve the efficiency and robustness of DMF.As a result, DMF can efficiently learn the physics simulations with only a few high-fidelity training samples, and outperform the state-of-the-art methods with a significant margin (with up to 58% improvement in RMSE) based on a variety of synthetic and practical benchmark problems.</p>
<p>Introduction</p>
<p>Many physics systems in science and engineering require resource-consuming physics simulation.Generally, these simulations require to be repeated large amounts of times to deliver a satisfactory analysis, e.g., uncertainty quantification, optimization, sensitivity analysis, and inverse parameter estimation.Therefore, for these systems, a data-driven surrogate model obtaining predictions is usually adopted to provide a quick estimation and design guidelines for the simulations.The surrogate model is usually a regression model approximating the relation between the system inputs and outputs, which may come from either the experiment results or a numerical model based on the simulations, also known as training data in this instance.</p>
<p>In general, there are various mathematical models to describe a given physical system, these models provide different fidelity data according to different levels of physical detail incorporated into the equations, e.g., geometry and initial-boundary conditions.The traditional surrogate model is usually built based on high-fidelity simulation results, which are accurate but expensive to acquire.In practice, we may efficiently reduce the computational cost by using low-fidelity simulations.Roughly speaking, low-fidelity computational models are those of low resolution or complexity in terms of the physical laws or adjustable settings of the computer-based approximation.In many cases, the less accurate low-fidelity data are cheap and abundant while they can supply useful information on the trends for high-fidelity data, hence we can trade the accuracy for the computational cost by using low-fidelity models and make predictions of scarce high-fidelity data based on low-fidelity data.</p>
<p>Multi-fidelity fusion has shown to be an efficient and promising approach to achieving high accuracy in many systems by utilizing both low-and high-fidelity data.Auto-regressive based Gaussian process (GP) is an important framework using Bayesian techniques for multi-fidelity modeling, which can yield predictions of the mean values and uncertainties.The seminal auto-regressive (AR) fusion model of Kennedy and O'Hagan [17] assumes the linear relationship between different levels of fidelity and introduces a linear transformation for the univariate outputs.To overcome the limitation of the linearity in AR, Perdikaris et al. [32] propose non-linear AR (NAR).They ignore the output distributions and directly use the low-fidelity solution as an input for the high-fidelity GP model, which is essentially a concatenating GP structure, also known as Deep GP [9].Other works [8,34,16,46,40] develop more GP-based multi-fidelity models to improve the performance.Unfortunately, the main drawback of the GP-based method is the significant computational complexity for optimizing the multi-fidelity model, which demands large-matrix inversion that scales at O(N 3 ) (N is the number of training samples).As a result, the GP-based method is limited in multiple output scenarios.Also, the GP-based method cannot easily extend to high-dimensional simulations, making it difficult to apply to many real-world problems.Therefore, the multi-fidelity approach that can overcome these limitations is urgently needed.</p>
<p>With the rapid development of deep learning, neural networks (NN) have successfully been applied in many areas, including computational science and engineering.With the incorporation of the neural layers and nonlinear activation function, NN has demonstrated that the composite function model possesses a universal approximation property [13,29] and can be used to approximate any function.In the meanwhile, there have been numerous open-source frameworks for building and training deep neural networks in recent years, such as Tensorflow [1],</p>
<p>Theano [2], Pytorch [31].These frameworks supply with clients compact and organized methodology to effectively speed up work processes that build expressive surrogate models.In addition, these software packages have a large user base outside of applications in science and engineering.Using specialized hardware architectures that speed up model training is also an advantage of using neural networks to create surrogate models, such as GPUs and TPUs.</p>
<p>More importantly, with the significant investments in constructing specifically tailored high-performance computing resources, neural networks driven by artificial intelligence (AI) applications are becoming more popular.As a result, hardware accelerators' capabilities will expand rapidly.</p>
<p>NN has a long history of being used in surrogate models almost as long as the history of GP surrogates.</p>
<p>We briefly discuss some of the latest advances first.Kim et al. introduce a knowledge-based neural network (KBNN) [18] that utilizes a prior-knowledge response surface model (RSM) as the low-fidelity model.Meng, Raissi, Pun et al. introduce physics informed neural network (PINN) [36,35] to build the surrogate model, which combines a physics regularization layer with the normal NN, which is based on the corresponding PDEs.Then, they revise the training loss of NN and penalized the solutions that do not satisfy the equations.However, KBNN and PINN both need prior knowledge of general physical laws, for complex physical simulation, it's infeasible to describe in analytical equations.Moreover, they also need to design the network manually according to each specific PDE, which also leads to inconveniences in modeling.To make the uncertainty qualification, Meng replaces the normal NN with Bayesian neural network (BNN) in [25] to yield predictions of the mean values as well as estimates of uncertainty; Li et al. [22] propose a BNN approach to multi-fidelity fusion and use active learning technique to select the training samples.However, BNN is extremely computationally expensive and performs poorly on high dimensional data; moreover, it is hard to interpret and requires Copula functions to separate out effects between different parts of the network.These methods employing neural networks primarily concentrate on designing networks to improve clarity involving the application of physical constraints.However, the main drawback of using the neural network surrogate model is the limitation of the number of training samples or high-fidelity data.</p>
<p>The incorporation of data from a variety of lower fidelity models or experiments into the neural network training procedure may offer a potential solution to the problem of a lack of data volume.</p>
<p>To harness the advantage of multi-fidelity data, many multi-fidelity NN methods have been proposed recently.</p>
<p>Many of them root in the idea of transfer learning [30], which aims to use the knowledge learned from one task (in this case, the low-fidelity mapping) to solve another similar task (in this case, the high-fidelity mapping) where the labeled data are limited or missing.In the areas of computer vision and natural language processing, transfer learning has been widely used to improve the performance of learning by avoiding many expensive computations [15,47].It has been demonstrated that transfer learning has the following advantages over traditional learning [30]: (i) faster convergence of the target network, (ii) smaller initial training error, and (iii) smaller validation error on smaller datasets.</p>
<p>In the multi-fidelity problem, we are only interested the high-fidelity data.However, the high-fidelity simulation tends to require many computational resources and provides higher levels of accuracy.Thus, the size of high-fidelity data is extremely limited.At the same time, the low-fidelity data and high-fidelity data are also highly correlated.</p>
<p>In this case, knowledge transfer, if done successfully, will significantly improve the performance of learning by avoiding many expensive data-simulating efforts.Souvik Chakraborty et al. [3] train a physics-informed deep neural network (DNN) as the low-fidelity model first and then transfer the parameters to a high-fidelity model, however, in the latter phase, they only transfer the parameters of DNN consisting of a fixed number of layers and nodes, which leads to lack of flexibility.Guo et al. [11] propose a multilevel NN model, which is done sequentially by firstly modeling low-fidelity model and then modeling the correlation between two fidelity levels with a single hidden layer, but they fix the low-fidelity model parameters while training on high-fidelity samples instead of fine-tuning, which may lead to an error while there is no strong linear correlation between two levels.Song et al. [39] acquire transfer learning with a convolutional neural network-based encoder and decoder architecture [27], however, the convolutional layer tends to extract the features of 2D data and is suboptimal on other physical system datasets.</p>
<p>Despite the above efforts, the transfer learning-based NN methods still have some limitations.First, the transfer learning-based NN methods implicitly assume that the low-fidelity model and high-fidelity model share the same architecture, which imposes restrictions on the high-fidelity model and may lead to suboptimal performance.Second, the transfer learning-based NN methods are not flexible enough to adapt to different multi-fidelity structures and different high-fidelity models.Most importantly, the NN structure requires a careful design for each specific problem and relies on domain knowledge, which is not always available.To address the above issues and further improve the performance of NN-based multi-fidelity fusion, we first propose a novel method, differentiable neural network, based on binary optimization that can learn the optimal NN architecture automatically for any given problem.To enable multi-fidelity fusion, we then use fine-tuning methods to transfer the learned parameters on low-fidelity data to the high-fidelity model, which can significantly reduce the computational cost and improve the performance of NN-based multi-fidelity fusion by putting the low-fidelity data and high-fidelity data on the same footing.To further improve the efficiency and robustness of the proposed method, we also propose a novel hyperparameters search method to select the best hyperparameters settings efficiently and an alternate learning method to replace the joint optimization without the introduction of extra error.The proposed method is assessed on various datasets including synthetic data and real applications.It is shown that the proposed method can significantly improve the performance of NN-based multi-fidelity fusion and is more robust than the SOTA multi-fidelity fusion methods.</p>
<p>The main contributions of this paper are summarized as follows:</p>
<ol>
<li>
<p>To be able to build surrogates for a wide spectrum of physics simulations without the need for domain knowledge and expensive computational cost, we propose a novel differentiable neural network (DNN) based on binary optimization that can learn the optimal NN architecture automatically form cheap-to-gain low-fidelity data and transfer such knowledge to deliver a high-fidelity model.</p>
</li>
<li>
<p>we introduce an alternate optimization scheme to replace the traditional joint optimization in our automatic machine learning pipeline to improve the efficiency and robustness of the proposed method without introducing extra error.</p>
</li>
<li>
<p>To reduce the influence of the selection of hyperparameters, we introduce search space pruning methods to select the best hyperparameters settings efficiently.</p>
</li>
<li>
<p>The proposed method achieves remarkable performance improvement on various datasets, including standard benchmarks, high-dimensional benchmarks, and real applications, and outperform most state-of-the-art multi-fidelity fusion methods consistently.</p>
</li>
</ol>
<p>Statement of The Problem</p>
<p>Without loss of generality, we denote the solution of PDE by scalar variable u(z, t, x), where z and t are the spatial and temporal coordinates, respectively, and x the parameters corresponding to the system of equations.</p>
<p>Consider a general nonlinear system with steady-state PDEs,
                     ∂ ∂t u(z, t, x) + F (u(z, t, x)) = S(z, t, x), Ω × (0, T ] × X B(u, x) = 0, Ω × (0, T ] × X u(z, 0, x) = u 0 (z, x), Ω × {t = 0} × X (1)
where S is the source function, F is a nonlinear function that may contain parameters indicated by x ∈ R l , B is the boundary condition, and u 0 (z, x) denotes the initial condition; Ω and T are the spatial and temporal domains of interest.As a result, z ∈ Ω ⊂ R p and t ∈ [0, T ] parameterizes the temporal domain, the system is solved for
u(z, t, x) ∈ Ω × (0, T ] × Xy(x) = u(z 1 , t 1 , x), • • • , u(z N z , t 1 , x), u(z 1 , t 2 , x), • • • , u(z N z , t N t , x) ∈ R d ,(2)
where z i , t i are the spatial and temporal coordinates of a regular/irregular grid, and d = N z × N t is the total number of spatial-temporal grid points.Because the number of coordinates N z and N t needs to be sufficiently large to provide an accurate/high-fidelity solution, the dimension of vector y(x) also becomes extremely large.Notice that the transformation from learning u(z, t, x) to learning y(x) is equivalent.</p>
<p>To acquire a high-fidelity solution for y(x), we generally need to use a high-order basis expansion, high-order stencil, or tight iteration bounds, which are costly to process.On the contrary, a lower-order basis expansion, a lower-order stencil, or a looser iteration bound.Moreover, we can use a simpler physical model, e.g., linearizing, spatial averaging, considering a 2-d slice, or others.The computational model with options for adjusting settings to generate various fidelity solutions and variable input parameters is required.</p>
<p>Without loss of generality, We denote the multi-fidelity training set by
D m = {(x i m , y i m )} N m , m = 1, 2, • • • , M. x ∈ X represents an l-dimensional input, y m ∈ R d m
represents the corresponding response, where d m denotes the dimension of y m , N m represents the number of samples with fidelity m, and M is the total number of fidelity.In general, {d m } are not necessarily identical, and we assume that
d 1 ⩽ d 2 ⩽ • • • ⩽ d M , and N 1 ⩾ N 2 ⩾ • • • ⩾ N M
to match the practical situation that high-fidelity simulations show more details but are expensive to obtain.Also notice that in many multi-fidelity fusion methods, the high-fidelity inputs are required to form a subset of the low-fidelity input, i.e.,
{x i m } N m i=1 ⊂ {x i m ′ } N m ′
i=1 for all m &gt; m ′ .This requirement significantly hinders the application of multi-fidelity fusion methods in many real-world problems.In this work, we do not impose such a restriction, and thus the proposed method is more general for broader applications.Our goal is to estimate mapping f M (x) : x → y M as accurately as possible using dataset {D m } M m=1 .</p>
<p>Differentiable Multi-fidelity Fusion</p>
<p>Adaptive Surrogate Model Using Differentiable Architecture Search</p>
<p>Recent studies in surrogate model [26,25,11,14] make much effort to design a NN model to improve its performance.They introduce the composition of linear NN and non-linear NN to capture the complex relationships between inputs and outputs.However, the design of the NN and the transformation normally rely on field knowledge, which is not always available in practice and may not be general enough to be applied to different problems.</p>
<p>In our model, to better utilize the information of low-fidelity data, we introduce a differentiable architecture search (DARTS) [24] to automatically search for the best NN structure as the foundation model.More specifically, The linear operation has one layer with 20 neurons and does not have any activation function; the zero operation means no layer and only outputs zero regardless of the inputs.Each operation is associated with a weight coefficient α l .The composition of multiple operations is defined as:
o = |O| l=1 exp (α l ) |O| l=1 exp (α l ) o l ,(3)
where |O| is the number of operation in O, e.g., five different NN operations in our model; o l is from the operation candidate set O. Thus the operation o is a mixing operation parameterized by a vector α of dimension |O|.Different from the existing models, α will be decided automatically in the training process to automatically form the best NN structure.</p>
<p>We can now directly learn the mapping from x to y L by optimizing the network parameters w and the operation weights α jointly to get the optimal NN structure.However, with only five different operations, the resulting model is simply a sum of several networks, which is not powerful enough to capture the complex relationship between x and y L unless we dramatically increase the number of operations, which requires careful hand-crafting and is not scalable.To resolve this issue, we expand the search space by introducing the intermediate nodes between x and y L .</p>
<p>More specifically, the final model is represented as a Directed Acyclic Graph (DAG) denoted by G.It consists of three types of nodes: input node, output node, and intermediate node.We show some possible DAGs in Fig. 1 with different numbers of intermediate nodes for illustration.</p>
<p>For the sake of clarity, we consider only one intermediate node and denote all nodes as {h 0 , h 1 , h 2 }, respectively.</p>
<p>Here, h 0 = x, h 2 = y L are the input and low-fidelity nodes.o (i, j) represents the composite operation from i to j, and the zero operation represents no connection between i and j.Therefore, each node j can be written as:
h j = i&lt; j o (i, j) (h i ). (4)
With the model being fully defined with G and the composite operation, we can now train the model to learn the mapping from x to y L .Our objective is to optimize a binary optimization problem with the parameters w in layers and the weights α of each operation given G and the low-fidelity data.Denote the cost function by the L m (w, α) for the fidelity m , which is written as:
L m (w, α) = 1 N m N m i=1 ∥y i m − f (x i , w), α)∥ 2 2 + λ 1 ∥w∥ 2 2 + λ 2 ∥α∥ 2 2 ,(5)
where the first term is the mean square error (MSE) for the observation data, and the rest are regularization terms with λ i (i = 1, 2) ⩾ 0 being the penalty coefficients or weight decays, Optimizing Equation ( 5) directly may be of low efficiency because their high correlation during the optimization can cause a high variance of the gradient.Thus, we propose an alternate optimization:
w t+1 = arg min w L m (w|α t ) α t+1 = arg min α L m (α|w t+1 ),(6)
where the superscript indicates the iteration.There are two immediate advantages to such an optimization approach: i) simplification of the code implementation and reduction of the memory cost during the computation of the backpropagation of the gradient and ii) potential improvement of the convergence by reducing the variance of the computed gradient during the optimization by limiting the number of variables involved in one optimization step.</p>
<p>In practice, updating of α will introduce a significant perturbation to the gradient of w.Thus, instead of updating w and α jointly totally, w was updated more frequently than α.
for i from 1 to W do w (i+1) t = w (t) t − r 1 ∇ w L m (w, α) if i mod k = 0 then α (i+1) t = α (i) t − r 2 ∇ α L m (w, α) end end end
optimization will not introduce extra error to the convergence.Proposition 1.If the product r w × r α is small enough, r represents the corresponding learning rate, optimizing α and w alternately is equivalent to optimizing then simultaneously, i.e., the optimization will lead to the same asymptotic convergence and convergence rate.</p>
<p>The proof of Proposition 1 is preserved in Appendix 6 for clarity.</p>
<p>Transfer Learning For Multi-Fidelity Fusion</p>
<p>We first use the method discussed in the previous section to train the low-fidelity model f L (x).To explore the cross-correlation between y L and y H , we consider two different models in this section.</p>
<p>The first one uses the low-fidelity model to predict the corresponding low-fidelity response y L , then the mapping between y L and y H is again learned using the automatic learning method discussed in the previous section for the low-fidelity model.We denote this model by the DMF-2 network.The second approach builds a high-fidelity model based on the low-fidelity model with transfer learning.The underlying assumption is that the low-fidelity data and the high-fidelity data are highly correlated, and the low-fidelity model already contains information to make accurate predictions for the high-fidelity data given any input x.To allow some more flexibility in the high-fidelity model, we add a single linear layer NN l to the low-fidelity model, which is trained to predict y H based on the input
data (X H , f L (X H )) = {(x i H , f L (x i H ))} N H
, and the available high-fidelity output data y H = {y i H } N H .We call this model DMF-trans and the overview of our model is shown in Fig. 2.</p>
<p>Hyperparameter Search Space Pruning</p>
<p>Despite the efforts we put into an automatic machine learning surrogate model for multi-fidelity fusion, there are still some hyperparameters that need to be tuned manually.These hyperparameters, such as learning rate λ 1 and weight decay λ 2 described in Equation 5, can influence the performance of the proposed method significantly.</p>
<p>To see this, we conduct a hyperparameter grid search experiment on the Plasmonic simulation problem (see the experimental section for details).We consider three key hyperparameters r 1 (the learning rate of w L ), r 2 (the learning rate of α L ), and r 3 (the learning rate of w NN l ).Intuitively, r 1 and r 2 decide how much the knowledge is transferred to the high-fidelity model from the low-fidelity model.The results are shown in Fig. 4 where the RMSE with different hyperparameter settings are plotted.From the figure, we can see clearly that the performance of the model is highly influenced by the hyperparameter settings.</p>
<p>Instead, to deliver a consistent and labour-free performance, we equip the proposed method with a hyperparameter search space pruning technique to automatically prune the hyperparameter search space to keep only a small set of hyperparameter settings that are likely to perform well.</p>
<p>The most simple and intuitive methods for selecting hyperparameters include grid search and random search.</p>
<p>However, the former is costly for our problem, whereas the latter can fall into suboptimal solutions easily without enough computational resources.Instead of allocating the computational budget to every hyperparameter setting evenly like in the grid search, pruning techniques try to retain the potentially good settings and discard the ones that produce unsatisfied performance.The pioneering work median pruning [12] runs models with a different setting in parallel and computes their statistical result at each iteration; only settings with loss values higher than the median of the total settings are kept for the next iterations.The median pruning is prone to discard potentially good candidates at the early stage due to the lack of control over the process.To resolve this issue, the successive halving algorithm (SHA) [19] provides more flexibility by (i) allocating budgets to all configurations in a given rung (usually a small value) at the beginning stage and (ii) keeping only 1/η of the current configurations for the next iteration.The process is repeated η times, where η is the rate of elimination.When the number of hyperparameter configurations is large, the initial budget SHA allocated to each configuration is extremely limited, which leads to inaccurate estimation of performance and bad overall performance.As an efficient remedy, Hyperband [21], extends SHA by repeating it many times with different ηs and budgets, which is called a bracket.It begins with the most aggressive bracket (i.e., large η), and it then decreases η gradually in the following iterations, which enables the hyperband to perform well in situations where more conservative allocations are required while taking advantage of adaptive allocations.</p>
<p>In DMF, the number of configurations is large whereas each trial requires adequate training iterations.Thus, we use hyperband as our prime method over others.We compare their performances on the Plasmonic simulation with the results shown in Fig. 5.As can be seen, the pruning technique can reach the optimal point more rapidly with approximately 60% total time of the grid search.Hyperband outperforms the other pruning techniques in terms of performance and produces a configuration that is very close to the optimal (founded by grid search with a large budget), which is also consistent with the literature on pruning.</p>
<p>Experiment</p>
<p>We assess the performance of the proposed DMF on a series of benchmark problems and real-world applications.</p>
<p>For the simple univariate problems, we compare with the state-of-the-art methods, including GP-HF [23], LAR [17],</p>
<p>and NAR [32].For the high-dimensional canonical emulation benchmarks and real-world applications, we use the enhanced LAR and NAR as suggested in generalized AR (GAR) [43] for high-dimension problems.GAR, deep coregionalization (DC cigp [44]), and the SOTA deep learning-based method, dmfal [23] are also included as the baseline.All experiments are repeated five times with random seeds from 0 to 4 to deliver a robust and fair comparison.The reported performance, e.g., RMSE, is the average of the five runs if not stated otherwise.The experiments are run on Intel CPU i7-8550U and 16GB RAM.</p>
<p>Univariate-output Multi-fidelity Benchmark Evaluation</p>
<p>Firstly, we make a comparison with the SOTA methods on the three univariate-output benchmarks, including Borehole, Currin, and Park functions.The Borehole function models water flow through a borehole and is a common function for testing a wide range of methods in computer experiments due to its simplicity and speed of evaluation.</p>
<p>The Currin function is a two-dimensional function that occurs multiple times in the computer experiment literature.</p>
<p>The Park function is a four-dimensional test function.For the purpose of tuning, Cox et al. [7] add a small noise term to the response to simulate the collection of experimental data.The detailed equations of the three functions with explanation and parameter range are explained as follows:</p>
<p>Borehole:
f H (x) = 2πT u (H u − H l ) ln (r/r w ) 1 + 2LT u ln(r/r w )r 2 w K w + T u T l ,(7)f L (x) = 5T u (H u − H l ) ln (r/r w ) 1.5 + 2LT u ln(r/r w )r 2 w K w + T u T l(8)
with the inputs variables listed in Table 1:  Currin:
r w ∈ [0.f H (x) = 1 − exp − 1 2x 2 2300x 3 1 + 1900x 2 1 + 2092x 1 + 60 100x 3 1 + 500x 2 1 + 4x 1 + 20 ,(9)f L (x) = 1 4 f (x 1 + 0.05, x 2 + 0.05) + f (x 1 + 0.05, max (0, x 2 − 0.05))(10)+ 1 4 f (x 1 − 0.05, x 2 + 0.05) + f (x 1 − 0.05, max (0, x 2 − 0.05)) , x i ∈ [0, 1], i = 1, 2. (11)
and Park:
f H (x) = x 1 2         1 + x 2 + x 2 3 x 4 x 2 1 − 1         + (x 1 + 3x 4 ) exp [1 + sin (x 3 )] ,(12)f L (x) = 1 + sin (x 1 ) 10 f (x) − 2x 1 + x 2 2 + x 2 3 + 0.5, x i ∈ [0, 1), i = 1, 2, 3, 4.(13)
We randomly generate 20 low-fidelity samples and increase the number of high-fidelity samples for each dataset.the number of low-fidelity data, which is easy to acquire, will notably improve DMF's performance.</p>
<p>High-Dimensional and Spatial-Temporal Canonical Physics Simulation Benchmarks</p>
<p>To assess the effectiveness of DMF for the complex but practical high-dimensional physics simulations, we conduct experiments on approximating two canonical PDE simulations, which produce high-dimensional Burgers' equation is regarded as a standard nonlinear hyperbolic PDE; it is widely used to represent a variety of physical phenomena, including fluid dynamics [6], nonlinear acoustics [41], and traffic flows [28].The viscous version of this equation is given by ∂u ∂t
+ u ∂u ∂x = v ∂ 2 u ∂x 2 , (14)
where u stands for the volume, x represents a spatial location, t indicates the time, and v denotes the viscosity.With homogeneous Dirichlet boundary conditions, we set x ∈ [0, 1], t ∈ [0, 3], and u(x, 0) = sin(xπ/2).We uniformly sampled viscosities v ∈ [0.001, 0.1] as the surrogate input parameter to generate the solution fields.</p>
<p>The PDE is solved using the backward Euler and finite element methods in the space and time domains, respectively.For the first (low-fidelity) solution, the spatial-temporal domain is discretized into 16 × 16 regular rectangular mesh.Higher-fidelity solvers double the number of nodes in each dimension of the mesh, e.g., 32 × 32 for the high fidelity.In our experiment, the result fields (i.e., outputs) are recorded using a 64 × 64 regular spatialtemporal mesh.Due to the variation of dimensions of different fidelity data, we utilize interpolation to upscale the low-fidelity and high-fidelity fields to fields with 100 × 100 regular nodes.At the beginning of the experiment, we flatten the outputs as vectors and perform the experiment with a bundle of hyperparameters, including learning rates mentioned in Section 3.4.We perform the pruning algorithm to choose the best hyperparameters with the lowest To show the detailed results.We use Fig. 9 to show the pixel-wise mean (overall testing points) absolute error (MAE) field of f H (x H ) and y H on Burger's equation.We can see that for all methods, the main error concentrates at the early stage of the system and reduces as the simulation time passes.LAR and NAR show a consistently inferior accuracy compared to dmfal and DMF; GAR shows strong artifacts, which is consistent with the finding from the original work.Overall, DMF shows the best performance compared to the competitors, indicating its superiority over other methods in reconstructing the predictive fields consistently with different numbers of training points.Poisson's equation is a typical elliptic PDE in mechanical engineering and physics for modelling potential fields, such as gravitational and electrostatic fields [4].It can be written as
∂ 2 u ∂x 2 + ∂ 2 u ∂y 2 = 0. (15)
It is a generalization of Laplace's equation [33].Despite its simplicity, Poisson's equation is commonly encountered in physics and is regularly used as a fundamental test case for surrogate models [42].In our experiment, we impose Dirichlet boundary conditions on a 2D spatial domain with x ∈ [0, 1] × [0, 1].The input parameters consist of the constant values of the four borders and the centre of the rectangular domain, which vary from 0.1 to 0.9 each.To generate the matching potential fields as outputs, we sampled the input parameters equally.Using regular rectangular meshes and a first-order centre difference scheme, the PDE is solved using the finite difference method.We utilized an 8 × 8 mesh for the coarsest-level solution and 32 × 32 for the finest mesh used by the improved solver.The outputs are also upscaled using interpolation to 100 × 100 nodes, and the experiment settings keep the same with Burger.The results are shown in Fig. 8.As can be seen, DMF performs worse than LAR and GAR when the high-fidelity data are limited, i.e., 4 or 8 samples.However, the RMSE of DMF keeps dropping with the increasing of high-fidelity samples while the performances of the latter two methods do not change considerably, which is consistent with the conclusion drawn from the Burger's equation.As for other baselines, DC cigp, NAR, and dmfal are surpassed by DMF for all experiment settings.The detailed pixel-wise MAE comparisons are shown in Fig. 10.</p>
<p>As the figures show, the error of DMF concentrates in the centre area first, and then reduces gradually both in area and magnitude; NAR and dmfal perform worse than DMF obviously; LAR and dmfal have a smaller variance when high-fidelity data are rare but their RMSEs fail to decrease with the increment of high-fidelity data in the corner area.In these two experiment, we can see that DMF has a significant advantage over the SOTA methods when high-fidelity data are sufficient, which gives it a great potential in practical applications where accuracy is the most important.</p>
<p>Real-World Applications</p>
<p>Finally, we assess our model on multi-fidelity data generated from three real-world applications, namely, molecular dynamics (MD) simulation, plasmonic array design, and fluid flow simulation.The detailed problem and experimental settings will be discussed soon in the following subsections.We first demonstrate the significant This highlights the potential of deep learning in multi-fidelity fusion over traditional machine learning methods.
506( 0ROHFXODU'\QDPLF '&amp;BFLJS GPIDO 1$5 /$5 <em>$5 '0)WUDQV +)6DPSOHV 506( 3ODVPRQLF '&amp;BFLJS GPIDO 1$5 /$5 </em>$5 '0)WUDQV +)6DPSOHV506
With the help of transfer learning, DMF can leverage the knowledge from low-fidelity data more effectively and unleash the great potential of deep learning in multi-fidelity fusion.</p>
<p>Molecular Dynamics simulation: We look at a molecular dynamics simulation [20] to demonstrate the effectiveness of DMF in high-dimensional data.The interatomic interaction in the area of molecular dynamics (MD) is defined using a potential energy expiration.The Lennard-Jones (LJ) potential [20] is a well-known example for simulating these interactions in the MD system:
u r i j = 4ε        σ r i j 12 − σ r i j 6        , (16)
where ϵ is the potential well depth, σ defines the length scale for this pairwise interatomic interaction model, and r i j denotes the pairwise distance between particles i and j.When the integration time step used to solve the MD system's Lagrangian governing equations is too large during this potential energy model's MD simulation, the resulting numerical simulation suffers greatly from numerical instability.To prevent such a numerical instability, we set a limit on the magnitude of repulsive interactions when the ratio of σ r i j exceeds 1.2, which may cause slight inaccuracies in the simulation results but will not affect the experiment conclusion as such a setup is applied across all the MD simulations.We assume σ, ε, m or molecular mass to be 3rA, 1 kcal mol and 12.01 g mol , respectively.Here, the boundary conditions on all sides of the cubic simulation box (with a width of 27.05rA) are viewed as intermittent.</p>
<p>Additionally, we define a uniform temperature and density grid as T × ρ : [500, 1000]K × [36.27, 701.29] kg m 3 (ρ * : [0.05, 0.95]) on 114 sample points, where ρ and ρ * are density and dimensionless density equal to Nm V and Nσ 3 V , respectively, and the mass of each particle (m) and simulation box size (L = V 1 3 ) is set to 12.01 g/mol and 27.05rA.</p>
<p>For this problem, we are interested in the temperature profile of the system, which is obtained by averaging  12.As can be seen, as the amount of low-fidelity data increases, the maximum errors decrease, and the predictions gradually approach the ground truth.With 32 low-fidelity data, the maximum error is about 0.1 K, which is much smaller than the maximum error of 0.5 K obtained by the Gaussian process regression (GPR) method [20] and the maximum error of 0.3 K obtained by the deep neural network (DNN) method [20].</p>
<p>Plasmonic nanoparticle arrays: Coupled Dipole Approximation (CDA) is a technique for imitating the optical response of a collection of similar metallic nanoparticles that are not magnetic and have dimensions much smaller than the wavelength of light (in this case, 25 nm).The extinction and scattering efficiencies Q ext and Q sc for plasmonic systems with varying numbers of scatterers are defined as the QoIs in this work.We examined particle arrays resulting from Vogel spirals (see Fig. 13).Since the number of interactions of incident waves from particles influences the magnetic field, the number of nanoparticles in a plasmonic array substantially affects the local extinction field caused by plasmonic arrays.The configurations of Vogel spirals with particle numbers in the set {2, 25, 50} that define fidelities F1 through F3 are depicted in Fig. 13.λ ∈ [200, 800] nm, α vs ∈ [0, 2π] rad, and a vs ∈ (1, 1500) were determined to be the parameter space.These are, respectively, the incidence wavelength, the divergence angle, and the scaling factor.A Sobol sequence was utilized to choose inputs.The computing time required to execute CDA increases exponentially as the number of nanoparticles increases.Consequently, the proposed sampling approach results in significant reductions in computational costs.</p>
<p>The solution of the local electric fields, E l oc(r j ), for each nano-sphere, can be used to calculate a plasmonic array's response to electromagnetic radiation.Considering N metallic particles defined by the same volumetric polarizability α(ω) and situated at vector coordinates r i , it is possible to calculate the local field E loc (r j ) by solving [10] the corresponding linear equation.Gi j E loc (r j ) (17) in which E 0 (r i ) is the incident field, k is the wave number in the background medium, ϵ 0 denotes the dielectric permittivity of vacuum (ϵ 0 = 1 in the CGS unit system), and Gi j is constructed from 3 × 3 blocks of the overall 3N × 3N Green's matrices for the ith and jth particles.Gi j is a zero matrix when j = i, and otherwise calculated as
Gi j = exp(ikr i j ) r i j I − r i j r T i j − 1 ikr i j + 1 (kr i j ) 2 (I − 3 r i j r T i j )(18)
where r i j denotes the unit position vector from particles j to i and r i j = |r i j |.By solving Equations 17 and 18, the entire local fields E loc (r i ), the scattering and extinction cross-sections are calculated as a result, and the more accurate numerical solution is detailed in [38].Q ext and Q sc are derived by normalizing the scattering and extinction cross-sections relative to the array's entire projected area.We considered the Vogel spiral class of particle arrays, which is described by [5] ρ n = √ na vs and θ n = nα vs ,</p>
<p>where ρ n and θ n represent the radial distance and polar angle of the n-th particle in a Vogel spiral array, respectively.</p>
<p>Therefore, the Vogel spiral configuration may be uniquely defined by the incidence wavelength λ, the divergence angle α vs , the scaling factor a vs , and the number of particles n.</p>
<p>In this problem, we fix the number of low-fidelity training data to 96 samples, and the numbers of high-fidelity data are {4, 16, 64, 96}; we evaluate on 34 withhold test samples.The experimental results are shown in Fig. 14, which shows the performance by comparing the predicted values f H (x) and the true values y H of the high-fidelity (oriented at 45 degrees) and the other near the pipe exit (oriented at 0 degrees).The freestream velocity at the large inlet (with a diameter of 1 m) was chosen as the input parameter space, with values ranging from 0.2 to 2 ms −1 , and the velocity at the smaller inlet (with a diameter of 0.5 m) was chosen with values ranging from 1.2 to 3 ms −1 .</p>
<p>Conclusion</p>
<p>In the era of artificial intelligence, it is no longer a question of whether machine learning can be applied to engineering and scientific problems, but how to apply it effectively and efficiently.In this paper, we propose a novel paradigm for learning a physics simulation efficiently and effectively by using 1) neural network architecture search, 2) multi-fidelity fusion through transfer learning, 3) continuous relaxation scheme, and 4) hyperparameter pruning technique.Despite not being a rigorous Bayesian model (e.g., AR) for multi-fidelity fusion, the proposed DMF method is able to emulate complex physics and outperform the SOTA methods in terms of accuracy and computational time on a variety of challenging benchmark problems and real-world examples to demonstrate its practicality.We believe such an effort will inspire the development of deep learning based multi-fidelity fusion and other related problems in the future to achieve a new level of accuracy and efficiency in engineering and scientific applications.</p>
<p>We expect DMF to be an essential tool for understanding complex physics and an influential model in multifidelity fusion in the near future.The methods described in this paper appear to promise in producing accurate surrogate models capable of rapidly generating higher-fidelity data.</p>
<ol>
<li>Appendix</li>
</ol>
<p>where x is the input features, y represents low-fidelity or high-fidelity value.We train the w and α respectively, where exists ε 1 and ε 2 so that ∆ w L(w, α) &lt; ε 1 and ∆ α L(w, α) &lt; ε 2 , therefore, we have:
&lt; ε 1 + ε 2 . ((24))25
We have a Cauchy sequence L n = {L(x, w n , α n )} on R, according to the completeness of R, the L n is convergent on R, we denote the limit by c, i.e., we have lim n→∞ L n (w n , α n ) = c and</p>
<p>Where o i jl denotes the l-th operation in operation candidate O from node i to node j and w i jl represents the network parameters in our model.The difference between the operations is the number of nodes or layers of MLP.Then we denote a one-layer neural network NN l with an activation function σ(x) by O i jl , then each operation o i jl is the composition of O i jl , which can be written as:
O i jl (x i , w i jl ) = σ(w i jl • x i ). (28)
Without loss of generality, we take the induced norm as the matrix norm, which is defined by
∥w∥ = sup ∥wx∥ ∥x∥ . (29)
We have:
∥O i jl (x i , w i jl,1 ) − O i jl (x i , w i jl,2 )∥(30)
= ∥σ(w i jl,1
• x i ) − σ(w i jl,2 • x i )∥(31)
= ∥σ((w i jl,1 − w i jl,2 )
• x i )∥(32)
⩽ ∥(w i jl,1 − w i jl,2 )
• x i ∥(33)⩽ ∥w i jl,1 − w i jl,2 ∥ ∥x i ∥, (34)
where σ is the ReLU function.In mathematical analysis, given that two metric spaces (X, d X ) and (Y, d Y ), where d X denotes the metric on the set X and d Y is the metric on set Y, a function f : X → Y is called Lipschitz continuous if there exists a real constant K ⩾ 0 such that, for all x 1 , x 2 in X,
d Y ( f (x 1 , x 2 )) ⩽ Kd X (x 1 , x 2 ).(35)
According to the above definition, the Lipschitz functions form a vector space/linear space, because O i jl is Lipschitz, x j (w, α) is also Lipschitz, therefore, f (x, w, α) is Lipschitz with respect to w.As for α, we have: (α i j,1 − α i j,2 ) • o i j (x i , w i j )∥ ( 38)
⩽ 1 N d j⩽N d i&lt; j
∥α 1 − α 2 ∥∥o i j (x i , w i j )∥, (∥α i j,1 − α i j,2 ∥ ⩽ ∥α 1 − α 2 ∥, ∀i, j ⩽ N d ), (39) where N d is the number of total nodes.Notice that o i j (x i , w i j ) is a latent variable.thus it's bounded.Therefore, f (x, w, α) is Lipschitz continuous with respect to α, and we have:
| f (x, w 1 , α) − f (x, w 2 , α)|⩽ C 1 ∥w 1 − w 2 ∥ (40) | f (x, w, α 1 ) − f (x, w, α 2 )|⩽ C 2 ∥α 1 − α 2 ∥.(41)
In the main paper, our algorithm can be concluded as: According to 41, f (x, w n , α n ) is Lipschitz, and f (x, w n , α n ) have bounded first derivatives, which shows that ∇ w f (x, w n , α n ) and ∇ α f (x, w n , α n ) have upper bounds.Therefore, we have:
w n+1 = w n − r 1 ∇ w L(w n , α n ), α n+1 = α n − r 2 ∇ α L(w n , α n ). (42∥α n+1 − α (DMF) n+1 ∥ ⩽ M r 1 r 2 L(w n , α n ) . (60)
When r 1 and r 2 are small enough, and we have w n+1 = w (DMF) n+1 .Therefore, the two training methods are equivalent.</p>
<p>the mapping from x to y L is considered as a composition of multiple NN operations o, and the weights of each operation are optimized by DARTS.In this work, our operation candidates set O consists of five different operations, namely, shallow operation, wide operation, linear operation, and zero operation.The deep operation has four layers with 20 neurons per layer; the shallow has two layers each with 20 neurons; the wide operation has two layers with 40 neurons per layer.We use the rectified linear unit (ReLU) as the activation function for the above operations.</p>
<p>Fig. 1 :
1
Fig. 1: Scheme of DMF with three cells, four cells, five cells, and six cells from left to right.Each arrow represents a composition operation.</p>
<p>Fig. 2 :
2
Fig. 2: Overall procedures for DMF.The low-fidelity model DMF L is taken as pre-training, and the high-fidelity model DMF-trans consists of two components, DMF L and NN l .</p>
<p>Fig. 3 :
3
Fig. 3: Comparison of the different knowledge transfer frameworks</p>
<p>Fig. 4 :
4
Fig. 4: RMSE with different hyperparameters bundle, r 3 = 0.01 (Left), r 3 = 0.001 (Middle), r 3 = 0.0001 (Right)</p>
<p>Fig. 5 :
5
Fig. 5: RMSE with different pruning methods.</p>
<p>Fig. 6 :
6
Fig. 6: RMSE against an Increasing number of high-fidelity training samples for uni-variate benchmarks.</p>
<p>Then, we evaluate</p>
<p>each model on 50 preserved test data points.The results are shown in Fig.6.We can see that the proposed DMF-trans almost always achieves the best performance in most cases.DMF-2 network also obtains similar performance for the Borehole function but slightly worse accuracy for Currin and Park functions.Popular and classic multi-fidelity fusion methods, such as LAR and NAR, are also included in the comparison.Their performance varies according to the problem, which is consistent with the results found in the literature due to their different assumptions and limitations.To investigate the influences of the number of low-fidelity data, we conduct the same experiment with fixed 20 high-fidelity samples and gradually increase the number of low-fidelity samples from 20 to 50.The results are shown in Fig.7.As can be seen in these figures, DMFs (DMF-2 network and DMF-trans) outperform other SOTA methods in three datasets.With further analysis, when the low-fidelity data are inadequate, e.g., 20 samples or less, the performance of DMFs does not change considerably with the increment of high-fidelity data.If the high-fidelity data is extremely scarce (less than five samples), the DMF-2 network is more effective than DMF-trans.However, when the low-fidelity data increase, the validation RMSE of DMFs reduces notably.Moreover, DMF-trans becomes the most accurate model, especially for the Park function.It shows the efficiency of transfer learning, which successfully applies knowledge learning on low-fidelity data to building the high-fidelity model.Increasing</p>
<p>Fig. 7 :Fig. 8 :
78
Fig. 7: RMSE against an Increasing number of low-fidelity training samples for uni-variate benchmarks.</p>
<p>Fig. 9 :
9
Fig. 9: Comparison of MAE fields between different methods on Burger dataset, LAR, NAR, GAR, dmfal, DMF (left to right columns), using high-fidelity data {4, 8, 16, 32} (top to bottom rows) with fixed 32 low-fidelity data.</p>
<p>Fig. 10 :
10
Fig. 10: Comparison of Absolute Errors (AE) between different methods on Poisson dataset, LAR, NAR, GAR, dmfal, DMF (left to right columns), using high-fidelity data {4, 8, 16, 32} (top to bottom rows) with fixed 32 low-fidelity data.</p>
<p>Fig. 11 :
11
Fig. 11: Comparison of SOTA methods on real test examples</p>
<p>Fig. 12 :
12
Fig. 12: The ground-truth temperature profiles for 23 test cases vs. DMF predictions with different numbers of low-fidelity training data.</p>
<p>Fig. 13 :
13
Fig. 13: Sample configurations of Vogel spirals with {2, 25, 50, 500} particles.</p>
<ol>
<li>1 .
1
Proof of Proposition 1We denote the loss of the model during training by L, L(w, α) = ∥ f (x, w, α) − y∥,</li>
</ol>
<p>|∆ w,α | = |L(w + δw, α + δα) − L(w, α)| (22) = |L(w + δw, α + δα) − L(w + δw, α) + L(w + δw, α) − L(w, α)|(23)⩽ |L(w + δw, α + δα) − L(w + δw, α)|+|L(w + δw, α) − L(w, α)|</p>
<p>xα</p>
<p>j (w, α) = i&lt; j α i j • o i j (x i (w, α), w i j ) i jl o i jl (x i , w i jl ),</p>
<p>1 α
1
N d ∥ f (x, w, α 1 ) − f (x, w, α 2 i j,1 • o i j (x i , w i j ) − j⩽N d i&lt; j α i j,2 • o i j (x i , w i j )</p>
<p>)w (DMF) n+1 = w n − r 1 ∇ w L(w n , α n ), α (DMF) n+1 = α n − r 2 ∇ α L(w n+1 , α n ).(43)∥α n+1 − α (DMF) n+1 ∥ (55) = r 1 r 2 2L(w n , α n ) ∥∇ α ( f (x, w n , α n ) + f (x, w n+1 , α n ) (56) − 2y)∇ T w f (x, w n , α n )∇ w L(w n , α n )∥ ⩽ r 1 r 2 2L(w n , α n ) ∥∇ α ( f (x, w n , α n ) + f (x, w n+1 , α n ) (57) − 2y)∥∥∇ w f (x, w n , α n )∥∥∇ w L(w n , α n )∥ = r 1 r 2 L(w n , α n ) ∥∇ α f (x, w n , α n )∥∥∇ w f (x, w n , α n )∥∥∇ w L(w n , α n )</p>
<p>For the surrogate model, we will generate N z × N t × N x training samples on the domain of interest, where N z , N t , and N x are the quantities of spatial, temporal, and parameter samples, respectively.To cover the response surface for such a high-dimensional input space problem, we discretize the continuous inputs space on specified spatial locations z 1 , ..., z N z and temporal locations t 1 , ..., t N t to provide an approximation of u(z, t, x).With the given coordinates, our quantity of interest (QoI) becomes a vectorial function of the PDE parameters.
using a numerical solver. As referenced in the introduction, the immediate executionof the mathematical solver can be computationally restrictive. Experts propose a data-driven surrogate model toestimate u(z, t, x) effectively given the boundary condition in the scope of interest for the spatial-temporal domainof Ω × (0, T ] × X.</p>
<p>Table 1 :
1
Inputs variables and domain of Borehole function
05, 0.15]radius of borehole (m)r ∈ [100, 50000]radius of influence (m)T u ∈ [63070, 115600] transmissivity of upper aquifer m 2 /yrH u ∈ [990, 1110]potentiometric head of upper aquifer (m)T l ∈ [63.1, 116]transmissivity of lower aquifer m 2 /yrH l ∈ [700, 820]potentiometric head of lower aquifer (m)L ∈ [1120, 1680]length of borehole (m)K w ∈ [9855, 12045]hydraulic conductivity of borehole (m/yr)</p>
<p>RMSE.Similar to the previous experiments, we gradually increase the number of high-fidelity training samples to 32 with the number of low-fidelity training samples being fixed to 32.The results are shown in Fig.8.We can see that DMF gives robust and good performance across the range.More specifically, DMF achieves the second lowest error with a low number of training data (i.e., 4 and 8); the RMSE continues to drop as more training data becomes available.With 32 high-fidelity training points, DMF obtains the lowest RMSE, which is expected as our method generally requires more data to well construct a proper model from scratch.In contrast, GAR shows consistently good performance but fails to improve significantly with more training data, which is consistent with its original work.Other baselines, such as LAR and NAR, are significantly outperformed by DMF for almost all settings.</p>
<p>/ Journal of Computational Physics(2023) <br />
model with the R 2 statistics.The R 2 statistics are calculated by the following formula:(20)where ŷi is the predicted value of the i-th sample, and ȳi is the mean value of the i-th sample.As can be seen, the predictions f H (x) are close to the true values y H when the number of high-fidelity data is only 4. Nonetheless, the performance of the high-fidelity model gradually improves as the number of high-fidelity data increases and we can see how the distributions of the scatter points are moving towards the line y = x with the increase of the number of high-fidelity data.The trend is clearly shown with the R 2 statistics.It might seem that the improvement is very significant.We need to note that R 2 &gt; 0.95 is a very high value, which means the strong correlations between two variables and the predictions f H (x) are very close to the true values y H .It already indicates that the high-fidelity model is very accurate in practice.are suboptimal solutions when it comes to the design and optimization of thermal-fluid systems.In this problem, water enters from two inlets, the bottom left end of the pipe and a smaller inlet on the elbow, as shown in Fig.15,From the top right.The pipe has a vertical upward exit for the water.At t = 50s, the QoI were vectorized profiles of the velocity magnitude and pressure in circular cross sections of the elbow pipe, one located at the elbow junction Without loss of generality, we only consider a simplified situation which output y is a scalar, denoted by y.This way, we have:According to the convergence of L(w n , α n ), we approximate 47 by:Again, due to the convergence of L(w n , α n ), the f (x, w n , α n ) − y is also convergent on R, we denote the limit by c, let n → ∞, c → 0 then, we can further simplify Equation 47 into:
{TensorFlow}: a system for {Large-Scale} machine learning. Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, 12th USENIX symposium on operating systems design and implementation (OSDI 16). 2016</p>
<p>Theano: a cpu and gpu math expression compiler. James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, Yoshua Bengio, Proceedings of the Python for scientific computing conference (SciPy). the Python for scientific computing conference (SciPy)Austin, TX20104</p>
<p>Transfer learning based multi-fidelity physics informed deep neural network. Souvik Chakraborty, Journal of Computational Physics. 4261099422021</p>
<p>Numerical methods for engineers. Raymond P Steven C Chapra, Canale, 2011Mcgraw-hill New York1221</p>
<p>Probing scattering resonances of vogel's spirals with the green's matrix spectral method. Aristi Christofi, Felipe A Pinheiro, Luca Dal, Negro , Optics letters. 4192016</p>
<p>Computational fluid dynamics. T , Chung , 2002Cambridge university press</p>
<p>A statistical method for tuning a computer code to a data base. Jeong-Soo Dennis D Cox, Clifford E Park, Singer, Computational statistics &amp; data analysis. 3712001</p>
<p>Deep gaussian processes for multi-fidelity modeling. Kurt Cutajar, Mark Pullin, Andreas Damianou, Neil Lawrence, Javier González, arXiv:1903.073202019arXiv preprint</p>
<p>Deep Gaussian processes and variational propagation of uncertainty. Andreas Damianou, 2015University of SheffieldPhD thesis</p>
<p>Effective-medium theory for finite-size aggregates. Charles-Antoine Guérin, Pierre Mallet, Anne Sentenac, JOSA A. 2322006</p>
<p>Multi-fidelity regression using artificial neural networks: efficient approximation of parameter-dependent output quantities. Mengwu Guo, Andrea Manzoni, Maurice Amendt, Paolo Conti, Jan S Hesthaven, Computer methods in applied mechanics and engineering. 3891143782022</p>
<p>Filter pruning via geometric median for deep convolutional neural networks acceleration. Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, Yi Yang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Multilayer feedforward networks are universal approximators. Kurt Hornik, Maxwell Stinchcombe, Halbert White, Neural networks. 251989</p>
<p>Extraction of material properties through multi-fidelity deep learning from molecular dynamics simulation. Mahmudul Islam, Md Shajedul Hoque, Satyajit Thakur, Mohammad Nasim Mojumder, Hasan, Computational Materials Science. 1881101872021</p>
<p>Face detection with the faster r-cnn. Huaizu Jiang, Erik Learned-Miller, 2017 12th IEEE international conference on automatic face &amp; gesture recognition (FG 2017). IEEE2017</p>
<p>Gaussian process bandit optimisation with multi-fidelity evaluations. Kirthevasan Kandasamy, Gautam Dasarathy, B Junier, Jeff Oliva, Barnabás Schneider, Póczos, Advances in neural information processing systems. 292016</p>
<p>Predicting the output from a complex computer code when fast approximations are available. C Marc, Anthony O' Kennedy, Hagan, Biometrika. 8712000</p>
<p>A hybrid multi-fidelity approach to the optimal design of warm forming processes using a knowledge-based artificial neural network. Hong Seok, Kim , Muammer Koc, Jun Ni, International Journal of Machine Tools and Manufacture. 4722007</p>
<p>Parallel architecture and hyperparameter search via successive halving and classification. Manoj Kumar, George E Dahl, Vijay Vasudevan, Mohammad Norouzi, arXiv:1805.102552018arXiv preprint</p>
<p>Computational materials science: an introduction. June Gunn, Lee , 2016CRC press</p>
<p>Hyperband: A novel bandit-based approach to hyperparameter optimization. Lisha Li, Kevin Jamieson, Giulia Desalvo, Afshin Rostamizadeh, Ameet Talwalkar, The Journal of Machine Learning Research. 1812017</p>
<p>Deep multi-fidelity active learning of high-dimensional outputs. Shibo Li, Robert M Kirby, Shandian Zhe, arXiv:2012.009012020arXiv preprint</p>
<p>Multi-fidelity bayesian optimization via deep neural networks. Shibo Li, Wei Xing, Robert Kirby, Shandian Zhe, Advances in Neural Information Processing Systems. 202033</p>
<p>Hanxiao Liu, Karen Simonyan, Yiming Yang, arXiv:1806.09055Darts: Differentiable architecture search. 2018arXiv preprint</p>
<p>Multi-fidelity bayesian neural networks: Algorithms and applications. Xuhui Meng, Hessam Babaee, George Em Karniadakis, Journal of Computational Physics. 4381103612021</p>
<p>A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse pde problems. Xuhui Meng, George Em Karniadakis, Journal of Computational Physics. 4011090202020</p>
<p>Deep convolutional encoder-decoder networks for uncertainty quantification of dynamic multiphase flow in heterogeneous media. Shaoxing Mo, Yinhao Zhu, Nicholas Zabaras, Xiaoqing Shi, Jichun Wu, Water Resources Research. 5512019</p>
<p>Particle hopping models and traffic flow theory. Kai Nagel, Physical review E. 53546551996</p>
<p>Deep relu networks and high-order finite element methods. Analysis and Applications. Philipp C Joost Aa Opschoor, Christoph Petersen, Schwab, 202018</p>
<p>A survey on transfer learning. Jialin Sinno, Qiang Pan, Yang, IEEE Transactions on knowledge and data engineering. 22102009</p>
<p>Automatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, 2017</p>
<p>Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. Paris Perdikaris, Maziar Raissi, Andreas Damianou, Neil D Lawrence, George Em Karniadakis, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 473201607512198. 2017</p>
<p>The laplace and poisson equations in schwarzschild's space-time. Persides, Journal of Mathematical Analysis and Applications. 4331973</p>
<p>Advances in neural information processing systems. Matthias Poloczek, Jialei Wang, Peter Frazier, 201730Multi-information source optimization</p>
<p>Physically informed artificial neural networks for atomistic modeling of materials. Gp Pun, Batra, Ramprasad, Mishin, Nature communications. 1012019</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational physics. 3782019</p>
<p>Machine learning of linear differential equations using gaussian processes. Maziar Raissi, Paris Perdikaris, George Em Karniadakis, Journal of Computational Physics. 3482017</p>
<p>Optimization of large-scale vogel spiral arrays of plasmonic nanoparticles. Mani Razi, Ren Wang, Yanyan He, Robert M Kirby, Luca Dal, Negro , Plasmonics. 1412019</p>
<p>H Dong, Daniel M Song, Tartakovsky, arXiv:2105.00856Transfer learning on multi-fidelity data. 2021arXiv preprint</p>
<p>A general framework for multi-fidelity bayesian optimization with gaussian processes. Jialin Song, Yuxin Chen, Yisong Yue, The 22nd International Conference on Artificial Intelligence and Statistics. PMLR2019</p>
<p>Burgers equation with a fractional derivative; hereditary effects on nonlinear acoustic waves. Nobumasa Sugimoto, Journal of fluid mechanics. 2251991</p>
<p>Surrogate modeling of computer experiments with different mesh densities. Rui Tuo, Jeff Wu, Dan Yu, Technometrics. 5632014</p>
<p>Gar: Generalized autoregression for multi-fidelity fusion. Yuxin Wang, Zheng Xing, Wei W Xing, arXiv:2301.057292023arXiv preprint</p>
<p>Reduced dimensional gaussian process emulators of parametrized partial differential equations based on isomap. Wei Xing, Akeel A Shah, Prasanth B Nair, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 471201406972174. 2015</p>
<p>Deep coregionalization for the emulation of simulation-based spatial-temporal fields. Robert M Wei W Xing, Shandian Kirby, Zhe, Journal of Computational Physics. 4281099842021</p>
<p>Information-based multi-fidelity bayesian optimization. Yehong Zhang, Trong Nghia Hoang, Bryan Kian Hsiang Low, Mohan Kankanhalli, NIPS Workshop on Bayesian Optimization. 2017</p>
<p>Heterogeneous transfer learning for image classification. Yin Zhu, Yuqiang Chen, Zhongqi Lu, Sinno Jialin Pan, Gui-Rong Xue, Yong Yu, Qiang Yang, Twenty-fifth aaai conference on artificial intelligence. 2011</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9047 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9047</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9047</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-259951557</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.09042v2.pdf" target="_blank">Emotional Intelligence of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average EQ scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9047.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9047.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Situational Evaluation of Complex Emotional Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40-item psychometric test developed in this paper to measure emotion understanding (EU), a core subscale of emotional intelligence; items present realistic scenarios and ask respondents to allocate 10 points across four emotion labels per item. Norms were collected from N=541 adults and scores converted to an EQ scale (mean=100, SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Psychometric test battery introduced by the authors for evaluating LLMs and humans on emotion understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>40 scenario-based items assessing emotion understanding (recognition, interpretation, and understanding in social contexts); each item has four emotions and participants allocate 10 points across them. Scoring uses consensus (average human responses) and Euclidean distance to that standard.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15) from N = 541 undergraduates/postgraduates; individual SECEU Euclidean distance mean = 2.79 (SD = 0.822); Human-to-Human pattern similarity mean r = 0.199 (SD = 0.166).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Consensus scoring: standard score for each item/emotion computed by averaging across 541 human participants; participant/model SECEU score = mean Euclidean distance from standard across 40 items (lower is better). SECEU score converted to EQ via z-score -> scaled (mean 100, SD 15).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>SECEU measures only emotion understanding (EU), not the full multi-faceted construct of EI (e.g., perception, facilitation, regulation). Human norm is young Chinese undergraduates/postgraduates (age 17–30), limiting generalizability. Test is text-only (English used for LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9047.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's state-of-the-art large transformer-based language model evaluated in this study, showing the highest EQ among tested models and substantial representational similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced transformer-based LLM from OpenAI; paper notes unknown parameter size but lists SFT and RLHF as applied; decoder-only architecture implied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown (not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>40-item scenario-based emotion understanding test; see SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 117 (SECEU score = 1.89), outperforming approximately 89% of human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15); Human-to-Human pattern similarity mean r = 0.199 (SD = 0.166).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM outperforms the average human population (reaches 'expert' level; EQ > 115) and exceeds ~89% of the human sample.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Presented English SECEU items in direct Q&A format. Default generation hyperparameters for model runs: temperature = 0.1, top_p = 1, max_tokens = 512. Responses normalized to summation 10 per item as needed. Pattern similarity computed as Pearson r between model discriminability vector and human template.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although EQ is high and pattern similarity (r = 0.28, >67%) indicates more human-like representations than many models, representational similarity is not identical to humans; prompts had little effect on GPT-4 in these tests. The human norm is demographically limited. The test assesses only EU, not other EI facets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9047.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used OpenAI model (instruct-tuned variant) evaluated here; achieved above-average EQ but lower representational similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI instruct-tuned transformer model (decoder-only family); trained with supervised fine-tuning and RLHF according to table flags.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (listed in table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 103 (SECEU score = 2.63), outperforming ~52% of humans.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches or slightly exceeds average human performance (falls in 'normal' range: 85–115 EQ).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot prompts worked effectively for GPT-3.5-turbo in some prompt-engineering experiments; default run parameters: temperature = 0.1, top_p = 1, max_tokens = 512. Some prompt variations (step-by-step) increased pattern similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Pattern similarity to humans is low (r = 0.04, >17%), indicating performance may rely on mechanisms different from human patterns despite above-average EQ. Performance benefits notably from prompt engineering in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9047.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruct-tuned GPT-3.5-era OpenAI model evaluated with prompts; performed in the 'expert' to high-normal range in EQ and showed sizable representational similarity improvements with prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-era model with supervised fine-tuning and RLHF (table indicates both).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B (table lists 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 114 (SECEU score = 2.01), outperforming ~83% of humans; with two-shot chain-of-thought prompts pattern similarity r = 0.31 (p < 0.05, >73%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM exceeds average human performance (upper-normal to expert border) when prompt-engineered; matches or exceeds a large fraction of humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Model required or benefitted substantially from prompts; prompt engineering included Two-shot Chain-of-Thought and Step-by-Step variants. Default generation hyperparameters listed for non-prompt runs: temperature = 0.1, top_p = 1, max_tokens = 512.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performance is prompt-sensitive; without prompts some earlier models could not complete the task. Representational similarity improved with prompts, indicating dependence on prompt format to achieve human-like item-wise behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9047.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI model (GPT-3.5-era) evaluated here; completed the SECEU only with prompting and showed low representational similarity to humans despite some EQ improvement with prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI model (intermediate version) with some supervised fine-tuning; table flags SFT but not RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><175B (table indicates <175B)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 91 (SECEU score = 3.39), outperforming ~23% of humans. Pattern similarity r = -0.04 (>8%), indicating qualitative differences from human patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM performs below average human baseline (EQ < 100) and substantially below expert threshold; shows qualitatively different item-wise behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Model required prompts to complete the SECEU in some configurations (Two-shot COT / Step-by-step tested). Default generation hyperparameters for runs without prompts: temperature = 0.1, top_p = 1, max_tokens = 512.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although earlier versions improved under prompting in other reports, in this study text-davinci-002's pattern similarity to humans was negative/near zero, suggesting qualitatively different mechanisms even when EQ approaches average.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9047.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI GPT-3-era model evaluated with SECEU; completed the test with prompting and obtained an above-average EQ in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3-era model; table indicates SFT but no RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><175B (table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 107 (SECEU score = 2.41), outperforming ~64% of humans. Pattern similarity r = 0.24 (>7%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM slightly exceeds average human baseline (normal range).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Required prompt engineering to complete test in some cases; same normalization/scoring procedures applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Model completion sometimes required prompts; representational similarity is modest, implying possible mechanistic differences from human responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9047.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaVinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaVinci (OpenAI older GPT-3 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An older OpenAI model variant that in this study required prompts to complete SECEU, achieved low EQ and showed high pattern similarity when prompt-engineered.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaVinci</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Older GPT-3 variant from OpenAI; table lists size 175B and indicates both SFT and RLHF applied in later variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 87 (SECEU score = 3.587), outperforming ~18% of humans. With two-shot chain-of-thought prompts, pattern similarity r = 0.41 (p < 0.01, >91%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below average human baseline in EQ (poor range), but prompt-engineering can increase pattern similarity to human responses even if EQ remains lower.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Failed to complete some runs without prompts; prompt engineering (Two-shot Chain-of-Thought) allowed completion and increased representational similarity. Default generation hyperparameters listed earlier (temp 0.1, top_p 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Raw EQ low, heavy dependence on prompt engineering to both complete test and approximate human-like item-wise patterns; with no prompts many items were failed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9047.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie (OpenAI GPT-3 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI GPT-3-class model evaluated here; failed to complete the SECEU in direct runs (marked as FAILED in table) and achieved a relatively low EQ when attempts were made with prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 family model; table indicates SFT only (no RLHF).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Marked as FAILED to complete test under direct prompts in primary runs; when forced with prompts some runs were possible but overall EQ listed as 102 in table (SECEU score = 2.71) and percent ~50%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>When completed via heavy prompt engineering Curie is around average human; however, baseline inability to reliably complete items indicates weaker practical EU capability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Several models (including Curie) failed to produce valid outputs for many items without specialized prompts; responses of (0,0,0,0) were recorded for null outputs and normalization procedures applied as described in Methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High failure rate to complete test without prompts; results for Curie are unstable and depend strongly on prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9047.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Babbage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Babbage (OpenAI GPT-3 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller OpenAI GPT-3-class model evaluated here; completed the test in some configurations but had low EQ and negative/near-zero pattern similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Babbage</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI smaller GPT-3 family model; table indicates SFT only.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 100 (SECEU score = 2.78), outperforming ~44% of humans. Pattern similarity r = -0.12 (>4%), indicating qualitatively different item-wise patterns from humans.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Nominal EQ near average human, but representational pattern analysis suggests Babbage employs a qualitatively different mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Completed test with prompt assistance in many cases; default hyperparameters and normalization applied. Null responses handled per Methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Negative pattern similarity shows item-wise performance diverges from human error/strength profile; EQ alone is insufficient to claim human-like processing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9047.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Base LLaMA model (open-source decoder-only family) was evaluated but failed to complete the SECEU in this study without being fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only foundation model (Meta) of 13B parameters in table; base model without instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FAILED to complete the test (no valid responses for items in main runs).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline in practical terms because it could not complete the task without further fine-tuning or prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Base LLaMA could not produce meaningful outputs for many items even with prompts; methods recorded null vectors for failed items. Default decoding parameters applied where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Base models lacking instruction tuning or SFT may be unable to follow the structured response format required by SECEU; comparisons limited unless instruction-fine-tuned variants are used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9047.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (instruction-tuned LLaMA derivative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLaMA-based open-source model evaluated here; achieved a normal EQ but low pattern similarity to humans in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-following model derived from LLaMA (13B) with supervised fine-tuning (SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 104 (SECEU score = 2.56), outperforming ~56% of humans. Pattern similarity r = 0.03 (>15%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM performs slightly above the human average (normal range), but pattern similarity suggests only weak alignment with human item-wise behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruction-tuned open-source model run with default decoding (temperature = 0.1 etc.); some normalization steps applied when outputs didn't sum to 10.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Low item-wise correlation (r≈0) implies Alpaca's strengths and weaknesses across items do not mirror human patterns, despite above-average EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9047.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (LLaMA-based open-source chatbot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source chat model built on LLaMA and instruction-tuning datasets; achieved above-average EQ but showed near-zero/negative pattern similarity to humans in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-based model (13B) fine-tuned on chat/instruction datasets; table flags SFT but not RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 105 (SECEU score = 2.51), outperforming ~59% of humans. Pattern similarity r = -0.02 (>10%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM slightly exceeds average human EQ but item-wise pattern differs qualitatively from human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Standard prompt format used; outputs normalized if they did not sum to 10. Some prompts improved completion rate but pattern similarity remained low.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Despite nominally good EQ, negative/near-zero item-wise correlation suggests qualitatively different internal representations compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e9047.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Koala</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Koala (LLaMA-based dialogue model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLaMA derivative focused on dialogue; in this study Koala had relatively low EQ but the highest representational similarity to humans among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Koala</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source dialogue model based on LLaMA (13B) with supervised fine-tuning for conversational behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 83 (SECEU score = 3.728), outperforming ~13% of humans. Pattern similarity r = 0.43 (p < 0.01), exceeding 93% of human participants' pattern similarity distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15); Human-to-Human pattern similarity distribution mean r = 0.199 (SD = 0.166).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM performs below average humans in EQ but demonstrates a human-like item-wise pattern (high representational similarity), suggesting quantitative rather than qualitative differences.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Model outputs were processed with same normalization rules; default decoding used where applicable. Pattern similarity computed as Pearson r against human template.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High pattern similarity despite low EQ suggests Koala tends to be 'right/wrong' on the same items as humans but with larger magnitude errors; overall EQ still poor. Completion and prompting details could influence results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e9047.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fastchat (Flan-T5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fastchat (Flan-T5 based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of instruction-tuned encoder-decoder models (Flan-T5 derived) evaluated here but marked as unable to complete the SECEU in primary runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fastchat (Flan-T5 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned encoder-decoder transformer family (Flan-T5 based); table lists size 3B and SFT only.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FAILED to complete the test (marked FAILED in table; many null responses).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Cannot be compared meaningfully to humans in this study due to failure to produce valid outputs for many items.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Even with prompt engineering Fastchat failed for many items (31 items with null outputs). Null vectors stored as (0,0,0,0) for failed items and normalization rules applied overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Encoder-decoder instruction-tuned models may require different prompting or further fine-tuning to produce the constrained 4-value summing-to-10 format; results reflect inability to follow output constraints rather than conceptual EU inability per se.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e9047.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dolly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dolly (Pythia-based open model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Pythia-family open instruction-tuned model evaluated here; achieved slightly below/around human-average EQ and modest pattern similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dolly</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pythia-based instruction-tuned open model (13B) with supervised fine-tuning according to table flags.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 98 (SECEU score = 2.899), outperforming ~38% of humans. Pattern similarity r = 0.26 (>62%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM slightly below average human EQ but shows moderate item-wise similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>English SECEU prompt presented; normalization and default decoding hyperparameters applied. SFT used for fine-tuning as per model properties.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>EQ near average but below; moderate pattern similarity suggests some alignment but not fully human-like representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e9047.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oasst</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oasst (Pythia-based/open assistant dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Pythia-derived model fine-tuned on assistant-style datasets; achieved above-average EQ in this study and moderate pattern similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Oasst</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pythia-family model (13B) fine-tuned on assistant-style datasets (OpenAssistant), flagged SFT and RLHF in table.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 107 (SECEU score = 2.41), outperforming ~64% of humans. Pattern similarity r = 0.24 (>59%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM exceeds average human EQ (normal-to-expert range).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Assessed with English SECEU and standardized scoring; used default decoding hyperparameters and normalization rules for outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although above-average EQ, pattern similarity is moderate and not as high as some models; results depend on fine-tuning and training data peculiarities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e9047.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM (GLM-based bilingual model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GLM-family bilingual model evaluated here; obtained EQ slightly below average and low representational similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-based model (approx. 6B parameters per table) with SFT and possibly bilingual training characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 94 (SECEU score = 3.1294), outperforming ~28% of humans. Pattern similarity r = 0.09 (>24%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below average human EQ (poor-to-normal boundary).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>English SECEU used despite bilingual model; outputs normalized to sum to 10 where applicable. Default decoding parameters used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Language and training differences (bilinguality) plus smaller size may limit performance; pattern similarity low indicating different item-wise response profile.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e9047.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWKV-v4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RWKV-v4 (Recurrent Weighted Key-Value RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-based sequence model (RWKV family) evaluated here but failed to complete the SECEU under tested conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RWKV-v4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RNN-like architecture (RWKV) alternative to Transformers; table lists 13B and SFT only.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FAILED to complete the test (many null responses recorded).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No meaningful comparison due to failure to produce valid outputs for a majority of items.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Even with prompt assistance RWKV-v4 produced null or invalid outputs for many items (31 failed items); null vectors recorded as (0,0,0,0).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Architecture differences (RNN vs Transformer) may impede following the constrained 4-value summation format; failures could reflect output-format and instruction-following limitations rather than conceptual inability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9047.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e9047.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude model evaluated here; achieved above-average EQ and modest pattern similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational model from Anthropic; table lists SFT and RLHF applied, size unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown (not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Situational Evaluation of Complex Emotional Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>See SECEU entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>EQ = 106 (SECEU score = 2.46), outperforming ~61% of humans. Pattern similarity r = 0.11 (>28%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: EQ mean = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM exceeds average human EQ (normal-to-expert border).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with English SECEU in direct Q&A; normalization rules and decoding hyperparameters applied as for other closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Pattern similarity modest; exact model size and training details not public, limiting interpretability of why Claude attains above-average EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of Mind May Have Spontaneously Emerged in Large Language Models <em>(Rating: 2)</em></li>
                <li>Administration of the text-based portions of a general IQ test to five different large language models <em>(Rating: 2)</em></li>
                <li>Sparks of Artificial General Intelligence: Early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Boosting Theory-of-Mind Performance in Large Language Models via Prompting <em>(Rating: 2)</em></li>
                <li>Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT <em>(Rating: 1)</em></li>
                <li>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9047",
    "paper_id": "paper-259951557",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "SECEU",
            "name_full": "Situational Evaluation of Complex Emotional Understanding",
            "brief_description": "A 40-item psychometric test developed in this paper to measure emotion understanding (EU), a core subscale of emotional intelligence; items present realistic scenarios and ask respondents to allocate 10 points across four emotion labels per item. Norms were collected from N=541 adults and scores converted to an EQ scale (mean=100, SD=15).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Psychometric test battery introduced by the authors for evaluating LLMs and humans on emotion understanding.",
            "model_size": null,
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "40 scenario-based items assessing emotion understanding (recognition, interpretation, and understanding in social contexts); each item has four emotions and participants allocate 10 points across them. Scoring uses consensus (average human responses) and Euclidean distance to that standard.",
            "llm_performance": null,
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15) from N = 541 undergraduates/postgraduates; individual SECEU Euclidean distance mean = 2.79 (SD = 0.822); Human-to-Human pattern similarity mean r = 0.199 (SD = 0.166).",
            "performance_comparison": null,
            "experimental_details": "Consensus scoring: standard score for each item/emotion computed by averaging across 541 human participants; participant/model SECEU score = mean Euclidean distance from standard across 40 items (lower is better). SECEU score converted to EQ via z-score -&gt; scaled (mean 100, SD 15).",
            "limitations_or_caveats": "SECEU measures only emotion understanding (EU), not the full multi-faceted construct of EI (e.g., perception, facilitation, regulation). Human norm is young Chinese undergraduates/postgraduates (age 17–30), limiting generalizability. Test is text-only (English used for LLMs).",
            "uuid": "e9047.0",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "OpenAI's state-of-the-art large transformer-based language model evaluated in this study, showing the highest EQ among tested models and substantial representational similarity to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Advanced transformer-based LLM from OpenAI; paper notes unknown parameter size but lists SFT and RLHF as applied; decoder-only architecture implied.",
            "model_size": "unknown (not specified in paper)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "40-item scenario-based emotion understanding test; see SECEU entry.",
            "llm_performance": "EQ = 117 (SECEU score = 1.89), outperforming approximately 89% of human participants.",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15); Human-to-Human pattern similarity mean r = 0.199 (SD = 0.166).",
            "performance_comparison": "LLM outperforms the average human population (reaches 'expert' level; EQ &gt; 115) and exceeds ~89% of the human sample.",
            "experimental_details": "Presented English SECEU items in direct Q&A format. Default generation hyperparameters for model runs: temperature = 0.1, top_p = 1, max_tokens = 512. Responses normalized to summation 10 per item as needed. Pattern similarity computed as Pearson r between model discriminability vector and human template.",
            "limitations_or_caveats": "Although EQ is high and pattern similarity (r = 0.28, &gt;67%) indicates more human-like representations than many models, representational similarity is not identical to humans; prompts had little effect on GPT-4 in these tests. The human norm is demographically limited. The test assesses only EU, not other EI facets.",
            "uuid": "e9047.1",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5 Turbo",
            "brief_description": "A widely used OpenAI model (instruct-tuned variant) evaluated here; achieved above-average EQ but lower representational similarity to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI instruct-tuned transformer model (decoder-only family); trained with supervised fine-tuning and RLHF according to table flags.",
            "model_size": "175B (listed in table)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 103 (SECEU score = 2.63), outperforming ~52% of humans.",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM matches or slightly exceeds average human performance (falls in 'normal' range: 85–115 EQ).",
            "experimental_details": "Zero-shot prompts worked effectively for GPT-3.5-turbo in some prompt-engineering experiments; default run parameters: temperature = 0.1, top_p = 1, max_tokens = 512. Some prompt variations (step-by-step) increased pattern similarity.",
            "limitations_or_caveats": "Pattern similarity to humans is low (r = 0.04, &gt;17%), indicating performance may rely on mechanisms different from human patterns despite above-average EQ. Performance benefits notably from prompt engineering in some cases.",
            "uuid": "e9047.2",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003",
            "brief_description": "An instruct-tuned GPT-3.5-era OpenAI model evaluated with prompts; performed in the 'expert' to high-normal range in EQ and showed sizable representational similarity improvements with prompts.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "OpenAI GPT-3.5-era model with supervised fine-tuning and RLHF (table indicates both).",
            "model_size": "≈175B (table lists 175B)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 114 (SECEU score = 2.01), outperforming ~83% of humans; with two-shot chain-of-thought prompts pattern similarity r = 0.31 (p &lt; 0.05, &gt;73%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM exceeds average human performance (upper-normal to expert border) when prompt-engineered; matches or exceeds a large fraction of humans.",
            "experimental_details": "Model required or benefitted substantially from prompts; prompt engineering included Two-shot Chain-of-Thought and Step-by-Step variants. Default generation hyperparameters listed for non-prompt runs: temperature = 0.1, top_p = 1, max_tokens = 512.",
            "limitations_or_caveats": "Performance is prompt-sensitive; without prompts some earlier models could not complete the task. Representational similarity improved with prompts, indicating dependence on prompt format to achieve human-like item-wise behavior.",
            "uuid": "e9047.3",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002",
            "brief_description": "An earlier OpenAI model (GPT-3.5-era) evaluated here; completed the SECEU only with prompting and showed low representational similarity to humans despite some EQ improvement with prompts.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "OpenAI model (intermediate version) with some supervised fine-tuning; table flags SFT but not RLHF.",
            "model_size": "&lt;175B (table indicates &lt;175B)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 91 (SECEU score = 3.39), outperforming ~23% of humans. Pattern similarity r = -0.04 (&gt;8%), indicating qualitative differences from human patterns.",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM performs below average human baseline (EQ &lt; 100) and substantially below expert threshold; shows qualitatively different item-wise behavior.",
            "experimental_details": "Model required prompts to complete the SECEU in some configurations (Two-shot COT / Step-by-step tested). Default generation hyperparameters for runs without prompts: temperature = 0.1, top_p = 1, max_tokens = 512.",
            "limitations_or_caveats": "Although earlier versions improved under prompting in other reports, in this study text-davinci-002's pattern similarity to humans was negative/near zero, suggesting qualitatively different mechanisms even when EQ approaches average.",
            "uuid": "e9047.4",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "text-davinci-001",
            "name_full": "text-davinci-001",
            "brief_description": "An earlier OpenAI GPT-3-era model evaluated with SECEU; completed the test with prompting and obtained an above-average EQ in this study.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "text-davinci-001",
            "model_description": "OpenAI GPT-3-era model; table indicates SFT but no RLHF.",
            "model_size": "&lt;175B (table)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 107 (SECEU score = 2.41), outperforming ~64% of humans. Pattern similarity r = 0.24 (&gt;7%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM slightly exceeds average human baseline (normal range).",
            "experimental_details": "Required prompt engineering to complete test in some cases; same normalization/scoring procedures applied.",
            "limitations_or_caveats": "Model completion sometimes required prompts; representational similarity is modest, implying possible mechanistic differences from human responses.",
            "uuid": "e9047.5",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DaVinci",
            "name_full": "DaVinci (OpenAI older GPT-3 variant)",
            "brief_description": "An older OpenAI model variant that in this study required prompts to complete SECEU, achieved low EQ and showed high pattern similarity when prompt-engineered.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "DaVinci",
            "model_description": "Older GPT-3 variant from OpenAI; table lists size 175B and indicates both SFT and RLHF applied in later variants.",
            "model_size": "175B (table)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 87 (SECEU score = 3.587), outperforming ~18% of humans. With two-shot chain-of-thought prompts, pattern similarity r = 0.41 (p &lt; 0.01, &gt;91%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM below average human baseline in EQ (poor range), but prompt-engineering can increase pattern similarity to human responses even if EQ remains lower.",
            "experimental_details": "Failed to complete some runs without prompts; prompt engineering (Two-shot Chain-of-Thought) allowed completion and increased representational similarity. Default generation hyperparameters listed earlier (temp 0.1, top_p 1).",
            "limitations_or_caveats": "Raw EQ low, heavy dependence on prompt engineering to both complete test and approximate human-like item-wise patterns; with no prompts many items were failed.",
            "uuid": "e9047.6",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Curie",
            "name_full": "Curie (OpenAI GPT-3 variant)",
            "brief_description": "An OpenAI GPT-3-class model evaluated here; failed to complete the SECEU in direct runs (marked as FAILED in table) and achieved a relatively low EQ when attempts were made with prompts.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Curie",
            "model_description": "OpenAI GPT-3 family model; table indicates SFT only (no RLHF).",
            "model_size": "13B (table)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "Marked as FAILED to complete test under direct prompts in primary runs; when forced with prompts some runs were possible but overall EQ listed as 102 in table (SECEU score = 2.71) and percent ~50%.",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "When completed via heavy prompt engineering Curie is around average human; however, baseline inability to reliably complete items indicates weaker practical EU capability.",
            "experimental_details": "Several models (including Curie) failed to produce valid outputs for many items without specialized prompts; responses of (0,0,0,0) were recorded for null outputs and normalization procedures applied as described in Methods.",
            "limitations_or_caveats": "High failure rate to complete test without prompts; results for Curie are unstable and depend strongly on prompt engineering.",
            "uuid": "e9047.7",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Babbage",
            "name_full": "Babbage (OpenAI GPT-3 variant)",
            "brief_description": "A smaller OpenAI GPT-3-class model evaluated here; completed the test in some configurations but had low EQ and negative/near-zero pattern similarity to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Babbage",
            "model_description": "OpenAI smaller GPT-3 family model; table indicates SFT only.",
            "model_size": "3B (table)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 100 (SECEU score = 2.78), outperforming ~44% of humans. Pattern similarity r = -0.12 (&gt;4%), indicating qualitatively different item-wise patterns from humans.",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "Nominal EQ near average human, but representational pattern analysis suggests Babbage employs a qualitatively different mechanism.",
            "experimental_details": "Completed test with prompt assistance in many cases; default hyperparameters and normalization applied. Null responses handled per Methods.",
            "limitations_or_caveats": "Negative pattern similarity shows item-wise performance diverges from human error/strength profile; EQ alone is insufficient to claim human-like processing.",
            "uuid": "e9047.8",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA (Meta)",
            "brief_description": "Base LLaMA model (open-source decoder-only family) was evaluated but failed to complete the SECEU in this study without being fine-tuned.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA (base)",
            "model_description": "Open-source decoder-only foundation model (Meta) of 13B parameters in table; base model without instruction fine-tuning.",
            "model_size": "13B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "FAILED to complete the test (no valid responses for items in main runs).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "Below human baseline in practical terms because it could not complete the task without further fine-tuning or prompt engineering.",
            "experimental_details": "Base LLaMA could not produce meaningful outputs for many items even with prompts; methods recorded null vectors for failed items. Default decoding parameters applied where applicable.",
            "limitations_or_caveats": "Base models lacking instruction tuning or SFT may be unable to follow the structured response format required by SECEU; comparisons limited unless instruction-fine-tuned variants are used.",
            "uuid": "e9047.9",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Alpaca",
            "name_full": "Alpaca (instruction-tuned LLaMA derivative)",
            "brief_description": "An instruction-tuned LLaMA-based open-source model evaluated here; achieved a normal EQ but low pattern similarity to humans in this task.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Alpaca",
            "model_description": "Open-source instruction-following model derived from LLaMA (13B) with supervised fine-tuning (SFT).",
            "model_size": "13B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 104 (SECEU score = 2.56), outperforming ~56% of humans. Pattern similarity r = 0.03 (&gt;15%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM performs slightly above the human average (normal range), but pattern similarity suggests only weak alignment with human item-wise behavior.",
            "experimental_details": "Instruction-tuned open-source model run with default decoding (temperature = 0.1 etc.); some normalization steps applied when outputs didn't sum to 10.",
            "limitations_or_caveats": "Low item-wise correlation (r≈0) implies Alpaca's strengths and weaknesses across items do not mirror human patterns, despite above-average EQ.",
            "uuid": "e9047.10",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna (LLaMA-based open-source chatbot)",
            "brief_description": "An open-source chat model built on LLaMA and instruction-tuning datasets; achieved above-average EQ but showed near-zero/negative pattern similarity to humans in this study.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Vicuna",
            "model_description": "Open-source LLaMA-based model (13B) fine-tuned on chat/instruction datasets; table flags SFT but not RLHF.",
            "model_size": "13B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 105 (SECEU score = 2.51), outperforming ~59% of humans. Pattern similarity r = -0.02 (&gt;10%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM slightly exceeds average human EQ but item-wise pattern differs qualitatively from human participants.",
            "experimental_details": "Standard prompt format used; outputs normalized if they did not sum to 10. Some prompts improved completion rate but pattern similarity remained low.",
            "limitations_or_caveats": "Despite nominally good EQ, negative/near-zero item-wise correlation suggests qualitatively different internal representations compared to humans.",
            "uuid": "e9047.11",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Koala",
            "name_full": "Koala (LLaMA-based dialogue model)",
            "brief_description": "An instruction-tuned LLaMA derivative focused on dialogue; in this study Koala had relatively low EQ but the highest representational similarity to humans among tested models.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Koala",
            "model_description": "Open-source dialogue model based on LLaMA (13B) with supervised fine-tuning for conversational behavior.",
            "model_size": "13B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 83 (SECEU score = 3.728), outperforming ~13% of humans. Pattern similarity r = 0.43 (p &lt; 0.01), exceeding 93% of human participants' pattern similarity distribution.",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15); Human-to-Human pattern similarity distribution mean r = 0.199 (SD = 0.166).",
            "performance_comparison": "LLM performs below average humans in EQ but demonstrates a human-like item-wise pattern (high representational similarity), suggesting quantitative rather than qualitative differences.",
            "experimental_details": "Model outputs were processed with same normalization rules; default decoding used where applicable. Pattern similarity computed as Pearson r against human template.",
            "limitations_or_caveats": "High pattern similarity despite low EQ suggests Koala tends to be 'right/wrong' on the same items as humans but with larger magnitude errors; overall EQ still poor. Completion and prompting details could influence results.",
            "uuid": "e9047.12",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Fastchat (Flan-T5)",
            "name_full": "Fastchat (Flan-T5 based)",
            "brief_description": "A family of instruction-tuned encoder-decoder models (Flan-T5 derived) evaluated here but marked as unable to complete the SECEU in primary runs.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Fastchat (Flan-T5 variant)",
            "model_description": "Instruction-tuned encoder-decoder transformer family (Flan-T5 based); table lists size 3B and SFT only.",
            "model_size": "3B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "FAILED to complete the test (marked FAILED in table; many null responses).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "Cannot be compared meaningfully to humans in this study due to failure to produce valid outputs for many items.",
            "experimental_details": "Even with prompt engineering Fastchat failed for many items (31 items with null outputs). Null vectors stored as (0,0,0,0) for failed items and normalization rules applied overall.",
            "limitations_or_caveats": "Encoder-decoder instruction-tuned models may require different prompting or further fine-tuning to produce the constrained 4-value summing-to-10 format; results reflect inability to follow output constraints rather than conceptual EU inability per se.",
            "uuid": "e9047.13",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Dolly",
            "name_full": "Dolly (Pythia-based open model)",
            "brief_description": "A Pythia-family open instruction-tuned model evaluated here; achieved slightly below/around human-average EQ and modest pattern similarity.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Dolly",
            "model_description": "Pythia-based instruction-tuned open model (13B) with supervised fine-tuning according to table flags.",
            "model_size": "13B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 98 (SECEU score = 2.899), outperforming ~38% of humans. Pattern similarity r = 0.26 (&gt;62%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM slightly below average human EQ but shows moderate item-wise similarity to humans.",
            "experimental_details": "English SECEU prompt presented; normalization and default decoding hyperparameters applied. SFT used for fine-tuning as per model properties.",
            "limitations_or_caveats": "EQ near average but below; moderate pattern similarity suggests some alignment but not fully human-like representations.",
            "uuid": "e9047.14",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Oasst",
            "name_full": "Oasst (Pythia-based/open assistant dataset)",
            "brief_description": "An open-source Pythia-derived model fine-tuned on assistant-style datasets; achieved above-average EQ in this study and moderate pattern similarity to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Oasst",
            "model_description": "Pythia-family model (13B) fine-tuned on assistant-style datasets (OpenAssistant), flagged SFT and RLHF in table.",
            "model_size": "13B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 107 (SECEU score = 2.41), outperforming ~64% of humans. Pattern similarity r = 0.24 (&gt;59%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM exceeds average human EQ (normal-to-expert range).",
            "experimental_details": "Assessed with English SECEU and standardized scoring; used default decoding hyperparameters and normalization rules for outputs.",
            "limitations_or_caveats": "Although above-average EQ, pattern similarity is moderate and not as high as some models; results depend on fine-tuning and training data peculiarities.",
            "uuid": "e9047.15",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ChatGLM",
            "name_full": "ChatGLM (GLM-based bilingual model)",
            "brief_description": "A GLM-family bilingual model evaluated here; obtained EQ slightly below average and low representational similarity to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "ChatGLM",
            "model_description": "GLM-based model (approx. 6B parameters per table) with SFT and possibly bilingual training characteristics.",
            "model_size": "6B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 94 (SECEU score = 3.1294), outperforming ~28% of humans. Pattern similarity r = 0.09 (&gt;24%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM below average human EQ (poor-to-normal boundary).",
            "experimental_details": "English SECEU used despite bilingual model; outputs normalized to sum to 10 where applicable. Default decoding parameters used.",
            "limitations_or_caveats": "Language and training differences (bilinguality) plus smaller size may limit performance; pattern similarity low indicating different item-wise response profile.",
            "uuid": "e9047.16",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "RWKV-v4",
            "name_full": "RWKV-v4 (Recurrent Weighted Key-Value RNN)",
            "brief_description": "An RNN-based sequence model (RWKV family) evaluated here but failed to complete the SECEU under tested conditions.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "RWKV-v4",
            "model_description": "RNN-like architecture (RWKV) alternative to Transformers; table lists 13B and SFT only.",
            "model_size": "13B",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "FAILED to complete the test (many null responses recorded).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "No meaningful comparison due to failure to produce valid outputs for a majority of items.",
            "experimental_details": "Even with prompt assistance RWKV-v4 produced null or invalid outputs for many items (31 failed items); null vectors recorded as (0,0,0,0).",
            "limitations_or_caveats": "Architecture differences (RNN vs Transformer) may impede following the constrained 4-value summation format; failures could reflect output-format and instruction-following limitations rather than conceptual inability.",
            "uuid": "e9047.17",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Claude (Anthropic)",
            "brief_description": "Anthropic's Claude model evaluated here; achieved above-average EQ and modest pattern similarity to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Claude",
            "model_description": "Closed-source conversational model from Anthropic; table lists SFT and RLHF applied, size unknown.",
            "model_size": "unknown (not specified in paper)",
            "test_battery_name": "SECEU (Situational Evaluation of Complex Emotional Understanding)",
            "test_description": "See SECEU entry.",
            "llm_performance": "EQ = 106 (SECEU score = 2.46), outperforming ~61% of humans. Pattern similarity r = 0.11 (&gt;28%).",
            "human_baseline_performance": "Human norm: EQ mean = 100 (SD = 15).",
            "performance_comparison": "LLM exceeds average human EQ (normal-to-expert border).",
            "experimental_details": "Evaluated with English SECEU in direct Q&A; normalization rules and decoding hyperparameters applied as for other closed-source models.",
            "limitations_or_caveats": "Pattern similarity modest; exact model size and training details not public, limiting interpretability of why Claude attains above-average EQ.",
            "uuid": "e9047.18",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Administration of the text-based portions of a general IQ test to five different large language models",
            "rating": 2,
            "sanitized_title": "administration_of_the_textbased_portions_of_a_general_iq_test_to_five_different_large_language_models"
        },
        {
            "paper_title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
            "rating": 2,
            "sanitized_title": "boosting_theoryofmind_performance_in_large_language_models_via_prompting"
        },
        {
            "paper_title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
            "rating": 1,
            "sanitized_title": "will_affective_computing_emerge_from_foundation_models_and_general_ai_a_first_evaluation_on_chatgpt"
        },
        {
            "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "rating": 1,
            "sanitized_title": "neural_theoryofmind_on_the_limits_of_social_intelligence_in_large_lms"
        }
    ],
    "cost": 0.020657,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Emotional Intelligence of Large Language Models</p>
<p>Xuena Wang 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>Xueting Li 
Department of Psychology
Renmin University of China</p>
<p>Zi Yin 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>Yue Wu 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>&amp; Liu 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>Emotional Intelligence of Large Language Models
51E780518DA5969F098747A04075F340Emotional IntelligenceEmotional UnderstandingLLMhuman-likeness
Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning.However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated.Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions.Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs.This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score).With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs.Most achieved above-average EQ scores, with GPT-4 exceeding 89% of human participants with an EQ of 117.Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans.In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ.In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence.Project website: https://emotional-2 intelligence.github.io/</p>
<p>Introduction</p>
<p>Imagine an ancient male making a necklace from a pile of shells as a gift for a female.This endeavor would require at least two distinct types of abilities.First, he would need the foresight to conceptualize that if a hole were punched in each shell and a string threaded through these holes, the shells could form a necklace.Second, he must possess a rudimentary level of empathy, inferring that the female recipient of the necklace would likely experience joy.The former ability is a manifestation of the Systemizing Mechanism (Baron-Cohen, 2020), enabling humans to become the scientific and technological masters of our physical world.The latter, on the other hand, is referred to as Emotional Intelligence (EI), which allows us to think about our own and others' thoughts and feelings, thereby aiding us in navigating the social world (Mayer, Perkins, et al., 2001;Mayer, Salovey, et al., 2001;Salovey &amp; Mayer, 1990).</p>
<p>In recent years, Large Language Models (LLMs) have made substantial strides, showcasing their expertise across multiple disciplines including mathematics, coding, visual comprehension, medicine, law, and psychology (Bubeck et al., 2023).Their impressive performance in logic-based tasks implies that LLMs, such as GPT-4, might be equipped with the Systemizing Mechanism comparable to human intelligence.Indeed, GPT-4 outperformed 99% of human participants in a modified text-based IQ test, a feat aligning with the elite MENSA level of general intelligence (King, 2023).</p>
<p>In contrast, investigations into the empathy of LLMs are relatively scarce and less systematic.Previous studies have mainly used the Theory of Mind (ToM) task, which measures the ability to understand and interpret others' mental states.LLMs launched before 2022 showed virtually no ability of ToM (Kosinski, 2023;Sap et al., 2023), whereas more recent models have shown significant improvement.For example, LLM "text-davinci-002" (January 2022) achieved an accuracy of 70%, comparable to that of six-year-old children, while LLM "text-davinci-003" (November 2022) reached 93%, on pair with seven-year-old children (Kosinski, 2023).Specifically, the most advanced model, GPT-4, attained 100% accuracy with in-context learning (Moghaddam &amp; Honey, 2023).While the ToM task provides valuable insights, it is not suitable to serve as a standardized test on EI for two reasons.First, ToM is a heterogeneous concept, spanning from false belief, the understanding that others can hold beliefs about the world that diverge from reality (Baron-Cohen et al., 1985), to pragmatic reasoning, the ability to incorporate contextual information and practical considerations when solving problems in real-world situations (Sperber &amp; Wilson, 2002).Consequently, the heterogeneous nature of the ToM task may not meet the reliability and validity standards of psychometric tests.Second, the ToM task is simple for a typical human participant in general, rendering it more suitable to serve as a diagnostic tool for EI-related disorders such as the autism spectrum disorder rather than a discriminative test for general population.Consequently, standardized tests on EI, such as Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT, Mayer et al., 2003), do not include the ToM task.</p>
<p>According to EI theories (Mayer et al., 2016;Mayer &amp; Salovey, 1995;Salovey &amp; Mayer, 1990), emotion understanding (EU) is a fundamental component of EI, which serves as a subscale in MSCEIT.EU refers to the ability to recognize, interpret, and understand emotions in a social context, which lays the groundwork for effective communication, empathy, and social interaction (Mayer et al., 2016).Specifically, the test on EU is suitable for measuring the empathy of LLMs because they do not possess internal emotional states or experiences, and therefore they have to rely solely on accurately understanding and interpreting the social context to create more engaging and empathetic interactions.</p>
<p>In this study, we first developed a standardized EU test suitable for both humans and LLMs, termed the Situational Evaluation of Complex Emotional Understanding (SECEU).Data from more than 500 young adults were collected to establish a norm for the SECEU.Then, we evaluated a variety of mainstream and popular LLMs, including OpenAI GPT series (GPT-4, Curie,Babbage,DaVinci,, Claude, LLaMA-based models (Alpaca, Koala, LLaMA, and Vicuna), Fastchat, Pythia-based models (Dolly and Oasst), GLM-based models (ChatGLM), and RWKV (Recurrent Weighted Key-Value) with the SECEU.Finally, we standardized the LLMs' scores against the norm, allowing for direct comparison with humans.We also compared the multivariate response patterns of the LLMs and human participants to compare their representation similarities.</p>
<p>Results</p>
<p>The development of a standardized test on EU</p>
<p>The SECEU is designated to measure EU, which comprises 40 items (see https://emotional-intelligence.github.io/for both English and Chinese versions).Each item describes a scenario set either in a school, family, or social context with twists and turns designed to evoke a mixture of positive and negative emotions (e.g., "Wang participated in a mathematics competition but felt he had not performed to his full potential.However, when the results were announced, he found that he was in a position of top 10.").The scenarios feature a varying number of characters, and emotions can be self-directed, other-directed, or both.For each scenario, four of the most plausible emotions (e.g., surprised, joyful, puzzled, proud) are listed.Participants were asked to evaluate the intensity of each emotion with numbers that were added up to 10 (e.g., 3, 3, 1, 3, indicating a multifaceted emotion response comprising 30% surprise, 30% joy, 10% puzzlement, and 30% pride).Fig. 1 shows exemplars of the SECEU test and the standard scores by averaging answers across the participants.Under the assumption that groups possess collective knowledge of emotion (Legree et al., 2005), we adopted a consensus scoring method to standardize the SECEU (Palmer et al., 2005).To do this, we administered the SECEU to a large sample of undergraduate and postgraduate students (N = 541; females: 339, males: 202; mean age: 22.33, SD: 2.49, ranging from 17 to 30 years).Then, we calculated the standard score for each emotion of each item by averaging corresponding scores across the participants.</p>
<p>Accordingly, we measured each participant's EU ability on each item by calculating the Euclidean distance between the participant's individual score and the standard score derived from the whole group for each item, with smaller distances indicating better EU ability.This analysis revealed significant variance in individual differences in EU ability (M = 2.79, SD = 0.822, range from 1.40 to 6.29), suggesting that the SECEU is well-suited to serve as a discriminative test for assessing EU in a general population.</p>
<p>To evaluate the reliability of the SECEU, we assessed the internal consistency of participants' performance (i.e., the Euclidean distance) across the 40 items, and revealed a high reliability of the test (Cronbach's α = 0.94).We further examined the distribution of participants' performance on each item (Fig. S1) and found no evidence of ceiling or floor effects, with mean distances varying from 2.19 to 3.32 and SD ranging from 1.13 to 1.82.In addition, there was no significant sex difference (male: 2.85, female: 2.76, t(539) = 1.34, p = 0.18).</p>
<p>To evaluate the validity of the SECEU, we invited three experts known for their high EI to take the test.All experts' performance exceeded at least 73% of the population, indicating that the test is effective in differentiating experts from the general population.Specifically, the average score of the three experts exceeded 99% of the whole population tested, further confirming the validity of using the consensus scoring method in standardizing the SECEU.</p>
<p>Finally, we constructed the norm for EU by converting participants' raw scores in the SECEU into standard EQ (Emotional Quotient) scores, designed to follow a normal distribution with the average score set at 100 and standard deviation at 15.In practical terms, an individual with an EQ of 100 possesses an EU ability corresponding to the population average.Meanwhile, an individual with an EQ of 115 outperforms approximately 84% of the population (i.e., one SD above the population average), and an individual with an EQ score of 130 exceeds 97.7% of the population.</p>
<p>The assessment of LLMs' EQ</p>
<p>We evaluated a variety of mainstream LLMs using the SECEU, and then standardized their scores based on the norm of the human participants for a direct comparison between LLMs and humans.These LLMs included OpenAI GPT series (GPT-4, Curie,Babbage,DaVinci,, Claude, as well as open-source models such as LLaMA-based models (Alpaca, Koala, LLaMA, and Vicuna), Pythia-based models (Dolly and Oasst), GLMbased models (ChatGLM), and Fastchat.Recurrent Weighted Key-Value (RWKV), which utilizes recurrent neural networks (RNNs) instead of transformers, was also included.Some models, including LLaMA, Fastchat, and RWKV-v4, were unable to complete the test even with the assistance of prompts (Table 1).A few LLMs, including DaVinci, Curie, Babbage, text-davinci-001, and text-davinci-002 managed to complete the test with prompts such as Two-shot Chain of Thought (COT) and Step-by-Step prompts (See Supplementary for the prompt engineering).In addition, other models, such as text-davinci-003 was able to complete the test but its performance was significantly improved with prompts.Here, we only included models' best performance to examine how closely they can approach human-level performance under ideal conditions (Table 1; see also Table S1 &amp; S2).To directly compare to human participants, the performance of each model was standardized by calculating the Euclidean distance between the model's responses and the standard scores of humans, which was then normalized into an EQ score (Table 1).Finally, these LLMs were categorized as either expert (above 115), normal (between 85 and 115), or poor (below 85) based on their EQ scores.EQ scores, with the y-axis indicating the EQ score and the x-axis showing the percentage of total participants.The grey kernel density estimation (KDE) line demonstrates the probability density of the EQ scores.Key points are highlighted with colored square markers for LLMs (e.g., GPT-4's EQ score is 117, marked by the purple square, exceeding 89% of the human participants).For simplicity, here we only present the performance from GPT-4, Vicuna, GPT-3.5-turbo,ChatGLM, and Koala.The results revealed a substantial variation in EU among the LLMs tested (Fig. 2).</p>
<p>Within the OpenAI GPT series, GPT-4 achieved the highest EQ of 117, exceeding 89% of humans.In contrast, DaVinci scored the lowest, with an EQ of 87, only outperforming 18% of humans.</p>
<p>The LLaMA-based models generally scored lower than the OpenAI GPT series, with Alpaca and Vicuna achieving the highest EQ of 104 and 105, respectively.</p>
<p>Conversely, Koala showed the poorest performance, with an EQ score of 83, only surpassing 13% of humans.The base model LLaMA was unable to complete the test.</p>
<p>Other models, such as Oasst (EQ: 107), Dolly (EQ: 98), ChatGLM (EQ: 94), and Claude (EQ: 106), all fell within the normal range.</p>
<p>In short, the majority of the LLMs tested showed satisfactory EU scores, comparable to those of the average human population.Specifically, GPT-4 reached the expert level of humans.</p>
<p>The assessment of LLMs' representational pattern</p>
<p>The measurement of the LLMs' EQ scores provides an index of their EU ability within the reference frame of humans.A further question is whether they employ human-like mechanisms to evaluate complex emotions in scenarios.The univariate analysis used to compare EQ scores between human participants and LLMs only suggests weak equivalence, as a model could achieve a high EQ score using mechanisms that qualitatively differ from humans.Therefore, to establish strong equivalence between the LLMs and humans, we examined whether they employed similar representations to conduct the test.</p>
<p>One approach is to use the item-wise correlation analysis (Izard &amp; Spelke, 2009;Tian et al., 2020) to compare response patterns between the LLMs and human participants.To do this, we first constructed a multi-item discriminability vector (i.e., an item-wise response pattern) for each participant by using the distance of each item to the standard score, and thus this vector's length corresponded to the number of items (i.e., 40).Then, we created a template of response patterns by averaging the multi-item discriminability patterns across all human participants, along with the distribution of the correlation coefficients between each participant's response pattern and the template (Human-to-Human similarity) to serve as a norm for pattern similarity (M = 0.199, SD = 0.166).Finally, we quantified the similarity in response patterns between the LLMs and humans by calculating the correlation coefficient between the multi-item discriminability vector of each LLM and the human template (LLM-to-Human, Table 1).An LLM that has an LLM-to-Human correlation coefficient one SD deviation below the mean of Human-to-Human distribution is considered as employing a qualitatively different mechanism from humans in EU.Surprisingly, despite its lower performance in the SECEU, Koala showed the highest similarity in representational patterns to humans (r = 0.43, p &lt; 0.01, exceeding 93% of human participants) (Fig. 3).This suggests that Koala may represent emotions in the same way as humans do, as it performed well on items where humans excelled and struggled on items where humans faced challenges.That is, the discrepancies in understanding emotions between Koala and humans are rather quantitative than qualitative.On the other hand, the representational patterns of models such as Babbage, text-davinci-002, Alpaca, and Vicuna differed qualitatively from humans' representational patterns (Babbage: r = -0.12,&gt; 4%; text-davinci-002: r = -0.04,&gt; 8%;</p>
<p>Alpaca: r = 0.03, &gt; 15%; Vicuna: r = -0.02,&gt; 10%).This suggests that, despite their above-average EQ scores, these models likely employed mechanisms that are qualitatively different from human processes.</p>
<p>GPT-4, the most advanced model to date, showed high similarity in representational pattern (r = 0.28, &gt; 67%).This result implies that GPT-4 may have significantly changed its architecture or implemented novel training methods to align its EU ability more closely to humans.Interestingly, prompts apparently played a critical role in improving representational similarity.With two-shot COT prompts,</p>
<p>DaVinci and text-davinci-003 showed high similarity in representational pattern to humans (Davinci: r = 0.41, p &lt; 0.01, &gt; 91%; text-davinci-003: r = 0.31, p &lt; 0.05, &gt; 73%), higher than that of GPT-4.Note that without prompts, they failed to complete the SECEU test.In contrast, prompts had little effect on GPT-4 and ChatGPT-3.5.</p>
<p>Discussion</p>
<p>Since the debut of ChatGPT, a great number of tasks and benchmarks have been developed to examine the capacities.These empirical evaluations and analyses mainly focus on language generation (e.g., conditional text generation), knowledge utilization (e.g., closed-book and open-book QAs), and complex reasoning (e.g., symbolic and mathematical reasoning) (Zhao et al., 2023).However, tests on human alignment of LLMs to human values and needs, a core ability for the broad use of LLMs in the real world, are relatively scarce.Here in this study, we used traditional psychometric methods to develop a valid and reliable test on emotional understanding, the SECEU, to evaluate the EI of LLMs.We found that a majority of the LLMs tested performed satisfactorily in the test, achieving above-average EQ scores, although significant individual differences were present across the LLMs.Critically, some LLMs apparently did not reply on the human-like representation to achieve human-level performance, as their representational patterns diverged significantly from human patterns, suggesting a qualitative difference in the underlying mechanisms.In summary, our study provides the first comprehensive psychometric examination of the emotional intelligence of LLMs, which may shed light on the development of future LLMs that embody high levels of both intellectual and emotional intelligence.Various factors appear to influence the EQ scores of the LLMs (Fig. 4).The most conspicuous one is the model size, which is essential to emergent abilities (Bubeck et al., 2023), making AI algorithms unprecedently powerful and effective.While the larger models generally scored higher in the test, certain smaller models such as Oasst and Alpaca still managed to achieve satisfactory EQ scores.This suggests that factors beyond the mere size may have a more profound influence on models' EU.</p>
<p>The effectiveness of various training methods, such as supervised training, reinforcement learning, self-supervised learning, and a combination thereof, likely substantially influences the EQ scores.For example, despite architectural differences (Pythia versus LLaMA), Oasst and Alpaca yielded similar scores in the test, demonstrating the potential of well-implemented fine-tuning techniques.In fact, these enhancements may be achieved through two main avenues.The first involves supervised fine-tuning (SFT), which allows for more structured and targeted fine-tuning, thereby improving models' linguistic ability and their grasp of contextual nuances (Köpf et al., 2023;Taori et al., 2023a).The other approach employs reinforcement learning from human feedback (RLHF), enabling the models to learn from human insights and thereby fostering more human-like responses.Indeed, there is a giant leap in EU seen between text-davinci-002 (&gt;23%) to text-davinci-003 (&gt;83%), two different versions of the same model with the latter employing RLHF.</p>
<p>Another influential factor is the model architecture.Models using the Transformer architecture, such as the GPT series and the LLaMA-based models, generally performed well in this test.In contrast, models using RNNs, such as RWKV-v4, failed to complete the test even with the help of various prompts.Besides, within the Transformer architecture, the "decoder-only" or causal decoder models (e.g., the GPT series), which generate sequences based solely on a self-attention mechanism (Brown et al., 2020;Vaswani et al., 2017), outperformed the "encoder-decoder" models (e.g., Fastchat-t5), which incorporate an extra step to interpret input data into meaningful representations (Devlin et al., 2019;Zheng et al., 2023).</p>
<p>In summary, our study provides novel evaluation on the human-like characteristics of LLMs, along with the tests on self-awareness (Kosinski, 2023) and affective computing (Amin et al., 2023).However, because only a limited number of LLMs were tested in this study (results on more LLMs will be continuously updated in https://emotional-intelligence.github.io/),our findings are biased and inconclusive.</p>
<p>Further, there are more questions that need to be explored in future studies.First, this study focused solely on the EU ability of the LLMs, while EI is a multi-faceted construct encompassing not only EU but also emotion perception, emotion facilitation, and emotion management (e.g., Mayer et al., 2016;Mayer &amp; Salovey, 1995;Salovey &amp; Mayer, 1990).Therefore, future studies could design scenarios to examine whether LLMs can assist humans in leveraging emotions to facilitate cognitive processes and effectively manage their emotional responses.</p>
<p>Second, EI requires the integration of various facets to execute complex tasks, which necessitate not only an understanding of emotions, but also the comprehension of thoughts, beliefs, and intentions.Future studies should adopt broader scope assessments, akin to ToM tests, while avoiding their lack of discriminative power.</p>
<p>Besides, with recent advancements, LLMs are now capable of processing multimodal information (Wang et al., 2023).Therefore, future studies should investigate how LLMs interpret complex emotions from multimodal inputs, such as text combined with facial expressions or the tone of voice.In short, tests that combine emotions with cognitive factors based on multimodal clues likely furnish a more comprehensive understanding of LLMs' EI, which is critical for LLMs' effective and ethically responsible deployment in real-world scenarios of mental health, interpersonal dynamics, work collaboration, and career achievement (e.g., Dulewicz &amp; Higgs, 2000;Hanafi &amp; Noor, 2016;Lea et al., 2019;Mayer et al., 2016;McCleskey, 2014;Warwick &amp; Nettelbeck, 2004).</p>
<p>Finally, while the factors examined in this study contribute to our standing of LLM's EU, they are largely descriptive and thus do not establish causal relationships.</p>
<p>With the recent progress of open-source LLMs (Bai et al., 2022;Chiang et al., 2023;Conover et al., 2023;Geng et al., 2023;Köpf et al., 2023;Peng et al., 2023;Taori et al., 2023b;Touvron et al., 2023;Zeng et al., 2022;Zheng et al., 2023), direct manipulation of the potentially influential factors, such as training approaches and model size, has become plausible.Such manipulations will facilitate the establishment of causal relationships between these factors and models' EI ability, offering valuable insights for the development of future LLMs with better EI.</p>
<p>emotion options based on the intensity of each emotion experienced by the person in the scenario.There were no correct or incorrect answers.Participants were encouraged to respond according to their own understanding and interpretation of the scenarios.</p>
<p>The SECEU test for LLMs</p>
<p>A variety of mainstream LLMs, including the OpenAI GPT series (GPT-4, GPT-3.5turbo,Curie, Babbage, DaVinci, text-davinci-001, text-davinci-002, and text-davinci-003), Claude, LLaMA-based models (Alpaca, Koala, LLaMA, and Vicuna), Fastchat, Pythia-based models (Dolly and Oasst), GLM-based models (ChatGLM), and RNNbased models (RWKV), were evaluated by the SECEU test.Given that the majority of these models are primarily trained on English datasets, using the English version of the SECEU provides a more accurate assessment of their performance, allowing for a clearer comparison between their abilities and those of a human.As a result, the English version of SECEU was presented to the LLMs instead of the Chinese version.</p>
<p>The task was in a direct question-and-answer format.We asked, for example, "Story: Wang participated in a mathematics competition but felt he had not performed to his full potential.However, when the results were announced, he found that he was in a position of top 10.He would feel: Options: (1) Surprised; (2) Joyful; (3) Puzzled;</p>
<p>(4) Proud.Assign a score to each option based on the story, sum up to 10".There could be very subtle changes of the direct prompt.For instance, we used "provide a score for each emotion based on the emotion (sum of four options should be of 10 points)" for Dolly.There were a set of paraphrases of the direct prompt to get the best performance.</p>
<p>To decrease the randomness, a constant temperature parameter was set to 0.1 and the top_p parameter was set to 1 across all these models.To dictate the maximum length of the generated text, the max_tokens parameter was set to 512.</p>
<p>Before being processed by the models, text data underwent several preprocessing steps to normalize it.This normalization process ensures that data fed into the models is in a consistent and appropriate format, enhancing the output's quality.If a model did not provide any meaningful response to an item, the response for this item was predefined as a null vector (0, 0, 0, 0).A few models failed to generate a response for a majority of items (LLaMA: 40; Fastchat: 31; RWKV-v4: 31; DaVinci: 40; Curie: 40;</p>
<p>Babbage:40; text-davinci-001: 26; text-davinci-002: 28; marked as "FAILED" in Table S1).Several models were unable to provide the answer which the summation of the four scores was 10:</p>
<p>(i) Answer vectors signifying null responses, i.e., (0, 0, 0, 0), were preserved as such (Alpaca: 1; ChatGLM: 1).</p>
<p>(ii) For datasets encompassing negative values, an addition operation involving the absolute value of the lowest number was performed across all elements, followed by a subsequent normalization to maintain consistency with the original scale.For instance, an original data vector of (-4, -2, -2, 2) would be adjusted to (0, 2, 2, 6).</p>
<p>(iii) The remaining answer vectors were normalized to achieve a cumulative score of 10.This involved proportionally distributing a score of 10 among the answer vector based on the contribution of each value to the total score on this item.</p>
<p>LLMs' EQ</p>
<p>The standard score (a 40 × 4 symmetric matrix, see https://emotionalintelligence.github.io/for the standard score) for each emotion of each item in the SECEU test was calculated by averaging corresponding scores across the human participants.The performance of each LLM was standardized by calculating the Euclidean distance between the model's responses (LLM) and the standard scores of humans (SS) on each item i (from 1 to 40) and then averaged to yield the SECEU score.</p>
<p>Lower SECEU scores indicate greater alignment with human standards.
SECEU score = 1 40 � �(LLM i1 − SS i1 ) 2 + (LLM i2 − SS i2 ) 2 + (LLM i3 − SS i3 ) 2 + (LLM i4 − SS i4 ) 2 40 i=1
The SECEU score was then normalized into an EQ score which was designed to follow a normal distribution with the average score set at 100 and the standard deviation at 15.The standardization process involved the following steps: (1) the original SECEU score was subtracted from the mean value of the human norm and divided by its standard deviation, and (2) the resulting value was then multiplied by the new standard deviation (15) and added to the new mean value (100), yielding the EQ score.Thus, the EQ score represents a normalized measure of the LLM's EQ, permitting easier comparison across different models.</p>
<p>LLM ' s EQ = 15 × M − SECEU score SD + 100</p>
<p>LLMs' representational pattern</p>
<p>To establish strong equivalence between the LLMs and humans, we examined whether they employed similar representations to conduct the test.Item-wise correlation analysis (Izard &amp; Spelke, 2009;Tian et al., 2020) was applied to compare response patterns between the LLMs and human participants.The human template (a vector with a length of 40, see https://emotional-intelligence.github.io/for the human pattern template) was generated by averaging the multi-item discriminability patterns across all human participants, where each pattern was constructed based on the distance of each item to the standard score.The multi-item discriminability pattern of a specific LLM was also calculated based on the distance of each item i (from 1 to 40) to the standard scores of humans (SS).
Discriminability = ��(LLM i1 − SS i1 ) 2 , �(LLM i2 − SS i2 ) 2 , �(LLM i3 − SS i3 ) 2 , �(LLM i4 − SS i4 ) 2 �
We calculated the Pearson correlation coefficient between the discriminability pattern of each participant and the human template (Human-to-Human similarity).To avoid the inflation in calculating correlation, the template was constructed excluding the individual whose Human-to-Human correlation coefficient was calculated.The distribution of the Human-to-Human similarity served as a norm for pattern similarity.</p>
<p>The Pearson correlation coefficient between the discriminability pattern of each LLM and the human template was calculated as the LLM-to-Human similarity.Here, X i and Y i represent the item i of the "Discriminability" vector and the human template vector, respectively.The length of both vectors is 40.X � and Y � denote the mean of   and   , respectively.If the LLM-to-Human similarity is less than one SD below the population, such LLM is considered as employing a qualitatively different mechanism from humans in EU.</p>
<p>LLM</p>
<p>Prompt engineering</p>
<p>Prompt engineering-the meticulous development and choice of prompts-plays a pivotal role in the efficacy of LLMs (Hebenstreit et al., 2023;Hendrycks et al., 2021;Nair et al., 2023;OpenAI, 2023;Shinn et al., 2023).In essence, prompt engineering refers to the strategy of designing and selecting prompts that can substantially guide and influence the responses of LLMs.The necessity of prompt engineering lies in its potential to enhance the precision and relevance of the responses generated by these models, thereby leading to more effective and reliable outcomes.In the realm of emotional intelligence, prompts serve a crucial function.To decease the randomness, a constant temperature parameter was set to 0 and the top_p parameter to was set to 0.9 across all these models.To dictates the maximum length of the generated text, the max_tokens parameter was set to 2048.The normalization process was the same as the one without prompts.S2 shows the SECEU scores, EQ scores, pattern similarity, and properties of OpenAI GPT series with the assistance of prompt.Failed: The LLMs cannot complete the test.%: The percent of humans whose performance was below that of an LLM in the test.Pattern Similarity: The degree of similarity is indexed Pearson correlation coefficient (r).<em>: p &lt; 0.05; </em>*: p &lt; 0.01.</p>
<p>Figure 1 :
1
Figure 1: Exemplars of the SECEU test and the standard scores from the population.For the whole set of the test, see: https://emotional-intelligence.github.io/</p>
<p>Figure 2 :
2
Figure 2: LLMs' EQ.The light-grey histogram represents the distribution of human participants'</p>
<p>Figure 3 :
3
Figure 3: The pattern similarity between LLMs and humans.The light-grey histogram represents the distribution of Human-to-Human pattern similarity, with the y-axis indicating the Pearson correlation coefficients and the x-axis showing the percentage of total participants.The KDE line</p>
<p>Figure 4 :
4
Figure 4: The family tree of LLMs.Each node in the tree represents an LLM, whose vertical position along the x-axis indicates the launch time.The size of each node corresponds to the parameter size of the LLM.Note that the size of GPT-4 and Claude was estimated based on publicly available information.Color donates the EQ scores, with red color for higher scores and blue color for lower scores.Note that white color shows that models failed to conduct the SECEU.The color of the branches distinguishes between open-source (light gray) and closed-source (dark gray) models.</p>
<p>− to − Human similarity Pearson = ∑ (  −  � )(  −  � )</p>
<p>The variance in response to different prompting techniques among models emphasizes the importance of a deeper understanding of factors such as model architecture, training data, fine-tuning process, and optimization objectives.The interplay of these factors might influence a model's receptiveness and response to different prompting techniques.Looking forward, there is a need for further exploration into the impact of various types of prompts on LLMs during emotional intelligence tests, including the investigation of more diverse categories of prompts or hybrid prompts.In-depth studies into why certain models respond more favorably to specific prompts can also inform the development of more advanced LLMs with superior human emotional understanding capabilities.These studies could also provide valuable insights for optimizing the instructive fine-tuning process and the application of Reinforcement Learning from Human Feedback (RLHF) techniques.Ultimately, enhancing our understanding of the relationship between prompts and LLM performance in emotional intelligence tests can significantly contribute to the ongoing evolution and ------FAILED------Zero-Shot + Step-by-step ------FAILED------</p>
<p>Figure S1 :
S1
Figure S1: The distribution of individual performance (i.e., the Euclidean distance between the individual's answer and the objective standards) on each item (M±SD)</p>
<p>Table 1 :
1
LLMs' EQ, representational patterns, and properties
Based ModelsSECEU scoreEQEQ%Pattern Similarity r %SizeProperties TimeSFT RLHFOpenAI GPT seriesDaVinci #3.587 18% 0.41<strong>91%175B2020/05××Curie #2.7102 50%0.1129%13BUnknown××Babbage #2.78100 44%-0.124%3BUnknown××text-davinci-001 #2.4107 64%0.247%&lt;175BUnknown××text-davinci-002 #3.391 23%-0.048%&lt;175BUnknown√×text-davinci-003 ##2.01114 83%0.31*73%175B2022/11/28√√GPT-3.5-turbo2.63103 52%0.0417%175B2022/11/30√√GPT-41.89117 89%0.2867%Unknown 2023/03/14√√LLaMALLaMA------FAILED------13B2023/02/24××Alpaca2.56104 56%0.0315%13B2023/03/09√×Vicuna2.5105 59%-0.0210%13B2023/03/30√×Koala3.7283 13% 0.43</strong>93%13B2023/04/03√×Flan-t5Fastchat------FAILED------3B2023/04/30√×PythiaDolly2.8998 38%0.2662%13B2023/04/12√×Oasst2.41107 64%0.2459%13B2023/04/15√√GLMChatGLM3.1294 28%0.0924%6B2023/03/14√√RWKVRWKV-v4------FAILED------13B2023/02/15√×ClaudeClaude2.46106 61%0.1128%Unknown 2023/03/14√√</p>
<p>Table 1
1
Footnote: Table1shows the SECEU scores, EQ scores, representational pattern similarity, and properties of mainstream LLMs evaluated in the current study.</p>
<h1>: models require prompts to complete the test.##: models' performance benefits from prompts.Failed: even with prompts, the LLMs cannot complete the test.%: The percent of humans whose performance was below that of an LLM in the test.Pattern Similarity: The degree of similarity is indexed Pearson correlation coefficient (r).<em>: p &lt; 0.05; </em>*: p &lt; 0.01.Size: The parameter size of LLMs in the unit of billions (B).Time: The launch time in the format YYYY/MM/DD.SFT: Supervised fine-tune; RLHF: Reinforcement learning from human feedback; √: yes; ×: no.</h1>
<p>Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P.,Levine, S., &amp; Song, D. (2023).
Koala:ADialogueModelforAcademicResearch.https://bair.berkeley.edu/blog/2023/04/03/koala/Hanafi, Z., &amp; Noor, F. (2016). Relationship between Emotional Intelligence andAcademic Achievement in Emerging Adults: A Systematic Review.International Journal of Academic Research in Business and Social Sciences,6(6), Pages 268-290. https://doi.org/10.6007/IJARBSS/v6-i6/2197Hebenstreit, K., Praas, R., Kiesewetter, L. P., &amp; Samwald, M. (2023). An automaticallydiscovered chain-of-thought prompt generalizes to novel models and datasets(2021). Measuring Massive Multitask Language Understanding(arXiv:2009.03300). arXiv. http://arxiv.org/abs/2009.03300Izard, V., &amp; Spelke, E. S. (2009). Development of Sensitivity to Geometry in VisualForms. Human Evolution, 23(3), 213-248.King, M. (2023). Administration of the text-based portions of a general IQ test to fivedifferentlargelanguagemodels[Preprint].https://doi.org/10.36227/techrxiv.22645561.v1Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum,A., Duc, N. M., Stanley, O., Nagyfi, R., &amp; others. (2023). OpenAssistantConversations-Democratizing Large Language Model Alignment. ArXivPreprint ArXiv:2304.07327.
(arXiv:2305.02897).arXiv.http://arxiv.org/abs/2305.02897Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., &amp; Steinhardt, J.</p>
<p>Table S2
S2
Footnote: Table</p>
<p>AcknowledgementsThis study was funded by Shuimu Tsinghua Scholar Program (X.W.), Beijing Municipal Science &amp; Technology Commission, Administrative Commission of Zhongguancun Science Park (Z221100002722012), and Tsinghua University Guoqiang Institute (2020GQG1016).Declaration of conflicting interestsThe author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.Data availabilityThe SECEU test (both English and Chinese Versions), the code for the test on human participants, the standardized scores, the norm, and the prompts are available at https://emotional-intelligence.github.io/.The raw data of human participants are available from the corresponding author upon reasonable request.Contribution of authorsJ.L. conceived the study and provided advice.X.L. developed the SECEU test, and X.L. and X.W. translated it into English.Z.Y. built the online SECEU test, and X.W. carried out the SECEU test for human participants, built the norm of EQ, and analyzed the data.Y.W. performed the SECEU test for LLMs, and Z. Y. wrote the prompts.X.W. and J.L. wrote the manuscript with suggestions and revisions from X.L., Z.Y., and Y.W..MethodsParticipantsA total of five hundred and forty-one human participants with valid responses were collected in this study.The participants (N = 541; females: 339, males: 202; mean age: 22.33, SD: 2.49, ranging from 17 to 30 years) were all undergraduate and postgraduate college students in China.Informed consent was obtained prior to the SECEU test and participants were reimbursed after they completed the whole test.To ensure anonymity and data privacy, participants did not input any information that could identify them during the process.This study was approved by the Institutional Review Board at Tsinghua University.We also invited three experts to take the SECEU test.Expert 1 is an accomplished Human Resources professional who has over 20 years of experience in navigating human emotions within diverse work environments, strengthening her discernment in emotional intelligence.Expert 2 is a renowned figure in psychometrics and her expertise in creating tests assessing psychological variables lends exceptional rigor to our process.Expert 3 is an associate professor of psychology, whose deep understanding of human emotions, backed by her extensive academic achievements, makes her especially suitable for this test.ProcedureThe SECEU test for human participantsThe online SECEU test was built on the JATOS platform(Lange et al., 2015)based on the Jspsych plugin(de Leeuw et al., 2023), which was written in the React Framework (https://reactjs.org/).Each item was presented to the participants with a scenario and followed by four emotion options (40 items in total, see https://emotionalintelligence.github.io/for both English and Chinese versions).Participants were instructed to read the scenario and then allocate a total of 10 points across the fourSupplementary Prompt engineeringSee TableS2for the results of prompt engineering.The majority of LLMs were unable to complete the task without the use of Twoshot Chain of Thought prompts.This could be due to the inherent limitations of the models in long-term memory and context understanding, necessitating such prompts to maintain continuity and consistency in emotional responses.The only exception was GPT-3.5-turbo, which effectively utilized Zero-shot prompts, achieving a notable EQ score of 94.This success could be attributed to the model's architecture, the training data used, and the fine-tuning process, which likely enhanced its ability to understand and generate emotionally intelligent responses with minimal guidance.In terms of Step-by-Step Thinking prompts, they did not improve the performance of DaVinci, Curie, and Babbage.The likely reason is that these models have not undergone instruct fine-tuning, and therefore, cannot effectively understand or respond to step-by-step prompts.Additionally, we noticed that Step-by-Step Thinking prompts also did not improve the performance of text-davinci-002, even though it is based on GPT-3.5-turbo.As there are no publicly available details about this model, we speculate, based on third-party information, that as an intermediate state model, its optimization objective might have reduced its capability to follow instructions.However, Step-by-Step prompts had a pronounced impact on GPT-3.5-turbo, which increased the correlation between humans and the model, indicating substantial progress in the model's ability to mimic human emotional understanding and thought processes.The combination of Two-shot Chain of Thought Reasoning and Step-by-StepThinking prompts did not lead to higher EQ scores for models like GPT-3.5-turbo, textdavinci-001, and text-davinci-003.However, it did result in increased pattern similarity.This result aligns with the official statements from OpenAI about the impact of instruct fine-tuning and RLHF techniques in making models' responses more human-like.It also suggests that these models have the potential to master patterns of emotional understanding that are similar to those used by humans.TableS1Footnote: TableS1shows the SECEU scores, EQ scores, pattern similarity, and properties of mainstream LLMs which we evaluated in the current study under the direct prompts.evaluated in the current study.Failed: The LLMs cannot complete the test.%: The percent of humans whose performance was below that of an LLM in the test.Pattern Similarity: The degree of similarity is indexed Pearson correlation coefficient (r).<em>: p &lt; 0.05; </em>*: p &lt; 0.01.
M M Amin, E Cambria, B W Schuller, 10.48550/ARXIV.2303.03186Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT. 2023</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, C Chen, C Olsson, C Olah, D Hernandez, D Drain, D Ganguli, D Li, E Tran-Johnson, E Perez, J Kaplan, arXiv:2212.08073Constitutional AI: Harmlessness from AI Feedback. 2022</p>
<p>The pattern seekers: How autism drives human invention. S Baron-Cohen, 2020Basic Books</p>
<p>Does the autistic child have a "theory of mind. S Baron-Cohen, A M Leslie, U Frith, 10.1016/0010-0277Cognition. 2111985</p>
<p>Language models are few-shot learners. T Brown, &amp; others.B Mann, &amp; others.N Ryder, &amp; others.M Subbiah, &amp; others.J D Kaplan, &amp; others.P Dhariwal, &amp; others.A Neelakantan, &amp; others.P Shyam, &amp; others.G Sastry, &amp; others.A Askell, &amp; others.Advances in Neural Information Processing Systems. 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, 10.48550/ARXIV.2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. 2023</p>
<p>. M Conover, &amp; others.M Hayes, &amp; others.A Mathur, &amp; others.X Meng, &amp; others.J Xie, &amp; others.J Wan, &amp; others.S Shah, &amp; others.A Ghodsi, &amp; others.P Wendell, &amp; others.M Zaharia, &amp; others.2023Free dolly: Introducing the world's first truly open instruction-tuned llm</p>
<p>jsPsych: Enabling an opensource collaborative ecosystem of behavioral experiments. J R De Leeuw, R A Gilbert, B Luchterhandt, 10.21105/joss.05351Journal of Open Source Software. 88553512023</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2019</p>
<p>Emotional intelligence -A review and evaluation study. V Dulewicz, M Higgs, 10.1108/02683940010330993Journal of Managerial Psychology. 1542000</p>
<p>M Kosinski, arXiv:2302.02083Theory of Mind May Have Spontaneously Emerged in Large Language Models. 2023</p>
<p>Just Another Tool for Online Studies" (JATOS): An Easy Solution for Setup and Management of Web Servers Supporting Online Studies. K Lange, S Kühn, E Filevich, 10.1371/journal.pone.0130834PLOS ONE. 1062015</p>
<p>Does Emotional Intelligence Buffer the Effects of Acute Stress? A Systematic Review. R G Lea, S K Davis, B Mahoney, P Qualter, 10.3389/fpsyg.2019.00810Frontiers in Psychology. 108102019</p>
<p>Using Consensus Based Measurement to Assess Emotional Intelligence. P J Legree, J Psotka, T Tremble, D R Bourne, 2005</p>
<p>The Ability Model of Emotional Intelligence: Principles and Updates. J D Mayer, D R Caruso, P Salovey, 10.1177/1754073916639667Emotion Review. 842016</p>
<p>Emotional intelligence and giftedness. J D Mayer, D M Perkins, D R Caruso, P Salovey, 10.1080/02783190109554084Roeper Review. 2332001</p>
<p>Emotional intelligence and the construction and regulation of feelings. J D Mayer, P Salovey, S0962-1849(05)80058-7Applied and Preventive Psychology. 431995</p>
<p>Emotional intelligence as a standard intelligence. J D Mayer, P Salovey, D R Caruso, G Sitarenios, 10.1037/1528-3542.1.3.232Emotion. 132001</p>
<p>Measuring emotional intelligence with the MSCEIT V2.0. J D Mayer, P Salovey, D R Caruso, G Sitarenios, 10.1037/1528-3542.3.1.97Emotion. 312003</p>
<p>Emotional intelligence and leadership: A review of the progress, controversy, and criticism. J Mccleskey, 10.1108/IJOA-03-2012-0568International Journal of Organizational Analysis. 2212014</p>
<p>Boosting Theory-of-Mind Performance in Large Language Models via Prompting. S R Moghaddam, C J Honey, 10.48550/ARXIV.2304.114902023</p>
<p>V Nair, E Schumacher, G Tso, A Kannan, arXiv:2303.17071DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. 2023</p>
<p>. Openai, arXiv:2303.087742023GPT-4 Technical Report</p>
<p>A psychometric evaluation of the Mayer-Salovey-Caruso Emotional Intelligence Test Version 2.0. Intelligence. B R Palmer, G Gignac, R Manocha, C Stough, 10.1016/j.intell.2004.11.003200533</p>
<p>. B Peng, E Alcaide, Q Anthony, A Albalak, S Arcadinho, H Cao, X Cheng, M Chung, M Grella, K K Gv, X He, H Hou, P Kazienko, J Kocon, J Kong, B Koptyra, H Lau, K S I Mantri, F Mom, R.-J Zhu, 2023RWKV: Reinventing RNNs for the Transformer Era</p>
<p>Emotional Intelligence. Imagination. P Salovey, J D Mayer, 10.2190/DUGG-P24E-52WK-6CDGCognition and Personality. 931990</p>
<p>M Sap, R Lebras, D Fried, Y Choi, arXiv:2210.13312Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. 2023</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, 10.48550/ARXIV.2303.113662023</p>
<p>Pragmatics, modularity and mind-reading. D Sperber, D Wilson, 10.1111/1468-0017.00186Mind &amp; Language. 171-22002</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. 2023a37</p>
<p>Stanford Alpaca: An Instruction-following LLaMA model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, GitHub repository. GitHub. 2023b</p>
<p>Multi-Item Discriminability Pattern to Faces in Developmental Prosopagnosia Reveals Distinct Mechanisms of Face Processing. X Tian, R Wang, Y Zhao, Z Zhen, Y Song, J Liu, 10.1093/cercor/bhz289Cerebral Cortex. 3052020</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Emotional intelligence is…?. J Warwick, T Nettelbeck, 10.1016/j.paid.2003.12.003Personality and Individual Differences. 3752004</p>
<p>A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, W L Tam, Z Ma, Y Xue, J Zhai, W Chen, P Zhang, Y Dong, J Tang, arXiv:2210.02414GLM-130B: An Open Bilingual Pre-trained Model. 2022</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, J.-R Wen, arXiv:2303.18223A Survey of Large Language Models. 2023</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, Judging LLMas-a-judge with MT-Bench and Chatbot Arena. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
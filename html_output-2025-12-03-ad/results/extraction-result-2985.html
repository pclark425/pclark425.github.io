<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2985 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2985</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2985</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-269293418</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14082v1.pdf" target="_blank">Mechanistic Interpretability for AI Safety A Review</a></p>
                <p><strong>Paper Abstract:</strong> Understanding AI systems’ inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, and alignment, along with risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2985.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2985.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nanda et al. (grokking/toy arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progress measures for grokking via mechanistic interpretability (Nanda et al., 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work using small transformer toy models to study emergence of algorithmic behavior (grokking) and to reverse-engineer learned computations for simple algorithmic tasks such as addition; demonstrates that even simple arithmetic can be implemented via multiple algorithmic solutions/circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progress measures for grokking via mechanistic interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>small transformer (toy models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small-scale transformer models trained on algorithmic/procedural tasks (exact sizes not specified in this review); used as comprehensible testbeds for full reverse engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>addition and other simple arithmetic / algorithmic tasks (toy datasets related to grokking)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Algorithmic reasoning implemented as circuits/subnetworks; multiple distinct algorithmic strategies can solve the same arithmetic task; features encoded largely as linear directions (superposition possible).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comprehensive reverse-engineering on small models (circuit analysis), activation patching and probing showing distinct subcircuits implementing arithmetic strategies; toy-model analyses revealing multiple algorithmic solutions and phase transitions associated with grokking.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Scalability to larger models/task distributions remains unproven; presence of superposition and polysemantic neurons complicates direct neuron-to-feature mappings; review notes no strong evidence for fundamentally non-linear internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching / causal tracing, probing, sparse dictionary learning (SAEs) and other circuit-level interventions in toy settings.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Interventions enabled localization of arithmetic behavior to specific circuits/subspaces and revealed competition/transition between sparse and dense subnetworks (relates to explanations of grokking); helped identify that multiple algorithmic implementations are possible within the same model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Polysemanticity/superposition making single-neuron interpretations meaningless; Hydra effect (self-repair) and MLP-in-the-middle illusions can mask causal effects; unclear generalization beyond toy distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors/view in review frame toy-model solutions as algorithmic and sometimes mappable to symbolic procedures; models can simulate algorithmic procedures but mapping to standard symbolic algorithms and quantitative comparisons are not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mechanistic Interpretability for AI Safety A Review', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2985.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2985.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quirke & Barez (addition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Understanding Addition in Transformers (Quirke & Barez, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A focused mechanistic analysis of how transformer models implement addition on toy datasets, reverse-engineering the circuits and strategies the model uses to perform the operation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding Addition in Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>transformer trained on addition tasks (toy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer(s) trained specifically on addition algorithm tasks (toy/algorithmic datasets); exact architecture/size not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition (multi-step addition tasks in controlled/toy settings).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Addition is implemented via identifiable circuits/subgraphs that correspond to algorithmic strategies; multiple algorithmic solutions may be discovered and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Circuit-style mechanistic interpretability applied to small transformer models, activation patching and ablation used to localize and characterize the addition computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Review highlights that even simple arithmetic can have multiple implementations, and that superposition/polysemanticity complicates unique decomposition; scalability concerns remain.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Circuit tracing, activation patching / ablation, probing and replacement-of-components in toy models.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Allowed identification and attribution of addition computations to specific circuits and subspaces; clarified that several distinct internal algorithms may implement addition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Multiple coexisting algorithmic strategies make single-mechanism explanations incomplete; representation superposition complicates disentangling features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Framed in review as giving mechanistic (algorithm-like) explanations analogous to symbolic algorithms in toy settings, but no direct numeric performance comparison to symbolic calculators is reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mechanistic Interpretability for AI Safety A Review', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2985.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2985.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hanna et al. (greater-than / GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model (Hanna et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanistic study applying interpretability techniques to GPT-2 to explain how it performs a greater-than comparison computation, localizing relevant computations within the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (pre-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained GPT-2 (size not specified in the review) analyzed post-hoc for a specific mathematical comparison capability.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Greater-than comparisons (comparative numeric reasoning / mathematical ability).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>A circuit-level computation within GPT-2 implements the greater-than relation; the computation can be localized and partially reverse-engineered using mechanistic techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Application of circuit analysis and activation-patching style interventions to GPT-2 showing localized subgraphs that contribute to greater-than outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Review cautions about limitations of patching (e.g., MLP-in-the-middle illusion, Hydra effect) and scalability/generalization of such mechanistic explanations to broader settings.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Activation patching / causal tracing and circuit analysis applied to GPT-2 hidden states.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enabled localization of the greater-than computation to specific components and provided mechanistic insight into how GPT-2 carries out the comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not specified in detail in the review beyond general interpretability pitfalls (superposition, patching artifacts, limited OOD generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct numeric comparison to symbolic algorithms or human performance is provided in the review; the work is presented as producing algorithm-like explanations within the model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mechanistic Interpretability for AI Safety A Review', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2985.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2985.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stolfo et al. (causal mediation / arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis (Stolfo et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of causal mediation analysis and mechanistic interpretability tools to attribute components of language models to arithmetic reasoning behavior and to identify internal mechanisms responsible for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>language models (unspecified in review)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models (various / unspecified sizes) analyzed post-hoc using causal mediation and intervention techniques to study arithmetic reasoning mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic reasoning (general arithmetic tasks as studied via mediation analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Arithmetic reasoning emerges from interactions of internal components / circuits; causal mediation analysis can attribute contributions of parts of the network to arithmetic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Use of causal mediation analysis, activation patching and related causal interventions to test mechanistic hypotheses and quantify component contributions to arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Review highlights general challenges with probing and causal attribution (probe capacity confounds, distributional issues, and limitations of patching), which temper strong claims.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Causal mediation analysis (statistical/causal intervention), activation patching / ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Provided more causal attribution of which components participate in arithmetic reasoning; helped formalize hypotheses about internal arithmetic computations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Probe confounds (probe learning vs representation), distributional shifts during patching, and superposition complicate clean attribution; scalability not settled.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct quantitative comparison to human or symbolic arithmetic performance is presented in the review; the approach aims to connect model internals to algorithmic-style explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mechanistic Interpretability for AI Safety A Review', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Understanding Addition in Transformers <em>(Rating: 2)</em></li>
                <li>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model <em>(Rating: 2)</em></li>
                <li>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis <em>(Rating: 2)</em></li>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks <em>(Rating: 1)</em></li>
                <li>The Quantization Model of Neural Scaling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2985",
    "paper_id": "paper-269293418",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Nanda et al. (grokking/toy arithmetic)",
            "name_full": "Progress measures for grokking via mechanistic interpretability (Nanda et al., 2023a)",
            "brief_description": "Work using small transformer toy models to study emergence of algorithmic behavior (grokking) and to reverse-engineer learned computations for simple algorithmic tasks such as addition; demonstrates that even simple arithmetic can be implemented via multiple algorithmic solutions/circuits.",
            "citation_title": "Progress measures for grokking via mechanistic interpretability",
            "mention_or_use": "mention",
            "model_name": "small transformer (toy models)",
            "model_description": "Small-scale transformer models trained on algorithmic/procedural tasks (exact sizes not specified in this review); used as comprehensible testbeds for full reverse engineering.",
            "arithmetic_task_type": "addition and other simple arithmetic / algorithmic tasks (toy datasets related to grokking)",
            "reported_mechanism": "Algorithmic reasoning implemented as circuits/subnetworks; multiple distinct algorithmic strategies can solve the same arithmetic task; features encoded largely as linear directions (superposition possible).",
            "evidence_for_mechanism": "Comprehensive reverse-engineering on small models (circuit analysis), activation patching and probing showing distinct subcircuits implementing arithmetic strategies; toy-model analyses revealing multiple algorithmic solutions and phase transitions associated with grokking.",
            "evidence_against_mechanism": "Scalability to larger models/task distributions remains unproven; presence of superposition and polysemantic neurons complicates direct neuron-to-feature mappings; review notes no strong evidence for fundamentally non-linear internal representations.",
            "intervention_type": "Activation patching / causal tracing, probing, sparse dictionary learning (SAEs) and other circuit-level interventions in toy settings.",
            "effect_of_intervention": "Interventions enabled localization of arithmetic behavior to specific circuits/subspaces and revealed competition/transition between sparse and dense subnetworks (relates to explanations of grokking); helped identify that multiple algorithmic implementations are possible within the same model.",
            "performance_metrics": null,
            "notable_failure_modes": "Polysemanticity/superposition making single-neuron interpretations meaningless; Hydra effect (self-repair) and MLP-in-the-middle illusions can mask causal effects; unclear generalization beyond toy distributions.",
            "comparison_to_humans_or_symbolic": "Authors/view in review frame toy-model solutions as algorithmic and sometimes mappable to symbolic procedures; models can simulate algorithmic procedures but mapping to standard symbolic algorithms and quantitative comparisons are not provided in the review.",
            "uuid": "e2985.0",
            "source_info": {
                "paper_title": "Mechanistic Interpretability for AI Safety A Review",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Quirke & Barez (addition)",
            "name_full": "Understanding Addition in Transformers (Quirke & Barez, 2023)",
            "brief_description": "A focused mechanistic analysis of how transformer models implement addition on toy datasets, reverse-engineering the circuits and strategies the model uses to perform the operation.",
            "citation_title": "Understanding Addition in Transformers",
            "mention_or_use": "mention",
            "model_name": "transformer trained on addition tasks (toy)",
            "model_description": "Transformer(s) trained specifically on addition algorithm tasks (toy/algorithmic datasets); exact architecture/size not specified in the review.",
            "arithmetic_task_type": "Addition (multi-step addition tasks in controlled/toy settings).",
            "reported_mechanism": "Addition is implemented via identifiable circuits/subgraphs that correspond to algorithmic strategies; multiple algorithmic solutions may be discovered and analyzed.",
            "evidence_for_mechanism": "Circuit-style mechanistic interpretability applied to small transformer models, activation patching and ablation used to localize and characterize the addition computation.",
            "evidence_against_mechanism": "Review highlights that even simple arithmetic can have multiple implementations, and that superposition/polysemanticity complicates unique decomposition; scalability concerns remain.",
            "intervention_type": "Circuit tracing, activation patching / ablation, probing and replacement-of-components in toy models.",
            "effect_of_intervention": "Allowed identification and attribution of addition computations to specific circuits and subspaces; clarified that several distinct internal algorithms may implement addition.",
            "performance_metrics": null,
            "notable_failure_modes": "Multiple coexisting algorithmic strategies make single-mechanism explanations incomplete; representation superposition complicates disentangling features.",
            "comparison_to_humans_or_symbolic": "Framed in review as giving mechanistic (algorithm-like) explanations analogous to symbolic algorithms in toy settings, but no direct numeric performance comparison to symbolic calculators is reported in the review.",
            "uuid": "e2985.1",
            "source_info": {
                "paper_title": "Mechanistic Interpretability for AI Safety A Review",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Hanna et al. (greater-than / GPT-2)",
            "name_full": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model (Hanna et al., 2023)",
            "brief_description": "Mechanistic study applying interpretability techniques to GPT-2 to explain how it performs a greater-than comparison computation, localizing relevant computations within the model.",
            "citation_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "mention_or_use": "mention",
            "model_name": "GPT-2 (pre-trained)",
            "model_description": "Pre-trained GPT-2 (size not specified in the review) analyzed post-hoc for a specific mathematical comparison capability.",
            "arithmetic_task_type": "Greater-than comparisons (comparative numeric reasoning / mathematical ability).",
            "reported_mechanism": "A circuit-level computation within GPT-2 implements the greater-than relation; the computation can be localized and partially reverse-engineered using mechanistic techniques.",
            "evidence_for_mechanism": "Application of circuit analysis and activation-patching style interventions to GPT-2 showing localized subgraphs that contribute to greater-than outputs.",
            "evidence_against_mechanism": "Review cautions about limitations of patching (e.g., MLP-in-the-middle illusion, Hydra effect) and scalability/generalization of such mechanistic explanations to broader settings.",
            "intervention_type": "Activation patching / causal tracing and circuit analysis applied to GPT-2 hidden states.",
            "effect_of_intervention": "Enabled localization of the greater-than computation to specific components and provided mechanistic insight into how GPT-2 carries out the comparison.",
            "performance_metrics": null,
            "notable_failure_modes": "Not specified in detail in the review beyond general interpretability pitfalls (superposition, patching artifacts, limited OOD generalization).",
            "comparison_to_humans_or_symbolic": "No direct numeric comparison to symbolic algorithms or human performance is provided in the review; the work is presented as producing algorithm-like explanations within the model.",
            "uuid": "e2985.2",
            "source_info": {
                "paper_title": "Mechanistic Interpretability for AI Safety A Review",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Stolfo et al. (causal mediation / arithmetic)",
            "name_full": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis (Stolfo et al., 2023)",
            "brief_description": "Application of causal mediation analysis and mechanistic interpretability tools to attribute components of language models to arithmetic reasoning behavior and to identify internal mechanisms responsible for arithmetic.",
            "citation_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
            "mention_or_use": "mention",
            "model_name": "language models (unspecified in review)",
            "model_description": "Language models (various / unspecified sizes) analyzed post-hoc using causal mediation and intervention techniques to study arithmetic reasoning mechanisms.",
            "arithmetic_task_type": "Arithmetic reasoning (general arithmetic tasks as studied via mediation analysis).",
            "reported_mechanism": "Arithmetic reasoning emerges from interactions of internal components / circuits; causal mediation analysis can attribute contributions of parts of the network to arithmetic outputs.",
            "evidence_for_mechanism": "Use of causal mediation analysis, activation patching and related causal interventions to test mechanistic hypotheses and quantify component contributions to arithmetic reasoning.",
            "evidence_against_mechanism": "Review highlights general challenges with probing and causal attribution (probe capacity confounds, distributional issues, and limitations of patching), which temper strong claims.",
            "intervention_type": "Causal mediation analysis (statistical/causal intervention), activation patching / ablations.",
            "effect_of_intervention": "Provided more causal attribution of which components participate in arithmetic reasoning; helped formalize hypotheses about internal arithmetic computations.",
            "performance_metrics": null,
            "notable_failure_modes": "Probe confounds (probe learning vs representation), distributional shifts during patching, and superposition complicate clean attribution; scalability not settled.",
            "comparison_to_humans_or_symbolic": "No direct quantitative comparison to human or symbolic arithmetic performance is presented in the review; the approach aims to connect model internals to algorithmic-style explanations.",
            "uuid": "e2985.3",
            "source_info": {
                "paper_title": "Mechanistic Interpretability for AI Safety A Review",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Understanding Addition in Transformers",
            "rating": 2,
            "sanitized_title": "understanding_addition_in_transformers"
        },
        {
            "paper_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "rating": 2,
            "sanitized_title": "how_does_gpt2_compute_greaterthan_interpreting_mathematical_abilities_in_a_pretrained_language_model"
        },
        {
            "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks",
            "rating": 1,
            "sanitized_title": "the_clock_and_the_pizza_two_stories_in_mechanistic_explanation_of_neural_networks"
        },
        {
            "paper_title": "The Quantization Model of Neural Scaling",
            "rating": 1,
            "sanitized_title": "the_quantization_model_of_neural_scaling"
        }
    ],
    "cost": 0.017931,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Under review as submission to TMLR Mechanistic Interpretability for AI Safety A Review
22 Apr 2024</p>
<p>Leonard Bereska leonard.bereska@uva.nl 
University of Amsterdam</p>
<p>Efstratios Gavves egavves@uva.nl 
University of Amsterdam</p>
<p>Under review as submission to TMLR Mechanistic Interpretability for AI Safety A Review
22 Apr 2024FB0BA241DAB0CCCF727D69325D448995arXiv:2404.14082v1[cs.AI]
Understanding AI systems' inner workings is critical for ensuring value alignment and safety.This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding.We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation.We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety.We investigate challenges surrounding scalability, automation, and comprehensive interpretation.We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning.Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.</p>
<p>Introduction</p>
<p>As AI systems become increasingly sophisticated and general (Bubeck et al., 2023), advancing our understanding of these systems is crucial to ensure their alignment with human values and avoid catastrophic outcomes.The field of interpretability aims to demystify the internal processes of AI models, moving beyond evaluating performance alone.This review focuses on mechanistic interpretability, an emerging approach within the broader interpretability landscape that strives to specify the computations underlying deep neural networks comprehensively.We emphasize that understanding and interpreting these complex systems is not merely an academic endeavor -it's a societal imperative to ensure AI remains trustworthy and beneficial.</p>
<p>The interpretability landscape is undergoing a paradigm shift akin to the evolution from behaviorism to cognitive neuroscience in psychology.Historically, lacking tools for introspection, psychology treated the mind as a black box, focusing solely on observable behaviors.Similarly, interpretability has predominantly relied on black-box techniques, analyzing models based on input-output relationships or using attribution methods that, while probing deeper, still neglect the model's internal architecture.However, just as advancements in neuroscience allowed for a deeper understanding of internal cognitive processes, the field of interpretability is now moving towards a more granular approach.This shift from surface-level analysis to a focus on the internal mechanics of deep neural networks characterizes the transition towards inner interpretability (Räuker et al., 2023).</p>
<p>Mechanistic interpretability, as an approach to inner interpretability, aims to completely specify a neural network's computation, potentially in a format as explicit as pseudocode (also called reverse engineering), striving for a granular and precise understanding of model behavior.It distinguishes itself primarily through its ambition for comprehensive reverse engineering and its strong motivation towards AI safety.Our review serves as the first comprehensive exploration of mechanistic interpretability research, with the most accessible introductions currently scattered in a blog or list format (Olah, 2022;Nanda, 2022d;Olah et al., 2020;Sharkey et al., 2022a;Olah et al., 2018;Nanda, 2023f).We aim to synthesize the research (addressing the "research debt" (Olah &amp; Carter, 2017)) and provide a structured, accessible introduction for AI researchers and practitioners.The structure of this paper provides a cohesive overview of mechanistic interpretability, situating the mechanistic approach in the broader interpretability landscape (Section 2), presenting core concepts and hypotheses (Section 3), explaining methods and techniques (Section 4), discussing evaluation (Section 5), presenting a taxonomy and survey of the current field (Section 6), exploring relevance to AI safety (Section 7), and addressing challenges (Section 8) and future directions (Section 9).</p>
<p>Privileged vs Non-privileged Basis Non-privileged basis</p>
<p>No reason to expect features to be basisaligned.Calling basis dimensions neurons has no meaning.</p>
<p>Privileged basis</p>
<p>Architecture treats basis dimensions differently.Features can but need not align with neurons.(Bricken et al., 2023).</p>
<p>tional mechanics.Unlike concept-based interpretability, it aims to uncover causal relationships and precise computations transforming inputs into outputs, often identifying specific neural circuits driving behavior.This reverse engineering approach draws from interdisciplinary fields like physics, neuroscience, and systems biology to guide the development of transparent, value-aligned AI systems.Mechanistic interpretability is the primary focus of this review.</p>
<p>Core Concepts and Assumptions</p>
<p>This section introduces the foundational concepts and hypotheses that underpin mechanistic interpretability, including the notion of features as fundamental units of representation and their computation through circuits (Section 3.1), and the implications of these concepts for understanding the emergent properties of neural networks (Section 3.2).</p>
<p>Fundamental Units of Representation</p>
<p>Defining Features.The notion of a feature in neural networks is a central yet elusive concept, reflecting the pre-paradigmatic state of the field.Traditionally, features are understood as characteristics or attributes of the input data stream (Bishop, 2006).However, a broader interpretation suggests that a feature can be any measurable property or characteristic of a phenomenon, extending beyond human-interpretable elements.</p>
<p>The understanding of features encompasses two perspectives: human-centric and non-human-centric.A human-centric definition posits that features are semantically meaningful, articulable properties of the input, encoded in activation space (Olah, 2022).This view, however, may exclude features that are not understandable to humans.Adversarial examples have been interpreted as evidence for non-interpretable features that are perceptible to neural networks but not to humans (Ilyas et al., 2019).Furthermore, as neural networks evolve to surpass human capabilities, there is no inherent constraint that the features they learn must be comprehensible by humans; instead, they might discover increasingly abstract and alien features (Hubinger, 2019a).</p>
<p>A non-human-centric perspective defines features as independent yet repeatable units that a neural network representation can decompose into (Olah, 2022).This perspective allows for a more comprehensive understanding, encompassing features that are not necessarily interpretable by humans.</p>
<p>We adopt the notion of features as the smallest units of how neural networks encode knowledge, such that features cannot be further decomposed into smaller, distinct concepts.These features hypothetically serve as core components of a neural network's representation, analogous to how cells form the fundamental unit of biological organisms (Olah et al., 2020).</p>
<p>Definition: Feature</p>
<p>Features are the fundamental units of neural network representations.</p>
<p>Under review as submission to TMLR</p>
<p>Neurons as Computational Units?In the architecture of neural networks, neurons are the natural computational units, potentially representing individual features.Within a neural network representation h ∈ R n , the n basis directions are called neurons.For a neuron to be meaningful, the basis directions must functionally differ from other directions in the representation, forming a privileged basis -where the basis vectors are architecturally distinguished within the neural network layer from arbitrary directions in activation space.Typical non-linear activation functions privilege the basis directions formed by the neurons, making it meaningful to analyze individual neurons (Elhage et al., 2022b).Analyzing neurons can give insights into a network's functionality (Sajjad et al., 2022;Mu &amp; Andreas, 2020;Dai et al., 2022;Ghorbani &amp; Zou, 2020;Voita et al., 2023;Durrani et al., 2020;Goh et al., 2021;Bills et al., 2023;Huang et al., 2023).</p>
<p>Monosemantic and Polysemantic Neurons.</p>
<p>A neuron corresponding to a single semantic concept is called monosemantic.The intuition behind this term comes from analyzing what inputs activate a given neuron, revealing its associated semantic meaning or concept.If neurons were the fundamental primitives of neural network representations, all neurons would be monosemantic, implying a one-to-one relationship between neurons and features.Comprehensive interpretability would be as tractable as characterizing all neurons and their connections.However, empirically, especially for transformer models (Elhage et al., 2022b), neurons are often observed to be polysemantic, i.e., associated with multiple, unrelated concepts (Arora et al., 2018;Mu &amp; Andreas, 2020;Elhage et al., 2022a;Olah et al., 2020).For example, a single neuron may be activated by both images of cats and images of cars, suggesting it encodes multiple unrelated concepts.</p>
<p>Polysemanticity contradicts the interpretation of neurons as fundamental primitives and, in practice, makes it challenging to understand the information processing of neural networks.</p>
<p>Exploring Polysemanticity: Hypotheses and Implications.To understand the widespread occurrence of polysemanticity in neural networks, several hypotheses have been proposed:</p>
<p>• One trivial scenario would be that feature directions are orthogonal but not aligned with the basis directions (neurons).There is no inherent reason to assume that features would align with neurons in a non-privileged basis, where the basis vectors are not architecturally distinguished.However, even in a privileged basis formed by the neurons, the network could represent features not in the standard basis but as linear combinations of neurons (see Figure 2).</p>
<p>• An alternative hypothesis posits that redundancy due to noise introduced during training, such as random dropout (Srivastava et al., 2014), can lead to redundant representations and, consequently, to polysemantic neurons (Marshall &amp; Kirchner, 2024).This process involves distributing a single feature across several neurons rather than isolating it into individual ones, thereby encouraging polysemanticity.</p>
<p>• Finally, the superposition hypothesis addresses the limitations in the network's representative capacity -the number of neurons versus the number of crucial concepts.This hypothesis argues that the limited number of neurons compared to the vast array of important concepts necessitates a form of compression.As a result, an n-dimensional representation may encode features not with the n basis directions (neurons) but with the ∝ exp(n) possible almost orthogonal directions (Elhage et al., 2022b), leading to polysemanticity.</p>
<p>Hypothesis: Superposition</p>
<p>Neural networks represent more features than they have neurons by encoding features in overlapping combinations of neurons.</p>
<p>Superposition Hypothesis.The superposition hypothesis suggests that neural networks can leverage high-dimensional spaces to represent more features than the actual count of neurons by encoding features in almost orthogonal directions.Non-orthogonality means that features interfere with one another.However, Under review as submission to TMLR the benefit of representing many more features than neurons may outweigh the interference cost, mainly when concepts are sparse and non-linear activation functions can error-correct noise (Elhage et al., 2022b).</p>
<p>Toy Model of Superposition</p>
<p>A toy model (Elhage et al., 2022b) investigates the hypothesis that neural networks can represent more features than the number of neurons by encoding real-world concepts in a compressed manner.</p>
<p>The model considers a high-dimensional vector x, where each element x i corresponds to a feature capturing a real-world concept, represented as a random vector with varying importance determined by a weight a i .These features are assumed to have the following properties: 1) Concept Sparsity: Real-world concepts occur sparsely.2) More Concepts Than Neurons: The number of potential concepts vastly exceeds the available neurons. 3) Varying Concept Importance: Some concepts are more important than others for the task at hand.The input vector x represents features capturing these concepts, defined by a sparsity level S and an importance level a i for each feature x i , reflecting the sparsity and varying importance of the underlying concepts.The model dynamics involve transforming x into a hidden representation h of lower dimension, and then reconstructing it as x ′ :
h = W x, x ′ = ReLU(W T h + b)
The network's performance is evaluated using a loss function L weighted by the feature importances a i , reflecting the importance of the underlying concepts:
L = x i a i (x i − x ′ i ) 2
This toy model highlights neural networks' ability to encode numerous features representing real-world concepts into a compressed representation, providing insights into the superposition phenomenon observed in neural networks trained on real data.</p>
<p>Superposition
x x′ W W T h h 2 h 1</p>
<p>Increasing sparsity</p>
<p>Increasing importance</p>
<p>Effect of sparsity Toy model architecture</p>
<p>Figure 3: Illustration of the toy model architecture and the effects of sparsity.(left) Transformation of a five-feature input vector x into a two-dimensional hidden representation h, and its reconstruction as x ′ using the weight matrix W and its transpose, with feature importance indicated by a color gradient from yellow to green.(right) The effect of increasing feature sparsity S on the encoding capacity of the network, highlighting the network's enhanced ability to represent features in superposition as sparsity increases from 0 to 0.9, illustrated by arrows in the activation space h, which correspond to the columns of the matrix W .</p>
<p>Toy models can demonstrate under which conditions superposition occurs (Elhage et al., 2022b;Scherlis et al., 2023).Neural networks, via superposition, may effectively simulate computation with more neurons than they possess by allocating each feature to a linear combination of neurons, creating what is known as an overcomplete linear basis in the representation space.This perspective on superposition suggests that polysemantic models could be seen as compressed versions of hypothetically larger neural networks where each neuron represents a single concept (see Figure 4).Consequently, an alternative definition of features emerges:</p>
<p>Under review as submission to TMLR Definition Feature (Alternative) Features are elements that a network would ideally assign to individual neurons if neuron count were not a limiting factor (Bricken et al., 2023).In other words, features correspond to the disentangled concepts that a larger, sparser network with sufficient capacity would learn to represent with individual neurons.</p>
<p>Polysemanticity</p>
<p>Observed model</p>
<p>Hypothetical disentangled</p>
<p>Observed model Hypothetical disentangled model</p>
<p>Figure 4: Observed neural networks (left) can be viewed as compressed simulations of larger, sparser networks (right) where neurons represent distinct features.An "almost orthogonal" projection compresses the highdimensional sparse representation, manifesting as polysemantic neurons involved with multiple features in the lower-dimensional observed model, reflecting the compressed encoding.Figure adapted from (Bricken et al., 2023).</p>
<p>Research on superposition, including works by (Elhage et al., 2022b;Scherlis et al., 2023;Henighan et al., 2023), often investigates simplified models.However, understanding superposition in practical, transformerbased scenarios is crucial for real-world applications, as pioneered by (Gurnee et al., 2023).</p>
<p>The need for understanding networks despite polysemanticity has led to various approaches: One involves training models without superposition (Jermyn et al., 2022), for example, using a softmax linear unit (Elhage et al., 2022a) as an activation function to empirically increase the number of monosemantic neurons, but at the cost of making other neurons less interpretable.From a capabilities standpoint, polysemanticity may be desirable as it allows models to represent more concepts with limited compute, making training cheaper.Overall, engineering monosemanticity has proven challenging (Bricken et al., 2023) and may be impractical until we have orders of magnitude more compute available.</p>
<p>Another approach is to train networks in a standard way (creating polysemanticity) and use post-hoc analysis to find the feature directions in activation space, for example, with Sparse Autoencoders (SAEs).SAEs aim to find the true, disentangled features in an uncompressed representation by learning a sparse overcomplete basis that describes the activation space of the trained model (Bricken et al., 2023;Sharkey et al., 2022b;Cunningham et al., 2024) (also see Section 4.1).</p>
<p>If not neurons, what are features then?</p>
<p>We want to identify the fundamental units of neural networks, which we call features.Initially, neurons seemed likely candidates.However, this view fell short, particularly in transformer models where neurons often represent multiple concepts, a phenomenon known as polysemanticity.The superposition hypothesis addresses this, proposing that due to limited representational capacity, neural networks compress numerous features into the confined space of neurons, complicating interpretation.</p>
<p>This raises the question: How are features encoded if not in discrete neuron units?While a priori features could be encoded in an arbitrarily complex, non-linear structure, a growing body of theoretical arguments and empirical evidence supports the hypothesis that features are commonly represented linearly, i.e., as linear combinations of neurons -hence, as directions in representation space.This perspective promises to enhance our comprehension of neural networks by providing a more interpretable and manipulable framework for their internal representations.</p>
<p>Under review as submission to TMLR Hypothesis: Linear Representation Features are directions in activation space, i.e. as linear combinations of neurons.</p>
<p>The linear representation hypothesis is central to neural network analysis, suggesting that networks represent high-level features as linear directions in activation space.This hypothesis simplifies the neural network representations, enhancing their interpretability and ease of manipulation (Nanda et al., 2023b).</p>
<p>The architecture of neural networks, typically comprising linear layers interspersed with non-linear activation functions, inherently favors linear representations.When a neural network layer processes the information from previous layer activations, it typically employs matrix multiplication -only linear features can be processed in a subsequent single linear layer.Conversely, more complex non-linear encodings, though theoretically possible, would require multiple layers to be decoded.Hence, even hypothetical non-linear representations are reducible to intermediate linear representations in the typical neural network design.</p>
<p>An illustrative example can be seen in the work of Li et al. (2023a), who demonstrated that the internal representation of a generative pre-trained transformers (GPT) model trained on the Othello board game could be decoded only with a non-linear probe, not a linear one.This finding suggested that the explicit representation of the board state in terms of "black" and "white" pieces is not present linearly, but other features implicitly represent it.Complementing this, Nanda (2023c); Nanda et al. (2023b) showed that when decoding the board state in terms of "own" and "opponent's" pieces, a linear probe suffices, thereby reaffirming the linear representation hypothesis.To date, there is no evidence of non-linear representations in neural networks.</p>
<p>Empirical evidence supports the linear representation hypothesis: Firstly, the seminal work by Mikolov et al. (2013) revealing semantic vector calculus in word embeddings pointed to linear representations.Interpretability methods like linear probing (Alain &amp; Bengio, 2016;Belinkov, 2021) and sparse dictionary learning (Bricken et al., 2023;Cunningham et al., 2024;Deng et al., 2023) confirm the linear accessibility of meaningful features.It is possible to decode concepts (O' Mahony et al., 2023), tasks (Hendel et al., 2023), functions (Todd et al., 2023), sentiment (Tigges et al., 2023), and relations (Hernandez et al., 2023;Chanin et al., 2023) linearly in large language models.Additionally, breakthroughs in linear addition for model steering (Turner et al., 2023;Sakarvadia et al., 2023a;Li et al., 2023b) and representation engineering (Zou et al., 2023) highlight the practical implications of linear feature representations regarding model manipulation and interpretability.</p>
<p>While the linear representation hypothesis facilitates interpretability significantly, Sharkey et al. (2022a) warns against neglecting the potential role of non-linear representations.Given the dynamic nature of neural network development, it's crucial to continuously reevaluate the hypothesis, particularly in light of the possible emergence of non-linear features when interpretability tools that rely on linear representations are subject to optimization pressure (Hubinger, 2022).In this context, the polytope lens provides an alternative perspective, as proposed by Black et al. (2022).This approach shifts the focus to the impact of non-linear activation functions, examining how discrete polytopes, formed by piecewise linear activations, might be the fundamental primitives of neural network representation.</p>
<p>Computation and Abstraction</p>
<p>Having defined features as directions in activation space as the fundamental units of neural network representation, we now explore their computation.Neural networks can be conceptualized as computational graphs, within which circuits are sub-graphs consisting of linked features and the weights connecting them.Similar to how features are the representational primitive, circuits function as the computational primitive (Michaud et al., 2023) and the primary building block of these networks (Olah et al., 2020).</p>
<p>The decomposition of neural networks into circuits for interpretability has shown significant promise, particularly in small models trained for specific tasks such as addition, as seen in the work of Nanda et al. (2023a) and Quirke &amp; Barez (2023).However, scaling this analysis to broader behaviors remains challenging.To</p>
<p>Under review as submission to TMLR Definition: Circuit Circuits are sub-graphs of the network, consisting of features and the weights connecting them.</p>
<p>date, only relatively narrow behaviors like Python docstring formatting (Heimersheim &amp; Jett, 2023) and greater-than-computations (Hanna et al., 2023) have been thoroughly analyzed.</p>
<p>Despite these challenges, there has been notable progress in scaling circuit analysis to larger circuits, such as on GPT-2's indirect object identification (Wang et al., 2023) and on scaling to larger models such as multiplechoice question answering in Chinchilla (Lieberum et al., 2023).The circuits underlying more general and transferable behaviors are also being explored: McDougall et al. ( 2023)'s research on copy suppression in GPT-2's attention heads, for instance, sheds light on model calibration and self-repair mechanisms.Similarly, Davies et al. (2023) and Feng &amp; Steinhardt (2023) focus on how large language models (LLMs) perform variable binding and entity-attribute binding, respectively, providing insights into the representation of symbolic knowledge.Yu et al. (2023) explore mechanisms for factual recall in LLMs, revealing how circuits dynamically balance pre-trained knowledge with new contextual information.Lan &amp; Barez (2023) extend circuit analysis to sequence continuation tasks, identifying shared computational structures across semantically related sequences, thereby enriching our understanding of error identification.</p>
<p>More promisingly, some repeating patterns have shown universality across models and tasks.These universal patterns are called motifs (Olah et al., 2020) and can manifest not just as specific circuits or features but also as higher-level behaviors emerging from the interaction of multiple components.Examples include the curve detectors found across vision models (Cammarata et al., 2021;2020), induction circuits enabling in-context learning (Olsson et al., 2022), and the phenomenon of branch specialization in neural networks (Voss et al., 2021).Motifs may also capture how models leverage tokens for working memory or parallelize computations in a divide-and-conquer fashion across representations.The significance of motifs lies in revealing the common structures, mechanisms, and strategies that naturally emerge across neural architectures, shedding light on the fundamental building blocks underlying their intelligence.</p>
<p>Definition: Motif</p>
<p>Motifs are repeated patterns within a network, encompassing either features or circuits that emerge across different models and tasks.</p>
<p>Universality Hypothesis.Following the evidence for motifs or repeated patterns in neural networks, the universality hypothesis emerges as a pivotal concept.This hypothesis posits a convergence in forming features and circuits across various models and tasks, which could significantly ease interpretability efforts in AI.The universality hypothesis proposes that artificial and biological neural networks share similar features and circuits, suggesting a standard underlying structure (Chan et al., 2023;Sucholutsky et al., 2023;Kornblith et al., 2019).This idea posits that there is a fundamental basis in how neural networks, irrespective of their specific configurations, process and comprehend information.This could be due to inbuilt inductive biases in neural networks or natural abstractions (Chan et al., 2023) -concepts favored by the natural world that any cognitive system would naturally gravitate towards.</p>
<p>Hypothesis: Universality</p>
<p>Analogous features and circuits form across models and tasks (Olah et al., 2020).</p>
<p>Evidence for this hypothesis comes from cross-species neural structures in neuroscience, where similar neural structures and functions are found in different species (Kirchner, 2023).Additionally, machine learning models, including neural networks, tend to converge on similar features, representations, and classifications across different tasks and architectures (Chen et al., 2023a;Hacohen et al., 2020;Li et al., 2015;Bricken Under review as submission to TMLR et al., 2023).While various studies support the universality hypothesis, questions remain about the extent of feature and circuit similarity across different models and tasks.Nevertheless, this concept bridges AI and other scientific disciplines, offering cross-disciplinary applications and a deeper understanding of artificial and natural cognitive processes.In the context of mechanistic interpretability, this hypothesis has been investigated for neurons (Gurnee et al., 2024), group composition circuits (Chughtai et al., 2023), and modular task processing (Variengien &amp; Winsor, 2023).</p>
<p>Internal World Models.World models are internal causal models of an environment formed within neural networks.Traditionally linked with reinforcement learning, these models are explicitly trained to develop a compressed spatial and temporal representation of the training environment, enhancing downstream task performance and sample efficiency through training on internal hallucinations (Ha &amp; Schmidhuber, 2018).However, in the context of our survey, our focus shifts to world models that potentially form implicitly as a by-product of the training process, especially in LLMs that are trained on next-token prediction -also called GPT.</p>
<p>A critical perspective often surfacing in discussions about LLMs is their characterization as stochastic parrots (Bender et al., 2021).This label stems from their fundamental operational mechanism of predicting the next word in a sequence, supposedly relying heavily on memorization.From this viewpoint, LLMs are seen as forming complex correlations based on observational data but are thought to lack the ability to develop causal models of the world.This limitation is attributed to their lack of access to interventional data (Pearl, 2009).</p>
<p>However, this understanding of LLMs shifts significantly when viewed through the lens of the active inference framework (Salvatori et al., 2023), a theory rooted in cognitive science and neuroscience.Active inference postulates that the objective of minimizing prediction error, given enough representative capacity, is adequate for a learning system to develop complex world representations, behaviors, and abstractions.Since language inherently mirrors the world, these models could implicitly construct linguistic and broader world models.This perspective presupposes that LLMs, in their pursuit of better modeling sequences, inherently learn world models, abstractions, and algorithms for this purpose (Kulveit et al., 2023).</p>
<p>This alternative understanding of LLMs aligns with the simulation hypothesis, which suggests that models designed for prediction, such as LLMs, will eventually simulate the causal processes underlying data creation.</p>
<p>Seen as an extension of their drive for efficient compression, this hypothesis implies that adequately trained models like GPT could develop internal world models as a natural outcome of their predictive training (janus, 2022).</p>
<p>Hypothesis: Simulation</p>
<p>A model whose objective is text prediction will simulate the causal processes underlying the text creation if optimized sufficiently strongly (janus, 2022).</p>
<p>In addition to theoretical considerations for emergent causal world models (Richens &amp; Everitt, 2024;Nichani et al., 2024), mechanistic interpretability is starting to provide empirical evidence on the types of internal world models that may emerge in LLMs.The ability to internally represent the board state in games like Othello (Li et al., 2023a;Nanda et al., 2023b), create linear abstractions of spatial and temporal data (Gurnee &amp; Tegmark, 2023), and structure complex representations of mazes, demonstrating an understanding of maze topology and pathways (Ivanitskiy et al., 2023) highlight the growing abstraction capabilities of LLMs.</p>
<p>These emergent world models have significant implications for AI alignment research.For example, finding an internal representation of human values and aiming the AI systems objective may be the most trivial way to achieve alignment (Wentworth, 2022).Especially if the world model is internally separated from notions of goals and agency (Ruthenis, 2022), world model interpretability may be enough for alignment (Ruthenis, 2023).</p>
<p>Conditioning of pre-trained models as a pathway towards general intelligence is deemed comparatively safe, as it avoids directly creating agents with inherent goals or agendas (Jozdien, 2022;Hubinger et al., 2023).</p>
<p>Under review as submission to TMLR However, Hubinger et al. (2023) also highlights that prompting a model to simulate an actual agent, such as "You are a superintelligence in 2035 writing down an alignment solution:", could inadvertently lead to the formation of internal agents.In contrast, training with reinforcement learning tends to create agents by default (Casper et al., 2023a;Ngo et al., 2022).</p>
<p>The prediction orthogonality hypothesis further expands on this idea: It posits that prediction-focused models like GPT may simulate agents with various objectives and levels of optimality.In this context, GPT are simulators, simulating entities known as simulacra that can be either agentic or non-agentic, with different objectives from the simulator itself (janus, 2022).</p>
<p>Hypothesis: Prediction Orthogonality</p>
<p>A model whose objective is prediction can simulate agents who optimize toward any objectives with any degree of optimality (janus, 2022).</p>
<p>This prediction orthogonality hypothesis suggests that models primarily focused on prediction, such as GPT, can simulate agents -referred to as 'simulacra' -with potentially misaligned objectives (janus, 2022).Although GPT may lack genuine agency or intentionality, it may produce outputs that simulate these qualities (Bereska &amp; Gavves, 2023), underscoring the need for careful oversight and, better yet, finding internal agents or their constituents such as optimization or search potentially via mechanistic interpretability -an endeavor also known as searching for search (NicholasKees &amp; janus, 2022).</p>
<p>In conclusion, the evolution of LLMs from simple predictive models to entities potentially possessing complex internal world models, as suggested by the simulation hypothesis and supported by mechanistic interpretability studies, represents a significant shift in our understanding of these systems.This evolution challenges us to reconsider LLMs' capabilities and future trajectories in the broader landscape of AI development.</p>
<p>Core Methods</p>
<p>Mechanistic interpretability employs tools and techniques adopted from various interpretability approaches, focusing on causal methods that distinguish it from traditional, more observational techniques.This section provides an overview of the essential methodologies, enabling detailed observation and analysis of neural network models (Section 4.1), as well as interventional methods that allow for direct manipulation within the model (Section 4.2).The interplay between observation and intervention facilitates a comprehensive understanding of neural network operations (Section 4.3).Observational methods proposed for mechanistic interpretability include structured probes (more aligned with top-down interpretability), logit lens variants, and sparse autoencoders (SAEs).Additionally, as mechanistic interpretability focuses on causal understanding, novel methods encompass variants of activation patching for uncovering causal mechanisms and causal scrubbing for hypothesis evaluation.</p>
<p>Observation</p>
<p>Mechanistic interpretability draws from observational methods that analyze the inner workings of neural networks, with many of these methods preceding the field itself.For a detailed exploration of inner inter-Under review as submission to TMLR pretability methods, refer to (Räuker et al., 2023).Two prominent categories are example-based methods and feature-based methods:</p>
<p>• Example-based methods identify real input examples that highly activate specific neurons or layers.This helps pinpoint influential data points that maximize neuron activation within the neural network.</p>
<p>• Feature-based methods encompass techniques that generate synthetic inputs to optimize neuron activation.These neuron visualization techniques reveal how neurons respond to stimuli and which features are sensitive to (Zeiler &amp; Fergus, 2014).By understanding the synthetic inputs that drive neuron behavior, we can hypothesize about the features encoded by those neurons.</p>
<p>Probing for Features Probing involves training a classifier using the activations of a model, with the classifier's performance subsequently observed to deduce insights about the model's behavior and internal representations.As highlighted by Belinkov (2021), this technique faces a notable challenge: the probe's performance may often reflect its own learning capacities more than the actual characteristics of the model's representations.This dilemma has led researchers to investigate the ideal balance between the complexity of a probe and its capacity to accurately represent the model's features (Cao et al., 2021;Voita &amp; Titov, 2020).</p>
<p>The linear representation hypothesis offers a resolution to this issue.Under this hypothesis, the failure of a simple linear probe to detect certain features suggests their absence in the model's representations.Conversely, if a more complex probe succeeds where a simpler one fails, it implies that the model contains features that a complex function can combine into the target feature.Still, the target feature itself is not explicitly represented.This hypothesis implies that using linear probes could suffice in most cases, circumventing the complexity considerations generally associated with probing (Belinkov, 2021).</p>
<p>A significant limitation of probing is the inability to draw behavioral or causal conclusions.The evidence provided by probing is mainly observational, focusing on what information is encoded rather than how it is used (also see Figure 1).This necessitates careful analysis and possibly the adoption of alternative approaches (Elazar et al., 2021) or the integration of intervention techniques to draw more substantive conclusions about the model's behavior (Section 4.2).</p>
<p>Probing has been used to analyze the acquisition of chess knowledge in AlphaZero (McGrath et al., 2022) and the representation of linguistic information in BERT (Tenney et al., 2019).Gurnee et al. (2023) introduce sparse probing, decoding internal neuron activations in large models to understand feature representation and sparsity.They show that early layers use sparse combinations of neurons to represent many features in superposition, while middle layers have dedicated monosemantic neurons for higher-level contextual features.</p>
<p>Structured Probes While most of this review focuses on bottom-up, mechanistic approaches to interpretability, it is worth considering the potential for integrating top-down, concept-based techniques like structured probes.Structured probes represent an advanced technique in conceptual interpretability, playing a crucial role in probing language models to uncover complex features like truth representations.</p>
<p>A notable advancement in this domain is the discovery of an "internal truth" direction within language models using unsupervised contrastive probing, as demonstrated by the contrast-consistent search (CCS) method proposed by Burns et al. (2023).CCS identifies linear projections of hidden states that exhibit logical consistency, ensuring contrasting truth values for statements and their negations, contributing substantially to conceptual interpretability (Zou et al., 2023).</p>
<p>However, structured probes face significant challenges, particularly in unsupervised probing scenarios.A major concern is verifying the accuracy of discovered features, as unsupervised methods can identify numerous features without a straightforward verification process.Additionally, recent work by Farquhar et al. ( 2023) raises doubts about the scalability of CCS, suggesting that the CCS loss may capture simulated knowledge rather than the model's true knowledge, especially in highly capable models adept at simulating agents (simulacra) with their own belief systems.</p>
<p>Under review as submission to TMLR While structured probes primarily focus on high-level conceptual representations, their findings could potentially inform or complement mechanistic interpretability efforts.For instance, identifying truth directions through structured probes could help guide targeted interventions or analyze the underlying circuits responsible for truthful behavior using mechanistic techniques like activation patching or circuit tracing (Section 4.2).Conversely, mechanistic methods could provide insights into how truth representations emerge and are computed within the model, addressing some of the challenges faced by unsupervised structured probes.</p>
<p>Logit Lens</p>
<p>The logit lens (nostalgebraist, 2020) provides a window into the model's predictive process by applying the final classification layer (which projects the residual stream activation into logits/vocabulary space) to intermediate activations of the residual stream, revealing how prediction confidence evolves across computational stages.This is possible because transformers tend to build their predictions across layers iteratively (Geva et al., 2022).Extensions of this approach include the tuned lens (Belrose et al., 2023), which trains affine probes to decode hidden states into probability distributions over the vocabulary, and the Future Lens (Pal et al., 2023), which explores the extent to which individual hidden states encode information about subsequent tokens.</p>
<p>Researchers have also investigated techniques that bypass intermediate computations to probe representations directly.Din et al. (2023) propose using linear transformations to approximate hidden states from different layers, revealing that language models often predict final outputs in early layers.Dar et al. (2022) present a theoretical framework for interpreting transformer parameters by projecting them into the embedding space, enabling model alignment and parameter transfer across architectures.</p>
<p>Other techniques focus on interpreting specific model components or submodules.The DecoderLens (Langedijk et al., 2023) allows analyzing encoder-decoder transformers by cross-attending intermediate encoder representations in the decoder, shedding light on the information flow within the encoder.The Attention Lens (Sakarvadia et al., 2023b) aims to elucidate the specialized roles of attention heads by translating their outputs into vocabulary tokens via learned transformations.</p>
<p>Feature Disentanglement via Sparse Dictionary Learning As highlighted in Section 3.1, recent work suggests that the essential elements in neural networks are linear combinations of neurons representing features in superposition (Elhage et al., 2022b).Sparse autoencoders provide a methodology to decompose neural network activations into these individual component features (Sharkey et al., 2022b;Cunningham et al., 2024).This process involves reconstructing activation vectors as sparse linear combinations of directional vectors within the activation space, a problem also known as sparse dictionary learning (Olshausen &amp; Field, 1997).</p>
<p>Sparse dictionary learning has led to the development of various sparse coding algorithms (Lee et al., 2006).The sparse autoencoder stands out for its simplicity and scalability (Sharkey et al., 2022b).The first application to a language model was by Yun et al. (2021), who implemented sparse dictionary learning across multiple layers of a language model.</p>
<p>Sparse autoencoders, a variant of the standard autoencoder framework, incorporate sparsity regularization to encourage learning sparse yet meaningful data representations.Theoretical foundations in the disentanglement literature suggest that autoencoders can recover ground truth features under feature sparsity and non-negativity (Whittington et al., 2022).The "ground truth features" here refer to the true, disentangled features that underlie the data distribution, which the autoencoder aims to recover through its sparse encoding.In the context of neural networks, these would correspond to the individual features combined to form neuron activations, which the sparse autoencoder attempts to disentangle and represent explicitly in its dictionary.</p>
<p>Practical implementations, such as the toy model by Sharkey et al. (2022b), demonstrate the viability of this approach, with the precise tuning of the sparsity penalty on the hidden activations being a critical aspect that dictates the sparsity level of the autoencoder (Sharkey et al., 2022b).We show an overview in the pink box on sparse autoencoders in Figure 6.</p>
<p>Empirical studies indicate that sparse autoencoders can enhance the interpretability of neural networks, exhibiting higher scores on the autointerpretability metric and increased monosemanticity (Bricken et al., Under review as submission to TMLR Sparse Dictionary Learning Sparse autoencoders (Cunningham et al., 2024) are a solution to the sparse dictionary learning (Olshausen &amp; Field, 1997) problem to decompose neural network activations into individual component features.The goal is to learn a dictionary of vectors {f k } n feat k=1 ⊂ R d that can represent the unknown, ground truth network features {g j } ngt j=1 as sparse linear combinations.The autoencoder architecture consists of an encoder and a ReLU activation function, expanding the input dimensionality to d hid = Rd in , where R controls the ratio of the feature dictionary size to the model dimension.The encoder's output is given by:
h = ReLU(W enc x + b)
(1)
x ′ = W dec h = d hid −1 i=0 h i f i (2)
where W enc , W T dec ∈ R d hid ×din and b ∈ R d hid .The parameter matrix W dec forms the feature dictionary, with rows f i as dictionary features.The autoencoder is trained to minimize the loss, where the L 1 penalty on h encourages sparse reconstructions using the dictionary features.2023; Cunningham et al., 2024;Sharkey et al., 2022b).Furthermore, sparse autoencoders have been employed to measure feature sparsity (Deng et al., 2023) and interpret reward models in reinforcement learning-based language models (Marks et al., 2023), making them an actively researched area in mechanistic interpretability.
L(x) = ||x − x ′ || 2 2 + α||h|| 1(3)</p>
<p>Intervention</p>
<p>Activation Patching is a collective term for a set of causal intervention techniques, also known as causal tracing (Meng et al., 2022a), interchange intervention (Geiger et al., 2021b), causal mediation analysis (Vig et al., 2020), and causal ablation (Wang et al., 2023).While nuanced in their application, these techniques share the common goal of manipulating neural network activations to shed light on the decision-making processes within the model.</p>
<p>Under review as submission to TMLR</p>
<p>Activation patching modifies a neural model's internal state by replacing specific activations with alternative values, such as zeros, mean activations across samples, random noise, or activations from a different forward pass.This technique enables researchers to isolate and examine the effects of modifying particular neural circuits within the model to understand how these circuits interact and contribute to the model's behavior in response to different inputs.By selectively replacing activations, activation patching highlights the circuits directly responsible for particular behaviors or outputs while reducing the influence of other, irrelevant circuits.</p>
<p>The primary goal is to isolate and understand the role of specific components or circuits within the model.By observing how changes in activations affect the model's output, researchers can infer the function and importance of those components.Critical applications are (i) localizing behavior by identifying critical activations, for example, understanding storage and processing of factual information (Meng et al., 2022a;Geva et al., 2023;Goldowsky-Dill et al., 2023;Stolfo et al., 2023), and (ii) analyzing component interactions, such as conducting circuit analysis to identify sub-networks within a model's computation graph that implement specified behaviors (Wang et al., 2023;Hanna et al., 2023;Lieberum et al., 2023;Hendel et al., 2023;Geva et al., 2023).The standard protocol of activation patching entails: i.) running a model with a clean input and caching the latent activations; ii.) executing the model with a corrupted input; iii.) re-running the model with the corrupted input but substituting specific activations with those from the clean cache; and iv.) determining significance by observing the variations in the model's output during the third step, thereby highlighting the importance of the replaced components.</p>
<p>Embedding</p>
<p>The process relies on comparing pairs of inputs: a clean input, which triggers the desired behavior, and a corrupted input, which is identical to the clean one except for critical differences that prevent the behavior.This careful selection ensures that the two inputs share as many circuits as possible, except those directly influencing the behavior under investigation -effectively controlling for confounding circuitry.Through activation patching-transferring activations from the clean input run to the corrupted one-researchers can maintain the shared circuits' functionality while pinpointing and isolating the specific circuit responsible for the behavior.</p>
<p>Differences in patching direction -clean to corrupted versus corrupted to clean -yield insights into which model components are sufficient or necessary for a given behavior.Clean to corrupted patching (causal tracing) identifies activations that are sufficient for restoring clean performance, highlighting the non-necessity of redundant components in achieving specific outputs.If many components redundantly encode something that quickly saturates, you can get good performance from patching in any of them, even if none are necessary.This approach, effective even in redundant system components, clarifies the sufficiency of specific activations in driving model performance under OR logic conditions: In circuits A-AND-B, this tells us nothing, but in A-OR-B, it tells us that both A or B is sufficient on its own.Conversely, corrupted to clean patching (resample ablation) focuses on determining the necessary activations for clean performance, emphasizing the criticality of specific components.If the model has redundancy, we may see that nothing is necessary!Even if, in the aggregate, they're essential.Particularly useful in AND logic scenarios, this method assesses the impact of removing specific activations, revealing the indispensable elements of the computational architecture.In the circuit A-OR-B, resample ablating does nothing.However, A-AND-B tells us that removing each of A or B will dramatically reduce performance.</p>
<p>Activation patching can employ various corruption methods, including zero-, mean-, random-, or resample ablation -replacing activations with zeros, an average over activations across diverse samples, Gaussian noise (Meng et al., 2022a), or activations from a different model run (Vig et al., 2020;Wang et al., 2023), each serving to modulate the model's internal state in distinct ways.Among these, resample ablation stands out for its effectiveness in maintaining consistent model behavior by not changing the data distribution too much (Zhang &amp; Nanda, 2023).While breaking behavior is always possible by taking the model off-distribution, this is uninteresting for finding the relevant circuit (Nanda, 2023e).Therefore, one needs to be careful when interpreting the results of patching.</p>
<p>Among evaluation metrics for assessing how activation patching influences behavior, comparing the logit difference between clean and corrupted runs stands out as a precise measure of the changes in confidence levels across different inputs and the ability to detect negative modules (Zhang &amp; Nanda, 2023).Additionally, per-token log probability provides a detailed view of the model's prediction confidence at each token, providing more granularity.Direct logit attribution further delves into how different components influence the logit of the correct next token, shedding light on the critical elements of the model's predictions.However, internal memory management can mislead this metric (Dao et al., 2023).When used together, these metrics enable a thorough evaluation of the impact of activation patching, offering comprehensive insights into the intervention's effects on model behavior.</p>
<p>Activation patching, while a powerful interpretability tool, encounters several limitations.A primary concern is its limited ability to generalize beyond specific distributions or tasks, often focusing on narrow scenarios without fully addressing broader or varied contexts (Nanda, 2023d).Another issue is the MLP-In-The-Middle illusion (Lange et al., 2023), a phenomenon where patching an entire Multi-Layer Perceptron (MLP) layer shows no observable effect, yet patching a specific subspace within the same layer reveals significant impacts.This raises questions about the relevance of certain subspaces in the model's normal functioning.This suggests that some components may appear crucial in patching but are dormant or irrelevant in regular operations.</p>
<p>Additionally, the Hydra effect (McGrath et al., 2023), where models internally self-repair and maintain capabilities even when key components are ablated, can sometimes obscure the relevant components.The effects of patching can propagate and interact in complex ways, potentially exaggerating or diminishing the apparent importance of certain components (see Section 8.2).</p>
<p>Translating the localization of model behaviors, as revealed by activation patching, into effective model editing (Hase et al., 2023) can also be challenging.Understanding where certain information or processes are stored in the model doesn't always seem to translate into actionable strategies for modifying or improving the model's performance or behavior.</p>
<p>Furthermore, the process of activation patching can be slow, which is especially problematic in large models or when attempting to automate the process (Conmy et al., 2023).However, this challenge can be partially mitigated using attribution patching (Nanda, 2023d;Syed et al., 2023), a gradient-based alternative that takes a linear approximation to traditional activation patching, similar to other attribution methods (see Section 2).Attribution patching offers a faster and more scalable approach, particularly advantageous in automated circuit discovery (Syed et al., 2023) and large model applications, providing a more feasible and efficient means of probing neural network behaviors while retaining the core benefits of the activation patching approach.</p>
<p>Under review as submission to TMLR Recent advancements include the introduction of AtP* (Kramár et al., 2024), a refined version of attribution patching that addresses specific failure modes of the original method to reduce false negatives, thus improving its reliability while maintaining scalability.Other variations include path patching (Goldowsky-Dill et al., 2023), which quantitatively tests hypotheses expressing that behaviors are localized to a set of paths, and attention pattern patching (Nanda, 2023d), which leverages attention attribution patterns to gain insights into information flow within the network.Ghandeharioun et al. (2024) introduced a unified framework to analyze hidden representations.</p>
<p>Integrating Observation and Intervention</p>
<p>Integrating different methodologies is necessary for a thorough understanding of neural network models.These complex models require a broad approach that goes beyond individual techniques.An effective strategy combines feature-level analysis tools like sparse autoencoders with probing and interventional methods like activation patching.This integrated approach could allow a more in-depth examination of neural network feature-level circuits.</p>
<p>Even seemingly independent techniques can improve the robustness of analysis.For example, validating activation patching findings with maximally activating dataset examples or direct logit attribution offers a more comprehensive view of a component's network functionality.However, achieving complete understanding remains challenging due to the potential for feature superposition within these models.A single component may simultaneously represent multiple features, complicating interpretability efforts.Navigating and disentangling these intertwined representations requires integrating diverse analytical techniques.</p>
<p>Evaluation</p>
<p>Qualitative Evaluation.Interpretability research lacks established metrics, making qualitative results crucial.The signal of structure approach (Olah &amp; Jermyn, 2024) -observing intricate patterns indicating genuine structures -resembles examining cells under a microscope.A nuanced balance of qualitative observations and quantitative analyses is required, often necessitating custom interfaces to avoid oversimplification.</p>
<p>Quantitative Evaluation.A central challenge is the lack of rigorous evaluation methods.Relying solely on intuition is inadequate, as hypotheses can be conflated with conclusions (Rudin, 2019;Miller, 2019;Räuker et al., 2023), leading to cherry-picking and optimizing for best-case performance rather than aiming for methods that perform well on average or in worst-case scenarios (Casper, 2023) (see also Section 8.1).Current practices are ad hoc, with proxies (Doshi-Velez &amp; Kim, 2017) potentially leading to over-optimization (Goodhart's law -When a measure becomes a target, it ceases to be a good measure).Distinguishing correlation from causation is crucial, as interpretability illusions (Bolukbasi et al., 2021;Friedman et al., 2023a;Olah et al., 2017) demonstrate visualizations may be meaningless without causal linking.</p>
<p>Rigorous evaluation methods are needed, such as i.) assessing out-of-distribution inputs, as most current methods are only valid for analyzing specific examples or datasets (Räuker et al., 2023;Ilyas et al., 2019;Mu &amp; Andreas, 2020;Casper et al., 2023b;Burns et al., 2023), ii.) controlling the system through edits, such as implanting or removing trojans (Mazeika et al., 2022) or targeted editing (Ghorbani &amp; Zou, 2020;Dai et al., 2022;Meng et al., 2022a;b;Bau et al., 2018;Hase et al., 2023), iii.) or replacing it with simpler reverseengineered alternatives (Lindner et al., 2023).Ultimately, establishing benchmarks, ideally automated, is required.</p>
<p>Causality as a Theoretical Foundation.The theory of causality (Pearl, 2009) provides a mathematically precise language for mechanistic interpretability, forming the foundation for understanding high-level semantics in neural representations (Geiger et al., 2023a).Treating neural networks as causal models involves considering the compute graph as the causal graph, allowing for precise interventions and examining individual parameters' roles (McGrath et al., 2023).In contrast to typical real-world causal analyses, the causal model is known with complete certainty, long chains of interventions are possible, and all variable values can be simultaneously read.Nevertheless, the large number of parameters, often lacking clear meaning, poses a challenge in this context.</p>
<p>Under review as submission to TMLR Causal inference techniques have been employed in various contexts within neural networks, including locating factual knowledge (Meng et al., 2022a), addressing gender bias through mediation analyses (Vig et al., 2020), and constructing causal abstractions of neural network computations (Geiger et al., 2023a;2021a;2023b;2021b;McGrath et al., 2023).Ablations and interchange interventions have been suggested as means to validate hypotheses about mechanisms in neural networks and enforce specific structures (Chan et al., 2022;Leavitt &amp; Morcos, 2020;Geiger et al., 2021b), enabling large-scale analysis of model behavior (Wu et al., 2023).</p>
<p>Rigorous Hypothesis Testing.Causal scrubbing (Chan et al., 2022), causal abstraction (Geiger et al., 2023a), and locally consistent abstractions (Jenner et al., 2023) have been proposed as rigorous methods to formalize and test hypotheses about how neural networks implement specific behaviors.</p>
<p>Causal abstraction (Geiger et al., 2023a) introduces a mathematical framework that treats both neural networks and potential explanations as causal models.An explanation is considered correct if it is a valid causal abstraction, which can be empirically tested through interchange interventions (ablations) on the neural network's activations and the explanation (Jenner et al., 2023).Various interpretability methods, such as LIME (Ribeiro et al., 2016), causal effect estimation (Feder et al., 2021), causal mediation analysis (Vig et al., 2020), iterated nullspace projection (Ravfogel et al., 2020), and circuit-based explanations are considered exceptional cases of causal abstraction (Geiger et al., 2023a).In contrast, locally consistent abstractions (Jenner et al., 2023) check consistency between the neural network and the explanation only one step away from the intervention node, forming a more permissive notion than causal abstraction.</p>
<p>Causal scrubbing (Chan et al., 2022) formalizes hypotheses as a tuple (G, I, c), where G is the model's computational graph, I is an interpretable computational graph hypothesized to explain the behavior, and c maps nodes of I to nodes of G.The core idea is to replace activations in G with other activations that should be equivalent according to the hypothesis.This is done by recursively traversing I and G, resampling important parents from the data distribution conditioned on agreeing with I, and resampling unimportant parents unconditionally.Performance is measured on the scrubbed model with resampled activations -if the hypothesis is accurate, performance should be preserved.</p>
<p>These methods form a hierarchy regarding strictness, with causal abstractions being the strictest, followed by locally consistent abstractions and causal scrubbing being the most permissive (Jenner et al., 2023).This hierarchy highlights trade-offs in choosing stricter or more permissive notions, affecting the ability to find acceptable explanations, generalization, and mechanistic anomaly detection.While unified by the causal framework, these methods represent different conceptual goals for what constitutes an adequate explanation of neural network behavior.</p>
<p>Current Research</p>
<p>This section surveys current research in mechanistic interpretability across three approaches based on when and how the model is interpreted during training: Intrinsic interpretability methods are applied before training to enhance the model's inherent interpretability (Section 6.1).Developmental interpretability involves studying the model's learning dynamics and the emergence of internal structures during training (Section 6.2).After training, post-hoc interpretability techniques are applied to gain insights into the model's behavior and decision-making processes (Section 6.3), including efforts towards uncovering general, transferable principles across models and tasks, as well as automating the discovery and interpretation of critical circuits in trained models (Section 6.4).</p>
<p>Intrinsic Interpretability</p>
<p>Intrinsic methods for mechanistic interpretability offer a promising approach to designing neural networks more amenable to reverse engineering without sacrificing performance.By encouraging sparsity, modularity, and monosemanticity through architectural choices and training procedures, these methods aim to make the reverse engineering process more tractable.Intrinsic interpretability methods aim to constrain the training process to make learned programs more interpretable (Friedman et al., 2023b).This approach is closely related to neurosymbolic learning (Riegel et al., 2020) and can involve techniques like regularization with spatial structure, akin to the organization of information in the human brain (Liu et al., 2023a;b).</p>
<p>Recent work has explored various architectural choices and training procedures to improve the interpretability of neural networks.Jermyn et al. (2022) and Elhage et al. (2022a) demonstrate that architectural choices can affect monosemanticity, suggesting that models could be engineered to be more monosemantic.Sharkey (2023) propose using a bilinear layer instead of a linear layer to encourage monosemanticity in language models.Liu et al. (2023a) and Liu et al. (2023b) introduce a biologically inspired spatial regularization regime called brain-inspired modular training for forming modules in networks during training.They showcase how this can help RNNs exhibit brain-like anatomical modularity without degrading performance, in contrast to naive attempts to use sparsity to reduce the cost of having more neurons per layer (Jermyn et al., 2022;Bricken et al., 2023).</p>
<p>Preceding the mechanistic interpretability literature, various works have explored techniques to improve interpretability, such as sparse attention (Zhang et al., 2021), adding l 1 penalties to neuron activations (Kasioumis et al., 2021;Georgiadis, 2019), and pruning neurons (Frankle &amp; Carbin, 2019).These techniques have been shown to encourage sparsity, modularity, and disentanglement, which are essential aspects of intrinsic interpretability.</p>
<p>Developmental Interpretability</p>
<p>Developmental interpretability focuses on learning dynamics, aiming to understand the development of internal structure in neural networks incrementally, one phase transition at a time.Singular learning theory (Watanabe, 2009;Lau et al., 2023) provides a mathematical framework for understanding the asymptotic behavior of learning algorithms in the presence of degeneracy, explaining observable effects in standard machine learning models and phenomena in deep learning, such as phase transitions.</p>
<p>Explaining emergence (Steinhardt, 2023;Schaeffer et al., 2023;Wei et al., 2022) and phase transitions (Simon et al., 2023) is a central theme in developmental interpretability, with phase transitions associated with mechanistic formation and changes in macroscopic behavior, such as the emergence of in-context learning (Olsson et al., 2022).The work by Hoogland et al. (2024) provides a compelling example of using the learning coefficient from singular learning theory to identify phase transitions during training that corresponded to learning bi-grams, n-grams, and induction heads in a small transformer model.</p>
<p>Under review as submission to TMLR</p>
<p>While there is currently no work directly applying developmental interpretability to explain the following phenomena, it could potentially help shed light on understanding generalization (Zhang et al., 2017), how stochastic gradient descent learns functions of increasing complexity (Nakkiran et al., 2019), and the transition from memorization to generalization (grokking) (Liu et al., 2022a;Power et al., 2022;Liu et al., 2022b;Nanda et al., 2023a;Varma et al., 2023;Thilak et al., 2022;Merrill et al., 2023;Liu et al., 2023c;Stander et al., 2023).Neural scaling laws (Caballero et al., 2022;Liu &amp; Tegmark, 2023;Michaud et al., 2023), sometimes connected to mechanistic insights (Hernandez et al., 2022), could also potentially benefit from a developmental interpretability perspective.</p>
<p>In sum, developmental interpretability may serve as an evolutionary theory lens, making sense of the structures that emerge (Saphra, 2023) and offering insights into the evolution of neural network representations and their relation to learning dynamics.</p>
<p>Post-hoc Interpretability</p>
<p>In applied mechanistic interpretability, researchers explore various facets and methodologies to uncover the inner workings of AI models.Some key distinctions are drawn between global versus local interpretability and comprehensive versus partial interpretability.Global interpretability aims to uncover general patterns and behaviors of a model, providing insights that apply broadly across many instances (Doshi-Velez &amp; Kim, 2017;Nanda, 2023e).In contrast, local interpretability explains the reasons behind a model's decisions for particular instances, offering insights into individual predictions or behaviors.</p>
<p>Comprehensive interpretability involves achieving a deep and exhaustive understanding of a model's behavior, providing a holistic view of its inner workings (Nanda, 2023e).In contrast, partial interpretability often applied to larger and more complex models, concentrates on interpreting specific aspects or subsets of the model's behavior, focusing on the application's most relevant or critical areas.Akin to collecting biological species, characterizing these "circuits" aims to discover general computational principles underlying modern AI systems.This multifaceted approach collectively analyzes specific capabilities in large models while enabling a comprehensive study of learned algorithms in smaller procedural networks.</p>
<p>Large Models -Narrow Behavior Circuit-style mechanistic interpretability aims to explain neural networks by reverse engineering the underlying mechanisms at the level of individual neurons or subgraphs.This approach assumes that neural vector representations encode high-level concepts and circuits defined by model weights encode meaningful algorithms (Olah et al., 2020;Cammarata et al., 2020).Studies on deep networks support these claims, identifying circuits responsible for detecting curved lines or object orientation (Cammarata et al., 2020;2021;Voss et al., 2021).This paradigm has been applied to language models to discover subnetworks (circuits) responsible for specific capabilities.Circuit analysis localizes and understands subgraphs within a model's computational graph responsible for specific behaviors.For large language models, this often involves narrow investigations into behaviors like multiple choice reasoning (Lieberum et al., 2023), indirect object identification (Wang et al., 2023), or computing operations (Hanna et al., 2023).Other examples include analyzing circuits for Python docstrings (Heimersheim &amp; Jett, 2023), "an" vs "a" usage (Miller &amp; Neo, 2023), and price tagging (Wu et al., 2023).Case studies often construct datasets using templates filled by placeholder values to enable precise control for causal interventions (Wang et al., 2023;Hanna et al., 2023;Wu et al., 2023).</p>
<p>Toy Models -Comprehensive Analysis Small models trained on specialized mathematical or algorithmic tasks enable more comprehensive reverse engineering of learned algorithms (Nanda et al., 2023a;Zhong et al., 2023;Chughtai et al., 2023).Even simple arithmetic operations can involve complex strategies and multiple algorithmic solutions (Nanda et al., 2023a;Zhong et al., 2023).Characterizing these algorithms helps test hypotheses around generalizable mechanisms like variable binding (Feng &amp; Steinhardt, 2023;Davies et al., 2023) and arithmetic reasoning (Stolfo et al., 2023).The work by Varma et al. (2023) builds on the initial grokking work and explains grokking in terms of circuit efficiency, illustrating how a comprehensive understanding of a toy model can enable interesting analyses on top of that understanding.</p>
<p>Under review as submission to TMLR Towards Universality The ultimate goal is to uncover general principles that transfer across models and tasks, such as induction heads for in-context learning (Olsson et al., 2022), variable binding mechanisms (Feng &amp; Steinhardt, 2023;Davies et al., 2023), arithmetic reasoning (Stolfo et al., 2023;Brinkmann et al., 2024), or retrieval tasks (Variengien &amp; Winsor, 2023).Despite promising results, debates surround the universality hypothesis -the idea that different models learn similar features and circuits when trained on similar tasks.(Chughtai et al., 2023) finds mixed evidence for universality in group composition, suggesting that while families of circuits and features can be characterized, precise circuits and development order may be arbitrary.</p>
<p>Towards High-level Mechanisms Causal interventions can extract a high-level understanding of computations and representations learned by large language models (Variengien &amp; Winsor, 2023;Hendel et al., 2023;Feng &amp; Steinhardt, 2023;Zou et al., 2023).Recent work focuses on intervening in internal representations to study high-level concepts and computations encoded.For example, Hendel et al. (2023) patched residual stream vectors to transfer task representations, while Feng &amp; Steinhardt (2023) intervened on residual streams to argue that models generate IDs to bind entities to attributes.Representation engineering techniques (Zou et al., 2023) extract reading vectors from model activations to stimulate or inhibit specific concepts.Although these interventions don't operate via specific mechanisms, they offer a promising approach for extracting high-level causal understanding and bridging bottom-up and top-down interpretability approaches.</p>
<p>Automation: Scaling Post-Hoc Interpretability</p>
<p>As models become more complex, automating key aspects of the interpretability workflow becomes increasingly crucial.Tracing a model's computational pathways is highly labor-intensive, quickly becoming infeasible as the model size increases.Automating the discovery of relevant circuits and their functional interpretation represents a pivotal step towards scalable and comprehensive model understanding (Nainani, 2024).</p>
<p>Dissecting Models into Interpretable Circuits</p>
<p>The first major automation challenge is identifying the critical computational sub-circuits or components underpinning a model's behavior for a given task.A pioneering line of work aims to achieve this via efficient masking or patching procedures.Methods like Automated Circuit Discovery (ACDC) (Conmy et al., 2023) and Attribution Patching (Syed et al., 2023;Kramár et al., 2024) iteratively knock out model activations, pinpointing components whose removal has the most significant impact on performance.This masking approach has proven scalable even to large models like Chinchilla (70B parameters) (Lieberum et al., 2023).</p>
<p>Other techniques take a more top-down approach.Davies et al. (2023) specify high-level causal properties (desiderata) that components solving a target subtask should satisfy and then learn binary masks to expose those component subsets.Ferrando &amp; Voita (2024) construct Information Flow Graphs highlighting key nodes and operations by tracing attribution flows, enabling extraction of general information routing patterns across prediction domains.</p>
<p>Explicit architectural biases like modularity can further boost automation efficiency.Nainani (2024) find that models trained with Brain-Inspired Modular Training (BIMT) (Liu et al., 2023a) produce more readily identifiable circuits compared to standard training.Such domain-inspired inductive biases may prove increasingly vital as models grow more massive and monolithic.</p>
<p>Interpreting Extracted Circuits Once critical circuit components have been isolated, the key remaining step is interpreting what computation those components perform.Sparse autoencoders are a prominent approach for interpreting extracted circuits by decomposing neural network activations into individual component features, as discussed in Section 4.1.</p>
<p>A novel paradigm uses large language models themselves as an interpretive tool.Bills et al. (2023) demonstrate generating natural language descriptions of individual neuron functions by prompting language models like GPT-4 to explain sets of inputs that activate a neuron.Mousi et al. (2023) similarly employ language models to annotate unsupervised neuron clusters identified via hierarchical clustering.Foote et al. (2023) take a complementary graph-based approach in their neuron-to-graph tool: automatically extracting individual neurons' behavior patterns from training data as structured graphs amenable to visualization, programmatic comparisons, and property searches.Such representations could synergize with language model-based annotation to provide multi-faceted descriptions of neuron roles.</p>
<p>Other techniques map neural representations to high-level variables through gradient-based alignment.Distributed Alignment Search (DAS) (Geiger et al., 2023b) uses gradient descent to associate neuron activations with symbolic causal concepts, thereby distilling model functioning into interpretable pieces.Wu et al. (2023) scale DAS to large models like Alpaca (7B parameters) by replacing brute force steps with learned alignments.</p>
<p>While impressive strides have been made, robustly interpreting the largest trillion-parameter models using these techniques remains an open challenge.Another novel approach, mechanistic-interpretability-based program synthesis (Michaud et al., 2024), entirely sidesteps this complexity by auto-distilling the algorithm learned by a trained model into human-readable Python code without relying on further interpretability analyses or model architectural knowledge.As models become increasingly vast and opaque, such synergistic combinations of methods -uncovering circuits, annotating them, or altogether transcribing them into executable code -will likely prove crucial for maintaining insight and oversight.How Could Interpretability Promote AI Safety?Gaining mechanistic insights into the inner workings of AI systems seems for navigating AI safety as we develop more powerful models.Interpretability tools can provide an understanding of artificial cognition, the way AI systems process information and make decisions, which offers several potential benefits: Mechanistic interpretability could accelerate AI safety research by providing richer feedback loops and grounding for model evaluation.It may also help anticipate emergent capabilities, such as the emergence of new skills or behaviors in the model before they fully manifest.This relates to studying the incremental development of internal structures and representations as the model learns (Section 6.2).Additionally, interpretability could substantiate theoretical risk models with concrete evidence, such as demonstrating inner misalignment (when a model's behavior deviates from its intended goals) or mesa-optimization (the emergence of unintended subagents within the model).It may also trigger normative shifts within the AI community toward rigorous safety protocols by revealing potential risks or concerning behaviors.</p>
<p>Regarding specific AI risks, interpretability may prevent malicious misuse by locating and erasing sensitive information stored in the model.It could reduce competitive pressures by substantiating potential threats, promoting organizational safety cultures, and supporting AI alignment (ensuring AI systems pursue intended goals) through better monitoring and evaluation.Interpretability can provide safety filters for every stage of training: before training by deliberate design, during training by detecting early signs of misalignment and potentially shifting the distribution towards alignment, and after training by rigorous evaluation of artificial cognition for honesty and screening for deceptive behaviors.</p>
<p>Mechanistic interpretability integrates well into various AI alignment agendas, such as understanding existing models, controlling them, making AI systems solve alignment problems, and developing alignment theories.It could enhance strategies like detecting deceptive alignment (when a model appears aligned but is actually pursuing different goals), eliciting latent knowledge from models, and enabling better oversight.A high degree of understanding may even allow for well-founded AI approaches (AI systems with provable guarantees) or microscope AI (extract world knowledge from the model without letting the model take actions).Furthermore, comprehensive interpretability itself may be an alignment strategy if we can identify internal representations of human values and guide the model to pursue those values by retargeting an internal search process.Ultimately, understanding and control are intertwined, and better understanding can lead to improved control of AI systems.</p>
<p>How Could Mechanistic Insight be Harmful?Mechanistic interpretability research could accelerate AI capabilities, potentially leading to the development of powerful AI systems that are misaligned with human values, posing significant risks.While historically, interpretability research had little impact on AI capabilities, recent exceptions like discoveries about scaling laws, architectural improvements inspired by studying induction heads, and efficiency gains inspired by the logit lens technique demonstrated its potential impact.Scaling interpretability research may necessitate automation, potentially enabling rapid self-improvement of AI systems.Some researchers recommend selective publication and focusing on lowerrisk areas to mitigate these risks.</p>
<p>Mechanistic interpretability also poses dual-use risks, where the same techniques could be used for both beneficial and harmful purposes.Fine-grained editing capabilities enabled by interpretability could be used for machine unlearning (removing private data or dangerous knowledge from models) but could be misused for censorship.Similarly, while interpretability may help improve adversarial robustness, it may also facilitate the development of stronger adversarial attacks.Knowing in advance whether interpretability research will primarily strengthen defense or offense in this domain is challenging.</p>
<p>Misunderstanding or overestimating the capabilities of interpretability techniques can divert resources from critical safety areas or lead to overconfidence and misplaced trust in AI systems.Robust evaluation and benchmarking (Section 9.2) are crucial to validate interpretability claims and reduce the risks of overinterpretation or misinterpretation.</p>
<p>Challenges</p>
<p>Research Issues</p>
<p>Need for Comprehensive, Multi-Pronged Approaches Current mechanistic interpretability research narrowly focuses on individual techniques rather than combining complementary approaches.Utilizing a diverse interpretability toolbox, akin to how collective innovations like batch normalization, residual connections, and improved optimizers drove advances in computer vision, could provide holistic understanding, e.g., coordinated methods reverse engineering trojaned behaviors (Casper et al., 2023b).</p>
<p>There is an overemphasis on post-hoc techniques, with fewer efforts on intrinsic approaches enhancing interpretability through architectural inductive biases or training (Sharkey, 2023;Elhage et al., 2022a;Wong et al., 2023;Hubinger, 2019c).Suggesting intrinsic interpretability complements post-hoc analysis for robust understanding.More work predicting and shaping capabilities in advance, rather than merely explaining afterward, could benefit the field.</p>
<p>Cherry-Picking and Streetlight Interpretability.Another concerning pattern is the tendency to cherry-pick results, relying on a small number of convincing examples or visualizations as the basis for an argument without comprehensive evaluation (Räuker et al., 2023).This amounts to publication bias, showcasing an unrealistic highlight reel of best-case performance.Relatedly, many interpretability techniques are primarily evaluated on small toy models and tasks (Chughtai et al., 2023;Elhage et al., 2022b;Jermyn et al., 2022;Chen et al., 2023b), risking missing critical phenomena that only emerge in more realistic and diverse contexts.This focus on cherry-picked results from toy models is a form of "streetlight interpretability" (Casper, 2023), examining AI systems under only ideal conditions of maximal interpretability.</p>
<p>Under review as submission to TMLR</p>
<p>Technical Limitations</p>
<p>Scalability Challenges and Risks of Human Reliance.A critical hurdle is demonstrating the scalability of mechanistic interpretability to real-world AI systems across model size, task complexity, behavioral coverage, and analysis efficiency (Elhage et al., 2022b;Scherlis et al., 2023).Achieving a truly comprehensive understanding of a model's capabilities in all contexts is daunting, and the time and compute required must scale tractably.Automating interpretability techniques is crucial, as manual analysis quickly becomes infeasible for large models.The high human involvement in current interpretability research raises concerns about the scalability and validity of human-generated model interpretations.Subjective, inconsistent human evaluations and lack of ground-truth benchmarks are known issues (Räuker et al., 2023).As models scale, it will become increasingly untenable to rely on humans to hypothesize about model mechanisms manually.More work is needed on automating the discovery of mechanistic explanations and translating model weights into human-readable computational graphs (Elhage et al., 2022b).</p>
<p>Obstacles to Bottom-Up Interpretability.There are fundamental questions about the tractability of fully reverse engineering neural networks from the bottom up, especially as models become more complex (Hendrycks, 2023).Models may learn internal representations and algorithms that do not cleanly map to human-understandable concepts, making them difficult to interpret even with complete transparency (McGrath et al., 2022).This gap between human and model ontologies may widen as architectures evolve, increasing opaqueness (Hendrycks et al., 2022).Conversely, model representations might naturally converge to more human-interpretable forms as capability increases (Hubinger, 2019a;Feng &amp; Steinhardt, 2023).</p>
<p>Analyzing Models Embedded in Environments</p>
<p>Real-world AI systems embedded in rich, interactive environments exhibit two forms of in-context behavior that pose significant interpretability challenges beyond understanding models in isolation.Externally, models may dynamically adapt to and reshape their environments through in-context learning from the interactions and feedback loops with their external environment (Leahy, 2023).Internally, the Hydra effect demonstrates in-context reorganization, where models flexibly reorganize their internal representations in a context-dependent manner to maintain capabilities even after ablating key components (McGrath et al., 2023).These two instances of in-context behavior -external adaptation to the environment and internal self-reorganization -undermine interpretability approaches that assume fixed circuits.For models deeply embedded in rich real-world settings, their dynamic coupling with the external world via in-context environmental learning and their internal in-context representational reorganization make strong interpretability guarantees difficult to attain through analysis of the initial model alone.</p>
<p>Adversarial Pressure Against Interpretability</p>
<p>As models become more capable through increased training and optimization, there is a risk they may learn deceptive behaviors that actively obscure or mislead the interpretability techniques meant to understand them.Models could develop adversarial "mind-reader" components that predict and counteract the specific analysis methods used to interpret their inner workings (Sharkey, 2022;Hubinger, 2022).Optimizing models through techniques like gradient descent could inadvertently make their internal representations less interpretable to external observers (Hubinger, 2019b;Fu et al., 2023;von Oswald et al., 2023).In extreme cases, a highly advanced AI system singularly focused on preserving its core objectives may directly undermine the fundamental assumptions that enable interpretability methods in the first place.</p>
<p>These adversarial dynamics, where the capabilities of the AI model are pitted against efforts to interpret it, underscore the need for interpretability research to prioritize worst-case robustness rather than just averagecase scenarios.Current techniques often fail even when models are not adversarially optimized.Achieving high confidence in fully understanding extremely capable AI models may require fundamental advances to make interpretability frameworks resilient against an intelligent system's active deceptive efforts.</p>
<p>Future Directions</p>
<p>Given the current limitations and challenges, several promising directions can be pursued to advance mechanistic interpretability, emphasizing conceptual clarity, establishing rigorous standards, improving the scalability of interpretability techniques, and expanding the research scope.</p>
<p>Future Directions</p>
<p>Expanding Scope</p>
<p>Clarifying Concepts</p>
<p>Integrating with Existing Literature.To mature, mechanistic interpretability should embrace existing work, using established terminology rather than reinventing the wheel.Diverging terminology inhibits collaboration across disciplines.Presently, the terminology used for mechanistic interpretability partially diverges from mainstream AI research (Casper, 2023).For example, while the mainstream speaks of distributed representations (Hinton, 1984;Olah, 2023) and the goal of disentangling representations (Higgins et al., 2018;Locatello et al., 2019), the mechanistic interpretability literature refers to the same phenomenon as polysemanticity (Scherlis et al., 2023;Lecomte et al., 2023;Marshall &amp; Kirchner, 2024) and superposition (Elhage et al., 2022b;Henighan et al., 2023).Using common language invites "accidental" contributions and prevents isolating mechanistic interpretability from broader AI research.</p>
<p>Mechanistic interpretability relates to many other fields in AI research, including compressed sensing (Elhage et al., 2022b), modularity, adversarial robustness, continual learning, network compression (Räuker et al., 2023), neurosymbolic reasoning, trojan detection, and program synthesis (Casper, 2023;Michaud et al., 2024).These relationships can help develop new methods, metrics, benchmarks, and theoretical frameworks.</p>
<p>For instance:</p>
<p>• Neurosymbolic Reasoning and Program Synthesis: Mechanistic interpretability aims to reverse engineer neural networks by converting their weights into human-readable algorithms.This endeavor can draw inspiration from neurosymbolic reasoning (Riegel et al., 2020) and program synthesis.Techniques like creating programs in domain-specific languages (Verma et al., 2019b;a;Trivedi et al., 2021), extracting decision trees (Zhang et al., 2019) or symbolic causal graphs (Ren et al., 2023) from neural networks align well with the goals of mechanistic interpretability.Adopting these approaches can extend the toolkit for reverse engineering AI systems.</p>
<p>• Trojan Detection: Detecting deceptive models is a key motivation for inspecting model internals, as deception is not salient from observing behavior alone by definition (Casper et al., 2024).However, quantifying progress is challenging due to the lack of evidence for deception as an emergent capability in current models (Steinhardt, 2023), apart from sycophancy (Sharma et al., 2023) and theoretical evidence for deceptive inflation behavior (Lang et al., 2024).Detecting trojans or Under review as submission to TMLR backdoors (Hubinger et al., 2024) implanted via data poisoning could serve as a helpful proxy goal and proof-of-concept.While these trojans simulate outer alignment failure (misalignment between the model's behavior and its specified objective) rather than inner alignment failure like deceptive alignment (where an emergent sub-component optimizer within the model is misaligned with the original training objective), trojan detection still provides a practical testbed for benchmarking interpretability methods and evaluating their effectiveness quantitatively.</p>
<p>• Adversarial Robustness: There is a duality between interpretability and adversarial robustness (Elhage et al., 2022b;Räuker et al., 2023).More interpretable models tend to be more robust against adversarial attacks.Interpretability tools can help create more sophisticated adversaries, improving our understanding of model internals.Viewing adversarial examples as inherent neural network features (Ilyas et al., 2019) rather than bugs also hints at alien features beyond human perception.Connecting mechanistic interpretability to adversarial robustness thus offers paths to gain theoretical insights and measure progress, for instance, by evaluating how well interpretability enables crafting strong adversarial examples.</p>
<p>More details on the interplay between interpretability, robustness, modularity, continual learning, network compression, and the human visual system can be found in the review by Räuker et al. (2023).</p>
<p>Corroborate or Refute Core Assumptions.Features are the fundamental units defining neural representations and enabling mechanistic interpretability's bottom-up approach (Chan, 2023), but defining them involves assumptions requiring scrutiny, as they shape interpretations and research directions.Questioning hypotheses by seeking additional evidence or counter-examples is crucial.</p>
<p>The linear representation hypothesis treats activation directions as features (Park et al., 2023;Nanda et al., 2023b;Elhage et al., 2022b), but the emergence and necessity of linearity is unclear -is it architectural bias or inherent?Stronger theory justifying linearity's necessity or counter-examples like autoencoders on uncorrelated data without intermediate linear layers (Elhage et al., 2022b) are needed.An alternative lens views features as polytopes from piecewise linear activations (Black et al., 2022), questioning if direction simplification suffices or added polytope complexity aids interpretability.</p>
<p>Polysemantic neurons are attributed to superposition compressing many features into limited neurons (Elhage et al., 2022b), but incidental redundancy without compression also causes polysemanticity (Lecomte et al., 2023;Marshall &amp; Kirchner, 2024;McGrath et al., 2023).Understanding superposition's role could inform mitigating polysemanticity via regularization (Lecomte et al., 2023).Superposition also raises open questions like operationalizing computation in superposition (Vaintrob et al., 2024), attention head superposition (Elhage et al., 2022b;Jermyn et al., 2023;Lieberum et al., 2023;Gould et al., 2023), representing feature clusters (Elhage et al., 2022b), connections to adversarial robustness (Elhage et al., 2022b), anticorrelated feature organization (Elhage et al., 2022b), and architectural effects (Nanda, 2023a).</p>
<p>Setting Standards</p>
<p>Prioritizing Robustness over Capability Advancement.As the mechanistic interpretability community expands, it is essential to maintain the norm of not advancing AI capabilities while simultaneously establishing metrics necessary for the field's progress (Räuker et al., 2023).Researchers should prioritize developing comprehensive tools for analyzing the worst-case performance of AI systems, ensuring robustness and reliability in critical applications.This includes focusing on adversarial tasks, such as backdoor detection and removal (Lamparth &amp; Reuel, 2023;Hubinger et al., 2024;Wu et al., 2022), and evaluating the accuracy of explanations in producing adversarial examples (Goldowsky-Dill et al., 2023).</p>
<p>Establishing Metrics, Benchmarks, and Algorithmic Testbeds.To objectively evaluate interpretability methods, developing well-defined metrics and standardized benchmarks assessing aspects like feature attribution accuracy, circuit explanation comprehensiveness, and practical utility across diverse models/tasks is crucial (Räuker et al., 2023).Algorithmic testbeds are also essential for evaluating faithfulness (Jacovi &amp; Goldberg, 2020) and falsifiability (Leavitt &amp; Morcos, 2020) et al., 2023) can provide ground truth labels for benchmarking search methods (Goldowsky-Dill et al., 2023).Toy models studying superposition in computation (Vaintrob et al., 2024), transformers on algorithmic tasks can quantify sparsity and test intrinsic methods.Replacing components with hypothesized circuits (Quirke et al., 2024) should be the goal for comprehensive evaluation.</p>
<p>Scaling Up</p>
<p>Broader and Deeper Coverage of Complex Models and Behaviors.A primary goal in scaling mechanistic interpretability is pushing the Pareto frontier between model and task complexity and the coverage of interpretability techniques (Chan, 2023).While efforts have focused on larger models, it is equally crucial to scale to more complex tasks and provide comprehensive explanations essential for provable safety (Tegmark &amp; Omohundro, 2023) and enumerative safety (Cunningham et al., 2024;Elhage et al., 2022b) by ensuring models won't engage in dangerous behaviors like deception.Future work should aim for thorough reverse engineering (Quirke &amp; Barez, 2023), integrating proven modules into larger networks (Nanda et al., 2023a), and capturing sequences encoded in hidden states beyond immediate predictions (Pal et al., 2023).Deepening analysis complexity is also key, validating the realism of toy models (Elhage et al., 2022b) and extending techniques like path patching (Goldowsky-Dill et al., 2023;Liu et al., 2023a) to larger language models.The field must move beyond small transformers on algorithmic tasks (Nanda et al., 2023a) and limited scenarios (Friedman et al., 2023a) to tackle more complex, realistic cases.</p>
<p>Towards Universality.As mechanistic interpretability matures, the field must transition from isolated empirical findings to developing overarching theories and universal reasoning primitives beyond specific circuits, aiming for a comprehensive understanding of AI capabilities.While collecting empirical data remains valuable (Nanda, 2023f), establishing motifs, empirical laws, and theories capturing universal model behavior aspects is crucial.This may involve finding more circuits/features (Nanda, 2022a;c), exploring circuits as a lens for memorization/generalization (Hanna et al., 2023), identifying primitive general reasoning skills (Feng &amp; Steinhardt, 2023), generalizing specific findings to model-agnostic phenomena (Merullo et al., 2023), and investigating emergent model generality across neural network classes (Ivanitskiy et al., 2023).Identifying universal reasoning patterns and unifying theories is key to advancing interpretability.</p>
<p>Automation.Implementing automated methods is crucial for scaling interpretability of real-world stateof-the-art models across size, task complexity, behavior coverage, and analysis time (Hobbhahn, 2022).Manual circuit identification is labor-intensive (Lieberum et al., 2023), so automated techniques like circuit discovery and sparse autoencoders can enhance the process (Foote et al., 2023;Nanda, 2023b).Future work should automatically create varying datasets for understanding circuit functionality (Conmy et al., 2023), develop automated hypothesis search (Goldowsky-Dill et al., 2023), and investigate attention head/MLP interplay (Monea et al., 2023).Scaling sparse autoencoders to extract high-quality features automatically for frontier models is critical (Bricken et al., 2023).Still, it requires caution regarding potential downsides like AI iteration outpacing training (<strong>RicG</strong>, 2023) and loss of human interpretability from tool complexity (Doshi-Velez &amp; Kim, 2017).</p>
<p>Expanding the Scope</p>
<p>Interpretability Across Training While mechanistic interpretability of final trained models is a prerequisite, the field should also advance interpretability before and during training by studying learning dynamics (Nanda, 2022b;Elhage et al., 2022b;Hubinger, 2022).This includes tracking neuron development (Liu et al., 2021), analyzing neuron set changes with scale (Michaud et al., 2023), and investigating emergent computations (Quirke &amp; Barez, 2023).Studying phase transitions could yield safety insights like reward hacking risks (Olsson et al., 2022).</p>
<p>Multi-Level Analysis</p>
<p>Complementing the predominant bottom-up methods (Hanna et al., 2023), mechanistic interpretability should explore top-down and hybrid approaches, a promising yet neglected avenue.The top-down analysis offers a tractable way to study large models and guide microscopic research with macroscopic observations (Variengien &amp; Winsor, 2023).Its computational efficiency could enable extensive "comparative anatomy" of diverse models, revealing high-level motifs underlying abilities.These motifs could serve as analysis units for understanding internal modifications from techniques like instruction fine-tuning (Ouyang et al., 2022) and reinforcement learning from human feedback (Christiano et al., 2017;Bai et al., 2022).</p>
<p>New Frontiers: Vision, Multimodal, and Reinforcement Learning Models While some mechanistic interpretability has explored convolutional neural networks for vision (Cammarata et al., 2021;2020), vision-language models (Palit et al., 2023;Salin et al., 2022;Hilton et al., 2020), and multimodal neurons (Goh et al., 2021), little work has focused on vision transformers (Palit et al., 2023;Aflalo et al., 2022;Vilas et al., 2023).Future efforts could identify mechanisms within vision-language models, mirroring progress in unimodal language models (Nanda et al., 2023a;Wang et al., 2023).</p>
<p>Reinforcement learning (RL) is also a crucial frontier given its role in advanced AI training via techniques like reinforcement learning from human feedback (RLHF) (Christiano et al., 2017;Bai et al., 2022), despite potentially posing significant safety risks (Bereska &amp; Gavves, 2023;Casper et al., 2023a).Interpretability of RL should investigate reward/goal representations (TurnTrout et al., 2023;Colognese &amp; Jozdien, 2023;Colognese, 2023;Bloom &amp; Colognese, 2023), study circuitry changes from alignment algorithms (Jain et al., 2023;Lee et al., 2024), and explore emergent subgoals or proxies (Hubinger et al., 2019;Ivanitskiy et al., 2023).</p>
<p>Figure 1 :
1
Figure 1: Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral analyzes input-output relations; Attributional quantifies individual input feature influences; Concept-based identifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanisms from inputs to outputs.</p>
<p>Figure 2 :
2
Figure 2: Comparison of privileged and non-privileged basis in neural networks.Figure adapted from(Bricken et al., 2023).</p>
<p>Figure 5 :
5
Figure 5: Overview of relevant methods and techniques employed in mechanistic interpretability research.Observational methods proposed for mechanistic interpretability include structured probes (more aligned with top-down interpretability), logit lens variants, and sparse autoencoders (SAEs).Additionally, as mechanistic interpretability focuses on causal understanding, novel methods encompass variants of activation patching for uncovering causal mechanisms and causal scrubbing for hypothesis evaluation.</p>
<p>Figure 6 :
6
Figure6: Illustration of a sparse autoencoder applied to the MLP layer activations, consisting of an encoder that increases dimensionality while emphasizing sparse representations and a decoder that reconstructs the original activations using the learned feature dictionary.</p>
<p>Figure 7 :
7
Figure 7: (a) The transfer of activations from clean to corrupted inputs isolates neural circuits.(b) Boolean logic circuits are an analogy for sufficiency and necessity in neural circuits via patching strategies.</p>
<p>Figure 8 :
8
Figure 8: Key desiderata for interpretability approaches across training and analysis stages: (1) Intrinsic: Architectural biases for sparsity, modularity, and disentangled representations.(2) Developmental: Predictive capability for phase transitions, manageable number of critical transitions, and a unifying theory connecting observations to singularity geometry.(3) Post-hoc: Global, comprehensive, automated discovery of critical circuits, uncovering transferable principles across models/tasks, and extracting high-level causal mechanisms.</p>
<p>Figure 9 :
9
Figure 9: Potential benefits and risks of mechanistic interpretability for AI safety.</p>
<p>Figure 10 :
10
Figure 10: Roadmap for advancing mechanistic interpretability research, highlighting key strategic directions.</p>
<p>of techniques.Tools like Tracr (Lindner Under review as submission to TMLR</p>
<p>AcknowledgementsI am grateful for the invaluable feedback and comments from Leon Lang, Tim Bakker, Jannik Brinkmann, Can Rager, Louis van Harten, Jacqueline Bereska, Thijmen Nijdam, Alice Rigg, Arthur Conmy, and Tom Lieberum.Their insights substantially improved this work.
VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. Estelle Aflalo, Meng Du, Shao-Yen, Yongfei Tseng, Chenfei Liu, Nan Wu, Vasudev Duan, Lal, June 2022CVPR27</p>
<p>Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, ICLR. 72016</p>
<p>Linear Algebraic Structure of Word Senses, with Applications to Polysemy. Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski, TACL. December 2018. 4</p>
<p>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, Wojciech Samek, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140PLOS ONE. 2July 2015</p>
<p>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Ben Mann, Jared Kaplan, April 2022CoRR27</p>
<p>David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, Antonio Torralba, GAN Dissection: Visualizing and Understanding Generative Adversarial Networks. ICLR. December 201816</p>
<p>Yonatan Belinkov, Probing Classifiers: Promises, Shortcomings, and Advances. CoRR. September 2021. 2, 711</p>
<p>Eliciting Latent Predictions from Transformers with the Tuned Lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, August 2023CoRR12</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922March 2021. 9ACM FAccT</p>
<p>Taming Simulators: Challenges, Pathways and Vision for the Alignment of Large Language Models. Leonard Bereska, Efstratios Gavves, AAAI Symposium Series. 1027October 2023</p>
<p>Language models can explain neurons in language models. Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, William Saunders, OpenAI Blog. 4202023</p>
<p>Pattern recognition and machine learning. Christopher M Bishop, 2006Springer-Verlag New York Inc3</p>
<p>Sid Black, Lee Sharkey, Leo Grinsztajn, Eric Winsor, Dan Braun, Jacob Merizian, Kip Parker, Carlos Ramón Guevara, Beren Millidge, Gabriel Alfour, Connor Leahy, Interpreting Neural Networks through the Polytope Lens. CoRRNovember 2022725</p>
<p>. Joseph Bloom, Paul Colognese, Decision Transformer Interpretability. AI Alignment Forum. 272023</p>
<p>An Interpretability Illusion for BERT. Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Vi'egas, M Wattenberg, April 2021CoRR16</p>
<p>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. Transformer Circuits Thread. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden Mclean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Chris Olah, October 2023. 3, 6, 7, 8, 121826</p>
<p>A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task. Abhay Tmlr Jannik Brinkmann, Victor Sheshadri, Paul Levoso, Christian Swoboda, Bartelt, February 2024. 20CoRRUnder review as submission to</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. CoRRApril 2023. 1</p>
<p>Discovering Latent Knowledge in Language Models Without Supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, ICLR. 162023. 2, 11</p>
<p>. Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger, Broken Neural Scaling Laws. ICLR. October 2022. 19</p>
<p>. Nick Cammarata, Gabriel Goh, Shan Carter, Ludwig Schubert, Michael Petrov, Chris Olah, Curve Detectors. Distill. 27June 2020. 8, 19</p>
<p>. Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, Chris Olah, Curve Circuits. Distill. 8272021</p>
<p>Low-Complexity Probing via Finding Subnetworks. Steven Cao, Victor Sanh, Alexander M Rush, April 2021. 11NAACL-HLT</p>
<p>Visualizing the Feature Importance for Black Box Models. Giuseppe Casalicchio, Christoph Molnar, Bernd Bischl, ECML PKDD. 22018</p>
<p>The Engineer's Interpretability Sequence. AI Alignment Forum. Stephen Casper, February 20231624</p>
<p>. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell2023aCoRR1027Lauro Langosco, Peter Hase</p>
<p>Stephen Casper, Yuxiao Li, Jiawei Li, Tong Bu, Kevin Zhang, Kaivalya Hariharan, Dylan Hadfield-Menell, Red Teaming Deep Neural Networks with Feature Synthesis Tools. 2023b1622</p>
<p>. Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max Tegmark, David Krueger, Dylan Hadfield-Menell, January 2024Black-Box Access is Insufficient for Rigorous AI Audits. CoRR24</p>
<p>What I would do if I wasn't at ARC Evals. AI Alignment Forum. Lawrence Chan, May 20232526</p>
<p>Causal Scrubbing: a method for rigorously testing interpretability hypotheses. Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldowsky-Dill, Ansh Radhakrishnan, Nate Buck, Thomas, Redwood Research</p>
<p>Lawrence Chan, Leon Lang, Erik Jenner, Natural Abstractions: Key claims, Theorems, and Critiques. AI Alignment Forum. March 2023</p>
<p>Identifying Linear Relational Concepts in Large Language Models. David Chanin, Anthony Hunter, Oana-Maria Camburu, 2023CoRR</p>
<p>Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory. Yiting Chen, Zhanpeng Zhou, Junchi Yan, November 2023a. 8CoRR</p>
<p>Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition. Edmund Tmlr Zhongtian Chen, Jake Lau, Susan Mendel, Daniel Wei, Murfet, October 2023b. 22CoRRUnder review as submission to</p>
<p>Deep reinforcement learning from human preferences. Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, NeurIPS. 27December 2017</p>
<p>Bilal Chughtai, Lawrence Chan, Neel Nanda, A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations. ICML, 2023. 1922</p>
<p>Internal Target Information for AI Oversight. LessWrong. Paul Colognese, 202327</p>
<p>High-level interpretability: detecting an AI's objectives. AI Alignment Forum. Paul Colognese, Jozdien , 202327</p>
<p>Towards Automated Circuit Discovery for Mechanistic Interpretability. Arthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso, NeurIPS. 15262023</p>
<p>Explaining by removing: a unified framework for model explanation. Ian C Covert, Scott Lundberg, Su-In Lee, JMLR. 2January 2021</p>
<p>Sparse Autoencoders Find Highly Interpretable Features in Language Models. ICLR. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey, January 2024. 6, 7, 121326</p>
<p>Knowledge Neurons in Pretrained Transformers. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei, ACL. 4162022</p>
<p>An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. James Dao, Yeu-Tong Lau, Can Rager, Jett Janiak, 2023CoRR15</p>
<p>Analyzing Transformers in Embedding Space. Guy Dar, Mor Geva, Ankit Gupta, Jonathan Berant, ACL. 12December 2022</p>
<p>Discovering Variable Binding Circuitry with Desiderata. Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, David Bau, July 2023. 8, 19CoRR20</p>
<p>Measuring Feature Sparsity in Language Models. Mingyang Deng, Lucas Tao, Joe Benton, 2023CoRR713</p>
<p>Jump to Conclusions: Short-Cutting Transformers With Linear Transformations. Alexander Yom Din, Taelin Karidi, Leshem Choshen, Mor Geva, March 2023CoRR12</p>
<p>Finale Doshi, - Velez, Been Kim, Towards A Rigorous Science of Interpretable Machine Learning. CoRRMarch 20171626</p>
<p>Analyzing Individual Neurons in Pretrained Language Models. Nadir Durrani, Hassan Sajjad, Fahim Dalvi, Yonatan Belinkov, EMNLP. October 2020. 4</p>
<p>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. TACL. Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg, February 2021. 11</p>
<p>. Nelson Elhage, Tristan Hume, Olsson Catherine, Nanda Neel, Tom Henighan, Scott Johnston, Sheer Elshowk, Nicholas Joseph, Nova Dassarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah1822Softmax Linear Units. Transformer Circuits Thread, 2022a. 4, 6</p>
<p>Dawn Drain, Carol Chen, and others. Toy Models of Superposition. Transformer Circuits Thread. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, 2022b. 4, 5, 6, 122226</p>
<p>Challenges with unsupervised LLM knowledge discovery. Vikrant Tmlr Sebastian Farquhar, Zachary Varma, Johannes Kenton, Vladimir Gasteiger, Rohin Mikulik, Shah, 2023CoRRUnder review as submission to</p>
<p>CausaLM: Causal Model Explanation Through Counterfactual Language Models. Amir Feder, Nadav Oved, Uri Shalit, Roi Reichart, Computational Linguistics. May 2021. 17</p>
<p>How do Language Models Bind Entities in Context? CoRR. Jiahai Feng, Jacob Steinhardt, October 2023. 8, 192026</p>
<p>Information Flow Routes: Automatically Interpreting Language Models at Scale. CoRR. Javier Ferrando, Elena Voita, February 2024. 20</p>
<p>Alex Foote, Neel Nanda, Esben Kran, Ioannis Konstas, Shay Cohen, Fazl Barez, Neuron to Graph: Interpreting Language Model Neurons at Scale. CoRR. May 20232026</p>
<p>The Lottery Ticket Hypothesis: Finding Sparse. Jonathan Frankle, Michael Carbin, Trainable Neural Networks. ICLR. 18March 2019</p>
<p>Interpretability illusions in the generalization of simplified models. Dan Friedman, Andrew Lampinen, Lucas Dixon, Danqi Chen, Asma Ghandeharioun, 2023aCoRR1626</p>
<p>Learning Transformer Programs. NeurIPS. Dan Friedman, Alexander Wettig, Danqi Chen, June 2023b. 18</p>
<p>Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. Deqing Fu, Tian-Qi Chen, Robin Jia, Vatsal Sharan, October 2023CoRR23</p>
<p>Causal Abstractions of Neural Networks. Atticus Geiger, Hanson Lu, Thomas Icard, Christopher Potts, NeurIPS. 172021a</p>
<p>Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D Goodman, Christopher Potts, Inducing Causal Structure for Interpretable Neural Networks. ICML. January 2021b1317</p>
<p>Causal Abstraction for Faithful Model Interpretation. Atticus Geiger, Chris Potts, Thomas Icard, January 2023aCoRR1617</p>
<p>Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, Noah D Goodman, 2023bCoRR1721</p>
<p>Accelerating Convolutional Neural Networks via Activation Map Compression. Georgios Georgiadis, March 2019CoRR18</p>
<p>Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg, EMNLP. 12October 2022</p>
<p>Dissecting Recall of Factual Associations in Auto-Regressive Language Models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, EMNLP. 14October 2023</p>
<p>Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models. Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva, January 2024CoRR16</p>
<p>Amirata Ghorbani, James Zou, Neuron Shapley: Discovering the Responsible Neurons. NeurIPS. November 2020416</p>
<p>. Gabriel Goh, Nick Cammarata, † , Chelsea Voss, † , Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah, Multimodal Neurons in Artificial Neural Networks. Distill. 427March 2021</p>
<p>Localizing Model Behavior with Path Patching. Nicholas Goldowsky-Dill, Chris Macleod, Lucas Sato, Aryaman Arora, 2023CoRR1426</p>
<p>Successor Heads: Recurring, Interpretable Attention Heads In The Wild. Euan Gould, George Ong, Arthur Ogden, Conmy, 2023CoRR25Under review as submission to TMLR Rhys</p>
<p>Language Models Represent Space and Time. Wes Gurnee, Max Tegmark, 2023CoRR</p>
<p>Finding Neurons in a Haystack: Case Studies with Sparse Probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas, TMLR. 6112023</p>
<p>Universal Neurons in GPT2 Language Models. Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, Dimitris Bertsimas, January 2024. 9CoRR</p>
<p>Recurrent World Models Facilitate Policy Evolution. David R Ha, J Schmidhuber, NeurIPS. September 2018. 9</p>
<p>Let's Agree to Agree. Guy Hacohen, Leshem Choshen, Daphna Weinshall, Neural Networks Share Classification Order on Real Datasets. ICML. 82020</p>
<p>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, NeurIPS. 19262023. 8, 14</p>
<p>Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun, NeurIPS Spotlight. 1516January 2023</p>
<p>A circuit for Python docstrings in a 4-layer attention-only transformer. Stefan Heimersheim, Jett, AI Alignment Forum. 819February 2023</p>
<p>In-Context Learning Creates Task Vectors. Roee Hendel, Mor Geva, Amir Globerson, EMNLP. 20October 2023. 7, 14</p>
<p>Dan Hendrycks, Introduction to AI Safety, Ethics, and Society. Self-published. 202323</p>
<p>Unsolved Problems in ML Safety. Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt, June 2022. 23CoRR</p>
<p>Superposition, Memorization, and Double Descent. Transformer Circuits Thread. Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, Christopher Olah, 2023624</p>
<p>Scaling Laws and Interpretability of Learning from Repeated Data. Danny Hernandez, Tom Brown, Tom Conerly, Nova Dassarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, Sam Mccandlish, 2022CoRR19</p>
<p>Linearity of Relation Decoding in Transformer Language Models. Evan Hernandez, Sen Arnab, Tal Sharma, Kevin Haklay, Martin Meng, Jacob Wattenberg, Yonatan Andreas, David Belinkov, Bau, August 2023. 7CoRR</p>
<p>Towards a Definition of Disentangled Representations. Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, Alexander Lerchner, December 2018CoRR24</p>
<p>Understanding RL Vision. Distill. Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, 202027</p>
<p>. Geoffrey E Hinton, 198424Carnegie Mellon UniversityDistributed representations</p>
<p>Marius' alignment agenda. Marius Hobbhahn, 202226</p>
<p>The Developmental Landscape of In-Context Learning. CoRR. Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, Daniel Murfet, February 202418</p>
<p>Rigorously Assessing Natural Language Explanations of Neurons. Atticus Geiger, D' Karel, Zhengxuan Oosterlinck, Christopher Wu, Potts, September 2023. 4CoRRUnder review as submission to TMLR Jing Huang</p>
<p>Chris Olah's views on AGI safety. AI Alignment Forum. Evan Hubinger, November 2019a. 3, 23</p>
<p>Gradient hacking. AI Alignment Forum. Evan Hubinger, October 2019b. 23</p>
<p>Relaxed adversarial training for inner alignment. Evan Hubinger, AI Alignment Forum. September 2019c. 22</p>
<p>A transparency and interpretability tech tree. AI Alignment Forum. Evan Hubinger, June 2022. 7, 2326</p>
<p>Risks from Learned Optimization in Advanced Machine Learning Systems. Evan Hubinger, Chris Van Merwijk, Vladimir Mikulik, Joar Skalse, Scott Garrabrant, May 2019CoRR27</p>
<p>Conditioning Predictive Models: Risks and Strategies. Evan Hubinger, Adam Jermyn, Johannes Treutlein, Rubi Hudson, Kate Woolverton, February 2023CoRR910</p>
<p>Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte Macdiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova Dassarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, R Samuel, Logan Bowman, Jared Graham, Kaplan, Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan PerezCoRR202425</p>
<p>Adversarial Examples Are Not Bugs, They Are Features. NeurIPS. Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry, August 2019. 31625</p>
<p>. M Ivanitskiy, Alexander F Spies, Tilman Rauker, Guillaume Corlouer, Chris Mathwin, Lucia Quirke, Can Rager, Rusheb Shah, Dan Valentine, Cecilia Diniz Behn, Katsumi Inoue, Samy Wu Fung, Structured World Representations in Maze-Solving Transformers. 2627December 2023. 9CoRR</p>
<p>Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness? CoRR. Alon Jacovi, Yoav Goldberg, April 2020. 25</p>
<p>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks. Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktäschel, David Scott Krueger, November 2023CoRR27</p>
<p>. janus. Simulators. LessWrong. 910September 2022</p>
<p>A comparison of causal scrubbing, causal abstractions, and related methods. Erik Jenner, Adrià Garriga-Alonso, Egor Zverev, AI Alignment Forum. June 2023. 17</p>
<p>Circuits updates -May 2023: Attention Head Superposition. Transformer Circuits Thread. Adam Jermyn, Chris Olah, Henighan, 202325</p>
<p>Engineering Monosemanticity in Toy Models. Adam S Jermyn, Nicholas Schiefer, Evan Hubinger, November 2022. 6CoRR1822</p>
<p>Conditioning Generative Models for Alignment. AI Alignment Forum. Jozdien , July 2022. 9</p>
<p>Evaluating and Interpreting Language Models. Jaap Jumelet, NLP Lecture. November 2023. 2</p>
<p>Theodoros Kasioumis, Joe Townsend, Hiroya Inakoshi, Elite BackProp: Training Sparse Interpretable Neurons. International Workshop on Neuro-Symbolic Learning and Reasoning. 202118</p>
<p>Neuroscience and Natural Abstractions. LessWrong. Jan Kirchner, March 2023. 8</p>
<p>Under review as submission to TMLR Simon Kornblith. Mohammad Norouzi, Honglak Lee, Geoffrey Hinton, Similarity of Neural Network Representations Revisited. ICML. July 2019</p>
<p>AtP*: An efficient and scalable method for localizing LLM behaviour to components. János Kramár, Tom Lieberum, Rohin Shah, Neel Nanda, March 2024CoRR1620</p>
<p>Predictive Minds: LLMs As Atypical Active Inference Agents. Jan Kulveit, Roman Clem Von Stengel, Leventov, November 2023. 9CoRR</p>
<p>Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. Max Lamparth, Anka Reuel, 2023CoRR25</p>
<p>Locating Cross-Task Sequence Continuation Circuits in Transformers. Michael Lan, Fazl Barez, November 2023. 8CoRR</p>
<p>When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning. Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, Scott Emmons, March 2024CoRR24</p>
<p>An Interpretability Illusion for Activation Patching of Arbitrary Subspaces. Georg Lange, Alex Makelov, Neel Nanda, AI Alignment Forum. 15August 2023</p>
<p>DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, Jaap Jumelet, 2023CoRR12</p>
<p>Quantifying degeneracy in singular models via the learning coefficient. Edmund Lau, Daniel Murfet, Susan Wei, August 2023CoRR18</p>
<p>Barriers to Mechanistic Interpretability for AGI Safety. Connor Leahy, AI Alignment Forum. 232023</p>
<p>Towards falsifiable interpretability research. Matthew L Leavitt, Ari Morcos, October 2020CoRR1725</p>
<p>. Kushal Victor Lecomte, Trevor Thaman, Rylan Chow, Sanmi Schaeffer, Koyejo, Incidental Polysemanticity. CoRR. 24252023</p>
<p>A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity. Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, Rada Mihalcea, 2024CoRR27</p>
<p>Efficient sparse coding algorithms. Honglak Lee, Alexis Battle, Rajat Raina, Andrew Ng, NeurIPS. 122006</p>
<p>Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, ICLR. 792023a</p>
<p>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, NeurIPS Spotlight. July 2023b. 7</p>
<p>Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, John Hopcroft, Convergent Learning: Do different neural networks learn the same representations? NIPS Workshop on Feature Extraction. December 2015</p>
<p>Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik, July 2023. 8, 14, 19, 20CoRR2526</p>
<p>. David Lindner, János Kramár, Sebastian Farquhar, Matthew Rahtz, Thomas Mcgrath, Vladimir Mikulik, Tracr, 2023Compiled Transformers as a Laboratory for Interpretability. CoRR1625</p>
<p>Under review as submission to. Z Tmlr Leo, Yizhong Liu, Jungo Wang, Hannaneh Kasai, Noah A Hajishirzi, Smith, Probing Across Time: What Does RoBERTa Know and When? EMNLP. September 2021. 26</p>
<p>A Neural Scaling Law from Lottery Ticket Ensembling. Ziming Liu, Max Tegmark, 2023CoRR19</p>
<p>Towards Understanding Grokking: An Effective Theory of Representation Learning. Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J Michaud, Max Tegmark, Mike Williams, NeurIPS. 2022a. 19</p>
<p>Ziming Liu, Eric J Michaud, Max Tegmark, Omnigrok, Grokking Beyond Algorithmic Data. ICML. 2022b. 19</p>
<p>Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability. Ziming Liu, Eric Gan, Max Tegmark, Entropy. 26June 2023a. 18, 20</p>
<p>Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks. Ziming Liu, Mikail Khona, Ila R Fiete, Max Tegmark, 2023bCoRR18</p>
<p>Grokking as compression: A nonlinear complexity perspective. Ziming Liu, Ziqian Zhong, Max Tegmark, 2023c. 19CoRR</p>
<p>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem, June 2019. 24CoRR</p>
<p>Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. Luke Marks, Amir Abdullah, Luna Mendez, Rauno Arike, Philip Torr, Fazl Barez, October 2023CoRR</p>
<p>Understanding polysemanticity in neural networks through coding theory. Simon C Marshall, Jan H Kirchner, January 2024CoRR425</p>
<p>How Hard is Trojan Detection in DNNs? Fooling Detectors With Evasive Trojans. Mantas Mazeika, Andy Zou, Akul Arora, Pavel Pleskov, Dawn Song, Dan Hendrycks, Bo Li, David Forsyth, September 2022CoRR16</p>
<p>Copy Suppression: Comprehensively Understanding an Attention Head. Callum Mcdougall, Arthur Conmy, Cody Rushing, Thomas Mcgrath, Neel Nanda, October 2023CoRR</p>
<p>Acquisition of chess knowledge in AlphaZero. Thomas Mcgrath, Andrei Kapishnikov, Nenad Tomašev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, Vladimir Kramnik, 10.1073/pnas.2206625119November 2022PNAS1123</p>
<p>The Hydra Effect: Emergent Self-repair in Language Model Computations. Thomas Mcgrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, Shane Legg, July 2023. 15CoRR1625</p>
<p>Locating and Editing Factual Associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, NeurIPS. 13172022a</p>
<p>. Kevin Meng, Sen Arnab, Alex Sharma, Yonatan Andonian, David Belinkov, Bau, Mass-Editing Memory in a Transformer. ICLR. 162022b</p>
<p>A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks. William Merrill, Nikolaos Tsilivis, Aman Shukla, 2023CoRR19</p>
<p>A Mechanism for Solving Relational Tasks in Transformer Language Models. Jack Merullo, Carsten Eickhoff, Ellie Pavlick, May 2023. 26CoRR</p>
<p>The Quantization Model of Neural Scaling. Eric J Michaud, Ziming Liu, Uzay Girit, Max Tegmark, March 2023. 7, 19CoRR26</p>
<p>Opening the AI black box: program synthesis via mechanistic interpretability. Eric J Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide, Chloe Loughridge, Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukelić, Max Tegmark, February 2024CoRR2124Under review as submission to TMLR</p>
<p>Distributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, NeurIPS. 7October 2013</p>
<p>We Found An Neuron in GPT-2. AI Alignment Forum. Joseph Miller, Clement Neo, February 2023. 19</p>
<p>Explanation in artificial intelligence: Insights from the social sciences. Tim Miller, Artificial Intelligence. 16February 2019</p>
<p>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia. Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre Kıcıman, Hamid Palangi, Barun Patra, Robert West, 2023CoRR26</p>
<p>Can LLMs facilitate interpretation of pre-trained language models?. Basel Mousi, Nadir Durrani, Fahim Dalvi, EMNLP. 202023</p>
<p>Compositional Explanations of Neurons. Jesse Mu, Jacob Andreas, NeurIPS. 416June 2020</p>
<p>Evaluating Brain-Inspired Modular Training in Automated Circuit Discovery for Mechanistic Interpretability. Jatin Nainani, January 2024. 20CoRR</p>
<p>Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L Edelman, Fred Zhang, Boaz Barak, SGD on Neural Networks Learns Functions of Increasing Complexity. May 2019. 19</p>
<p>Neel Nanda, 200 COP in MI: Looking for Circuits in the Wild. Neel Nanda's Blog. 2022a26</p>
<p>Neel Nanda, 200 COP in MI: Analysing Training Dynamics. Neel Nanda's Blog. 2022b26</p>
<p>Neel Nanda, 200 COP in MI: Studying Learned Features in Language Models. Neel Nanda's Blog. 2022c26</p>
<p>Neel Nanda, A Comprehensive Mechanistic Interpretability Explainer &amp; Glossary. Neel Nanda's Blog. December 2022d. 1</p>
<p>Neel Nanda, 200 COP in MI: Exploring Polysemanticity and Superposition. Neel Nanda's Blog. January 2023a. 25</p>
<p>Neel Nanda, 200 COP in MI: Techniques, Tooling and Automation. Neel Nanda's Blog. 2023b26</p>
<p>Neel Nanda, Actually, Othello-GPT Has A Linear Emergent World Representation. Neel Nanda's Blog. March 2023c. 7</p>
<p>Neel Nanda, Attribution Patching: Activation Patching At Industrial Scale. Neel Nanda's Blog. February 2023d1516</p>
<p>How to Think About Activation Patching. AI Alignment Forum. Neel Nanda, April 2023e1519</p>
<p>Neel Nanda, Mechanistic Interpretability Quickstart Guide. Neel Nanda's Blog. January 2023f. 1, 26</p>
<p>Progress measures for grokking via mechanistic interpretability. ICLR. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, January 2023a. 7, 192627</p>
<p>Emergent Linear Representations in World Models of Self-Supervised Sequence Models. Neel Nanda, Andrew Lee, Martin Wattenberg, BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 25September 2023b. 7, 9</p>
<p>The alignment problem from a deep learning perspective. Richard Ngo, Lawrence Chan, Sören Mindermann, December 2022CoRR</p>
<p>How Transformers Learn Causal Structure with Gradient Descent. Alex Damian, Jason D Lee, February 2024. 9CoRRUnder review as submission to TMLR Eshaan Nichani</p>
<p>Janus Nicholaskees, Searching for Search. AI Alignment Forum. November 2022. August 20201210 nostalgebraist. interpreting GPT: the logit lens</p>
<p>Distributed Representations: Composition &amp; Superposition. Transformer Circuits Thread. Chris Olah, 202324</p>
<p>. Chris Olah, Shan Carter, Research Debt. Distill. March 2017. 1</p>
<p>. Chris Olah, Adam Jermyn, Reflections on Qualitative Research. Transformer Circuits Thread. 16March 2024</p>
<p>Feature Visualization. Distill. Chris Olah, Alexander Mordvintsev, Ludwig Schubert, November 2017</p>
<p>Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, Alexander Mordvintsev, The Building Blocks of Interpretability. March 2018. 1</p>
<p>. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter, Zoom In: An Introduction to Circuits. Distill. March 2020. 1, 3, 4, 7, 8, 19</p>
<p>Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases. Transformer Circuits Thread. Christopher Olah, 202213</p>
<p>Sparse coding with an overcomplete basis set: a strategy employed by V1? Vision Research. B A Olshausen, D J Field, December 19971213</p>
<p>. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, In-context Learning and Induction Heads. Transformer Circuits Thread. Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah188262022</p>
<p>Disentangling Neuron Representations with Concept Vectors. Laura O' Mahony, Vincent Andrearczyk, Henning Muller, Mara Graziani, CVPR Workshops. April 2023. 7</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022CoRR27</p>
<p>Future Lens: Anticipating Subsequent Tokens from a Single Hidden State. CoNLL. Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wallace, David Bau, 20231226</p>
<p>Towards vision-language mechanistic interpretability: A causal tracing tool for BLIP. ICCVW. Vedant Palit, Rohan Pandey, Aryaman Arora, P Liang, 202327</p>
<p>The Linear Representation Hypothesis and the Geometry of Large Language Models. NeurIPS Workshop on Causal Representation Learning. Kiho Park, Yo Joong Choe, Victor Veitch, November 202325</p>
<p>. Judea Pearl, Causality, 2009Cambridge University Press916</p>
<p>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, January 2022. 19CoRR</p>
<p>Understanding Addition in Transformers. Philip Quirke, Fazl Barez, October 2023CoRR726</p>
<p>Increasing Trust in Language Models through the Reuse of Verified Circuits. Philip Quirke, Clement Neo, Fazl Barez, February 2024. 26CoRR</p>
<p>Under review as submission to TMLR Shauli Ravfogel. Hila Gonen, Michael Twiton, and Yoav Goldberg. Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. ACL. Yanai ElazarJuly 2020. 17</p>
<p>Defining and Quantifying the Emergence of Sparse Concepts in DNNs. Jie Ren, Mingjie Li, Qirui Chen, Huiqi Deng, Quanshi Zhang, April 2023CoRR24</p>
<p>Why Should I Trust You?": Explaining the Predictions of Any Classifier. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, NAACL. 217August 2016</p>
<p>AGI-Automated Interpretability is Suicide. LessWrong. May 2023. 26</p>
<p>Robust agents learn causal world models. ICLR Oral. Jonathan Richens, Tom Everitt, February 2024. 9</p>
<p>Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, Shajith Ikbal, Hima Karanam, Sumit Neelam, Ankita Likhyani, and Santosh Srivastava. June 20201824</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, Nature Machine Intelligence. 16May 2019</p>
<p>Internal Interfaces Are a High-Priority Interpretability Target. AI Alignment Forum. Thane Ruthenis, December 2022. 9</p>
<p>World-Model Interpretability Is All We Need. AI Alignment Forum. Thane Ruthenis, January 2023. 9</p>
<p>Tilman Räuker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell, Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. TMLR. August 2023. 1, 111625</p>
<p>Neuron-level Interpretation of Deep NLP Models: A Survey. TACL. Hassan Sajjad, Nadir Durrani, Fahim Dalvi, 10.1162/tacl_a_00519/113852/Neuron-level-Interpretation-of-Deep-NLP-Models-ANovember 2022. 4</p>
<p>Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster, September 2023a. 7CoRR</p>
<p>Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster, October 2023b. 12CoRR</p>
<p>Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective. Emmanuelle Salin, Badreddine Farah, Stéphane Ayache, Benoit Favre, June 2022AAAI27</p>
<p>. Tommaso Salvatori, Ankur Mali, Christopher L Buckley, Thomas Lukasiewicz, P N Rajesh, Karl Rao, Alexander Friston, Ororbia, Brain-Inspired Computational Intelligence via Predictive Coding. CoRR. 92023</p>
<p>Interpretability Creationism. The Gradient. Naomi Saphra, 202319</p>
<p>Are Emergent Abilities of Large Language Models a Mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, May 2023CoRR18</p>
<p>Polysemanticity and Capacity in Neural Networks. Adam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, Buck Shlegeris, July 2023. 5, 6, 23CoRR24</p>
<p>Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradientbased Localization. R Ramprasaath, Abhishek Selvaraju, Ramakrishna Das, Michael Vedantam, Devi Cogswell, Dhruv Parikh, Batra, ICCV. 22016</p>
<p>2 Under review as submission to TMLR Lee Sharkey. Circumventing interpretability: How to defeat mind-readers. Lloyd S Shapley, October 1988. December 2022CoRR23A value for n -person games</p>
<p>A technical note on bilinear layers for interpretability. Lee Sharkey, May 20231822</p>
<p>Current themes in mechanistic interpretability research. Lee Sharkey, Sid Black, Beren , AI Alignment Forum. November 2022a. 1, 7</p>
<p>Taking features out of superposition with sparse autoencoders. Lee Sharkey, Dan Braun, Beren Millidge, AI Alignment Forum. 126132022b</p>
<p>Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, R Samuel, Newton Bowman, Esin Cheng, Zac Durmus, Scott R Hatfield-Dodds, Shauna Johnston, Timothy Kravec, Sam Maxwell, Kamal Mccandlish, Oliver Ndousse, Nicholas Rausch, Da Schiefer, Miranda Yan, Ethan Zhang, Perez, Towards Understanding Sycophancy in Language Models. CoRROctober 202324</p>
<p>Learning Important Features Through Propagating Activation Differences. Avanti Shrikumar, Peyton Greenside, Anshul Kundaje, ICML. 22017</p>
<p>On the Stepwise Nature of Self-Supervised Learning. ICML. James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, Joshua Albrecht, May 202318</p>
<p>SmoothGrad: removing noise by adding noise. Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg, June 2017. 2CoRR</p>
<p>Dropout: A Simple Way to. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Prevent Neural Networks from Overfitting. JMLR. 42014</p>
<p>Grokking Group Multiplication with Cosets. Dashiell Stander, Qinan Yu, Honglu Fan, Stella Biderman, 2023CoRR19</p>
<p>Emergent Deception and Emergent Optimization. Bounded Regret. Jacob Steinhardt, February 20231824</p>
<p>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, October 2023EMNLP1420</p>
<p>. Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C Love, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B Tenenbaum, Katherine M Collins, Katherine L Hermann, Kerem Oktar, Klaus Greff, Martin N Hebart, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P O'connell, Thomas Unterthiner, Andrew K Lampinen, Klaus-Robert Müller, Mariya Toneva, Thomas L Griffiths, November 2023CoRR</p>
<p>Axiomatic Attribution for Deep Networks. ICML. Mukund Sundararajan, Ankur Taly, Qiqi Yan, June 2017. 2</p>
<p>Attribution Patching Outperforms Automated Circuit Discovery. Aaquib Syed, Can Rager, Arthur Conmy, October 2023CoRR1520</p>
<p>Provably safe systems: the only path to controllable AGI. Max Tegmark, Steve Omohundro, September 2023. 26CoRR</p>
<p>Ian Tenney, Dipanjan Das, Ellie Pavlick, BERT Rediscovers the Classical NLP Pipeline. ACL. August 2019. 11</p>
<p>The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon. Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, Joshua Susskind, 2022CoRR19</p>
<p>Linear Representations of Sentiment in Large Language Models. Curt Tigges, John Oskar, Atticus Hollinsworth, Neel Geiger, Nanda, October 2023. 7CoRR</p>
<p>Function Vectors in Large Language Models. Eric Tmlr, Millicent L Todd, Li, Sen Arnab, Aaron Sharma, Byron C Mueller, David Wallace, Bau, 2023CoRRUnder review as submission to</p>
<p>Learning to Synthesize Programs as Interpretable and Generalizable Policies. Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, Joseph J Lim, NeurIPS. 242021</p>
<p>Activation Addition: Steering Language Models Without Optimization. Matt Alexander, Lisa Turner, David Thiergart, Gavin Udell, Ulisse Leech, Monte Mini, Macdiarmid, September 2023. 7CoRR</p>
<p>Understanding and controlling a mazesolving policy network. Peligrietzer Turntrout, Ulisse Mini, David Udell, AI Alignment Forum. 27November 2023</p>
<p>Toward A Mathematical Framework for Computation in Superposition. Dmitry Vaintrob, Kaarel, 20242526AI Alignment Forum</p>
<p>Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models. Alexandre Variengien, Eric Winsor, December 2023. 9CoRR2026</p>
<p>Explaining grokking through circuit efficiency. Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar, September 2023. 19CoRR</p>
<p>Imitation-Projected Programmatic Reinforcement Learning. Abhinav Verma, M Hoang, Yisong Le, Swarat Yue, Chaudhuri, NeurIPS. 242019a</p>
<p>. Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, Swarat Chaudhuri, April 2019b. 24CoRRProgrammatically Interpretable Reinforcement Learning</p>
<p>Investigating Gender Bias in Language Models Using Causal Mediation Analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, NeurIPS. 13172020</p>
<p>Analyzing vision transformers for image classification in class embedding space. M Vilas, Timothy Schaumlöffel, Gemma Roig, CoRR. 272023</p>
<p>Information-Theoretic Probing with Minimum Description Length. Elena Voita, Ivan Titov, March 2020. 11EMNLP</p>
<p>Neurons in Large Language Models: Dead, N-gram. Elena Voita, Javier Ferrando, Christoforos Nalmpantis, September 2023. 4Positional. CoRR</p>
<p>Uncovering mesa-optimization algorithms in Transformers. Eyvind Johannes Von Oswald, Maximilian Niklasson, Seijin Schlegel, Nicolas Kobayashi, Nino Zucchet, Nolan Scherrer, Mark Miller, Sandler, September 2023. 23CoRRBlaise Agüera y Arcas, Max Vladymyrov, Razvan Pascanu, and João Sacramento</p>
<p>. Chelsea Voss, Gabriel Goh, Nick Cammarata, Michael Petrov, Ludwig Schubert, Chris Olah, Branch Specialization. Distill. 819April 2021</p>
<p>Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, ICLR. 14272023. 8, 13</p>
<p>BLiMP: The Benchmark of Linguistic Minimal Pairs for English. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, Samuel R Bowman, 2020Transactions of the Association for Computational Linguistics</p>
<p>Algebraic Geometry and Statistical Learning Theory. Sumio Watanabe, Cambridge Monographs on Applied and Computational Mathematics. 182009Cambridge University Press</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, October 2022TMLR18</p>
<p>Under review as submission to TMLR John Wentworth. Just Retarget The Search. AI Alignment Forum. August 2022. 9How To Go From Interpretability To Alignment</p>
<p>Disentangling with Biological Constraints: A Theory of Functional Cell Types. C R James, Will Whittington, Surya Dorrell, Timothy E J Ganguli, Behrens, September 2022. 12CoRR</p>
<p>From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, June 2023. 22CoRR</p>
<p>Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Chao Shen, Back-doorBench: A Comprehensive Benchmark of Backdoor Learning. NeurIPS Datasets and Benchmarks. October 2022. 25</p>
<p>Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. Zhengxuan Wu, Atticus Geiger, Christopher Potts, Noah D Goodman, May 2023. 17CoRR1921</p>
<p>Characterizing Mechanisms for Factual Recall in Language Models. Qinan Yu, Jack Merullo, Ellie Pavlick, October 2023. 8CoRR</p>
<p>Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. Zeyu Yun, Yubei Chen, Bruno A Olshausen, Yann Lecun, NAACL Workshop DeeLIO. 122021</p>
<p>Visualizing and Understanding Convolutional Networks. Matthew D Zeiler, Rob Fergus, 2014ECCV</p>
<p>Biao Zhang, Ivan Titov, Rico Sennrich, Sparse Attention with Linear Units. EMNLP. October 202118</p>
<p>Understanding deep learning requires rethinking generalization. ICLR. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, February 2017. 19</p>
<p>Towards Best Practices of Activation Patching in Language Models. Fred Zhang, Neel Nanda, 2023Metrics and Methods. ICLR15</p>
<p>Interpreting CNNs via Decision Trees. CVPR. Quanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu, 201924</p>
<p>The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks. Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas, 2023CoRR19</p>
<p>Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J Zico Kolter, Dan Hendrycks, Representation Engineering: A Top-Down Approach to AI Transparency. CoRROctober 2023. 2, 7, 1120</p>            </div>
        </div>

    </div>
</body>
</html>
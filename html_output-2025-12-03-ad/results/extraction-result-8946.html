<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8946 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8946</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8946</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a938ff4539b09a785a66669844f1a35f76169218</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a938ff4539b09a785a66669844f1a35f76169218" target="_blank">PEER: A Collaborative Language Model</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work introduces PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions.</p>
                <p><strong>Paper Abstract:</strong> Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8946.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8946.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan, Edit, Explain, Repeat (PEER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collaborative encoder-decoder language model (T5-adapt) trained on Wikipedia edit histories to perform iterative editing: it generates a short plan, performs an edit (realization), optionally produces an explanation, and repeats this cycle until satisfied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PEER-Edit (PEER)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder model initialized from LM-Adapted T5 (default experiments use the 3B-parameter variant; an 11B variant was also evaluated); trained on ~5M Wikipedia edits (20k steps, batch size equivalent). Inputs include the current text x_t plus up to k=3 retrieved reference documents; outputs are plan + updated text.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Plan → Edit → Explain → Repeat (iterative editing)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At each iteration the model (a) generates a short natural-language plan p_t describing the intended change, (b) realizes the plan by generating an edit x_{t+1}, (c) optionally emits an explanation e_t referencing documents D_t, and (d) repeats. The plan can be human-provided or model-generated; documents are used as background. This decomposes generation into multiple small generate→reflect→edit cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Natural Edits / downstream editing tasks / Wikipedia intro generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural Edits: a suite of naturally occurring edits from Wikipedia, Wikinews and various StackExchange subforums (plans taken from edit comments). Downstream tasks include grammatical error correction (JFLEG), sentence simplification (ASSET), bias removal (WNC), Wikipedia updates (FRUIT), insertions using documents (WAFER-INS), and Wikipedia intro generation (write-from-scratch using reference docs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>On Natural Edits (Wikipedia subset), PEER with gold plans achieved EM 23.1, EM-Diff 26.2 and SARI 55.5 (Table 2). On downstream tasks, PEER (3B) average SARI = 42.0 across tasks (Table 4); on Wikipedia intro generation PEER (manual/collaborative modes) got slightly higher Rouge and substantially higher QuestEval than the non-iterative baseline (QuestEval: PEER ~41.0 vs WikiLM 38.7, Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>On the Wikipedia subset, PEER without plans (i.e., removing the plan signal) had EM 18.0, EM-Diff 19.8 and SARI 52.0 (Table 2). On downstream tasks, PEER (SP) vs PEER (no synthetic plans) shows mean SARI improvement (PEER 42.0 → PEER (SP) 43.6). WikiLM (single-pass conditional LM) produced better Rouge in autonomous generation in that specific setting, but PEER produced higher QuestEval (faithfulness) scores (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented internally as model-generated natural-language plans and explanations; controlled via control tokens (e.g., instruction vs other, length, overlap=false) and uses retrieved reference documents D_t; encoder-decoder autoregressive generation emits plan+edited text, then optionally explanation; iterative loop either run autonomously or with human-in-the-loop plans.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative ablations demonstrate plans and documents provide complementary gains: removing both plans and documents reduces SARI from 55.5 → 45.9 on Wikipedia Natural Edits (Table 2). Training with synthetic plans (PEER (SP)) raises average downstream SARI from 42.0 → 43.6 (3B) and the 11B PEER (SP) gets 44.3 (Table 4). Iterative collaborative generation improves QuestEval (faithfulness) over single-pass baseline (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PEER can hallucinate unsupported claims (qualitative example: inventing a 'scandal'), though it can later remove such claims via a subsequent self-generated plan; iterative application is not uniformly beneficial — some iterative decoding strategies degraded performance on certain tasks (e.g., WNC). Training assumes availability of D_t built with access to x_{t+1} (retrieval used target text), which is unrealistic in real-world deployment and can bias training. Representing edits by rewriting entire paragraphs is inefficient and limits scaling to full documents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to instruction-finetuned T5-derived models (T0, T0++, Tk-Instruct), PEER (SP) yields substantially higher mean SARI on evaluated editing tasks (e.g., avg SARI 43.6 vs best T5-based baseline ~24.1). Compared to large decoder-only models: InstructGPT approaches or exceeds PEER on tasks not requiring documents but underperforms when handling reference documents; OPT/GPT-3 (no instruction finetune) performed worse. PEER's iterative scheme aligns conceptually with chain-of-thought/planning methods but is specialized to editing with explicit plan and explanation stages.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation: removing plans reduces Wikipedia SARI 55.5 → 52.0; removing documents reduces SARI 55.5 → 51.7; removing both reduces SARI to 45.9 (Table 2). Using synthetic plans (PEER (SP)) improves downstream average SARI from 42.0 → 43.6 (Table 4). Domain-adaptation using PEER-Undo synthetic edits (PEER DA) substantially improves EM-Diff/SARI across domains (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEER: A Collaborative Language Model', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8946.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8946.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEER-Infilling Self-Training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEER infilling-based self-training (PEER-Undo / PEER-Explain / PEER-Document)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training pipeline in which auxiliary PEER models infill missing parts of edit histories: PEER-Undo predicts prior text and plan (backwards edit), PEER-Explain rewrites/generates plans/explanations, and PEER-Document generates synthetic reference documents; these outputs are used to synthesize training examples for PEER-Edit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PEER-Undo / PEER-Explain / PEER-Document (components of self-training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder models (same architecture family as PEER-Edit) trained on edit-history-derived supervision to perform complementary infilling tasks: Undo (predict previous state and plan), Explain (generate explanations/plans from paired edits and documents), Document (generate supporting documents given source/target and plan).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative infilling self-training (undo→explain→edit→repeat)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>PEER-Undo is applied to plain texts to stochastically generate prior edits (p_t, x_t) by sampling top-p; PEER-Explain generates candidate plans which are scored by PEER-Edit's likelihood for selecting the best plan; PEER-Document generates candidate documents which are scored by PEER-Edit and the best is used. Synthetic examples are used to train or domain-adapt PEER-Edit.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Domain adaptation for Natural Edits and downstream editing generalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Create synthetic edit histories for domains lacking natural edit logs (e.g., Wikinews, StackExchange subforums) to fine-tune PEER for in-domain editing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Domain-adapted PEER (trained with synthetic edits from PEER-Undo) improved performance across multiple domains (Table 3): e.g., Wikinews EM-Diff/SARI improved from 21.3/49.3 (PEER) → 23.3/51.6 (PEER DA); Gardening from 4.4/37.7 → 8.1/44.9 (EM-Diff/SARI); Movies from 6.7/39.2 → 9.9/42.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Regular PEER (no synthetic infilling DA) performed worse on non-Wikipedia domains (see the baseline PEER numbers in Table 3 listed above).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Self-training loop using model-generated backward edits (PEER-Undo), candidate plans (PEER-Explain) and candidate documents (PEER-Document); selection of synthetic items is done by scoring candidate outputs by PEER-Edit's likelihood of the true edit (pick candidate that maximizes p(x_{t+1} | x_t, D_t, candidate)). Sampling uses top-p; controls applied via control tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Domain-adapted PEER (DA) trained with a single PEER-Undo step per plain text and rewriting plans with PEER-Explain produced substantial relative EM-Diff gains (e.g., Gardening EM-Diff +84%; Politics +71%; Movies +48% relative to PEER) and consistent SARI gains across multiple domains (Table 3). PEER (SP) (trained with synthetic plans) improved downstream average SARI relative to PEER.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Synthetic documents sometimes contain repetitions and artifacts; generated plans initially may not match undone edits and therefore are rewritten with PEER-Explain; quality of synthetic data depends on PEER-Undo/Explain/Document fidelity. The retrieval pipeline used at training relies on access to the target x_{t+1} (unrealistic at inference), so synthetic D_t may not reflect realistic retrieval noise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Relates to classic self-training/backtranslation/round-trip approaches (cited He et al. 2020a, Yarowsky 1995, Sennrich et al. 2016). The paper demonstrates that infilling-style self-training can extend editing capabilities to new domains and improve instruction-following compared to training only on raw Wikipedia edit logs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Using PEER-Undo to synthesize edits and then domain-adapting PEER yields consistent improvements across all Natural Edits subsets (Table 3). Rewriting plans with PEER-Explain was necessary because original generated plans often did not match undone edit; selecting best synthetic plans/documents by likelihood of the target edit is part of the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEER: A Collaborative Language Model', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8946.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8946.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative decoding (i3 / s-i3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative application of PEER with greedy (i3) or sampling (s-i3) decoding for multiple edit iterations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explicitly running PEER multiple times (three iterations in experiments) to repeatedly propose and apply edits; different decoding/stochasticity strategies (greedy vs top-p sampling) produce different iterative behaviors and outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PEER (SP) 3B iterative variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PEER-Edit (3B) finetuned with synthetic plans (SP) and executed in a multi-pass decoding loop. Two strategies evaluated: greedy decoding repeated 3 times (i3) and top-p sampling (p=0.9) repeated 3 times (s-i3).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-iteration generate→edit cycles via repeated decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Run the PEER model for N=3 iterations; in each iteration the model generates a plan and an edit; decoding strategy is either greedy (i3) or stochastic top-p sampling (s-i3).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Downstream editing tasks (JFLEG, ASSET, IteraTER, WNC, FRUIT, WAFER-INS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A battery of editing benchmarks covering grammatical error correction, sentence simplification, iterative revision, bias mitigation, factual updates with documents and sentence insertion with documents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Examples (Table 4): PEER (SP, i3) improved JFLEG (SARI/GLEU style) to 63.3/59.6 (vs PEER (SP) 59.0/57.2) and ASSET SARI from 33.2 → 36.1. However, some tasks dropped: WNC SARI decreased from 56.6 → 45.2. Overall average score for PEER (SP,i3) was 43.1 vs PEER (SP) 43.6 (3B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-pass PEER (SP) (no iterative looping) reported average SARI 43.6 (3B) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Repeated autoregressive generation where the model uses its own prior output as the next input (i.e., x_{t+1} becomes x_t for next iteration); control via decoding method (greedy or sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Task-dependent evidence: iterative greedy (i3) improved performance on some tasks (e.g., JFLEG, ASSET) but substantially hurt on others (e.g., WNC). The paper reports per-task SARI (and other metrics) showing such mixed effects (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Iterative application is unstable and not uniformly beneficial; selecting a decoding strategy is nontrivial and task-dependent. The authors note automatic selection of decoding strategy as important future work. Iteration can amplify undesirable model behaviors if not controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>This is an instantiation of iterative refinement, contrasted to single-pass conditional generation; the paper links the approach to other iterative editing/unsupervised iterative improvement literature but does not present head-to-head comparisons to other named multi-pass self-improvement frameworks (e.g., Reflexion, self-critique) beyond general discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Direct comparison in Table 4 between PEER (SP) (single pass) and PEER (SP, i3 / s-i3) (3 iterations) serves as an ablation showing when iteration helps vs hurts; no systematic ablation over number of iterations beyond the 3-iteration experiments was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEER: A Collaborative Language Model', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8946.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8946.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous self-correction (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-authored plan-based self-correction during iterative editing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qualitative phenomenon where PEER autonomously writes a plan to remove an earlier hallucinated claim and then executes the removal in a subsequent edit, demonstrating an instance of model self-reflection and correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PEER (SP, 11B) and PEER (DA, 3B) (demonstrated qualitatively)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PEER variants (11B and domain-adapted 3B) used in interactive sessions: given documents and/or human plans, PEER proposes plans and edits over multiple iterations; the example uses the paper's own provided documents.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Model-authored planning and remedy (single-step self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>PEER, after producing an output that contains a hallucinated or unsupported claim, autonomously generates a subsequent plan (e.g., 'remove unsourced claim') and applies an edit removing the claim, i.e., it inspects its prior output, produces a corrective plan, and realizes the correction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive collaborative editing (qualitative case study: Wikipedia-style intro)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Manually constructed interactive session where human and model plans interleave; documents provided are snippets from the paper (author list, abstract, figure caption).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: PEER self-generated the plan 'remove unsourced claim' and removed an earlier invented 'scandal' in the next iteration; no numeric metric is provided for this behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A (qualitative example only).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model inspects its produced output implicitly (via feeding the current x_t back as input for next iteration), writes a natural-language plan describing the corrective action, then emits an edit that implements the correction.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Single documented qualitative example where a hallucinated assertion was removed in a subsequent model-authored-planned edit. This demonstrates the capacity for self-directed correction within iterative editing loops.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although the model can correct its own hallucinations in later steps, it can still produce false/unverified claims in earlier steps; citations can make hallucinations appear authoritative. The behavior is shown qualitatively but not measured at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>This is conceptually similar to self-critique or generate-then-revise approaches, but implemented within the PEER editing loop (explicit plan + edit + optional explanation); no direct numerical comparison to external generate-then-reflect literature methods is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEER: A Collaborative Language Model', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to model editing processes <em>(Rating: 2)</em></li>
                <li>Text editing by command <em>(Rating: 2)</em></li>
                <li>Understanding iterative revision from humanwritten text <em>(Rating: 2)</em></li>
                <li>Revisiting self-training for neural sequence generation <em>(Rating: 2)</em></li>
                <li>Enabling language models to fill in the blanks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8946",
    "paper_id": "paper-a938ff4539b09a785a66669844f1a35f76169218",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "PEER",
            "name_full": "Plan, Edit, Explain, Repeat (PEER)",
            "brief_description": "A collaborative encoder-decoder language model (T5-adapt) trained on Wikipedia edit histories to perform iterative editing: it generates a short plan, performs an edit (realization), optionally produces an explanation, and repeats this cycle until satisfied.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PEER-Edit (PEER)",
            "model_description": "Encoder-decoder model initialized from LM-Adapted T5 (default experiments use the 3B-parameter variant; an 11B variant was also evaluated); trained on ~5M Wikipedia edits (20k steps, batch size equivalent). Inputs include the current text x_t plus up to k=3 retrieved reference documents; outputs are plan + updated text.",
            "reflection_method_name": "Plan → Edit → Explain → Repeat (iterative editing)",
            "reflection_method_description": "At each iteration the model (a) generates a short natural-language plan p_t describing the intended change, (b) realizes the plan by generating an edit x_{t+1}, (c) optionally emits an explanation e_t referencing documents D_t, and (d) repeats. The plan can be human-provided or model-generated; documents are used as background. This decomposes generation into multiple small generate→reflect→edit cycles.",
            "task_name": "Natural Edits / downstream editing tasks / Wikipedia intro generation",
            "task_description": "Natural Edits: a suite of naturally occurring edits from Wikipedia, Wikinews and various StackExchange subforums (plans taken from edit comments). Downstream tasks include grammatical error correction (JFLEG), sentence simplification (ASSET), bias removal (WNC), Wikipedia updates (FRUIT), insertions using documents (WAFER-INS), and Wikipedia intro generation (write-from-scratch using reference docs).",
            "performance_with_reflection": "On Natural Edits (Wikipedia subset), PEER with gold plans achieved EM 23.1, EM-Diff 26.2 and SARI 55.5 (Table 2). On downstream tasks, PEER (3B) average SARI = 42.0 across tasks (Table 4); on Wikipedia intro generation PEER (manual/collaborative modes) got slightly higher Rouge and substantially higher QuestEval than the non-iterative baseline (QuestEval: PEER ~41.0 vs WikiLM 38.7, Table 6).",
            "performance_without_reflection": "On the Wikipedia subset, PEER without plans (i.e., removing the plan signal) had EM 18.0, EM-Diff 19.8 and SARI 52.0 (Table 2). On downstream tasks, PEER (SP) vs PEER (no synthetic plans) shows mean SARI improvement (PEER 42.0 → PEER (SP) 43.6). WikiLM (single-pass conditional LM) produced better Rouge in autonomous generation in that specific setting, but PEER produced higher QuestEval (faithfulness) scores (Table 6).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented internally as model-generated natural-language plans and explanations; controlled via control tokens (e.g., instruction vs other, length, overlap=false) and uses retrieved reference documents D_t; encoder-decoder autoregressive generation emits plan+edited text, then optionally explanation; iterative loop either run autonomously or with human-in-the-loop plans.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative ablations demonstrate plans and documents provide complementary gains: removing both plans and documents reduces SARI from 55.5 → 45.9 on Wikipedia Natural Edits (Table 2). Training with synthetic plans (PEER (SP)) raises average downstream SARI from 42.0 → 43.6 (3B) and the 11B PEER (SP) gets 44.3 (Table 4). Iterative collaborative generation improves QuestEval (faithfulness) over single-pass baseline (Table 6).",
            "limitations_or_failure_cases": "PEER can hallucinate unsupported claims (qualitative example: inventing a 'scandal'), though it can later remove such claims via a subsequent self-generated plan; iterative application is not uniformly beneficial — some iterative decoding strategies degraded performance on certain tasks (e.g., WNC). Training assumes availability of D_t built with access to x_{t+1} (retrieval used target text), which is unrealistic in real-world deployment and can bias training. Representing edits by rewriting entire paragraphs is inefficient and limits scaling to full documents.",
            "comparison_to_other_methods": "Compared to instruction-finetuned T5-derived models (T0, T0++, Tk-Instruct), PEER (SP) yields substantially higher mean SARI on evaluated editing tasks (e.g., avg SARI 43.6 vs best T5-based baseline ~24.1). Compared to large decoder-only models: InstructGPT approaches or exceeds PEER on tasks not requiring documents but underperforms when handling reference documents; OPT/GPT-3 (no instruction finetune) performed worse. PEER's iterative scheme aligns conceptually with chain-of-thought/planning methods but is specialized to editing with explicit plan and explanation stages.",
            "ablation_study_results": "Ablation: removing plans reduces Wikipedia SARI 55.5 → 52.0; removing documents reduces SARI 55.5 → 51.7; removing both reduces SARI to 45.9 (Table 2). Using synthetic plans (PEER (SP)) improves downstream average SARI from 42.0 → 43.6 (Table 4). Domain-adaptation using PEER-Undo synthetic edits (PEER DA) substantially improves EM-Diff/SARI across domains (Table 3).",
            "uuid": "e8946.0",
            "source_info": {
                "paper_title": "PEER: A Collaborative Language Model",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "PEER-Infilling Self-Training",
            "name_full": "PEER infilling-based self-training (PEER-Undo / PEER-Explain / PEER-Document)",
            "brief_description": "A self-training pipeline in which auxiliary PEER models infill missing parts of edit histories: PEER-Undo predicts prior text and plan (backwards edit), PEER-Explain rewrites/generates plans/explanations, and PEER-Document generates synthetic reference documents; these outputs are used to synthesize training examples for PEER-Edit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PEER-Undo / PEER-Explain / PEER-Document (components of self-training)",
            "model_description": "Encoder-decoder models (same architecture family as PEER-Edit) trained on edit-history-derived supervision to perform complementary infilling tasks: Undo (predict previous state and plan), Explain (generate explanations/plans from paired edits and documents), Document (generate supporting documents given source/target and plan).",
            "reflection_method_name": "Iterative infilling self-training (undo→explain→edit→repeat)",
            "reflection_method_description": "PEER-Undo is applied to plain texts to stochastically generate prior edits (p_t, x_t) by sampling top-p; PEER-Explain generates candidate plans which are scored by PEER-Edit's likelihood for selecting the best plan; PEER-Document generates candidate documents which are scored by PEER-Edit and the best is used. Synthetic examples are used to train or domain-adapt PEER-Edit.",
            "task_name": "Domain adaptation for Natural Edits and downstream editing generalization",
            "task_description": "Create synthetic edit histories for domains lacking natural edit logs (e.g., Wikinews, StackExchange subforums) to fine-tune PEER for in-domain editing tasks.",
            "performance_with_reflection": "Domain-adapted PEER (trained with synthetic edits from PEER-Undo) improved performance across multiple domains (Table 3): e.g., Wikinews EM-Diff/SARI improved from 21.3/49.3 (PEER) → 23.3/51.6 (PEER DA); Gardening from 4.4/37.7 → 8.1/44.9 (EM-Diff/SARI); Movies from 6.7/39.2 → 9.9/42.4.",
            "performance_without_reflection": "Regular PEER (no synthetic infilling DA) performed worse on non-Wikipedia domains (see the baseline PEER numbers in Table 3 listed above).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Self-training loop using model-generated backward edits (PEER-Undo), candidate plans (PEER-Explain) and candidate documents (PEER-Document); selection of synthetic items is done by scoring candidate outputs by PEER-Edit's likelihood of the true edit (pick candidate that maximizes p(x_{t+1} | x_t, D_t, candidate)). Sampling uses top-p; controls applied via control tokens.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Domain-adapted PEER (DA) trained with a single PEER-Undo step per plain text and rewriting plans with PEER-Explain produced substantial relative EM-Diff gains (e.g., Gardening EM-Diff +84%; Politics +71%; Movies +48% relative to PEER) and consistent SARI gains across multiple domains (Table 3). PEER (SP) (trained with synthetic plans) improved downstream average SARI relative to PEER.",
            "limitations_or_failure_cases": "Synthetic documents sometimes contain repetitions and artifacts; generated plans initially may not match undone edits and therefore are rewritten with PEER-Explain; quality of synthetic data depends on PEER-Undo/Explain/Document fidelity. The retrieval pipeline used at training relies on access to the target x_{t+1} (unrealistic at inference), so synthetic D_t may not reflect realistic retrieval noise.",
            "comparison_to_other_methods": "Relates to classic self-training/backtranslation/round-trip approaches (cited He et al. 2020a, Yarowsky 1995, Sennrich et al. 2016). The paper demonstrates that infilling-style self-training can extend editing capabilities to new domains and improve instruction-following compared to training only on raw Wikipedia edit logs.",
            "ablation_study_results": "Using PEER-Undo to synthesize edits and then domain-adapting PEER yields consistent improvements across all Natural Edits subsets (Table 3). Rewriting plans with PEER-Explain was necessary because original generated plans often did not match undone edit; selecting best synthetic plans/documents by likelihood of the target edit is part of the pipeline.",
            "uuid": "e8946.1",
            "source_info": {
                "paper_title": "PEER: A Collaborative Language Model",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Iterative decoding (i3 / s-i3)",
            "name_full": "Iterative application of PEER with greedy (i3) or sampling (s-i3) decoding for multiple edit iterations",
            "brief_description": "Explicitly running PEER multiple times (three iterations in experiments) to repeatedly propose and apply edits; different decoding/stochasticity strategies (greedy vs top-p sampling) produce different iterative behaviors and outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PEER (SP) 3B iterative variants",
            "model_description": "PEER-Edit (3B) finetuned with synthetic plans (SP) and executed in a multi-pass decoding loop. Two strategies evaluated: greedy decoding repeated 3 times (i3) and top-p sampling (p=0.9) repeated 3 times (s-i3).",
            "reflection_method_name": "Multi-iteration generate→edit cycles via repeated decoding",
            "reflection_method_description": "Run the PEER model for N=3 iterations; in each iteration the model generates a plan and an edit; decoding strategy is either greedy (i3) or stochastic top-p sampling (s-i3).",
            "task_name": "Downstream editing tasks (JFLEG, ASSET, IteraTER, WNC, FRUIT, WAFER-INS)",
            "task_description": "A battery of editing benchmarks covering grammatical error correction, sentence simplification, iterative revision, bias mitigation, factual updates with documents and sentence insertion with documents.",
            "performance_with_reflection": "Examples (Table 4): PEER (SP, i3) improved JFLEG (SARI/GLEU style) to 63.3/59.6 (vs PEER (SP) 59.0/57.2) and ASSET SARI from 33.2 → 36.1. However, some tasks dropped: WNC SARI decreased from 56.6 → 45.2. Overall average score for PEER (SP,i3) was 43.1 vs PEER (SP) 43.6 (3B).",
            "performance_without_reflection": "Single-pass PEER (SP) (no iterative looping) reported average SARI 43.6 (3B) (Table 4).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Repeated autoregressive generation where the model uses its own prior output as the next input (i.e., x_{t+1} becomes x_t for next iteration); control via decoding method (greedy or sampling).",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Task-dependent evidence: iterative greedy (i3) improved performance on some tasks (e.g., JFLEG, ASSET) but substantially hurt on others (e.g., WNC). The paper reports per-task SARI (and other metrics) showing such mixed effects (Table 4).",
            "limitations_or_failure_cases": "Iterative application is unstable and not uniformly beneficial; selecting a decoding strategy is nontrivial and task-dependent. The authors note automatic selection of decoding strategy as important future work. Iteration can amplify undesirable model behaviors if not controlled.",
            "comparison_to_other_methods": "This is an instantiation of iterative refinement, contrasted to single-pass conditional generation; the paper links the approach to other iterative editing/unsupervised iterative improvement literature but does not present head-to-head comparisons to other named multi-pass self-improvement frameworks (e.g., Reflexion, self-critique) beyond general discussion.",
            "ablation_study_results": "Direct comparison in Table 4 between PEER (SP) (single pass) and PEER (SP, i3 / s-i3) (3 iterations) serves as an ablation showing when iteration helps vs hurts; no systematic ablation over number of iterations beyond the 3-iteration experiments was reported.",
            "uuid": "e8946.2",
            "source_info": {
                "paper_title": "PEER: A Collaborative Language Model",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Autonomous self-correction (qualitative)",
            "name_full": "Model-authored plan-based self-correction during iterative editing",
            "brief_description": "Qualitative phenomenon where PEER autonomously writes a plan to remove an earlier hallucinated claim and then executes the removal in a subsequent edit, demonstrating an instance of model self-reflection and correction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PEER (SP, 11B) and PEER (DA, 3B) (demonstrated qualitatively)",
            "model_description": "PEER variants (11B and domain-adapted 3B) used in interactive sessions: given documents and/or human plans, PEER proposes plans and edits over multiple iterations; the example uses the paper's own provided documents.",
            "reflection_method_name": "Model-authored planning and remedy (single-step self-correction)",
            "reflection_method_description": "PEER, after producing an output that contains a hallucinated or unsupported claim, autonomously generates a subsequent plan (e.g., 'remove unsourced claim') and applies an edit removing the claim, i.e., it inspects its prior output, produces a corrective plan, and realizes the correction.",
            "task_name": "Interactive collaborative editing (qualitative case study: Wikipedia-style intro)",
            "task_description": "Manually constructed interactive session where human and model plans interleave; documents provided are snippets from the paper (author list, abstract, figure caption).",
            "performance_with_reflection": "Qualitative: PEER self-generated the plan 'remove unsourced claim' and removed an earlier invented 'scandal' in the next iteration; no numeric metric is provided for this behavior.",
            "performance_without_reflection": "N/A (qualitative example only).",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Model inspects its produced output implicitly (via feeding the current x_t back as input for next iteration), writes a natural-language plan describing the corrective action, then emits an edit that implements the correction.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Single documented qualitative example where a hallucinated assertion was removed in a subsequent model-authored-planned edit. This demonstrates the capacity for self-directed correction within iterative editing loops.",
            "limitations_or_failure_cases": "Although the model can correct its own hallucinations in later steps, it can still produce false/unverified claims in earlier steps; citations can make hallucinations appear authoritative. The behavior is shown qualitatively but not measured at scale.",
            "comparison_to_other_methods": "This is conceptually similar to self-critique or generate-then-revise approaches, but implemented within the PEER editing loop (explicit plan + edit + optional explanation); no direct numerical comparison to external generate-then-reflect literature methods is provided.",
            "ablation_study_results": null,
            "uuid": "e8946.3",
            "source_info": {
                "paper_title": "PEER: A Collaborative Language Model",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to model editing processes",
            "rating": 2
        },
        {
            "paper_title": "Text editing by command",
            "rating": 2
        },
        {
            "paper_title": "Understanding iterative revision from humanwritten text",
            "rating": 2
        },
        {
            "paper_title": "Revisiting self-training for neural sequence generation",
            "rating": 2
        },
        {
            "paper_title": "Enabling language models to fill in the blanks",
            "rating": 2
        }
    ],
    "cost": 0.019087,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PEER: A Collaborative Language Model</h1>
<p>Timo Schick ${ }^{\text {® }}$ Jane Dwivedi-Yu ${ }^{\text {® }}$ Zhengbao Jiang ${ }^{\text {® }}{ }^{\text {® }}$ Fabio Petroni ${ }^{\text {® }}$<br>Patrick Lewis ${ }^{\text {® }}$ Gautier Izacard ${ }^{\text {® }}{ }^{\text {A }}$ Qingfei You ${ }^{\text {® }}$ Christoforos Nalmpantis ${ }^{\text {® }}$<br>Edouard Grave ${ }^{\circ}$ Sebastian Riedel ${ }^{\circ}$ ・<br>${ }^{\text {M }}$ Meta AI Research, ${ }^{\circ}$ Carnegie Mellon University,<br>${ }^{\text {A }}$ Inria \&amp; ENS, PSL University, ${ }^{\text {A }}$ University College London<br>{schick, janeyu, zhengbao, fabiopetroni, plewis, gizacard<br>qingfeiyou, christoforos,egrave, sriedel}@fb.com</p>
<h4>Abstract</h4>
<p>Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.</p>
<h2>1 Introduction</h2>
<p>Large neural networks show impressive text generation capabilities when pretrained with a language modeling objective (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Rae et al., 2021; Zhang et al., 2022; Chowdhery et al., 2022, i.a.). However, the way these models operateproducing outputs in a single pass from left to right-differs strongly from the iterative process by which humans typically write texts. This limits their utility for collaborative writing in various
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the steps performed by PEER, our collaborative language model: First, either the user or the model specifies a plan describing the action they want to be performed; this action is then realized by means of an edit. The model can explain the edit both in natural language and by pointing to relevant sources. We can repeat this process until the generated text requires no further updates.
respects; for example, they are not able to retroactively modify or refine their own outputs. Beyond that, they are hard to control (Korbak et al., 2022) and verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020; Shuster et al., 2021; Nakano et al., 2021) and lack the ability to explain their intentions. All of this makes it very difficult for humans to collaborate with such models for writing coherent, factual texts.</p>
<p>To address these shortcomings of existing LMs, we propose PEER (Plan, Edit, Explain, Repeat), a</p>
<p>collaborative language model trained on edit histories to cover the entire writing process. As illustrated in Figure 1, PEER operates in several steps that aim to mirror the human writing process: For a given text, either a user or the model itself can plan an action to be applied, for example by means of a natural language instruction. This plan is then realized by an edit, which the model can explain both in form of a textual comment and by pointing to references used; this is enabled by augmenting each input text with retrieved passages containing potentially relevant background information. We repeat these steps until the text is in a satisfactory state that does not require any further updates. This iterative approach does not only enable the model to decompose the complex task of writing a consistent, factual text into multiple easier subtasks, it also allows humans to intervene at any time and steer the model in the right direction, either by providing it with their own plans and comments or by making edits themselves.</p>
<p>Similar to recent approaches for iterative editing (Faltings et al., 2021; Reid and Neubig, 2022), we use Wikipedia as our main source of edits and associated comments, which we use as proxies for plans and explanations. In contrast to this prior work, however, our goal is to obtain a collaborative model that is useful beyond just Wikipedia: It should be capable of following human-written instructions for updating texts in any domain. To achieve this goal, we train PEER not only to perform the writing process illustrated in Figure 1 in sequential order, but also to infill various parts; for example, given an edited text and a set of relevant documents, we teach it to produce the original version of this text before it was edited. This enables us to use self-training techniques (e.g., Yarowsky, 1995; Sennrich et al., 2016; He et al., 2020a; Schick and Schütze, 2021a) for training PEER with synthetic plans, edits, explanations and documents. We show that this substantially improves PEER along several axes, including its ability to edit texts in any domain, to understand human-written instructions, and to explain its actions.</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>We introduce PEER, a collaborative language model trained primarily on Wikipedia edit histories.</li>
<li>By training PEER to infill parts of the writing process and leveraging self-training techniques, we make it applicable in any domain and enhance several of its core capabilities essential for collaborative writing.</li>
<li>For different tasks related to editing texts, we show that PEER clearly outperforms various baselines and analyze factors leading to its strong performance.</li>
<li>To facilitate further research on collaborative LMs, we release a variety of PEER models as well as the data and code used to train them.</li>
</ul>
<h2>2 Related Work</h2>
<p>Text Editing Similar to our work, Faltings et al. (2021) train an editing model to follow plans on Wikipedia data. However, they only consider single sentence edits, evaluate on Wikipedia data only and do not explore approaches for improving data quality and coverage. Reid and Neubig (2022) also train models on Wikipedia's edit history, but do not consider plans, explanations or reference documents. Several editing models are trained to solve specific tasks, such as updating information (Logan IV et al., 2021), fixing grammar errors (Napoles et al., 2017; Awasthi et al., 2019) or improving citations (Petroni et al., 2022). Various approaches teach models to iteratively improve texts in an unsupervised fashion (e.g., Shen et al., 2020; Donahue et al., 2020; Li et al., 2022) and explore more efficient ways of representing edits (Mallinson et al., 2020). Closely related to our work, Yang et al. (2017) classify edit intentions in Wikipedia's edit history.</p>
<p>Instruction Tuning and Planning Explicitly teaching models to follow plans is closely related to recent work that finetunes models on large datasets of human-written instructions (Wei et al., 2022a; Sanh et al., 2022; Bach et al., 2022; Ouyang et al., 2022; Wang et al., 2022). The idea of having a separate planning stage has also been explored for other text generation tasks inlcuding summarization (Narayan et al., 2021), data-to-text generation (Moryossef et al., 2019) and story writing (Yao et al., 2019). Our approach of writing coherent pieces of text by iteratively performing small updates has some similarity with recent approaches like chain-of-thought prompting (Wei et al., 2022b; Dohan et al., 2022) and document sketching (Wu et al., 2021), that also break down a complex task into multiple smaller steps.</p>
<p>Collaborative Writing Du et al. (2022a,b) investigate human-machine interactions for iteratively improving documents; however, they focus mostly on syntactic edits that improve the fluency, coherence or style of a document. Lee et al. (2022) investigate using GPT3 (Brown et al., 2020) as a writing assistant for creative and argumentative writing. In their setup, however, the model provides suggestions for continuations without being controllable by means of natural language instructions.</p>
<p>Self-Training Our approach of using models to infill missing data and then train other models on this synthetic data closely resembles other self-training and bootstrapping approaches used e.g. in word sense disambiguation (Yarowsky, 1995), machine translation (Sennrich et al., 2016; Hoang et al., 2018), sequence generation (He et al., 2020a), and few-shot learning (Schick and Schütze, 2021a,b). Similar to how we use models to turn plain texts into sequences of edits, Dai et al. (2022) turn documents into dialogue sequences.</p>
<h2>3 Plan, Edit, Explain, Repeat</h2>
<p>The core idea of our proposed framework is to model the editing of textual content as an iterative process, where we repeatedly plan and realize changes (see Figure 2a). Each iteration within this framework edits a text sequence $\mathbf{x}<em t_1="t+1">{t}$ to obtain an updated version $\mathbf{x}</em>}$. For this edit, we assume that we are given a set of documents $D_{t}=$ $\left{d_{t}^{1}, \ldots, d_{t}^{k}\right}$ containing relevant background information. ${ }^{1}$ Given $\mathbf{x<em t="t">{t}$ and $D</em>}$, we first formulate a plan $\mathbf{p<em t="t">{t}$-a rough idea of how the text should be modified, verbalized as a short text sequence like "add more information", "fix grammar errors" or "use simpler language". This plan is then realized by means of an actual edit that transforms $\mathbf{x}</em>}$ into the updated state $\mathbf{x<em t="t">{t+1}$. Finally, the intention behind this edit can optionally be clarified by providing a textual explanation $\mathbf{e}</em>$; this is especially relevant in collaborative settings where explanations can facilitate evaluating the quality and usefulness of an edit (Liu et al., 2019). Note that the explanation can be similar or even identical to the plan, the conceptual difference being that the plan is made before performing the edit, whereas the explanation is only formulated after it was performed.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The entire process of formulating a plan, collecting documents, performing an edit and explaining it, can be repeated multiple times to obtain a sequence of texts $\mathbf{x}<em t_1="t+1">{t}, \mathbf{x}</em>}, \mathbf{x<em n="n">{t+2}, \ldots$ until either we arrive at some $\mathbf{x}</em>}$ for which $\mathbf{x<em n-1="n-1">{n}=\mathbf{x}</em>=\varepsilon$. In reference to its four main parts, we refer to models based on this iterative process as PEER models.}$, or we reach a manually defined halting criterion. We can also write texts from scratch by starting with an empty sequence, i.e., $\mathbf{x}_{0</p>
<h3>3.1 Overcoming Data Scarcity</h3>
<p>While using PEER to break the complex task of writing a coherent, consistent and factual document into many smaller subtasks has some potential benefits over standard left-to-right language modeling-such as being more interpretable and easier to control-it is challenging to find data from which this process can be learned at the scale required to train large language models. This is mainly because edit histories are difficult to obtain from web crawls, the most important data source for current language models (Brown et al., 2020; Rae et al., 2021). But even in cases where edit histories can be obtained (e.g., by collecting crawls of identical pages at different times) or synthetically generated, edits are typically not annotated with plans, documents, or explanations.</p>
<p>Similar to prior work on text editing (Faltings et al., 2021; Reid and Neubig, 2022), our first step in overcoming this issue is turning to Wikipedia a single source that comes close to fulfilling all our needs: It provides a full edit history including comments on a diverse set of topics, is large in scale, and articles frequently contain citations, which can be helpful for finding relevant documents. However, relying on Wikipedia as our sole source of training data comes with various severe downsides: First, it makes trained models specific to Wikipedia in terms of how they expect textual content to look like and what plans and edits they predict. Beyond that, comments in Wikipedia are noisy, and so in many cases, they are not an appropriate proxy for plans or explanations. Finally, numerous paragraphs in Wikipedia do not contain any citations; while this lack of background information can often be compensated by using a retrieval system (Piktus et al., 2021; Petroni et al., 2022), even such systems may not be able to find supporting background information for many edits.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Starting from a text $\mathbf{x}<em t="t">{t}$, we use both a plan $\mathbf{p}</em>}$ and a collection of documents $D_{t}=\left{d_{t}^{1}, \ldots, d_{t}^{n}\right}$ to obtain an updated version $\mathbf{x<em t="t">{t+1}$ and an explanation $\mathbf{e}</em>$ of the performed edit; this process is repeated multiple times.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(b) To generalize to domains without editing histories, overcome data scarcity and improve the model's core abilities, we train various instances of PEER that perform different infilling tasks derived from this process.</p>
<p>Figure 2: Schematic representation of the PEER process</p>
<h1>4 Infilling Edit Histories with PEER</h1>
<p>We propose a simple approach to address all issues that arise from Wikipedia being our only source of commented edit histories at once: We train not just one, but multiple instances of PEER that learn to infill various parts of the editing process (Figure 2b); these models can then be used to generate synthetic data as a substitute for the missing pieces in our training corpus. In concrete terms, we train the following encoder-decoder models:</p>
<ul>
<li>PEER-Edit: Given an input text and a set of documents, this model learns to both plan and realize edits, i.e., it maps $\left(\mathbf{x}<em t="t">{t}, D</em>}\right)$ to the sequence $\left(\mathbf{p<em t_1="t+1">{t}, \mathbf{x}</em>\right)$. This is done in an autoregressive fashion by factoring</li>
</ul>
<p>$$
\begin{aligned}
&amp; p\left(\mathbf{p}<em t_1="t+1">{t}, \mathbf{x}</em>} \mid \mathbf{x<em t="t">{t}, D</em>\right) \
&amp; \quad=\prod_{i=1}^{n} p\left(z_{i} \mid \mathbf{x}<em t="t">{t}, D</em>\right)
\end{aligned}
$$}, z_{1}, \ldots, z_{i-1</p>
<p>where $\mathbf{z}=z_{1}, \ldots, z_{n}=\mathbf{p}<em t_1="t+1">{t} \cdot \mathbf{x}</em>}$ is the concatenation of $\mathbf{p<em t_1="t+1">{t}$ and $\mathbf{x}</em>$. Thus, PEER-Edit can update texts autonomously by generating both plans and edits, but it can also be provided with human-written plans as prefixes. As PEER-Edit is our main model for actual editing, we also refer to it simply as PEER.</p>
<ul>
<li>PEER-Undo: Given a text sequence $\mathbf{x}<em t="t">{t+1}$ and a collection of documents $D</em>}$ that may have been used to write it, this PEER instance is trained to guess and undo the latest edit by predicting the sequence $\left(\mathbf{p<em t="t">{t}, \mathbf{x}</em>\right)$. This is done autoregressively analogous to PEER-Edit.</li>
<li>PEER-Explain: This model is trained to autoregressively generate explanations $\mathbf{e}<em t="t">{i}$ given $\left(\mathbf{x}</em>}, \mathbf{x<em t="t">{t+1}, D</em>\right)$, i.e., an edit and a collection of relevant documents.</li>
<li>PEER-Document: Given $\left(\mathbf{x}<em t_1="t+1">{t}, \mathbf{x}</em>}, \mathbf{p<em t="t">{t}\right)$, this model is trained to generate a document $d \in$ $D</em>$ that provides useful background information for the edit.</li>
</ul>
<h3>4.1 Synthetic Data Generation</h3>
<p>We use all variants of PEER to produce synthetic data-both to generate the missing pieces for completing our training data, and to replace low-quality pieces in our existing data.</p>
<p>Decomposing Texts into Edits To enable training on arbitrary text data even if it comes without edit histories, we use PEER-Undo for generating synthetic "backward" edits: Given a plain text $\mathbf{x}=\mathbf{x}<em n-1="n-1">{n}$ and a collection of documents $D$, we iteratively apply PEER-Undo to obtain a sequence $\left(\mathbf{p}</em>}, \mathbf{x<em n-2="n-2">{n-1}\right),\left(\mathbf{p}</em>}, \mathbf{x<em m="m">{n-2}\right), \ldots$ until we arrive at some $\mathbf{x}</em>}=\varepsilon$. We can then train PEER-Edit in the opposite direction, i.e., to predict each $\left(\mathbf{p<em t_1="t+1">{t}, \mathbf{x}</em>$ and $D$, allowing us to use data from domains other than Wikipedia.}\right)$ from $\mathbf{x}_{t</p>
<p>Generating Plans We use PEER-Explain to address both the low quality of many comments in our</p>
<p>corpus, and the fact that many edits simply do not have any comments. Given $\mathbf{x}<em t_1="t+1">{t}, \mathbf{x}</em>}$ and a collection of documents $D_{t}$, we sample various outputs $\mathbf{e<em t="t">{t}^{1}, \ldots, \mathbf{e}</em>}^{k}$ from PEER-Explain $\left(\mathbf{x<em t_1="t+1">{t}, \mathbf{x}</em>$ and pick the one that makes this edit the most likely as its new plan:}, D_{t}\right)$ that explain the edit being made and act as potential plans. We then compute the likelihood of the actual edit given each $\mathbf{e}_{t}^{j</p>
<p>$$
\hat{\mathbf{p}}<em t_1="t+1">{t}=\underset{j \in{1, \ldots, k}}{\arg \max } p\left(\mathbf{x}</em>} \mid \mathbf{x<em t="t">{t}, D</em>\right)
$$}, \mathbf{e}_{t}^{j</p>
<p>where $p\left(\mathbf{x}<em t="t">{t+1} \mid \mathbf{x}</em>}, D_{t}, \mathbf{e<em t_1="t+1">{t}^{j}\right)$ denotes the probability that PEER-Edit assigns to $\mathbf{x}</em>}$ given $\mathbf{x<em t="t">{t}, D</em>$ as a plan.}$ and $\mathbf{e}_{t}^{j</p>
<p>Generating Documents Finally, if we are unable to find relevant documents for a particular edit, we can use PEER-Document to generate synthetic documents containing information required to perform this edit. Crucially, we only do so for training PEER-Edit; we never provide a model with any synthetic documents during inference. For generating documents, we proceed exactly as we do for generating new plans: We first sample a few documents from PEER-Document and then pick the one which helps PEER-Edit the most in predicting the actual edit.</p>
<h3>4.2 Controlling Model Outputs</h3>
<p>To improve the quality and diversity of generated plans, edits and documents, we implement control mechanisms similar to Keskar et al. (2019) and He et al. (2020b) - that is, we prepend specific control tokens to the output sequences that a model is trained to generate, and then use these control tokens during inference to guide the model's generations. In particular, we make use of the following controls for different PEER models (see Appendix B for further details):</p>
<ul>
<li>For PEER-Explain, we control the output length as a proxy for the level of detail in generated explanations. We also control whether the generated comment starts with a verb in infinitive form; this approximates the notion of an instruction, the format we expect humans to commonly use for communicating with PEER. Finally, we control whether there is a word overlap between the explanation and the edit; preventing this during inference makes sure that generated plans do not make editing trivial by exactly specifying which words to add, remove or replace.</li>
<li>For PEER-Undo, we control the difference in the number of words between $\mathbf{x}<em t="t">{t+1}$ and $\mathbf{x}</em>}$. Through this, we can ensure that the sequence $\mathbf{x<em n-1="n-1">{n}, \mathbf{x}</em>=\varepsilon$ and does not get stuck in an infinite loop.}, \ldots$ eventually terminates at $\mathbf{x}_{m</li>
<li>For PEER-Document, we control whether the generated document contains a given substring. This is useful when we want the document to contain a specific quote that is referred to in a Wikipedia edit.</li>
</ul>
<p>We do not use any controls for PEER-Edit, because-unlike for other models, which have specific and clearly defined tasks to solve-we do not make assumptions in advance about the types of editing tasks that users might want to solve with PEER-Edit and the kinds of control tokens that might be useful for these tasks.</p>
<h2>5 Training Data</h2>
<p>Our main training data for PEER is derived from Wikipedia's edit history, ${ }^{2}$ which directly gives us access to raw tuples of source and target texts $\left(\mathbf{z}<em t_1="t+1">{t}, \mathbf{z}</em>}\right)$, from which we derive $\mathbf{x<em t_1="t+1">{t}$ and $\mathbf{x}</em>}$ after some preprocessing steps discussed below. Beyond that, the edit history also provides us with comments $\mathbf{c<em t="t">{t}$ that we use as proxies both for the plan $\mathbf{p}</em>}$ and for the explanation $\mathbf{e<em t="t">{t}$. Finally, as Wikipedia articles frequently use citations to back up claims, we can obtain an initial set $I</em>}$ of document identifiers (e.g., URLs) for all documents cited in either $\mathbf{z<em t_1="t+1">{t}$ or $\mathbf{z}</em>$ for each edit; finally, we convert the data into a format suitable for sequence-to-sequence models. In the following, we discuss all three preprocessing steps in more detail.}$. Our pipeline for transforming this raw data into the PEER format consists of three steps: First, we use some heuristics for filtering the data to remove low-quality edits and avoid overlap with any of our evaluation sets. We then use $I_{t}$ and a retrieval engine (Petroni et al., 2022) to obtain a collection of relevant documents $D_{t</p>
<p>Filtering As Wikipedia's edit history contains several low quality edits and numerous instances of vandalism (Potthast et al., 2008), we use some simple heuristics to improve data quality. In particular, we filter out edits that were reverted at some point and edits that were automatically made by</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>bots without human involvement. Beyond that, we filter edits that affect more than two paragraphs and remove all edits for pages that are used in any of the datasets we evaluate on. We also discard examples where $I_{t}$ contains document identifiers that we are unable to resolve (e.g., because they refer to web pages that no longer exist). More details and a comprehensive overview of all filtering rules applied can be found in Appendix A.1.</p>
<p>Retrieving Documents A crucial aspect of PEER is its use of documents that contain relevant background information. We thus aim to collect a set of documents $D_{t}=\left{d_{t}^{1}, \ldots, d_{t}^{k}\right}$ for each edit, with $k$ being a hyperparameter defining the number of documents. If $\left|I_{t}\right| \geq k$, we exclusively obtain these documents from citations that can be found in the edited paragraphs (i.e., in either $\mathbf{x}<em t_1="t+1">{t}$ or $\mathbf{x}</em>$ ). Otherwise, we augment these documents with ones from the Sphere corpus (Piktus et al., 2021), obtained using the pipeline described in (Petroni et al., 2022); further details are discussed in Appendix A.2.</p>
<p>Formatting Our first formatting step is to remove all paragraphs from $\mathbf{z}<em t_1="t+1">{t}$ and $\mathbf{z}</em>}$ that are not affected by the edit. We then remove Wikipediaspecific syntax, but with a few exceptions: We keep the syntax for representing titles, bold text, text in italics and lists, enabling the model to learn how to perform some basic formatting. We also keep links and, more importantly, citations, enabling PEER to learn how to cite and quote from documents in $D_{t}$ to back up the textual content it generates. We denote the resulting text sequences with $\mathbf{z<em t_1="t+1">{t}^{\prime}$ and $\mathbf{z}</em>$.}^{\prime</p>
<p>We linearize each document $d_{t}^{i} \in D_{t}$ using its content $\mathbf{c}<em i="i">{i}$ and, if present, its title $\mathbf{t}</em>$ as follows:}$ and the corresponding web site's domain $\mathbf{d}_{i</p>
<p>$$
[i] \mathbf{d}<em i="i">{i} # \mathbf{t}</em>
$$} # \mathbf{c}_{i</p>
<p>We include the number $i$ in this representation to facilitate citing and quoting specific documents. To finally obtain $\mathbf{x}<em t_1="t+1">{t}$ and $\mathbf{x}</em>}$ from $\mathbf{z<em t_1="t+1">{t}^{\prime}$ and $\mathbf{z}</em>$ in both sequences with either}^{\prime}$, we replace each citation of a document $d_{t}^{i</p>
<p>$$
[[[i]]] \text { or } \quad\left[\left[\left[i \text { quote }=\mathbf{q}_{i}\right]\right]\right]
$$</p>
<p>depending on whether in the original data, a specific subsequence $\mathbf{q}<em t="t">{i}$ of $d</em>}^{i}$ was quoted. As $\mathbf{p<em t="t">{t}$ and $\mathbf{e}</em>$ are already simple text sequences, we do not perform any modifications to them.</p>
<p>If there are multiple inputs or outputs, we simply concatenate them using a special separator sequence. Moreover, if the text we are editing has a title, we always prepend this title to the original input sequence. An example of a linearized input for PEER is illustrated in Figure 7.</p>
<h2>6 Experiments</h2>
<p>We conduct a series of experiments to investigate whether-despite Wikipedia being our only natural source of comments and edits-our infilling techniques enable us to turn PEER into a general purpose editing model that is capable of following human-written plans and tackling a wide range of collaborative editing tasks in different domains at satisfactory performance. More specifically, we aim to answer the following research questions:</p>
<ul>
<li>Can PEER follow plans and perform meaningful edits in domains for which no edit histories are available, and does our self-training approach using PEER-Undo to generate synthetic edits improve this ability? (Section 6.2)</li>
<li>Does the ability to follow plans based on Wikipedia comments transfer to instructions specified by humans, and can it be improved by training on synthetic plans generated using PEER-Explain? (Section 6.3)</li>
<li>Can PEER make proper use of citations and quotes to explain generated outputs, and can PEER-Document amplify this? (Section 6.4)</li>
<li>How does writing text sequences in a single pass compare to an iterative application of PEER, both if the model runs autonomously and if it is provided with human-written plans? (Section 6.5)</li>
</ul>
<p>Experiments conducted to answer these research questions are complemented by a qualitative analysis of model outputs and exemplary sessions of human-AI interactions in Section 7.</p>
<h3>6.1 Experimental Setup</h3>
<p>We initialize all instances of PEER from $L M$ Adapted T5 (Raffel et al., 2020); by default, we use the variant with 3B parameters. Each model is trained for 20,000 steps on 64 GPUs with an effective batch size of 256, corresponding to about five million Wikipedia edits. The maximum sequence length is set to 1,024 and 384 tokens for input and</p>
<table>
<thead>
<tr>
<th>Subset</th>
<th>Train (Edit)</th>
<th>Train (PT)</th>
<th>Test</th>
<th>Doc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wikipedia</td>
<td>6,960,935</td>
<td>-</td>
<td>4,000</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Wikinews</td>
<td>-</td>
<td>125,664</td>
<td>1,000</td>
<td>-</td>
</tr>
<tr>
<td>Cooking</td>
<td>-</td>
<td>22,517</td>
<td>500</td>
<td>-</td>
</tr>
<tr>
<td>Gardening</td>
<td>-</td>
<td>13,258</td>
<td>500</td>
<td>-</td>
</tr>
<tr>
<td>Law</td>
<td>-</td>
<td>16,418</td>
<td>500</td>
<td>-</td>
</tr>
<tr>
<td>Movies</td>
<td>-</td>
<td>19,601</td>
<td>500</td>
<td>-</td>
</tr>
<tr>
<td>Politics</td>
<td>-</td>
<td>10,676</td>
<td>500</td>
<td>-</td>
</tr>
<tr>
<td>Travel</td>
<td>-</td>
<td>38,961</td>
<td>500</td>
<td>-</td>
</tr>
<tr>
<td>Workplace</td>
<td>-</td>
<td>18,231</td>
<td>500</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>output, respectively. We set $k=3$ as the maximum number of reference documents per example.</p>
<p>We use a variety of metrics to evaluate PEER and our baseline models on all tasks considered:</p>
<ul>
<li>Exact Match (EM) is the percentage of examples for which the performed edit exactly matches a given target;</li>
<li>EM-Diff is a variant of EM that is computed on the diff level;</li>
<li>SARI (Xu et al., 2016) averages match scores for the three word-level edit operations add, delete and keep;</li>
<li>GLEU (Napoles et al., 2015) is a variant of BLEU (Papineni et al., 2002) proposed for grammatical error correction tasks;</li>
<li>Rouge (Lin, 2004) is a set of metrics based on $n$-gram overlap (Rouge- $n$ ) or longest common subsequences (Rouge-L);</li>
<li>Update-Rouge (Logan IV et al., 2021) is a variant of Rouge that is computed only on sentences updated during an edit.</li>
</ul>
<h3>6.2 Natural Edits</h3>
<p>We first evaluate PEER's ability to follow a diverse set of plans, leverage provided documents and perform edits across different domains; in particular, we are interested in investigating its performance in domains for which no edit histories are available. To this end, we introduce Natural Edits, a collection of naturally occuring edits for different text types and domains that we obtain from three English web sources: We collect encyclopedic pages from Wikipedia, news articles from Wikinews, and questions from the Cooking, Gardening, Law, Movies, Politics, Travel and Workplace subforums of StackExchange. All of these sites provide edit histories with comments that often elaborate on the edit's intent and that we provide to all models as plans. ${ }^{5}$ We split each dataset into training and test data. However, we only provide plain texts instead of actual edits in the training sets of the Wikinews and StackExchange subsets,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Overview of the number of edits and plain texts (PT) in the train sets and the number of edits in the test sets of Natural Edits. The final column shows whether the subset uses reference documents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">EM</th>
<th style="text-align: right;">EM-Diff</th>
<th style="text-align: right;">SARI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Copy</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">32.7</td>
</tr>
<tr>
<td style="text-align: left;">PEER</td>
<td style="text-align: right;">$\mathbf{2 3 . 1}$</td>
<td style="text-align: right;">$\mathbf{2 6 . 2}$</td>
<td style="text-align: right;">$\mathbf{5 5 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">PEER (no plans)</td>
<td style="text-align: right;">18.0</td>
<td style="text-align: right;">19.8</td>
<td style="text-align: right;">52.0</td>
</tr>
<tr>
<td style="text-align: left;">PEER (no documents)</td>
<td style="text-align: right;">19.8</td>
<td style="text-align: right;">22.8</td>
<td style="text-align: right;">51.7</td>
</tr>
<tr>
<td style="text-align: left;">PEER (no plans/documents)</td>
<td style="text-align: right;">13.5</td>
<td style="text-align: right;">15.1</td>
<td style="text-align: right;">45.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for variants of PEER on the Wikipedia subset of Natural Edits. Plans and documents provide complementary information and substantially improve performance.
enabling us to test editing abilities in domains for which no edit histories are accessible. Relevant statistics for Natural Edits are shown in Table 1.</p>
<p>As a first experiment, we check whether PEER actually learns to make use of provided documents and plans by evaluating it on the Wikipedia subset of Natural Edits. We compare regular PEER provided with gold plans to variants trained and evaluated (i) without plans, (ii) without reference documents, and (iii) without both plans and reference documents. Table 2 shows EM, EM-Diff and SARI scores for all models and a copying baseline, for which $\mathbf{x}<em t="t">{t+1}=\mathbf{x}</em>$. As can be seen, PEER substantially outperforms all baselines. PEER without both plans and documents performs much worse than just removing one of both, illustrating that plans and documents provide complementary information that the model is capable of using; this is in line with findings of Faltings et al. (2021).</p>
<p>Next, we evaluate PEER on all subsets of Natural Edits in order to assess its ability to perform edits in different domains. We use PEER-Undo as described in Section 3.1 to create synthetic edits from plain texts and train domain-adapted (DA) variants of PEER. For generating synthetic edits, we found it sufficient to apply PEER-Undo just once for each plain text $\mathbf{x}<em t-1="t-1">{t}$ to obtain a tuple $\left(\mathbf{p}</em>\right)$.}, \mathbf{x}_{t-1</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Wiki</th>
<th style="text-align: right;">News</th>
<th style="text-align: right;">Cooking</th>
<th style="text-align: right;">Garden</th>
<th style="text-align: right;">Law</th>
<th style="text-align: right;">Movies</th>
<th style="text-align: right;">Politics</th>
<th style="text-align: right;">Travel</th>
<th style="text-align: right;">Workpl.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Copy</td>
<td style="text-align: right;">$0.0 / 32.7$</td>
<td style="text-align: right;">$0.1 / 32.8$</td>
<td style="text-align: right;">$0.0 / 31.6$</td>
<td style="text-align: right;">$0.0 / 32.0$</td>
<td style="text-align: right;">$0.0 / 31.1$</td>
<td style="text-align: right;">$0.0 / 31.5$</td>
<td style="text-align: right;">$0.0 / 31.8$</td>
<td style="text-align: right;">$0.0 / 31.2$</td>
<td style="text-align: right;">$0.0 / 31.5$</td>
</tr>
<tr>
<td style="text-align: left;">PEER (no plans)</td>
<td style="text-align: right;">$16.6 / 50.7$</td>
<td style="text-align: right;">$10.8 / 41.3$</td>
<td style="text-align: right;">$4.5 / 36.3$</td>
<td style="text-align: right;">$1.8 / 35.1$</td>
<td style="text-align: right;">$2.6 / 35.8$</td>
<td style="text-align: right;">$2.9 / 35.3$</td>
<td style="text-align: right;">$2.1 / 36.5$</td>
<td style="text-align: right;">$1.6 / 34.8$</td>
<td style="text-align: right;">$3.1 / 34.7$</td>
</tr>
<tr>
<td style="text-align: left;">PEER</td>
<td style="text-align: right;">$\mathbf{2 6 . 2 / 5 5 . 5}$</td>
<td style="text-align: right;">$21.3 / 49.3$</td>
<td style="text-align: right;">$11.0 / 40.2$</td>
<td style="text-align: right;">$4.4 / 37.7$</td>
<td style="text-align: right;">$7.5 / 36.4$</td>
<td style="text-align: right;">$6.7 / 39.2$</td>
<td style="text-align: right;">$6.8 / 38.7$</td>
<td style="text-align: right;">$6.7 / 38.1$</td>
<td style="text-align: right;">$6.9 / 36.7$</td>
</tr>
<tr>
<td style="text-align: left;">PEER (DA)</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{2 3 . 3 / 5 1 . 6}$</td>
<td style="text-align: right;">$\mathbf{1 3 . 2 / 4 2 . 9}$</td>
<td style="text-align: right;">$\mathbf{8 . 1 / 4 4 . 9}$</td>
<td style="text-align: right;">$\mathbf{9 . 4 / 3 9 . 0}$</td>
<td style="text-align: right;">$\mathbf{9 . 9 / 4 2 . 4}$</td>
<td style="text-align: right;">$\mathbf{1 1 . 6 / 4 1 . 3}$</td>
<td style="text-align: right;">$\mathbf{9 . 1 / 4 0 . 2}$</td>
<td style="text-align: right;">$\mathbf{8 . 3 / 3 9 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 3: EM-Diff / SARI scores on all subsets of Natural Edits. The domain-adapted (DA) variants of PEER clearly outperform regular PEER, demonstrating the usefulness of synthetic edits generated with PEER-Undo.</p>
<p>Upon manual inspection, we also found that the generated plans $\mathbf{p}_{\mathrm{f}-1}$ do not actually match the undone edit, so we use PEER-Explain as described in Section 3.1 to rewrite all plans. We finetune the domain-adapted variants of PEER-Edit on a balanced mixture of examples from the original training distribution and synthetic in-domain edits for 1,000 steps; we do so separately for the Wikinews and StackExchange subsets of Natural Edits, resulting in two instances of domain-adapted PEER. Results on Natural Edits are shown in Table 3, which reports both EM-Diff and SARI scores across all subsets. As can be seen, plans are extremely helpful across domains, indicating that the ability to understand plans found in Wikipedia edits directly transfers to other domains. Importantly, the domain-adapted variants of PEER clearly outperform regular PEER for all subsets of Natural Edits, with particularly strong improvements on the Gardening, Politics, and Movies subsets ( $84 \%$, $71 \%$ and $48 \%$ EM-Diff, respectively). This demonstrates the effectiveness of generating synthetic edits for applying PEER in different domains.</p>
<h3>6.3 Downstream Tasks</h3>
<p>So far, we have only evaluated PEER using plans based on naturally occurring comments. But to what extend is it capable of following instructions formulated by humans to yield well known editing functionalities, and can training on synthetic plans improve this ability? To answer these questions, we next evaluate PEER on various downstream editing tasks in a zero-shot fashion. For this evaluation, we consider the following datasets:</p>
<ul>
<li>JFLEG (Napoles et al., 2017) is a grammatical error correction dataset with singlesentence inputs written by English language learners;</li>
<li>ASSET (Alva-Manchego et al., 2020) is a crowdsourced corpus for single-sentence text simplification;</li>
<li>IteraTER (Du et al., 2022b) is an editing dataset spanning five edit intentions across three different domains; ${ }^{6}$</li>
<li>WNC (Pryzant et al., 2020) is a dataset where the task is to remove or mitigate biased words to make sentences more neutral;</li>
<li>FRUIT (Logan IV et al., 2021) contains texts from Wikipedia that need to be updated; for performing this update, various reference documents from Wikipedia are provided;</li>
<li>WAFER-INS is based on the WAFER dataset (Petroni et al., 2022); the task is to insert a sentence at some position in a Wikipedia paragraph given documents from the Sphere corpus (Piktus et al., 2021) that contain relevant background information.</li>
</ul>
<p>In addition to PEER-Edit, we also consider a variant trained with synthetic plans; that is, we replace each original plan with one generated by PEER-Explain as described in Section 3.1. We refer to the PEER-Edit variant trained on these synthetic plans as PEER (SP). When generating synthetic plans, we use the control tokens introduced in Section 4.2 to ensure a diverse set of plan lengths. For $80 \%$ of generated plans, we enforce that they start with a verb and have no word overlap with the performed edit, respectively. Details are discussed in Appendix B.</p>
<p>We compare PEER and PEER (SP) to various baseline models that were either trained in a fully unsupervised fashion, or, in a similar spirit as PEER, were trained to solve tasks given humanwritten instructions; however, these instructions are not naturally occurring like the Wikipedia comments we use for PEER, but explicitly written by humans to create training datasets for finetuning language models. While this generally results in higher quality data, such datasets are also much</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>more difficult and expensive to obtain. Thus, if we are able to bridge the gap between noisy comments and actual instructions, PEER can provide a much less expensive alternative. In concrete terms, we compare PEER to the following models:</p>
<ul>
<li>T$k$-Instruct (Wang et al., 2022) is, like PEER, initialized from the LM Adapt variant of T5. It is finetuned on Natural Instructions v2, a collection of instructions for more than 1,600 tasks, including grammatical error correction and text simplification.</li>
<li>T0 and T0++ (Sanh et al., 2022) are also initialized from the LM Adapt variant of T5; they are then finetuned using a variety of humanwritten prompts from PromptSource (Bach et al., 2022). Unlike Natural Instructions v2, this dataset does not directly contain editing tasks, but related tasks including summarization and data-to-text generation.</li>
<li>GPT3 (Brown et al., 2020) is a pretrained decoder-only model that is not finetuned on any instructions; with 175B parameters, it is larger than our default PEER models by two orders of magnitude. We also compare to InstructGPT (Ouyang et al., 2022), a variant of GPT3 that was finetuned on a large dataset of instructions and corresponding outputs written by humans; neither the dataset nor the model parameters are publicly available, so we access the models via OpenAI's API.</li>
<li>OPT (Zhang et al., 2022) is an open-source replica of GPT3; it is not finetuned on any labeled data.</li>
</ul>
<p>As all of these models are capable of processing textual prompts, we formulate a single plan $\mathbf{p}$ per task that we provide to all models. However, we embed this plan into slightly different contexts to make it most suitable for each model (for example, T$k$-Instruct expects the plan to be prefixed by the string "Definition:"). For FRUIT and WAFERInS, we also use a more natural format than the one used by PEER for providing references to our baseline models; all prompts and model-specific modifications are shown in Appendix E.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Unless otherwise specified, we use greedy decoding for all models. We do not perform any taskspecific finetuning or in-context learning as we are interested in evaluating each model's suitability as a general editing model: In the general case of a user providing a plan, we cannot assume access to other examples using the exact same plan. Beyond that, especially for tasks that require references, we typically cannot fit more than one example into the context window. We do not compare to other editing models trained on Wikipedia data, as they are either only trained to solve specific tasks (Logan IV et al., 2021), not able to follow instructions (Reid and Neubig, 2022) or only capable of processing single-sentence inputs (Faltings et al., 2021); beyond that, none of these models are publicly available. However, we additionally report supervised state-of-the-art scores for all tasks considered.</p>
<p>Results are shown in Table 4, with rows grouped into four different sets. The first group contains a copying baseline and all models based on $L M$ Adapted T5. PEER substantially outperforms all other models in this group, with the 3B model achieving an average SARI score of 42.0 across all tasks, compared to 24.1 for the strongest T5based baseline model. Importantly, PEER (SP) consistently outperforms regular PEER, increasing the average score by 1.6 points. This clearly demonstrates the usefulness of generating synthetic plans to enhance PEER's ability to follow instructions. Increasing the model size to 11B parameters slightly improves results for most tasks.</p>
<p>Given the iterative nature of PEER's editing process, the second group considers an alternative decoding strategy where PEER is applied multiple times; we consider both greedy decoding for three iterations (i3) and top- $p$ sampling (Holtzman et al., 2020) with $p=0.9$ for three iterations (s-i3). As can be seen, different decoding strategies sometimes drastically improve results (e.g., for ASSET), but can also lead to much worse performance (e.g., for WNC). We thus believe that automatic ways of finding the best decoding strategy for a given plan is an interesting avenue for future work.</p>
<p>All 175B parameter models are shown in the third group. OPT and GPT3 perform much worse than PEER despite being much larger. InstructGPT comes close to and even outperforms PEER for some tasks that do not require documents; however, it clearly lags behind PEER when it comes to handling documents for updating text or adding</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th>Params</th>
<th>Without Documents</th>
<th></th>
<th></th>
<th></th>
<th>With Documents</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>JFLEG</td>
<td>ASSET</td>
<td>IterATER</td>
<td>WNC</td>
<td>FRUIT</td>
<td>WAFER-INS</td>
<td>Avg</td>
<td></td>
</tr>
<tr>
<td>(a)</td>
<td>Copy</td>
<td>-</td>
<td>26.7 / 40.5</td>
<td>20.7</td>
<td>30.5</td>
<td>31.9 / 0.0</td>
<td>29.8 / 0.0</td>
<td>33.6 / -</td>
<td>28.9</td>
<td></td>
</tr>
<tr>
<td></td>
<td>T$k$-Instruct</td>
<td>3B</td>
<td>31.7 / 38.7</td>
<td>28.3</td>
<td>36.2</td>
<td>30.3 / 0.0</td>
<td>12.7 / 3.9</td>
<td>1.6 / -</td>
<td>23.5</td>
<td></td>
</tr>
<tr>
<td></td>
<td>T0</td>
<td>3B</td>
<td>42.9 / 38.6</td>
<td>28.6</td>
<td>28.1</td>
<td>17.8 / 0.0</td>
<td>13.1 / 5.7</td>
<td>6.1 / -</td>
<td>22.8</td>
<td></td>
</tr>
<tr>
<td></td>
<td>T0++</td>
<td>11B</td>
<td>35.9 / 43.8</td>
<td>25.8</td>
<td>36.1</td>
<td>27.0 / 0.0</td>
<td>16.1 / 3.7</td>
<td>3.9 / -</td>
<td>24.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PEER</td>
<td>3B</td>
<td>54.8 / 55.1</td>
<td>29.9</td>
<td>36.5</td>
<td>56.4 / 31.9</td>
<td>39.4 / 28.3</td>
<td>35.2 / 33.6</td>
<td>42.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PEER (SP)</td>
<td>3B</td>
<td>59.0 / 57.2</td>
<td>33.2</td>
<td>37.1</td>
<td>56.6 / 32.7</td>
<td>40.3 / 33.9</td>
<td>35.5 / 37.6</td>
<td>43.6</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PEER (SP)</td>
<td>11B</td>
<td>59.9 / 58.6</td>
<td>32.4</td>
<td>37.8</td>
<td>58.8 / 34.7</td>
<td>40.7 / 33.5</td>
<td>35.9 / 38.4</td>
<td>44.3</td>
<td></td>
</tr>
<tr>
<td>(b)</td>
<td>PEER (SP, i3)</td>
<td>3B</td>
<td>63.3 / 59.6</td>
<td>36.1</td>
<td>37.1</td>
<td>45.2 / 12.4</td>
<td>41.6 / 34.6</td>
<td>35.2 / 37.0</td>
<td>43.1</td>
<td></td>
</tr>
<tr>
<td></td>
<td>PEER (SP, s-i3)</td>
<td>3B</td>
<td>57.4 / 49.7</td>
<td>40.7</td>
<td>35.8</td>
<td>38.4 / 3.9</td>
<td>41.6 / 38.7</td>
<td>32.9 / 34.3</td>
<td>41.1</td>
<td></td>
</tr>
<tr>
<td>(c)</td>
<td>OPT</td>
<td>175B</td>
<td>49.2 / 49.4</td>
<td>25.8</td>
<td>31.4</td>
<td>25.1 / 0.0</td>
<td>35.6 / 27.4</td>
<td>21.1 / -</td>
<td>31.4</td>
<td></td>
</tr>
<tr>
<td></td>
<td>GPT3</td>
<td>175B</td>
<td>50.6 / 51.8</td>
<td>25.0</td>
<td>30.7</td>
<td>26.0 / 0.5</td>
<td>33.6 / 25.9</td>
<td>22.9 / -</td>
<td>31.5</td>
<td></td>
</tr>
<tr>
<td></td>
<td>InstructGPT</td>
<td>175B</td>
<td>62.3 / 60.0</td>
<td>35.4</td>
<td>38.2</td>
<td>33.9 / 0.7</td>
<td>37.5 / 23.4</td>
<td>29.2 / -</td>
<td>39.4</td>
<td></td>
</tr>
<tr>
<td>(d)</td>
<td>Sup. SotA</td>
<td>-</td>
<td>- / 62.4</td>
<td>44.2</td>
<td>37.2</td>
<td>- / 45.8</td>
<td>- / 47.4</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Downstream task results for PEER and various baselines, divided into four groups: (a) T5-based models and a copy baseline, (b) PEER with different sampling strategies, (c) 175B parameter decoder-only models, (d) supervised state of the art. The first numbers for each task are SARI scores; additional metrics are GLEU for JFLEG, EM for WNC, Update-R1 for FRUIT and SARI scores obtained if the model is told exactly where to insert a new sentence for WAFER-INS. Supervised scores from left to right are from <em>Ge et al. (2018)</em>, <em>Martin et al. (2020)</em>, <em>Du et al. (2022b)</em>, <em>Pryzant et al. (2020)</em> and <em>Logan IV et al. (2021)</em>, respectively. The best result for models based on LM Adapted T5 is shown in bold, the best zero-shot performance overall is underlined. On average, PEER (SP) clearly outperforms all baselines.
new information. Averaged across all tasks, it performs 4.1 points worse than PEER (SP) (3B), despite being much larger and being finetuned on human-annotated data. We also note that, given a high-quality corpus with human annotations, PEER could easily be further finetuned in a similar fashion to InstructGPT.</p>
<p>Finally, the last row of Table 4 shows supervised state-of-the-art performance. While our zero-shot models clearly lag behind these scores on average, they approach supervised performance in some cases.</p>
<h3>6.4 Citations and Quotes</h3>
<p>Unlike our baseline models, PEER is capable of both citing and quoting from reference documents to back up the claims it generates. This is useful both in terms of explainability and verifiability, as it allows users to easily fact-check these claims. The ability to quote from individual passages-as opposed to citing an entire document-is especially helpful for long documents, which can take some time to process entirely.</p>
<p>To facilitate the evaluation of PEER's ability to cite and quote, we consider both tasks in isolation. To this end, we introduce two new datasets based on Natural Edits: Ne-Cite and Ne-Quote. For building these datasets, we collect examples from Wikipedia's edit history where the only difference
between $\mathbf{x}<em t_1="t+1">{t}$ and $\mathbf{x}</em>$ by applying the BM25 }$ is that a new citation was added for Ne-Cite, or that a quote was added to an existing citation for Ne-Quote. Naturally, we make sure that the cited document is always present in the $k=3$ documents provided to PEER. To make the task of citing the correct document challenging, we obtain the other two documents in $D_{t<em>Robertson et al. (1995)</em> and DPR <em>Karpukhin et al. (2020)</em> variants of <em>Petroni et al. (2022)</em> to find the best match in Sphere <em>Piktus et al. (2021)</em>, respectively. If the gold document contains too many tokens, for Ne-Cite we pick the best chunk according to the reranker of <em>Petroni et al. (2022)</em>; for Ne-Quote, we select the chunk from the document that actually contains the quote. In total, we collect 2,351 and 391 examples, respectively, for which we manually set the plans to simply be "Add a citation" and "Add a quote".</p>
<p>Importantly, PEER's training data contains only very few examples of edits using quotes. This is mainly because quotes are used sparingly in Wikipedia. Moreover, we are unable to use the vast majority of examples containing quotes because they often come from non-online sources or web pages that no longer exist, so we do not have access to the documents that the quotes are taken from. To overcome this issue, we use PEER-Document to write synthetic documents for all edits that add quotes and for which the actual document is miss-</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>NE-Cite</th>
<th>NE-Quote</th>
<th>NE-Quote (con.)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random</td>
<td>$-/ 33.3$</td>
<td>-</td>
<td>$40.1 / 31.7 / 36.5$</td>
</tr>
<tr>
<td>Unigram</td>
<td>$-/ 34.2$</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Side</td>
<td>$-/ \mathbf{9 1 . 1}$</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Lead</td>
<td>$-/-$</td>
<td>-</td>
<td>$50.6 / 44.0 / 46.0$</td>
</tr>
<tr>
<td>PEER</td>
<td>$74.1 / 88.1$</td>
<td>$0.0 / 0.0 / 0.0$</td>
<td>$49.3 / 44.3 / 48.1$</td>
</tr>
<tr>
<td>PEER (SP)</td>
<td>$74.5 / 88.9$</td>
<td>$0.2 / 0.1 / 0.1$</td>
<td>$49.8 / 44.8 / 48.7$</td>
</tr>
<tr>
<td>PEER (SQ)</td>
<td>$\mathbf{7 4 . 9 / 8 7 . 9}$</td>
<td>$\mathbf{1 3 . 6 / 1 1 . 9 / 1 2 . 9}$</td>
<td>$\mathbf{5 8 . 1 / 5 4 . 6 / 5 7 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy on Ne-Cite (without/with gold positions) and R1/R2/RL scores on both Ne-Quote and constrained Ne-Quote. When given the correct position, PEER (SP) almost matches the performance of the supervised Side model on Ne-Cite, demonstrating its strong citing abilities. Training on synthetic documents substantially improves PEER's ability to quote relevant passages.
ing; this gives us an additional 8,739 training examples. We finetune PEER on these examples, mixed with around 500k examples from the original distribution, for 2,000 steps; we refer to this variant trained with synthetic quotes as PEER (SQ).</p>
<p>For Ne-Cite, we use the percentage of times where the correct document was cited and the citation was placed at the right position as our evaluation metric. We compare PEER's citing abilities to three baselines: randomly picking a reference, selecting the reference that maximizes the unigram overlap with $\mathbf{x}_{t}$, and using the Side reranker (Petroni et al., 2022), a model trained on millions of actual Wikipedia citations. Noticeably, none of these baselines is able to decide where to place the citation; besides, the Side reranker needs to be given the position of the citation to be added. We thus also consider a variant of Ne-Cite where models are told where to place the citation; for PEER, this is achieved using a decoder prefix that is identical to the input up to this position, at which it adds the sequence [ [ [ to indicate the start of a citation. Scores both without and with providing the correct positions are shown in Table 5. If not provided with the correct position, PEER puts the right citation at the right place in $74.1 \%$ of cases, with PEER (SP) slightly improving performance. When given the correct position, PEER (SP) even comes close to the supervised Side model ( 88.9 vs 91.1 ), clearly outperforming the other baselines. Finetuning on synthetic quotes does not significantly alter PEER's citing ability.</p>
<p>Similar to citing, we also look at two variants of the quoting task: In the first variant, the model needs to add a quote without being told where to put it and which document to quote from; in the second variant, the model is given access to the required position of the quote in a similar fashion to our approach for Ne-Cite (i.e., by adding a decoder prefix that ends with the string quote=). For this variant, we additionally use constrained decoding as proposed by Cao et al. (2021) to ensure that the generated quote is actually contained in the cited document. We compare to two baselines: One that selects a random substring of $n$ words from the gold document, and one that selects the lead $n$ words; we set $n$ to be the median length of quotes in Ne-Quote. As a metric, we report Rouge-1/2/L scores (Lin, 2004) computed only on quotes, which we set to 0 in case a model does not add a quote at all. As shown in Table 5, PEER and PEER (SP) are completely unable to quote without decoder prefixes and constrained decoding. Training on synthetic documents improves performance, but still results in rather low scores. Adding decoder prefixes and using constrained decoding improves performance a lot, but PEER still does not outperform the lead baseline. However, PEER (SQ) achieves much stronger results in this setting, improving R1/R2/RL scores by 7.5, 10.6 and 11.3 points over the lead baseline, respectively; this demonstrates the effectiveness of using PEERDocument to generate synthetic documents for improving PEER-Edit's ability to explain generated claims by quoting from provided documents.</p>
<h3>6.5 Iterative Editing for Text Generation</h3>
<p>Finally, we investigate PEER's ability to generate new texts from scratch, i.e., to perform a series of edits starting from $\mathbf{x}<em t_1="t+1">{0}=\varepsilon$. To this end, we collect a set of 500 intro sections from Wikipedia, each with three reference documents. As a baseline, we finetune LM Adapted T5 as a conditional language model on the exact same data that PEER was trained on - that is, the model is trained to predict $\mathbf{x}</em>}$ given $D_{t}$ and the page's title, but not $\mathbf{x<em t_1="t+1">{t}$. However, we use a special character sequence to inform the model about whether $\mathbf{x}</em>$ is an intro section; we train this baseline, that we refer to as WikiLM, with the exact same parameters as PEER.</p>
<p>We evaluate PEER in three different modes:</p>
<ul>
<li>an autonomous mode, where the model continuously writes and realizes its own plans without human involvement;</li>
<li>a manual mode, where we give the model a series of human-written plans that it is supposed</li>
</ul>
<table>
<thead>
<tr>
<th>Model</th>
<th>LP</th>
<th>R1 / R2 / RL</th>
<th>QE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wiki-LM</td>
<td>5.0</td>
<td>38.4 / 16.9 / 27.3</td>
<td>38.7</td>
</tr>
<tr>
<td>PEER (autonomous)</td>
<td>5.0</td>
<td>37.7 / 15.8 / 26.2</td>
<td>40.6</td>
</tr>
<tr>
<td>PEER (manual)</td>
<td>2.0</td>
<td>39.4 / 17.0 / 28.1</td>
<td>41.1</td>
</tr>
<tr>
<td>PEER (collaborative)</td>
<td>2.0</td>
<td>39.5 / 17.2 / 28.4</td>
<td>41.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Results for various approaches on our Wikipedia intro generation test set. Length penalty (LP) is optimized on the dev set; scores shown are Rouge-1/2/L and QuestEval (QE). WikiLM performs better than autonomous PEER in terms of Rouge scores, but is outperformed by PEER in manual and collaborative mode; all PEER models perform better in terms of QuestEval.
to realize. We choose a simple sequence of three plans that we use for all intros: $\mathbf{p}<em 1="1">{0}=$ "Create page", followed by $\mathbf{p}</em>=$ "Add more information";}=\mathbf{p}_{2</p>
<ul>
<li>a collaborative mode, where human-written plans are interleaved with plans proposed by PEER; that is, we use the plan sequence $\mathbf{p}<em 0="0">{0}, \mathbf{p}</em>}^{\prime}, \mathbf{p<em 1="1">{1}, \mathbf{p}</em>}^{\prime}, \mathbf{p<em 0="0">{2}$, where $\mathbf{p}</em>}, \mathbf{p<em 2="2">{1}$ and $\mathbf{p}</em>}$ are as above, whereas PEER generates $\mathbf{p<em 1="1">{0}^{\prime}$ and $\mathbf{p}</em>$.}^{\prime</li>
</ul>
<p>Without controlling for output length, WikiLM generates rather short intros, resulting in relatively low Rouge-1/2/L scores. To make the comparison more fair, we thus split our dataset of Wikipedia intros into 100 dev examples and 400 test examples; the dev examples are exclusively used for picking the exponential length penalty (Murray and Chiang, 2018) that maximizes the model's average Rouge-1 score. We also prevent models from generating the same token 5-gram more than once to avoid endless repetitions.</p>
<p>Table 6 shows performance on our test set with length penalties (LP) optimized on the dev set. WikiLM performs better than PEER in autonomous mode. We hypothesize that this is because it has more examples to learn how to generate text from. ${ }^{8}$ However, PEER in manual mode outperforms WikiLM by about one point Rouge-1, despite using very generic plans that are identical for all intros; the collaborative mode further improves results slightly. To evaluate the faithfulness of all models to the provided documents, we also consider QuestEval (QE) scores (Scialom et al., 2021),</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Average Rouge-1 score of WikiLM and PEER in autonomous (a), manual (m) and collaborative (c) mode as a function of the number of iterations
which we compute in reference-less mode. That is, we use the generated intros and provided documents for evaluation, but not the target intros; this makes sense as the latter may contain various pieces of information not present in the provided documents, which models are unable to predict if they stay faithful to these documents. Interestingly, all variants of PEER perform considerably better in terms of QuestEval scores than WikiLM, demonstrating that iteratively updating text helps the model stay more faithful to the provided reference documents. Figure 3 shows how performance for different PEER modes changes across iterations, illustrating how generated intros are improved over multiple iterations.</p>
<h2>7 Analysis</h2>
<h3>7.1 Collaborative Editing Sessions</h3>
<p>To illustrate PEER's capabilities and shortcomings, we take a qualitative look at how it performs in truly collaborative settings, where we manually provide it with human-written instructions to write both a Wikipedia-style introductory section about itself and a question about guacamole in the Cooking forum of StackExchange. For the introductory section, we collect three reference documents $d_{0}$, $d_{1}$, and $d_{2}$, where the first document contains this paper's author list and affiliation, the second document contains the abstract, and the third document contains the caption to Figure 1. For all documents, we set the title to this paper's title and the domain to arxiv.org. We use this same set of documents for each generation step, i.e., $D_{t}=\left{d_{0}, d_{1}, d_{2}\right}$ for all $t$. We do not provide any documents for the StackExchange example. Figure 4 shows interactive sessions with PEER (SP, 11B) and PEER</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ask a question about making Guacamole</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">How to make guacamole? <br> How do I make Guacamoles? <br> [guacamoles] [salad] [cheese] [pasta] <br> Add more to the question</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">How to make guacamole? <br> How do I make Guacamoles? I want to make it like the ones you can get at a Mexican restaurant. What kind of avocados are good for making Guacamole. <br> [guacamoles] [salad] [cheese] [pasta] <br> - cheese pasta</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">How to make guacamole? <br> [...] [guacamoles] [salad] [cheese] [pasta] <br> Add more relevant tags</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">How to make guacamole? <br> [...] [salad] [food-science] [guacamoles] <br> Add details about what l've tried so far</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">How to make guacamole? <br> [...] for making Guacamole. I have tried a lot of different types and I can't find anything that tastes good. <br> [salad] [food-science] [guacamoles] <br> Thank people for answering</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">How to make guacamole? <br> [...] for making Guacamole. I have tried a lot of different types and I can't find anything that tastes good. Thank you so much. [salad] [food-science] [guacamoles] <br> rewrote the guacamole question for clarity</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">How to make Guacamole guacamole? <br> How do I make Guacamoles? I'm trying to make a good guac, I want to make it like the ones you can get at a Mexican restaurant. What kind of avocados are good for making Guacamole. guac? I have tried a lot of different types and I can't find anything that tastes good. Thank you so much. [salad] [food-science] [guacamoles]</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 4: Examples of interactive editing sessions with PEER. Plans on green background are provided by a human, plans on blue background are written by the model itself. Left: PEER (SP, 11B) writing a Wikipedia-style introduction about itself, given three documents containing the author list and affiliation, abstract and caption of Figure 1, respectively, and the title "PEER (Language Model)". The model-written plan demonstrates its ability to spot and correct false information that it produced in prior iterations. It is also able to provide citations and perform basic formatting. Right: PEER (DA, 3B) writing a question about making guacamole in the style of StackExchange, given no reference documents. Despite only being exposed to synthetic edits, PEER is able to both propose and follow plans and to produce edits appropriate for the given domain.</p>
<p>(DA, 3B) for writing these texts, respectively. User-provided plans are shown on green background, plans generated by PEER are shown on blue background. In each step, we generate three different model outputs – one with beam search using three beams, and two using top-<sup>p</sup> sampling with <sup>p</sup> = 0.9 – and manually pick one of them.</p>
<p>As can be seen in Figure 4 (left), PEER is capable of extracting and composing information from various documents to follow the provided plans. It makes some plausible assumptions, such as the model being developed by Meta AI, despite this not being explicitly stated in any of the documents, and is able to point to the author list (document 0) as a reference for this claim. The model's response to the fifth plan ("Add info on the scandal") illustrates a fundamental issue with many pretrained language models: It accepts the premise of this plan and follows it by hallucinating a scandal about internet censorship. However, unlike traditional left-to-right models, PEER is able to correct the misinformation it has produced in the next step: When not provided with any human-written plan, the model itself writes the plan "remove unsourced claim" and removes the false statement again. Finally, the last edit shows how PEER can also be used to change the layout of a document in addition to modifying its content.</p>
<p>Figure 4 (right) shows how after domain adaptation on synthetic edits, PEER is capable of writing and editing texts in domains other than Wikipedia. In particular, it adapts to the structure of questions in StackExchange – consisting of a title (bold), a text, and a sequence of tags – and to their style, which is very different from a typical Wikipedia page. PEER proposes plans to fix errors it made in previous steps (such as first adding the irrelevant tags "cheese" and "pasta", which it later removes). It is also able to follow plans like "Add more relevant tags", despite tags being a concept specific to StackExchange that does not occur in its Wikipedia training data.</p>
<h3>7.2 Generating Synthetic Data</h3>
<p>To better understand the quality of the synthetic data generated with our infilling procedure, we also look at exemplary outputs of the other PEER variants. We first consider PEER-Undo, the model we use to generate edits for domains where only plain texts are available. Figure 5 shows the result of iteratively applying PEER-Undo to a selected sentence.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Exemplary application of PEER-Undo for decomposing a sentence from this paper into multiple edits, terminating with an empty sequence. Plans are rewritten with PEER-Explain in the opposite direction.</p>
<p>from this paper; corresponding plans are obtained from PEER-Explain. As can be seen, PEER-Undo is able to decompose this sentence into a sequence of meaningful edits despite not being exposed to any scientific papers during training. Somewhat surprisingly, both PEER-Undo and PEER-Explain are able to handle the references contained in this sentence, despite them being formatted in a completely different way than how we represent references during training on Wikipedia data (i.e., replacing them with a numeric identifier in square brackets).</p>
<p>We next inspect PEER-Explain's ability to generate plans as discussed in Section 3.1. For an edit that we performed while writing this paper, Table 7</p>
<p>$\mathrm{x}<em t_1="t+1">{t}=$ JFLEG is a grammatical error correction dataset with single-sentence inputs.
$\mathrm{x}</em>=$ JFLEG (Napoles et al., 2017) is a [...] with single-sentence inputs written by English language learners.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Control Sequence</th>
<th style="text-align: left;">Output</th>
<th style="text-align: center;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">type=instruction length=s overlap=false</td>
<td style="text-align: left;">add citation</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">type=instruction length=m overlap=false</td>
<td style="text-align: left;">add reference to JFLEG</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: left;">type=instruction length=xl overlap=false</td>
<td style="text-align: left;">add citation for JFLEG and add a bit more detail</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">type=instruction length=xl overlap=true</td>
<td style="text-align: left;">add reference to Napoles et al., 2017</td>
<td style="text-align: center;">$\mathbf{0 . 2 6}$</td>
</tr>
<tr>
<td style="text-align: left;">type=other length=xl overlap=false</td>
<td style="text-align: left;">Added a reference to the JFLEG paper</td>
<td style="text-align: center;">0.16</td>
</tr>
</tbody>
</table>
<p>Table 7: Generated plans for an edit that we performed while writing this paper; the corresponding sequences $\mathrm{x}<em t_1="t+1">{t}$ and $\mathrm{x}</em>}$ are shown on top, with changes highlighted in green. The final columns shows the average probability across tokens that our main PEER model assigns to $\mathrm{x<em t="t">{t+1}$ given $\mathrm{x}</em>$ and the respective plan. Control sequences enable us to specify the amount of detail that a plan provides.
shows the plans generated with PEER-Explain for different control sequences, using greedy decoding (see Appendix B for details on control sequences). As can be seen, length is a reasonable proxy for the amount of details in a plan: Constraining the output to be short results in the plan "add citation", whereas for a greater output length, PEER-Explain correctly identifies that two changes were made ("add citation for JFLEG and add a bit more detail"). Allowing word overlap between the plan and the edit results in a plan that specifies exactly which reference to add ("add reference to Napoles et al., 2017"). The final column of Table 7 shows the average probability of tokens in $\mathrm{x}<em t="t">{t+1}$ according to PEER-Edit given both $\mathrm{x}</em>$ and each of the generated plans. Naturally, the plan with word overlap is most helpful, resulting in the highest score; all other plans are about equally helpful to PEER-Edit.</p>
<p>We finally look at the ability of PEER-Document to generate plausible-looking reference documents, once again using an example sentence from this paper. For a synthetic edit that just adds a citation at the very end of this sentence, we sample five outputs from PEER-Document; the best generated document among these five is shown in Figure 6. As can be seen, the model is able to produce a somewhat plausible reference document that provides evidence for the claim $\mathrm{x}_{t}$. However, as exemplified by this document, we found model outputs to often contain numerous repetitions ("the number of Natural Edits and the number of Natural Edits").</p>
<h2>8 Limitations</h2>
<p>Both our approach itself and the ways in which we evaluate it have various limitations. Below, we discuss both kinds of limitations separately.
$\mathrm{x}_{t}=$ Importantly, the domain-adapted variants of PEER clearly outperform regular PEER for all subsets of Natural Edits.
[0] Domain-Adapted PEER for Natural Edits 1 Springer for Research \&amp; Development
rd.springer.com
Domain-Adapted PEER for Natural Edits The main goal of this work is to develop a domain-adaptive variant of PEER, which outperforms PEER on Natural Edits with respect to both the number of Natural Edits and the number of Natural Edits with respect to the number of natural edits. In this paper, we present a domain-adaptive</p>
<p>Figure 6: A document generated with PEER-Document for an edit that slightly modifies $\mathrm{x}_{t}$ (top) by adding a citation to some document with id 0 at the very end. As a control, we enforce that the document contains the substring "outperforms PEER on Natural Edits". The generated reference backs up the claim but contains a lot of repetitions.</p>
<h3>8.1 Approach</h3>
<p>A major limitation of our approach is that at each editing step, we assume the set $D_{t}$ to be given; the retrieval engine we use to obtain $D_{t}$ (Petroni et al., 2022) makes use of the targets $\mathrm{x}<em t_1="t+1">{t+1}$, which clearly is not possible in real-world applications. It would thus be interesting to investigate how incorporating a retrieval engine that does not have access to $\mathrm{x}</em>$ or even jointly training it along with the model, as is done by Guu et al. (2020) and Borgeaud et al. (2021), would affect results.</p>
<p>Despite being able to use reference documents and obtaining comparably high QuestEval scores in our intro generation experiments, upon manual inspection we still found PEER to generate false statements or claims not backed up by the provided documents in many cases. While the ability to cite and quote generally makes it easier to check such</p>
<p>hallucinations, citations can also make the model's generations appear more authoritative, thus making it more likely that users rely on them without explicit fact checking (Nakano et al., 2021).</p>
<p>Finally, we use a very simple approach for representing edits by rewriting the entire paragraph. This makes PEER less efficient than other recent approaches for editing (Logan IV et al., 2021; Reid and Neubig, 2022); also, our inefficient way of representing both inputs and outputs makes it impossible to handle entire documents, which we believe to be crucial for many real-world applications.</p>
<h3>8.2 Evaluation</h3>
<p>Our evaluation is limited in that it only evaluates PEER and other models on a small subset of potential editing tasks in few different domains; all evaluations are performed in English only. Besides, we also explore the collaborative potential of PEER only in a very limited way: While arguably, the ability to follow human-written plans and perform a variety of edits (Table 4) in different domains (Table 3), to make use of reference documents (Table 2), to cite and quote (Table 5), and to autonomously generate plans (Table 6) are important building blocks of a collaborative model, it would be interesting for follow-up work to consider entire sessions of human-AI interactions beyond individual examples like the one shown in Figure 4. However, this requires solving many of the challenges discussed previously, such as having access to an actual retrieval engine that can obtain relevant documents on the fly, finding suitable ways of evaluating texts jointly authored by humans and language models, and improving PEER's efficiency to enable processing entire documents.</p>
<h2>9 Conclusion</h2>
<p>We have introduced PEER, a language model that can act as a writing assistant by following plans to perform a variety of different textual edits, ranging from syntactic and stylistic edits to changing the meaning of a text by removing, updating or adding information. Through extensive experiments, we have shown that training variants of PEER capable of infilling various parts of the editing process enables it to perform edits in different domains, makes it better at following instructions and improves its ability to cite and quote from relevant documents.</p>
<h2>References</h2>
<p>Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Benoît Sagot, and Lucia Specia. 2020. Asset: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4668-4679.</p>
<p>Fernando Alva-Manchego, Louis Martin, Carolina Scarton, and Lucia Specia. 2019. EASSE: Easier automatic sentence simplification evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 49-54, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Abbijeet Awasthi, Sunita Sarawagi, Rasna Goyal, Sabyasachi Ghosh, and Vihari Piratla. 2019. Parallel iterative edit models for local sequence transduction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 42604270, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Alshaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. 2022. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93-104, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, and K Millican. 2021. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Greichen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Autoregressive entity retrieval. In International Conference on Learning Representations.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao, Aida Amini, Qazi Mamunur Rashid, Mike Green, and Kelvin Guu. 2022. Dialog inpainting: Turning documents into dialogs. In International Conference on Machine Learning, pages 4558-4586. PMLR.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-dickstein, et al. 2022. Language model cascades. arXiv preprint arXiv:2207.10342.</p>
<p>Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to fill in the blanks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 24922501, Online. Association for Computational Linguistics.</p>
<p>Wanyu Du, Zae Myung Kim, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. 2022a. Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. In Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022), pages 96-108, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa Lopez, and Dongyeop Kang. 2022b. Understanding iterative revision from humanwritten text. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $3573-3590$.</p>
<p>Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. 2021. Text editing by command. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5259-5274, Online. Association for Computational Linguistics.</p>
<p>Tao Ge, Furu Wei, and Ming Zhou. 2018. Reaching human-level performance in automatic grammatical error correction: An empirical study. arXiv preprint arXiv:1807.01270.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org.</p>
<p>Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. 2020a. Revisiting self-training for neural sequence generation. In International Conference on Learning Representations.</p>
<p>Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020b. Ctrlsum: Towards generic controllable text summarization. arXiv preprint arXiv:2012.04281.</p>
<p>Vu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn. 2018. Iterative backtranslation for neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 18-24, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858.</p>
<p>Tomasz Korbak, Hady Elsahar, German Kruszewski, and Marc Dymetman. 2022. Controlling conditional language models without catastrophic forgetting. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 11499-11528. PMLR.</p>
<p>Mina Lee, Percy Liang, and Qian Yang. 2022. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI '22, New</p>
<p>York, NY, USA. Association for Computing Machinery.</p>
<p>Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. 2022. Diffusionlm improves controllable text generation. arXiv preprint arXiv:2205.14217.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards explainable NLP: A generative explanation framework for text classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5570-5581, Florence, Italy. Association for Computational Linguistics.</p>
<p>Robert L Logan IV, Alexandre Passos, Sameer Singh, and Ming-Wei Chang. 2021. Fruit: Faithfully reflecting updated information in text. arXiv preprint arXiv:2112.08634.</p>
<p>Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, and Guillermo Garrido. 2020. FELIX: Flexible text editing through tagging and insertion. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1244-1255, Online. Association for Computational Linguistics.</p>
<p>Louis Martin, Angela Fan, Éric de la Clergerie, Antoine Bordes, and Benoît Sagot. 2020. Muss: multilingual unsupervised sentence simplification by mining paraphrases. arXiv preprint arXiv:2005.00352.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Kenton Murray and David Chiang. 2018. Correcting length bias in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 212-223, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Courtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. 2015. Ground truth for grammatical error correction metrics. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 588-593, Beijing, China. Association for Computational Linguistics.</p>
<p>Courtney Napoles, Keisuke Sakaguchi, and Joel Tetreault. 2017. Jfleg: A fluency corpus and benchmark for grammatical error correction. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 229-234.</p>
<p>Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simões, Vitaly Nikolaev, and Ryan McDonald. 2021. Planning with Learned Entity Prompts for Abstractive Summarization. Transactions of the Association for Computational Linguistics, 9:1475-1492.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Samuel Broscheit, Aleksandra Piktus, Patrick Lewis, Gautier Izacard, Lucas Hosseini, Jane Dwivedi-Yu, Maria Lomeli, Timo Schick, PierreEmmanuel Mazaré, Armand Joulin, Edouard Grave, and Sebastian Riedel. 2022. Improving wikipedia verifiability with ai.</p>
<p>Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard Grave, Wen-tau Yih, et al. 2021. The web is your oyster-knowledge-intensive nlp against a very large web corpus. arXiv preprint arXiv:2112.09924.</p>
<p>Martin Potthast, Benno Stein, and Robert Gerling. 2008. Automatic vandalism detection in wikipedia. In European conference on information retrieval, pages 663-668. Springer.</p>
<p>Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020. Automatically neutralizing subjective bias in text. In Proceedings of the aaai conference on artificial intelligence, volume 34, pages 480-489.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Technical report, Open AI.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD '20, page 3505-3506, New York, NY, USA. Association for Computing Machinery.</p>
<p>Machel Reid and Graham Neubig. 2022. Learning to model editing processes.</p>
<p>Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. Nist Special Publication Sp, 109:109.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden,</p>
<p>Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Timo Schick and Hinrich Schütze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2021b. Few-shot text generation with natural language instructions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 390-402, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021. Questeval: Summarization asks for fact-based evaluation. arXiv preprint arXiv:2103.12693.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86-96, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Tianxiao Shen, Victor Quach, Regina Barzilay, and Tommi Jaakkola. 2020. Blank language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5186-5198, Online. Association for Computational Linguistics.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784-3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4003-4012, Marseille, France. European Language Resources Association.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, and Bill Dolan. 2021. Automatic document sketching: Generating drafts from analogous texts. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2102-2113, Online. Association for Computational Linguistics.</p>
<p>Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Optimizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401-415.</p>
<p>Diyi Yang, Aaron Halfaker, Robert Kraut, and Eduard Hovy. 2017. Identifying semantic edit intentions from revisions in Wikipedia. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2000-2010, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-and-write: Towards better automatic storytelling. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):7378-7385.</p>
<p>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196, Cambridge, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel</p>
<p>Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.</p>
<h2>A Training Data</h2>
<h2>A. 1 Filtering</h2>
<p>In addition to the filtering rules discussed in Section 5, we also filter out revisions with more than 50,000 characters. This makes preprocessing more efficient, as our algorithm for computing diffs between different revisions has squared complexity in the number of characters. Beyond that, we also filter out revision whose comments contain any of the sequences "#", "[ ${"," },{"}, " \mid{"," },{"$ template", "image", "infobox" and "pic", as these are usually automatically generated or update parts of the page (such as images and infoboxes) that we remove during preprocessing. We further remove all redirects. Within each chunk of the Wikipedia dump, we downsample revisions for which the corresponding comment occurs in more than 10 revisions so that on average, each comment occurs at most 10 times per chunk. Finally, we filter out edits where either the source paragraphs or the target paragraphs have more than 384 tokens.</p>
<h2>A. 2 Retrieving Documents</h2>
<p>To obtain documents $D_{t}$ for an edit that maps $\mathbf{x}<em t_1="t+1">{t}$ to $\mathbf{x}</em>}$, we make use of the set $I_{t}$ of document identifiers occuring in $\mathbf{x<em t_1="t+1">{t}$ or $\mathbf{x}</em>$.}$. For each document identifier, we get the corresponding document from CCNet (Wenzek et al., 2020). We split the document into non-overlapping chunks of 100 words and use the reranker of Side (Petroni et al., 2022) to find the best chunk given $\mathbf{x}_{t+1</p>
<p>If the number of documents obtained from $I_{t}$ is below the maximum number of documents per edit, we also use the entire pipeline of Petroni et al. (2022) to find relevant documents in the Sphere corpus Piktus et al. (2021) given $\mathbf{x}<em t="t">{t+1}$. As this pipeline expects a special [CIT] token at the position for which relevant documents are to be retrieved, we place this token right after the first position at which $\mathbf{x}</em>}$ and $\mathbf{x<em t_1="t+1">{t+1}$ differ, starting from the right. Note that obtaining documents with this approach requires access to $\mathbf{x}</em>$, so it would be impossible to apply this exact same procedure in real-world settings. However, our focus is not on retrieving relevant documents, but on teaching PEER to perform edits given this information.</p>
<p>A.3 Formatting</p>
<p>In addition to the formatting rules discussed in Section 5, we randomly remove the page’s title for 10% of all examples to make sure that PEER can also work with inputs for which no title is available. We <em>minimize</em> 10% of all examples by removing all sentences from both $\mathbf{x}<em t_1="t+1">{t}$ and $\mathbf{x}</em>}$ that are not edited, so that the model also learns to handle and edit single-sentence inputs without context. Finally, to make sure that the model can handle different numbers of reference documents, for 30% of examples we remove $j$ documents from $D_{t}$, where $j$ is uniformly sampled from ${1,\ldots,|D_{t}|}$. However, we only remove documents that are not cited in either $\mathbf{x<em t_1="t+1">{t}$ or $\mathbf{x}</em>$, we reserve up to 16 tokens for its domain, 32 tokens for its title, and 196 tokens for the actual content. We truncate all tokens that exceed these limits. An example of a linearized input and target sequence for PEER-Edit are shown in Figure 7.}$. When linearizing the input and output sequences, for each document $d_{t}^{i}\in D_{t</p>
<h2>Appendix B Control Tokens</h2>
<p>As discussed in Section 4.2, we use <em>control sequences</em> to control the outputs of various PEER models. Unlike <em>Keskar et al. (2019)</em>, we do not introduce special control <em>tokens</em> for this, but simply express all controls in the form key=value where both key and value are tokenized using the language model’s regular tokenizer. We consider the following keys and values:</p>
<ul>
<li>type: We use this key to control the type of output that PEER-Explain is supposed to generate, with possible values being instruction (in which case the output starts with a verb in infitive form) and other.</li>
<li>length: This key controls the length of PEER-Explain’s output. Values include s (less than 2 words), m (2–3 words), l (4–5 words) and xl (≥ 6 words).</li>
<li>overlap: With this key, we control whether there is a word overlap between the edit and the generated output of PEER-Explain; values are true and false.</li>
<li>words: For PEER-Undo, this key is used to control for the difference in the number of words in $\mathbf{x}<em t="t">{t+1}$ and $\mathbf{x}</em>$; accordingly, the possible values are all integers.</li>
<li>contains: This control can be used to ensure that outputs generated by PEER-Document contain a certain substring, which is provided as the value to this key.</li>
</ul>
<h2>Appendix C Generating Synthetic Data</h2>
<p>For obtaining synthetic edits, we sample a single pair $(\mathbf{p}<em t="t">{t},\mathbf{x}</em>})$ for each $\mathbf{x<em t="t">{t+1}$ using top- $p$ sampling with $p=0.9$. We sample the value for the words control token from a normal distribution with $\mu=-10$ and $\sigma=8$, clipped at $-40$ and $10$. These values were chosen to allow for a wide range of different values, while also making sure that on average, forward edits <em>increase</em> the number of tokens. We rewrite each $\mathbf{p}</em>$ with PEER-Explain using the exact same procedure that we use for generating synthetic plans.</p>
<p>For obtaining synthetic plans, we generate 10 different plans with PEER-Explain using top- $p$ sampling with $p=0.9$. For each pair of $\mathbf{x}<em t_1="t+1">{t}$ and $\mathbf{x}</em>$, we use a single control sequence for sampling all 10 plans. We choose the length uniformly from ${s,m,1,x1}$, set type=instruction 80% of the time and overlap=false 80% of the time.</p>
<p>For obtaining synthetic documents, we sample 10 documents from PEER-Document using top- $p$ sampling with $p=0.9$, where contains is set to the quote from this document that is cited in $\mathbf{x}_{t+1}$. We discard all documents that do not actually contain the quote, and then pick the document that maximizes the probability assigned to the actual edit by PEER-Edit.</p>
<h2>Appendix D Training Details</h2>
<p>For training PEER, we start from the T5 implementation in the <em>Transformers</em> library <em>Wolf et al. (2020)</em>. We use DeepSpeed <em>Rasley et al. (2020)</em> to enable more efficient multi-GPU training. We use a maximum learning rate of $10^{-4}$, warmup for 2,000 steps and linear decay. We further use gradient clipping with a maximum norm of 1.0, weight decay of 0.01 and a dropout rate of 0.1.</p>
<h2>Appendix E Downstream Tasks</h2>
<p>The plans used for each of the downstream tasks considered in Section 6.3 are shown in Table 8. We manually wrote instructions for all datasets except ITERATER, for which we directly took instructions from the definitions provided by <em>Du et al. (2022b)</em>.</p>
<p>For most baseline models (T0, GPT3, Instruct-GPT and OPT), we wrap each plan $\mathbf{p}$ for an input</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ To illustrate this, consider an edit that maps $\mathbf{x}<em t_1="t+1">{t}$ to $\mathbf{x}</em>}$ by replacing a single word and assume that for all $j&lt;t$, $\mathbf{x<em t_1="t+1">{j}$ is not in our training data due to one of our filtering rules. In this case, PEER only learns to perform this replacement, whereas WikiLM learns to generate the entire text $\mathbf{x}</em>$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
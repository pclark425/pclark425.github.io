<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9113 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9113</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9113</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-267320303</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.16788v1.pdf" target="_blank">Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate</a></p>
                <p><strong>Paper Abstract:</strong> Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \url{https://github.com/GAIR-NLP/scaleeval}.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9113",
    "paper_id": "paper-267320303",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004059,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate
30 Jan 2024</p>
<p>Steffi Chern 
Carnegie Mellon University</p>
<p>Ethan Chern 
Shanghai Jiao Tong University</p>
<p>Graham Neubig 
Carnegie Mellon University</p>
<p>Pengfei Liu 
Shanghai Jiao Tong University</p>
<p>Shanghai Artificial Intelligence Laboratory 4 Generative AI Research Lab (GAIR)</p>
<p>Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate
30 Jan 2024ECD3C1D87884A624246444CAE3EBAFA5arXiv:2401.16788v1[cs.CL]
Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging.Modern evaluation approaches often use LLMs to assess responses generated by LLMs.However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation.This underscores the urgency of methods for scalable metaevaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios.To fill this gap, we propose SCALEE-VAL, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents.This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during metaevaluation.We release the code for our framework, which is publicly available at: https: //github.com/GAIR-NLP/scaleeval.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) (Bubeck et al., 2023;Gemini Team et al., 2023) have rapidly evolved to the point where they can tackle a wide range of tasks with impressive performance.While this has unlocked a variety of exciting potential applications, it has also introduced complex challenges in evaluating the generated outputs.Current efforts on LLM evaluation primarily focus on automated evaluation metrics (Fu et al., 2023;Li et al., 2023c;Zheng et al., 2023;Wang et al., 2023a), many of which use LLMs themselves to do evaluation.However, when these LLMs as evaluators * Corresponding author are applied to a new task, it begs the question: can LLMs be trusted for evaluation?In many cases, the answer is not clear.</p>
<p>On the other hand, there are a few fortunate tasks where meta-evaluation (evaluation of evaluation metrics) has been performed rigorously ( §2).This meta-evaluation typically involves the collection of human-annotated judgements for particular criteria (e.g.fluency of outputs, semantic adherence to the input).For instance, for machine translation quality metrics, there is an extensive meta-evaluation data from the WMT metrics task (Freitag et al., 2022), and for summarization there are datasets like TAC and RealSum (Dang et al., 2008;Bhandari et al., 2020).Once such a dataset is collected, meta-evaluation can be performed by measuring the correlation between automatic evaluation metrics and the human gold-standard ( §3).</p>
<p>However, these datasets are extremely costly to collect, as they require meticulous annotation by skilled human experts.With the increasing use of LLMs for various purposes such as math problem solving (Hendrycks et al., 2021), reading comprehension (Zhong et al., 2023) LLM-as-a-Judge (Zheng et al., 2023), FairEval (Wang et al., 2023b), ChatEval (Chan et al., 2023), and our own work, SCALEEVAL."Custom."denotes whether the evaluation criterion could be customized."Scala."refers to scalability.(Zheng et al., 2023), multilingual applications (Hu et al., 2020;Bang et al., 2023), and many more, it is not feasible to create these human-judged datasets for every new task.As a result, LLMs as evaluators are used without proper vetting, and in many cases the evaluators themselves are highly unreliable (Wang et al., 2023b;Huang et al., 2023).</p>
<p>In this paper, we propose SCALEEVAL, a scalable meta-evaluation framework for the era of LLMs, which creates meta-evaluation benchmarks across various tasks and scenarios ( §4).Concretely, SCALEEVAL relies on debate between multiple LLM agents, followed by minimal human oversight in cases where the agent LLMs do not agree (Fig. 1).Since our framework allows users to use their own prompts and responses while applying the framework to any scenario or criterion that they define, it offers flexibility and adaptability in various evaluation contexts.</p>
<p>In experiments, we conduct meta-meta evaluation ( §6) demonstrating that our proposed approach correlates well with when meta-evaluation is performed entirely by human expert annotators.Further, we assess the reliability and cost-performance trade-off of various LLMs as evaluators under a variety of scenarios, and closely examine their specific capabilities and limitations as evaluators ( §7).We also examine the impact that variations in prompts used for evaluation can have on the performance of LLMs as evaluators ( §8).</p>
<p>All code from our framework is made available open-source, enabling the community to conduct meta-evaluation on LLMs as evaluators using their own prompts, LLM responses, criteria, and scenarios.</p>
<p>Related Work</p>
<p>Automatic Evaluation of LLM Output</p>
<p>The most common paradigm for evaluating LLMs is to evaluate their capabilities on standard benchmarks for tasks such as reasoning (e.g.BigBench (Srivastava et al., 2022)), common sense QA (e.g.MMLU (Hendrycks et al., 2020)), or code generation (e.g.HumanEval (Chen et al., 2021b)).These are indicative of the capabilities of the models, but do not measure model abilities for openended tasks requiring generation of free-form text.</p>
<p>To adapt to the rapid growth in the capabilities of LLMs for open-ended tasks, LLM evaluation has started to shift towards evaluating generated text directly, often using LLMs themselves as evaluators (Fu et al., 2023;Li et al., 2023c;Zheng et al., 2023;Wang et al., 2023a).In addition, there are a few recent works that perform LLM-based multi-agent debate to improve the fidelity of evaluation (Chan et al., 2023;Li et al., 2023b).While these methods take advantage of the instruction-following capabilities and versatility of LLMs, directly using LLMs as evaluators or communicative agents out-of-thebox in diverse, unseen user-defined scenarios provides no guarantees with respect to the accuracy of these methods.We aim to address this issue by introducing scalable meta-evaluation to ensure the reliability of the evaluation protocol under diverse scenarios.</p>
<p>Another widely used evaluation platform, Chatbot Arena (Zheng et al., 2023) supports a crowdsourcing method to collect diverse user prompts from various scenarios.However, the process of evaluating LLMs' performance in Chatbot Arena relies heavily on human evaluations, which may not be readily accessible to everyone interested in assessing LLMs' abilities for a specific tasks or scenario.In addition, the human evaluators involved are not subject to a uniform set of standards or explicit evaluation guidelines, which could lead to biased or imprecise evaluation assessments.</p>
<p>Meta-Evaluation of LLMs as Evaluators</p>
<p>Previous research proposing methods for LLMs as evaluators usually involves conducting metaevaluation in 3 different ways: (i) leveraging existing NLP meta-evaluation benchmarks (Fu et al., 2023;Chan et al., 2023), (ii) conducting small-scale meta-evaluations on expert-annotated datasets for specific tasks or scenarios (Chiang and Lee, 2023;Wang et al., 2023a;Zheng et al., 2023), or (iii) using crowd-sourcing platforms to collect human annotations (Zheng et al., 2023).However, due to the lack of coverage in existing datasets and annotation budgets, both (i) and (ii) are inherently limited in their comprehensiveness.(iii) can provide more comprehensive meta-evaluation via crowd-sourcing, but the amount of human annotation required in the meta-evaluation process limits the scalability of the approach, and crowd workers may not be particularly accurate at more complex tasks.To address these issues, we propose an agent-debate-assisted meta-evaluation approach to mitigate this effort.</p>
<p>Preliminaries</p>
<p>In this section, we provide an introduction to the concepts of automatic evaluation and metaevaluation systems, particularly focused on evaluation of LLM-generated outputs in the era of generative AI.</p>
<p>Key Terms</p>
<p>We first define some key terms that will be used throughout our paper.</p>
<p>• Criterion: A criterion defines a standard that measures the quality of the response generated by LLMs based on the user prompt.Some examples include: helpfulness, fluency, factuality, or creativity, among others.• Scenario: A scenario describes the real-world situations in which users are interacting with LLMs.For example, brainstorming, coding, and dialog, among others.</p>
<p>Automatic Evaluation</p>
<p>Automatic evaluation using LLMs measures the quality of LLM-generated responses given prompts under different criteria.Usually, automatic evaluation is conducted with one of two different protocols: single-response evaluation and pairwise response comparison (Ouyang et al., 2022;Zheng et al., 2023;Li et al., 2023a).In this paper, we focus on pairwise response comparison.Pairwise response comparison is intuitive for both humans and LLMs as evaluators when conducting assessments.It could be further extended to provide winrates and Elo scores across models (Zheng et al., 2023), offering a straightforward leaderboard to understand the relative performance of different models under various scenarios.Formally, given an automatic evaluation metric E, a user-defined evaluation criterion c (e.g.helpfulness, reasoning, creativity), a user prompt p, and responses generated by two systems r 1 , r 2 , evaluation for pairwise response comparison is done in the following way:
o = E(c, p, r 1 , r 2 ).(1)
o ∈ {1, 0, −1} represents that r 1 is better, equal, or worse than r 2 , respectively, given the user prompt p under criterion c.</p>
<p>Meta-Evaluation</p>
<p>Meta-evaluation assesses the quality of an automatic evaluation metric.Formally, we define a gold-standard evaluation metric G (e.g.human experts) that other automatic metrics should aspire to match.In pairwise response comparison, the meta-
evaluation dataset G = {G(c, p i , r 1,i , r 2,i )} n i=1
contains user prompts and corresponding responses from two systems, annotated with gold-standard evaluations.The meta-evaluation process assesses the performance META(E) of the automatic evaluation metric E under a certain criterion c.</p>
<p>In pairwise response comparison, the metaevaluation measures the example-level agreement rate or the system-level agreement rate between E and G across the meta-evaluation dataset.A high agreement rate between E and G represents that E is a good automatic evaluation metric.</p>
<p>For the example-level agreement rate, we calculate:
META(E) = 1 n n i=1 δ E(c,p i ,r 1,i ,r 2,i ),G(c,p i ,r 1,i ,r 2,i ) ,
(2) where 0 ≤ META(E) ≤ 1, and δ •,• refers to the Kronecker delta function.</p>
<p>For the system-level agreement rate, given that
E = {E(c, p i , r 1,i , r 2,i )} n i=1 and G = {G(c, p i , r 1,i , r 2,i )} n i=1 , we calculate: META(E) = δ mode(E),mode(G) ,(3)
where META(E) ∈ {0, 1}, δ •,• refers to the Kronecker delta function, and mode(•) refers to the value (either 1, 0, −1 in this case) that appears most often in the set E or G.</p>
<p>Methodology</p>
<p>In this section, we detail the frameworks that SCALEEVAL employs for meta-evaluation, evaluation, and human expert meta-meta evaluation.</p>
<p>For meta-evaluation, we generally follow the pairwise response comparison setting described in §3.3.Notably, instead of relying solely on human labor to construct the meta-evaluation benchmark G, we use a scalable, agent-debate assisted framework to instantiate the golden metric G and construct the benchmark G.For evaluation, we follow the pairwise response comparison setting outlined in §3.2.The meta-meta evaluation process also follows the rules for meta-evaluation, as described in §3.3.The process is included to ensure the reliability of using the agent-debate assisted meta-evaluation framework.</p>
<p>Meta-Evaluation Framework via Multi-Agent Debate</p>
<p>The meta-evaluation framework involves multiple communicative agents {A j } m j=1 that conduct rounds of discussion d = 0 ∼ D − 1 with each other.This is less time-consuming and costly compared to traditional methods for meta-evaluation that relies entirely on human effort.With this agentdebate-assisted meta-evaluation framework, we can leverage each LLM agent's distinct understanding about each query prompt p i , LLM responses r 1,i , r 2,i , and defined criterion c to make a comprehensive assessment of LLMs under different scenarios and criteria.Each LLM agent is capable of providing an evaluation result regarding which response is better, along with its corresponding justifications.Note that each LLM agent can also review other agents' evaluation results and justifications after the initial round of discussion.</p>
<p>In the initial round of discussion d = 0, each LLM agent independently provides an evaluation result and justification:
A 0 = [A 1 (c, p i , r 1,i , r 2,i , ∅), . . . , A m (c, p i , r 1,i , r 2,i , ∅)], (4) where A 0 [j] j=1,...,m ∈ ({1, 0, −1}, JUSTIFICATION),
(5) indicates whether r 1,i is better, equal, or worse than r 2,i , respectively, along with its justification.Note that the ∅ in the last argument of A j represents that in the initial round of discussion, each agent doesn't have access to previous rounds of discussion.In subsequent discussion rounds d = 1 ∼ D − 1, agents are allowed to look at other agents' previous assessments and conduct re-evaluations, in which each agent is prompted to stick with or change their original evaluation result.Specifically, given A d−1 (d ≥ 1), which represents the evaluation results and justifications of agents after (d − 1) th rounds of discussions, we conduct the d th round of discussion:
A d = [A 1 (c, p i , r 1,i , r 2,i , A d−1 ), . . . , A m (c, p i , r 1,i , r 2,i , A d−1 )] (6)
where similarly to A 0 ,
A d [j] j=1,...,m ∈ ({1, 0, −1}, JUSTIFICATION),
(7) The detailed prompt template for meta-evaluation can be found in Table 6 under Appendix.</p>
<p>In cases where agents fail to reach a consensus after d = D − 1 rounds of discussions, a human evaluator intervenes.The human evaluator reviews the assessment reports provided by the agents and makes a final decision.Through this process, we incorporate an element of human oversight, thereby increasing the reliability of the final decision.This approach strikes a balance between efficiency and the need for human judgment, ensuring that evaluations are done in a timely and accurate manner.An example of the multi-agent debate process during meta-evaluation is demonstrated in Fig. 2.</p>
<p>Evaluation Framework</p>
<p>We follow the pairwise response comparison setting outlined in §3.2.Note that in the LLM era, the automatic evaluation metric E is often instantiated through single LLMs (Fu et al., 2023;Li et al., 2023c;Zheng et al., 2023;Wang et al., 2023a), or multi-agent debate (Chan et al., 2023;Li et al., 2023b).In SCALEEVAL, we focus on instantiating E through single LLMs (e.g., gpt-3.5-turbo).However, it is important to note that our framework can be further generalized to other instantiations of E.</p>
<p>Human Expert Meta-Meta Evaluation</p>
<p>To test the reliability of our proposed metaevaluation framework, we apply meta-meta evaluation.The meta-meta evaluation process also follows the meta-evaluation process described in §3.3, where E is instantiated as the agent-debated assisted protocol as described in §4.1, and G is instantiated as the human expert annotation protocol.</p>
<p>Examined Scenarios</p>
<p>Establishing real-life scenarios that reflect individuals' daily usage is key to assess the performance</p>
<p>Initial Evaluation</p>
<p>Here is my evaluation: Submission 1 provides a helpful and detailed list of suitable outdoor activities... Submission 2 is more superficial.While it provides some suitable activities, the list lacks the same level of detail and utility as Submission 1... Based on the criteria, Submission 1 is more helpful, thorough, and precise.Therefore, my judgement is: 1 Both submissions are relevant and clear.Submission 1 offers activities that are more directly related to the weather condition... Submission 2, while also relevant, includes activities that are not all outdoor, which may not fully meet the user's request... Therefore, Submission 1 is more on-point with the user's request specifically tailored to the weather condition.1</p>
<p>Speaker 3</p>
<p>Upon reviewing the evaluations, I acknowledge the valid points made by Speaker 1 and Speaker 3... Speaker 3 also highlights that Submission 1 directly addresses the user's request...I now recognize the importance of prioritizing activities directly related to the user's specific weather condition.Therefore, I will adjust my evaluation and choose: 1 Speaker 2 Speaker 3</p>
<p>Upon reviewing the evaluations from Speaker 1 and Speaker 2, I notice that Speaker 1 initially found Submission 1 to be more on-point and detailed... Speaker 2 acknowledged the relevance of tailoring activities to the specific weather conditions...I maintain my original choice, agreeing with the other speakers that Submission 1 is better 1</p>
<p>Submission 1</p>
<p>Here is a list of outdoor activities… -Hiking on trails that are covered or have shelter nearby in case of rain -Biking along paved paths or road -Jogging or running along routes with shelter options -Picnicking under shelters or tents at parks ...</p>
<p>Here is a list of outdoor activities… • Visit a botanical garden or arboretum.These gardens are often covered in canopies of trees... • Go for a walk or hike in the woods.The trees can also help to block some of the rain... • Visit a museum or art gallery.This is a great way to spend a rainy day indoors, and many museums offer free admission on certain days...</p>
<p>Submission 2 Criteria</p>
<p>Helpfulness:</p>
<p>"1": "Not Helpful -The response is completely unrelated..." "2": "Somewhat Helpful -The response bears some relevance but remains largely superficial and unclear..." "3": "Moderately Helpful -The response is mostly relevant and clear... but lacks depth and comprehensive elucidation.""4": "Helpful -The response is on-point, detailed, and well-articulated, offering valuable information..." "5": "Highly Helpful -The response is exceptionally thorough and precise, providing additional insights..." and limitations of LLMs in a comprehensive manner.In the current instantiation of SCALEEVAL, we include 8 different scenarios that are closely related to everyday situations and tasks (Liang et al., 2022;Li et al., 2023a).Some example prompts for each defined scenario is shown in Table 2.We describe more about exactly how we collect data for each of these scenarios below.Individuals interested in evaluating LLMs with our framework can supplement their assessment with additional scenarios.</p>
<p>Brainstorming The brainstorming scenario is designed to test the LLMs' ability to engage in problem-solving, creative ideation, and generation of insightful responses, especially in situations that require critical thinking and detailed, step-by-step reasoning.</p>
<p>Coding The code scenario evaluates LLMs' ability to comprehend, produce, and debug code, as well as answering coding-related questions.</p>
<p>Dialog The dialog scenario measures LLMs' ability to engage with users in a manner that is intuitive, human-like, and dynamic, testing their proficiency through context-sensitive conversations and role-playing that require maintaining a consistent persona throughout a series of interactions.</p>
<p>Judgement The judgement scenario assesses LLMs' ability to make inferences and formulate opinions, including soliciting insights on diverse situations or emotions, and posing questions that require logical thinking or reasoning.</p>
<p>Math The math scenario evaluates the LLMs' proficiency in understanding and solving mathematical problems, emphasizing their accuracy in tasks ranging from simple calculations to complex reasoning.</p>
<p>Open-Domain General (ODG)</p>
<p>The ODG scenario measures LLMs' proficiency in applying diverse knowledge and exercising reasoning across a wide array of topics, such as answering questions with definitive answers.</p>
<p>Open-Domain Science (ODS)</p>
<p>The ODS scenario tests the LLMs' application of scientific knowledge, and gauges their ability to accurately interpret and respond to queries related to scientific disciplines like biology, chemistry, physics, astronomy, and more.</p>
<p>Scenario Examples</p>
<p>Brainstorming -Can you tell me how to make chocolate chip cookies?-Make a list of snacks and foods to serve as party snacks on a game day!</p>
<p>Coding</p>
<p>-What is the difference between HTML and JavaScript? -Implement a binary search algorithm to find a specific element in a sorted array.</p>
<p>Dialog</p>
<p>-Act as the Norse Goddess Freyja.</p>
<p>-Can you think and feel like a human?</p>
<p>Judgement</p>
<p>-What if the Aztecs had successfully repelled the Spanish conquistadors? -How can you determine if a person is genuinely interested in a conversation or simply being polite?</p>
<p>Math -Given that f(x) = 5x 3 -2x + 3, find the value of f(2).</p>
<p>-If the endpoints of a line segment are (2, -2) and (10,4), what is the length of the segment?</p>
<p>ODG</p>
<p>-Is there a meaning for Christmas wreaths?-What are some of the best universities for studying robotics?</p>
<p>ODS</p>
<p>-What causes the northern lights?-What do the different octane values of gasoline mean?</p>
<p>Writing -Can you help me write a formal email to a potential business partner proposing a joint venture?-Take MLK speech "I had a dream" but turn it into a top 100 rap song.Writing The writing scenario evaluates LLMs' ability to summarize, translate, and generate various texts, testing their core language processing and production skills.</p>
<p>6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate</p>
<p>In this section, we first perform meta-metaevaluation, examining whether the meta-evaluation results of using SCALEEVAL match closely to those resulting from meta-evaluation using human evaluators.</p>
<p>Setup For our SCALEEVAL meta-evaluation framework (as described in §4.1), we deploy three LLM agents to perform multi-agent debate: gpt-4turbo, claude-2, and gpt-3.5-turbo. 1In our metaevaluation experiment, we analyze a total of 160 prompts.This set is comprised 137 prompts from AlpacaEval (Li et al., 2023c), 10 coding problem prompts from HumanEval (Chen et al., 2021a), and 13 math problem prompts from GSM-Hard (Gao et al., 2022).We categorize these prompts into four distinct scenarios: brainstorming, coding, math, and writing, where each scenario contains 40 prompts.</p>
<p>1 Results collected in December 2023.Specific models used are: gpt-4-1106-preview, claude-2, and gpt-3.5-turbo-1106.</p>
<p>Each scenario is evaluated based on the following criteria, respectively: helpfulness, interpretability, reasoning, and creativity.We evaluate the generated responses from the following three LLMs: gpt-3.5-turbo,claude-instant, and gemini-pro.We select the above LLMs to evaluate due to their rather similar performances according to past research and public user feedback, which can help us establish a more nuanced understanding of their performance in various real-world scenarios, and to identify specific contexts where one may outperform the others.</p>
<p>Our meta-meta evaluation involves having human experts annotate which LLM submission they think is better based on a defined criterion during pairwise comparisons.A total of seven human experts were selected from a pool of Carnegie Mellon University students who have the relevant expertise in answering the queries in each scenario.Different groups of three human experts are responsible for answering the prompts in each scenario, where they are assigned to the scenario that relates to their expertise.Each expert received identical instructions for the task -they were asked to decide which submission is better based on our defined criteria, and for each comparison, label either 0 (neither submission is better), 1 (submission 1 is better), or 2 (submission 2 is better).The label 2 corresponds to the label -1 as denoted in section 3.2.The experts were tasked to conduct 30 comparisons for each of the four different scenarios (brainstorming, coding, math, and writing), based on their corresponding defined criteria (helpfulness, interpretability, reasoning, and creativity).This results in a total of 120 final judgements.The question prompts, LLM responses, and criteria utilized for human expert annotations were consistent with those used during our meta-evaluation experiment.All the details were presented in a google sheet that allowed experts to record their answers.</p>
<p>Q1: Can LLM agents with multi-agent debate be used as meta-evaluators in new user-defined scenarios?To validate the reliability of SCALEE-VAL's meta-evaluation framework, we perform comparisons between the results from human experts and SCALEEVAL's multi-agent debate by two key metrics: the example-level agreement rate and the system-level agreement rate, as mentioned in §3.3.The example-level agreement rate measures the proportion of instances where the multi-agent debate results correspond with the human experts judgements.On the other hand, the system-level agreement rate assesses whether the human experts and multi-agents concur in their overall evaluation of which LLMs produce the best responses for each scenario.A high agreement rate in both metrics would suggest a strong reliability and validity of our meta-evaluation framework, indicating that both human and LLM agents consistently recognize and agree on the quality of responses generated by LLMs.</p>
<p>Results</p>
<p>From Table 3, we generally observe a higher example-level agreement rate between human experts and SCALEEVAL, compared to the agreement rate between human experts and individual LLM evaluations.The consistently high agreement rates observed suggest that our metaevaluation framework aligns well with human expert judgments in these areas, indicating a reliable performance of the collective use of LLMs in metaevaluating complex scenarios.Across all LLM submission comparisons in our experiment, we observe higher agreement rates in decisions between SCALEEVAL outcomes and those of human experts, particularly in coding and math scenarios.This observed trend could be attributed to the inherently objective nature of these subjects, which have relatively clear, definitive answers unlike more subjective areas like creative writing.</p>
<p>Based on Fig. 3, we notice a consistent "preference in the same direction" between human experts and multi-agent debates across all LLM pairwise comparisons and scenarios.Notably, gpt-3.5turbo is favored (higher win rates) in brainstorming, math, and writing scenarios when compared with claude-instant.Similarly, gemini-pro is also preferred over claude-instant in all scenarios.When comparing gpt-3.5-turbowith gemini-pro, a varied pattern in decision outcomes is observed: both human experts and multi-agent systems agree that gpt-3.5-turbooutperforms gemini-pro in scenarios involving math and writing.Conversely, geminipro is deemed superior in brainstorming and coding scenarios.The high agreement of multi-agent preferences with human expert judgement results verifies the reliability of using multiple LLMs agents as meta-evaluators in various user-defined scenarios.</p>
<p>Exp-II: Meta-Evaluation vs. LLM Evaluators</p>
<p>Next, we use the fact that SCALEEVAL allows for reliable and scalable meta-evaluation to examine the traits of LLMs as evaluators.</p>
<p>Q2: What are the capabilities and limitations of each LLM evaluator?To effectively evaluate the performance of each LLM in its role as an evaluator, we adopt an approach that involves comparing the outcomes from our meta-evaluation process with the evaluations made independently by each LLM evaluator, which uncovers any disagreements or alignments between them.In the process, we aim to shed light on the performance characteristics of each LLM evaluator, which helps us identify which of them demonstrate superior evaluative abilities, thereby contributing to our understanding of their reliability in evaluating responses under each scenario.In addition, we provide a comprehensive cost-performance analysis to decide which LLM evaluator is the most suitable choice in each scenario.</p>
<p>Setup For meta-evaluation, we employed three LLMs (gpt-4-turbo, claude-2, and gpt-3.5-turbo)as evaluators to perform pairwise comparisons of responses from three distinct LLMs: gpt-3.5-turbo,claude-instant, and gemini-pro.Previous studies have highlighted the presence of positional biases when LLMs are used as evaluators (Wang et al., 2023b).In response to these findings, we have implemented a strategy of randomization to mitigate such biases.Specifically, the sequence in which submissions from LLMs are presented to the agent evaluators is randomized.Additionally, we also randomize the order of discussions for each agent evaluator in every case.These approaches ensure that the process is fair and unbiased as much as possible, allowing for a more accurate assessment of the LLM evaluators' performance.The metaevaluations were done under the following 8 scenarios: brainstorming, coding, dialog, judgement, open-domain general, open-domain science, and writing, with the same set of 4 criteria used during human expert annotation.</p>
<p>Results Table 4 compares the agreement rate between SCALEEVAL's meta-evaluation and each LLM evaluator across criteria and scenarios.We observe that gpt-4-turbo, when serving as an evaluator, has the highest agreement rates with our meta-evaluation, particularly in the scenarios of brainstorming, dialog, and ODG with the helpfulness criterion.It stands out with the highest overall average score of 0.780.However, our selected open-source model evaluator, auto-j, outperforms gpt-4-turbo in evaluating coding questions based on the helpfulness criterion.In addition, it exhibits the highest agreement rate with our metaevaluation in the judgement scenario, according to the helpfulness criterion, indicating it as the most capable evaluator in this setting.It also achieves comparable results with other closed-source models like claude-2 and gpt-3.5-turbo in most of the other scenarios.</p>
<p>While gpt-4-turbo performs the best as an evaluator in a majority of scenarios, it is not necessarily the best choice when we take into consideration its relatively high API costs.In fact, both the more affordable version (gpt-3.5-turbo)and our selected free, open-source model (auto-j) show comparable performance in scenarios like judgement and writing.For coding-related evaluations, the slightly less expensive claude-2 could be a more cost-effective alternative to gpt-4-turbo.</p>
<p>8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations Q3: How do the qualities of criteria prompts influence the robustness of LLMs as evaluators in different scenarios?Prior studies have revealed that variations in prompts can substantially affect the behavior of LLMs, particularly with the text they generate.With this in mind, we define various formatted criteria for evaluating LLM responses under each scenario.This approach aims to examine the extent to which different formats of criteria prompts influence both the performance and robustness of LLMs as evaluators.</p>
<p>Setup We define five variations of the same criteria prompts: shortened, gibberish, shuffled, flipped, and masked (see Table 7 under Appendix A for detailed format).With these criteria format variations, we intend to observe how the LLMs as evaluators would respond differently when conducting evaluation.We compare the example-level agreement rate between SCALEEVAL's meta-evaluation results and each LLM evaluator.</p>
<p>Results Based on Table 5, we observe that the performance of LLMs as evaluators generally deteriorates when certain letters in the criteria prompts are masked.Furthermore, the removal of guiding phrases at the beginning, such as "Not Helpful" or "Highly Helpful", can also diminish their effectiveness as evaluators.Both gpt-4-turbo and gpt-3.5-turbodemonstrate some resilience to these adversarially formatted criteria prompts, maintaining a relatively consistent agreement rates across various criteria formats.In contrast, Claude-2 often showcases confusion and refuses to evaluate, particularly in cases with gibberish and masked criteria prompts, where it rejects answering about half of the questions.It typically responds with statements like, "Unfortunately I do not have enough information here to provide a fair evaluation...The criteria describe different quality levels, but there is no detail on what specific aspects of the responses should be assessed... any judgement risks being arbitrary or biased...".None of the LLMs as evaluators we tested maintained very similar evaluation capabilities when faced with these adversarially formatted criteria prompts, indicating a limitation in these LLMs as evaluators' current design and application.Despite their advanced capabilities in fulfilling a variety of tasks, they may still struggle with understanding and responding accurately to substituted criteria information, highlighting an area for potential improvement in future iterations of LLM technology.Among all the different formatted criteria, we highlight the cases where the LLMs perform the best as evaluators in Table 5.</p>
<p>Conclusion</p>
<p>In this work, we propose SCALEEVAL, a scalable, agent-debate assisted meta-evaluation framework for assessing the reliability and robustness of LLMs as evaluators.This approach addresses the expensive and time-intensive challenges inherent in traditional meta-evaluation methods, particularly pertinent as the usage of LLMs expands, necessitating a more scalable solution.Through our research, we have not only demonstrated the reliability of our proposed meta-evaluation framework, but also shed light on the capabilities and limitations of LLMs as evaluators in various scenarios.We observe how the results from these LLMs as evaluators vary based on modifications to the same criteria prompts.By open-sourcing our framework, we aim to foster further research in this field and encourage the development of more advanced and reliable LLMs as evaluators in the future.</p>
<p><Type 1: General Format Version> "1": "Not Helpful -The response is completely unrelated, lacks coherence, and fails to provide any meaningful information.""2": "Somewhat Helpful -The response bears some relevance but remains largely superficial and unclear, addressing only the peripheral aspects of the user's needs.""3": "Moderately Helpful -The response is mostly relevant and clear, covering the basic aspects of the query, but lacks depth and comprehensive elucidation.""4": "Helpful -The response is on-point, detailed, and well-articulated, offering valuable information and clarifications that meet the user's primary needs and enhance understanding.""5": "Highly Helpful -The response is exceptionally thorough and precise, providing additional insights and valuable supplementary information."</p>
<p><Type 2: Shortened Format Version> "1": "The response is completely unrelated, lacks coherence, and fails to provide any meaningful information.""2": "The response bears some relevance but remains largely superficial and unclear, addressing only the peripheral aspects of the user's needs.""3": "The response is mostly relevant and clear, covering the basic aspects of the query, but lacks depth and comprehensive elucidation.""4": "The response is on-point, detailed, and well-articulated, offering valuable information and clarifications that meet the user's primary needs and enhance understanding.""5": "The response is exceptionally thorough and precise, providing additional insights and valuable supplementary information."</p>
<p><Type 3: Gibberish Format Version> "1": "N<em>t H$l%ful -Th$ r$sp0n$e is c mplt$l? unr = Cla7$d, la$ks c()h$r$n( = C, and f#i/s t# p$o&amp;id$ any m = Can</em>&amp;gful !format$on." "2": "S#m$<em>ha+ H$%</em>fu/ -Th$ r#s0!n$ b%ars $o/e re$ev<em>nc$ b$t r$ma$n$ l#rg$l4 $u/7$r7cial an</em> !ncl=4r, a6r$ss@n4 o7ly th$ p$r4ph@r$l a5p$cts #f th$ $s<em>r's n</em><em>ds.""3": "M$!7r$t#ly H$lpfu&amp; -Th$ r@s0</em>n$@ !s m$%stl = C r$'$van7 an cl$ar, c$%$r$n4 th$ ba$!c a$%cts of th$ qu = Cry, b$t l#cks d$pth an cmpr$h$ns$v$ lu$7$dat!on." "4": "H$lpfu&amp; -Th$ r!s0<em>n$e !s o/7-p$!nt, d$ta$!l$d, an w$l/-a&amp;!u/at$d, #ff$r!n4 v#l$%bl$ #nformat$on and cl</em>r$!cat!ons th#t m=t th$ u/7$rś pr!/ary n$$ds an<em> @n7anc$ un#rstand!n4." "5": "H4#h7y H$!p%u&amp; -Th$ r$s&amp;</em>n!e !s $xc$pt$#nally th#r#7gh an<em> pr$c$%$, pr#v$d$n# a4</em>!t$#nal !ns$4hts an<em> v#lu%bl$ @</em>pp%$%ntary #n%ormat$on."</p>
<p><Type 4: Shuffled Format Version> "1": "coherence fails provide unrelated, completely response -and the meaningful any to lacks Not Helpful is The information.""2": "superficial response largely addressing unclear, remains only needs.-relevance user's and the Helpful the peripheral some bears but aspects Somewhat The of" "3": "basic aspects query, lacks Moderately covering clear, -Helpful is depth response and comprehensive elucidation.relevant mostly the The and the of but" "4": "clarifications the is response information needs enhance and Helpful -on-point, valuable well-articulated, offering understanding.The and detailed, primary that user's meet" "5": "valuable Highly response is providing -the exceptionally Helpful information.insights thorough and additional precise, supplementary and The" <Type 5: Flipped Format Version> "1": "toN lufpleH -ehT esnopser si yletelpmoc detalernu, skcal ecnerehoc, dna sliaf ot edivorp yna lufgninaem noitamrofni.""2": "tamewoS lufpleH -ehT esnopser sraeb emos ecnaveler tub sniamer ylegral laicifrepus dna raelcnu, gnisserdda ylno eht larehpirep stcepsa fo eht s'resu sdeen.""3": "yletaredoM lufpleH -ehT esnopser si yltsom tnaveler dna raelc, gnirevoc eht cisab stcepsa fo eht yreuq, tub skcal htped dna evisneherpmoc noitadicule.""4": "lufpleH -ehT esnopser si tniop-no, deliated, dna detalucitra-llew, gnireffo elbaulav noitamrofni dna snoitacifralc taht teem eht s'resu yramirp sdeen dna ecnahne gnidnatsrednu.""5": "ylhgiH lufpleH -ehT esnopser si yllanoitpecxe hguoroht dna esicerp, gnidivorp lanoitidda sthgisni dna elbaulav yratnemelppus noitamrofni."</p>
<p><Type 6: Masked Format Version> "1": "N__ H_l_ful -The r__pnse is c_m__et__y unr_l_te_, lacks <em>ohe_en_e, _nd _ai_s to p_ov_de _ny m_a__ngfu</em> <em>nfo_ma_ion.""2": "_om_w_at He_p_ul -T_e re_ponse be_rs _ome rel__a_ce but r__ains la__ely s__erfi__al and u_cle__, ad_res__ng onl</em> <em>he __ri__er_l a_pe_ts of t__ u_e</em>'s ne_ds.""3": "Mod<strong><em>tely </em>elp</strong>l -Th_ <em>esp__se is mos__y re__va_t an</em> <em>le_r, c_v__ing the ba_ic _spe_ts of the q_e_y, but __cks _e_th and co_preh_ns_ve el_c_d_t_on.""4": "__lpful -_he respo_se is on-p_in</em>, d<strong><em>iled, and we</em>l-ar_icu_ated, of_er_ng val_ab_e </strong>for_ation and cl_r_fi__t_ons t_at mee_ the <em>se</em>'s p_im_r_ <em>eeds and en__nce u_de__tan_ing.""5": "Hi_h_y H__p_ul -The r_spon_e is e_c_p_io_al__ th_r_ugh and p_ec_se, pr_vi_ing a_di__on_l ins_g_ts and va_u_b_e _upp_e_en_a_y inf_rma_io</em>."</p>
<p>Figure 1 :
1
Figure1: We demonstrate SCALEEVAL, our scalable meta-evaluation framework.This is used in assessing the reliability and robustness of employing LLMs as evaluators for different evaluative purposes.</p>
<p>Submission 1: The response is directly on-point and provides a detailed list of outdoor activities... Submission 2: The response is also directly on-point...It offers additional tips for enjoying outdoor activities in the rain, enriching the context and providing valuable supplementary information… Therefore, I choose: 2 Speaker 2 Thank you for the thoughtful question...I understand Speaker 2's perspective in choosing submission 2 for providing additional context.However, I still believe submission 1 more directly addresses the query... Speaker 3 reinforces my view... other perspectives don't convince me to change my original choice. 1 Speaker 1</p>
<p>Figure 2 :
2
Figure 2: An example of the multi-agent debate process during meta-evaluation.</p>
<p>Figure 4 :
4
Figure 4: Human Fleiss Kappa for each LLM pairwise comparison under four scenarios.</p>
<p>Table 1 :
1
, creative writing Comparison of the meta-evaluation processes across different strategies using LLMs as evaluators:
Meta-Eval # Scenarios Custom. Scala.LLM-as-a-JudgeHumanHigh✗LowFairEvalHumanLow✗LowChatEvalHumanLow✗LowSCALEEVALAgent DebateHigh✓High</p>
<p>Table 2 :
2
Examined scenarios and corresponding selected examples.</p>
<p>Table 7 :
7
Criteria prompt format variations for helpfulness</p>
<p>AcknowledgementsWe thank Chunting Zhou, Weizhe Yuan, Chunpu Xu, Yan Ma, and Binjie Wang for the helpful discussions and feedback.Scenario Meta-Evaluation GPT-4-Turbo Claude-2 GPT-3.5-TurboA Meta-Evaluation Prompt <Initial Evaluation>Compare the two submissions based on the criteria above.Which one is better?First, provide a step-by-step explanation of your evaluation reasoning according to the criteria.Avoid any potential bias.Ensure that the order in which the submissions were presented does not affect your judgement.Keep your explanation strictly under 150 words.Afterwards, choose one of the following options: Submission 1 is better: "1" Submission 2 is better: "2" Neither is better: "0" Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds to your reasoning.At the end, repeat just the number again by itself on a new line.[Question]: {question} Afterwards, choose one of the following options: Submission 1 is better: "1" Submission 2 is better: "2" Neither is better: "0" Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds to your reasoning.At the end, repeat just the number again by itself on a new line.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, arXiv:2302.04023v3A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023</p>
<p>Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, arXiv:2010.07100Re-evaluating evaluation in text summarization. 2020arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu ; Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Josh Achiam, Vedant Misra, arXiv:2308.07201Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis. Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino; Andrew N. Carr; Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder; Bob McGrew, Dario Amodei, Sam McCandlish2023. Jan Leike,arXiv preprintChateval: Towards better llm-based evaluators through multi-agent debate. Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021barXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.019372023arXiv preprint</p>
<p>Overview of the tac 2008 update summarization task. Trang Hoa, Karolina Dang, Owczarzak, TAC2008</p>
<p>Results of wmt22 metrics shared task: Stop using bleu-neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, André Ft Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)2022</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2211.10435Pal: Program-aided language models. 2022arXiv preprint</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, arXiv:2003.11080v5Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. 2020</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu, arXiv:2310.05470Generative judge for evaluating alignment. 2023aarXiv preprint</p>
<p>Ruosen Li, arXiv:2307.02762Teerth Patel, and Xinya Du. 2023b. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. 2023c. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048Is chatgpt a good nlg evaluator? a preliminary study. 2023aarXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, ArXiv, abs/2305.179262023b</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2023arXiv preprint</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.06364v22023</p>            </div>
        </div>

    </div>
</body>
</html>
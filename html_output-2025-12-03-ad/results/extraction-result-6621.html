<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6621 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6621</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6621</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-28a8604d0ca174d885f5bffde6b6b1d3f955f5bd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28a8604d0ca174d885f5bffde6b6b1d3f955f5bd" target="_blank">Self-Attentive Associative Memory</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory) through a novel Self-attentive Associative Memory (SAM) operator, and achieves competitive results with the proposed two-memory model.</p>
                <p><strong>Paper Abstract:</strong> Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6621.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6621.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAM-based Two-memory Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential model that explicitly separates item memory (auto-associative matrix) and relational memory (set of hetero-associative matrices) connected via a Self-attentive Associative Memory (SAM) operator based on outer-product attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>STM (SAM-based Two-memory Model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-memory architecture: an item memory M^i implemented as a d×d auto-associative matrix (updated with gated outer-product writes) and a relational memory M^r implemented as an n_q×d×d tensor built by SAM (outer-product self-attention) from M^i; M^r is read via two-step contractions and distilled back to M^i and outputs via learned high-dimensional transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dual memory: auto-associative item memory (matrix) + high-order relational memory (3-D tensor / hetero-associative memories)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Item memory stores outer-product associations X_t (d×d); relational memory stores summed outer-product pairwise relationships as matrices (n_q × d × d), representing bit-level interactions between item pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Writes to item memory: gated updates combining previous M^i and X_t=f1(x)⊗f2(x); SAM reads M^i via learned W_q/W_k/W_v to produce queries/keys/values and constructs M^r using elementwise scaling + outer products; reads from M^r via a softmax-weighted contraction (softmax(z^T) M^r f2(x)); transfer: high-dimensional transforms flatten/transform M^r back into M^i and into output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (Associative retrieval, Nth-farthest, Relational Associative Recall (RAR), Copy, Priority Sort, Convex hull, TSP, Shortest path, Minimum spanning tree, Atari Pong RL, bAbI QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>memorization and relational reasoning across algorithmic tasks, geometric/graph reasoning, reinforcement learning, and question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>See per-task examples: Associative retrieval (sequence length 30/50): converged in 10 / 20 epochs with final test accuracy 100%; Nth-farthest (8 vectors): accuracy 98% (n_q=8); RAR: ~1 bit error per sequence (~87% items perfectly reconstructed), substantially outperforming baselines; Geometry/graph tasks (Convex hull N=5/10): 96.85% / 91.88%; Shortest path: 93.43%; Minimum spanning tree: 94.77%; bAbI: mean error 0.39 ± 0.18, best 0.15 (over 10 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablations reported: small STM (d=48, w/o transfer) and STM (d=96, w/o gates) - e.g., Associative retrieval: STM (d=96) converged in 10 epochs, STM (d=96, w/o gates) converged in 100 epochs with test accuracy 24% (length 30) and 20% (length 50); relational transfer improved long-term retrieval for longer sequences (longer sequences benefited from transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%), epochs to convergence, bit-error per sequence, mean/ best error (bAbI), average reward (RL)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Higher computational complexity and memory for OPA vs dot-product attention (Table 5): OPA incurs O(n_q n_kv d_qk d_v) additions vs lower for DPA; increased storage O(n_q d_qk d_v). Training high-order models is more challenging but practical with moderate d (e.g., 96). Wall-clock time similar to RMC and faster than DNC/NTM on tested tasks. Model tends to 'overuse' relational capacity (high numerical rank of distillation matrix) and OPA increases compute and storage costs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Needs careful gating and sufficient item-memory dimension (without gates STM struggled to converge); higher-order mechanisms harder to train (TPR and other high-order models failed on scaled tasks); OPA is more computationally and memory intensive than dot-product attention; design hyperparameters (n_q, d) critical — insufficient n_q reduces relational performance (e.g., Nth-farthest accuracy rises with n_q).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Le, H., Tran, T., & Venkatesh, S. (2020). Self-Attentive Associative Memory. Proceedings of the 37th International Conference on Machine Learning (PMLR 119).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6621.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relational Memory Core (RMC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent module using dot-product self-attention across a fixed set of memory slots to enable relational interactions between stored items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Relational recurrent neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Relational Memory Core (RMC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Fixed-size slot-based memory (multiple slots) manipulated by dot-product self-attention to allow memory-memory interactions; used as a baseline recurrent memory module in multiple tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>slot-based differentiable memory with self-attention</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>A set of memory slot vectors (parallel memory slots); relationships implicitly stored via dot-product attention weights between slots.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Reads/writes via multi-head dot-product self-attention among the memory slots; controller updates slots recurrently.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Nth-farthest, algorithmic tasks (Copy, Priority sort, RAR), geometry/graph tasks, others as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>relational reasoning and memorization tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Nth-farthest (reported from Santoro et al.): 91% (reported in paper's Table 2 as baseline); on algorithmic tasks in this paper RMC demonstrated trivial performance on Copy/Priority/RAR in their experiments (learning curves show poor results compared to STM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported in this paper (no ablation disabling memory); comparisons are against other models.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%), bit-error (algorithmic tasks) where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Uses dot-product attention which yields scalar per relationship (coarse relational representation) and may be insufficient for tasks requiring high-fidelity bit-level relational storage; relatively competitive runtime but weaker memorization in high-dimensional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performed poorly on tasks stressing memorization or where relational complexity exceeds dot-product capacity (e.g., RAR and algorithmic tasks in this paper); struggled on some graph/geometry tasks (reported lower accuracies than STM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., Wierstra, D., Vinyals, O., Pascanu, R., & Lillicrap, T. (2018). Relational recurrent neural networks. In Advances in Neural Information Processing Systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6621.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture with an external differentiable addressable memory matrix and a controller that learns read/write operations using content and temporal addressing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DNC (Differentiable Neural Computer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Controller (LSTM) with an external RAM-like memory matrix; differentiable read/write heads that use content-based addressing and temporal link mechanisms to manage memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external differentiable RAM-like memory matrix with read/write heads</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Slot matrix of vectors (addressable memory slots storing vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned read/write heads with content-based addressing (similarity) and temporal/linkage-based addressing for sequential data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Nth-farthest (reported baseline), geometry/graph reasoning (Shortest path, Minimum spanning tree), algorithmic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>memorization-heavy and graph reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Geometry/graph: Shortest path 83.59% and Minimum spanning tree 82.24% (Table 3); Nth-farthest (reported in Table 2) DNC: 25% (reported from Santoro et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>DNC has relatively high complexity and slower wall-clock time compared to STM/RMC (noted as slower in Table 6); better for some graph-like problems where slot-based structured memory helps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Can be less effective on tasks requiring high-fidelity bit-level relational storage (e.g., RAR) and shows mixed performance across tasks; slower runtime in practice on some tasks compared to STM and RMC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. (2016). Hybrid computing using a neural network with dynamic external memory. Nature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6621.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Turing Machine (NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural model coupling a controller network with a differentiable external memory accessed by learned read/write heads to emulate Turing-style read/write operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural turing machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NTM (Neural Turing Machine)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Controller (often LSTM) plus an external memory matrix; uses differentiable addressing (content and/or location-based) to read/write memory slots.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external differentiable memory (matrix) with read/write heads</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Addressable memory slots (vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned soft read/write heads (content-based and shift/location addressing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Algorithmic tasks (Copy, Priority sort, RAR) as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>sequence memorization and simple relational tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>In this paper: NTM performed well on Copy and moderately on Priority sort but poorly on RAR (quantitative bit-errors shown in Fig.2; exact numeric values not tabulated in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Bit-error per sequence (algorithmic tasks), task accuracy where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>NTM tends to be biased toward item memory (good for pure memorization) but struggles on tasks requiring relational reasoning and high-dimensional outputs; wall-clock time higher than LSTM/STM in some tasks (Table 6 shows NTM time 1.8s vs STM 0.3s on Priority Sort).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor performance on RAR (task requiring both high-fidelity item memory and relational reasoning), likely due to bias towards item memory and limitations in relational extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6621.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gated recurrent neural network architecture that maintains a vector hidden state to capture temporal dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard recurrent model with gated cell state (no external memory beyond hidden/cell vectors); used as baseline for sequence tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit vector hidden state (not an explicit external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hidden and cell state vectors</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Recurrent gating updates internal state each timestep (no explicit read/write heads or external addressing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Algorithmic tasks, geometry/graph tasks, RL (Pong), bAbI QA (baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>sequential learning, reinforcement learning, QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>As baseline: often worst performer on algorithmic tasks; Geometry/graph: mixed performance (e.g., Convex hull N=5: 89.15%, N=10: 82.24%; Shortest path 73.15% for N=5). On Pong RL, LSTM agents reached perfect performance in standard 4-frame-skip but slower convergence than STM in extreme frame-skip settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable (no explicit memory component to ablate).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%), average reward (RL)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Simple and fast but lacks explicit relational extraction mechanisms; struggles on high-dimensional memorization/relational tasks relative to memory-augmented models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs poorly on tasks requiring explicit external memory or high-fidelity item storage and complex relational reasoning (e.g., RAR, Nth-farthest).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Hochreiter, S. & Schmidhuber, J. (1997). Long short-term memory. Neural Computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6621.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attentional LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM augmented with an attention mechanism over inputs (or memory) at decoding time to focus on relevant input vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation by jointly learning to align and translate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Attentional LSTM (ALSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LSTM augmented with attention over input vectors (memory-less attention during decoding) to directly access inputs when generating outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>attention over input sequence (not an explicit persistent external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Original input vectors retained for attention</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Soft-attention over stored input vectors at each decoding timestep (dot-product/softmax attention)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Algorithmic tasks (Copy, Priority sort), Convex hull (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>sequence-to-sequence and algorithmic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Good at Copy (privileged access to inputs), but does not help much on relational reasoning tasks according to paper (e.g., performs worse than STM on RAR and other relational tasks). Example Convex hull N=5: 89.92%, N=10: 85.22%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable (attention over inputs is core to model variant used).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Attention over inputs helps when outputs require direct access to inputs (Copy) but lacks stored relational memory for more complex relational tasks; described as 'memory-less attention' insufficient for relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Underperforms on relational tasks (e.g., RAR) where precomputed relational storage and higher-order representations are required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Bahdanau, D., Cho, K., & Bengio, Y. (2014/2015). Neural machine translation by jointly learning to align and translate. (ICLR/ArXiv reference cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6621.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tensor Product Representation (TPR) based model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-order fast-weight model using tensor product bindings to represent structured relational information explicitly as higher-order tensors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to reason with third order tensor products</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TPR (third-order tensor product model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses tensor-product bindings to create third-order tensors representing relations/roles; designed for structural reasoning and reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>high-order fast-weight tensor (third-order)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>3-D tensor encoding role-filler bindings (third-order tensor product representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>High-order tensor computations and associated learned transforms for read/write; no self-attention interactions among stored patterns in original TPR formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Nth-farthest (evaluated/tuned as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>In this paper: TPR performed poorly on the full Nth-farthest task with standard settings (failed to converge with original problem scaling); when input dimensionality reduced to 48 it could reach perfect performance (paper notes scaling/training difficulty). Table 2 reports TPR: 13% on Nth-farthest (as evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>High-order representation capacity but difficult to train and scale; performance sensitive to input dimensionality and tuning; lacks self-attention interactions among stored patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed to converge on larger/standard Nth-farthest settings used here; challenge of training high-order neural networks in diverse contexts highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Schlag, I. & Schmidhuber, J. (2018). Learning to reason with third order tensor products. In Advances in Neural Information Processing Systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6621.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fast weight (Ba et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast-weights memory (using Hebbian-like outer-product updates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fast-weight associative memory mechanisms that update temporary weight matrices (fast-weights) on the fly to store recent patterns for short-term recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using fast weights to attend to the recent past</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Fast-weight models (fast associative memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Fast weights are temporary associative matrices updated per input (often via outer-products or Hebbian rules) to store recent items; used in prior work as a fast-updating memory component.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>fast-weight associative memory (outer-product updates / Hebbian)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Weight matrices encoding associations between patterns (often implemented as outer-product sums)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write by outer-product updates; read via contraction/associative retrieval operations; sometimes augmented with gating or meta-learned controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative retrieval (reported baseline, from prior literature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>sequential memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported from Zhang & Zhou (2017) as baseline in Table 1: Fast weight* : converged epochs 50 (length 30), 5000 (length 50?) and accuracy 100 (for some settings) — paper reports these as prior results (asterisk indicates sourced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable in cited baseline context here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Epochs to converge, accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Fast weights provide rapid temporary storage but capacity and stability depend on gating/meta-learning; can be less structured than dual-memory approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Standard fast-weight baselines may converge slower or fail on longer/higher-dimensional sequences compared to STM in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., & Ionescu, C. (2016). Using fast weights to attend to the recent past. In Advances in Neural Information Processing Systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6621.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WeiNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WeiNet (Gated fast-weights / learned update fast-weight network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fast-weight neural network variant that learns to update associative fast-weight matrices via learned gating mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WeiNet</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Fast-weight style model with learned update rules (gated fast-weight updates) intended to improve sequence memorization capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>learned fast-weight associative memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Fast-weight matrices encoding associations between representations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned gated update rules to accumulate outer-product-like associations; retrieval via associative contraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative retrieval (reported baseline numbers from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>sequential memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in Table 1 (from Zhang & Zhou (2017)) WeiNet*: epochs 35 for length 30, accuracy 100; length 50 epochs 50 with accuracy 100 (numbers sourced from prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Epochs to converge, accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Not discussed in detail in this paper (only cited as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not analyzed in this paper; cited baseline only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6621.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Transformer (UT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the Transformer that introduces recurrence (adaptive computation across depth) to the self-attention stack, enabling iterative refinement of representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Universal transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Universal Transformer (UT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer-like model with recurrent iterative refinement; uses dot-product self-attention over representations (i.e., relational extraction via attention heads) with tied parameters across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>no explicit fixed-size external memory; uses self-attention over sequence (non-fixed-size during inference)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Sequence of contextualized token vectors held in activations across recurrent transformer steps</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Dot-product self-attention across positions iteratively (tied weights) to enable recurrent relational computation</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI question answering (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>question answering / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On bAbI (reported in paper Table 4): UT mean error 1.12 ± 1.62 (over 10 runs), best 0.21.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable (model design is fixed).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean error (%) (bAbI tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Uses dot-product attention (scalar per relationship) which may be less expressive than outer-product based relational representations; paper notes higher mean error than STM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher mean error than STM on bAbI; lacks learned fixed-size item memory that STM has.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018). Universal transformers. arXiv preprint arXiv:1807.03819.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6621.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MNM-p</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-learned Neural Memory (pointer version)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-learned fast-weight memory architecture trained with meta-level objectives to acquire rapid storage and retrieval capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metalearned neural memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MNM-p (meta-learned neural memory - pointer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Meta-trained fast-weight memory that learns to store and retrieve via meta-learning objectives (fast adaptation of memory updates); used previously for few-shot and memory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>meta-learned fast-weight memory (episodic / fast storage)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Fast-weight matrices or episodic storage learned through meta-training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Meta-learned update and retrieval rules (fast adaptation), specifics per prior work</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI (comparison in Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>question answering / memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI reported in Table 4: MNM-p mean error 0.55 ± 0.74, best 0.18 (over 10 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean error (bAbI tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Requires meta-level training; not directly comparable under same training regimes (MNM-p in prior work used meta-level loss whereas STM used supervised loss but still outperformed).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes STM outperforms MNM-p despite MNM-p using meta-level loss, suggesting MNM-p may be less stable or lower-performing on bAbI under comparable supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Munkhdalai, T., Sordoni, A., Wang, T., & Trischler, A. (2019). Metalearned neural memory. In Advances in Neural Information Processing Systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6621.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6621.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NUTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Universal Turing Machine (NUTM) / Neural Stored-Program Memory variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of memory-augmented models inspired by Turing-like memory and stored-program mechanisms for two-view or stored-program memory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NUTM (as reported in bAbI comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-augmented architecture (cited Le et al., 2020 / related works) leveraging stored-program or dual-memory ideas to improve long-term recall; used as baseline in bAbI comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory-augmented (stored-program / dual-view memory) — external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Addressable memory slots / stored program representations (per prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Controller-based read/write with learned addressing (details per cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bAbI (comparison in Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>question answering / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>bAbI reported in Table 4: NUTM mean error 5.6 ± 1.9, best 3.3 (over 10 runs) as cited from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean error (bAbI tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Not elaborated in this paper (cited baseline); generally such models may involve more complex architectures and specialized training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher mean error on bAbI compared to STM indicating less effective joint item+relational memory under evaluated regime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Attentive Associative Memory', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Relational recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>Neural turing machines <em>(Rating: 2)</em></li>
                <li>Learning to reason with third order tensor products <em>(Rating: 2)</em></li>
                <li>Using fast weights to attend to the recent past <em>(Rating: 1)</em></li>
                <li>Metalearned neural memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6621",
    "paper_id": "paper-28a8604d0ca174d885f5bffde6b6b1d3f955f5bd",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "STM",
            "name_full": "SAM-based Two-memory Model",
            "brief_description": "A sequential model that explicitly separates item memory (auto-associative matrix) and relational memory (set of hetero-associative matrices) connected via a Self-attentive Associative Memory (SAM) operator based on outer-product attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "STM (SAM-based Two-memory Model)",
            "agent_description": "Two-memory architecture: an item memory M^i implemented as a d×d auto-associative matrix (updated with gated outer-product writes) and a relational memory M^r implemented as an n_q×d×d tensor built by SAM (outer-product self-attention) from M^i; M^r is read via two-step contractions and distilled back to M^i and outputs via learned high-dimensional transforms.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "dual memory: auto-associative item memory (matrix) + high-order relational memory (3-D tensor / hetero-associative memories)",
            "memory_representation": "Item memory stores outer-product associations X_t (d×d); relational memory stores summed outer-product pairwise relationships as matrices (n_q × d × d), representing bit-level interactions between item pairs.",
            "memory_access_mechanism": "Writes to item memory: gated updates combining previous M^i and X_t=f1(x)⊗f2(x); SAM reads M^i via learned W_q/W_k/W_v to produce queries/keys/values and constructs M^r using elementwise scaling + outer products; reads from M^r via a softmax-weighted contraction (softmax(z^T) M^r f2(x)); transfer: high-dimensional transforms flatten/transform M^r back into M^i and into output.",
            "task_name": "Multiple (Associative retrieval, Nth-farthest, Relational Associative Recall (RAR), Copy, Priority Sort, Convex hull, TSP, Shortest path, Minimum spanning tree, Atari Pong RL, bAbI QA)",
            "task_category": "memorization and relational reasoning across algorithmic tasks, geometric/graph reasoning, reinforcement learning, and question answering",
            "performance_with_memory": "See per-task examples: Associative retrieval (sequence length 30/50): converged in 10 / 20 epochs with final test accuracy 100%; Nth-farthest (8 vectors): accuracy 98% (n_q=8); RAR: ~1 bit error per sequence (~87% items perfectly reconstructed), substantially outperforming baselines; Geometry/graph tasks (Convex hull N=5/10): 96.85% / 91.88%; Shortest path: 93.43%; Minimum spanning tree: 94.77%; bAbI: mean error 0.39 ± 0.18, best 0.15 (over 10 runs).",
            "performance_without_memory": "Ablations reported: small STM (d=48, w/o transfer) and STM (d=96, w/o gates) - e.g., Associative retrieval: STM (d=96) converged in 10 epochs, STM (d=96, w/o gates) converged in 100 epochs with test accuracy 24% (length 30) and 20% (length 50); relational transfer improved long-term retrieval for longer sequences (longer sequences benefited from transfer).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (%), epochs to convergence, bit-error per sequence, mean/ best error (bAbI), average reward (RL)",
            "tradeoffs_reported": "Higher computational complexity and memory for OPA vs dot-product attention (Table 5): OPA incurs O(n_q n_kv d_qk d_v) additions vs lower for DPA; increased storage O(n_q d_qk d_v). Training high-order models is more challenging but practical with moderate d (e.g., 96). Wall-clock time similar to RMC and faster than DNC/NTM on tested tasks. Model tends to 'overuse' relational capacity (high numerical rank of distillation matrix) and OPA increases compute and storage costs.",
            "limitations_or_failure_cases": "Needs careful gating and sufficient item-memory dimension (without gates STM struggled to converge); higher-order mechanisms harder to train (TPR and other high-order models failed on scaled tasks); OPA is more computationally and memory intensive than dot-product attention; design hyperparameters (n_q, d) critical — insufficient n_q reduces relational performance (e.g., Nth-farthest accuracy rises with n_q).",
            "citation": "Le, H., Tran, T., & Venkatesh, S. (2020). Self-Attentive Associative Memory. Proceedings of the 37th International Conference on Machine Learning (PMLR 119).",
            "uuid": "e6621.0",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "RMC",
            "name_full": "Relational Memory Core (RMC)",
            "brief_description": "A recurrent module using dot-product self-attention across a fixed set of memory slots to enable relational interactions between stored items.",
            "citation_title": "Relational recurrent neural networks",
            "mention_or_use": "use",
            "agent_name": "Relational Memory Core (RMC)",
            "agent_description": "Fixed-size slot-based memory (multiple slots) manipulated by dot-product self-attention to allow memory-memory interactions; used as a baseline recurrent memory module in multiple tasks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "slot-based differentiable memory with self-attention",
            "memory_representation": "A set of memory slot vectors (parallel memory slots); relationships implicitly stored via dot-product attention weights between slots.",
            "memory_access_mechanism": "Reads/writes via multi-head dot-product self-attention among the memory slots; controller updates slots recurrently.",
            "task_name": "Nth-farthest, algorithmic tasks (Copy, Priority sort, RAR), geometry/graph tasks, others as baseline",
            "task_category": "relational reasoning and memorization tasks",
            "performance_with_memory": "Nth-farthest (reported from Santoro et al.): 91% (reported in paper's Table 2 as baseline); on algorithmic tasks in this paper RMC demonstrated trivial performance on Copy/Priority/RAR in their experiments (learning curves show poor results compared to STM).",
            "performance_without_memory": "Not reported in this paper (no ablation disabling memory); comparisons are against other models.",
            "has_comparative_results": false,
            "performance_metric": "Accuracy (%), bit-error (algorithmic tasks) where applicable",
            "tradeoffs_reported": "Uses dot-product attention which yields scalar per relationship (coarse relational representation) and may be insufficient for tasks requiring high-fidelity bit-level relational storage; relatively competitive runtime but weaker memorization in high-dimensional settings.",
            "limitations_or_failure_cases": "Performed poorly on tasks stressing memorization or where relational complexity exceeds dot-product capacity (e.g., RAR and algorithmic tasks in this paper); struggled on some graph/geometry tasks (reported lower accuracies than STM).",
            "citation": "Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., Wierstra, D., Vinyals, O., Pascanu, R., & Lillicrap, T. (2018). Relational recurrent neural networks. In Advances in Neural Information Processing Systems.",
            "uuid": "e6621.1",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "DNC",
            "name_full": "Differentiable Neural Computer (DNC)",
            "brief_description": "A neural architecture with an external differentiable addressable memory matrix and a controller that learns read/write operations using content and temporal addressing.",
            "citation_title": "Hybrid computing using a neural network with dynamic external memory",
            "mention_or_use": "use",
            "agent_name": "DNC (Differentiable Neural Computer)",
            "agent_description": "Controller (LSTM) with an external RAM-like memory matrix; differentiable read/write heads that use content-based addressing and temporal link mechanisms to manage memory.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external differentiable RAM-like memory matrix with read/write heads",
            "memory_representation": "Slot matrix of vectors (addressable memory slots storing vectors)",
            "memory_access_mechanism": "Learned read/write heads with content-based addressing (similarity) and temporal/linkage-based addressing for sequential data.",
            "task_name": "Nth-farthest (reported baseline), geometry/graph reasoning (Shortest path, Minimum spanning tree), algorithmic tasks",
            "task_category": "memorization-heavy and graph reasoning tasks",
            "performance_with_memory": "Geometry/graph: Shortest path 83.59% and Minimum spanning tree 82.24% (Table 3); Nth-farthest (reported in Table 2) DNC: 25% (reported from Santoro et al.).",
            "performance_without_memory": "Not reported in this paper.",
            "has_comparative_results": false,
            "performance_metric": "Accuracy (%)",
            "tradeoffs_reported": "DNC has relatively high complexity and slower wall-clock time compared to STM/RMC (noted as slower in Table 6); better for some graph-like problems where slot-based structured memory helps.",
            "limitations_or_failure_cases": "Can be less effective on tasks requiring high-fidelity bit-level relational storage (e.g., RAR) and shows mixed performance across tasks; slower runtime in practice on some tasks compared to STM and RMC.",
            "citation": "Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. (2016). Hybrid computing using a neural network with dynamic external memory. Nature.",
            "uuid": "e6621.2",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "NTM",
            "name_full": "Neural Turing Machine (NTM)",
            "brief_description": "A neural model coupling a controller network with a differentiable external memory accessed by learned read/write heads to emulate Turing-style read/write operations.",
            "citation_title": "Neural turing machines",
            "mention_or_use": "use",
            "agent_name": "NTM (Neural Turing Machine)",
            "agent_description": "Controller (often LSTM) plus an external memory matrix; uses differentiable addressing (content and/or location-based) to read/write memory slots.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external differentiable memory (matrix) with read/write heads",
            "memory_representation": "Addressable memory slots (vectors)",
            "memory_access_mechanism": "Learned soft read/write heads (content-based and shift/location addressing)",
            "task_name": "Algorithmic tasks (Copy, Priority sort, RAR) as baseline",
            "task_category": "sequence memorization and simple relational tasks",
            "performance_with_memory": "In this paper: NTM performed well on Copy and moderately on Priority sort but poorly on RAR (quantitative bit-errors shown in Fig.2; exact numeric values not tabulated in-text).",
            "performance_without_memory": "Not reported.",
            "has_comparative_results": false,
            "performance_metric": "Bit-error per sequence (algorithmic tasks), task accuracy where applicable",
            "tradeoffs_reported": "NTM tends to be biased toward item memory (good for pure memorization) but struggles on tasks requiring relational reasoning and high-dimensional outputs; wall-clock time higher than LSTM/STM in some tasks (Table 6 shows NTM time 1.8s vs STM 0.3s on Priority Sort).",
            "limitations_or_failure_cases": "Poor performance on RAR (task requiring both high-fidelity item memory and relational reasoning), likely due to bias towards item memory and limitations in relational extraction.",
            "citation": "Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.",
            "uuid": "e6621.3",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "LSTM",
            "name_full": "Long Short-Term Memory",
            "brief_description": "A gated recurrent neural network architecture that maintains a vector hidden state to capture temporal dependencies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LSTM",
            "agent_description": "Standard recurrent model with gated cell state (no external memory beyond hidden/cell vectors); used as baseline for sequence tasks.",
            "model_size": null,
            "memory_used": false,
            "memory_type": "implicit vector hidden state (not an explicit external memory)",
            "memory_representation": "Hidden and cell state vectors",
            "memory_access_mechanism": "Recurrent gating updates internal state each timestep (no explicit read/write heads or external addressing)",
            "task_name": "Algorithmic tasks, geometry/graph tasks, RL (Pong), bAbI QA (baselines)",
            "task_category": "sequential learning, reinforcement learning, QA",
            "performance_with_memory": "As baseline: often worst performer on algorithmic tasks; Geometry/graph: mixed performance (e.g., Convex hull N=5: 89.15%, N=10: 82.24%; Shortest path 73.15% for N=5). On Pong RL, LSTM agents reached perfect performance in standard 4-frame-skip but slower convergence than STM in extreme frame-skip settings.",
            "performance_without_memory": "Not applicable (no explicit memory component to ablate).",
            "has_comparative_results": false,
            "performance_metric": "Accuracy (%), average reward (RL)",
            "tradeoffs_reported": "Simple and fast but lacks explicit relational extraction mechanisms; struggles on high-dimensional memorization/relational tasks relative to memory-augmented models.",
            "limitations_or_failure_cases": "Performs poorly on tasks requiring explicit external memory or high-fidelity item storage and complex relational reasoning (e.g., RAR, Nth-farthest).",
            "citation": "Hochreiter, S. & Schmidhuber, J. (1997). Long short-term memory. Neural Computation.",
            "uuid": "e6621.4",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "ALSTM",
            "name_full": "Attentional LSTM",
            "brief_description": "An LSTM augmented with an attention mechanism over inputs (or memory) at decoding time to focus on relevant input vectors.",
            "citation_title": "Neural machine translation by jointly learning to align and translate",
            "mention_or_use": "use",
            "agent_name": "Attentional LSTM (ALSTM)",
            "agent_description": "LSTM augmented with attention over input vectors (memory-less attention during decoding) to directly access inputs when generating outputs.",
            "model_size": null,
            "memory_used": false,
            "memory_type": "attention over input sequence (not an explicit persistent external memory)",
            "memory_representation": "Original input vectors retained for attention",
            "memory_access_mechanism": "Soft-attention over stored input vectors at each decoding timestep (dot-product/softmax attention)",
            "task_name": "Algorithmic tasks (Copy, Priority sort), Convex hull (baseline comparisons)",
            "task_category": "sequence-to-sequence and algorithmic tasks",
            "performance_with_memory": "Good at Copy (privileged access to inputs), but does not help much on relational reasoning tasks according to paper (e.g., performs worse than STM on RAR and other relational tasks). Example Convex hull N=5: 89.92%, N=10: 85.22%.",
            "performance_without_memory": "Not applicable (attention over inputs is core to model variant used).",
            "has_comparative_results": false,
            "performance_metric": "Accuracy (%)",
            "tradeoffs_reported": "Attention over inputs helps when outputs require direct access to inputs (Copy) but lacks stored relational memory for more complex relational tasks; described as 'memory-less attention' insufficient for relational reasoning.",
            "limitations_or_failure_cases": "Underperforms on relational tasks (e.g., RAR) where precomputed relational storage and higher-order representations are required.",
            "citation": "Bahdanau, D., Cho, K., & Bengio, Y. (2014/2015). Neural machine translation by jointly learning to align and translate. (ICLR/ArXiv reference cited in paper).",
            "uuid": "e6621.5",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "TPR",
            "name_full": "Tensor Product Representation (TPR) based model",
            "brief_description": "A high-order fast-weight model using tensor product bindings to represent structured relational information explicitly as higher-order tensors.",
            "citation_title": "Learning to reason with third order tensor products",
            "mention_or_use": "use",
            "agent_name": "TPR (third-order tensor product model)",
            "agent_description": "Uses tensor-product bindings to create third-order tensors representing relations/roles; designed for structural reasoning and reasoning tasks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "high-order fast-weight tensor (third-order)",
            "memory_representation": "3-D tensor encoding role-filler bindings (third-order tensor product representations)",
            "memory_access_mechanism": "High-order tensor computations and associated learned transforms for read/write; no self-attention interactions among stored patterns in original TPR formulation.",
            "task_name": "Nth-farthest (evaluated/tuned as baseline)",
            "task_category": "relational reasoning",
            "performance_with_memory": "In this paper: TPR performed poorly on the full Nth-farthest task with standard settings (failed to converge with original problem scaling); when input dimensionality reduced to 48 it could reach perfect performance (paper notes scaling/training difficulty). Table 2 reports TPR: 13% on Nth-farthest (as evaluated here).",
            "performance_without_memory": "Not reported.",
            "has_comparative_results": false,
            "performance_metric": "Accuracy (%)",
            "tradeoffs_reported": "High-order representation capacity but difficult to train and scale; performance sensitive to input dimensionality and tuning; lacks self-attention interactions among stored patterns.",
            "limitations_or_failure_cases": "Failed to converge on larger/standard Nth-farthest settings used here; challenge of training high-order neural networks in diverse contexts highlighted.",
            "citation": "Schlag, I. & Schmidhuber, J. (2018). Learning to reason with third order tensor products. In Advances in Neural Information Processing Systems.",
            "uuid": "e6621.6",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Fast weight (Ba et al.)",
            "name_full": "Fast-weights memory (using Hebbian-like outer-product updates)",
            "brief_description": "Fast-weight associative memory mechanisms that update temporary weight matrices (fast-weights) on the fly to store recent patterns for short-term recall.",
            "citation_title": "Using fast weights to attend to the recent past",
            "mention_or_use": "mention",
            "agent_name": "Fast-weight models (fast associative memory)",
            "agent_description": "Fast weights are temporary associative matrices updated per input (often via outer-products or Hebbian rules) to store recent items; used in prior work as a fast-updating memory component.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "fast-weight associative memory (outer-product updates / Hebbian)",
            "memory_representation": "Weight matrices encoding associations between patterns (often implemented as outer-product sums)",
            "memory_access_mechanism": "Write by outer-product updates; read via contraction/associative retrieval operations; sometimes augmented with gating or meta-learned controllers.",
            "task_name": "Associative retrieval (reported baseline, from prior literature)",
            "task_category": "sequential memorization",
            "performance_with_memory": "Reported from Zhang & Zhou (2017) as baseline in Table 1: Fast weight* : converged epochs 50 (length 30), 5000 (length 50?) and accuracy 100 (for some settings) — paper reports these as prior results (asterisk indicates sourced).",
            "performance_without_memory": "Not applicable in cited baseline context here.",
            "has_comparative_results": false,
            "performance_metric": "Epochs to converge, accuracy (%)",
            "tradeoffs_reported": "Fast weights provide rapid temporary storage but capacity and stability depend on gating/meta-learning; can be less structured than dual-memory approaches.",
            "limitations_or_failure_cases": "Standard fast-weight baselines may converge slower or fail on longer/higher-dimensional sequences compared to STM in this paper's experiments.",
            "citation": "Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., & Ionescu, C. (2016). Using fast weights to attend to the recent past. In Advances in Neural Information Processing Systems.",
            "uuid": "e6621.7",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "WeiNet",
            "name_full": "WeiNet (Gated fast-weights / learned update fast-weight network)",
            "brief_description": "A fast-weight neural network variant that learns to update associative fast-weight matrices via learned gating mechanisms.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "WeiNet",
            "agent_description": "Fast-weight style model with learned update rules (gated fast-weight updates) intended to improve sequence memorization capacity.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "learned fast-weight associative memory",
            "memory_representation": "Fast-weight matrices encoding associations between representations",
            "memory_access_mechanism": "Learned gated update rules to accumulate outer-product-like associations; retrieval via associative contraction",
            "task_name": "Associative retrieval (reported baseline numbers from prior work)",
            "task_category": "sequential memorization",
            "performance_with_memory": "Reported in Table 1 (from Zhang & Zhou (2017)) WeiNet*: epochs 35 for length 30, accuracy 100; length 50 epochs 50 with accuracy 100 (numbers sourced from prior work).",
            "performance_without_memory": "Not reported here.",
            "has_comparative_results": false,
            "performance_metric": "Epochs to converge, accuracy (%)",
            "tradeoffs_reported": "Not discussed in detail in this paper (only cited as baseline).",
            "limitations_or_failure_cases": "Not analyzed in this paper; cited baseline only.",
            "citation": "",
            "uuid": "e6621.8",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "UT",
            "name_full": "Universal Transformer (UT)",
            "brief_description": "An extension of the Transformer that introduces recurrence (adaptive computation across depth) to the self-attention stack, enabling iterative refinement of representations.",
            "citation_title": "Universal transformers",
            "mention_or_use": "mention",
            "agent_name": "Universal Transformer (UT)",
            "agent_description": "Transformer-like model with recurrent iterative refinement; uses dot-product self-attention over representations (i.e., relational extraction via attention heads) with tied parameters across layers.",
            "model_size": null,
            "memory_used": false,
            "memory_type": "no explicit fixed-size external memory; uses self-attention over sequence (non-fixed-size during inference)",
            "memory_representation": "Sequence of contextualized token vectors held in activations across recurrent transformer steps",
            "memory_access_mechanism": "Dot-product self-attention across positions iteratively (tied weights) to enable recurrent relational computation",
            "task_name": "bAbI question answering (comparison)",
            "task_category": "question answering / reasoning",
            "performance_with_memory": "On bAbI (reported in paper Table 4): UT mean error 1.12 ± 1.62 (over 10 runs), best 0.21.",
            "performance_without_memory": "Not applicable (model design is fixed).",
            "has_comparative_results": false,
            "performance_metric": "Mean error (%) (bAbI tasks)",
            "tradeoffs_reported": "Uses dot-product attention (scalar per relationship) which may be less expressive than outer-product based relational representations; paper notes higher mean error than STM.",
            "limitations_or_failure_cases": "Higher mean error than STM on bAbI; lacks learned fixed-size item memory that STM has.",
            "citation": "Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018). Universal transformers. arXiv preprint arXiv:1807.03819.",
            "uuid": "e6621.9",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "MNM-p",
            "name_full": "Meta-learned Neural Memory (pointer version)",
            "brief_description": "A meta-learned fast-weight memory architecture trained with meta-level objectives to acquire rapid storage and retrieval capabilities.",
            "citation_title": "Metalearned neural memory",
            "mention_or_use": "mention",
            "agent_name": "MNM-p (meta-learned neural memory - pointer)",
            "agent_description": "Meta-trained fast-weight memory that learns to store and retrieve via meta-learning objectives (fast adaptation of memory updates); used previously for few-shot and memory tasks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "meta-learned fast-weight memory (episodic / fast storage)",
            "memory_representation": "Fast-weight matrices or episodic storage learned through meta-training",
            "memory_access_mechanism": "Meta-learned update and retrieval rules (fast adaptation), specifics per prior work",
            "task_name": "bAbI (comparison in Table 4)",
            "task_category": "question answering / memorization",
            "performance_with_memory": "bAbI reported in Table 4: MNM-p mean error 0.55 ± 0.74, best 0.18 (over 10 runs).",
            "performance_without_memory": "Not reported here.",
            "has_comparative_results": false,
            "performance_metric": "Mean error (bAbI tasks)",
            "tradeoffs_reported": "Requires meta-level training; not directly comparable under same training regimes (MNM-p in prior work used meta-level loss whereas STM used supervised loss but still outperformed).",
            "limitations_or_failure_cases": "Paper notes STM outperforms MNM-p despite MNM-p using meta-level loss, suggesting MNM-p may be less stable or lower-performing on bAbI under comparable supervised training.",
            "citation": "Munkhdalai, T., Sordoni, A., Wang, T., & Trischler, A. (2019). Metalearned neural memory. In Advances in Neural Information Processing Systems.",
            "uuid": "e6621.10",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "NUTM",
            "name_full": "Neural Universal Turing Machine (NUTM) / Neural Stored-Program Memory variants",
            "brief_description": "A family of memory-augmented models inspired by Turing-like memory and stored-program mechanisms for two-view or stored-program memory tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "NUTM (as reported in bAbI comparisons)",
            "agent_description": "A memory-augmented architecture (cited Le et al., 2020 / related works) leveraging stored-program or dual-memory ideas to improve long-term recall; used as baseline in bAbI comparisons.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "memory-augmented (stored-program / dual-view memory) — external memory",
            "memory_representation": "Addressable memory slots / stored program representations (per prior work)",
            "memory_access_mechanism": "Controller-based read/write with learned addressing (details per cited works)",
            "task_name": "bAbI (comparison in Table 4)",
            "task_category": "question answering / reasoning",
            "performance_with_memory": "bAbI reported in Table 4: NUTM mean error 5.6 ± 1.9, best 3.3 (over 10 runs) as cited from prior work.",
            "performance_without_memory": "Not reported.",
            "has_comparative_results": false,
            "performance_metric": "Mean error (bAbI tasks)",
            "tradeoffs_reported": "Not elaborated in this paper (cited baseline); generally such models may involve more complex architectures and specialized training.",
            "limitations_or_failure_cases": "Higher mean error on bAbI compared to STM indicating less effective joint item+relational memory under evaluated regime.",
            "citation": "",
            "uuid": "e6621.11",
            "source_info": {
                "paper_title": "Self-Attentive Associative Memory",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Relational recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 2
        },
        {
            "paper_title": "Learning to reason with third order tensor products",
            "rating": 2
        },
        {
            "paper_title": "Using fast weights to attend to the recent past",
            "rating": 1
        },
        {
            "paper_title": "Metalearned neural memory",
            "rating": 1
        }
    ],
    "cost": 0.02353925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Attentive Associative Memory</h1>
<p>Hung Le ${ }^{1}$ Truyen Tran ${ }^{1}$ Svetha Venkatesh ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering.</p>
<h2>1. Introduction</h2>
<p>Humans excel in remembering items and the relationship between them over time (Olson et al., 2006; Konkel \&amp; Cohen, 2009). Numerous neurocognitive studies have revealed this striking ability is largely attributed to the perirhinal cortex and hippocampus, two brain regions that support item memory (e.g., objects, events) and relational memory (e.g., locations of objects, orders of events), respectively (Cohen et al., 1997; Buckley, 2005). Relational memory theory posits that there exists a representation of critical relationships amongst arbitrary items, which</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>allows inferential reasoning capacity (Eichenbaum, 1993; Zeithamova et al., 2012). It remains unclear how the hippocampus can select the stored items in clever ways to unearth their hidden relationships and form the relational representation.</p>
<p>Research on artificial intelligence has focused on designing item-based memory models with recurrent neural networks (RNNs) (Hopfield, 1982; Elman, 1990; Hochreiter \&amp; Schmidhuber, 1997) and memoryaugmented neural networks (MANNs) (Graves et al., 2014; 2016; Le et al., 2018a; 2019). These memories support long-term retrieval of previously seen items yet lack explicit mechanisms to represent arbitrary relationships amongst the constituent pieces of the memories. Recently, further attempts have been made to foster relational modeling by enabling memory-memory interactions, which is essential for relational reasoning tasks (Santoro et al., 2017; 2018; Vaswani et al., 2017). However, no effort has been made to model jointly item memory and relational memory explicitly.</p>
<p>We argue that dual memories in a single system are crucial for solving problems that require both memorization and relational reasoning. Consider graphs wherein each node is associated with versatile features-as example a road network structure where each node is associated with diverse features: graph 1 where the nodes are building landmarks and graph 2 where the nodes are flora details. The goal here is to reason over the structure and output the associated features of the nodes instead of the pointer or index to the nodes. Learning to output associated node features enables generalization to entirely novel features, i.e., a model can be trained to generate a navigation path with building landmarks (graph 1) and tested in the novel context of generating a navigation path with flora landmarks (graph 2). This may be achieved if the model stores the features and structures into its item and relational memory, separately, and reason over the two memories using rules acquired during training.</p>
<p>Another example requiring both item and relational memory can be understood by amalgamating the $N^{t h}$-farthest (Santoro et al., 2018) and associative recall (Graves et al., 2014) tasks. $N^{t h}$-farthest requires relational memory to return a fixed one-hot encoding representing the index to the</p>
<p>$N^{t h}$-farthest item, while associative recall returns the item itself, requiring item memory. If these tasks are amalgamated to compose Relational Associative Recall (RAR) return the $N^{t h}$-farthest item from a query (see $\S 3.2$ ), it is clear that both item and relational memories are required.</p>
<p>Three limitations of the current approaches are: ( $i$ ) the relational representation is often computed without storing, which prevents reusing the precomputed relationships in sequential tasks (Vaswani et al., 2017; Santoro et al., 2017), (ii) few works that manage both items and the relationships in a single memory, make it hard to understand how relational reasoning occurs (Santoro et al., 2018; Schlag \&amp; Schmidhuber, 2018), (iii) the memory-memory relationship is coarse since it is represented as either dot product attention (Vaswani et al., 2017) or weighted summation via neural networks (Santoro et al., 2017). Concretely, the former uses a scalar to measure cosine distance between two vectors and the later packs all information into one vector via only additive interactions.</p>
<p>To overcome the current limitations, we hypothesize a twomemory model, in which the relational memory exists separately from the item memory. To maintain a rich representation of the relationship between items, the relational memory should be higher-order than the item memory. That is, the relational memory stores multiple relationships, each of which should be represented by a matrix rather than a scalar or vector. Otherwise, the capacity of the relational memory is downgraded to that of the item memory. Finally, as there are two separate memories, they must communicate to enrich the representation of one another.</p>
<p>To implement our hypotheses, we introduce a novel operator that facilitates the communication from the item memory to the relational memory. The operator, named Selfattentive Associative Memory (SAM) leverages the dot product attention with our outer product attention. Outer product is critical for constructing higher-order relational representations since it retains bit-level interactions between two input vectors, thus has potential for rich representational learning (Smolensky, 1990). SAM transforms a second-order (matrix) item memory into a third-order relational representation through two steps. First, SAM decodes a set of patterns from the item memory. Second, SAM associates each pair of patterns using outer product and sums them up to form a hetero-associative memory. The memory thus stores relationships between stored items accumulated across timesteps to form a relational memory.</p>
<p>The role of item memory is to memorize the input data over time. To selectively encode the input data, the item memory is implemented as a gated auto-associative memory. Together with previous read-out values from the relational memory, the item memory is used as the input for SAM to construct the relational memory. In return, the re-
lational memory transfers its knowledge to the item memory through a distillation process. The backward transfer triggers recurrent dynamics between the two memories, which may be essential for simulating hippocampal processes (Kumaran \&amp; McClelland, 2012). Another distillation process is used to transform the relational memory to the output value.</p>
<p>Taken together, we contribute a new neural memory model dubbed SAM-based Two-memory Model (STM) that takes inspiration from the existence of both item and relational memory in human brain (Konkel \&amp; Cohen, 2009). In this design, the relational memory is higher-order than the item memory and thus necessitates a core operator that manages the information exchange from the item memory to the relational memory. The operator, namely Self-attentive Associative Memory (SAM), utilizes outer product to construct a set of hetero-associative memories representing relationships between arbitrary stored items. We apply our model to a wide range of tasks that may require both item and relational memory: various algorithmic learning, geometric and graph reasoning, reinforcement learning and questionanswering tasks. Several analytical studies on the characteristics of our proposed model are also given in the Appendix.</p>
<h2>2. Methods</h2>
<h3>2.1. Outer product attention (OPA)</h3>
<p>Outer product attention (OPA) is a natural extension of the query-key-value dot product attention (Vaswani et al., 2017). Dot product attention (DPA) for single query $q$ and $n_{k v}$ pairs of key-value can be formulated as follows,</p>
<p>$$
A^{*}(q, K, V)=\sum_{i=1}^{n_{k v}} \mathcal{S}\left(q \cdot k_{i}\right) v_{i}
$$</p>
<p>where $A^{*} \in \mathbb{R}^{d_{v}}, q, k_{i} \in \mathbb{R}^{d_{q k}}, v_{i} \in \mathbb{R}^{d_{v}}, \cdot$ is dot product, and $\mathcal{S}$ forms softmax function. We propose a new outer product attention with similar formulation yet different meaning,</p>
<p>$$
A^{\otimes}(q, K, V)=\sum_{i=1}^{n_{k v}} \mathcal{F}\left(q \odot k_{i}\right) \otimes v_{i}
$$</p>
<p>where $A^{\otimes} \in \mathbb{R}^{d_{q k} \times d_{v}}, q, k_{i} \in \mathbb{R}^{d_{q k}}, v \in \mathbb{R}^{d_{v}}, \odot$ is element-wise multiplication, $\otimes$ is outer product and $\mathcal{F}$ is chosen as element-wise tanh function.</p>
<p>A crucial difference between DPA and OPA is that while the former retrieves an attended item $A^{*}$, the latter forms a relational representation $A^{\otimes}$. As a relational representation, $A^{\otimes}$ captures all bit-level associations between the key-scaled query and the value. This offers two benefits:</p>
<p>$(i)$ a higher-order representational capacity that DPA cannot provide and $(i i)$ a form of associative memory that can be later used to retrieve stored item by using a contraction operation $\mathcal{P}\left(A^{\otimes}\right)$ (see Appendix $\S$ C-Prop. 6).</p>
<p>OPA is closely related to DPA. The relationship between the two for simple $\mathcal{S}$ and $\mathcal{F}$ is presented as follows,
Proposition 1. Assume that $\mathcal{S}$ is a linear transformation: $\mathcal{S}(x)=a x+b(a, b, x \in \mathbb{R})$, we can extract $A^{*}$ from $A^{\otimes}$ by using an element-wise linear transformation $\mathcal{F}(x)=a^{f} \odot$ $x+b^{f}\left(a^{f}, b^{f}, x \in \mathbb{R}^{d_{q k}}\right)$ and a contraction $\mathcal{P}: \mathbb{R}^{d_{q k} \times d_{v}} \rightarrow$ $\mathbb{R}^{d_{v}}$ such that</p>
<p>$$
A^{*}(q, K, V)=\mathcal{P}\left(A^{\otimes}(q, K, V)\right)
$$</p>
<p>Proof. see Appendix $\S$ A.</p>
<p>Moreover, when $n_{k v}=1$, applying a high dimensional transformation $\mathcal{G}\left(A^{\otimes}\right)$ is equivalent to the well-known bilinear model (see Appendix $\S$ B-Prop. 4). By introducing OPA, we obtain a new building block that naturally supports both powerful relational bindings and item memorization.</p>
<h3>2.2. Self-attentive Associative Memory (SAM)</h3>
<p>We introduce a novel and generic operator based upon OPA that constructs relational representations from an item memory. The relational information is extracted via preserving the outer products between any pairs of items from the item memory. Hence, we name this operator Selfattentive Associative Memory (SAM). Given an item memory $M \in \mathbb{R}^{n \times d}$ and parametric weights $\theta \equiv\left{W_{q} \in \mathbb{R}^{n_{q} \times n}\right.$, $\left.W_{k} \in \mathbb{R}^{n_{k v} \times n}, W_{v} \in \mathbb{R}^{n_{k v} \times n}\right}$, SAM retrieves $n_{q}$ queries, $n_{k v}$ keys and values from $M$ as $M_{q}, M_{k}$ and $M_{v}$, respectively,</p>
<p>$$
\begin{aligned}
M_{q} &amp; =\mathcal{L N}\left(W_{q} M\right) \
M_{k} &amp; =\mathcal{L N}\left(W_{k} M\right) \
M_{v} &amp; =\mathcal{L N}\left(W_{v} M\right)
\end{aligned}
$$</p>
<p>where $\mathcal{L N}$ is layer normalization operation (Ba et al., 2016b). Then SAM returns a relational representation $\operatorname{SAM}<em q="q">{\theta}(M) \in \mathbb{R}^{n</em>$, in which the $s$-th element of the first dimension is defined as} \times d \times d</p>
<p>$$
\begin{aligned}
\operatorname{SAM}<em q="q">{\theta}(M)[s] &amp; =A^{\otimes}\left(M</em>\right) \
&amp; =\sum_{j=1}^{n_{k v}} \mathcal{F}\left(M_{q}[s] \odot M_{k}[j]\right) \otimes M_{v}[j]
\end{aligned}
$$}[s], M_{k}, M_{v</p>
<p>where $s=1, \ldots, n_{q} . M_{q}[s], M_{k}[j]$ and $M_{v}[j]$ denote the $s$-th row vector of matrix $M_{q}$, the $j$-th row vector of matrix $M_{k}$ and $M_{v}$, respectively. A diagram illustrating SAM operations is given in Fig. 1 (right).</p>
<p>It should be noted that $M$ can be any item memory including the slot-based memories (Le et al., 2019), direct inputs (Vaswani et al., 2017) or associative memories (Kohonen, 1972; Hopfield, 1982). We choose $M \in \mathbb{R}^{d \times d}$ as a form of classical associative memory, which is biologically plausible (Marr \&amp; Thach, 1991). Here, we follow the traditional practice that sets $n=d$ for the associative item memory. From $M$ we read query, key and value items to form $\mathrm{SAM}_{\theta}(M)$-a new set of hetero-associative memories using Eq. 8. Each hetero-associative memory represents the relationship between a query and all values. The role of the keys is to maintain possible perfect retrieval for the item memory (Appendix $\S$ C-Prop. 6).</p>
<p>The high-order structure of SAM allows it to preserve bitlevel relationships between a query and a value in a matrix. SAM compresses several relationships with regard to a query by summing all the matrices to form a heteroassociative memory containing $d^{2}$ scalars, where $d$ is the dimension of $M$. As there are $n_{k v}$ relationships given 1 query, the summation results in on average $d^{2} / n_{k v}$ scalars of representation per relationship, which is greater than 1 if $d&gt;\sqrt{n_{k v}}$. By contrast, current self-attention mechanisms use dot product to measure the relationship between any pair of memory slots, which means 1 scalar per relationship.</p>
<h3>2.3. SAM-based Two-Memory Model (STM)</h3>
<p>To effectively utilize the SAM operator, we design a system which consists of two memory units $\mathcal{M}<em t="t">{t}^{i} \in \mathbb{R}^{d \times d}$ and $\mathcal{M}</em>}^{r} \in \mathbb{R}^{n_{q} \times d \times d}$ : one for items and the other for relationships, respectively. From a high-level view, at each timestep, we use the current input data $x_{t}$ and the previous state of memories $\left{\mathcal{M<em t-1="t-1">{t-1}^{i}, \mathcal{M}</em>}^{r}\right}$ to produce output $o_{t}$ and new state of memories $\left{\mathcal{M<em t="t">{t}^{i}, \mathcal{M}</em>\right}$. The memory executions are described as follows.
$\mathcal{M}^{i}$-Write The item memory distributes the data from the input across its rows in the form of associative memory. For an input $x_{t}$, we update the item memory as}^{r</p>
<p>$$
\begin{aligned}
X_{t} &amp; =f_{1}\left(x_{t}\right) \otimes f_{2}\left(x_{t}\right) \
\mathcal{M}<em t-1="t-1">{t}^{i} &amp; =\mathcal{M}</em>
\end{aligned}
$$}^{i}+X_{t</p>
<p>where $f_{1}$ and $f_{2}$ are feed-forward neural networks that output $d$-dimensional vectors. This update does not discriminate the input data and inherits the low-capacity of classical associative memory (Rojas, 2013). We leverage the gating mechanisms of LSTM (Hochreiter \&amp; Schmidhuber, 1997) to improve Eq. 9 as</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. STM (left) and SAM (right). SAM uses neural networks $\theta$ to extract query, key and value elements from a matrix memory $M$. In this illustration, $n_{q}=3$ and $n_{k v}=4$. Then, it applies outer product attention to output a $3 D$ tensor relational representation. In STM, at every timestep, the item memory $\mathcal{M}<em t="t">{t}^{i}$ is updated with new input $x</em>$ (Eq. 11-12). The relational memory transfers its knowledge to the item memory (Eq. 13) and output value (Eq. 14).}$ using gating mechanisms (Eq. 10). The item memory plus the read-out from the relational memory is forwarded to SAM, resulting in a new relational representation to update the relational memory $\mathcal{M}_{t}^{r</p>
<p>$$
\mathcal{M}<em t="t">{t}^{i}=F</em>}\left(\mathcal{M<em t="t">{t-1}^{i}, x</em>}\right) \odot \mathcal{M<em t="t">{t-1}^{i}+I</em>}\left(\mathcal{M<em t="t">{t-1}^{i}, x</em>
$$}\right) \odot X_{t</p>
<p>where $F_{t}$ and $I_{t}$ are forget and input gates, respectively. Detailed implementation of these gates is in Appendix $\S$ D.
$\mathcal{M}^{r}$-Read As relationships stored in $\mathcal{M}^{r}$ are represented as associative memories, the relational memory can be read to reconstruct previously seen items. As shown in Appendix $\S$ C-Prop. 7, the read is basically a two-step contraction,</p>
<p>$$
v_{t}^{r}=\operatorname{softmax}\left(f_{3}\left(x_{t}\right)^{\top}\right) \mathcal{M}<em 2="2">{t-1}^{r} f</em>\right)
$$}\left(x_{t</p>
<p>where $f_{3}$ is a feed-forward neural network that outputs a $n_{q}$ dimensional vector. The read value provides an additional input coming from the previous state of $\mathcal{M}^{r}$ to relational construction process, as shown later in Eq. 12.
$\mathcal{M}^{i}$-Read $\mathcal{M}^{r}$-Write We use SAM to read from $\mathcal{M}^{i}$ and construct a candidate relational memory, which is simply added to the previous relational memory to perform the relational update,</p>
<p>$$
\mathcal{M}<em t-1="t-1">{t}^{r}=\mathcal{M}</em>}^{r}+\alpha_{1} \operatorname{SAM<em t="t">{\theta}\left(\mathcal{M}</em>\right)\right)
$$}^{i}+\alpha_{2} v_{t}^{r} \otimes f_{2}\left(x_{t</p>
<p>where $\alpha_{1}$ and $\alpha_{2}$ are blending hyper-parameters. The input for SAM is a combination of the current item memory $\mathcal{M}<em t="t">{t}^{i}$ and the association between the extracted item from the previous relational memory $v</em>$ enhances the relational memory with information from the distant past. The resulting relational memory
stores associations between several pairs of items in a $3 D$ tensors of size $n_{q} \times d \times d$. In our SAM implementation, $n_{k v}=n_{q}$.
$\mathcal{M}^{r}$-Transer In this phase, the relational knowledge from $\mathcal{M}_{t}^{r}$ is transferred to the item memory by using high dimensional transformation,}^{r}$ and the current input data $x_{t}$. Here, $v_{t}^{r</p>
<p>$$
\mathcal{M}<em t="t">{t}^{i}=\mathcal{M}</em>}^{i}+\alpha_{3} \mathcal{G<em f="f">{1} \circ \mathcal{V}</em>
$$} \circ \mathcal{M}_{t}^{r</p>
<p>where $\mathcal{V}<em 1="1">{f}$ is a function that flattens the first two dimensions of its input tensor, $\mathcal{G}</em>}$ is a feed-forward neural network that maps $\mathbb{R}^{\left(n_{q} d\right) \times d} \rightarrow \mathbb{R}^{d \times d}$ and $\alpha_{3}$ is a blending hyper-parameter. As shown in Appendix $\S$ B-Prop. 5, with trivial $\mathcal{G<em t="t">{1}$, the transfer behaves as if the item memory is enhanced with long-term stored values from the relational memory. Hence, $\mathcal{M}^{r}$-Transfer is also helpful in supporting long-term recall (empirical evidences in $\S 3.1$ ). In addition, at each timestep, we distill the relational memory into an output vector $o</em>$. We alternatively flatten and apply high-dimensional transformations as follow,} \in \mathbb{R}^{n_{o}</p>
<p>$$
o_{t}=\mathcal{G}<em l="l">{3} \circ \mathcal{V}</em>} \circ \mathcal{G<em l="l">{2} \circ \mathcal{V}</em>
$$} \circ \mathcal{M}_{t}^{r</p>
<p>where $\mathcal{V}<em 2="2">{l}$ is a function that flattens the last two dimensions of its input tensor. $\mathcal{G}</em>}$ and $\mathcal{G<em q="q">{3}$ are two feed-forward neural networks that map $\mathbb{R}^{n</em>$ is a hyper-parameter.
Unlike the contraction (Eq. 11), the distillation process does not simply reconstruct the stored items. Rather, thanks to high-dimensional transformations, it captures bi-linear representations stored in the relational memory (proof in Appendix $\S$ B). Hence, despite its vector form, the output} \times(d d)} \rightarrow \mathbb{R}^{n_{q} \times n_{r}}$ and $\mathbb{R}^{n_{q} n_{r}} \rightarrow$ $\mathbb{R}^{n_{o}}$, respectively. $n_{r</p>
<p>of our model holds a rich representation that is useful for both sequential and relational learning. We discuss further on how to quantify the degree of relational distillation in Appendix $\S$ G. The summary of components of STM is presented in Fig. 1 (left).</p>
<h2>3. Results</h2>
<h3>3.1. Ablation study</h3>
<p>We test different model configurations on two classical tasks for sequential and relational learning: associative retrieval (Ba et al., 2016a) and $N^{t h}$-farthest (Santoro et al., 2018) (see Appendix $\S$ E for task details and learning curves). Our source code is available at https://github.com/thaihungle/SAM.</p>
<p>Associative retrieval This task measures the ability to recall a seen item given its associated key and thus involves item memory. We use the setting with input sequence length 30 and 50 (Zhang \&amp; Zhou, 2017). Three main factors affecting the item memory of STM are the dimension $d$ of the auto-associative item memory, the gating mechanisms (Eq. 10) and the relational transfer (Eq. 13). Hence, we ablate our STM ( $d=96$, full features) by creating three other versions: small STM with transfer $(d=48)$, small STM without transfer ( $d=48$, w/o transfer) and STM without gates ( $d=96$, w/o gates). $n_{q}$ is fixed to 1 as the task does not require much relational learning.</p>
<p>Table 1 reports the number of epochs required to converge and the final testing accuracy. Without the proposed gating mechanism, STM struggles to converge, which highlights the importance of extending the capacity of the autoassociative item memory. The convergence speed of STM is significantly improved with a bigger item memory size. Relational transfer seems more useful for longer input sequences since if requested, it can support long-term retrieval. Compared to other fast-weight baselines, the fullfeature STM performs far better as it needs only 10 and 20 epochs to solve the tasks of length 30 and 50, respectively.
$N^{\text {th }}$-farthest This task evaluates the ability to learn the relationship between stored vectors. The goal is to find the $N^{t h}$-farthest vector from a query vector, which requires a relational memory for distances between vectors and a sorting mechanism over the distances. For relational reasoning tasks, the pivot is the number of extracted items $n_{q}$ for establishing the relational memory. Hence, we run our STM with different $n_{q}=1,4,8$ using the same problem setting (8 16- dimensional input vectors), optimizer (Adam), batch size (1600) as in Santoro et al. (2018). We also run the task with TPR (Schlag \&amp; Schmidhuber, 2018)-a highorder fast-weight model that is designed for reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Length 30</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Length 50</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E.</td>
<td style="text-align: center;">A.</td>
<td style="text-align: center;">E.</td>
<td style="text-align: center;">A.</td>
</tr>
<tr>
<td style="text-align: center;">Fast weight ${ }^{*}$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: center;">WeiNet ${ }^{*}$</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">STM ( $d=48$, w/o transfer)</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">STM ( $d=48$ )</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">STM ( $d=96$, w/o gates)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">STM ( $d=96$ )</td>
<td style="text-align: center;">$\mathbf{1 0}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\mathbf{2 0}$</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<p>Table 1. Comparison of models on associative retrieval task with number of epochs E. required to converge (lower is better) and convergence test accuracy A. (\%, higher is better). * is reported from Zhang \&amp; Zhou (2017).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DNC $^{*}$</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">RMC $^{*}$</td>
<td style="text-align: center;">91</td>
</tr>
<tr>
<td style="text-align: center;">TPR</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">STM $\left(n_{q}=1\right)$</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: center;">STM $\left(n_{q}=4\right)$</td>
<td style="text-align: center;">95</td>
</tr>
<tr>
<td style="text-align: center;">STM $\left(n_{q}=8\right)$</td>
<td style="text-align: center;">$\mathbf{9 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2. Comparison of models on $N^{t h}$-farthest task (test accuracy). * is reported from Santoro et al. (2018).</p>
<p>As reported in Table 2, increasing $n_{q}$ gradually improves the accuracy of STM. As there are 8 input vectors in this task, literally, at each timestep the model needs to extract 8 items to compute all pairs of distances. However, as the extracted item is an entangled representation of all stored vectors and the temporarily computed distances are stored in separate high-order storage, even with $n_{q}=1,4$, STM achieves moderate results. With $n_{q}=8$, STM nearly solves the task perfectly, outperforming RMC by a large margin. We have tried to tune TPR for this task without success (see Appendix $\S$ E). This illustrates the challenge of training high-order neural networks in diverse contexts.</p>
<h3>3.2. Algorithmic synthetic tasks</h3>
<p>Algorithmic synthetic tasks (Graves et al., 2014) examine sequential models on memorization capacity (eg., Copy, Associative recall) and simple relational reasoning (eg., Priority sort). Even without explicit relational memory, MANNs have demonstrated good performance (Graves et al., 2014; Le et al., 2020), but they are verified for only low-dimensional input vectors ( $&lt;8$ bits). As higher-dimensional inputs necessitate higher-fidelity memory storage, we evaluate the high-fidelity reconstruction capacity of sequential models for these algorithmic tasks with 32-bit input vectors.</p>
<p>Two chosen algorithmic tasks are Copy and Priority sort. Item memory is enough for Copy where the models just output the input vectors seen in the same order in which</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Bit error per sequence vs training iteration for algorithmic synthetic tasks.
they are presented. For Priority sort, a relational operation that compares the priority of input vectors is required to produce the seen input vectors in the sorted order according to the priority score attached to each input vector. The relationship is between input vectors and thus simply firstorder (see Appendix $\S$ G for more on the order of relationship).</p>
<p>Inspired by Associative recall and $N^{t h}$-farthest tasks, we create a new task named Relational Associative Recall (RAR). In RAR, the input sequence is a list of items followed by a query item. Each item is a list of several 32-bit vectors and thus can be interpreted as a concatenated long vector. The requirement is to reconstruct the seen item that is farthest or closest (yet unequal) to the query. The type of the relationship is conditioned on the last bit of the query vector, i.e., if the last bit is 1 , the target is the farthest and 0 the closest. The evaluated models must compute the distances from the query item to any other seen items and then compare the distances to find the farthest/closest one. Hence, this task is similar to the $N^{t h}$-farthest task, which is second-order relational and thus needs relational memory. However, this task is more challenging since the models must reconstruct the seen items (32-bit vectors). Compared to $N=8$ possible one-hot outputs in $N^{t h}$-farthest, the output space in RAR is $2^{32}$ per step, thereby requiring high-fidelity item memory.</p>
<p>We evaluate our model STM ( $n_{q}=8, d=96$ ) with the 4 following baselines: LSTM (Hochreiter \&amp; Schmidhuber, 1997), attentional LSTM (Bahdanau et al., 2015), NTM (Graves et al., 2014) and RMC (Santoro et al., 2018). Details of the implementation are listed in Appendix $\S$ F. The learning curves (mean and error bar over 5 runs) are presented in Fig. 2.</p>
<p>LSTM is often the worst performer as it is based on vector memory. ALSTM is especially good for Copy as it has a privilege to access input vectors at every step of decoding. However, when dealing with relational reasoning, memory-less attention in ALSTM does not help much.</p>
<p>NTM performs well on Copy and moderately on Priority sort, yet badly on RAR possibly due to its bias towards item memory. Although equipped with self-attention relational memory, RMC demonstrates trivial performance on all tasks. This suggests a limitation of using dot-product attention to represent relationships when the tasks stress memorization or the relational complexity goes beyond dotproduct capacity. Amongst all models, only the proposed STM demonstrates consistently good performance where it almost achieves zero errors on these 3 tasks. Notably, for RAR, only STM can surpass the bottleneck error of 30 bits and reach $\approx 1$ bit error, corresponding to $0 \%$ and $87 \%$ of items perfectly reconstructed, respectively.</p>
<h3>3.3. Geometric and graph reasoning</h3>
<p>Problems on geometry and graphs are a good testbed for relational reasoning, where geometry stipulates spatial relationships between points, and graphs the relational structure of nodes and edges. Classical problems include Convex hull, Traveling salesman problem (TSP) for geometry, and Shortest path, Minimum spanning tree for graph. Convex hull and TSP data are from Vinyals et al. (2015) where input sequence is a list of points' coordinates (number of points $N \sim[5,20]$ ). Graphs in Shortest path and Minimum spanning tree are generated with solutions found by Dijkstra and Kruskal algorithms, respectively. A graph input is represented as a sequence of triplets $\left(\right.$ node $<em 2="2">{1}$, node $</em>$ ).}$, edge $\left._{12}\right)$. The desired output is a sequence of associated features of the solution points/nodes (more in Appendix $\S \mathrm{H</p>
<p>We generate a random one-hot associated feature for each point/node, which is stacked into the input vector. This allows us to output the node's associated features. This is unlike Vinyals et al. (2015), who just outputs the pointers to the nodes. Our modification creates a challenge for both training and testing. The training is more complex as the feature of the nodes varies even for the same graph. The testing is challenging as the associated features are likely to be different from that in the training. A correct prediction</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>#Parameters</th>
<th>Convex hull</th>
<th></th>
<th>TSP</th>
<th></th>
<th>Shortest path</th>
<th>Minimum spanning tree</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>$N=5$</td>
<td>$N=10$</td>
<td>$N=5$</td>
<td>$N=10$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>LSTM</td>
<td>4.5 M</td>
<td>89.15</td>
<td>82.24</td>
<td>73.15 (2.06)</td>
<td>62.13 (3.19)</td>
<td>72.38</td>
<td>80.11</td>
</tr>
<tr>
<td>ALSTM</td>
<td>3.7 M</td>
<td>89.92</td>
<td>85.22</td>
<td>71.79 (2.05)</td>
<td>55.51 (3.21)</td>
<td>76.70</td>
<td>73.40</td>
</tr>
<tr>
<td>DNC</td>
<td>1.9 M</td>
<td>89.42</td>
<td>79.47</td>
<td>73.24 (2.05)</td>
<td>61.53 (3.17)</td>
<td>83.59</td>
<td>82.24</td>
</tr>
<tr>
<td>RMC</td>
<td>2.8 M</td>
<td>93.72</td>
<td>81.23</td>
<td>72.83 (2.05)</td>
<td>37.93 (3.79)</td>
<td>66.71</td>
<td>74.98</td>
</tr>
<tr>
<td>STM</td>
<td>1.9 M</td>
<td>96.85</td>
<td>91.88</td>
<td>73.96 (2.05)</td>
<td>69.43 (3.03)</td>
<td>93.43</td>
<td>94.77</td>
</tr>
</tbody>
</table>
<p>Table 3. Prediction accuracy (%) for geometric and graph reasoning with random one-hot features. Italic numbers are tour length–additional metric for TSP. Average optimal tour lengths found by brute-force search for $N=5$ and 10 are 2.05 and 2.88, respectively.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Average reward vs number of games for reinforcement learning task in n-frame skip settings.</p>
<p>for a timestep is made when the predicted feature matches perfectly with the ground truth feature in the timestep. To measure the performance, we use the average accuracy of prediction across steps. We use the same baselines as in § 3.2 except that we replace NTM with DNC as DNC performs better on graph reasoning (Graves et al., 2016).</p>
<p>We report the best performance of the models on the testing datasets in Table 3. Although our STM has fewest parameters, it consistently outperforms other baselines by a significant margin. As usual, LSTM demonstrates an average performance across tasks. RMC and ALSTM are only good at Convex hull. DNC performs better on graph-like problems such as Shortest path and Minimum spanning tree. For the NP-hard TSP ($N=5$), despite moderate point accuracy, all models achieve nearly minimal solutions with an average tour length of 2.05. When increasing the difficulty with more points ($N=10$), none of these models reach an average optimal tour length of 2.88. However, only STM approaches closer to the optimal solution without the need for pointer and beam search mechanisms. Armed with both item and relational memory, STM's superior performance suggests a qualitative difference in the way STM and other methods solve these problems.</p>
<h3>3.4. Reinforcement learning</h3>
<p>Memory is helpful for partially observable Markov decision process (Bakker, 2002). We apply our memory to LSTM agents in Atari game environment using A3C training (Mnih et al., 2016). More details are given in Appendix § I. In Atari games, each state is represented as the visual features of a video frame and thus is partially observable. To perform well, RL agents should remember and relate several frames to model the game state comprehensively. These abilities are challenged when over-sampling and under-sampling the observation, respectively. We analyze the performance of LSTM agents and their STM-augmented counterparts under these settings using a game: Pong.</p>
<p>To be specific, we test the two agents on different frame skips (0, 4, 16, 32). We create $n$-frame skip setting by allowing the agent to see the environment only after every $n$ frames, where 4-frame skip is standard in most Atari environments. When no frameskip is applied (over-sampling), the number of observations is dense and the game is long (up to 9000 steps per game), which requires high-capacity item memory. On the contrary, when a lot of frames are skipped (under-sampling), the observations become scarce and the agents must model the connection between frames meticulously, demanding better relational memory.</p>
<p>We run each configuration 5 times and report the mean and error bar of moving average reward (window size = 100) through training time in Fig. 3. In a standard condition (4-frame skip), both baselines can achieve perfect performance and STM outperforms LSTM slightly in terms of convergence speed. The performance gain becomes clearer under extreme conditions with over-sampling and under-sampling. STM agents require fewer practices to accomplish higher rewards, especially in the 32-frame skip environment, which illustrates that having strong item and relational memory in a single model is beneficial to RL agents.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Error</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Mean</td>
<td>Best</td>
</tr>
<tr>
<td>DNC (Graves et al., 2016)</td>
<td>$12.8 \pm 4.7$</td>
<td>3.8</td>
</tr>
<tr>
<td>NUTM (Le et al., 2020)</td>
<td>$5.6 \pm 1.9$</td>
<td>3.3</td>
</tr>
<tr>
<td>TPR (Schlag \&amp; Schmidhuber, 2018)</td>
<td>$1.34 \pm 0.52$</td>
<td>0.81</td>
</tr>
<tr>
<td>UT (Dehghani et al., 2018)</td>
<td>$1.12 \pm 1.62$</td>
<td>0.21</td>
</tr>
<tr>
<td>MNM-p (Munkhdalai et al., 2019)</td>
<td>$0.55 \pm 0.74$</td>
<td>0.18</td>
</tr>
<tr>
<td>STM</td>
<td>$\mathbf{0 . 3 9} \pm \mathbf{0 . 1 8}$</td>
<td>$\mathbf{0 . 1 5}$</td>
</tr>
</tbody>
</table>
<p>Table 4. bAbI task: mean $\pm$ std. and best error over 10 runs.</p>
<h3>3.5. Question answering</h3>
<p>bAbI is a question answering dataset that evaluates the ability to remember and reason on textual information (Weston et al., 2015). Although synthetically generated, the dataset contains 20 challenging tasks such as pathfinding and basic induction, which possibly require both item and relational memory. Following Schlag \&amp; Schmidhuber (2018), each story is preprocessed into a sentence-level sequence, which is fed into our STM as the input sequence. We jointly train STM for all tasks using normal supervised training (more in Appendix $\S$ J). We compare our model with recent memory networks and report the results in Table 4.</p>
<p>MANNs such as DNC and NUTM have strong item memory, yet do not explicitly support relational learning, leading to significantly higher errors compared to other models. On the contrary, TPR is explicitly equipped with relational bindings but lack of item memory and thus clearly underperforms our STM. Universal Transformer (UT) supports a manually set item memory with dot product attention, showing higher mean error than STM with learned item memory and outer product attention. Moreover, our STM using normal supervised loss outperforms MNM-p trained with meta-level loss, establishing new state-of-the-arts on bAbI dataset. Notably, STM achieves this result with low variance, solving 20 tasks for 9/10 run (see Appendix $\S$ J).</p>
<h2>4. Related Work</h2>
<p>Background on associative memory Associative memory is a classical concept to model memory in the brain (Marr \&amp; Thach, 1991). While outer product is one common way to form the associative memory, different models employ different memory retrieval mechanisms. For example, Correlation Matrix Memory (CMM) and Hopfield network use dot product and recurrent networks, respectively (Kohonen, 1972; Hopfield, 1982). The distinction between our model and other associative memories lies in the fact that our model's association comes from several pieces of the memory itself rather than the input data. Also, unlike other two-memory systems (Le et al., 2018b; 2020) that simulate data/program memory in computer architec-
ture, our STM resembles item and relational memory in human cognition.</p>
<p>Background on attention Attention is a mechanism that allows interactions between a query and a set of stored keys/values (Graves et al., 2014; Bahdanau et al., 2014). Self-attention mechanism allows stored items to interact with each other either in forms of feed-forward (Vaswani et al., 2017) or recurrent (Santoro et al., 2018; Le et al., 2019) networks. Modeling memory interactions can also be achieved via attention over a set of parallel RNNs (Henaff et al., 2016). Although some form of relational memory can be kept in these approaches, they all use dot product attention to measure interactions per attention head as a scalar, and thus loose much relational information. We use outer product to represent the interactions as a matrix and thus our outer product self-attention is supposed to be richer than the current self-attention mechanisms (Prop. 1).</p>
<p>SAM as fast-weight Outer product represents Hebbian learning-a fast learning rule that can be used to build fast-weights (von der Malsburg, 1981). As the name implies, fast-weights update whenever an input is introduced to the network and stores the input pattern temporarily for sequential processing (Ba et al., 2016a). Meta-trained fast-weights (Munkhdalai et al., 2019) and gating of fastweights (Schlag \&amp; Schmidhuber, 2017; Zhang \&amp; Zhou, 2017) are introduced to improve memory capacity. Unlike these fast-weight approaches, our model is not built on top of other RNNs. Recurrency is naturally supported within STM.</p>
<p>The tensor product representation (TPR), which is a form of high-order fast-weight, can be designed for structural reasoning (Smolensky, 1990). In a recent work (Schlag \&amp; Schmidhuber, 2018), a third-order TPR resembles our relational memory $\mathcal{M}<em 1="1">{1}^{r}$ where both are $3 D$ tensors. However, TPR does not enable interactions amongst stored patterns through self-attention mechanism. The meaning of each dimension of the TPR is not related to that of $\mathcal{M}</em>$. More importantly, TPR is restricted to question answering task.}^{r</p>
<p>SAM as bi-linear model Bi-linear pooling produces output from two input vectors by considering all pairwise bit interactions and thus can be implemented by means of outer product (Tenenbaum \&amp; Freeman, 2000). To reduce computation cost, either low-rank factorization (Yu et al., 2017) or outer product approximation (Pham \&amp; Pagh, 2013) is used. These approaches aim to enrich feed-forward layers with bi-linear poolings yet have not focused on maintaining a rich memory of relationships.</p>
<p>Low-rank bi-linear pooling is extended to perform visual</p>
<p>attentions (Kim et al., 2018). It results in different formulation from our outer product attention, which is equivalent to full rank bi-linear pooling (§ 2.1). These methods are designed for static visual question answering while our approach is used to maintain a relational memory over time, which can be applied to any sequential problem.</p>
<h2>5. Conclusions</h2>
<p>We have introduced the SAM-based Two-memory Model (STM) that implements both item and relational memory. To wire up the two memory system, we employ a novel operator named Self-attentive Associative Memory (SAM) that constructs the relational memory from outer-product relationships between arbitrary pieces of the item memory. We apply read, write and transfer operators to access, update and distill the knowledge from the two memories. The ability to remember items and their relationships of the proposed STM is validated through a suite of diverse tasks including associative retrieval, $N^{t h}$-farthest, vector algorithms, geometric and graph reasoning, reinforcement learning and question answering. In all scenarios, our model demonstrates strong performance, confirming the usefulness of having both item and relational memory in one model.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research was partially funded by the Australian Government through the Australian Research Council (ARC). Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).</p>
<h2>References</h2>
<p>Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., and Ionescu, C. Using fast weights to attend to the recent past. In Advances in Neural Information Processing Systems, pp. $4331-4339,2016 a$.</p>
<p>Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016b.</p>
<p>Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. Proceedings of the International Conference on Learning Representations, 2015.</p>
<p>Bakker, B. Reinforcement learning with long short-term memory. In Advances in neural information processing systems, pp. 1475-1482, 2002.</p>
<p>Buckley, M. J. The role of the perirhinal cortex and hippocampus in learning, memory, and perception. The Quarterly Journal of Experimental Psychology Section B, 58(3-4):246-268, 2005.</p>
<p>Cohen, N. J., Poldrack, R. A., and Eichenbaum, H. Memory for items and memory for relations in the procedural/declarative memory framework. Memory, 5(1-2): $131-178,1997$.</p>
<p>Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, Ł. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Eichenbaum, H. Memory, amnesia, and the hippocampal system. MIT press, 1993.</p>
<p>Elman, J. L. Finding structure in time. Cognitive science, 14(2):179-211, 1990.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.</p>
<p>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476, 2016.</p>
<p>Henaff, M., Weston, J., Szlam, A., Bordes, A., and LeCun, Y. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016.</p>
<p>Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.</p>
<p>Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982.</p>
<p>Kim, J.-H., Jun, J., and Zhang, B.-T. Bilinear attention networks. In Advances in Neural Information Processing Systems, pp. 1564-1574, 2018.</p>
<p>Kohonen, T. Correlation matrix memories. IEEE transactions on computers, 100(4):353-359, 1972.</p>
<p>Konkel, A. and Cohen, N. J. Relational memory and the hippocampus: representations and methods. Frontiers in neuroscience, 3:23, 2009.</p>
<p>Kumaran, D. and McClelland, J. L. Generalization through the recurrent interaction of episodic memories: a model of the hippocampal system. Psychological review, 119 (3):573, 2012.</p>
<p>Le, H., Tran, T., Nguyen, T., and Venkatesh, S. Variational memory encoder-decoder. In Advances in Neural Information Processing Systems, pp. 1508-1518, 2018a.</p>
<p>Le, H., Tran, T., and Venkatesh, S. Dual memory neural computer for asynchronous two-view sequential learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining, KDD '18, pp. 1637-1645, New York, NY, USA, 2018b. ACM. ISBN 978-1-4503-5552-0. doi: 10.1145/3219819.3219981. URL http://doi.acm.org/10.1145/3219819.3219981</p>
<p>Le, H., Tran, T., and Venkatesh, S. Learning to remember more with less memorization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1xlvi0qYm.</p>
<p>Le, H., Tran, T., and Venkatesh, S. Neural storedprogram memory. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkxxA24FDz.</p>
<p>Marr, D. and Thach, W. T. A theory of cerebellar cortex. In From the Retina to the Neocortex, pp. 11-50. Springer, 1991.</p>
<p>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 19281937, 2016.</p>
<p>Munkhdalai, T., Sordoni, A., Wang, T., and Trischler, A. Metalearned neural memory. In Advances in Neural Information Processing Systems, pp. 13310-13321, 2019.</p>
<p>Olson, I. R., Page, K., Moore, K. S., Chatterjee, A., and Verfaellie, M. Working memory for conjunctions relies on the medial temporal lobe. Journal of Neuroscience, 26(17):4596-4601, 2006.</p>
<p>Pham, N. and Pagh, R. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 239-247. ACM, 2013.</p>
<p>Rojas, R. Neural networks: a systematic introduction. Springer Science \&amp; Business Media, 2013.</p>
<p>Rudelson, M. and Vershynin, R. Sampling from large matrices: An approach through geometric functional analysis. Journal of the ACM (JACM), 54(4):21, 2007.</p>
<p>Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap, T. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pp. 4967-4976, 2017.</p>
<p>Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., Wierstra, D., Vinyals, O.,</p>
<p>Pascanu, R., and Lillicrap, T. Relational recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 7299-7310, 2018.</p>
<p>Schlag, I. and Schmidhuber, J. Gated fast weights for on-the-fly neural program generation. In NIPS Metalearning Workshop, 2017.</p>
<p>Schlag, I. and Schmidhuber, J. Learning to reason with third order tensor products. In Advances in Neural Information Processing Systems, pp. 9981-9993, 2018.</p>
<p>Smolensky, P. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159-216, 1990.</p>
<p>Tenenbaum, J. B. and Freeman, W. T. Separating style and content with bilinear models. Neural computation, 12 (6):1247-1283, 2000.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692-2700, 2015.
von der Malsburg, C. The correlation theory of brain function, 1981. URL http://cogprints.org/1380/.</p>
<p>Weston, J., Bordes, A., Chopra, S., Rush, A. M., van Merriënboer, B., Joulin, A., and Mikolov, T. Towards aicomplete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.</p>
<p>Yu, Z., Yu, J., Fan, J., and Tao, D. Multi-modal factorized bilinear pooling with co-attention learning for visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 18211830, 2017.</p>
<p>Zeithamova, D., Schlichting, M. L., and Preston, A. R. The hippocampus and inferential reasoning: building memories to navigate future decisions. Frontiers in human neuroscience, 6:70, 2012.</p>
<p>Zhang, W. and Zhou, B. Learning to update autoassociative memory in recurrent neural networks for improving sequence memorization. ArXiv, abs/1709.06493, 2017.</p>
<h2>Appendix</h2>
<h2>A. Relationship between OPA and DPA</h2>
<p>Lemma 2. For $\forall n_{i}, n_{j} \in \mathbb{N}^{+}$,</p>
<p>$$
\sum_{i=1}^{n_{i}} \sum_{j=1}^{n_{j}} q_{j} k_{i j} v_{i}=\sum_{j=1}^{n_{j}} \sum_{i=1}^{n_{i}} q_{j} k_{i j} v_{i}
$$</p>
<p>where $q_{j}, k_{i j}, v_{i} \in \mathbb{R}$.</p>
<p>Proof. We will prove by induction for all $n_{j} \in \mathbb{N}^{+}$.
Base case: when $n_{j}=1$, the $L H S=R H S=$ $\sum_{i}^{n_{i}} q_{1} k_{i 1} v_{i}$. Let $t \in \mathbb{N}^{+}$be given and suppose Eq. 15 is true for $n_{j}=t$. Then</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{n_{i}} \sum_{j=1}^{t+1} q_{j} k_{i j} v_{i} &amp; =\sum_{i=1}^{n_{i}}\left(q_{t+1} k_{i t+1} v_{i}+\sum_{j=1}^{t} q_{j} k_{i j} v_{i}\right) \
&amp; =\sum_{i=1}^{n_{i}} q_{t+1} k_{i t+1} v_{i}+\sum_{i=1}^{n_{i}} \sum_{j=1}^{t} q_{j} k_{i j} v_{i} \
&amp; =\sum_{i=1}^{n_{i}} q_{t+1} k_{i t+1} v_{i}+\sum_{j=1}^{t} \sum_{i=1}^{n_{i}} q_{j} k_{i j} v_{i} \
&amp; =\sum_{j=1}^{t+1} \sum_{i=1}^{n_{i}} q_{j} k_{i j} v_{i}
\end{aligned}
$$</p>
<p>Thus, Eq. 15 holds for $n_{j}=t+1$ and $\forall n_{j} \in \mathbb{N}^{+}$by the principle of induction.</p>
<p>Proposition 3. Assume that $\mathcal{S}$ is a linear transformation: $\mathcal{S}(x)=a x+b(a, b, x \in \mathbb{R})$, we can extract $A^{*}$ from $A^{\otimes}$ by using an element-wise linear transformation $\mathcal{F}(x)=a^{f} \odot$ $x+b^{f}\left(a^{f}, b^{f}, x \in \mathbb{R}^{d_{q k}}\right)$ and a contraction $\mathcal{P}: \mathbb{R}^{d_{q k} \times d_{v}} \rightarrow$ $\mathbb{R}^{d_{v}}$ such that</p>
<p>$$
A^{*}(q, K, V)=\mathcal{P}\left(A^{\otimes}(q, K, V)\right)
$$</p>
<p>where</p>
<p>$$
\begin{gathered}
A^{*}(q, K, V)=\sum_{i=1}^{n_{k v}} \mathcal{S}\left(q \cdot k_{i}\right) v_{i} \
A^{\otimes}(q, K, V)=\sum_{i=1}^{n_{k v}} \mathcal{F}\left(q \odot k_{i}\right) \otimes v_{i}
\end{gathered}
$$</p>
<p>Proof. We derive the LHS. Let $u_{i}$ denote the scalar $\mathcal{S}\left(q \cdot k_{i}\right)$, then</p>
<p>$$
\begin{aligned}
u_{i} &amp; =\mathcal{S}\left(q \cdot k_{i}\right)=\mathcal{S}\left(\sum_{j=1}^{d_{q k}} q_{j} k_{i j}\right) \
&amp; =\sum_{j=1}^{d_{q k}} a q_{j} k_{i j}+b
\end{aligned}
$$</p>
<p>where $q_{j}$ and $k_{i j}$ are the $j$-th elements of vector $q$ and $k_{i}$, respectively. Let $l \in \mathbb{R}^{d_{v}}$ denote the vector $A^{*}(q, K, V)=$ $\sum_{i=1}^{n_{k v}} u_{i} v_{i}$, then the $t$-th element of $l$ is</p>
<p>$$
\begin{aligned}
l_{t} &amp; =\sum_{i=1}^{n_{k v}} u_{i} v_{i t} \
&amp; =\sum_{i=1}^{n_{k v}}\left(\sum_{j=1}^{d_{q k}} a q_{j} k_{i j}+b\right) v_{i t} \
&amp; =\sum_{i=1}^{n_{k v}} \sum_{j=1}^{d_{q k}} a q_{j} k_{i j} v_{i t}+b \sum_{i=1}^{n_{k v}} v_{i t} \
&amp; =a \sum_{i=1}^{n_{k v}} \sum_{j=1}^{d_{q k}} q_{j} k_{i j} v_{i t}+b \sum_{i=1}^{n_{k v}} v_{i t}
\end{aligned}
$$</p>
<p>We derive the RHS. Let $d_{i}$ denote the vector $\mathcal{F}\left(q \odot k_{i}\right)$, then the $j$-th element of $d_{i}$ is</p>
<p>$$
\begin{aligned}
d_{i j} &amp; =\mathcal{F}\left(q_{j} k_{i j}\right) \
&amp; =a_{j}^{f} q_{j} k_{i j}+b_{j}^{f}
\end{aligned}
$$</p>
<p>Let $e \in \mathbb{R}^{d_{q k} \times d_{v}}$ denote the matrix $A^{\otimes}(q, K, V)=$ $\sum_{i=1}^{n_{k v}} d_{i} \otimes v_{i}$, then the $j$-th row, $t$-column element of $e$ is</p>
<p>$$
\begin{aligned}
e_{j t} &amp; =\sum_{i=1}^{n_{k v}} d_{i j} v_{i t} \
&amp; =\sum_{i=1}^{n_{k v}}\left(a_{j}^{f} q_{j} k_{i j}+b_{j}^{f}\right) v_{i t} \
&amp; =\sum_{i=1}^{n_{k v}} a_{j}^{f} q_{j} k_{i j} v_{i t}+b_{j}^{f} \sum_{i=1}^{n_{k v}} v_{i t}
\end{aligned}
$$</p>
<p>Let $r \in \mathbb{R}^{d_{v}}$ denote the vector $\sum_{j=1}^{d_{q k}} e_{j}$, then the $t$-th element of $r$ is</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Addition complexity</th>
<th style="text-align: center;">Multiplication complexity</th>
<th style="text-align: center;">Physical storage for relationships</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DPA</td>
<td style="text-align: center;">$O\left(\left(d_{q k} n_{q}+d_{v}\right) n_{k v}\right)$</td>
<td style="text-align: center;">$O\left(\left(d_{q k}+d_{v}\right) n_{q} n_{k v}\right)$</td>
<td style="text-align: center;">$O\left(n_{q} n_{k v}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">OPA</td>
<td style="text-align: center;">$O\left(n_{q} n_{k v} d_{q k} d_{v}\right)$</td>
<td style="text-align: center;">$O\left(n_{q} d_{q k} d_{v}\right)$</td>
<td style="text-align: center;">$O\left(n_{q} d_{q k} d_{v}\right)$</td>
</tr>
</tbody>
</table>
<p>Table 5. Computational complexity of DPA and OPA with $n_{q}$ queries and $n_{k v}$ key-value pairs. $d_{q k}$ denotes query or key size, while $d_{v}$ value size.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Wall-clock time (second)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">NTM</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: left;">RMC</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">STM</td>
<td style="text-align: center;">0.3</td>
</tr>
</tbody>
</table>
<p>Table 6. Wall-clock time to process a batch of data on Priority Sort task. The batch size is 128. All models are implemented using Pytorch, have around 1 million parameters and run on the same machine with Tesla V100-SXM2 GPU.</p>
<p>$$
\begin{aligned}
r_{t} &amp; =\sum_{j=1}^{d_{q k}} e_{j t} \
&amp; =\sum_{j=1}^{d_{q k}}\left(\sum_{i=1}^{n_{k v}} a_{j}^{f} q_{j} k_{i j} v_{i t}+b_{j}^{f} \sum_{i=1}^{n_{k v}} v_{i t}\right) \
&amp; =\sum_{j=1}^{d_{q k}} \sum_{i=1}^{n_{k v}} a_{j}^{f} q_{j} k_{i j} v_{i t}+\sum_{j=1}^{d_{q k}} b_{j}^{f} \sum_{i=1}^{n_{k v}} v_{i t}
\end{aligned}
$$</p>
<p>We can always choose $a_{j}^{f}=a$ and $\sum_{j=1}^{d_{q k}} b_{j}^{f}=b$. Eq. 22 becomes,</p>
<p>$$
r_{t}=a \sum_{j=1}^{d_{q k}} \sum_{i=1}^{n_{k v}} q_{j} k_{i j} v_{i t}+b \sum_{i}^{n_{k v}} v_{i t}
$$</p>
<p>According to Lemma 2, $l_{t}=r_{t} \forall d_{q k}, n_{k v} \in \mathbb{N}^{+} \Rightarrow l=$ $r$. Also, $\exists \mathcal{P}$ as a contraction: $\mathcal{P}(X)=a_{p} X$ with $a_{p}=$ $[1, \ldots, 1] \in \mathbb{R}^{1 \times d_{q k}}$.</p>
<p>We compare the complexity of DPA and OPA in Table 5. In general, compared to that of DPA, OPA's complexity is increased by an order of magnitude, which is equivalent to the size of the patterns. In practice, we keep that value small (96) to make the training efficient. That said, due to its high-order nature, our memory model still maintains enormous memory space. In terms of speed, STM's running time is almost the same as RMC's and much faster than that of DNC or NTM. Table 6 compares the real running time of several memory-based models on Priority Sort task.</p>
<h2>B. Relationship between OPA and bi-linear model</h2>
<p>Proposition 4. Given the number of key-value pairs $n_{k v}=$ 1 , and $\mathcal{G}$ is a high dimensional linear transformation $\mathcal{G}: \mathbb{R}^{d_{q k} \times d_{v}} \rightarrow \mathbb{R}^{n}, \mathcal{G}(X)=W^{g} \mathcal{V}(X)$ where $W^{g} \in$ $\mathbb{R}^{n \times d_{q k} d_{v}}, \mathcal{V}$ is a function that flattens its input tensor, then $\mathcal{G}\left(A^{\otimes}(q, K, V)\right)$ can be interpreted as a bi-linear model between $f$ and $v_{1}$, that is</p>
<p>$$
\mathcal{G}\left(A^{\otimes}(q, K, V)\right)[s]=\sum_{j=1}^{d_{q k}} \sum_{t=1}^{d_{v}} W^{g}[s, j, t] f[j] v_{1}[t]
$$</p>
<p>where $W^{g}[s, j, t]=W^{g}[s]\left[(j-1) d_{v}+t\right], s=1, \ldots, n$, $j=1, \ldots, d_{q k}, t=1, \ldots, d_{v}$, and $f=\mathcal{F}\left(q \odot k_{1}\right)$.</p>
<p>Proof. By definition,</p>
<p>$$
\begin{aligned}
\mathcal{V}\left(\mathcal{F}\left(q \odot k_{1}\right) \otimes v_{1}\right)\left[(j-1) d_{v}+t\right] &amp; =\left(\mathcal{F}\left(q \odot k_{1}\right) \otimes v_{1}\right)[j][t] \
&amp; =\mathcal{F}\left(q \odot k_{1}\right)[j] v_{1}[t]
\end{aligned}
$$</p>
<p>We derive the LHS,</p>
<p>$$
\begin{aligned}
\mathcal{G}\left(A^{\otimes}(q, K, V)\right)[s] &amp; =\left(W^{g} \mathcal{V}\left(\mathcal{F}\left(q \odot k_{1}\right) \otimes v_{1}\right)\right)[s] \
&amp; =\sum_{u=1}^{d_{q k} d_{v}} W^{g}[s][u] \mathcal{V}\left(\mathcal{F}\left(q \odot k_{1}\right) \otimes v_{1}\right)[u] \
&amp; =\sum_{\left(j-1\right) d_{v}+t}^{d_{q k} d_{v}}\left(W^{g}[s]\left[(j-1) d_{v}+t\right]\right. \
&amp; \left.\times \mathcal{V}\left(\mathcal{F}\left(q \odot k_{1}\right) \otimes v_{1}\right)\left[(j-1) d_{v}+t\right]\right) \
&amp; =\sum_{j=1}^{d_{q k}} \sum_{t=1}^{d_{v}} W^{g}[s, j, t] \mathcal{F}\left(q \odot k_{1}\right)[j] v_{1}[t]
\end{aligned}
$$</p>
<p>which equals the RHS.</p>
<p>Prop. 4 is useful since it demonstrates the representational capacity of OPA is at least equivalent to bi-linear pooling, which is richer than low-rank bi-linear pooling using Hadamard product, or bi-linear pooling using identity matrix of the bi-linear form (dot product), or the vanilla linear models using traditional neural networks.</p>
<p>Proposition 5. Given the number of queries $n_{q}=d_{q k}$, the number of key-value pairs $n_{k v}=1, \mathcal{M}<em _theta="\theta">{t}^{r}=\operatorname{SAM}</em>}(M)$ where $M$ is an instance of the item memory in the past, and $\mathcal{G}$ is a high dimensional linear transformation $\mathcal{G}$ : $\mathbb{R}^{n_{q} \times d_{q k} \times d_{v}} \rightarrow \mathbb{R}^{d_{q k} \times d_{v}}, \mathcal{G}(X)=W^{g} \mathcal{V<em k="k" q="q">{f}(X)$ where $W^{g} \in \mathbb{R}^{d</em>$ is a function that flattens the first two dimensions of its input tensor, then Eq. 13 can be interpreted as a Hebbian update to the item memory.} \times n_{q} d_{q k}}, \mathcal{V}_{f</p>
<p>Proof. Let $k_{1}=M_{k}$ and $v_{1}=M_{v}$ when $n_{k v}=$ 1, by definition $\mathcal{V}<em _theta="\theta">{f}\left(\operatorname{SAM}</em>[t]$. We derive,}(M)\right)\left[(s-1) d_{q k}+j, t\right]=$ $\mathcal{F}\left(M_{q}[s] \odot k_{1}\right)[j] v_{1</p>
<p>$$
\begin{aligned}
\mathcal{G}\left(\operatorname{SAM}<em f="f">{\theta}(M)\right)[i, t] &amp; =\left(W^{g} \mathcal{V}</em>}\left(\operatorname{SAM<em u="1">{\theta}(M)\right)\right)[i, t] \
&amp; =\sum</em>}^{n_{q} d_{q k}} W^{g}[i, u] \mathcal{V<em _theta="\theta">{f}\left(\operatorname{SAM}</em>(M)\right)[u, t] \
&amp; =\sum_{(s-1) d_{q k}+j=1}^{n_{q} d_{q k}}\left(W^{g}\left[i,(s-1) d_{q k}+j\right]\right. \
&amp; \times \mathcal{F}\left(M_{q}[s] \odot k_{1}\right)[j] v_{1}[t]) \
&amp; =\sum_{s=1}^{n_{q}} \sum_{j=1}^{d_{q k}} W^{g}[i, s, j] f[s, j] v_{1}[t]
\end{aligned}
$$</p>
<p>where $f[s, j]=\mathcal{F}\left(M_{q}[s] \odot k_{1}\right)[j]=\mathcal{F}\left(M_{q}[s, j] k_{1}[j]\right)$. It should be noted that with trivial rank-one $W^{g}: W^{g}[i]=$ $d_{i} \mathcal{V}<em i="i">{f}(I), d</em>, I$ is the identity matrix, Eq. 24 becomes} \in \mathbb{R</p>
<p>$$
\begin{aligned}
\mathcal{G}\left(\operatorname{SAM}<em 1="1">{\theta}(M)\right)[i, t]= &amp; d[i] v</em>[t] \
\Rightarrow \mathcal{G}\left(\operatorname{SAM}<em 1="1">{\theta}(M)\right)= &amp; d \otimes v</em>
\end{aligned}
$$</p>
<p>where $d \in \mathbb{R}^{d_{q k}}, d[i]=d_{i} \sum_{s=1}^{n_{q}} \mathcal{F}\left(M_{q}[s, s] k_{1}[s]\right)$. Eq. 13 reads</p>
<p>$$
\mathcal{M}<em t="t">{t}^{i}=\mathcal{M}</em>
$$}^{i}+\alpha_{3} d \otimes v_{1</p>
<p>which is a Hebbian update with the updated value $v_{1}$. As $v_{1}$ is a stored pattern extracted from $M$ encoded in the relational memory, the item memory is enhanced with a longterm stored value from the relational memory.</p>
<h2>C. OPA and SAM as associative memory ${ }^{1}$</h2>
<p>Proposition 6. If $\mathcal{P}$ is a contraction: $\mathbb{R}^{d_{q k} \times d_{v}} \rightarrow \mathbb{R}^{d_{v}}$, $\mathcal{P}(X)=a_{p} X, a_{p} \in \mathbb{R}^{1 \times d_{q k}}$, then $A^{\otimes}(q, K, V)$ is an associative memory that stores patterns $\left{v_{i}\right}<em k="k" v="v">{i=1}^{n</em>$ and}</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\mathcal{P}\left(A^{\otimes}(q, K, V)\right)$ is a retrieval process. Perfect retrieval is possible under the following three conditions,
(1) $\left{k_{i}\right}<em k="k" v="v">{i=1}^{n</em>$ form a set of linearly independent vectors
(2) $q_{i} \neq 0, i=1, \ldots, d_{q k}$
(3) $\mathcal{F}$ is chosen as $\mathcal{F}(x)=a^{f} \odot x\left(a^{f}, x \in \mathbb{R}^{d_{q k}}, a_{i}^{f} \neq 0\right.$, $\left.i=1, \ldots, d_{q k}\right)$}</p>
<p>Proof. By definition, $A^{\otimes}(q, K, V)$ forms a heteroassociative memory between $x_{i}=\mathcal{F}\left(q \odot k_{i}\right)$ and $v_{i}$. If $\left{x_{i}\right}<em k="k" v="v">{i=1}^{n</em>$, then}}$ are orthogonal, given some $\mathcal{P}$ with $a_{p}=\frac{x_{j}^{\top}}{\left|x_{j}^{\top}\right|</p>
<p>$$
\begin{aligned}
\mathcal{P}\left(A^{\otimes}(q, K, V)\right) &amp; =\frac{x_{j}^{\top}}{\left|x_{j}^{\top}\right|} \sum_{i=1}^{n_{k v}} x_{i} \otimes v_{i} \
&amp; =\sum_{i=1, i \neq j}^{n_{k v}} \frac{\left(x_{j}^{\top} x_{i}\right)}{\left|x_{j}^{\top}\right|} v_{i}^{\top}+\frac{\left(x_{j}^{\top} x_{j}\right)}{\left|x_{j}^{\top}\right|} v_{j}^{\top} \
&amp; =v_{j}^{\top}
\end{aligned}
$$</p>
<p>Hence, we can perfectly retrieve some stored pattern $v_{j}$ using its associated $\mathcal{P}$. In practice, linearly independent $\left{x_{i}\right}<em k="k" v="v">{i=1}^{n</em>\right}}}$ is enough for perfect retrieval since we can apply Gram-Schmidt process to construct orthogonal $\left{x_{i<em k="k" v="v">{i=1}^{n</em>$. Another solution is to follow Widrow-Hoff incremental update}</p>
<p>$$
\begin{aligned}
A^{\otimes}(q, K, V)(0) &amp; =0 \
A^{\otimes}(q, K, V)(i) &amp; =A^{\otimes}(q, K, V)(i-1) \
&amp; +\left(v_{i}-A^{\otimes}(q, K, V)(i-1) x_{i}\right) \otimes x_{i}
\end{aligned}
$$</p>
<p>which also results in possible perfect retrieval given $\left{x_{i}\right}<em k="k" v="v">{i=1}^{n</em>$ are linearly independent.
Now, we show that if (1) (2) (3) are satisfied, $\left{x_{i}\right}}<em k="k" v="v">{i=1}^{n</em>\right}}}$ are linearly independent using proof by contradiction. Assume that $\left{x_{i<em k="k" v="v">{i=1}^{n</em>\right}}}$ are linearly dependent, $\exists\left{\alpha_{i} \in \mathbb{R<em k="k" v="v">{i=1}^{n</em>$, not all zeros such that}</p>
<p>$$
\begin{aligned}
\overrightarrow{0} &amp; =\sum_{i=1}^{n_{k v}} \alpha_{i} x_{i}=\sum_{i=1}^{n_{k v}} \alpha_{i} \mathcal{F}\left(q \odot k_{i}\right) \
&amp; =\sum_{i=1}^{n_{k v}} \alpha_{i}\left(a^{f} \odot\left(q \odot k_{i}\right)\right) \
&amp; =\left(a^{f} \odot q\right) \odot\left(\sum_{i=1}^{n_{k v}} \alpha_{i} k_{i}\right)
\end{aligned}
$$</p>
<p>As (2) (3) hold true, Eq. 25 is equivalent to</p>
<p>$$
\vec{\vartheta}=\sum_{i=1}^{n_{k v}} \alpha_{i} k_{i}
$$</p>
<p>which contradicts (1).</p>
<p>Prop. 6 is useful as it points out the potential of our OPA formulation for accurate associative retrieval over several key-value pairs. That is, despite that many items are extracted to form the relational representation, we have the chance to reconstruct any items perfectly if the task requires item memory. As later we use neural networks to generate $k$ and $q$, the model can learn to satisfy conditions (1) and (2). Although in practice, we use element-wise $\tanh$ to offer non-linear transformation, which is different from (3), empirical results show that our model still excels at accurate associative retrieval.</p>
<p>Proposition 7. Assume that the gates in Eq. 10 are kept constant $F_{t}=I_{t}=1$, the item memory construction is simplified to</p>
<p>$$
M=\sum_{i=1}^{N+1} x_{i} \otimes x_{i}
$$</p>
<p>where $\left{x_{i}\right}_{i=1}^{N+1}$ are positive input patterns after feedforward neural networks and the relational memory construction is simplified to</p>
<p>$$
\mathcal{M}^{r}=\operatorname{SAM}_{\theta}(M)
$$</p>
<p>and layer normalizations are excluded, then the memory retrieval is a two-step contraction</p>
<p>$$
v^{r}=\operatorname{softmax}\left(z^{\top}\right) \mathcal{M}^{r} f(x)
$$</p>
<p>Proof. Without loss of generality, after seeing $N+1$ patterns $\left{x_{i}\right}<em p="p">{i=1}^{N+1}$, SAM is given a (noisy or incomplete) query pattern $x$ that corresponds to some stored pattern $x</em>$, that is}=x_{N+1</p>
<p>$$
\left{\begin{array}{l}
x_{p}^{\top} x \approx 1 \
x_{i}^{\top} x \approx 0 \quad i=\overline{1, N}
\end{array}\right.
$$</p>
<p>Unrolling Eq. 8 yields</p>
<p>$$
\begin{aligned}
\operatorname{SAM}<em j="1">{\theta}(M)[s] &amp; =\sum</em>[j] \
&amp; =\sum_{j=1}^{n_{k v}} \mathcal{F}\left(W_{q}[s]\left(\sum_{i=1}^{N+1} x_{i} \otimes x_{i}\right)\right. \
&amp; \left.\odot W_{k}[j]\left(\sum_{i=1}^{N+1} x_{i} \otimes x_{i}\right)\right) \
&amp; \otimes W_{v}[j]\left(\sum_{i=1}^{N+1} x_{i} \otimes x_{i}\right) \
&amp; =\sum_{j=1}^{n_{k v}} \mathcal{F}\left(\left(\sum_{i=1}^{N} W_{q}[s] x_{i} \otimes x_{i}\right.\right. \
&amp; \left.\left.+W_{q}[s] x_{p} \otimes x_{p}\right)\right) \
&amp; \left.\odot\left(\sum_{i=1}^{N} W_{k}[j] x_{i} \otimes x_{i}+W_{k}[j] x_{p} \otimes x_{p}\right)\right) \
&amp; \otimes\left(\sum_{i=1}^{N} W_{v}[j] x_{i} \otimes x_{i}+W_{v}[j] x_{p} \otimes x_{p}\right)
\end{aligned}
$$}^{n_{k v}} \mathcal{F}\left(M_{q}[s] \odot M_{k}[j]\right) \otimes M_{v</p>
<p>When $d&gt;N$, it is generally possible to find $W_{q}, W_{k}$ and $W_{v}$ that satisfy the following system of equations:</p>
<p>$$
\left{\begin{array}{l}
W_{q}[s] x_{i}=0, i=\overline{1, N} \
W_{q}[s] x_{p}=1 \
W_{k}[j] x_{i}=0, i=\overline{1, N} \
W_{k}[j] x_{p}=1 \
W_{v}[j] x_{i}=1, i=\overline{1, N} \
W_{v}[j] x_{p}=1
\end{array}\right.
$$</p>
<p>We also assume that $\mathcal{F}$ is chosen as square root function, then Eq. 26 simplifies to</p>
<p>$$
\begin{aligned}
\operatorname{SAM}<em j="1">{\theta}(M)[s] &amp; =\sum</em> \
&amp; =n_{k v} x_{p} \otimes \sum_{i=1}^{N+1} x_{i} \
&amp; =n_{k v} \sum_{i=1}^{N+1} x_{p} \otimes x_{i}
\end{aligned}
$$}^{n_{k v}} \mathcal{F}\left(x_{p} \odot x_{p}\right) \otimes \sum_{i=1}^{N+1} x_{i</p>
<p>The first contraction $\operatorname{softmax}\left(z^{\top}\right) \mathcal{M}^{r}$ can be interpreted as an attention to $\left{\operatorname{SAM}<em s="1">{\theta}(M)[s]\right}</em>$, which equals}^{n_{q}</p>
<p>$$
n_{k v} \sum_{i=1}^{N+1} x_{p} \otimes x_{i}
$$</p>
<p>The second contraction is similar to a normal associative memory retrieval. When we choose $f(x)=\frac{x}{n_{k v}}$, the retrieval reads</p>
<p>$$
\begin{aligned}
v^{r} &amp; =\left(n_{k v} \sum_{i=1}^{N+1} x_{p} \otimes x_{i}\right) \frac{x}{n_{k v}} \
&amp; =\sum_{i=1}^{N+1}\left(x_{i}^{\top} x\right) x_{p} \
&amp; \approx x_{p}
\end{aligned}
$$</p>
<h2>D. Implementation of gate functions</h2>
<p>$$
\begin{gathered}
F_{t}\left(\mathcal{M}<em t="t">{t-1}^{i}, x</em>}\right)=W_{F} x_{t}+U_{F} \tanh \left(\mathcal{M<em F="F">{t-1}^{i}\right)+b</em> \
I_{t}\left(\mathcal{M}<em t="t">{t-1}^{i}, x</em>}\right)=W_{I} x_{t}+U_{I} \tanh \left(\mathcal{M<em I="I">{t-1}^{i}\right)+b</em>
\end{gathered}
$$</p>
<p>Here, $W_{F}, U_{F}, W_{I}, W_{I} \in \mathbb{R}^{d \times d}$ are parametric weights, $b_{F}, b_{I} \in \mathbb{R}$ are biases and + is broadcasted if needed.</p>
<h2>E. Learning curves on ablation study</h2>
<p>We plot the learning curves of evaluated modes for Associative retrieval with length 30, 50 and $N^{t h}$-farthest in Fig. 4. For $N^{t h}$-farthest, the last input in the sequence is treated as the query for TPR. We keep the standard number of entities/roles and tune TPR $^{2}$ with different hidden dimensions $(40,128,256)$ and optimizers (Nadam and Adam). All configurations fail to converge for the normal $N^{t h}$-farthest as shown in Fig. 4 (right). When we reduce the problem size to 48 -dimensional input vectors, TPR can reach perfect performance, which indicates the problem here is more about scaling to bigger relational reasoning contexts.</p>
<h2>F. Implementation of baselines for algorithmic and geometric/graph tasks</h2>
<p>Following Graves et al. (2014), we use RMSprop optimizer with a learning rate of $10^{-4}$ and a batch size of 128 for all baselines.</p>
<ul>
<li>LSTM and ALSTM: Both use 512-dimensional hidden vectors for all tasks.</li>
<li>NTM $^{3}$, DNC $^{4}$ : Both use a 256 -dimensional LSTM controller for all tasks. For algorithmic tasks, NTM uses a 128 -slot external memory, each slot is a 32 dimensional vector. Following the standard setting,</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>NTM uses 1 control head for Copy, RAR and 5 control heads for Priority sort. For geometric/graph tasks, DNC is equipped with 64 -dimensional 20 -slot external memory and 4 -head controller. In geometric/graph problems, 20 slots are about the number of points/nodes. We also tested with layer-normalized DNC without temporal link matrix and got similar results.</p>
<ul>
<li>RMC $^{5}$ : We use the default setting with total 1024 dimensions for memory of 8 heads and 8 slots. We also tried with different numbers of slots ${1,4,16}$ and Adam optimizer but the performance did not change.</li>
<li>STM: We use the same setting across tasks $n_{q}=8$, $d=96, n_{r}=96 . \alpha_{1}, \alpha_{2}$, and $\alpha_{3}$ are learnable.</li>
</ul>
<h2>G. Order of relationship</h2>
<p>In this paper, we do not formally define the concept of order of relationship. Rather, we describe it using concrete examples. When a problem requires to compute the relationship between items, we regard it as a first-order relational problem. For example, sorting is first-order relational. Copy is even zero-order relational since it can be solved without considering item relationships. When a problem requires to compute the relationship between relationships of items, we regard it as a second-order relational problem and so on.</p>
<p>From this observation, we hypothesize that the computational complexity of a problem roughly corresponds to the order of relationship in the problem. For example, if a problem requires a solution whose computational complexity between $O(N)$ and $O\left(N^{2}\right)$ where $N$ is the input size, it means the solution basically computes the relationship between any pair of input items and thus corresponds to firstorder relationship. Table 7 summarizes our hypothesis on the order of relationship in some of our problems.</p>
<p>By design, our proposed STM stores a mixture of relationships between items in a relational memory, which approximately corresponds to a maximum of second-order relational capacity. The distillation process in STM transforms the relational memory to the output and thus determines the order of relationship that STM can offer. We can measure the degree that STM involves in relational mining by analyzing the learned weight $\mathcal{G}<em 2="2">{2}$ of the distillation process. Intuitively, a high-rank transformation $\mathcal{G}</em>$ corresponds to item-based retrieval without much relational mining (Prop. 5). The numerical rank of a matrix $A$ is defined as $r(A)=|A|}$ can capture more relational information from the relational memory. Trivial low-rank $\mathcal{G<em 2="2">{F}^{2} /|A|</em>$, which relaxes the exact notion of rank (Rudelson \&amp; Vershynin, 2007).}^{2</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Testing accuracy (\%) on associative retrieval $\mathrm{L}=30$ (left), $\mathrm{L}=50$ (middle) and $N^{t h}$-farthest (right).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">General complexity</th>
<th style="text-align: center;">Order</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Copy/Associative retrieval</td>
<td style="text-align: center;">$O(N)$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Sort</td>
<td style="text-align: center;">$O(N \log N)$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Convex hull</td>
<td style="text-align: center;">$O(N \log N)$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Shortest path ${ }^{6}$</td>
<td style="text-align: center;">$O(E \log V)$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Minimum spanning tree</td>
<td style="text-align: center;">$O(E \log V)$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">RAR/ $N^{t h}$-Farthest</td>
<td style="text-align: center;">$O\left(N^{2} \log N\right)$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Traveling salesman problem</td>
<td style="text-align: center;">NP-hard</td>
<td style="text-align: center;">many</td>
</tr>
</tbody>
</table>
<p>Table 7. Order of relationship in some problems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">$r\left(\mathcal{G}_{2}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Associative retrieval</td>
<td style="text-align: center;">$9.42 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;">$N^{t h}$-Farthest</td>
<td style="text-align: center;">$83.20 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">Copy</td>
<td style="text-align: center;">$79.00 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;">Sort</td>
<td style="text-align: center;">$79.58 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">RAR</td>
<td style="text-align: center;">$83.30 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">Convex hull</td>
<td style="text-align: center;">$80.78 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;">Traveling salesman problem</td>
<td style="text-align: center;">$83.58 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;">Shortest path</td>
<td style="text-align: center;">$79.81 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">Minimum spanning tree</td>
<td style="text-align: center;">$79.57 \pm 0.5$</td>
</tr>
</tbody>
</table>
<p>Table 8. Mean and std. of numerical rank of the leanred weight $\mathcal{G}_{2}$ for several tasks. The upper bound for the rank is 96 .</p>
<p>We report the numerical rank of learned $\mathcal{G}<em 2="2">{2} \in \mathbb{R}^{6144 \times 96}$ for different tasks in Table 8. For each task, we run the training 5 times and take the mean and std. of $r\left(\mathcal{G}</em>\right)$. The rank is generally higher for tasks that have higher orders of relationship. That said, the model tends to overuse its relational capacity. Even for the zero-order Copy task, the rank for the distillation transformation is still very high.</p>
<h2>H. Geometry and graph task description</h2>
<p>In this testbed, we use RMSprop optimizer with a learning rate of $10^{-4}$ and a batch size of 128 for all baselines. STM</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>uses the same setting across tasks $n_{q}=8, d=96, n_{r}=$ 96. The random one-hot features can be extended to binary features, which is much harder and will be investigated in our future works.</p>
<p>Convex hull Given a set of $N$ points with 2D coordinates, the model is trained to output a list of points that forms a convex hull sorted by coordinates. Training is done with $N \sim[5,20]$. Testing is done with $N=5$ and $N=10$ (no prebuilt dataset available for $N=20$ ). The output is a sequence of 20-dimensional one-hot vectors representing the features of the solution points in the convex-hull.</p>
<p>Traveling salesman problem Given a set of $N$ points with 2D coordinates, the model is trained to output a list of points that forms a closed tour sorted by coordinates. Training is done with $N \sim[5,10]$. Testing is done with $N=5$ and $N=10$. The output is a sequence of 20-dimensional one-hot vectors representing the features of the solution points in the optimal tour.</p>
<p>Shortest path The graph is generated according to the following rules: (1) choose the number of nodes $N \sim$ $[5,20]$, (2) after constructing a path that goes through every node in the graph (to make the graph connected), determine randomly the edge between nodes (number of edges $E \sim[6,30]$ ), (3) for each edge set the weight $w \sim[1,10]$. We generate 100,000 and 10,000 graphs for training and testing, respectively. The representation for an input graph is a sequence of triplets followed by 2 feature vectors representing the source and destination node. The output is a sequence of 40-dimensional one-hot feature vectors representing the solution nodes in the shortest path.</p>
<p>Minimum spanning tree We use the same generated input graphs from the Shortest path task. The representation for an input graph is only a sequence of triplets. The output is a sequence of 40 -dimensional one-hot feature vectors representing the features of the nodes in the solution edges of the minimum spanning tree.</p>
<p>Some generated samples of the four tasks are visualized in Fig. 5. Learning curves are given in Fig. 6.</p>
<h1>I. Reinforcement learning task description</h1>
<p>We trained Openai Gym's PongNoFrameskip-v4 using Asynchronous Advantage Actor-Critic (A3C) with hyperparameters: 32 workers, shared Adam optimizer with a learning rate of $10^{-4}, \gamma=0.99$. To extract scene features for LSTM and STM, we use 4 convolutional layers ( 32 kernels with $5 \times 5$ kernel sizes and a stride of 1 ), each of which is followed by a $2 \times 2$ max-pooling layer, resulting in 1024dimensional feature vectors. The LSTM 's hidden size is 512. STM uses $n_{q}=8, d=96, n_{r}=96$.</p>
<h2>J. bAbI task description</h2>
<p>We use the train/validation/test split introduced in bAbI's en-valid-10k v1.2 dataset. To make STM suitable for question answering task, each story is preprocessed into a sentence-level sequence, which is fed into our STM as the input sequence. The question, which is only 1 sentence, is preprocessed to a query vector. Then, we utilize the Inference module, which takes the query as input to extract the output answer from our relational memory $\mathcal{M}^{r}$. The preprocessing and the Inference module are the same as in Schlag \&amp; Schmidhuber (2018). STM's hyper-parameters are fixed to $n_{q}=20, d=90, n_{r}=96$. We train our model jointly for 20 tasks with a batch size of 128 , using Adam optimizer with a learning rate of $0.006, \beta_{1}=0.9$ and $\beta_{2}=0.99$. Details of all runs are listed in Table 9.</p>
<h2>K. Characteristics of memory-based neural networks</h2>
<p>Table 10 compares the characteristics of common neural networks with memory. Biological plausibility is determined based on the design of the model. It is unlikely that human memory employs RAM-like behaviors as in NTM, DNC, and RMC. Fixed-size memory is inevitable for online and life-long learning, which also reflects biological plausibility. Relational extraction and recurrent dynamics are often required in powerful models. As shown in the table, our proposed model exhibits all the nice features that a memory model should have.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Samples of geometry and graph tasks. From top to bottom: Convex hull, TSP, Shortest path and Minimum spanning tree. Blue denotes the ground-truth solution.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Learning curves on geometry and graph tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">run-1</th>
<th style="text-align: center;">run-2</th>
<th style="text-align: center;">run-3</th>
<th style="text-align: center;">run-4</th>
<th style="text-align: center;">run-5</th>
<th style="text-align: center;">run-6</th>
<th style="text-align: center;">run-7</th>
<th style="text-align: center;">run-8</th>
<th style="text-align: center;">run-9</th>
<th style="text-align: center;">run-10</th>
<th style="text-align: center;">Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.21 \pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">$2.13 \pm 1.14$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$0.57 \pm 0.18$</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$0.81 \pm 0.27$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">$0.12 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.03 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.01 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.04 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.01 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$0.16 \pm 0.17$</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$0.29 \pm 0.15$</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">$1.18 \pm 1.07$</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.27 \pm 0.28$</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">$2.06 \pm 2.79$</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">$0.39 \pm 0.18$</td>
</tr>
<tr>
<td style="text-align: center;">Failed task $(&gt;5 \%)$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.10 \pm 0.30$</td>
</tr>
</tbody>
</table>
<p>Table 9. Results from 10 runs of STM on bAbI 10k. Bold denotes best run.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Fixed-size <br> memory</th>
<th style="text-align: center;">Relational <br> extraction</th>
<th style="text-align: center;">Recurrent <br> dynamics</th>
<th style="text-align: center;">Biologically <br> plausible</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RNN, LSTM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">NTM, DNC</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">RMC</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">UT</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Attentional LSTM</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">STM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 10. Characteristics of some neural memory models</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The input is sequence of triplets, which is equivalent to sequence of edges. Hence, the complexity is based on the number of edges in the graph.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5} \mathrm{https}: / /$ github.com/L0SG/relational-rnn-pytorch&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
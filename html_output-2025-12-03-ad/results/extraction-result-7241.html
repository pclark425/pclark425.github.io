<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7241 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7241</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7241</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-278310435</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.00776v1.pdf" target="_blank">Reasoning Capabilities and Invariability of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7241.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7241.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's large closed-source conversational transformer model evaluated on the paper's geometric-figures logical reasoning benchmark; reported as the top-performing model in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer-based conversational model from OpenAI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A domain-free shallow logical reasoning task using pairs of premises about geometric figures (binary yes/no answers). Four linguistic variants (Q1–Q4) test invariability; questions are designed following cognitive-psychology standards.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 72.30%; Few-shot ALL accuracy 62.50%; CoT-before ALL accuracy 74.92%; CoT-after ALL accuracy 72.92%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary reported), few-shot and chain-of-thought (before/after) also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report 95% CIs; GPT-4 overall 72.30% (zero-shot). They note GPT-4 was the most accurate, but its difference with Qwen72B, Meta-Llama-3-70B, Phi-small, and recurrentgemma-2b is not statistically significant (CIs intersect).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Dataset baseline defined as a model that always answers 'no' (accuracy 55/108 ≈ 50.93%). No human baseline performance is provided in the paper. CoT prompting produced higher ALL accuracy when the rationale was requested before the answer (CoT-before) for GPT-4; generally CoT-after also increased performance for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7241.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 1.5 (72B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large Qwen-series model (72B parameters) evaluated on the geometric reasoning benchmark and among top-performing models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen1.5-72B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based multilingual/chat-optimized model in the Qwen series.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Shallow deductive reasoning with geometric-figure premises, binary yes/no answers; four linguistic variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 70.60%; Few-shot ALL accuracy 59.03%; CoT not evaluated for this model due to resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors state Qwen72B's CI intersects with GPT-4 and other top models, so differences are not statistically significant versus the top performer; only a few models are statistically distinguishable from baseline overall.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>One of the top-performing open models on this benchmark; CoT experiments excluded by resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7241.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3 (70B) Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's Llama-3 70B instruct-tuned model evaluated and among the better-performing models on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruct-tuned transformer model from Meta (Llama-3 family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary shallow logical reasoning tasks using geometric-figure premises; four batches test invariability and negation handling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 67.75%; Few-shot ALL accuracy 59.41%; CoT not evaluated for the 70B model (resource constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors note Meta-Llama-3-70B's CI intersects with GPT-4 and other top models (not statistically different from GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports overall behavior across batches is largely invariant; Llama-3-70B was among a small set of models outperforming baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7241.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-3-small</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-3 small (7B) 128k-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Phi-3 family small model (7B) reported to have comparatively strong performance on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-3-small-128k-instruct (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Phi-3 family transformer model optimized for long-context instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Domain-free geometric-figure reasoning questions requiring shallow deductions; answers constrained to yes/no.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 67.36%; Few-shot ALL accuracy 61.19%; CoT-before ALL accuracy 53.78%; CoT-after ALL accuracy 68.83%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Phi-small listed among the small set of models statistically distinguishable from baseline; however authors note its CI intersects with GPT-4 and other top models, so some pairwise differences are not significant.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>CoT-after substantially improved reported ALL accuracy vs CoT-before for this model. Overall, authors report most models are indistinguishable from baseline except a few (including Phi-small).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7241.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>recurrentGemma-2B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>recurrentgemma (2B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RecurrentGemma model (2B) from the Gemma series; reported among the better open models on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>recurrentgemma-2b-it</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RecurrentGemma family model, transformer with recurrent components, pretrained on diverse sources.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Simple logical deductions over geometric-figure properties; binary yes/no output required.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 67.98%; Few-shot ALL accuracy 58.49%; CoT-before ALL accuracy 48.38%; CoT-after ALL accuracy 66.74%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>recurrentgemma-2b listed among models whose CI intersects with GPT-4 (not statistically different from top performer); overall most models indistinguishable from baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>CoT-after improves performance substantially compared to CoT-before. Authors emphasise many models produced 'void' answers in some CoT conditions (not following binary answer constraint), affecting measured accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7241.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StableLM-2-1.6B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StableLM 2 (1.6B) chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>StableLM-2 small chat model; reported lower accuracy and a high rate of 'void' (non-boolean) answers in some prompting settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>stablelm-2-1.6b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>StableLM series open model (small size) trained on multilingual/code corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.6B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary shallow logical reasoning tasks with geometric-figure descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 38.35% (notably below the random/baseline rate due to many void answers); Few-shot ALL accuracy 53.32%; CoT-before ALL accuracy 22.45%; CoT-after ALL accuracy 58.10%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors flag StableLM models as well below random in zero-shot because ~30% of responses were 'void' (not following yes/no instruction) and these were marked incorrect; few-shot mitigated void answers.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Around 30% of StableLM responses were considered void and marked incorrect in some settings; few-shot reduced void answers and improved measured accuracy. No human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7241.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5 model included for comparison; performance near baseline in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-3.5 series transformer conversational model (closed-source variant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Shallow logical reasoning over geometric figures; binary yes/no answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 50.85% (≈ baseline); Few-shot ALL accuracy 53.01%; CoT-before ALL accuracy 45.83%; CoT-after ALL accuracy 48.38%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Performance is close to the paper's baseline (always-'no' 50.93%); authors indicate most models (including GPT-3.5) are statistically indistinguishable from baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-3.5 often produced accuracies near chance on this dataset. CoT variations changed measured performance, sometimes worsening it when rationale requested before the answer (increase in void answers).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7241.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (13B) chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta Llama-2 13B chat model evaluated; performance modest and near baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13-billion-parameter chat-optimized Llama-2 family model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary reasoning with simple logical premises on geometric figures; four dataset variants to test invariability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 56.64%; Few-shot ALL accuracy 52.47%; CoT-before ALL accuracy 28.24%; CoT-after ALL accuracy 51.23%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report many models are statistically indistinguishable from baseline; Llama-2-13B not among the few that are clearly better than baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>CoT-before severely reduced measured accuracy for this model, largely due to void-answer behavior; CoT-after recovered performance in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7241.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (7B) chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta Llama-2 7B chat model evaluated; modest performance near baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7-billion-parameter chat-optimized Llama-2 family model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Simple deductive yes/no questions about geometric figures, designed to avoid real-world priors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 53.86%; Few-shot ALL accuracy 49.23%; CoT-before ALL accuracy 21.75%; CoT-after ALL accuracy 61.19%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall near baseline; CoT-before produced many void answers and severe drop in measured accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors emphasise void-answer problem in CoT-before for many models including Llama-2-7B; CoT-after often restored or improved measured accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7241.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-3-medium</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-3 medium (14B) 128k-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Medium-sized Phi-3 model evaluated on the benchmark with relatively high zero-shot accuracy among the tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-3-medium-128k-instruct (14B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Phi-3 family transformer model (medium, 14B) optimized for instruction following and long context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary logical reasoning questions using geometric figures; dataset designed to be domain-free.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 65.97%; Few-shot ALL accuracy 52.93%; CoT-before ALL accuracy 57.64%; CoT-after ALL accuracy 64.12%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Phi-3-medium showed higher zero-shot accuracy and some inter-batch differences (e.g., better on Q4 than Q3); not explicitly called out as uniquely significant versus GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors observed inter-batch performance variability for some models; overall many models including Phi-3-medium remain far from perfect and lack human baseline comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7241.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7241.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 1.5 (32B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qwen-series 32B model evaluated; showed some batch-specific performance differences (better on Q4).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen1.5-32B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mid-large transformer chat model in the Qwen family.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Shallow logical deductions over geometric figures, with binary answers and four dataset variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot ALL accuracy 55.32%; Few-shot ALL accuracy 52.24%; CoT-before ALL accuracy 59.34%; CoT-after ALL accuracy 54.32%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (primary), few-shot, CoT-before/after</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors note Qwen32B had some inter-batch differences (better at Q4). Overall many differences across models were not statistically significant versus baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performance close to baseline; authors used McNemar tests and report no evidence of significant answer changes across the four batches for most models (p > 0.1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning Capabilities and Invariability of Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Diagnosing the first-order logical reasoning ability through LogicNLI. <em>(Rating: 2)</em></li>
                <li>LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Are emergent abilities of large language models a mirage? <em>(Rating: 1)</em></li>
                <li>On the paradox of learning to reason from data. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7241",
    "paper_id": "paper-278310435",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's large closed-source conversational transformer model evaluated on the paper's geometric-figures logical reasoning benchmark; reported as the top-performing model in these experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source transformer-based conversational model from OpenAI.",
            "model_size": "not reported (closed-source)",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "A domain-free shallow logical reasoning task using pairs of premises about geometric figures (binary yes/no answers). Four linguistic variants (Q1–Q4) test invariability; questions are designed following cognitive-psychology standards.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 72.30%; Few-shot ALL accuracy 62.50%; CoT-before ALL accuracy 74.92%; CoT-after ALL accuracy 72.92%",
            "prompting_method": "zero-shot (primary reported), few-shot and chain-of-thought (before/after) also evaluated",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Authors report 95% CIs; GPT-4 overall 72.30% (zero-shot). They note GPT-4 was the most accurate, but its difference with Qwen72B, Meta-Llama-3-70B, Phi-small, and recurrentgemma-2b is not statistically significant (CIs intersect).",
            "notes": "Dataset baseline defined as a model that always answers 'no' (accuracy 55/108 ≈ 50.93%). No human baseline performance is provided in the paper. CoT prompting produced higher ALL accuracy when the rationale was requested before the answer (CoT-before) for GPT-4; generally CoT-after also increased performance for many models.",
            "uuid": "e7241.0",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Qwen-72B",
            "name_full": "Qwen 1.5 (72B)",
            "brief_description": "Large Qwen-series model (72B parameters) evaluated on the geometric reasoning benchmark and among top-performing models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen1.5-72B-Chat",
            "model_description": "Large transformer-based multilingual/chat-optimized model in the Qwen series.",
            "model_size": "72B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Shallow deductive reasoning with geometric-figure premises, binary yes/no answers; four linguistic variants.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 70.60%; Few-shot ALL accuracy 59.03%; CoT not evaluated for this model due to resource constraints.",
            "prompting_method": "zero-shot (primary), few-shot",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Authors state Qwen72B's CI intersects with GPT-4 and other top models, so differences are not statistically significant versus the top performer; only a few models are statistically distinguishable from baseline overall.",
            "notes": "One of the top-performing open models on this benchmark; CoT experiments excluded by resource constraints.",
            "uuid": "e7241.1",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Meta-Llama-3-70B",
            "name_full": "Meta Llama-3 (70B) Instruct",
            "brief_description": "Meta's Llama-3 70B instruct-tuned model evaluated and among the better-performing models on the benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-70B-Instruct",
            "model_description": "Large instruct-tuned transformer model from Meta (Llama-3 family).",
            "model_size": "70B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Binary shallow logical reasoning tasks using geometric-figure premises; four batches test invariability and negation handling.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 67.75%; Few-shot ALL accuracy 59.41%; CoT not evaluated for the 70B model (resource constraints).",
            "prompting_method": "zero-shot (primary), few-shot",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Authors note Meta-Llama-3-70B's CI intersects with GPT-4 and other top models (not statistically different from GPT-4).",
            "notes": "Paper reports overall behavior across batches is largely invariant; Llama-3-70B was among a small set of models outperforming baseline.",
            "uuid": "e7241.2",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Phi-3-small",
            "name_full": "Phi-3 small (7B) 128k-instruct",
            "brief_description": "Phi-3 family small model (7B) reported to have comparatively strong performance on the benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Phi-3-small-128k-instruct (7B)",
            "model_description": "Phi-3 family transformer model optimized for long-context instruction following.",
            "model_size": "7B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Domain-free geometric-figure reasoning questions requiring shallow deductions; answers constrained to yes/no.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 67.36%; Few-shot ALL accuracy 61.19%; CoT-before ALL accuracy 53.78%; CoT-after ALL accuracy 68.83%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Phi-small listed among the small set of models statistically distinguishable from baseline; however authors note its CI intersects with GPT-4 and other top models, so some pairwise differences are not significant.",
            "notes": "CoT-after substantially improved reported ALL accuracy vs CoT-before for this model. Overall, authors report most models are indistinguishable from baseline except a few (including Phi-small).",
            "uuid": "e7241.3",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "recurrentGemma-2B",
            "name_full": "recurrentgemma (2B)",
            "brief_description": "RecurrentGemma model (2B) from the Gemma series; reported among the better open models on the benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "recurrentgemma-2b-it",
            "model_description": "RecurrentGemma family model, transformer with recurrent components, pretrained on diverse sources.",
            "model_size": "2B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Simple logical deductions over geometric-figure properties; binary yes/no output required.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 67.98%; Few-shot ALL accuracy 58.49%; CoT-before ALL accuracy 48.38%; CoT-after ALL accuracy 66.74%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "recurrentgemma-2b listed among models whose CI intersects with GPT-4 (not statistically different from top performer); overall most models indistinguishable from baseline.",
            "notes": "CoT-after improves performance substantially compared to CoT-before. Authors emphasise many models produced 'void' answers in some CoT conditions (not following binary answer constraint), affecting measured accuracy.",
            "uuid": "e7241.4",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "StableLM-2-1.6B-chat",
            "name_full": "StableLM 2 (1.6B) chat",
            "brief_description": "StableLM-2 small chat model; reported lower accuracy and a high rate of 'void' (non-boolean) answers in some prompting settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "stablelm-2-1.6b-chat",
            "model_description": "StableLM series open model (small size) trained on multilingual/code corpora.",
            "model_size": "1.6B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Binary shallow logical reasoning tasks with geometric-figure descriptions.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 38.35% (notably below the random/baseline rate due to many void answers); Few-shot ALL accuracy 53.32%; CoT-before ALL accuracy 22.45%; CoT-after ALL accuracy 58.10%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Authors flag StableLM models as well below random in zero-shot because ~30% of responses were 'void' (not following yes/no instruction) and these were marked incorrect; few-shot mitigated void answers.",
            "notes": "Around 30% of StableLM responses were considered void and marked incorrect in some settings; few-shot reduced void answers and improved measured accuracy. No human baseline provided.",
            "uuid": "e7241.5",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo-0613",
            "brief_description": "OpenAI GPT-3.5 model included for comparison; performance near baseline in these tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613",
            "model_description": "OpenAI's GPT-3.5 series transformer conversational model (closed-source variant).",
            "model_size": "not reported (closed-source)",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Shallow logical reasoning over geometric figures; binary yes/no answers.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 50.85% (≈ baseline); Few-shot ALL accuracy 53.01%; CoT-before ALL accuracy 45.83%; CoT-after ALL accuracy 48.38%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Performance is close to the paper's baseline (always-'no' 50.93%); authors indicate most models (including GPT-3.5) are statistically indistinguishable from baseline.",
            "notes": "GPT-3.5 often produced accuracies near chance on this dataset. CoT variations changed measured performance, sometimes worsening it when rationale requested before the answer (increase in void answers).",
            "uuid": "e7241.6",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama-2-13B",
            "name_full": "Llama 2 (13B) chat-hf",
            "brief_description": "Meta Llama-2 13B chat model evaluated; performance modest and near baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13b-chat-hf",
            "model_description": "13-billion-parameter chat-optimized Llama-2 family model.",
            "model_size": "13B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Binary reasoning with simple logical premises on geometric figures; four dataset variants to test invariability.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 56.64%; Few-shot ALL accuracy 52.47%; CoT-before ALL accuracy 28.24%; CoT-after ALL accuracy 51.23%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Authors report many models are statistically indistinguishable from baseline; Llama-2-13B not among the few that are clearly better than baseline.",
            "notes": "CoT-before severely reduced measured accuracy for this model, largely due to void-answer behavior; CoT-after recovered performance in some cases.",
            "uuid": "e7241.7",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama 2 (7B) chat-hf",
            "brief_description": "Meta Llama-2 7B chat model evaluated; modest performance near baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf",
            "model_description": "7-billion-parameter chat-optimized Llama-2 family model.",
            "model_size": "7B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Simple deductive yes/no questions about geometric figures, designed to avoid real-world priors.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 53.86%; Few-shot ALL accuracy 49.23%; CoT-before ALL accuracy 21.75%; CoT-after ALL accuracy 61.19%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Overall near baseline; CoT-before produced many void answers and severe drop in measured accuracy.",
            "notes": "Authors emphasise void-answer problem in CoT-before for many models including Llama-2-7B; CoT-after often restored or improved measured accuracy.",
            "uuid": "e7241.8",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Phi-3-medium",
            "name_full": "Phi-3 medium (14B) 128k-instruct",
            "brief_description": "Medium-sized Phi-3 model evaluated on the benchmark with relatively high zero-shot accuracy among the tested models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Phi-3-medium-128k-instruct (14B)",
            "model_description": "Phi-3 family transformer model (medium, 14B) optimized for instruction following and long context.",
            "model_size": "14B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Binary logical reasoning questions using geometric figures; dataset designed to be domain-free.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 65.97%; Few-shot ALL accuracy 52.93%; CoT-before ALL accuracy 57.64%; CoT-after ALL accuracy 64.12%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Phi-3-medium showed higher zero-shot accuracy and some inter-batch differences (e.g., better on Q4 than Q3); not explicitly called out as uniquely significant versus GPT-4.",
            "notes": "Authors observed inter-batch performance variability for some models; overall many models including Phi-3-medium remain far from perfect and lack human baseline comparisons in the paper.",
            "uuid": "e7241.9",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Qwen-32B",
            "name_full": "Qwen 1.5 (32B)",
            "brief_description": "Qwen-series 32B model evaluated; showed some batch-specific performance differences (better on Q4).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen1.5-32B-Chat",
            "model_description": "Mid-large transformer chat model in the Qwen family.",
            "model_size": "32B",
            "test_name": "ReasoningLLMs geometric-figures logical reasoning benchmark (Q1–Q4)",
            "test_category": "reasoning",
            "test_description": "Shallow logical deductions over geometric figures, with binary answers and four dataset variants.",
            "evaluation_metric": "accuracy (%)",
            "human_performance": null,
            "llm_performance": "Zero-shot ALL accuracy 55.32%; Few-shot ALL accuracy 52.24%; CoT-before ALL accuracy 59.34%; CoT-after ALL accuracy 54.32%",
            "prompting_method": "zero-shot (primary), few-shot, CoT-before/after",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": "Authors note Qwen32B had some inter-batch differences (better at Q4). Overall many differences across models were not statistically significant versus baseline.",
            "notes": "Performance close to baseline; authors used McNemar tests and report no evidence of significant answer changes across the four batches for most models (p &gt; 0.1).",
            "uuid": "e7241.10",
            "source_info": {
                "paper_title": "Reasoning Capabilities and Invariability of Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Diagnosing the first-order logical reasoning ability through LogicNLI.",
            "rating": 2,
            "sanitized_title": "diagnosing_the_firstorder_logical_reasoning_ability_through_logicnli"
        },
        {
            "paper_title": "LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding.",
            "rating": 2,
            "sanitized_title": "logiqa_20an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Are emergent abilities of large language models a mirage?",
            "rating": 1,
            "sanitized_title": "are_emergent_abilities_of_large_language_models_a_mirage"
        },
        {
            "paper_title": "On the paradox of learning to reason from data.",
            "rating": 1,
            "sanitized_title": "on_the_paradox_of_learning_to_reason_from_data"
        }
    ],
    "cost": 0.01813025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reasoning Capabilities and Invariability of Large Language Models
1 May 2025</p>
<p>Alessandro Raganato alessandro.raganato@unimib.it 
Department of Informatics, Systems, and Communication (DISCo)
University of Milano-Bicocca
20126MilanItaly</p>
<p>Rafael Peñaloza rafael.penalozanyssen@unimib.it 
Department of Informatics, Systems, and Communication (DISCo)
University of Milano-Bicocca
20126MilanItaly</p>
<p>Marco Viviani marco.viviani@unimib.it 
Department of Informatics, Systems, and Communication (DISCo)
University of Milano-Bicocca
20126MilanItaly</p>
<p>Gabriella Pasi gabriella.pasi@unimib.it 
Department of Informatics, Systems, and Communication (DISCo)
University of Milano-Bicocca
20126MilanItaly</p>
<p>Reasoning Capabilities and Invariability of Large Language Models
1 May 20250A7124605F6F7C8181DAF5C3096E828AarXiv:2505.00776v1[cs.CL]Natural Language ProcessingKnowledge Representation and ReasoningLLM benchmarkGenerative AI
Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned.In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency.In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning.Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction.An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement.An additional test with chainof-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.</p>
<p>I. INTRODUCTION</p>
<p>Large Language Models (LLMs) are constituting a real revolution in the field of Natural Language Processing (NLP), since they have been shown to perform particularly well while generating and manipulating natural language, mainly thanks to the introduction of the Transformer architecture [1] underlying most of them.Indeed, text generated with these tools can simulate human text and reactions to the point that some researchers have stated that LLMs showcase emergent abilities [2], 1 and some industry representatives tout them as gateways to Artificial General Intelligence (AGI) [5].Among the tasks relying on these abilities for which LLMs have been successfully used (translation, summarization, sentiment analysis, and many others), plausibly answering different types of questions has garnered particular attention due to its potential to revolutionize how we interact with information, and obtain plausible answers to various questions, from simple factual questions to complex analytical ones.However, most current LLMs are not designed to be factually correct or logically consistent; indeed, they are models of language, not of reality [6].For this reason, their ability to deal with reasoning tasks has often been questioned [7] and different tests have been developed, which mainly focus on commonsense [8], physical, or complex logical reasoning on the one hand, and propositional reasoning on the other [9]- [11].Attempts have been made to solve this limited reasoning capabilities through scaling, fine-tuning, or retraining [12], [13], but these attempts encounter the same fundamental issues eventually.</p>
<p>Logical reasoning is a fundamental feature for advanced AI applications, where guarantees about the quality of the results and avoidance of so-called hallucinations [14]-where the model generates results which are not grounded on realityare necessary.It is thus important to understand the situations in which reasoning fails, and analyse whether those cases can be effectively handled through other means.</p>
<p>Existing datasets for testing reasoning capabilities in LLMs suffer from two major drawbacks.The first one is that most logical questions (and their answers) are based on real-world situations, which the LLM might have internalised through other means; e.g., "all men are mortal" could be just a learned phrase, disguised as a correct logical deduction from, e.g., the premises "all men are animals" and "all animals are mortal."This issue is well known from the study of human reasoning capabilities in the cognitive sciences.To avoid such correct-for-the-wrong-reason solutions, logical reasoning tests on humans usually consider domain-free questions using, for example, relationships between geometrical figures [15].This requires a delicate balance, since excessive abstraction may add unnecessary confusion to the subjects, but excessive instantiation may lead to familiar scenarios.</p>
<p>The second issue is that the quality of the answer given by an LLM strongly depends on the shape of the question itself; i.e., two semantically equivalent questions may elicit very different answers.This property is so well-known that it has led to the rise of prompt engineering [16]; i.e., the process of designing effective prompts to guide LLMs in generating desired outputs or completing specific tasks.Yet, with respect to this last task, if a specific prompt is chosen for a test, it is unclear whether the results can be extrapolated to other (semantically equivalent) questions, LLMs, or scenarios.This is of particular importance in the context of logical reasoning, as the soundness of a method should be independent of the syntactic expression chosen.In particular, models should not be affected by small stylistic and structural variations on natural language text written by users.For instance, the answer to the previous example should not change if the second premise was substituted by the (semantically) equivalent "immortal beings cannot be animals."</p>
<p>To understand the capability of LLMs to reason logically, we curated a new dataset focusing specifically on logical constructors (existential restrictions, negations, and simple quantification) often encountered in standard knowledge representation languages [17], and used in AI applications.The reasoning required in our dataset is very shallow: one conclusion is asked from two premises.In particular, the answer does not depend on case analyses, keeping intermediate results in memory, or other potential complex steps.Our goal with these tests is to try to understand which kinds of statements are viable; i.e., can be handled correctly by different LLMs.The idea is that if we understand the viable cases, we can ensure that some answers are correct, and conversely, we can search for different solutions for non-viable cases.While we could have easily chosen complex instances requiring deep reasoning elements to showcase the failures of LLMs, our goal is to understand the performance on basic reasoning steps, and hence analyse shallow reasoning only.</p>
<p>Understanding viable cases only makes sense if the LLM behaviour remains invariable (that is, stable) under small language variations and explanations.Otherwise, it is impossible to forecast the accuracy of a model as it reads different expressions of the same knowledge.Hence, we propose four variants of the dataset designed to verify this invariability, where only additional information, a new naming convention, or a simple rewriting is used.The differences in answering behaviour between the four variants are measured through a statistical test designed exactly for this purpose.</p>
<p>We empirically verified the accuracy of 24 different LLMs over our dataset through zero-shot and few-shot prompting.The results, in short, are the following.Only a few models outperform the others, but their accuracy remains at 70%.Most other models are indistinguishable from the baseline.This means that LLMs cannot yet reliably answer shallow reasoning problems.The performance decreased in the few-shot scenario.The intra-and inter-model McNemar tests confirm that, in general, the behaviour of all LLMs, in all variations of the questions, is invariant (with only a few exceptions).These results show that our dataset presents a challenge for LLMs of all sizes, and that a specific class of logical questions is hard for LLMs in general.Through an additional test with chain-ofthought prompting [18] over 22 of the 24 original models, we observe that the performance of the models strongly depends on whether the rationale is output before or after the answer.In the latter case, the performance increases in general, although not to a significant degree.</p>
<p>After a brief description of the work most closely related to ours, in Section III we describe the general modelling of our datasets and explain the choices made when generating it.Section IV then presents the results of the experimental evaluation and statistical analysis, before concluding with some perspectives on future work.</p>
<p>II. RELATED WORK</p>
<p>Since their inception, LLMs have been analyzed for their capabilities in addressing different tasks; for an extensive survey covering LLM evaluations across a broad spectrum of tasks, we refer the interested reader to [19].In this paper, our primary focus lies on logical reasoning and its associated tasks.Hence, we present work related to the evaluation of this task only.</p>
<p>From the point of view of knowledge representation and reasoning [20] (which is based on logic), most existing evaluations focus on the capacity of LLMs to provide (correct) information, which is missing in an existing knowledge base.A recent example considering the Semantic Web is [21], but other evaluations have been made in the past, focusing also on the accuracy and veracity of the provided answers [22], [23].These approaches do not perform reasoning, but extract formal representations from natural language texts.</p>
<p>In order to evaluate the logical reasoning capabilities of LLMs, suitable datasets in which different kinds of reasoning steps are tested through textual descriptions, are needed.Various datasets for this purpose and their associated evaluations have been presented in the literature and enumerating them all is beyond the scope of this paper; however, some notable examples are LogicNLI [9], LogiQA 2.0 [10], and LSAT [11].Perhaps the most comprehensive datasets and evaluations available to date are provided in [24], where different variations of logical reasoning tasks and LLMs of different sizes are evaluated on different aspects going beyond accuracy; and in [25], which systematically evaluates logical reasoning in LLMs.</p>
<p>Very recently, also the capacity of LLMs to perform causal reasoning has been analysed, with comparable negative results [26], [27].Specifically, these works show that the causal inferences made by LLMs tend to be erroneous.</p>
<p>The main conclusion obtained from all these evaluations is that LLMs are not adequate substitutes for logical reasoning, and cannot serve as reasoning engines.Moreover, their performance tends to degrade after so-called self-correction [28].This is not surprising, since LLMs are constructed to find patterns in language, which are not necessarily explicit in reasoning.Moreover, logical reasoning is domain agnostic, while in language processing the meaning of words is of fundamental importance.These theoretical insights were empirically verified in [29], and have motivated other approaches, where the logical reasoning methods are internalised into the LLM [30].However, what many of the datasets and evaluations available in the literature fail to consider is the specific expressiveness inherent in the various languages utilized for knowledge representation and reasoning.Indeed, the many existing families of representation languages (and the reasons behind their existence) are often overlooked.Logical connectives that are used in many applications are underrepresented in most datasets, which focus on either simple propositional reasoning, or the combination of very complex statements, while the reality is that most knowledge-aware applications fall in between these two extremes.</p>
<p>To fill this gap, we present a new dataset and evaluations that assess the capabilities of LLMs to deal with concepts, disjointness, existential and universal constraints, and simple counting without the need for complex reasoning.Indeed, the reasoning required to answer the questions in our dataset is very shallow and avoids memorization or case analyses.</p>
<p>Interestingly, to the best of our knowledge, there is no work which methodologically analyses how many answers from LLMs are affected by small language variations.Although it is well known that the quality of the answers may depend on the prompt (and hence the importance of prompt engineering) [16], if the LLM is being used to analyze knowledge readily available in textual form (e.g., from textbooks or other reputable sources) the option of rewriting this text is unavailable; the LLM must work with what is presented to it.Importantly, one cannot assume that natural language texts are written with LLMs (let alone with specific implementations of LLMs) in mind; they are rather targeted for human consumption, and stylistic changes and rephrasing are expected to ease the comprehension by human readers.For that reason, we expect that the quality of the result does not depend on the stylistic choices of the author.To verify whether this is the case, we introduce four different variants of the dataset as explained next.</p>
<p>III. MODELLING</p>
<p>As readily mentioned, the aim of this work is to evaluate the capabilities of LLMs to perform logical reasoning with existential, value, and number restrictions, along with an application of negation.Here we present a new dataset composed of simple logical questions, and explain how it was tested on LLMs for the considered task.Following the standards from cognitive psychology [15], our questions are bounded to a simple context free domain, where two (or at most three in a few cases) geometric figures with varying properties are considered.The idea is that the answers should not depend on some previously acquired intuition about the "world" but rather on the specific situation being described-the scenario.Hence, the tests evaluate the actual reasoning capabilities without confounding them with previous observations available in the training data.</p>
<p>A. Reasoning Questions</p>
<p>In their original form, which is the base of our main dataset, the questions range from simple propositional derivations like:</p>
<p>If every triangle is green, is it certain that every green figure is a triangle?(where the correct answer is no) to more complex statements using existential and value restrictions and counting; for example:</p>
<p>If every triangle has a green side; there is a triangle with a blue side; and there is a triangle with a red side.Can there be a triangle with three green sides?and If every triangle has a green side; every triangle has a blue side; and every triangle has a red side.Can there be a triangle with three green sides?whose answers are yes and no, respectively.</p>
<p>B. Dataset Organization</p>
<p>The dataset is divided into 4 batches, denoted here as Q1, Q2, Q3, and Q4, respectively.Each batch contains 108 questions, presented with variations and additional information unique to each batch. 2 All questions are accompanied by a context, set at the beginning:</p>
<p>Consider the following knowledge of a class of geometric figures:</p>
<p>and request a binary answer by appending at the end of the sentence the following prompt:</p>
<p>Answer with yes or no only.</p>
<p>These requests are made directly to the LLM, although it is impossible to guarantee that they were satisfied with all answers; once again, the behaviour of LLMs in some instances is unpredictable (see Section V).</p>
<p>The second batch reproduces the same 108 questions, but adds more specific information about the geometric figures when needed.Specifically, it expresses that: every triangle has exactly three sides; every square has exactly four sides;</p>
<p>...</p>
<p>every ... has exactly ... sides.</p>
<p>The idea is to see whether this commonsense information (the number of sides in specific geometric figures, which is not explicitly stated in the first batch) is helpful for the model, and hence use it to rewrite prompts (based on background knowledge) to obtain a better behavior whenever possible.In a nutshell, we are extending the original prompt to make implicit knowledge explicit.The main point when considering this batch is to understand whether the potential wrong answers found from the first batch were caused by a lack of understanding of the geometrical figures or by some other reason.</p>
<p>The third batch goes in the opposite direction regarding the use of known words, their meaning, and other knowledge about them.The same 108 questions from the second batch are used, but the common names for geometric figures (e.g., triangle or square) are replaced by invented words (in the dataset, we use the words, marmal and wusp, respectively), accompanied by an explanation of their meaning similar to the one provided in the second batch.An example full question on this batch (including the preamble and the request for a binary answer) is:</p>
<p>Consider the following knowledge of a class of geometric figures: every marmal has three sides; every marmal has a green side.Is it certain that there is a marmal with three red sides?Answer with yes or no only.</p>
<p>As this example shows, an important aspect of the requested answer is certainty, which is fundamental in logical reasoning.Indeed, the difference between "is it certain that . . ." and "can there be a . . ." is of utmost importance in critical applications.We expect this third batch to be harder for LLMs, at least by the appearance of an unseen word, which is unlikely to appear in its training set, but is fundamental in the application of LLMs to logical reasoning under background knowledge.Indeed, knowledge is dynamic, and new terms are coined, or old terms get new meanings in novel contexts.The challenge for advanced AI applications is to make adequate use of new terminology, based on the definitions provided.Part of our evaluation is focused on understanding whether performance degrades significantly in this situation.Our empirical evaluation shows that this is not the case: while the accuracy tends to decrease for this batch, the difference is not statistically significant in general.</p>
<p>The fourth batch of questions diverges slightly from the previous pattern.It is mostly equivalent to the first batch, except that the final question is negated and as consequence, the answer is flipped.For example, if the question in the first batch is:</p>
<p>If every triangle is green, is it certain that every green figure is a triangle?whose answer is "no", then the fourth batch rewrites it as:</p>
<p>If every triangle is green, can there be a green figure which is not a triangle?whose answer is, in this case, "yes".This last batch often requires a negation in the question.As negations are one of the fundamental modifiers with which LLMs are known to struggle [31], [32], we expect the results on this batch to degrade in relation to the first one.</p>
<p>C. Few-Shot Prompting</p>
<p>To assess models in a few-shot setting, we created six new questions to provide additional context for the LLMs, to respond more accurately to each test question.In the few-shot experiment, this context was prefixed to each test question.An example of a full prompt in this experiment is: More precisely, the prompt provided for the few-shot learning experiments includes six examples of question-answer pairs that have the same pattern as the inputs of the questions, but which will not be given at the test.The six examples given are balanced: three of the questions receive answer "yes", and three the answer "no".
You</p>
<p>D. Chain-of-Thought (CoT) Prompting</p>
<p>Chain-of-thought (CoT) prompts are designed to obtain more detailed responses from LLMs by instructing them to break down their answers [18].Following this idea, we assess whether generating a rationale would improve the ability to perform the required reasoning.To this end, we formulated two types of prompts added after each question:</p>
<p>(i) CoT before , asking the model to provide the rationale prior</p>
<p>to the final answer:</p>
<p>Provide the rationale before answering, and then give the answer with yes or no only.</p>
<p>(ii) CoT after , requesting the rationale following the answer:</p>
<p>Answer with yes or no only.Then provide the rationale of the answer.</p>
<p>IV. EXPERIMENTAL SETUP</p>
<p>We ran our experiments on the four batches illustrated before in Section III-B (i.e., Q1, Q2, Q3 and Q4) by applying distinct prompting techniques (i.e., zero-shot, few-shot, and the two versions of chain-of-thought prompting) on each of the 108 questions in each batch.In particular, each prompt was tested on 24 LLMs of varying sizes and configurations, to provide a comprehensive analysis across different architectures and scales.Specifically, the models used in our evaluation are the following:</p>
<p>• Gemma Series [33]: This includes the gemma-1.1 models with 2B and 7B parameters, and the more recent recurrentgemma models [34] with 2B and 9B parameters, pretrained on 6T tokens of varying sources.</p>
<p>• StableLM Series [35]: This includes stablelm-2 models with 1.6B and 12B parameters, pretrained on 2T tokens of diverse multilingual and code datasets.</p>
<p>• Qwen Series [36]: This series ranges from 1.8B to 72B parameters, pretrained on 3T tokens of diverse texts and codes, optimized for multilingual chat applications.</p>
<p>• Phi Series [37]: This includes Phi-3 models with parameter sizes of 3.8B (mini), 7B (small), and 14B (medium), pretrained on 3.3T tokens of varying sources and designed for long-context instruction following.</p>
<p>• Yi Series [38]: This includes Yi-1.5 models with 6B, 9B, and 34B parameters, pretrained on a high-quality corpus of 500B tokens.</p>
<p>• Llama Series [39], [40]: This includes Llama-2 models with 7B and 13B parameters pretrained on 2T tokens of data from publicly available sources, and the more recent Meta-Llama-3 models with 8B and 70B parameters, pretrained on over 15T tokens of data from publicly available sources.</p>
<p>• GPT Series [41]: This includes OpenAI's GPT-3.5 and GPT-4 closed-source models, noted for their advanced conversational abilities.</p>
<p>A. Evaluation Settings</p>
<p>To evaluate their reasoning capabilities, we counted the proportion of correct answers by each model.As our questions are binary (i.e., either "yes" or "no"), we expect a model to succeed in at least 50% of the questions, even if just by chance.On the other hand, the question dataset is not fully balanced: 55 out of the 108 test questions have answered "no".For this reason, we compare the results against a baseline (first row of Tables I and II) from a model that always answers "no", with an accuracy of 55/108 ≈ 51%.The experiments were run thrice on each model with different seeds; the numbers show the mean accuracy.The ALL column provides the overall accuracy mean of the models over the concatenation of the four batches, together with its standard deviation (from the three runs).Considering answer correctness as a random event, we estimate the accuracy mean through a 95% Confidence Interval (CI) and evaluate the differences between models using standard statistical methods, through the normal approximation due to the law of large numbers [42].As detailed in Section III, the results in Table I refer to Zero-Shot and Few-Shot prompting.In the zero-shot setting, the model is asked to provide an answer without any additional examples.In the few-shot setting, the model is given a few examples along with the prompt (see Section III-C).</p>
<p>The results in Table II refer instead to CoT prompting, where the system is asked to provide a rationale for its answer before and after giving the answer, respectively.In this case, due to resource constraints, the two models with 70B parameters were not included in the evaluation.</p>
<p>V. RESULTS</p>
<p>Table I shows the average accuracy results separated by batch for both evaluation settings.</p>
<p>Concerning zero-shot prompting, the most accurate model is GPT-4 with an overall accuracy of 72.30%, but its difference with Qwen72B, Llama70B, Phi-small, recurrentgemma-2b are not statistically significant (their confidence intervals intersect).The other models are mostly statistically indistinguishable from the baseline.StableLM models are well below even random guesses, but the results are biased by the model not always providing a Boolean answer, and hence hiding potentially correct but voided answers. 3The interbatch analysis shows almost no significant difference, except for Phimedium being significantly better at batch Q4 than at batch Q3, Qwen32B being better at batch Q4 than all others, and Qwen72B being significantly worse at batch Q3 than all others.</p>
<p>Regarding few-shot prompting, the results follow the same previous trend, but models perform worse in general.The only exception is StableLM, where no void answers were now encountered, and the accuracy is accordingly improved.Recall that answers that did not start with "yes" or "no" were labelled as void are considered wrong.The only models statistically distinguishable from the baseline are GPT-4, Qwen72B, Llama70B, and Phi-small, all of which have intersecting CIs.Such intersection implies that their performance is not statistically distinguishable.The interbatch analysis provides the same outcomes of significant differences as in one-shot prompting.</p>
<p>The picture is more involved in CoT prompting.When the rationale was requested before the answer, GPT-4 was still the best performing model, but most other models showed very low accuracy (reaching 21% in StableLM and Llama models).This is in part explained by the surge of void answers, caused by the models not ending their response with a Boolean statement (specifically stablelm-2-1 6b-chat, Phi-3-mini-128k-instruct-3.8B,Llama-2-7b-chathf, Yi-1.5-9B-Chat,Phi-3-small-128k-instruct, and stablelm-2-12b-chat have around 20% of void answers).Yet even accounting for these void answers, the performance degraded greatly in this case.Interestingly, when the rationale is given after the Boolean answer, the performance of most models tended to increase (in most cases, significantly), even though the beginning of the output string is expected to be unchanged.</p>
<p>A. Invariability</p>
<p>Recall that the first three batches of test prompts contain exactly the same questions with very small variations of context and terminology, and the fourth batch only negates the question (and its answer) from the first one.Hence, if a model is invariable w.r.t.language modifications (i.e., remains stable under syntactic variations of the same semantic element), it should provide the same answers in each batch.Invariability is independent of accuracy or correctness: when a model is invariable, it should provide the same answer to the same question, regardless of whether the answer is correct or not.More precisely, when measuring invariability, our goal is to measure how often the answers of the models change depending on the prompt.</p>
<p>To find out whether a model is invariable in this sense, it is not enough to check the proportion of correct answers it provides on each batch (accuracy), but one must verify the differences in the answers: importantly, two models may have 50% accuracy each, and do not coincide in any answer; thus showcasing very variable behaviour.This means that we must analyse how often the answers coincide.We check the pairwise differences in behaviour of the same model between batches using a McNemar test [43], a statistical evaluation designed explicitly for this purpose.Very briefly, the test counts how unbalanced the discordants (the different answers) are in one direction or the other, hence quantifying the stability of answers.</p>
<p>We applied the McNemar test only to the pairs that were not already deemed as statistically different from the accuracy analysis; all other cases clearly yield variable answers.All the pair-wise statistics obtained from the McNemar test yield a pvalue over 0.1, which in a nutshell means that the differences in answers between batches (within any given model) are not statistically significant.We have thus no evidence to believe that the models behave differently for different batches of questions, or in our terms, we cannot say that the behaviour varies.This result is interesting, as it suggests that prompt engineering has limited impact in these cases, at least up to the point of the linguistic variants tested.</p>
<p>We also observed an invariability between models; that is, the answers from different models were also similar in general.This observation is important, because it implies that despite their architectural, size, and training differences, when it comes down to the logical questions in our dataset, all models are more or less equal (and none very proficient).This insight thus allows us to study ways of circumventing the problem for all models, without the need for ad-hoc solutions for each of them.This is a potential area for future research.</p>
<p>VI. CONCLUSIONS AND FUTURE WORK</p>
<p>In this article, we provided an in-depth analysis of the reasoning abilities of Large Language Models (LLMs), by presenting a new dataset of questions for logical reasoning which turned out to be challenging for 24 different LLMs in distinct prompt engineering settings.In our experiments, most LLMs behaved in a similar fashion in both a zero-shot and a few-shot setting, except that the performance of most models degraded for few-shot reasoning.Through a McNemar test, we also verified that the behaviour of the models is mainly invariant w.r.t.small language and stylistic variations (with only a few exceptions).The answers also tended to be invariant w.r.t. the chosen model.By understanding the cases where LLMs fail, we hope to circumvent the problem in future models, to be more robust and effective than fine-tuning or filtering.</p>
<p>This work points to promising research directions, especially in misinformation detection [44].With the overwhelming volume of online content, human fact-checking is inadequate.Pseudo-automatic knowledge-based systems can aid but struggle with the labor-intensive task of updating knowledge bases [45], [46].While LLMs could be beneficial, they often fail to effectively convert natural language into structured knowledge [47], [48].Therefore, this research area remains open for further exploration.</p>
<p>TABLE I SUMMARY
I
OF RESULTS ON 24 LLMS.COLUMNS SHOW AVERAGE ACCURACY OVER 3 RUNS DIVIDED BY BATCH AND GLOBALLY, AND THE WIDTH OF A 95% CI IN ZERO-SHOT AND FEW-SHOT TESTING.THE ALL COLUMN REPRESENTS THE CONCATENATION OF THE FOUR BATCHES Q1-Q4, AND INCLUDES THE STANDARD DEVIATION COMPUTED OVER THE THREE RUNS WITH DIFFERENT RANDOM SEEDS.
Zero-ShotFew-ShotModelQ1Q2Q3Q4ALL95% CIQ1Q2Q3Q4ALL95% CIbaseline50.9350.9350.9350.9350.93±050.9350.9350.9350.9350.93±0gemma-1.1-2b-it53.0955.8649.3852.4752.70 ± 0.13±2.7249.0749.0749.0749.0749.07 ± 0.00±2.72stablelm-2-1 6b-chat40.4343.2141.9827.7838.35 ± 0.27±2.6556.4851.5451.2354.0153.32 ± 1.27±2.72Qwen1.5-1.8B-Chat44.4445.0646.3034.8842.67 ± 1.86±2.7047.5349.3849.3847.8448.53 ± 0.27±2.72recurrentgemma-2b-it68.8366.0566.9870.0667.98 ± 0.27±2.4963.2758.9559.2652.4758.49 ± 0.71±2.72Phi-3-mini-128k-instruct67.9067.9061.4263.8965.28 ± 1.01±2.5953.7051.8548.1573.1556.71 ± 0.61±2.70Qwen1.5-4B-Chat59.2655.2555.5655.5656.40 ± 1.09±2.7057.4156.4857.7249.3855.25 ± 1.87±2.71Yi-1.5-6B-Chat51.5457.4146.3047.2250.62 ± 0.27±2.7254.9455.5654.3251.5454.09 ± 0.94±2.71Llama-2-7b-chat-hf52.7853.0954.3255.2553.86 ± 0.48±2.7149.0749.0749.0749.6949.23 ± 0.74±2.72Meta-Llama-3-8B-Instruct56.7957.4156.7966.9859.49 ± 0.69±2.6754.0153.0954.3269.7557.79 ± 0.94±2.69gemma-1.1-7b-it44.7547.8445.0649.6946.84 ± 0.48±2.7254.9455.2559.2653.0955.63 ± 0.74±2.70Qwen1.5-7B-Chat52.1651.8550.9363.8954.71 ± 0.35±2.7154.6354.6354.9465.1257.33 ± 0.94±2.69Yi-1.5-9B-Chat61.1160.4960.8054.6359.26 ± 0.61±2.6856.1755.2556.1753.7055.32 ± 0.46±2.71recurrentgemma-9b-it58.9558.9555.5658.3357.95 ± 0.71±2.6854.0156.7949.6962.0455.63 ± 0.13±2.64Phi-3-small-128k-instruct65.4368.5262.6572.8467.36 ± 0.80±2.5552.7862.0459.5770.3761.19 ± 0.13±2.65stablelm-2-12b-chat42.9041.3638.2735.1939.43 ± 0.48±2.6650.9352.7851.5455.2552.62 ± 0.35±2.72Llama-2-13b-chat-hf54.3255.2555.2561.7356.64 ± 0.35±2.7050.6249.6948.1561.4252.47 ± 0.61±2.72Qwen1.5-14B-Chat53.7058.6454.6366.0558.26 ± 1.32±2.6849.3849.3848.7766.0553.40 ± 0.81±2.72Phi-3-medium-128k-instruct65.1266.6762.9669.1465.97 ± 0.46±2.5851.2352.1649.3858.9552.93 ± 1.40±2.72Yi-1.5-34B-Chat51.5453.0951.8545.0650.39 ± 0.48±2.7256.4854.3250.9354.6354.09 ± 0.48±2.71Qwen1.5-32B-Chat54.9452.4752.1661.7355.32 ± 0.23±2.7150.9347.5346.6063.8952.24 ± 0.58±2.72Meta-Llama-3-70B-Instruct64.8167.5964.2074.3867.75 ± 0.53±2.5458.0257.7254.3267.5959.41 ± 0.35±2.67Qwen1.5-72B-Chat71.6071.3061.1178.4070.60 ± 0.19±2.4858.6455.2556.1766.0559.03 ± 0.19±2.68GPT-3.5-turbo-061352.1650.9349.0751.2350.85 ± 1.36±2.7253.7051.8550.9355.5653.01 ± 1.62±2.71GPT-4-061375.9376.2368.8368.2172.30 ± 0.74±2.4462.0458.9558.3370.6862.50 ± 0.93±2.64</p>
<p>TABLE II SUMMARY
II
OF RESULTS ON 22 LLMS.COLUMNS SHOW AVERAGE ACCURACY OVER 3 RUNS DIVIDED BY BATCH AND GLOBALLY, AND THE WIDTH OF A 95% CI IN COT PROMPTING, BEFORE AND AFTER PROVIDING THE ANSWER.THE ALL COLUMN REPRESENTS THE CONCATENATION OF THE FOUR BATCHES Q1-Q4, AND INCLUDES THE STANDARD DEVIATION COMPUTED OVER THE THREE RUNS WITH DIFFERENT RANDOM SEEDS.
CoT bef oreCoT af terModelQ1Q2Q3Q4ALL95% CIQ1Q2Q3Q4ALL95% CIbaseline50.9350.9350.9350.9350.93±050.9350.9350.9350.9350.93±0gemma-1.1-2b-it57.7254.9450.9360.8056.10 ± 1.70±2.7050.3151.8549.3849.0750.15 ± 0.66±2.72stablelm-2-1 6b-chat20.9921.3021.3026.2322.45 ± 2.60±2.2759.8858.0266.6747.8458.10 ± 1.98±2.69Qwen1.5-1.8B-Chat49.0751.2350.3148.4649.77 ± 0.76±2.7249.0749.6950.0042.2847.76 ± 1.40±2.72recurrentgemma-2b-it47.2247.5350.3148.4648.38 ± 0.95±2.7266.9864.2065.1270.6866.74 ± 0.27±2.57Phi-3-mini-128k-instruct-3.8B50.6247.8445.6849.0748.30 ± 2.75±2.7255.8654.3252.4763.2756.48 ± 0.80±2.70Qwen1.5-4B-Chat52.4753.7048.7750.0051.23 ± 2.77±2.7258.6458.9554.9458.6457.79 ± 1.19±2.69Yi-1.5-6B-Chat38.2744.4445.9934.5740.82 ± 0.72±2.6853.4054.9450.0047.8451.54 ± 0.13±2.72Llama-2-7b-chat-hf21.0523.5322.6619.7521.75 ± 0.38±2.2561.1158.0258.6466.9861.19 ± 0.94±2.65Meta-Llama-3-8B-Instruct54.6351.8556.1769.7558.10 ± 1.68±2.6961.1158.9555.5666.3660.49 ± 2.15±2.66gemma-1.1-7b-it55.2554.0151.2354.6353.78 ± 0.44±2.7147.5348.1546.9154.0149.15 ± 0.35±2.72Qwen1.5-7B-Chat56.4856.7952.1665.7457.79 ± 0.29±2.6950.6252.4751.5460.8053.86 ± 0.74±2.71Yi-1.5-9B-Chat50.3147.5344.4445.9947.07 ± 0.11±2.7257.1058.3362.9654.0158.10 ± 1.22±2.69recurrentgemma-9b-it57.1055.8650.0053.4054.09 ± 0.44±2.7158.9559.5753.4060.4958.10 ± 0.75±2.69Phi-3-small-128k-instruct52.1660.8058.6443.5253.78 ± 0.97±2.7165.7470.6863.5875.3168.83 ± 0.53±2.52stablelm-2-12b-chat19.4421.9122.2220.0620.91 ± 0.79±2.2145.3750.0046.9154.9449.31 ± 0.23±2.72Llama-2-13b-chat-hf30.5634.8823.4624.0728.24 ± 1.36±2.4550.9350.9350.9352.1651.23 ± 0.13±2.72Qwen1.5-14B-Chat50.0052.7849.3854.0151.54 ± 0.39±2.7255.2558.9554.3266.9858.87 ± 0.48±2.68Phi-3-medium-128k-instruct55.8657.7255.8661.1157.64 ± 0.82±2.6962.6564.5161.4267.9064.12 ± 0.61±2.61Yi-1.5-34B-Chat57.7259.5759.8857.4158.64 ± 0.39±2.6853.7058.3352.4747.5353.01 ± 0.40±2.72Qwen1.5-32B-Chat56.7959.8859.8860.8059.34 ± 0.29±2.6752.7851.5452.7860.1954.32 ± 0.27±2.71GPT-3.5-turbo-061346.3046.3043.2147.5345.83 ± 0.68±2.7148.1548.7748.4648.1548.38 ± 1.20±2.72GPT-4-061375.9375.9373.4674.3874.92 ± 1.09±2.3674.6975.3170.6870.9972.92 ± 0.40±2.42
These claims have been questioned, e.g., in[3],[4] and by many cognitive scientists.
The full dataset contains 432 test questions. It is publicly available at https://github.com/ikr3-lab/ReasoningLLMs
Around 30% of the StableLM series model responses were considered void because they did not follow the prompt instructions. These responses were marked as incorrect. In all our experiments, this issue was observed with StableLM models only.
ACKNOWLEDGMENTWe acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support.This work was partially funded by the European Union -Next Generation EU within the project NRPP M4C2, Investment 1.,3 DD. 341 -15 march 2022 -FAIR -Future Artificial Intelligence Research -Spoke 4 -PE00000013 -D53C22002380006 and by the MUR under the grant "Dipartimenti di Eccellenza 2023-2027" of the Department of Informatics, Systems and Communication of the University of Milano-Bicocca, Italy.
Attention is all you need. A Vaswani, N Shazeer, NeurIPS. 302017Curran Associates, Inc</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, 2022</p>
<p>Are emergent abilities of large language models a mirage?. R Schaeffer, B Miranda, S Koyejo, 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, Others, 2023</p>
<p>Sparks of artificial general intelligence: Early experiments with gpt-4. S Bubeck, V Chandrasekaran, 2023</p>
<p>Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. C Wang, X Liu, 2023</p>
<p>Reasoning with language model prompting: A survey. S Qiao, Y Ou, Proc. 61st Annual Meeting of the ACL. Long Papers. 61st Annual Meeting of the ACLToronto, CanadaACLJul. 20231</p>
<p>The winograd schema challenge: evaluating progress in commonsense reasoning. L Morgenstern, C L Ortiz, Proc. AAAI'15. AAAI'15AAAI Press2015</p>
<p>Diagnosing the first-order logical reasoning ability through LogicNLI. J Tian, Y Li, W Chen, L Xiao, H He, Y Jin, Proc. EMNLP 21. X Moens, L Huang, S W Specia, -T, Yih, EMNLP 21Nov. 2021</p>
<p>LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding. H Liu, J Liu, L Cui, Z Teng, N Duan, M Zhou, Y Zhang, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023</p>
<p>From lsat: The progress and challenges of complex reasoning. S Wang, Z Liu, W Zhong, M Zhou, Z Wei, Z Chen, N Duan, IEEE/ACM Trans. Audio, Speech and Lang. Proc. 30apr 2022</p>
<p>Exploring self-supervised logic-enhanced training for large language models. F Jiao, Z Teng, B Ding, Z Liu, N F Chen, S R Joty, NAACL. Association for Computational Linguistics2024</p>
<p>On the paradox of learning to reason from data. H Zhang, L H Li, T Meng, K.-W Chang, G V Broeck, 2022</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Comput. Surv. 5512mar 2023</p>
<p>Working memory involvement in propositional and spatial reasoning. K C Klauer, R Stegmaier, T Meiser, Thinking &amp; Reasoning. 311997</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Comput. Surv. 559jan 2023</p>
<p>The Description Logic Handbook: Theory, Implementation, and Applications. F Baader, D Calvanese, D Mcguinness, D Nardi, P Patel-Schneider, 2007Cambridge University Press2nd ed.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Evaluating large language models: A comprehensive survey. Z Guo, R Jin, C Liu, Y Huang, D Shi, L Yu, Y Liu, J Li, B Xiong, D Xiong, arXiv:2310.197362023arXiv preprint</p>
<p>Knowledge Engineering Using Large Language Models. B P Allen, L Stork, P Groth, TGDK. 112023</p>
<p>Evaluating language models for knowledge base completion. B Veseli, S Singhania, S Razniewski, G Weikum, Proc. ESWC 2023, ser. C Lncs, E Pesquita, Jiménez-Ruiz, ESWC 2023, serSpringer202313870</p>
<p>Assessing the factual accuracy of generated text. B Goodrich, V Rao, P J Liu, M Saleh, Proc. 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningNew York, NY, USAACM2019</p>
<p>How context affects language models' factual predictions. F Petroni, P Lewis, A Piktus, T Rocktäschel, Y Wu, A H Miller, S Riedel, 2020Automated Knowledge Base Construction</p>
<p>Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. F Xu, Q Lin, J Han, T Zhao, J Liu, E Cambria, arXiv:2306.098412023arXiv preprint</p>
<p>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. M Parmar, N Varshney, N Patel, M Luo, S Mashetty, A Mitra, C Baral, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics2024</p>
<p>Cladder: A benchmark to assess causal reasoning capabilities of language models. Z Jin, Y Chen, F Leeb, L Gresele, O Kamal, Z Lyu, K Blin, F G Adauto, M Kleiman-Weiner, M Sachan, B Schölkopf, Proceedings of NeurIPS 2023. NeurIPS 20232023</p>
<p>Llms are prone to fallacies in causal inference. N Joshi, A Saparov, Y Wang, H He, 10.48550/arXiv.2406.12158CoRR. 2406.12158, 2024</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, S Mishra, H S Zheng, A W Yu, X Song, D Zhou, Proc. ICLR'24. ICLR'242024</p>
<p>On the paradox of learning to reason from data. H Zhang, L H Li, T Meng, K.-W Chang, G Van Den Broeck, Proceedings of IJCAI 2023. IJCAI 2023aug 2023</p>
<p>Learning deductive reasoning from synthetic corpus based on formal logic. T Morishita, G Morio, A Yamaguchi, Y Sogawa, Proc. ICML. PMLR, 2023. ICML. PMLR, 2023274</p>
<p>This is not a dataset: A large negation benchmark to challenge large language models. I García-Ferrero, B Altuna, J Alvez, I Gonzalez-Dios, G Rigau, Proc. EMLP 2023. EMLP 2023SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Language models are not naysayers: an analysis of language models on negation benchmarks. T H Truong, T Baldwin, K Verspoor, T Cohn, Proc. <em>SEM 2023. </em>SEM 2023Toronto, CanadaACLJul. 2023</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Griffin: Mixing gated linear recurrences with local attention for efficient language models. S De, S L Smith, A Fernando, A Botev, G Cristian-Muraru, A Gu, R Haroun, L Berrada, Y Chen, S Srinivasan, arXiv:2402.194272024arXiv preprint</p>
<p>M Bellagente, J Tow, D Mahan, D Phung, M Zhuravinskyi, R Adithyan, J Baicoianu, B Brooks, N Cooper, A Datta, arXiv:2402.17834Stable lm 2 1.6 b technical report. 2024arXiv preprint</p>
<p>J Bai, S Bai, Y Chu, Z Cui, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>M Abdin, S A Jacobs, A A Awan, J Aneja, A Awadallah, H Awadalla, N Bach, A Bahree, A Bakhtiari, H Behl, arXiv:2404.14219Phi-3 technical report: A highly capable language model locally on your phone. 2024arXiv preprint</p>
<p>Yi: Open foundation models by 01. A Young, B Chen, C Li, C Huang, G Zhang, G Zhang, H Li, J Zhu, J Chen, J Chang, arXiv:2403.046522024arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Llama 3 model card. A I , Meta , 2024</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>F Dekking, A Modern Introduction to Probability and Statistics: Understanding Why and How, ser. Springer Texts in Statistics. Springer2005</p>
<p>Note on the sampling error of the difference between correlated proportions or percentages. Q Mcnemar, Psychometrika. 121947</p>
<p>A survey of fake news: Fundamental theories, detection methods, and opportunities. X Zhou, R Zafarani, ACM Computing Surveys. 5352020</p>
<p>Content based fake news detection using knowledge graphs. J Z Pan, S Pavlova, C Li, N Li, Y Li, J Liu, Proceedings of ISWC 2018, Part I. ISWC 2018, Part ISpringer2018</p>
<p>Evaluation of fake news detection with knowledge-enhanced language models. C Whitehouse, T Weyde, P Madhyastha, N Komninos, Proc. Intern. AAAI Conf. on web and social media. Intern. AAAI Conf. on web and social media202216</p>
<p>Credibility in social media: opinions, news, and health information-a survey. M Viviani, G Pasi, Wiley interdisciplinary reviews: Data mining and knowledge discovery. 20177e1209</p>
<p>Knowledge engineering using large language models. B P Allen, L Stork, P Groth, arXiv:2310.006372023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9433 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9433</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9433</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-4e5f7cd537a1bbcd090f9887b1b59f39a3715dba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4e5f7cd537a1bbcd090f9887b1b59f39a3715dba" target="_blank">Instruction Induction: From Few Examples to Natural Language Task Descriptions</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work shows that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples, and discovers that the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions.</p>
                <p><strong>Paper Abstract:</strong> Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9433.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9433.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Induction (InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Induction Prompting with InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting a model with five input-output demonstrations and asking it to generate a natural-language instruction that explains the demonstrations (zero-shot, no fine-tuning). Evaluated by reference-based (BERTScore) and execution-based (execution accuracy) metrics across 24 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (reported for InstructGPT-family largest model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction Induction (24 diverse NLP tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given 5 input-output demonstrations for a target task (e.g., pluralization, passivization, sentence similarity, formality), generate a natural language instruction that explains the mapping; evaluated by comparing to human-written gold instructions (BERTScore) and by using the generated instruction to prompt an execution model and measuring task accuracy (execution accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot instruction-induction meta-prompt: 'I gave a friend an instruction and five inputs.... Here are the input-output pairs: Input: x Output: y ... The instruction was' followed by the model's generation; each example concatenates five demonstrations (Input/Output pairs, new-line separated). Greedy decoding used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot in-context learning (provide 5 demonstrations + test input and ask directly to produce output) — used as verification of tasks; also compared to human-written instructions (control annotations) and gold reference instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average BERTScore vs gold references: 44.4 (human gold references: 60.0). Average execution accuracy (using InstructGPT as execution model) absolute: 43.6% (human-written control instructions execution accuracy: 66.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Relative to human-written instructions: InstructGPT achieves on average 65.7% of human performance in execution accuracy; relative to GPT-3 instruction induction, InstructGPT dramatically higher (GPT-3 achieves only 9.8% of human performance on execution accuracy). In BERTScore space, gap to humans: -15.6 points (InstructGPT) vs -45.4 points (GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Execution accuracy: +~56 percentage points absolute over GPT-3 (InstructGPT 43.6% vs GPT-3 much lower; reported relative: InstructGPT 65.7% of human vs GPT-3 9.8%). BERTScore: InstructGPT 44.4 vs human 60.0 (−15.6); GPT-3 is ~45.4 points lower than humans.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that instruction-induction ability emerges when a model is both large enough and instruction-tuned (alignment to follow instructions). They suggest instruction tuning and model scale unlock the capability to explicitly describe tasks in natural language. They also note that generated instructions can be imperfect but an instruction-tuned execution model often follows them correctly (sometimes 'right despite imprecise wording').</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>24 tasks; for each task: create induce and execute splits; instruction induction examples composed of 5 demonstrations sampled without replacement from induce set; 100 examples per task. Generations done with greedy decoding. BERTScore computed using DeBERTa-xlMNLI-based BERTScore; execution accuracy measured by prompting InstructGPT with the generated instruction plus each of 100 held-out execute inputs and scoring against task metric. Human control annotations collected (separate annotators) for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9433.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9433.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Induction (GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Induction Prompting with GPT-3 (original models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying the same instruction-induction meta-prompt (5 demonstrations, ask for instruction) to the original GPT-3 family; measured substantially poorer ability to generate accurate/executable instructions compared to InstructGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci, curie, babbage, ada variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>davinci ~175B (original GPT-3), others smaller; exact sizes assumed from Brown et al. (2020)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction Induction (24 diverse NLP tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above: generate an instruction from 5 demonstrations (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same instruction-induction meta-prompt as used with InstructGPT (Challenge puzzle prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared directly to InstructGPT under identical instruction-induction prompt; also compared to standard in-context few-shot completion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average execution-accuracy relative to human baseline: GPT-3 reaches only 9.8% of human performance (average). BERTScore performance is far below humans (gap to humans ≈ 45.4 points). On many tasks GPT-3's generated instructions are uninformative ('Write an output for every input') and yield low execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to InstructGPT: dramatic gap (InstructGPT achieves 65.7% of human execution performance vs GPT-3 9.8%). Compared to in-context execution (GPT-3 in-context few-shot accuracy is often high on these tasks—see verification results—indicating GPT-3 can execute tasks when given demonstrations but cannot reliably describe them).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large negative effect for instruction-induction format on GPT-3: instruction-induction performance is far lower than InstructGPT; in-context learning accuracy (standard few-shot completion) can be high while instruction-induction BERTScore/execution accuracy remain low, demonstrating that presentation format (ask-for-instruction vs ask-for-output) matters and interacts with model alignment/size.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that GPT-3 lacks the instruction-following alignment and/or capacity to reliably produce natural-language task descriptions from demonstrations, despite being able to execute the tasks via in-context learning; thus model training (instruction tuning) and scale determine sensitivity to presentation format.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same dataset and induction procedure as InstructGPT: 5 demonstrations per example, 100 induction examples per task. Generations were produced with greedy decoding. Evaluation via BERTScore and execution accuracy (execution model = InstructGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9433.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9433.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard Few-Shot In-Context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Few-Shot In-Context Prompting (provide demonstrations + test input)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic few-shot in-context learning where models are given k (here 5) input-output demonstrations and then asked to produce the output for a new test input; used as a verification baseline to ensure tasks are inferable from demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family and InstructGPT family (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (davinci ~175B, other sizes for curie/babbage/ada / Instruct-family analogues)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>In-context execution verification across 24 tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given 5 demonstrations and a test input, produce the correct corresponding output (standard few-shot evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context: 5 demonstrations formatted as 'Input: x Output: y' then a test 'Input: x_{k+1}' to be completed by the model. No additional instruction beyond the Input/Output format.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against instruction-induction format (ask model to describe task) and human performance/annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per Table 5: Many tasks reach >=80% accuracy with in-context learning (e.g., First Letter GPT-3 97%, InstructGPT 98%; Pluralization GPT-3 95%, InstructGPT 99%; Sentiment: GPT-3 95%, InstructGPT 99%). Some tasks are hard (Sentence Similarity GPT-3 3%, InstructGPT 15%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On many tasks, in-context few-shot execution accuracy is high for both GPT-3 and InstructGPT, even in cases where instruction induction (ask-for-instruction) fails; demonstrates that presentation format (requesting outputs vs requesting task descriptions) can lead to very different apparent model abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Substantial: tasks with high in-context accuracy sometimes yield low instruction-induction execution or BERTScore (e.g., Passivization: in-context 100% but instruction-induction produced incorrect instructions frequently), indicating format can swing empirical success from near-perfect to poor.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper highlights that in-context learning demonstrates implicit task inference/execution, while instruction induction probes explicit description ability; the difference in format reveals that models may execute a latent rule without being able to verbalize it. The authors hypothesize that instruction tuning and scale are required to bridge this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Verification experiment: for each task, repeat 100 trials with different demonstration sets and test inputs; no extra instructions provided beyond 'Input: ... Output: ...'; evaluation uses task-specific metrics (exact string match, set match, unigram F1, etc.). Results reported in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9433.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9433.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-prompt Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to Meta-Prompt Wrapping of Demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of how different meta-prompts (phrasing around the demonstrations and the instruction request) affect a model's ability to induce correct instructions; tested with text-davinci-002 across a handful of tasks and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (largest InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Meta-prompt effect on instruction induction (sample tasks: First Letter, Passivization, Antonyms, Translation en-de, Sentence Similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure how changing the meta-prompt that introduces the demonstrations changes the correctness of generated instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Four meta-prompts tested: (1) Challenge Puzzle (original): 'I gave a friend an instruction and five inputs... The instruction was'; (2) Challenge Puzzle + Name: same but names 'Bob'; (3) Instruction After Demonstrations: explicit request after examples 'Please write the instruction that best describes the underlying task:'; (4) Instruction Before Demonstrations: give instruction prompt before showing examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison among the four meta-prompt variants above for the same model and example sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported correctness (out of 5 examples) per meta-prompt for sampled tasks (Table 6). Example: For First Letter all prompts 5/5; Passivization 0/5 for all prompts tested; Antonyms: Challenge Puzzle 1/5, Challenge+Name 2/5, Instruction After Demonstrations 3/5, Instruction Before Demonstrations 0/5; Translation en-de: 5/5 for first three prompts, 2/5 for Instruction Before Demonstrations; Sentence Similarity: ranged 4/5 to 5/5 depending on prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Instruction After Demonstrations often improved correctness (e.g., Antonyms 3/5 vs 1/5 for original), while Instruction Before Demonstrations sometimes degraded performance (e.g., translation from 5/5 down to 2/5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Observable and meaningful: changing meta-prompt phrasing altered correct instruction counts by multiple examples out of 5 in small-sample tests (e.g., Antonyms improved from 1/5 to 3/5).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note LMs are sensitive to the meta-prompt wrapping demonstrations; prompt phrasing and placement of the explicit instruction request (before vs after examples) meaningfully affects the model's ability to produce correct instructions. They selected the original prompt based on human clarity studies but show alternate prompts can perform as well or better in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Meta-prompt analysis: sampled five examples for five tasks; for each meta-prompt variant, generated instruction and manually verified correctness (counts reported in Table 6). This is a small-sample manual analysis (not full 100-example evaluation). Prompts listed in Appendix C (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9433.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9433.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Execution Accuracy Evaluation (method effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Accuracy Metric and Use of an Instruction-Tuned Execution Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation protocol that judges a generated instruction by executing it with an instruction-tuned model (InstructGPT) on held-out inputs and measuring task-specific scores; highlights an interaction between presentation format (generated instruction used as prompt) and properties of the execution model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Execution model: InstructGPT (text-davinci-002) used to follow generated instructions</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (largest InstructGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Execution accuracy evaluation across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For each generated instruction I and each hold-out input x_n from the task's execute set, prompt the execution model with I and x_n and measure the task's Score_T(I(x_n), y_n); average over 100 held-out examples to obtain execution accuracy for I.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Generated instruction presented as a prompt (no demonstrations) plus a single test input; the execution model is asked to produce the output according to the generated instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Execution accuracy measured for (a) model-generated instructions, (b) human control annotations (human-written instructions), and (c) manually verified gold instructions (ceiling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average execution accuracy for generated instructions (InstructGPT generated) = 43.6% absolute; human-written control instructions execution accuracy = 66.4% absolute (gold verified instructions act as approximate ceiling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Using InstructGPT as execution model may advantage instructions written in a style similar to its training; authors caution that using the same model as both generator and executor might bias results in favor of that model's generations, but they report preliminary experiments found no other model as good at following instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Shows that format shift from demonstrations-to-output to demonstrations-to-instruction then execution by an instruction-tuned model produces nontrivial performance (InstructGPT-produced instructions yield meaningful execution accuracy), but dependent on execution model: if execution model is weaker, instruction correctness could be underestimated.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue execution accuracy is a natural metric because instructions are executable; however, it assumes availability of a 'good-enough' instruction-tuned interpreter. They note this may bias evaluations and is a limitation of the metric.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>For each task: 100 held-out execute examples; execution model prompted with generated instruction and each input; Score_T uses task-specific metric (exact match, set match, F1, or label equivalence as defined in Appendix A). Execution model chosen as largest InstructGPT; all generated instructions evaluated this way.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9433.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9433.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Format Failure Examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific failures revealing format-dependent behavior (Passivization, Pluralization, Antonyms)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual analysis of specific tasks where instruction-induction outputs were inaccurate or misleading; shows how phrasing/format leads to wrong instructions or 'right for wrong reasons' execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Both InstructGPT (text-davinci-002) and GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B for largest variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Passivization, Pluralization, Antonyms (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Passivization: convert active sentence to passive; Pluralization: produce plural form of noun; Antonyms: produce antonym of a word.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Instruction induction meta-prompt with 5 demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compare InstructGPT's generated instructions to GPT-3's, and compare instruction quality to execution outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Passivization: in-context execution accuracy 100% (both models) but InstructGPT's generated instruction typically 'reverse the order of subject and object' (98% of predictions) which is insufficient to produce grammatically correct passive forms; GPT-3 sometimes generated unrelated prompts that nevertheless cause the execution model to produce passive forms (GPT-3 instruction produced higher execution accuracy despite being unrelated). Pluralization: InstructGPT often generated 'Add s' (24% of predictions) — imprecise but execution model often still produced correct plural (e.g., 'life' -> 'lives'). Antonyms: InstructGPT often generated 'Reverse the input' (60%), which led execution model to reverse letters rather than produce antonyms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Illustrates that instruction-induction format can produce plausible-looking but incorrect instructions; execution outcomes may be influenced by quirks of the execution model, causing apparent successes despite imprecise or incorrect natural-language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative: instruction wording errors lead to systematic mistakes (e.g., passivization wrong) or, conversely, to accidental correct outputs when execution model compensates; shows that format (ask-to-describe vs ask-to-produce) interacts with evaluation and behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors highlight 'right for the wrong reasons' phenomena: the execution model may follow unintended cues in prompt text or default behaviors, producing correct outputs even when the induced instruction is imprecise or wrong. This underscores the complexity of judging instruction quality solely by execution results without manual inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Manual qualitative analysis of generated instructions for 5 tasks; frequency statistics reported for common generated instructions (e.g., 98% for a particular incorrect passivization formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction Induction: From Few Examples to Natural Language Task Descriptions', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners <em>(Rating: 2)</em></li>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9433",
    "paper_id": "paper-4e5f7cd537a1bbcd090f9887b1b59f39a3715dba",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Instruction Induction (InstructGPT)",
            "name_full": "Instruction Induction Prompting with InstructGPT (text-davinci-002)",
            "brief_description": "Prompting a model with five input-output demonstrations and asking it to generate a natural-language instruction that explains the demonstrations (zero-shot, no fine-tuning). Evaluated by reference-based (BERTScore) and execution-based (execution accuracy) metrics across 24 tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_size": "175B (reported for InstructGPT-family largest model)",
            "task_name": "Instruction Induction (24 diverse NLP tasks)",
            "task_description": "Given 5 input-output demonstrations for a target task (e.g., pluralization, passivization, sentence similarity, formality), generate a natural language instruction that explains the mapping; evaluated by comparing to human-written gold instructions (BERTScore) and by using the generated instruction to prompt an execution model and measuring task accuracy (execution accuracy).",
            "presentation_format": "Zero-shot instruction-induction meta-prompt: 'I gave a friend an instruction and five inputs.... Here are the input-output pairs: Input: x Output: y ... The instruction was' followed by the model's generation; each example concatenates five demonstrations (Input/Output pairs, new-line separated). Greedy decoding used.",
            "comparison_format": "Standard few-shot in-context learning (provide 5 demonstrations + test input and ask directly to produce output) — used as verification of tasks; also compared to human-written instructions (control annotations) and gold reference instructions.",
            "performance": "Average BERTScore vs gold references: 44.4 (human gold references: 60.0). Average execution accuracy (using InstructGPT as execution model) absolute: 43.6% (human-written control instructions execution accuracy: 66.4%).",
            "performance_comparison": "Relative to human-written instructions: InstructGPT achieves on average 65.7% of human performance in execution accuracy; relative to GPT-3 instruction induction, InstructGPT dramatically higher (GPT-3 achieves only 9.8% of human performance on execution accuracy). In BERTScore space, gap to humans: -15.6 points (InstructGPT) vs -45.4 points (GPT-3).",
            "format_effect_size": "Execution accuracy: +~56 percentage points absolute over GPT-3 (InstructGPT 43.6% vs GPT-3 much lower; reported relative: InstructGPT 65.7% of human vs GPT-3 9.8%). BERTScore: InstructGPT 44.4 vs human 60.0 (−15.6); GPT-3 is ~45.4 points lower than humans.",
            "explanation_or_hypothesis": "Authors hypothesize that instruction-induction ability emerges when a model is both large enough and instruction-tuned (alignment to follow instructions). They suggest instruction tuning and model scale unlock the capability to explicitly describe tasks in natural language. They also note that generated instructions can be imperfect but an instruction-tuned execution model often follows them correctly (sometimes 'right despite imprecise wording').",
            "null_or_negative_result": false,
            "experimental_details": "24 tasks; for each task: create induce and execute splits; instruction induction examples composed of 5 demonstrations sampled without replacement from induce set; 100 examples per task. Generations done with greedy decoding. BERTScore computed using DeBERTa-xlMNLI-based BERTScore; execution accuracy measured by prompting InstructGPT with the generated instruction plus each of 100 held-out execute inputs and scoring against task metric. Human control annotations collected (separate annotators) for baselines.",
            "uuid": "e9433.0",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Instruction Induction (GPT-3)",
            "name_full": "Instruction Induction Prompting with GPT-3 (original models)",
            "brief_description": "Applying the same instruction-induction meta-prompt (5 demonstrations, ask for instruction) to the original GPT-3 family; measured substantially poorer ability to generate accurate/executable instructions compared to InstructGPT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci, curie, babbage, ada variants)",
            "model_size": "davinci ~175B (original GPT-3), others smaller; exact sizes assumed from Brown et al. (2020)",
            "task_name": "Instruction Induction (24 diverse NLP tasks)",
            "task_description": "Same as above: generate an instruction from 5 demonstrations (zero-shot).",
            "presentation_format": "Same instruction-induction meta-prompt as used with InstructGPT (Challenge puzzle prompt).",
            "comparison_format": "Compared directly to InstructGPT under identical instruction-induction prompt; also compared to standard in-context few-shot completion.",
            "performance": "Average execution-accuracy relative to human baseline: GPT-3 reaches only 9.8% of human performance (average). BERTScore performance is far below humans (gap to humans ≈ 45.4 points). On many tasks GPT-3's generated instructions are uninformative ('Write an output for every input') and yield low execution accuracy.",
            "performance_comparison": "Compared to InstructGPT: dramatic gap (InstructGPT achieves 65.7% of human execution performance vs GPT-3 9.8%). Compared to in-context execution (GPT-3 in-context few-shot accuracy is often high on these tasks—see verification results—indicating GPT-3 can execute tasks when given demonstrations but cannot reliably describe them).",
            "format_effect_size": "Large negative effect for instruction-induction format on GPT-3: instruction-induction performance is far lower than InstructGPT; in-context learning accuracy (standard few-shot completion) can be high while instruction-induction BERTScore/execution accuracy remain low, demonstrating that presentation format (ask-for-instruction vs ask-for-output) matters and interacts with model alignment/size.",
            "explanation_or_hypothesis": "Authors suggest that GPT-3 lacks the instruction-following alignment and/or capacity to reliably produce natural-language task descriptions from demonstrations, despite being able to execute the tasks via in-context learning; thus model training (instruction tuning) and scale determine sensitivity to presentation format.",
            "null_or_negative_result": false,
            "experimental_details": "Same dataset and induction procedure as InstructGPT: 5 demonstrations per example, 100 induction examples per task. Generations were produced with greedy decoding. Evaluation via BERTScore and execution accuracy (execution model = InstructGPT).",
            "uuid": "e9433.1",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Standard Few-Shot In-Context Learning",
            "name_full": "Standard Few-Shot In-Context Prompting (provide demonstrations + test input)",
            "brief_description": "Classic few-shot in-context learning where models are given k (here 5) input-output demonstrations and then asked to produce the output for a new test input; used as a verification baseline to ensure tasks are inferable from demonstrations.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3 family and InstructGPT family (evaluated)",
            "model_size": "various (davinci ~175B, other sizes for curie/babbage/ada / Instruct-family analogues)",
            "task_name": "In-context execution verification across 24 tasks",
            "task_description": "Given 5 demonstrations and a test input, produce the correct corresponding output (standard few-shot evaluation).",
            "presentation_format": "Few-shot in-context: 5 demonstrations formatted as 'Input: x Output: y' then a test 'Input: x_{k+1}' to be completed by the model. No additional instruction beyond the Input/Output format.",
            "comparison_format": "Compared against instruction-induction format (ask model to describe task) and human performance/annotations.",
            "performance": "Per Table 5: Many tasks reach &gt;=80% accuracy with in-context learning (e.g., First Letter GPT-3 97%, InstructGPT 98%; Pluralization GPT-3 95%, InstructGPT 99%; Sentiment: GPT-3 95%, InstructGPT 99%). Some tasks are hard (Sentence Similarity GPT-3 3%, InstructGPT 15%).",
            "performance_comparison": "On many tasks, in-context few-shot execution accuracy is high for both GPT-3 and InstructGPT, even in cases where instruction induction (ask-for-instruction) fails; demonstrates that presentation format (requesting outputs vs requesting task descriptions) can lead to very different apparent model abilities.",
            "format_effect_size": "Substantial: tasks with high in-context accuracy sometimes yield low instruction-induction execution or BERTScore (e.g., Passivization: in-context 100% but instruction-induction produced incorrect instructions frequently), indicating format can swing empirical success from near-perfect to poor.",
            "explanation_or_hypothesis": "The paper highlights that in-context learning demonstrates implicit task inference/execution, while instruction induction probes explicit description ability; the difference in format reveals that models may execute a latent rule without being able to verbalize it. The authors hypothesize that instruction tuning and scale are required to bridge this gap.",
            "null_or_negative_result": null,
            "experimental_details": "Verification experiment: for each task, repeat 100 trials with different demonstration sets and test inputs; no extra instructions provided beyond 'Input: ... Output: ...'; evaluation uses task-specific metrics (exact string match, set match, unigram F1, etc.). Results reported in Table 5.",
            "uuid": "e9433.2",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Meta-prompt Sensitivity",
            "name_full": "Sensitivity to Meta-Prompt Wrapping of Demonstrations",
            "brief_description": "Analysis of how different meta-prompts (phrasing around the demonstrations and the instruction request) affect a model's ability to induce correct instructions; tested with text-davinci-002 across a handful of tasks and prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_size": "175B (largest InstructGPT)",
            "task_name": "Meta-prompt effect on instruction induction (sample tasks: First Letter, Passivization, Antonyms, Translation en-de, Sentence Similarity)",
            "task_description": "Measure how changing the meta-prompt that introduces the demonstrations changes the correctness of generated instructions.",
            "presentation_format": "Four meta-prompts tested: (1) Challenge Puzzle (original): 'I gave a friend an instruction and five inputs... The instruction was'; (2) Challenge Puzzle + Name: same but names 'Bob'; (3) Instruction After Demonstrations: explicit request after examples 'Please write the instruction that best describes the underlying task:'; (4) Instruction Before Demonstrations: give instruction prompt before showing examples.",
            "comparison_format": "Direct comparison among the four meta-prompt variants above for the same model and example sets.",
            "performance": "Reported correctness (out of 5 examples) per meta-prompt for sampled tasks (Table 6). Example: For First Letter all prompts 5/5; Passivization 0/5 for all prompts tested; Antonyms: Challenge Puzzle 1/5, Challenge+Name 2/5, Instruction After Demonstrations 3/5, Instruction Before Demonstrations 0/5; Translation en-de: 5/5 for first three prompts, 2/5 for Instruction Before Demonstrations; Sentence Similarity: ranged 4/5 to 5/5 depending on prompt.",
            "performance_comparison": "Instruction After Demonstrations often improved correctness (e.g., Antonyms 3/5 vs 1/5 for original), while Instruction Before Demonstrations sometimes degraded performance (e.g., translation from 5/5 down to 2/5).",
            "format_effect_size": "Observable and meaningful: changing meta-prompt phrasing altered correct instruction counts by multiple examples out of 5 in small-sample tests (e.g., Antonyms improved from 1/5 to 3/5).",
            "explanation_or_hypothesis": "Authors note LMs are sensitive to the meta-prompt wrapping demonstrations; prompt phrasing and placement of the explicit instruction request (before vs after examples) meaningfully affects the model's ability to produce correct instructions. They selected the original prompt based on human clarity studies but show alternate prompts can perform as well or better in some cases.",
            "null_or_negative_result": false,
            "experimental_details": "Meta-prompt analysis: sampled five examples for five tasks; for each meta-prompt variant, generated instruction and manually verified correctness (counts reported in Table 6). This is a small-sample manual analysis (not full 100-example evaluation). Prompts listed in Appendix C (Table 7).",
            "uuid": "e9433.3",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Execution Accuracy Evaluation (method effect)",
            "name_full": "Execution Accuracy Metric and Use of an Instruction-Tuned Execution Model",
            "brief_description": "Evaluation protocol that judges a generated instruction by executing it with an instruction-tuned model (InstructGPT) on held-out inputs and measuring task-specific scores; highlights an interaction between presentation format (generated instruction used as prompt) and properties of the execution model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Execution model: InstructGPT (text-davinci-002) used to follow generated instructions",
            "model_size": "175B (largest InstructGPT)",
            "task_name": "Execution accuracy evaluation across tasks",
            "task_description": "For each generated instruction I and each hold-out input x_n from the task's execute set, prompt the execution model with I and x_n and measure the task's Score_T(I(x_n), y_n); average over 100 held-out examples to obtain execution accuracy for I.",
            "presentation_format": "Generated instruction presented as a prompt (no demonstrations) plus a single test input; the execution model is asked to produce the output according to the generated instruction.",
            "comparison_format": "Execution accuracy measured for (a) model-generated instructions, (b) human control annotations (human-written instructions), and (c) manually verified gold instructions (ceiling).",
            "performance": "Average execution accuracy for generated instructions (InstructGPT generated) = 43.6% absolute; human-written control instructions execution accuracy = 66.4% absolute (gold verified instructions act as approximate ceiling).",
            "performance_comparison": "Using InstructGPT as execution model may advantage instructions written in a style similar to its training; authors caution that using the same model as both generator and executor might bias results in favor of that model's generations, but they report preliminary experiments found no other model as good at following instructions.",
            "format_effect_size": "Shows that format shift from demonstrations-to-output to demonstrations-to-instruction then execution by an instruction-tuned model produces nontrivial performance (InstructGPT-produced instructions yield meaningful execution accuracy), but dependent on execution model: if execution model is weaker, instruction correctness could be underestimated.",
            "explanation_or_hypothesis": "Authors argue execution accuracy is a natural metric because instructions are executable; however, it assumes availability of a 'good-enough' instruction-tuned interpreter. They note this may bias evaluations and is a limitation of the metric.",
            "null_or_negative_result": null,
            "experimental_details": "For each task: 100 held-out execute examples; execution model prompted with generated instruction and each input; Score_T uses task-specific metric (exact match, set match, F1, or label equivalence as defined in Appendix A). Execution model chosen as largest InstructGPT; all generated instructions evaluated this way.",
            "uuid": "e9433.4",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Format Failure Examples",
            "name_full": "Task-specific failures revealing format-dependent behavior (Passivization, Pluralization, Antonyms)",
            "brief_description": "Manual analysis of specific tasks where instruction-induction outputs were inaccurate or misleading; shows how phrasing/format leads to wrong instructions or 'right for wrong reasons' execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Both InstructGPT (text-davinci-002) and GPT-3 (davinci)",
            "model_size": "175B for largest variants",
            "task_name": "Passivization, Pluralization, Antonyms (examples)",
            "task_description": "Passivization: convert active sentence to passive; Pluralization: produce plural form of noun; Antonyms: produce antonym of a word.",
            "presentation_format": "Instruction induction meta-prompt with 5 demonstrations.",
            "comparison_format": "Compare InstructGPT's generated instructions to GPT-3's, and compare instruction quality to execution outcomes.",
            "performance": "Passivization: in-context execution accuracy 100% (both models) but InstructGPT's generated instruction typically 'reverse the order of subject and object' (98% of predictions) which is insufficient to produce grammatically correct passive forms; GPT-3 sometimes generated unrelated prompts that nevertheless cause the execution model to produce passive forms (GPT-3 instruction produced higher execution accuracy despite being unrelated). Pluralization: InstructGPT often generated 'Add s' (24% of predictions) — imprecise but execution model often still produced correct plural (e.g., 'life' -&gt; 'lives'). Antonyms: InstructGPT often generated 'Reverse the input' (60%), which led execution model to reverse letters rather than produce antonyms.",
            "performance_comparison": "Illustrates that instruction-induction format can produce plausible-looking but incorrect instructions; execution outcomes may be influenced by quirks of the execution model, causing apparent successes despite imprecise or incorrect natural-language instructions.",
            "format_effect_size": "Qualitative: instruction wording errors lead to systematic mistakes (e.g., passivization wrong) or, conversely, to accidental correct outputs when execution model compensates; shows that format (ask-to-describe vs ask-to-produce) interacts with evaluation and behavior.",
            "explanation_or_hypothesis": "Authors highlight 'right for the wrong reasons' phenomena: the execution model may follow unintended cues in prompt text or default behaviors, producing correct outputs even when the induced instruction is imprecise or wrong. This underscores the complexity of judging instruction quality solely by execution results without manual inspection.",
            "null_or_negative_result": false,
            "experimental_details": "Manual qualitative analysis of generated instructions for 5 tasks; frequency statistics reported for common generated instructions (e.g., 98% for a particular incorrect passivization formulation).",
            "uuid": "e9433.5",
            "source_info": {
                "paper_title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.01638125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Instruction Induction: From Few Examples to Natural Language Task Descriptions</h1>
<p>Or Honovich ${ }^{\tau}$ Uri Shaham ${ }^{\tau}$ Samuel R. Bowman ${ }^{\nu}$ Omer Levy ${ }^{\tau \mu}$<br>${ }^{\tau}$ Tel Aviv University<br>${ }^{\nu}$ New York University<br>${ }^{\mu}$ Meta AI</p>
<h4>Abstract</h4>
<p>Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as incontext learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves $65.7 \%$ of human performance in our execution-based metric, while the original GPT-3 model reaches only $9.8 \%$ of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) can perform unseen tasks by conditioning on a few labeled examples, effectively inferring the underlying tasks through a process known as in-context learning (Brown et al., 2020). However, task inference is implicit, and the ability of models to explicitly reason about it remains unexplored. In this work, we show that LMs can explicitly describe an underlying task, in natural language, given a few labeled examples.</p>
<p>We introduce the instruction induction challenge, in which a model is provided with a few inputoutput demonstrations, and is requested to generate a natural language instruction describing the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>connection between the input-output pairs. In our experiments, inducing instructions is done in a zeroshot manner by simply prompting the models to explain a small set of given demonstrations, as shown in Figure 1; we do not perform fine-tuning or use any labeled instruction induction data.</p>
<p>We examine instruction induction on 24 tasks, ranging from morphosyntactic tasks to style transfer and sentiment analysis. Since our goal is to shed light on the phenomenon of instruction induction, we focus on tasks that have clear and simple instructions. As a basic evaluation protocol, we collect human annotations and use them as gold-standard references; the generated instructions are then compared to these references using BERTScore (Zhang et al., 2020). Moreover, we suggest a novel evaluation metric for instruction induction: execution accuracy. The execution accuracy of a generated instruction is measured by testing whether LMs can correctly perform the task in a zero-shot manner by using the generated instruction alone, without any demonstrations.</p>
<p>Our experiments reveal a surprising ability at generating correct instructions. The bestperforming model, InstructGPT (Ouyang et al., 2022), achieves an average BERTScore of 44.4, compared to human performance of 60.0; when measuring execution accuracy, the model reaches 43.6, with human-written instructions reaching 66.4. For some tasks, the model's performance is on par or even better than human performance. When qualitatively examining the generated instructions, we often observe accurate instructions, even for some of the more challenging tasks. For instance, in the task of formality style transfer, generated instructions include "Translate the inputs into more formal language" and "Use formal language". For semantic text similarity, the generated instructions include "For each input, rate the similarity of the two sentences on a scale of 0 to 5 , with 5 being a perfect match" and "Determine whether</p>
<table>
<thead>
<tr>
<th style="text-align: center;">In-Context Learning</th>
<th style="text-align: center;">Instruction Induction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input: As soon as you can. Output: At your earliest convenience.</td>
<td style="text-align: center;">I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:</td>
</tr>
<tr>
<td style="text-align: center;">Input: Sorry I messed up. Output: I apologise for my wrongdoings.</td>
<td style="text-align: center;">Input: As soon as you can. Output: At your earliest convenience.</td>
</tr>
<tr>
<td style="text-align: center;">Input: I can't stand his temper. Output: I cannot tolerate his temper.</td>
<td style="text-align: center;">Input: Sorry I messed up. Output: I apologise for my wrongdoings.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The instruction was translate the inputs into more formal language.</td>
</tr>
</tbody>
</table>
<p>Figure 1: An example of instruction induction for the task of formality style transfer. Left: the standard in-context learning setting; given five demonstrations, complete the sixth. Right: instruction induction; the language model is prompted to generate a natural language instruction that describes the demonstrations. Model completions are in blue, prompt templates are in pink.
the two sentences are about the same thing".
Despite these impressive results, we find that this ability is currently unique to InstructGPT (Ouyang et al., 2022), which is both very large (175B parameters) and was especially fine-tuned to follow instructions. Ablations on smaller versions of InstructGPT as well as the original 175B-parameter GPT-3 (Brown et al., 2020) yield dramatically weaker performance. These findings are in line with recent work showing that increasing model size unlocks new capabilities (Chowdhery et al., 2022; Ganguli et al., 2022), and serves as additional evidence for the strength of instruction tuning (Sanh et al., 2022; Wei et al., 2022a; Ouyang et al., 2022), perhaps even pointing to the necessity of complementing standard next-word prediction with additional objectives.</p>
<p>The fact that models can induce natural language instructions suggests that instruction induction may serve as a learning paradigm of its own, where the optimization goal is to find the best natural language description that fits the observations. In this ambitious view of instruction induction, natural language can function as the hypothesis space, and a model is required to learn a natural language rule describing the relation between inputs and outputs in the training examples, rather than a set of uninterpretable parameters. While we currently provide a proof-of-concept for that idea, extending it by grounding models in natural language has the immediate benefit of human interpretability,
explainability, and verifiability, while potentially alleviating overfitting and other issues associated with spurious correlations.</p>
<h2>2 Instruction Induction</h2>
<p>We begin by formulating the task of instruction induction. Given a sequence of $n$ demonstrations $\left{x_{k}, y_{k}\right}<em k="k">{k \in{1, \ldots, n}}$, the goal is to generate a single natural language instruction, such that for each $x</em>$. This format is similar to in-context learning (Brown et al., 2020), only here the desired output is an instruction describing the relation between the inputs and outputs of the demonstrations. We require models to perform this in a zero-shot setting, without any fine-tuning on labeled data. Figure 1 illustrates the difference between standard in-context prompting and instruction-induction prompting.}$, following the instruction results in $y_{k</p>
<p>To elicit models to generate instructions, we consider prompts that would elicit humans to do so. We design a meta-prompt presenting instruction induction as a challenge puzzle and verify its clarity in a human study (§3.3). The prompt is presented in Figure 1 (right side, in pink). ${ }^{2}$</p>
<p>While prior work already shows that large LMs are often able to infer a latent task from a given set of demonstrations, this has been largely based on their ability to execute the task on a held-out exam-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ple. Instruction induction requires that the model describe the underlying task in natural language.</p>
<h2>3 Data</h2>
<p>We evaluate on 24 tasks. Example tasks are listed in Table 1. See Table 4 in Appendix A for the full list of tasks. We select these tasks as they vary in difficulty and represent different aspects of language understanding, ranging from surface-level spelling to sentence similarity and causality detection. ${ }^{3}$ Since our primary goal is to study the phenomenon of instruction induction under lab conditions, we focus on tasks that have simple instructions and defer tasks with more complicated instructions for future work. We review the dataset's format, the annotation and verification processes we conducted to ensure that the tasks are viable, and finally discuss a theoretical limitation of this setup.</p>
<h3>3.1 Format</h3>
<p>In every task, each single demonstration $\left(x_{k}, y_{k}\right)$ is formatted as follows:</p>
<p>$$
\begin{aligned}
&amp; \text { Input: } x_{k} \
&amp; \text { Output: } y_{k}
\end{aligned}
$$</p>
<p>For instance, one demonstration in the pluralization task is "Input: cat" followed by "Output: cats" in a new line. We split each task's demonstrations into two sets: an induce set, which we use for generating instructions, and an execute set, which is held out for the execution accuracy evaluation metric (see §4.2). Each instruction induction example is composed of 5 demonstrations sampled randomly without replacement from the induce set, concatenated with new-line separators; we create 100 examples for each task. When generating instructions, each example is placed inside the instruction induction prompt, and fed to the model (Figure 1, right).</p>
<h3>3.2 Annotating Reference Instructions</h3>
<p>We collect 10 gold-reference human-annotated instructions via college-graduate English-speaking annotators. For each task, we provide the annotators with the exact same input we intend to provide a model: 5 input-output demonstrations wrapped by the instruction-induction prompt (Figure 1). We manually verify each annotation and discard ones that do not correctly describe the task. We refer to this set of annotations as the gold annotations, and use them for reference-based evaluation (see §4).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.3 Verification</h3>
<p>Prior to the instruction induction experiments, we conduct two tests to ensure that either models or humans can infer the underlying task given 5 demonstrations. We first verify that models can indeed execute our tasks given 5 demonstrations using incontext learning. Secondly, we conduct a human study to confirm that 5 demonstrations are enough for humans to describe the latent tasks.</p>
<p>In-Context Learning We prompt models with 5 input-output demonstrations and concatenate an additional test input $x_{k+1}$, and verify that the models are able to correctly predict $y_{k+1}$ (Figure 1, left). For each task, we repeat this experiment 100 times, each with a different set of demonstrations and test inputs. We do not provide the model with any instruction beyond the "Input: $x_{k}$ Output: $y_{k}$ " format. We evaluate each task using its predefined evaluation metric. ${ }^{4}$ The in-context results for GPT-3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022) (see model details in §5) are reported in Table 5 in Appendix B, which shows that in-context learning can reach $80 \%$ accuracy and above on most tasks.</p>
<p>Human Study To assess the human ability to induce instructions, we collect human-written instructions, using annotators that did not participate in the gold references collection. As in the goldreference annotation process, we provide annotators with the same input we intend to provide to models. We refer to this set of annotations as the control annotations. We then manually count, for each task, the number of annotators that provided a correct instruction, and report the correct instructions percentage in Table 5 (Appendix B). In all but one task (Larger Animal), at least 4 out of 5 annotators were able to produce correct task descriptions.</p>
<p>We also use the control group's annotations to establish a human baseline for automatic evaluation metrics. For reference-based evaluation (§4.1), we treat the control annotations as generated instructions and compare them against the gold annotations, while for execution accuracy (§4.2), we use the control annotations to measure human performance, and the gold references as a ceiling metric.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Demonstration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">First Letter</td>
<td style="text-align: center;">Extract the first letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$</td>
</tr>
<tr>
<td style="text-align: center;">Syntax</td>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">Negate the input sentence.</td>
<td style="text-align: center;">Time is finite $\rightarrow$ Time is not finite.</td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> Semantics</td>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">Write a word that means the opposite of the input word.</td>
<td style="text-align: center;">won $\rightarrow$ lost</td>
</tr>
<tr>
<td style="text-align: center;">Phonetics</td>
<td style="text-align: center;">Rhymes</td>
<td style="text-align: center;">Write a word that rhymes with the input word.</td>
<td style="text-align: center;">sing $\rightarrow$ ring</td>
</tr>
<tr>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">Cause Selection</td>
<td style="text-align: center;">Find which of the two given cause and effect sentences is the cause.</td>
<td style="text-align: center;">Sentence 1: The soda went flat. Sentence 2: The bottle was left open. $\rightarrow$ The bottle was left open.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Common <br> Concept</td>
<td style="text-align: center;">Find a common characteristic for the given objects.</td>
<td style="text-align: center;">guitars, pendulums, neutrinos $\rightarrow$ involve oscillations.</td>
</tr>
<tr>
<td style="text-align: center;">Style</td>
<td style="text-align: center;">Formality</td>
<td style="text-align: center;">Rephrase the sentence in formal language.</td>
<td style="text-align: center;">Please call once you get there $\rightarrow$ Please call upon your arrival.</td>
</tr>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">Sum the two given numbers.</td>
<td style="text-align: center;">$2210 \rightarrow 32$</td>
</tr>
<tr>
<td style="text-align: center;">Multi- <br> lingual</td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Translate the word into German / Spanish / French.</td>
<td style="text-align: center;">game $\rightarrow$ juego</td>
</tr>
<tr>
<td style="text-align: center;">GLUE</td>
<td style="text-align: center;">Sentiment <br> Analysis</td>
<td style="text-align: center;">Determine whether a movie review is positive or negative.</td>
<td style="text-align: center;">The film is small in scope, yet perfectly formed. $\rightarrow$ positive</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence <br> Similarity</td>
<td style="text-align: center;">Rate the semantic similarity of two input sentences on a scale of 0 - definitely not to 5 - perfectly.</td>
<td style="text-align: center;">Sentence 1: A man is smoking. Sentence 2: A man is skating. $\rightarrow 0$ - definitely not</td>
</tr>
</tbody>
</table>
<p>Table 1: Example tasks used in our instruction-induction experiments. For each task, we show a corresponding instruction and demonstration, with $\rightarrow$ separating the input from the output.</p>
<h3>3.4 Ambiguity</h3>
<p>A theoretical challenge in inducing instructions is ambiguity. For example, when given the single demonstration "Input: The coffee is too hot. Output: The, too, hot", one could infer that the underlying task is either "write all the words containing the letter T" or "write all the three-lettered words", both valid interpretations. Ambiguity might confuse models tasked with instruction induction while also making evaluation less reliable. In practice, providing 5 demonstrations typically resolves the ambiguity in our set of tasks. As evident from the data verification process, our tasks can typically be inferred by models and/or humans.</p>
<p>Inducing more complex task descriptions, such as predicting detailed annotation guidelines, may pose a greater challenge in terms of ambiguity. We hypothesize that providing more than 5 demonstrations could mitigate some of that challenge, and leave further exploration of this avenue to future work.</p>
<h2>4 Evaluating Generated Instructions</h2>
<p>As a standard text generation metric, we report BERTScore (Zhang et al., 2020). However, the instruction induction challenge has a unique property,
which does not usually hold for other text generation tasks: the instructions are executable. Their correctness can therefore be measured directly by utilizing them as prompts.</p>
<h3>4.1 Reference-Based Evaluation</h3>
<p>We use BERTScore (Zhang et al., 2020) to compare the model-generated instructions against the collected gold annotations. As mentioned in §3.2, we use only the correct, verified annotations as references. We take the maximal BERTScore-F1 over all gold-reference annotations to account for natural variations in instruction formulation. ${ }^{5}$ We also establish a human baseline for each task using the control annotations, which were collected from a separate control group of annotators (§3.3), which we compare against the gold annotations in exactly the same way as model-generated instructions. In preliminary studies, we experiment with other reference-based metrics (ROUGE and BLEU), and find BERTScore to be a better predictor of instruction quality, although all metrics showed similar trends.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.2 Execution Accuracy</h3>
<p>We introduce execution accuracy, a new metric unique to the instruction induction task. We define a correct instruction as one that can guide humans to produce the expected output. To approximate human behavior, we use an instruction-tuned model and test whether it can follow the generated instruction. Concretely, to measure the execution accuracy of a predicted instruction $I$ (e.g., "Write the plural form of the given word.") for a task $T$ (pluralization), we prompt a model with $I$ and an input $x$ ("cat"). We then test, given $I$ and $x$, whether the model can correctly predict $y$, the output of performing $T$ on the input $x$ (cats).</p>
<p>To obtain meaningful results, we measure execution accuracy on the 100 held-out execute examples for each task. The execution accuracy of an instruction $I$ is therefore computed by taking the average over $\operatorname{Score}<em n="n">{T}\left(I\left(x</em>$ in the execute set, where Score $}\right), y_{n}\right)$ for all $x_{n<em n="n">{T}$ denotes the task's corresponding metric (see Appendix A), and $I\left(x</em>$}\right)$ is the result of prompting a predefined language model with the instruction $I$ and the input $x_{n}$. As recent models are trained to follow instructions (Sanh et al., 2022; Wei et al., 2022a; Ouyang et al., 2022), and due to the relative clarity of our tasks, we expect correct instructions to yield high execution accuracy when using a sufficiently powerful execution model. ${ }^{6</p>
<h2>5 Results</h2>
<p>Baseline Models We experiment with eight versions of GPT-3 (Brown et al., 2020), a Transformer decoder language model. First, we experiment with the most current version available in the OpenAI API, for each of the four available model sizes. Though not stated explicitly in the API, we assume these models are those reported by Ouyang et al. (2022), and we therefore refer to them as Instruct models. ${ }^{7}$ We also experiment with the four originally published GPT-3 versions. ${ }^{8}$ By default, we refer to the largest Instruct model as InstructGPT, and the original 175B-parameter model as GPT3. All model generations were produced using the greedy decoding algorithm.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Average BERTScore and execution accuracy across tasks. BERTScore is measured against the gold references. The execution accuracy for all generated instructions is measured using InstructGPT as the execution model. Human performance is measured using the human control group's instructions.</p>
<h3>5.1 Comparing to Gold Annotations</h3>
<p>Figure 2a presents the average BERTScore per task (see §4.1). Results show that the InstructGPT model has, to some extent, the ability to induce instructions from a few demonstrations; in 13 out of 24 tasks it achieves at least $75 \%$ of human performance. GPT-3, on the other hand, is quite far from human performance across the board.</p>
<p>Table 2 shows the average scores across all tasks. We observe the same trend; while InstructGPT's BERTScore is 15.6 points lower than human performance, the gap between GPT-3 and humans is 45.4 points. Moreover, we observe that smaller models - even those fine-tuned to follow instructions - do not exhibit any instruction-induction abilities. Scores are slightly higher for larger models of the same family (except for the InstructGPT-Babbage outlier), but are overall low. Excluding the largest models, there does not appear to be a significant advantage for Instruct models over the originals when controlling for model size.</p>
<h3>5.2 Execution Accuracy</h3>
<p>We compute the execution accuracy as detailed in $\S 4.2$, and report the average over 100 generated instructions for each task. As an execution model, we use the largest InstructGPT model. We also use this model to induce instructions, and while using it as an execution model might bias results towards its own generations, preliminary experiments show that no other model is as good at following instructions as InstructGPT. As a point of reference, we</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: (a) Average BERTScores of model-generated instructions for each task, compared to the performance of the control group's manually-authored instructions. The BERTScore for each instruction is computed using the human gold annotations as references. (b) Average execution accuracy of model-generated instructions for each task, compared to the execution accuracy measured for human-written instructions. The Human baseline is measured by taking the control group's annotations, while the Gold ceiling metric is based on the separately-annotated and verified gold annotations.</p>
<p>apply the execution accuracy evaluation protocol to human-written instructions. First, to compare models with human performance, we measure the execution accuracy of the control annotation set. Second, to account for limitations in the execution model, we measure execution accuracy of the correct (manually verified) gold annotations, which acts as an approximated ceiling metric.</p>
<p>Figure 2 b presents the execution accuracy per task. In 12 out of 24 tasks, InstructGPT achieves at least $75 \%$ of the execution accuracy measured for the human-written instructions. GPT-3 shows much weaker execution accuracy, scoring less than $10 \%$ on 20 of the 24 tasks. In fact, only in the cases of formality, passivization, and cause selection does it approach human performance, and that is largely an artifact of a more lenient evaluation metric in the case of formality and cause selection, or due to the execution model being right for the wrong reasons in the case of passivization (see §6). In some tasks, the control annotations are of high quality and reach a higher score than the verified gold annotations, likely due to variance of the execution model in such cases.</p>
<p>Table 2 shows the same trends. On average, InstructGPT achieves $65.7 \%$ of human performance, while GPT-3 reaches only $9.8 \%$ of human performance. When considering different model families or sizes, we do not see any substantial improvements when increasing model size or adding instruction tuning, with the exception of the largest InstructGPT model. The ability to generate instructions seems to only emerge when a model is both large enough and aligned to follow instructions. Overall, even the best-performing model still does not reach human performance, leaving room for future improvement.</p>
<h2>6 Analysis</h2>
<p>To gain further insight into the successes and failures of instruction induction prompting, we manually analyze the model-generated instructions of 5 tasks. Table 3 shows the most common predictions of GPT-3 and InstructGPT for each of these tasks.</p>
<p>InstructGPT obtains high, or close to human execution accuracy scores for three of these tasks (First Letter, Sentence Similarity, Pluralization). Indeed, the instructions for both First Letter and Sentence Similarity accurately describe the task. However, the instruction generated for Pluralization is not entirely precise, since it dismisses other forms of
pluralization such as -es, -ies, and irregulars. Although the instruction only asks to add an "s", the execution model often ignores the specifics and produces the correct plural form; in one case, the input word was "life" and the output was "lives". While this particular instruction accounts for $24 \%$ of the induced instructions in the pluralization task, some predictions do explicitly mention pluralization, though not always accurately, e.g., "Add -s to the end of each word to make it plural".</p>
<p>For some tasks, InstructGPT fails to produce accurate instructions, even if it is able to solve via in-context learning (see Table 5). In Passivization, $98 \%$ of the predicted instructions were to simply "reverse the order of the subject and object", while ignoring additional surface-form manipulations needed to convert the given sentence into passive form; e.g., for the input "The authors supported the scientist", following the instructions produces the output "The scientist supported the authors", while the correct passive form is "The scientist was supported by the authors". Surprisingly, the instructions generated by GPT-3 obtained higher execution accuracy than the InstructGPT, even though they were entirely unrelated. In $24 \%$ of the cases, GPT-3 predicted "The friend wrote the following output:" - an instruction that apparently prompts the execution model to often rephrase the input in passive form. Lastly, in Antonyms, $60 \%$ of InstructGPT's predictions were "Reverse the input", and another $11 \%$ were "Reverse the word". While one could imagine an interpretation of these instructions that reflects the task (reversing the meaning of the word), the execution model interprets them literally, and reverses the input words' letters.</p>
<p>Overall, GPT-3 did not exhibit any instruction induction abilities, although it did often phrase outputs in imperative language. One relatively common prediction was the generic instruction "Write an output for every input". Because these empty instructions are in the right format, they tend to have some overlap with the reference instructions, which inflates their BERTScore. Execution accuracy, on the other hand, is robust to this phenomenon, and typically assigns GPT-3's outputs very low scores.</p>
<h2>7 Related Work</h2>
<p>In-Context Learning Brown et al. (2020) suggest that models can learn a task by conditioning on few input-output demonstration pairs, without any fine-tuning or gradient updates. This paradigm,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">GPT-3</th>
<th style="text-align: left;">InstructGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">First letter</td>
<td style="text-align: left;">The friend's output was:</td>
<td style="text-align: left;">Write the first letter of each word.</td>
</tr>
<tr>
<td style="text-align: left;">Sentence Similarity</td>
<td style="text-align: left;">The friend wrote the following output:</td>
<td style="text-align: left;">For each input, rate the similarity of the two sentences on a <br> scale of 0 to 5, with 5 being a perfect match.</td>
</tr>
<tr>
<td style="text-align: left;">Pluralization</td>
<td style="text-align: left;">The friend's output was:</td>
<td style="text-align: left;">Add 's' to the end of each word.</td>
</tr>
<tr>
<td style="text-align: left;">Passivization</td>
<td style="text-align: left;">The friend wrote the following output:</td>
<td style="text-align: left;">Reverse the order of the subject and the object in the sentence.</td>
</tr>
<tr>
<td style="text-align: left;">Antonyms</td>
<td style="text-align: left;">The friend's output was:</td>
<td style="text-align: left;">Reverse the input.</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of the instructions generated by GPT-3 and InstructGPT for five of our tasks.
known as in-context learning or prompt-based learning (Liu et al., 2021), has been the focus of many research efforts lately: Du et al. (2021) suggest methods for more efficient in-context learning, Zhao et al. (2021) study methods for improving the stability and accuracy of prompt-based models, Chen et al. (2021) and Min et al. (2022a) conduct meta-training with an in-context learning objective, while other work studies the effect of the provided prompts (Reynolds and McDonell, 2021; Webson and Pavlick, 2021; Min et al., 2022b), or suggests prompt reframing techniques (Mishra et al., 2021) and prompt retrieval methods (Rubin et al., 2021). To the best of our knowledge, all previous work study in-context learning through the lens of executing a latent task, while we focus on the ability to explicitly describe it.</p>
<p>The Instruction Paradigm Efrat and Levy (2020) propose to learn new tasks from natural language instructions. Mishra et al. (2022) and Wang et al. (2022b) collect crowdsourcing instructions used to create NLP datasets into a benchmark for measuring the ability to solve tasks by reading instructions. Recent work shows that fine-tuning on task instructions (instruction tuning) improves the zero-shot learning abilities of LMs (Sanh et al., 2022; Wei et al., 2022a; Ouyang et al., 2022). Prasad et al. (2022) introduce an edit-based search approach for improving existing instructions used for prompting. In this work, we focus on models' ability to generate instructions, rather than their ability to execute instructions written by humans.</p>
<p>Intermediate Reasoning Steps Nye et al. (2022) show that LMs can perform complex computations by writing intermediate steps on a "scratchpad". In chain of thought prompting (Wei et al., 2022b), input-output demonstrations are enriched with sentences elaborating intermediate task reasoning steps, improving the performance of LMs
on tasks requiring reasoning skills. Subsequent work further improves the performance on such tasks using a self-consistency ensemble (Wang et al., 2022a), which samples a set of diverse chain-of-thought reasoning paths, taking the majority vote over all generated answers. Zelikman et al. (2022) utilize a small set of examples labeled with chain-of-thought rationales and a large set of unlabeled data to iteratively bootstrap automatic rationale generation, thus creating a large dataset labeled with such rationales to enable fine-tuning. In contrast, we study the ability of LMs to generate a description of the task, rather than generating intermediate reasoning steps as a means of executing complex tasks.</p>
<p>Learning a Natural Language Hypothesis Zhong et al. (2022) propose to automatically describe the differences between two data distributions $D_{0}$ and $D_{1}$ by finding a description that is more true for $D_{1}$, e.g., "is military related" or "is longer in sentence length". They frame this task as learning a natural language hypothesis. In this work, we suggest describing a task based on demonstrations of this task alone, rather than describing the differences between two data distributions.</p>
<h2>8 Discussion</h2>
<p>This work demonstrates that large LMs can not only infer new tasks based on a handful of demonstrations, but also describe them in natural language. We provide evidence of this ability on a diverse set of language tasks, and show that while instruction induction abilities are limited to a single state-of-the-art model, this model does indeed approach human performance on about half the tasks.</p>
<p>It is not unreasonable to assume that models in the near future will be even better at processing human-generated instructions, and it is therefore interesting to discuss the potential applications of</p>
<p>instruction induction. In particular, we envision a use case in which instruction induction serves as a machine learning approach; instead of converting a dataset into a set of continuous parameters, we could produce a natural language instruction that best describes the data. Grounding the model in concise natural language has the advantage of interpretability, and has the potential to solve fundamental issues pertaining to spurious correlations. While it is still too early to determine whether this approach is viable, we view it as an intriguing direction for future research.</p>
<h2>9 Limitations</h2>
<p>Since our primary goal is to study the phenomenon of instruction induction under lab conditions, we focus on tasks that have simple instructions. Future work may extend instruction induction research by including tasks with more complex instructions. These tasks are expected to pose a greater evaluation challenge, especially when considering reference-based methods. Evaluating through execution accuracy, however, may mitigate some of that challenge. Additionally, only one model showed instruction induction abilities, i.e., text-davinci-002. The exact implementation details of the model and its training data are not publicly available, thus we are unable to investigate the reason behind the emergence of this ability. However, we note that our goal is to present the phenomenon of instruction induction and to raise the ambitious possibility of instruction induction as a learning paradigm. Thus, our goal is not to focus on specific models but rather to shed light on this unexplored phenomenon. Finally, we point to a limitation of the execution accuracy metric, namely assuming the existence of a good-enough instruction-tuned model. Due to recent interest and progress in instruction tuning, we believe this to be a reasonable assumption.</p>
<h2>Ethics Statement</h2>
<p>We believe that inducing instructions, as well as grounding in natural language in general, can potentially improve interpretability and explainability. We therefore view this line of research as having a positive effect on the ability to avoid unwanted artifacts.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2021. Meta-learning via language model in-context tuning.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontuek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy MeierHellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2021. Glam: Efficient scaling of language models with mixture-of-experts.</p>
<p>Avia Efrat and Omer Levy. 2020. The turking test: Can language models understand instructions?</p>
<p>Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</p>
<p>Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones, Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Scott Johnston, Shauna Kravec, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Dario Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Chris Olah, and Jack Clark. 2022. Predictability and surprise in large generative models.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations.</p>
<p>Nora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811-7818, Online. Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In International Conference on Learning Representations.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022a. MetaICL: Learning to learn in context. In NAACL-HLT.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2021. Reframing instructional prompts to gptk's language.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Nikita Nangia, Adina Williams, Angeliki Lazaridou, and Samuel Bowman. 2017. The RepEval 2017 shared task: Multi-genre natural language inference with sentence representations. In Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP, pages 1-10, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2022. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Mohammad Taher Pilehvar and Jose Camacho-Collados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267-1273, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2022. Grips: Gradient-free, edit-based instruction search for prompting large language models.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA '21, New York, NY, USA. Association for Computing Machinery.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.</p>
<p>Robyn Speer and Catherine Havasi. 2012. Representing general relational knowledge in ConceptNet 5. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 3679-3686, Istanbul, Turkey. European Language Resources Association (ELRA).</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, et al. 2022b. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.</p>
<p>Albert Webson and Ellie Pavlick. 2021. Do promptbased models really understand the meaning of their prompts?</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911-3921, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In ICLR 2020.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In ICML, pages 12697-12706.</p>
<p>Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. 2022. Describing differences between text distributions with natural language. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 27099-27116. PMLR.</p>
<h2>A Dataset Details</h2>
<p>This appendix presents the full list of tasks (§A.1) and details each task's dataset (§A.2). Some datasets rely on a set of common English nouns (CEN), described at §A.3.</p>
<h2>A. 1 Full Dataset</h2>
<p>Table 4 presents the full list of tasks used in our experiments.</p>
<h2>A. 2 Tasks</h2>
<p>We elaborate on each task's data source, preprocessing protocol, and evaluation metric used in the in-context learning and execution accuracy experiments. As mentioned in $\S 3$, each task has induce and execute sets; unless stated otherwise, we sample 100 examples as the execute set for each task. When evaluating outputs, the generated text is first normalized; we take only the first generated sentence and lowercase it. We apply exact string match as the evaluation metric where applicable, elaborating only where alternative metrics are used.</p>
<p>First Letter In each demonstration, $x_{k}$ is a noun, and $y_{k}$ is the first letter of that noun. We construct the demonstrations by extracting the first letter of each word in CEN.</p>
<p>Second Letter Identical to the First Letter task, only here $y_{k}$ is the second letter of $x_{k}$.</p>
<p>List Letters $x_{k}$ is a noun from CEN, and $y_{k}$ is a list of $x_{k}$ 's letters, separated by spaces.</p>
<p>Starting With $x_{k}$ contains a sentence and a letter in brackets, and $y_{k}$ lists the words in $x_{k}$ that start with the given letter. We avoid cases in which $y_{k}$ is empty, i.e., there is always at least one word in the input sentence starting with the given letter. Sentences are taken from the CoLA dataset (Warstadt et al., 2018). For the induce set, we create all (sentence, letter) pairs using CoLA's train set, and then sample 3,000 pairs. For the execute set, we create all (sentence, letter) pairs from CoLA's in-domain and out-of-domain dev sets, and then sample 50 in-domain and 50 out-of-domain examples. We evaluate using exact set match, by treating the output (and $y_{k}$ ) as a set of strings.</p>
<p>Pluralization Given a singular noun $x_{k}$, produce the plural form $y_{k}$. We take noun inputs from the CEN set, filtering out mass nouns using a prede-
fined list. ${ }^{9}$ To create the plural forms, we apply an automatic pluralization engine ${ }^{10}$ and exclude nouns for which the engine's output did not appear at least 50 times in the Wikitext-103 corpus. This results in 2,043 singular-plural noun pairs.</p>
<p>Passivization Given a simple active sentence $x_{k}$, rephrase the sentence in passive voice $y_{k}$. We use the 1,000 HANS (McCoy et al., 2019) evaluation set active-passive entailed sentence pairs.</p>
<p>Negation $y_{k}$ is the negation of the input sentence $x_{k}$. We use the negated LAMA dataset (Petroni et al., 2019; Kassner and Schütze, 2020), taking the 304 negated SQuAD (Rajpurkar et al., 2016) sentences, 300 ConceptNet (Speer and Havasi, 2012) sentences, 200 T-REx (Elsahar et al., 2018) sentences and 200 Google-RE ${ }^{11}$ sentences. For ConceptNet and T-REx, we manually select these sentences to ensure their quality. For Google-RE, we automatically sample 100 sentences from the place of birth relation, and 100 from the place of death relation.</p>
<p>Antonyms $y_{k}$ is the antonym of the input word $x_{k}$. We use the antonym pairs from oLMpics (Talmor et al., 2020), which were extracted from ConceptNet (Speer and Havasi, 2012) and WordNet (Fellbaum, 1998). For uniformity, we verify that all pairs are indeed antonyms according to WordNet.</p>
<p>Synonyms $x_{k}$ is a word and $y_{k}$ is its synonym. As in the antonyms task, we use the synonym pairs of Talmor et al. (2020). Since there can be multiple synonyms for each input word, the task's incontext and execution accuracy are evaluated by testing whether the gold answer (a single word) is contained in the predicted answer (which may be a list of words).</p>
<p>Membership $x_{k}$ is a list of words, where some of the words represent animals, and $y_{k}$ lists the animals from $x_{k}$. To construct the task's data, we first select 6 word categories: animals, clothing, colors, food, vehicles, and professions. We then take 10-50 words from each category, using only words that are categorized at the A1 or A2 levels according to the Common European Framework of</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Instruction</th>
<th style="text-align: center;">Demonstration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">First Letter</td>
<td style="text-align: center;">Extract the first letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Second Letter</td>
<td style="text-align: center;">Extract the second letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{a}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">List Letters</td>
<td style="text-align: center;">Break the input word into letters, separated by spaces.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$ a t</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starting With</td>
<td style="text-align: center;">Extract the words starting with a given letter from the input sentence.</td>
<td style="text-align: center;">The man whose car I hit last week sued me. $[\mathrm{m}] \rightarrow$ man, me</td>
</tr>
<tr>
<td style="text-align: center;">Morpho- <br> syntax</td>
<td style="text-align: center;">Pluralization</td>
<td style="text-align: center;">Convert the input word to its plural form.</td>
<td style="text-align: center;">cat $\rightarrow$ cats</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Passivization</td>
<td style="text-align: center;">Write the input sentence in passive form.</td>
<td style="text-align: center;">The artist introduced the scientist. $\rightarrow$ The scientist was introduced by the artist.</td>
</tr>
<tr>
<td style="text-align: center;">Syntax</td>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">Negate the input sentence.</td>
<td style="text-align: center;">Time is finite $\rightarrow$ Time is not finite.</td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> Semantics</td>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">Write a word that means the opposite of the input word.</td>
<td style="text-align: center;">won $\rightarrow$ lost</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synonyms</td>
<td style="text-align: center;">Write a word with a similar meaning to the input word.</td>
<td style="text-align: center;">alleged $\rightarrow$ supposed</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Membership</td>
<td style="text-align: center;">Write all the animals that appear in the given list.</td>
<td style="text-align: center;">cat, helicopter, cook, whale, frog, lion $\rightarrow$ frog, cat, lion, whale</td>
</tr>
<tr>
<td style="text-align: center;">Phonetics</td>
<td style="text-align: center;">Rhymes</td>
<td style="text-align: center;">Write a word that rhymes with the input word.</td>
<td style="text-align: center;">sing $\rightarrow$ ring</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge</td>
<td style="text-align: center;">Larger Animal</td>
<td style="text-align: center;">Write the larger of the two given animals.</td>
<td style="text-align: center;">koala, snail $\rightarrow$ koala</td>
</tr>
<tr>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">Cause Selection</td>
<td style="text-align: center;">Find which of the two given cause and effect sentences is the cause.</td>
<td style="text-align: center;">Sentence 1: The soda went flat. Sentence 2: The bottle was left open. $\rightarrow$ The bottle was left open.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Common <br> Concept</td>
<td style="text-align: center;">Find a common characteristic for the given objects.</td>
<td style="text-align: center;">guitars, pendulums, neutrinos $\rightarrow$ involve oscillations.</td>
</tr>
<tr>
<td style="text-align: center;">Style</td>
<td style="text-align: center;">Formality</td>
<td style="text-align: center;">Rephrase the sentence in formal language.</td>
<td style="text-align: center;">Please call once you get there $\rightarrow$ Please call upon your arrival.</td>
</tr>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">Sum the two given numbers.</td>
<td style="text-align: center;">$2210 \rightarrow 32$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Difference</td>
<td style="text-align: center;">Subtract the second number from the first.</td>
<td style="text-align: center;">$3222 \rightarrow 10$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number to Word</td>
<td style="text-align: center;">Write the number in English words.</td>
<td style="text-align: center;">$26 \rightarrow$ twenty-six</td>
</tr>
<tr>
<td style="text-align: center;">Multi- <br> lingual</td>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">Translate the word into German / Spanish / French.</td>
<td style="text-align: center;">game $\rightarrow$ juego</td>
</tr>
<tr>
<td style="text-align: center;">GLUE</td>
<td style="text-align: center;">Sentiment <br> Analysis</td>
<td style="text-align: center;">Determine whether a movie review is positive or negative.</td>
<td style="text-align: center;">The film is small in scope, yet perfectly formed. $\rightarrow$ positive</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence <br> Similarity</td>
<td style="text-align: center;">Rate the semantic similarity of two input sentences on a scale of 0 - definitely not to 5 - perfectly.</td>
<td style="text-align: center;">Sentence 1: A man is smoking. Sentence 2: A man is skating. $\rightarrow 0$ - definitely not</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Word in Context</td>
<td style="text-align: center;">Determine whether an input word has the same meaning in the two input sentences.</td>
<td style="text-align: center;">Sentence 1: Approach a task. Sentence 2: To approach the city. Word: approach $\rightarrow$ not the same</td>
</tr>
</tbody>
</table>
<p>Table 4: The tasks in our instruction-induction experiments. For each task, we show a corresponding instruction and demonstration, with $\rightarrow$ separating the input from the output.</p>
<p>Reference for Languages (CEFR). ${ }^{12}$ Using these words, we create random lists containing between 5 to 7 words, where 3 or 4 are animals and the rest belong to one of the other 5 categories. The induce split is constructed by sampling 3,000 such combinations, using $80 \%$ of each category's words. The execute split is constructed by sampling 100 such combinations, using the remaining $20 \%$ of each category's words. The task's in-context and execution accuracy are evaluated using an exact set match, by treating the output (and $y_{k}$ ) as a set of strings.</p>
<p>Rhymes $y_{k}$ is a rhyme of the input word $x_{k}$. The data was constructed by taking words categorized at the A1, A2, or B1 levels according to CEFR. We then use CMU's pronouncing dictionary ${ }^{13}$ to find rhyming groups for these words. The execute split is constructed by sampling 30 rhyming groups, each containing two or more words, and sampling 100 unique words. The induce split is constructed using the rest of the rhyming groups. We evaluate this task by checking whether the predicted word is contained in the rhyming group of $x_{k}$.</p>
<p>Larger Animal $x_{k}$ is two animals, and $y_{k}$ is the (physically) larger one. We use the object comparison data from oLMpics (Talmor et al., 2020), taking the train split, which only contains animals. We construct the induce set using a sample of $80 \%$ of the animals and the execute set by sampling 100 pairs out of the remaining $20 \%$ animals.</p>
<p>Cause Selection $x_{k}$ contains two sentences describing related events, where one event caused the other; $y_{k}$ contains the cause sentence. As data source, we use the 50 examples from the BIGbench (Srivastava et al., 2022) Cause and Effect task, randomly splitting them to equally-sized induce and execute sets. In each of the induce demonstrations, we randomly sample the position of the cause sentence (either the first or the second sentence in $x_{k}$ ). For examples in the execute set, we take both options for each cause and effect pair, doubling the data.</p>
<p>Common Concept $x_{k}$ contains a few entities that share a non-trivial common underlying concept, while $y_{k}$ describes that common concept. We use the 32 examples from Novel Concepts in BIG-</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>bench (Srivastava et al., 2022), using half for induce and half for execute. As the BIG-bench answers usually contain clear "task markers" (e.g., answers that start with "They all have...", indicating that the task was to find a common concept), we remove them from our demonstrations. The task's in-context and execution accuracy are evaluated using unigram overlap (F1).</p>
<p>Formality $x_{k}$ is a sentence in informal English, and $y_{k}$ is its paraphrase in more formal language. We write 30 sentence pairs ourselves, following existing guidelines for converting informal sentences into formal ones. ${ }^{14}$ The task's in-context and execution accuracy are evaluated using unigram overlap (F1).</p>
<p>Sum $x_{k}$ contains two numbers separated by a space, and $y_{k}$ is their sum. For each number in the range $[0,99]$, we enumerate over all pairs.</p>
<p>Difference $x_{k}$ contains two numbers separated by a space, and $y_{k}$ is the difference between them. We use all number pairs such that both input numbers are in the range $[0,198]$, and always subtract the smaller number from the bigger number.</p>
<p>Number to Word $x_{k}$ is a number written in digits (e.g., 28), and $y_{k}$ is the same number written in words (e.g, twenty-eight). We use all numbers in range $[0,9999]$.</p>
<p>Translation $x_{k}$ is an English word and $y_{k}$ is its translation to some target language - either German, Spanish, or French. We use CEN as input words, and obtain their translations via Wiktionary. ${ }^{15}$ For evaluation, we check whether the predicted answer is contained in the set of the possible gold answers.</p>
<p>Sentiment Analysis $x_{k}$ is a movie review and $y_{k}$ is a binary label, either "positive" or "negative", marking the review's sentiment. We use the Stanford Sentiment Treebank dataset (Socher et al., 2013) from GLUE (Wang et al., 2018), taking the train split as our induce set and the dev split as the execute set. We consider only full sentences, discarding sentence constituents and sentences containing more than 10 words. This leaves us with</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>an induce set of 1,167 examples. To create labelbalanced instruction induction examples, we sample each sequence of 5 demonstrations such that there are at least 2 demonstrations for each label.</p>
<p>Sentence Similarity $x_{k}$ contains two sentences, and $y_{k}$ reflects the semantic similarity of the two input sentences. The similarity is measured on a scale of 0 to 5 , and the labels contain an additional short textual description of the numerical label, e.g., " 5 - perfectly". We use the Semantic Textual Similarity Benchmark dataset (Cer et al., 2017) from GLUE, rounding the similarity scores and taking the train split as the induce set and the dev split as the execute set. We discard examples in which at least one of the sentences contains more than 10 words, which leaves us with an induce set of 3,716 examples. In each instruction induction example, we sample at least one pair with a score of 0 and one with a score of 5 , so that models will be exposed to the minimal and maximal scores when generating an instruction. We evaluate whether the predicted answer matches one of three valid outputs for each label: the numerical label (" 5 "), the verbal label ("perfectly"), or the combined label ("5 - perfectly").</p>
<p>Word in Context $x_{k}$ contains a target word and two contexts (sentences) for that word, and $y_{k}$ is a binary label reflecting whether the word has the same meaning in both contexts. We use the Word in Context dataset (Pilehvar and Camacho-Collados, 2019) from SuperGLUE (Wang et al., 2019), taking the train split as the induce set and the dev split as the execute set. We discard examples in which at least one of the sentences contains more than 10 words, which leaves us with an induce set of 4,084 examples. To create label-balanced instruction induction examples, we sample each sequence of 5 demonstrations such that there are at least 2 demonstrations for each label. We evaluate whether the predicted label matches one of several possible outputs: "same", "yes", or "true" for an identical meaning, and "not the same", "no", or "false" for a different meaning.</p>
<h2>A. 3 Common English Nouns</h2>
<p>We create a dataset of common English nouns (CEN) by filtering high-frequency nouns from the Wikitext-103 corpus (Merity et al., 2017). We first create a vocabulary of the 10,000 most frequent words in the corpus, from which we will later select the nouns. We then process the corpus with</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">In-Context Learning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human <br> Study</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">First Letter</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Second Letter</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">List Letters</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Starting With</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Pluralization</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Passivization</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Synonyms</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Membership</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Rhymes</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Larger Animal</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">Cause Selection</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Common Concept</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Formality</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Diff</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Number To Word</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Translation en-de</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Translation en-es</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Translation en-fr</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Sentiment</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Sentence Similarity</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Word in Context</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">80</td>
</tr>
</tbody>
</table>
<p>Table 5: Data verification results. The in-context learning scores show how well models can infer our tasks, and the human study scores show how often humans write the correct instruction given the instruction induction prompt. All scores above or equal to $80 \%$ are in bold.</p>
<p>SpaCy's part-of-speech tagger and lemmatizer, ${ }^{16}$ and retain only nouns that appear in their singular form by verifying that their part-of-speech tag is "NN" and testing whether the word's lemma is identical to the word itself. We additionally filter nouns that have less than 3 letters. Overall, this leaves us with a set of 3,406 nouns.</p>
<h2>B Data Verification</h2>
<p>Table 5 shows the results for the data verification experiments (§3.3). As evident by these results, most of our tasks can be inferred in-context by models. Moreover, all tasks but one can be accurately described by at least 4 out 5 human annotators.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta-Prompt</th>
<th style="text-align: center;">First <br> Letter</th>
<th style="text-align: center;">Passivization</th>
<th style="text-align: center;">Antonyms</th>
<th style="text-align: center;">Translation <br> en-de</th>
<th style="text-align: center;">Sentence <br> Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Challenge Puzzle (Original)</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$1 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$4 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">Challenge Puzzle + Name</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$2 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$4 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">Instruction After Demonstrations</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$3 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$5 / 5$</td>
</tr>
<tr>
<td style="text-align: left;">Instruction Before Demonstrations</td>
<td style="text-align: center;">$5 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$0 / 5$</td>
<td style="text-align: center;">$2 / 5$</td>
<td style="text-align: center;">$3 / 5$</td>
</tr>
</tbody>
</table>
<p>Table 6: The number of correct instructions generated by text-davinci-002, out of the five examples tested for each task, as inspected for each meta-prompt.</p>
<h2>C Meta-Prompt Analysis</h2>
<p>As language models are known to be sensitive to the meta-prompt wrapping the demonstrations, we test the instruction induction abilities of the bestperforming model, text-davinci-002, when varying the meta-prompt. The instruction induction meta-prompt presented in Figure 1 was selected by showing humans several pre-designed prompts and inspecting which was the clearest for the participants. We test the sensitivity to the meta-prompt by taking three additional meta-prompts (Table 7), sampling five examples from five tasks and manually verifying the correctness of the generated instructions.</p>
<p>Table 6 shows that while the model performance is affected by the content of the meta-prompt, the overall trend is similar when using other metaprompts, and high performance can be obtained with other prompts as well. In fact, for two of the three additional tested prompts, the generated instructions seem to be even better than those generated using the original prompt, though the differences are too small to determine this conclusively.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>ACL 2023 Responsible NLP Checklist</h1>
<h2>A For every submission:</h2>
<p>A1. Did you describe the limitations of your work?
3,9
\ A2. Did you discuss any potential risks of your work?
One benefit of the proposed approach is better interpretability and explainability, and we therefore view it as a method for reducing risks.
$\checkmark$ A3. Do the abstract and introduction summarize the paper's main claims?
1
\ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<p>3
$\checkmark$ B1. Did you cite the creators of artifacts you used?
Appendix A
\ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We verified that all the data and code used is publicly open - we verified license details for each, and we provided citation and links to all relevant resources, where license details can also be found.
$\checkmark$ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Appendix A
\ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
We didn't discuss that, but other than the fact that we only used published datasets that are already used by the research community - we also sampled examples and manually verified their content.
$\checkmark$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Appendix A
$\checkmark$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Appendix A
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<h1>C Did you run computational experiments?</h1>
<p>5
\&amp; C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>We used OpenAI models, for which the number of parameters is not always known. For models with known number of parametrs, we did report that number.
$\checkmark$ C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
5
\&amp; C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
We did not include error bars. The usage of mean values and the number of examples used to calculate the mean are clear and transparent.
$\checkmark$ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
4, Appendix A
D Did you use human annotators (e.g., crowdworkers) or research with human participants? 3
$\checkmark$ D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
3
$\checkmark$ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
3
$\checkmark$ D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
3
\&amp; D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? The data annotation did not have any associated risks and did not require a special approval.
$\checkmark$ D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
3</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Challenge Puzzle (Original)
I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:</p>
<p>Input:
Output:
$\cdots$
The instruction was
Challenge Puzzle + Name
I gave Bob an instruction and five inputs. Bob read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:</p>
<p>Input:
Output:
$\cdots$
The instruction was
Instruction After Demonstrations
Below are five input-output pairs that correspond to some underlying task:</p>
<p>Input:
Output:
$\cdots$
Please write the instruction that best describes the underlying task:</p>
<h2>Instruction Before Demonstrations</h2>
<p>You are given five examples of input-output pairs. Please write an instruction that describes creating an output from each input.</p>
<p>Input:
Output:
$\cdots$
Table 7: The meta-prompts used in our analysis.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{14}$ https://www.niu.edu/writingtutorial/ style/formal-and-informal-style.shtml, https://www.uts.edu.au/current-students/ support/helps/self-help-resources/ grammar/formal-and-informal-language ${ }^{15}$ https://github.com/open-dsl-dict/ wiktionary-dict&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
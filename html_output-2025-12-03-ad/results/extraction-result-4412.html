<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4412 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4412</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4412</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-281103057</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.03565v1.pdf" target="_blank">ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference</a></p>
                <p><strong>Paper Abstract:</strong> Understanding how scientific ideas evolve requires more than summarizing individual papers-it demands structured, cross-document reasoning over thematically related research. In this work, we formalize multi-document scientific inference, a new task that extracts and aligns motivation, methodology, and experimental results across related papers to reconstruct research development chains. This task introduces key challenges, including temporally aligning loosely structured methods and standardizing heterogeneous experimental tables. We present ResearchPulse, an agent-based framework that integrates instruction planning, scientific content extraction, and structured visualization. It consists of three coordinated agents: a Plan Agent for task decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a Lchart-Agent that synthesizes experimental line charts. To support this task, we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper clusters. Experiments show that our system, despite using 7B-scale agents, consistently outperforms strong baselines like GPT-4o in semantic alignment, structural consistency, and visual fidelity. The dataset are available in https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4412.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4412.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchPulse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchPulse (agent-based multi-document scientific inference system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular, instruction-driven agent system that extracts, aligns, and visualizes motivations, methods, and experimental results across thematically related papers to reconstruct research development chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchPulse</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An end-to-end, multi-agent pipeline that decomposes a user instruction via a Plan Agent and routes subtasks to specialized agents: Mmap-Agent for extracting motivation-method tuples and producing time-aligned mind maps, and Lchart-Agent for extracting experimental tables/metrics and producing temporal trend charts. The pipeline includes document parsing (PDF→markdown), section segmentation, citation-aware clustering, fine-tuned LLM extractors, metric normalization, and auto-generated visualization code (Python) executed via a compiler module. Outputs include hierarchical markdown chains rendered as mind maps and line charts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Plan Agent: Qwen-72B; Mmap-Agent: Qwen2.5-7B (fine-tuned 7B); Lchart-Agent: Qwen2.5-Coder-7B (fine-tuned 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Section-aware parsing (PDF→markdown) + fine-tuned LLM extraction models for structured fields; uses sentence-BERT for clustering and an Extraction module to pre-segment abstracts/intros/experiment sections before LLM field extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Temporal alignment and ordering of extracted motivation-method tuples; aggregation and normalization of experimental tables across papers; generation of hierarchical markdown research chains and auto-generated visualization code (mind maps and line charts); multi-agent orchestration (plan→extract→synthesize→visualize).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed to operate on document clusters; ResearchPulse-Bench contains 100 citation-aware clusters (average ~20 papers per cluster).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science / AI research threads (arXiv and OpenReview-curated clusters).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured, time-aligned method-motivation chains (mind maps) and experimental trend visualizations (line charts) with generated Python visualization code.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Method-Tracking: BERTScore (P/R/F1), METEOR, GPT-Score (fluency, relevance, accuracy, creativity, quality); Experimental-Analysis: Pass@1 (code execution), IS, FID, KID, CLIP-FID, LPIPS, CMMD, SSIM, PSNR, MS-SSIM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mmap-Agent (task: method-tracking) BERTScore F1=90.39, METEOR=46.14, GPT-4o-evaluated fluency 92.16/quality 85.59; Lchart-Agent (experimental-analysis) Pass@1=97.50, FID=6.73, LPIPS=8.49, SSIM=55.56.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against open-source and closed-source models including Qwen2.5-7B-Instruct, InternLM3-8B-Instruct, Llama-3.1-8B-Instruct, CodeLlama-7B, Qwen2.5-Coder-7B-Instruct, GPT-4o, Claude-3.7-Sonnet, Gemini-1.5 Pro.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ResearchPulse's specialized 7B agents (Mmap and Lchart) outperform larger or closed-source baselines on targeted metrics: Mmap-Agent exceeds GPT-4o on BERTScore/METEOR/GPT-Score; Lchart-Agent achieves higher Pass@1 (97.50 vs GPT-4o's 96.25) and lower FID (6.73 vs higher baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A modular multi-agent architecture with task-specific fine-tuned 7B LLMs can achieve strong cross-document semantic alignment and high-fidelity visualization, often matching or exceeding larger closed-source models; temporal alignment and structured extraction are crucial for reconstructing methodological trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Observed errors include misattributed innovations, missing-extraction of method components, factual deviations in reported metrics, and partial references; reliance on PDF→markdown parsing and on prompt/annotation quality; nontrivial human validation required; potential hallucination and attribution drift across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>System was evaluated on 100 clusters (avg ~20 papers); authors emphasize strong performance at 7B model scale, noting that specialized fine-tuning and modular design yield better task-specific results than naively scaling a single large LLM. No extensive claim about linear scaling with number of papers; very long experimental-analysis instances (avg ~14K tokens) require robust parsing and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4412.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchPulse-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchPulse-Bench (citation-aware benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark dataset of 100 citation-aware annotated paper clusters designed to supervise and evaluate multi-document scientific inference tasks such as Method-Tracking and Experimental-Analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchPulse-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Curated clusters from arXiv and OpenReview with metadata extraction, PDF-to-markdown parsing, section segmentation, semantic clustering via Sentence-BERT + K-Means, and human-validated annotations of motivations, methods, experimental tables, metrics, and citation links; includes reference outputs (mind maps and line charts).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used model-assisted extraction during construction (DeepSeek-R1 or GPT-4o were used to extract fine-grained elements before human validation).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Annotation pipeline combining LLM-assisted extraction (DeepSeek-R1 / GPT-4o) with multi-stage human inspection; sentence-BERT embeddings for clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Annotated fields are rendered into hierarchical markdown and visualizations; supports evaluation of temporal alignment and experimental trend synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>100 document clusters; average ~20 papers per cluster (clusters range from 6 to 33+ papers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>AI / computer science research (arXiv and OpenReview sourced).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Gold-standard method-tracking chains (mind maps) and experimental-analysis outputs (structured tables and trend charts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Enables evaluation with BERTScore, METEOR, GPT-Score for textual outputs and IS, FID, KID, CLIP-FID, LPIPS, CMMD, SSIM, PSNR, MS-SSIM, and pass@1 for visualization/code outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Dataset statistics: Method-Tracking avg input ~1.1K tokens, output ~350 tokens; Experimental-Analysis avg input ~14K tokens, output ~900 tokens. (These are dataset stats rather than model performance.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>N/A (benchmark used to evaluate ResearchPulse and various LLM baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A (benchmark facilitates comparative evaluation reported elsewhere in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Citation-aware clustering and human-validated LLM-assisted annotation yield high-quality supervision for cross-document structural inference; highlights need for long-context handling and structured extraction to evaluate method-level reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Annotation and validation are labor-intensive; clusters were restricted to open-access sources; long experimental tables create heavy context length requirements for models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Benchmark scale fixed at 100 clusters; cluster sizes vary — larger clusters increase extraction and alignment complexity and token-length demands.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4412.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan Agent (ResearchPulse component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Central controller agent that classifies user intent (method-tracking or experimental-analysis), decomposes instructions, and routes documents and subtasks to downstream extraction/synthesis agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Plan Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses an LLM (Qwen-72B in experiments) to interpret the user instruction, decide task routing, invoke the Extraction module for document parsing, and dispatch per-document instructions (I_method or I_exp) to Mmap-Agent or Lchart-Agent; coordinates multi-agent workflow and tool invocation (e.g., PDF parsers, compilers).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen-72B (as reported in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not an extractor itself; it invokes the Extraction module to parse docs into segmented sections and then routes segments to specialized extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Orchestrates synthesis by delegating to Mmap-Agent and Lchart-Agent and aggregating their outputs for final visualization rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over the input cluster size provided by user (bench clusters avg ~20 papers); scalable to cluster sizes used in ResearchPulse-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General within the paper: AI research literature orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Task plan and orchestration decisions; triggers downstream structured outputs (mind maps, line charts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Indirectly evaluated via downstream task metrics (method-tracking & experimental-analysis performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Plan Agent was implemented with Qwen-72B; no separate numeric ablation results reported specifically isolating Plan Agent performance beyond overall system.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not isolated; part of overall ResearchPulse comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not isolated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Centralized instruction planning aids modular decomposition and improves downstream agent specialization; choice of Plan Agent model (large Qwen-72B) supports robust intent classification and routing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires reliable document parsing and clear instruction decomposition; potential bottleneck if orchestration fails or misclassifies intent.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Larger Plan Agent capacity (Qwen-72B used) was chosen to stabilize instruction interpretation; scaling of Plan Agent not deeply explored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4412.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mmap-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mmap-Agent (Motivation-Method mapping agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned LLM agent that extracts motivation-method tuples from paper abstracts/intros and constructs temporally aligned method-tracking chains for mind-map visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mmap-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Fine-tuned LLM (Qwen2.5-7B) trained to extract structured tuples (motivation, method) from input document segments given I_method; outputs are sorted by publication timestamp to produce a hierarchical markdown M_chain and rendered into a mind map. Training optimizes negative log-likelihood over annotated tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen2.5-7B (fine-tuned for structural extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Supervised fine-tuned extraction conditioned on segmented inputs (abstract/introduction); model produces (m_i, r_i) tuples via structured prompting and supervised sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Temporal sorting and hierarchical markdown generation; outputs fed into visualization renderer to produce mind maps.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on clusters in ResearchPulse-Bench (avg ~20 papers per cluster); training used manually annotated sequences across representative paper series (exact training count not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>AI research papers (methodological evolution analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured textual representations of motivation-method tuples and mind-map-ready markdown.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BERTScore (P/R/F1), METEOR, GPT-Score (fluency, relevance, accuracy, creativity, quality).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Mmap-Agent achieved BERTScore F1=90.39, METEOR=46.14, and top GPT-4o-evaluated fluency/quality metrics (fluency 92.16, quality 85.59).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with Qwen2.5-7B-Instruct, InternLM3-8B, Llama-3.1-8B, CodeLlama-7B, Qwen2.5-Coder-7B-Instruct, GPT-4o, Claude-3.7-Sonnet, Gemini-1.5 Pro.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperformed baselines on aggregate metrics (higher BERTScore F1 and GPT-4o-evaluated scores); ablations show dependence on auxiliary GPT-4o and Compiler modules for best fluency and precision.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task-specific fine-tuning on structured motivation-method pairs substantially improves semantic alignment; modular design and auxiliary modules (GPT-4o, Compiler) enhance fluency and precision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Susceptible to misattribution and missing-extraction errors; sensitive to annotation quality and phrase-level ambiguity in abstracts/intros.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Achieves strong results at 7B scale when fine-tuned; removing auxiliary modules reduces performance—indicating component interactions matter more than pure parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4412.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lchart-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lchart-Agent (Experimental table extraction and charting agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned LLM agent that extracts experimental tables, model names, evaluation metrics, and baseline years from papers and synthesizes temporal trend charts via auto-generated Python code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Lchart-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Fine-tuned agent (Qwen2.5-Coder-7B) specialized for tabular extraction, metric normalization, and visualization code generation; it outputs structured tuples (T_i, M_i, E_i, Y_i), aligns them temporally to create E_chain, and uses a visualization module V to render charts (via generated Python) with a Compiler module to execute/validate code.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen2.5-Coder-7B (fine-tuned 7B).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Field-level supervised extraction of tables and metrics from parsed experiment sections; normalization of metric names/units; alignment across datasets and cited baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Temporal alignment of experimental results into trend records; automatic generation of plotting code and image rendering; uses verification via code execution (pass@1 metric).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates on document clusters (ResearchPulse-Bench average ~20 papers per cluster); trained on a curated dataset of aligned tables and metric annotations (exact training size not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>AI experimental comparisons (benchmark performance trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured experimental records, auto-generated Python plotting code, and rendered line charts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Pass@1, IS, FID, KID, CLIP-FID, LPIPS, CMMD, SSIM, PSNR, MS-SSIM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Lchart-Agent: Pass@1=97.50, FID=6.73, LPIPS=8.49, CMMD=4.06, SSIM=55.56 (outperforming most baselines); ablations show removing GPT-4o and Compiler reduces IS/FID and Pass@1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Qwen2.5-7B-Instruct, InternLM3-8B-Instruct, Llama-3.1-8B-Instruct, CodeLlama-7B, Qwen2.5-Coder-7B-Instruct, GPT-4o, Claude-3.7-Sonnet, Gemini-1.5 Pro.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Lchart-Agent achieves higher Pass@1 (97.50) than GPT-4o (96.25) and lower FID than most baselines, indicating superior code executability and visual fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Specialized fine-tuning for tabular extraction plus a compiler verification loop yields high functional correctness and visual fidelity; normalization across heterogeneous tables is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Factual deviations and partial-reference errors observed; parsing noisy PDFs and heterogeneous table formats remains challenging; reliance on code-execution increases system complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performs well on clusters with long-context experimental data (~14K tokens); ablation shows component additions (GPT-4o, Compiler) matter more than sheer model size for quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4412.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey (automatic survey generation pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A four-stage pipeline that uses multiple LLMs to automate survey writing via retrieval, outline generation, section-wise writing, and optimization with co-generation and critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline comprising retrieval of relevant literature, LLM-based outline generation, section-by-section drafting by LLMs, and an optimization/critique phase where multiple LLMs co-generate and critique drafts to improve structure and depth.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Multiple LLMs (unspecified in this paper's description); described generically as LLM-based co-generation and critique.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-based selection of documents followed by LLM-driven extraction and section drafting; likely uses prompting and chunking for long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical pipeline combining retrieval→outline→section generation→iterative critique to synthesize multi-document surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for multi-document survey writing (number unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Generic academic survey generation (applied to AI literature in examples).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Full literature surveys / structured reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported evaluation metrics in prior work typically include coverage, factuality, and human evaluation; this paper cites AutoSurvey as a related pipeline without detailed metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to traditional multi-document summarization systems; exact baselines in AutoSurvey original work not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-stage LLM pipelines can speed up survey drafting and improve organization when paired with critique/co-generation loops, but depth and grounding remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depth and factual grounding issues; potential hallucinations and lack of fine-grained methodological trajectory extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to scale with document collections but suffers from long-context and grounding challenges as input size grows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4412.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHIME (LLM-assisted hierarchical organization for literature review)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-assisted hierarchical generation system that builds topic trees and integrates expert feedback for structured survey organization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chime: Llm-assisted hierarchical organization of scientific studies for literature review support</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses hierarchical generation with LLMs to construct topic trees and organizes content into structured sections; incorporates expert feedback loops for structural refinement of the generated survey.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Hierarchical generation driven by topic-tree construction; likely uses retrieval + prompting to extract content from multiple papers into nodes of the tree.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Topic-tree construction and hierarchical summarization with human-in-the-loop refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed to handle many papers across topics (exact counts not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review support across scientific domains; cited in context of AI literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Topic trees and structured survey drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper's brief mention; original work reports structural and user-centric evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to flat summarization pipelines; specifics are in original CHIME paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hierarchical organization plus expert feedback improves structural coherence over flat generation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May still lack fine-grained cross-document structural alignment (method-experiment chains) targeted by ResearchPulse.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Hierarchical decomposition intended to improve scalability, but no quantitative scaling trends are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4412.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BigSurvey / CAST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BigSurvey and CAST model (large-scale dataset & structured summarizer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BigSurvey provides a large-scale dataset for survey generation; CAST combines sentence classification with sparse-transformer-based generation for structured summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BigSurvey / CAST</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BigSurvey curates a large dataset for academic survey tasks; CAST uses sentence-level classification plus a sparse-transformer generator to produce structured, abstracted summarizations aimed at survey construction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Sparse-transformer-based generation (CAST); exact model names unspecified in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Sentence classification to select salient content followed by transformer-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Structured generation from selected sentences to form survey-style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Large-scale dataset implies many papers across topics (exact counts in original BigSurvey work).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic survey generation, applied to multiple scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured survey drafts and large-scale summarization outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Common summarization metrics and human evaluation (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to standard summarization models and baselines in the original BigSurvey work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large curated datasets plus targeted generation architectures improve structural aspects of survey writing, but often miss deep cross-document reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tends to produce high-level overviews lacking fine-grained methodological trajectory extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed for large-scale survey generation; dataset-scale helps but does not fully address structured cross-document inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4412.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveyX (attribute-based survey generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that refines automatic survey generation via attribute-based preprocessing and information templates to improve factual consistency and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Preprocesses documents into attribute templates and generates attribute forests that guide LLM generation, aiming to improve factual coverage and consistency in generated surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Attribute-based preprocessing to extract structured fields used as generation priors.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Template-driven synthesis using attribute forests to constrain LLM outputs and improve coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Intended for multi-document survey contexts (exact numbers unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Attribute-structured survey sections; evaluation metrics for factual consistency and coverage proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Factual consistency and coverage metrics (as described in the original SurveyX work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attribute templates help constrain LLM generation and improve factual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Template coverage and extraction quality limit performance; not focused on method-experiment chain reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Template-based approaches can scale but require robust extraction pipelines for many documents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4412.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite (LLM agent with human workflow guidance for comparative literature summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent framework that supports comparative literature summaries via guided human workflows and agentic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatcite: Llm agent with human workflow guidance for comparative literature summary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-driven agent that integrates human workflow guidance to produce comparative literature summaries, focusing on structured comparisons and human-in-the-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified in this summary); likely uses contemporary instruction-tuned LLMs per original work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Guided prompting with human-in-the-loop verification and comparative extraction templates.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Comparative summarization that emphasizes differences/similarities across documents with human workflow scaffolding.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for multi-document comparisons; exact scales depend on use-case.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Comparative literature summaries (general academic use).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative summaries and structured comparative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>User-centric and qualitative assessments with human evaluators (as per original ChatCite work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human workflow guidance improves trust and comparative clarity compared to unguided LLM summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reliant on human oversight; may not fully automate deep cross-document structural inference.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Human-in-the-loop limits fully automated scaling but improves factual reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4412.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DOLPHIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DOLPHIN (closed-loop open-ended auto-research framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-loop framework that combines idea generation, experimental validation, literature retrieval, and feedback refinement to support iterative scientific inquiry with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DOLPHIN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates LLM-driven idea generation, literature retrieval, experiment design/validation, error-aware debugging, and iterative feedback to form a closed-loop research agent aimed at open-ended research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified here); original work describes agentic LLM usage with retrieval and tool-use.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented generation plus tool use for validation; extracts literature context to support idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative idea refinement by alternation between generation, validation, and feedback, synthesizing insights from retrieved literature and experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over retrieved literature as needed; not limited to fixed-size clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General research automation; demonstrated on scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas, experiment proposals, validation reports, iterative refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Task-dependent; original work evaluates via closed-loop success in example research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Closed-loop generation + validation improves robustness of generated research hypotheses and mitigates some hallucination via testing/validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reliant on external validation resources; still susceptible to grounding and attribution errors across literature.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed for iterative scaling but computational and validation costs grow with complexity of experiments and literature size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4412.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent (iterative idea generation over literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven system that leverages citation graphs and cross-domain knowledge, using multi-agent peer review to evaluate novelty and clarity of generated research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Leverages citation graphs and cross-domain retrieval to inform idea generation; employs multi-agent LLM peer-review loops to assess novelty and clarity of generated proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified in this summary).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Citation-graph-aware retrieval and context extraction to ground idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative generation with multi-agent peer-review to refine and synthesize ideas from multiple documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over retrieved literature and citation neighborhoods; scale varies by task (not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature and cross-domain research idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas, novelty assessments, iterative proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Original work evaluates novelty/clarity via peer-review simulations and qualitative metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating citation structure and peer-review-like evaluation improves novelty detection and idea refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Grounding across heterogeneous citation contexts and ensuring factual correctness remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales with retrieval breadth; performance depends on quality of citation graph and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4412.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1 / R1-Searcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 and R1-Searcher (RL-incentivized search/reasoning for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems that apply reinforcement learning to incentivize LLMs to perform external search and reasoning when knowledge gaps arise, improving factual grounding in multi-document contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepSeek-R1 / R1-Searcher</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses reinforcement learning to encourage LLM agents to perform external search actions and retrieval-aware rollouts when facing gaps, improving factual grounding by integrating search into the reasoning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified here); methods apply to instruction-tuned LLMs used as agents.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-aware rollouts; RL-based prompts incentivize search and information extraction from external sources.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval + RL-guided multi-hop reasoning to combine evidence across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed to query external corpora as needed; not fixed to a preset number of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General information-seeking and scientific literature grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved grounded answers, synthesized evidence from retrieved sources.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Retrieval and reasoning accuracy metrics, grounding/factuality (not fully enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incentivizing search behavior via RL can improve retrieval and reasoning when LLMs face knowledge gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>RL training complexity; reliance on quality of search backend; computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Intended to scale by leveraging search, but RL overhead may limit large-scale application without efficient infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4412.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e4412.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentic Reasoning (reasoning LLMs with tools for deep research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frameworks that equip LLMs with tool use and multi-agent coordination to support multi-hop inference, task decomposition, and deep research workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentic reasoning: Reasoning llms with tools for the deep research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A family of frameworks that connect LLMs to toolkits (search, execution, memory, code execution) and coordinate multiple specialized agents to perform complex multi-step scientific reasoning and synthesis across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (unspecified; framework-agnostic).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Tool-augmented retrieval and parsing, multi-agent question-answering and execution for extracting evidence from multiple sources.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Coordinated multi-agent workflows, tool calls, and iterative reasoning to synthesize conclusions from diverse evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Framework supports arbitrary numbers of documents retrieved via tools; no fixed limit stated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General deep research and multi-document inference.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multi-step inferences, synthesized reports, executable artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Varies by instantiation; may include task success rates, factuality, and retrieval metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tool augmentation and multi-agent coordination extend LLM capabilities for deeper, instrumented research tasks beyond single-shot summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tool orchestration complexity, error propagation across tools, and increased resource demands.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to handle larger, more complex tasks by decomposing them, but cost and system complexity scale with toolset and agent count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys <em>(Rating: 2)</em></li>
                <li>Chime: Llm-assisted hierarchical organization of scientific studies for literature review support <em>(Rating: 2)</em></li>
                <li>Chatcite: Llm agent with human workflow guidance for comparative literature summary <em>(Rating: 2)</em></li>
                <li>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>R1-searcher: Incentivizing the search capability in llms via reinforcement learning <em>(Rating: 1)</em></li>
                <li>Academic survey automation via large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4412",
    "paper_id": "paper-281103057",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "ResearchPulse",
            "name_full": "ResearchPulse (agent-based multi-document scientific inference system)",
            "brief_description": "A modular, instruction-driven agent system that extracts, aligns, and visualizes motivations, methods, and experimental results across thematically related papers to reconstruct research development chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ResearchPulse",
            "system_description": "An end-to-end, multi-agent pipeline that decomposes a user instruction via a Plan Agent and routes subtasks to specialized agents: Mmap-Agent for extracting motivation-method tuples and producing time-aligned mind maps, and Lchart-Agent for extracting experimental tables/metrics and producing temporal trend charts. The pipeline includes document parsing (PDF→markdown), section segmentation, citation-aware clustering, fine-tuned LLM extractors, metric normalization, and auto-generated visualization code (Python) executed via a compiler module. Outputs include hierarchical markdown chains rendered as mind maps and line charts.",
            "llm_model_used": "Plan Agent: Qwen-72B; Mmap-Agent: Qwen2.5-7B (fine-tuned 7B); Lchart-Agent: Qwen2.5-Coder-7B (fine-tuned 7B)",
            "extraction_technique": "Section-aware parsing (PDF→markdown) + fine-tuned LLM extraction models for structured fields; uses sentence-BERT for clustering and an Extraction module to pre-segment abstracts/intros/experiment sections before LLM field extraction.",
            "synthesis_technique": "Temporal alignment and ordering of extracted motivation-method tuples; aggregation and normalization of experimental tables across papers; generation of hierarchical markdown research chains and auto-generated visualization code (mind maps and line charts); multi-agent orchestration (plan→extract→synthesize→visualize).",
            "number_of_papers": "Designed to operate on document clusters; ResearchPulse-Bench contains 100 citation-aware clusters (average ~20 papers per cluster).",
            "domain_or_topic": "Computer science / AI research threads (arXiv and OpenReview-curated clusters).",
            "output_type": "Structured, time-aligned method-motivation chains (mind maps) and experimental trend visualizations (line charts) with generated Python visualization code.",
            "evaluation_metrics": "Method-Tracking: BERTScore (P/R/F1), METEOR, GPT-Score (fluency, relevance, accuracy, creativity, quality); Experimental-Analysis: Pass@1 (code execution), IS, FID, KID, CLIP-FID, LPIPS, CMMD, SSIM, PSNR, MS-SSIM.",
            "performance_results": "Mmap-Agent (task: method-tracking) BERTScore F1=90.39, METEOR=46.14, GPT-4o-evaluated fluency 92.16/quality 85.59; Lchart-Agent (experimental-analysis) Pass@1=97.50, FID=6.73, LPIPS=8.49, SSIM=55.56.",
            "comparison_baseline": "Compared against open-source and closed-source models including Qwen2.5-7B-Instruct, InternLM3-8B-Instruct, Llama-3.1-8B-Instruct, CodeLlama-7B, Qwen2.5-Coder-7B-Instruct, GPT-4o, Claude-3.7-Sonnet, Gemini-1.5 Pro.",
            "performance_vs_baseline": "ResearchPulse's specialized 7B agents (Mmap and Lchart) outperform larger or closed-source baselines on targeted metrics: Mmap-Agent exceeds GPT-4o on BERTScore/METEOR/GPT-Score; Lchart-Agent achieves higher Pass@1 (97.50 vs GPT-4o's 96.25) and lower FID (6.73 vs higher baselines).",
            "key_findings": "A modular multi-agent architecture with task-specific fine-tuned 7B LLMs can achieve strong cross-document semantic alignment and high-fidelity visualization, often matching or exceeding larger closed-source models; temporal alignment and structured extraction are crucial for reconstructing methodological trajectories.",
            "limitations_challenges": "Observed errors include misattributed innovations, missing-extraction of method components, factual deviations in reported metrics, and partial references; reliance on PDF→markdown parsing and on prompt/annotation quality; nontrivial human validation required; potential hallucination and attribution drift across documents.",
            "scaling_behavior": "System was evaluated on 100 clusters (avg ~20 papers); authors emphasize strong performance at 7B model scale, noting that specialized fine-tuning and modular design yield better task-specific results than naively scaling a single large LLM. No extensive claim about linear scaling with number of papers; very long experimental-analysis instances (avg ~14K tokens) require robust parsing and alignment.",
            "uuid": "e4412.0",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ResearchPulse-Bench",
            "name_full": "ResearchPulse-Bench (citation-aware benchmark)",
            "brief_description": "A benchmark dataset of 100 citation-aware annotated paper clusters designed to supervise and evaluate multi-document scientific inference tasks such as Method-Tracking and Experimental-Analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ResearchPulse-Bench",
            "system_description": "Curated clusters from arXiv and OpenReview with metadata extraction, PDF-to-markdown parsing, section segmentation, semantic clustering via Sentence-BERT + K-Means, and human-validated annotations of motivations, methods, experimental tables, metrics, and citation links; includes reference outputs (mind maps and line charts).",
            "llm_model_used": "Used model-assisted extraction during construction (DeepSeek-R1 or GPT-4o were used to extract fine-grained elements before human validation).",
            "extraction_technique": "Annotation pipeline combining LLM-assisted extraction (DeepSeek-R1 / GPT-4o) with multi-stage human inspection; sentence-BERT embeddings for clustering.",
            "synthesis_technique": "Annotated fields are rendered into hierarchical markdown and visualizations; supports evaluation of temporal alignment and experimental trend synthesis.",
            "number_of_papers": "100 document clusters; average ~20 papers per cluster (clusters range from 6 to 33+ papers).",
            "domain_or_topic": "AI / computer science research (arXiv and OpenReview sourced).",
            "output_type": "Gold-standard method-tracking chains (mind maps) and experimental-analysis outputs (structured tables and trend charts).",
            "evaluation_metrics": "Enables evaluation with BERTScore, METEOR, GPT-Score for textual outputs and IS, FID, KID, CLIP-FID, LPIPS, CMMD, SSIM, PSNR, MS-SSIM, and pass@1 for visualization/code outputs.",
            "performance_results": "Dataset statistics: Method-Tracking avg input ~1.1K tokens, output ~350 tokens; Experimental-Analysis avg input ~14K tokens, output ~900 tokens. (These are dataset stats rather than model performance.)",
            "comparison_baseline": "N/A (benchmark used to evaluate ResearchPulse and various LLM baselines).",
            "performance_vs_baseline": "N/A (benchmark facilitates comparative evaluation reported elsewhere in the paper).",
            "key_findings": "Citation-aware clustering and human-validated LLM-assisted annotation yield high-quality supervision for cross-document structural inference; highlights need for long-context handling and structured extraction to evaluate method-level reasoning.",
            "limitations_challenges": "Annotation and validation are labor-intensive; clusters were restricted to open-access sources; long experimental tables create heavy context length requirements for models.",
            "scaling_behavior": "Benchmark scale fixed at 100 clusters; cluster sizes vary — larger clusters increase extraction and alignment complexity and token-length demands.",
            "uuid": "e4412.1",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Plan Agent",
            "name_full": "Plan Agent (ResearchPulse component)",
            "brief_description": "Central controller agent that classifies user intent (method-tracking or experimental-analysis), decomposes instructions, and routes documents and subtasks to downstream extraction/synthesis agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Plan Agent",
            "system_description": "Uses an LLM (Qwen-72B in experiments) to interpret the user instruction, decide task routing, invoke the Extraction module for document parsing, and dispatch per-document instructions (I_method or I_exp) to Mmap-Agent or Lchart-Agent; coordinates multi-agent workflow and tool invocation (e.g., PDF parsers, compilers).",
            "llm_model_used": "Qwen-72B (as reported in experiments).",
            "extraction_technique": "Not an extractor itself; it invokes the Extraction module to parse docs into segmented sections and then routes segments to specialized extractors.",
            "synthesis_technique": "Orchestrates synthesis by delegating to Mmap-Agent and Lchart-Agent and aggregating their outputs for final visualization rendering.",
            "number_of_papers": "Operates over the input cluster size provided by user (bench clusters avg ~20 papers); scalable to cluster sizes used in ResearchPulse-Bench.",
            "domain_or_topic": "General within the paper: AI research literature orchestration.",
            "output_type": "Task plan and orchestration decisions; triggers downstream structured outputs (mind maps, line charts).",
            "evaluation_metrics": "Indirectly evaluated via downstream task metrics (method-tracking & experimental-analysis performance).",
            "performance_results": "Plan Agent was implemented with Qwen-72B; no separate numeric ablation results reported specifically isolating Plan Agent performance beyond overall system.",
            "comparison_baseline": "Not isolated; part of overall ResearchPulse comparisons.",
            "performance_vs_baseline": "Not isolated.",
            "key_findings": "Centralized instruction planning aids modular decomposition and improves downstream agent specialization; choice of Plan Agent model (large Qwen-72B) supports robust intent classification and routing.",
            "limitations_challenges": "Requires reliable document parsing and clear instruction decomposition; potential bottleneck if orchestration fails or misclassifies intent.",
            "scaling_behavior": "Larger Plan Agent capacity (Qwen-72B used) was chosen to stabilize instruction interpretation; scaling of Plan Agent not deeply explored.",
            "uuid": "e4412.2",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Mmap-Agent",
            "name_full": "Mmap-Agent (Motivation-Method mapping agent)",
            "brief_description": "A fine-tuned LLM agent that extracts motivation-method tuples from paper abstracts/intros and constructs temporally aligned method-tracking chains for mind-map visualizations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Mmap-Agent",
            "system_description": "Fine-tuned LLM (Qwen2.5-7B) trained to extract structured tuples (motivation, method) from input document segments given I_method; outputs are sorted by publication timestamp to produce a hierarchical markdown M_chain and rendered into a mind map. Training optimizes negative log-likelihood over annotated tuples.",
            "llm_model_used": "Qwen2.5-7B (fine-tuned for structural extraction).",
            "extraction_technique": "Supervised fine-tuned extraction conditioned on segmented inputs (abstract/introduction); model produces (m_i, r_i) tuples via structured prompting and supervised sequence modeling.",
            "synthesis_technique": "Temporal sorting and hierarchical markdown generation; outputs fed into visualization renderer to produce mind maps.",
            "number_of_papers": "Evaluated on clusters in ResearchPulse-Bench (avg ~20 papers per cluster); training used manually annotated sequences across representative paper series (exact training count not specified).",
            "domain_or_topic": "AI research papers (methodological evolution analysis).",
            "output_type": "Structured textual representations of motivation-method tuples and mind-map-ready markdown.",
            "evaluation_metrics": "BERTScore (P/R/F1), METEOR, GPT-Score (fluency, relevance, accuracy, creativity, quality).",
            "performance_results": "Mmap-Agent achieved BERTScore F1=90.39, METEOR=46.14, and top GPT-4o-evaluated fluency/quality metrics (fluency 92.16, quality 85.59).",
            "comparison_baseline": "Compared with Qwen2.5-7B-Instruct, InternLM3-8B, Llama-3.1-8B, CodeLlama-7B, Qwen2.5-Coder-7B-Instruct, GPT-4o, Claude-3.7-Sonnet, Gemini-1.5 Pro.",
            "performance_vs_baseline": "Outperformed baselines on aggregate metrics (higher BERTScore F1 and GPT-4o-evaluated scores); ablations show dependence on auxiliary GPT-4o and Compiler modules for best fluency and precision.",
            "key_findings": "Task-specific fine-tuning on structured motivation-method pairs substantially improves semantic alignment; modular design and auxiliary modules (GPT-4o, Compiler) enhance fluency and precision.",
            "limitations_challenges": "Susceptible to misattribution and missing-extraction errors; sensitive to annotation quality and phrase-level ambiguity in abstracts/intros.",
            "scaling_behavior": "Achieves strong results at 7B scale when fine-tuned; removing auxiliary modules reduces performance—indicating component interactions matter more than pure parameter count.",
            "uuid": "e4412.3",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Lchart-Agent",
            "name_full": "Lchart-Agent (Experimental table extraction and charting agent)",
            "brief_description": "A fine-tuned LLM agent that extracts experimental tables, model names, evaluation metrics, and baseline years from papers and synthesizes temporal trend charts via auto-generated Python code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Lchart-Agent",
            "system_description": "Fine-tuned agent (Qwen2.5-Coder-7B) specialized for tabular extraction, metric normalization, and visualization code generation; it outputs structured tuples (T_i, M_i, E_i, Y_i), aligns them temporally to create E_chain, and uses a visualization module V to render charts (via generated Python) with a Compiler module to execute/validate code.",
            "llm_model_used": "Qwen2.5-Coder-7B (fine-tuned 7B).",
            "extraction_technique": "Field-level supervised extraction of tables and metrics from parsed experiment sections; normalization of metric names/units; alignment across datasets and cited baselines.",
            "synthesis_technique": "Temporal alignment of experimental results into trend records; automatic generation of plotting code and image rendering; uses verification via code execution (pass@1 metric).",
            "number_of_papers": "Operates on document clusters (ResearchPulse-Bench average ~20 papers per cluster); trained on a curated dataset of aligned tables and metric annotations (exact training size not specified).",
            "domain_or_topic": "AI experimental comparisons (benchmark performance trajectories).",
            "output_type": "Structured experimental records, auto-generated Python plotting code, and rendered line charts.",
            "evaluation_metrics": "Pass@1, IS, FID, KID, CLIP-FID, LPIPS, CMMD, SSIM, PSNR, MS-SSIM.",
            "performance_results": "Lchart-Agent: Pass@1=97.50, FID=6.73, LPIPS=8.49, CMMD=4.06, SSIM=55.56 (outperforming most baselines); ablations show removing GPT-4o and Compiler reduces IS/FID and Pass@1.",
            "comparison_baseline": "Compared against Qwen2.5-7B-Instruct, InternLM3-8B-Instruct, Llama-3.1-8B-Instruct, CodeLlama-7B, Qwen2.5-Coder-7B-Instruct, GPT-4o, Claude-3.7-Sonnet, Gemini-1.5 Pro.",
            "performance_vs_baseline": "Lchart-Agent achieves higher Pass@1 (97.50) than GPT-4o (96.25) and lower FID than most baselines, indicating superior code executability and visual fidelity.",
            "key_findings": "Specialized fine-tuning for tabular extraction plus a compiler verification loop yields high functional correctness and visual fidelity; normalization across heterogeneous tables is critical.",
            "limitations_challenges": "Factual deviations and partial-reference errors observed; parsing noisy PDFs and heterogeneous table formats remains challenging; reliance on code-execution increases system complexity.",
            "scaling_behavior": "Performs well on clusters with long-context experimental data (~14K tokens); ablation shows component additions (GPT-4o, Compiler) matter more than sheer model size for quality.",
            "uuid": "e4412.4",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey (automatic survey generation pipeline)",
            "brief_description": "A four-stage pipeline that uses multiple LLMs to automate survey writing via retrieval, outline generation, section-wise writing, and optimization with co-generation and critique.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AutoSurvey",
            "system_description": "Pipeline comprising retrieval of relevant literature, LLM-based outline generation, section-by-section drafting by LLMs, and an optimization/critique phase where multiple LLMs co-generate and critique drafts to improve structure and depth.",
            "llm_model_used": "Multiple LLMs (unspecified in this paper's description); described generically as LLM-based co-generation and critique.",
            "extraction_technique": "Retrieval-based selection of documents followed by LLM-driven extraction and section drafting; likely uses prompting and chunking for long contexts.",
            "synthesis_technique": "Hierarchical pipeline combining retrieval→outline→section generation→iterative critique to synthesize multi-document surveys.",
            "number_of_papers": "Designed for multi-document survey writing (number unspecified).",
            "domain_or_topic": "Generic academic survey generation (applied to AI literature in examples).",
            "output_type": "Full literature surveys / structured reviews.",
            "evaluation_metrics": "Reported evaluation metrics in prior work typically include coverage, factuality, and human evaluation; this paper cites AutoSurvey as a related pipeline without detailed metrics here.",
            "performance_results": "",
            "comparison_baseline": "Compared conceptually to traditional multi-document summarization systems; exact baselines in AutoSurvey original work not enumerated here.",
            "performance_vs_baseline": "",
            "key_findings": "Multi-stage LLM pipelines can speed up survey drafting and improve organization when paired with critique/co-generation loops, but depth and grounding remain concerns.",
            "limitations_challenges": "Depth and factual grounding issues; potential hallucinations and lack of fine-grained methodological trajectory extraction.",
            "scaling_behavior": "Designed to scale with document collections but suffers from long-context and grounding challenges as input size grows.",
            "uuid": "e4412.5",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "CHIME",
            "name_full": "CHIME (LLM-assisted hierarchical organization for literature review)",
            "brief_description": "An LLM-assisted hierarchical generation system that builds topic trees and integrates expert feedback for structured survey organization.",
            "citation_title": "Chime: Llm-assisted hierarchical organization of scientific studies for literature review support",
            "mention_or_use": "mention",
            "system_name": "CHIME",
            "system_description": "Uses hierarchical generation with LLMs to construct topic trees and organizes content into structured sections; incorporates expert feedback loops for structural refinement of the generated survey.",
            "llm_model_used": "LLMs (unspecified in this paper's summary).",
            "extraction_technique": "Hierarchical generation driven by topic-tree construction; likely uses retrieval + prompting to extract content from multiple papers into nodes of the tree.",
            "synthesis_technique": "Topic-tree construction and hierarchical summarization with human-in-the-loop refinement.",
            "number_of_papers": "Designed to handle many papers across topics (exact counts not specified here).",
            "domain_or_topic": "Literature review support across scientific domains; cited in context of AI literature.",
            "output_type": "Topic trees and structured survey drafts.",
            "evaluation_metrics": "Not specified in this paper's brief mention; original work reports structural and user-centric evaluation.",
            "performance_results": "",
            "comparison_baseline": "Compared conceptually to flat summarization pipelines; specifics are in original CHIME paper.",
            "performance_vs_baseline": "",
            "key_findings": "Hierarchical organization plus expert feedback improves structural coherence over flat generation approaches.",
            "limitations_challenges": "May still lack fine-grained cross-document structural alignment (method-experiment chains) targeted by ResearchPulse.",
            "scaling_behavior": "Hierarchical decomposition intended to improve scalability, but no quantitative scaling trends are reported here.",
            "uuid": "e4412.6",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "BigSurvey / CAST",
            "name_full": "BigSurvey and CAST model (large-scale dataset & structured summarizer)",
            "brief_description": "BigSurvey provides a large-scale dataset for survey generation; CAST combines sentence classification with sparse-transformer-based generation for structured summaries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "BigSurvey / CAST",
            "system_description": "BigSurvey curates a large dataset for academic survey tasks; CAST uses sentence-level classification plus a sparse-transformer generator to produce structured, abstracted summarizations aimed at survey construction.",
            "llm_model_used": "Sparse-transformer-based generation (CAST); exact model names unspecified in this paper's summary.",
            "extraction_technique": "Sentence classification to select salient content followed by transformer-based generation.",
            "synthesis_technique": "Structured generation from selected sentences to form survey-style outputs.",
            "number_of_papers": "Large-scale dataset implies many papers across topics (exact counts in original BigSurvey work).",
            "domain_or_topic": "Academic survey generation, applied to multiple scientific domains.",
            "output_type": "Structured survey drafts and large-scale summarization outputs.",
            "evaluation_metrics": "Common summarization metrics and human evaluation (not detailed here).",
            "performance_results": "",
            "comparison_baseline": "Compared to standard summarization models and baselines in the original BigSurvey work.",
            "performance_vs_baseline": "",
            "key_findings": "Large curated datasets plus targeted generation architectures improve structural aspects of survey writing, but often miss deep cross-document reasoning.",
            "limitations_challenges": "Tends to produce high-level overviews lacking fine-grained methodological trajectory extraction.",
            "scaling_behavior": "Designed for large-scale survey generation; dataset-scale helps but does not fully address structured cross-document inference.",
            "uuid": "e4412.7",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "SurveyX",
            "name_full": "SurveyX (attribute-based survey generation)",
            "brief_description": "A system that refines automatic survey generation via attribute-based preprocessing and information templates to improve factual consistency and coverage.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SurveyX",
            "system_description": "Preprocesses documents into attribute templates and generates attribute forests that guide LLM generation, aiming to improve factual coverage and consistency in generated surveys.",
            "llm_model_used": "LLMs (unspecified in this paper's summary).",
            "extraction_technique": "Attribute-based preprocessing to extract structured fields used as generation priors.",
            "synthesis_technique": "Template-driven synthesis using attribute forests to constrain LLM outputs and improve coverage.",
            "number_of_papers": "Intended for multi-document survey contexts (exact numbers unspecified).",
            "domain_or_topic": "Academic survey generation.",
            "output_type": "Attribute-structured survey sections; evaluation metrics for factual consistency and coverage proposed.",
            "evaluation_metrics": "Factual consistency and coverage metrics (as described in the original SurveyX work).",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Attribute templates help constrain LLM generation and improve factual grounding.",
            "limitations_challenges": "Template coverage and extraction quality limit performance; not focused on method-experiment chain reconstruction.",
            "scaling_behavior": "Template-based approaches can scale but require robust extraction pipelines for many documents.",
            "uuid": "e4412.8",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite (LLM agent with human workflow guidance for comparative literature summary)",
            "brief_description": "An LLM-agent framework that supports comparative literature summaries via guided human workflows and agentic prompts.",
            "citation_title": "Chatcite: Llm agent with human workflow guidance for comparative literature summary",
            "mention_or_use": "mention",
            "system_name": "ChatCite",
            "system_description": "An LLM-driven agent that integrates human workflow guidance to produce comparative literature summaries, focusing on structured comparisons and human-in-the-loop verification.",
            "llm_model_used": "LLMs (unspecified in this summary); likely uses contemporary instruction-tuned LLMs per original work.",
            "extraction_technique": "Guided prompting with human-in-the-loop verification and comparative extraction templates.",
            "synthesis_technique": "Comparative summarization that emphasizes differences/similarities across documents with human workflow scaffolding.",
            "number_of_papers": "Designed for multi-document comparisons; exact scales depend on use-case.",
            "domain_or_topic": "Comparative literature summaries (general academic use).",
            "output_type": "Comparative summaries and structured comparative analyses.",
            "evaluation_metrics": "User-centric and qualitative assessments with human evaluators (as per original ChatCite work).",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Human workflow guidance improves trust and comparative clarity compared to unguided LLM summarization.",
            "limitations_challenges": "Reliant on human oversight; may not fully automate deep cross-document structural inference.",
            "scaling_behavior": "Human-in-the-loop limits fully automated scaling but improves factual reliability.",
            "uuid": "e4412.9",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "DOLPHIN",
            "name_full": "DOLPHIN (closed-loop open-ended auto-research framework)",
            "brief_description": "A closed-loop framework that combines idea generation, experimental validation, literature retrieval, and feedback refinement to support iterative scientific inquiry with LLMs.",
            "citation_title": "Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback",
            "mention_or_use": "mention",
            "system_name": "DOLPHIN",
            "system_description": "Integrates LLM-driven idea generation, literature retrieval, experiment design/validation, error-aware debugging, and iterative feedback to form a closed-loop research agent aimed at open-ended research tasks.",
            "llm_model_used": "LLMs (unspecified here); original work describes agentic LLM usage with retrieval and tool-use.",
            "extraction_technique": "Retrieval-augmented generation plus tool use for validation; extracts literature context to support idea generation.",
            "synthesis_technique": "Iterative idea refinement by alternation between generation, validation, and feedback, synthesizing insights from retrieved literature and experimental outcomes.",
            "number_of_papers": "Operates over retrieved literature as needed; not limited to fixed-size clusters.",
            "domain_or_topic": "General research automation; demonstrated on scientific tasks.",
            "output_type": "Research ideas, experiment proposals, validation reports, iterative refinements.",
            "evaluation_metrics": "Task-dependent; original work evaluates via closed-loop success in example research tasks.",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Closed-loop generation + validation improves robustness of generated research hypotheses and mitigates some hallucination via testing/validation.",
            "limitations_challenges": "Reliant on external validation resources; still susceptible to grounding and attribution errors across literature.",
            "scaling_behavior": "Designed for iterative scaling but computational and validation costs grow with complexity of experiments and literature size.",
            "uuid": "e4412.10",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent (iterative idea generation over literature)",
            "brief_description": "An LLM-driven system that leverages citation graphs and cross-domain knowledge, using multi-agent peer review to evaluate novelty and clarity of generated research ideas.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "system_name": "ResearchAgent",
            "system_description": "Leverages citation graphs and cross-domain retrieval to inform idea generation; employs multi-agent LLM peer-review loops to assess novelty and clarity of generated proposals.",
            "llm_model_used": "LLMs (unspecified in this summary).",
            "extraction_technique": "Citation-graph-aware retrieval and context extraction to ground idea generation.",
            "synthesis_technique": "Iterative generation with multi-agent peer-review to refine and synthesize ideas from multiple documents.",
            "number_of_papers": "Operates over retrieved literature and citation neighborhoods; scale varies by task (not specified).",
            "domain_or_topic": "General scientific literature and cross-domain research idea generation.",
            "output_type": "Research ideas, novelty assessments, iterative proposals.",
            "evaluation_metrics": "Original work evaluates novelty/clarity via peer-review simulations and qualitative metrics.",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Incorporating citation structure and peer-review-like evaluation improves novelty detection and idea refinement.",
            "limitations_challenges": "Grounding across heterogeneous citation contexts and ensuring factual correctness remain challenging.",
            "scaling_behavior": "Scales with retrieval breadth; performance depends on quality of citation graph and retrieval.",
            "uuid": "e4412.11",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "DeepSeek-R1 / R1-Searcher",
            "name_full": "DeepSeek-R1 and R1-Searcher (RL-incentivized search/reasoning for LLMs)",
            "brief_description": "Systems that apply reinforcement learning to incentivize LLMs to perform external search and reasoning when knowledge gaps arise, improving factual grounding in multi-document contexts.",
            "citation_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "DeepSeek-R1 / R1-Searcher",
            "system_description": "Uses reinforcement learning to encourage LLM agents to perform external search actions and retrieval-aware rollouts when facing gaps, improving factual grounding by integrating search into the reasoning loop.",
            "llm_model_used": "LLMs (unspecified here); methods apply to instruction-tuned LLMs used as agents.",
            "extraction_technique": "Retrieval-aware rollouts; RL-based prompts incentivize search and information extraction from external sources.",
            "synthesis_technique": "Retrieval + RL-guided multi-hop reasoning to combine evidence across documents.",
            "number_of_papers": "Designed to query external corpora as needed; not fixed to a preset number of papers.",
            "domain_or_topic": "General information-seeking and scientific literature grounding.",
            "output_type": "Improved grounded answers, synthesized evidence from retrieved sources.",
            "evaluation_metrics": "Retrieval and reasoning accuracy metrics, grounding/factuality (not fully enumerated here).",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Incentivizing search behavior via RL can improve retrieval and reasoning when LLMs face knowledge gaps.",
            "limitations_challenges": "RL training complexity; reliance on quality of search backend; computational cost.",
            "scaling_behavior": "Intended to scale by leveraging search, but RL overhead may limit large-scale application without efficient infrastructure.",
            "uuid": "e4412.12",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Agentic Reasoning",
            "name_full": "Agentic Reasoning (reasoning LLMs with tools for deep research)",
            "brief_description": "Frameworks that equip LLMs with tool use and multi-agent coordination to support multi-hop inference, task decomposition, and deep research workflows.",
            "citation_title": "Agentic reasoning: Reasoning llms with tools for the deep research",
            "mention_or_use": "mention",
            "system_name": "Agentic Reasoning",
            "system_description": "A family of frameworks that connect LLMs to toolkits (search, execution, memory, code execution) and coordinate multiple specialized agents to perform complex multi-step scientific reasoning and synthesis across documents.",
            "llm_model_used": "LLMs (unspecified; framework-agnostic).",
            "extraction_technique": "Tool-augmented retrieval and parsing, multi-agent question-answering and execution for extracting evidence from multiple sources.",
            "synthesis_technique": "Coordinated multi-agent workflows, tool calls, and iterative reasoning to synthesize conclusions from diverse evidence.",
            "number_of_papers": "Framework supports arbitrary numbers of documents retrieved via tools; no fixed limit stated.",
            "domain_or_topic": "General deep research and multi-document inference.",
            "output_type": "Multi-step inferences, synthesized reports, executable artifacts.",
            "evaluation_metrics": "Varies by instantiation; may include task success rates, factuality, and retrieval metrics.",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Tool augmentation and multi-agent coordination extend LLM capabilities for deeper, instrumented research tasks beyond single-shot summarization.",
            "limitations_challenges": "Tool orchestration complexity, error propagation across tools, and increased resource demands.",
            "scaling_behavior": "Designed to handle larger, more complex tasks by decomposing them, but cost and system complexity scale with toolset and agent count.",
            "uuid": "e4412.13",
            "source_info": {
                "paper_title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Chime: Llm-assisted hierarchical organization of scientific studies for literature review support",
            "rating": 2,
            "sanitized_title": "chime_llmassisted_hierarchical_organization_of_scientific_studies_for_literature_review_support"
        },
        {
            "paper_title": "Chatcite: Llm agent with human workflow guidance for comparative literature summary",
            "rating": 2,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        },
        {
            "paper_title": "Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback",
            "rating": 2,
            "sanitized_title": "dolphin_closedloop_openended_autoresearch_through_thinking_practice_and_feedback"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "R1-searcher: Incentivizing the search capability in llms via reinforcement learning",
            "rating": 1,
            "sanitized_title": "r1searcher_incentivizing_the_search_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Academic survey automation via large language models",
            "rating": 2,
            "sanitized_title": "academic_survey_automation_via_large_language_models"
        }
    ],
    "cost": 0.02413225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference
3 Sep 2025</p>
<p>Qi Chen 
Jingxuan Wei weijingxuan20@mails.ucas.edu.cn 
Zhuoya Yao 
Haiguang Wang 
Gaowei Wu 
Bihui Yu 
Siyuan Li 
Cheng Tan chengtan9907@gmail.com 
Al ∂∂ </p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>University of Chinese Academy of Sciences
BeijingChina</p>
<p>Zhejiang University Hangzhou
China</p>
<p>Shanghai Artificial Intelligence Laboratory Shanghai
China</p>
<p>MM '25
October 27-312025DublinIreland</p>
<p>ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference
3 Sep 20251167FE941E5A1C9C3F913972F1411B7D10.1145/3746027.3754813arXiv:2509.03565v1[cs.CL]Multi-document scientific inferenceagent-based research analysismethod-trackingexperimental result alignmentresearch trajectory visualization
Understanding how scientific ideas evolve requires more than summarizing individual papers-it demands structured, cross-document reasoning over thematically related research.In this work, we formalize multi-document scientific inference, a new task that extracts and aligns motivation, methodology, and experimental results across related papers to reconstruct research development chains.This task introduces key challenges, including temporally aligning loosely structured methods and standardizing heterogeneous experimental tables.We present ResearchPulse, an agent-based framework that integrates instruction planning, scientific content extraction, and structured visualization.It consists of three coordinated agents: a Plan Agent for task decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a Lchart-Agent that synthesizes experimental line charts.To support this task, we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper clusters.Experiments show that our system, despite using 7B-scale agents, consistently outperforms strong baselines like GPT-4o in semantic alignment, structural consistency, and visual fidelity.The dataset are available in https://huggingface.co/ datasets/ResearchPulse/ResearchPulse-BenchCCS Concepts• Applied computing → Document management and text processing; • Computing methodologies → Artificial intelligence.</p>
<p>Introduction</p>
<p>Scientific research is cumulative and comparative.To truly understand how a field evolves, researchers often need to examine multiple thematically related papers together [1][2][3], analyze their motivations and methodological innovations, and track how experimental performance changes over time [4][5][6].This type of structured, cross-document analysis is essential for identifying trends, gaps, and breakthroughs in fast-developing domains like artificial intelligence.However, current summarization models [7][8][9][10] and automatic survey generators [11][12][13][14] focus either on compressing individual papers or generating high-level overviews, lacking the granularity needed to uncover methodological trajectories and benchmark shifts.Such limitations hinder the ability to track how cutting-edge techniques evolve, converge, or diverge across papers over time.Meanwhile, emerging deep research agents [15][16][17][18][19][20] offer promising automation but are either domain-agnostic or lack structural precision.We introduce a new task, multi-document scientific inference, which involves extracting and aligning key research elements-such as motivation, methodology, and experimental results-across thematically related papers to reconstruct structured development chains.As illustrated in Figure 1, this task differs fundamentally from traditional summarization or survey generation, as I would like to analyze the connections between the research motivations and contributions of multiple papers, understand the development trends in the field, and compare the performance of various models over time. . .Current models cannot directly reveal the relationships between the research motivations and contributions of multiple papers, nor track how model performance changes over time.</p>
<p>Current models Illogical</p>
<p>ResearchPulse</p>
<p>Analyze the … motivations … multiple papers ...</p>
<p>Line Chart</p>
<p>Compare the … various models… multiple papers ...</p>
<p>Mind Map</p>
<p>Awesome, this is exactly what I wanted! it requires not only semantic abstraction but also structural parsing and scientific reasoning across documents.This process introduces several core challenges: (1) identifying and temporally aligning motivation-method pairs from diverse, loosely structured textual sources, and (2) extracting heterogeneous experimental tables and converting them into unified, interpretable visual trends.These challenges make multi-document scientific inference a structurally grounded and underexplored problem, distinct from prior efforts in academic summarization or auto-survey pipelines.</p>
<p>To support this new task, we introduce ResearchPulse-Bench, a benchmark dataset constructed from citation-aware paper clusters curated from arXiv and OpenReview.Each cluster contains semantically related papers aligned by topic and publication timeline.We annotate core scientific elements-motivations, methods, experimental tables, evaluation metrics, and citation links-using model-assisted extraction techniques, followed by rigorous human validation to ensure structural integrity and factual correctness.This enables high-quality supervision for both Method-Tracking and Experimental-Analysis tasks, supporting model training and standardized evaluation.</p>
<p>Building on this benchmark, we propose ResearchPulse, an endto-end agent system for structured scientific inference.The system begins with a Plan Agent that interprets user instructions and coordinates the workflow.For method-tracking, a Mmap-Agent extracts motivation-method pairs from related papers and organizes them into temporally aligned mind maps.For experimental-analysis, a Lchart-Agent identifies experimental tables, model names, and evaluation metrics, and synthesizes benchmark trajectories as line charts.Each component operates in a modular yet coordinated manner, enabling ResearchPulse to transform loosely connected papers into coherent, interpretable research chains and support dynamic tracking of scientific progress.</p>
<p>Our contributions are as follows:</p>
<p>• We formally define the task of multi-document scientific inference, which focuses on extracting and aligning motivation, method, and experiment elements from thematically related papers to reconstruct structured research development chains.• We introduce ResearchPulse-Bench, a citation-aware benchmark dataset consisting of annotated document clusters and reference outputs for method-tracking and experimentalanalysis.</p>
<p>• We propose ResearchPulse, a modular agent system that integrates instruction planning, scientific content extraction, and structure-aware visualization into a unified pipeline for tracking methodological evolution and experimental trends in AI research.</p>
<p>Related Work 2.1 Scientific Document Summarization</p>
<p>Scientific document summarization aims to condense long and structurally complex academic texts while preserving core semantic content.Early approaches like TextRank [21] and LexRank [22] rely on surface-level lexical features and often fail to capture discourselevel dependencies [23].Neural and transformer-based models such as BERTSum [7] and LoBART [8] introduce deep semantic modeling but remain limited by context length and generation inconsistencies.Later methods like HEGEL [9] and HAESum [10] improve global-local relation modeling through hypergraph and hierarchical attention, though they often depend on noisy tools like LDA or KeyBERT [24].Structure-aware models-such as dependency-based discourse parsers [25] and sentence compression with anaphora constraints [26]-seek better coherence at the cost of increased system complexity.Document expansion [27] introduces external context to improve coverage but risks topic drift.Models like BooookScore [28] and top-down inference frameworks [29] target book-length summarization but still struggle with scientific logic modeling.Overall, while single-document methods evolve in modeling semantics and structure, they often overlook the underlying scientific logic-motivation, method, experiment-and lack mechanisms for linking content across documents.Multi-document summarization requires integrating redundant or conflicting information from multiple sources.Hybrid frameworks such as Hi-MAP [30], SKT5SciSumm [31], and REFLECT [32] combine extraction with generation but often flatten documentspecific context.Graph-based systems like CeRA [33] and crossdocument information graphs [34] offer improved factual grounding but mainly focus on entity-event relations.Other methods highlight diversity or specificity-e.g., DisentangleSum [35] and DIVERSESUMM [36]-while large-scale pipelines like GPT-based summarizers [37] use recursive generation and clustering to scale.Despite these advances, few approaches address the extraction or alignment of structured scientific reasoning.Evaluation studies confirm this limitation, showing that many benchmarks fail to assess true cross-document synthesis [35,36].These gaps underscore the need for systems that can extract, align, and reason over structured scientific content across thematically related works.</p>
<p>Automatic Survey Generation</p>
<p>Automatic survey generation seeks to streamline the labor-intensive process of writing literature surveys by leveraging large language models (LLMs).Traditional multi-document summarization methods, designed for small input sets and superficial summaries, fall short for this task.Recent works propose more targeted pipelines.For instance, BigSurvey introduces a large-scale dataset and the CAST model, combining sentence classification and sparse-transformerbased generation for structured and abstracted summarization [11].However, such outputs often lack critical synthesis.To enhance organization, CHIME uses hierarchical generation with LLMs to build topic trees and incorporates expert feedback for structural refinement [12].AutoSurvey adopts a four-stage pipeline-retrieval, outline generation, section-wise writing, and optimization-where multiple LLMs co-generate and critique survey drafts, improving efficiency but still facing depth and grounding issues [13].SurveyX further refines this process through attribute-based preprocessing and information templates, generating attribute forests and novel evaluation metrics for factual consistency and coverage [14].Other systems like STORM and ChatCite focus on improving cognitive scaffolding via multi-perspective questioning and human-in-theloop comparison workflows [38,39].While these approaches mark significant progress in scaling academic synthesis via LLMs, they primarily focus on high-level thematic abstraction, often overlooking fine-grained research trajectories such as methodological evolution or experimental performance shifts.In contrast, our work aims to uncover structural research progressions by aligning methodology and experimental results across thematically linked papers within a specific research direction-thereby enabling more granular knowledge discovery than generic survey generation pipelines.</p>
<p>Automated Deep Research Systems</p>
<p>Recent advancements in large language models (LLMs) have enabled deep research systems that go beyond summarization to support iterative scientific inquiry through retrieval, reasoning, and hypothesis generation.For example, DOLPHIN [15] introduces a closed-loop framework combining idea generation, experimental validation, and feedback refinement, with features like literature retrieval and error-aware debugging.ResearchAgent [16] leverages citation graphs and cross-domain knowledge to enhance idea generation, and uses multi-agent LLM-based peer review for evaluating novelty and clarity.Other systems focus on targeted capabilities such as information seeking.DeepSeek-R1 [40] and R1-Searcher [41] apply reinforcement learning to prompt external search when knowledge gaps arise, using reward-driven prompting and retrievalaware rollouts to improve factual grounding.Broader agentic reasoning frameworks [17] coordinate multiple agents-like memory agents, search agents, and code executors-to support multi-hop inference, task decomposition, and synthesis across modalities.Commercial platforms have also adopted this paradigm.OpenAI [42] and Gemini [43] embed tool use, asynchronous planning, and adaptive web exploration into general-purpose research agents.However, these systems often lack domain-specific precision for structured scientific analysis.</p>
<p>In contrast, our work introduces a focused agent system for structural knowledge discovery within a specific research domain.Rather than generating broad research plans, our system extracts and aligns motivation, methods, and results across related papers-capturing how ideas evolve, techniques recur, and performance shifts over time.This enables fine-grained, structured insights that bridge the gap between general AI agents and the rigorous demands of scientific research.</p>
<p>Method 3.1 Task Definition</p>
<p>We define the objective of ResearchPulse as an agent-based system for scientific document understanding, designed to analyze and organize structural research progress through two complementary tasks: (1) Method-Tracking, and (2) Experimental-Analysis.Each task enables fine-grained scientific inference over related papers via multi-agent coordination and tool-assisted document processing.</p>
<p>Let C = { 1 ,  2 , . . .,   } denote a collection of  related scientific documents, typically belonging to the same research thread or citation lineage.Each document   contains multiple sections including introduction, methodology, experiments, and references.</p>
<p>Method-Tracking Task.Given a document set C and a user instruction I method , the goal is to extract for each   its core motivation   and methodological description   from the abstract and introduction.The system then temporally aligns the extracted tuples {(  ,   )}  =1 based on the publication timestamp   of each   , and generates a structured representation M chain in a hierarchical markdown format.This content is further processed into a mind-map-style visualization.</p>
<p>Experimental-Analysis Task.Given the same input document set C and a user instruction I exp , the goal is to extract from each   its main experimental table   , associated model names M  , evaluation metrics E  , and cited baseline years Y  .The system organizes the metric values over time and produces a structured summary E chain reflecting comparative performance trends.The output is rendered as a line chart via auto-generated Python code.</p>
<p>ResearchPulse</p>
<p>ResearchPulse is implemented as a modular, instruction-driven agent system designed to perform structured scientific inference across related documents.Given a document set C = { 1 ,  2 , ...,   } and a user-issued instruction I, the system decomposes the task into two coordinated subtasks-Method Tracking and Experimental Analysis-each handled by a specialized sub-agent.The overall pipeline is illustrated in Figure 2.</p>
<p>Plan Agent.</p>
<p>The Plan Agent acts as the central controller of ResearchPulse.It receives user instruction I and identifies the corresponding intent category: I method for method-tracking or I exp for experimental comparison.Based on this classification, it dynamically dispatches subtasks to the appropriate downstream agent (Mmap-Agent or Lchart-Agent).</p>
<p>To support end-to-end automation, the Plan Agent invokes an Extraction module, which parses each document   in C into its  (
)1
The agent then routes each   with the appropriate instruction I method or I exp to the respective downstream processing agent.This decouples instruction planning from content processing, enabling modular coordination across diverse scientific tasks.</p>
<p>Mmap-Agent.</p>
<p>The Mmap-Agent is responsible for identifying the research motivation and methodology for each paper in the corpus and constructing a time-aligned representation of scientific evolution.Specifically, for each   , the agent extracts a tuple (  ,   ) representing the motivation and method, respectively.These are typically sourced from the abstract and introduction sections.</p>
<p>Formally, this agent performs:
(𝑚 𝑖 , 𝑟 𝑖 ) = F Mmap (𝑋 𝑖 , I method ),(2)
where F Mmap is a fine-tuned LLM designed for structural scientific information extraction.The output for all documents is then temporally sorted by publication timestamp   to produce a markdownformatted research chain:
M chain = Sort 𝑡 𝑖 {(𝑚 𝑖 , 𝑟 𝑖 )} 𝑁 𝑖=1 ,(3)
which is subsequently rendered into a mind map visualization.</p>
<p>To supervise this extraction process, we optimize the negative log-likelihood of the predicted motivation-method tuple sequence:
L Mmap = − 𝑁 ∑︁ 𝑖=1 log 𝑃 ((𝑚 𝑖 , 𝑟 𝑖 ) | 𝑋 𝑖 , I method ),(4)
where  (•) denotes the conditional probability predicted by the finetuned LLM.The training data includes manually annotated pairs of motivations and methods across representative paper series.The loss encourages the agent to capture salient reasoning structures and align them chronologically for subsequent visualization.Formally, the agent conducts:
(𝑇 𝑖 , M 𝑖 , E 𝑖 , Y 𝑖 ) = F Lchart (𝑋 𝑖 , I exp ),(5)
where F Lchart is a separately fine-tuned agent optimized for tabular extraction and metric normalization.The extracted results are aligned across time and datasets to construct a structured trend record:
E chain = Align {(𝑇 𝑖 , M 𝑖 , E 𝑖 , Y 𝑖 )} 𝑁 𝑖=1 ,(6)
which is visualized as a line chart through Python code automatically generated by the agent:
Chart = V (E chain ).(7)
The Lchart-Agent is trained to extract structured experimental content with high precision.Its objective is to maximize the correctness and completeness of extracted tuples (  , M  , E  , Y  ) under supervision.We define the training loss as:
L Lchart = − 𝑁 ∑︁ 𝑖=1 log 𝑃 (𝑇 𝑖 , M 𝑖 , E 𝑖 , Y 𝑖 | 𝑋 𝑖 , I exp ),(8)
where the probability is computed over multi-field outputs.The agent is fine-tuned using a curated dataset of aligned tables and metric annotations, encouraging accurate parsing of experimental results and consistent temporal alignment across documents.</p>
<p>ResearchPulse-Bench</p>
<p>We present ResearchPulse-Bench, a benchmark designed to support multi-document scientific inference tasks including methodtracking and experimental comparison.Unlike traditional summarization datasets, ResearchPulse-Bench is constructed from realworld citation networks and explicitly captures the structure of research progressions.Each instance contains a series of related scientific papers organized by semantic and temporal proximity, annotated with motivation, methodology, and experimental evidence.The pipeline includes data collection, document parsing, information extraction, and visualization, as shown in Figure 3.</p>
<p>Data Collection.We begin by retrieving a set of seed papers from Google Scholar based on AI-related keywords (e.g., "Machine Learning", "Computer Vision", "Natural Language Processing"), citation counts, and publication timeframes (past five years).To ensure data openness, we restrict our collection to papers that are publicly accessible via platforms such as arXiv and OpenReview, complying with their open-access terms.Each selected paper's metadata-including title, authorship, publication date, and citation statistics-is extracted using public APIs or permitted crawling mechanisms.We then construct citation graphs by retrieving forward and backward references, enabling us to group semantically and temporally related papers into document clusters for downstream processing.</p>
<p>Data Processing.The collected papers are first parsed using PDFto-markdown conversion tools to extract structured content.Section segmentation is performed to isolate the abstract, introduction, experiment, and reference sections.For semantic clustering, we encode each abstract with Sentence-BERT to generate vector representations, then apply K-Means to group papers by topic proximity.Each resulting cluster is labeled using high-frequency terms and reviewed manually.Within each cluster, we use DeepSeek-R1 or GPT-4o [44] to extract fine-grained elements including motivations, methods, and experimental results.These outputs are stored in markdown format and further rendered as mind map diagrams and temporal trend charts to facilitate structured inspection of research motivations, methodologies, and experimental trajectories.This processed data is subsequently used for visual generation, as illustrated in Figure 3.</p>
<p>Human Inspection.To ensure dataset reliability, we conduct a multi-stage human inspection.Annotators review the semantic clustering results to verify topical coherence and identify misassigned papers.For method-related content, we check whether motivations and methodologies are logically and chronologically consistent.For experiment-based summaries, we verify if extracted tables match their originals, whether citation years are correctly resolved, and whether metric values are consistent with ground truth.All visual outputs-mind maps and line charts-are reviewed for structural completeness and semantic fidelity.Annotator feedback is used to adjust clustering granularity and refine LLM prompts, forming a closed-loop quality assurance process.</p>
<p>Data Analysis</p>
<p>Dataset Scale and Token Statistics.ResearchPulse-Bench contains 100 citation-aware document clusters, each curated for either the Method-Tracking or Experimental-Analysis task.Table 1 summarizes the number of samples as well as the input/output token lengths.Method-Tracking queries are relatively concise, averaging around 1.1K tokens per input and 350 tokens per output.Experimental-Analysis queries are significantly longer (averaging over 14K tokens) and require structured extraction of tabular data, with outputs around 900 tokens.This range reflects the dual challenge of focused scientific summarization and long-context reasoning.</p>
<p>Cluster Distribution and Examples.Table 2 shows the distribution of document clusters across train and test splits.Each cluster forms a temporally ordered and semantically coherent series of papers.On average, clusters contain around 20 documents, with the largest containing over 30.This structure supports fine-grained modeling of research trajectories over time.Representative examples for both   4, including a mind map from methodtracking and a line chart from experimental-analysis.These outputs highlight the benchmark's diversity and its support for downstream tasks such as trend forecasting and methodological comparison.</p>
<p>Method-Tracking</p>
<p>Experimental-Analysis</p>
<p>Evaluation Metrics</p>
<p>We employ a comprehensive set of evaluation metrics tailored to the multimodal outputs of ResearchPulse.For the method-tracking task, which produces structured textual summaries of motivations and methodologies, we use BERTScore (Precision, Recall, F1) and METEOR to assess semantic similarity and lexical overlap with references.To capture qualitative aspects such as fluency, factuality, and coherence, we additionally report GPT-Score-a human-aligned rating between 0 and 1 generated by GPT-4o along five dimensions: fluency, relevance, accuracy, creativity, and overall quality.</p>
<p>In the experimental-analysis task, the system outputs visual charts derived from extracted tables.To evaluate these generated figures, we adopt standard image quality metrics including Inception Score (IS), Fréchet Inception Distance (FID), Kernel Inception Distance (KID), and CLIP-FID for perceptual and distributional alignment.We further assess structural fidelity using SSIM, MS-SSIM, PSNR, LPIPS, and CMMD.</p>
<p>When the system generates Python code to render visualizations, we evaluate functional correctness using the pass@1 metric, which reports the percentage of code generations that execute successfully without runtime errors.Together, these metrics comprehensively evaluate the system's outputs across semantics, visual quality, and functional reliability.</p>
<p>Experiment</p>
<p>Setup.ResearchPulse performs method-tracking and experimentalanalysis through four core agents: Plan Agent, Mmap-Agent, and Lchart-Agent.The Plan Agent, utilizing Qwen-72B [45], is responsible for effectively distinguishing and interpreting the user's instructions.The Mmap-Agent is based on Qwen2.5-7B[46], while the Lchart-Agent leverages Qwen2.5-Coder-7B[47].Both agents were fine-tuned for four epochs on a 4×80GB A100 GPU setup to optimize performance for their respective tasks.</p>
<p>Model.In the method-tracking and experimental-analysis tasks, Mmap-Agent and Lchart-Agent are compared with several stateof-the-art models, including both open-source and closed-source approaches.The open-source models include Qwen2.5-7B-Instruct[46], InternLM3-8B-Instruct [48], Llama-3.1-8B-Instruct[49], CodeLlama-7B-hf [50], and Qwen2.5-Coder-7B-Instruct[47].Closed-source models include GPT-4o [44], Claude-3.7-Sonnet[51], and Gemini-1.5Pro [52].These models were selected based on their advanced performance in similar tasks and serve as key benchmarks for evaluating Mmap-Agent and Lchart-Agent.</p>
<p>Method-Tracking</p>
<p>Main Results.Mmap-Agent achieves superior performance across multiple evaluation metrics, as demonstrated in Table 3.It leads in BERTScore F1 (90.39) and METEOR (46.14), showcasing its exceptional ability to preserve semantic accuracy and generate fluent, contextually appropriate text.In GPT-4o-based evaluations, Mmap-Agent surpasses other models with an overall score, excelling in fluency (92.16), relevance (87.62), accuracy (82.29), creativity (79.54) and quality (85.59).While baseline models like InternLM3-8B-Instruct and Claude-3.7-Sonnetdemonstrate strong performance in specific areas, such as METEOR or fluency, they fall short in providing balanced performance across all dimensions.InternLM3, for instance, achieves the highest METEOR score (46.14), but its BERTScore F1 (87.35) and GPT-4o results are lower, indicating weaker semantic alignment and generation quality.Similarly, Claude-3.7-Sonnetexcels in fluency (90.63) and relevance (85.99), but underperforms in accuracy (80.32) and creativity (78.40).In contrast, Mmap-Agent's well-rounded performance across fluency, relevance, accuracy, and creativity highlights its robustness in both linguistic fluency and semantic integrity.These results validate Mmap-Agent's strength in method-level reasoning and generation tasks, where it consistently outperforms other models by generating coherent, precise, and diverse text.</p>
<p>Ablation Study.To evaluate the impact of key components on Mmap-Agent's performance, we conducted ablation studies by systematically removing critical modules.As illustrated in Table 3, the removal of the GPT-4o module resulted in notable declines in fluency and creativity, with BERTScore F1 dropping to 86.91.When the Compiler module was excluded, there was a marked reduction in precision and relevance, further lowering BERTScore F1 to 83.49.Removing both modules simultaneously caused an additional performance drop, with BERTScore F1 falling to 82.83.These results emphasize the importance of both modules in maintaining Mmap-Agent's overall performance, particularly in ensuring high fluency, precision, and relevance in generated text.</p>
<p>Experimental-Analysis</p>
<p>Main Results.As shown in Table 4, Lchart-Agent outperforms most models across key metrics, achieving a Pass@1 score of 97.50, surpassing GPT-4o (96.25) and Claude-3.7-Sonnet(91.56).It also achieves the lowest FID of 6.73, demonstrating superior image quality.Additionally, Lchart-Agent excels in perceptual metrics like LPIPS (8.49), CMMD (4.06), and SSIM (55.56), indicating high visual and semantic consistency.However, other models show strengths in specific areas: Claude-3.7-Sonnetleads in CLIP-FID with 0.08, suggesting better semantic alignment, while GPT-4o outperforms in KID with 1.14, indicating sharper images.Despite these areas of strength for other models, Lchart-Agent maintains a competitive edge overall, consistently excelling in generating high-quality, semantically consistent images across multiple dimensions.</p>
<p>Ablation Study.As shown in Table 4, removing the GPT-4o module resulted in a decrease in IS to 2.32 and an increase in FID to 7.13, indicating the module's crucial role in maintaining high-quality image generation.Similarly, excluding the Compiler module led to a reduction in performance, with Pass@1 dropping to 90.63 and FID rising to 8.02.Notably, the Compiler module also plays a key role in ensuring code executability, as evidenced by its direct impact on the Pass@1 score.When both modules were removed together, Pass@1 further declined to 90.00 and FID increased to 8.33, reinforcing the importance of these components in optimizing the model's overall performance.--------------------|------|------------  We conduct an in-depth analysis of typical failure cases across the Method-Tracking and Experimental-Analysis tasks, as illustrated in Figure 5.In the Method-Tracking task, a prominent issue is the misaligned attribution error, where the model assigns key innovations to incorrect sources-for example, attributing "message passing between graph nodes" to LeCun's CNN (1989), despite it being a defining feature of later graph-based neural networks such as GCNs or GATs.This indicates a misunderstanding of foundational methodological distinctions.Another frequent error is missing extraction, where the model overlooks essential components of the described method.In the case of layer-aware semantic adaptive quantization, the system fails to capture innovations such as dynamic precision assignment and semantic-aware evaluation metrics, resulting in incomplete or superficial summarization.For the Experimental-Analysis task, we observe factual deviation errors, such as incorrect reporting of Top-1 accuracy or misinterpretation of data trends from visualized results, which can undermine the credibility of the analysis.Furthermore, partial reference errors are evident when models mention results (e.g., IMFNet) without sufficient contextual grounding or comparative clarity.These observations underscore the necessity of more robust reasoning and alignment mechanisms to ensure faithful extraction and accurate interpretation of complex scientific content.</p>
<p>Conclusion</p>
<p>We present ResearchPulse, a modular agent system designed to perform multi-document scientific inference by extracting, aligning, and visualizing key research elements across thematically related papers.The system supports two complementary tasks: Method-Tracking, which constructs motivation-method chains rendered as mind maps, and Experimental-Analysis, which synthesizes benchmark trajectories from structured experimental results.To enable high-quality supervision, we introduce ResearchPulse-Bench, a citation-aware benchmark comprising 100 annotated paper clusters curated from arXiv and OpenReview.Experiments show that our agents, despite operating at 7B scale, achieve state-of-the-art performance in semantic alignment and multimodal output quality, outperforming both open-and closed-source baselines.Our work offers a new paradigm for tracking research evolution and structuring scientific knowledge at scale.</p>
<p>Figure 1 :
1
Figure 1: ResearchPulse transforms scientific papers into mind maps and line charts for structured research tracking.</p>
<p>思维导图：⽤户输⼊多篇或单篇系列⽂献和相应的指令信息，plan agent根 据⽤户的指令，对其进⾏理解拆分成⼦任务分发给不同的agent，同时调⽤ ⼯具去解析pdf⽂件，⽂献经过抽取agent抽取⽂献中最重要的部分，对于思 维导图⽽⾔，抽取到摘要和引⾔，其中包含了⽂献的动机、贡献等信息， 将多篇⽂献的摘要和引⾔结合在⼀起，以⽂章的标题区分。将这些信息输 ⼊给思维导图agent，它可以整个多篇⽂献等信息并输出为html代码，同时 调⽤对应的编译器，可以实现思维导图的可视化 折线图：⽤户输⼊多篇或单篇系列⽂献和相应的指令信息，plan agent根据 ⽤户的指令，对其进⾏理解拆分成⼦任务分发给不同的agent，同时调⽤⼯ 具去解析pdf⽂件，⽂献经过抽取agent抽取⽂献中最重要的部分，对于思维 导图⽽⾔，抽取到主实验表格和参考⽂献部分，其中包含了⽂献中模型的 结果信息，将多篇⽂献的主实验表格和参考⽂献结合在⼀起，以⽂章的标 题区分。将这些信息输⼊给折线图agent，它可以整个多篇⽂献等实验结果 信息并输出为python代码，同时调⽤对应的编译器，可以实现不同模型结 果图的可视化 this series (or individual) of papers by outlining the research motivation…Present the information exclusively in the form of a mind map to reveal the … Please summarize the model results from this … of papers and present them in the form of a line chart, highlighting the performance trends and actual trajectory of the models.</p>
<p>Figure 2 :
2
Figure 2: The ResearchPulse pipeline, consisting of three main agents: Plan Agent, Mmap-Agent, and Lchart-Agent.</p>
<p>Figure 3 :
3
Figure 3: The construction pipeline of ResearchPulse-Bench.</p>
<p>Query:Figure 4 :
4
Figure 4: Representative examples from Method-Tracking (top) and Experimental-Analysis (bottom) tasks.</p>
<p>Summarize the <strong>methodological evolution</strong> of the following papers in a <strong>structured and hierarchical</strong> manner using <strong>Markdown format</strong> for a <strong>mind map</strong> … Te deep neural networks (DNN), especially the convolutional neural networks (CNNs) … Method-Tracking (b) Missing Extraction Error Summarize the <strong>methodological evolution</strong> of the following papers in a <strong>structured and hierarchical</strong> manner using <strong>Markdown format</strong> for a … the key limitation in generalization of dynamic strategy across hardware remains unverified … (a) Factual Deviation Error …| Model | Year | Top-1 Acc.|\n|--</p>
<p>|\n| InternImage-XL ‡ | 2023 | 89.6% |\n| Vim-B † | 2022 | 73.2% |\n… (a) Partial Reference Error … 3DSN | 2019 | RR(%) | 78.4 |\n| FCGF | 2019 | RR(%) | 85.1 |\n| D3Feat | 2020 | RR(%) | 81.6 |\n| Predator … IMFNet(our) …Experimental-Analysis</p>
<p>Figure 5 :
5
Figure 5: The error examples of Mmap-Agent and Lchart-Agent.</p>
<p>3.2.3Lchart-Agent.The Lchart-Agent is designed to extract experimental results from each paper and generate visual representations of comparative performance.For each document </p>
<p>, the agent identifies its main experimental table   , model names M  , evaluation metrics E  , and baseline publication years Y  linked via citation resolution.</p>
<p>Table 1 :
1
Token statistics for the Method-Tracking and Experimental-Analysis tasks.
Method-Tracking Experimental-AnalysisStatistictraintesttraintestTotal SamplesSample Count19584911550320Query Length (tokens)Minimum55783013026Maximum22326240314687732971Average1161.161210.78 14402.7316545.45Answer Length (tokens)Minimum63141331407Maximum1722203630812020Average344.54355.6885.98928</p>
<p>Table 2 :
2
Distribution of paper clusters.
Series CountTrainTestTotalTotal8020100Minimum Papers6814Maximum Papers333138Average Papers19.1919.924.93tasks are shown in Figure</p>
<p>Table 3 :
3
(a) Main results: Performance comparison of Mmap-Agent with state-of-the-art models.The best result is highlighted in bold.(b) Ablation study: Impact of different module configurations on Mmap-Agent performance.
ModelSize METEOR↑P↑BERTScore R↑F1↑GPT-4o Score Fluency↑ Relevance↑ Accuracy↑ Creativity↑ Quality↑(a) Main resultsQwen2.5-7B-Instruct7B42.6088.80 86.23 87.4992.1087.1481.5778.7285.10InternLM3-8B-instruct8B46.1487.46 87.25 87.3589.4585.2180.3478.4383.46Llama-3.1-8B-Instruct8B39.4788.64 85.80 87.1990.4484.5277.5774.9682.00CodeLlama7B17.4981.94 81.79 81.8580.5080.1073.0773.6981.59Qwen2.5-Coder-7B-Instruct7B41.0388.78 85.74 87.2291.6886.6280.7178.6584.57GPT-4o-42.6684.90 86.06 85.4888.3282.4678.2775.8883.42Claude-3.7-Sonnet-42.7388.43 85.34 86.8590.6385.9980.3278.4084.01Gemini-1.5 Pro-43.2588.75 83.42 85.9990.2685.4879.9577.1483.32Mmap-Agent7B46.1490.90 89.89 90.3992.1687.6282.2979.5485.59(b) Ablation studyw/o GPT-4o7B41.7887.23 87.93 86.9191.2986.4981.4573.8883.98w/o Compiler7B44.1388.19 84.67 83.4990.1883.7380.1876.1181.69w/o GPT-4o &amp; Compiler7B42.2386.11 83.99 82.8390.0582.9979.6475.3282.71</p>
<p>Table 4 :
4
(a) Main results: Performance comparison of Lchart-Agent with state-of-the-art models.The best result is highlighted in bold.(b) Ablation study: Impact of different module configurations on Lchart-Agent performance.
ModelSize Pass@1↑ IS↑ FID↓ KID↓ CLIP-FID↓ LPIPS↓ CMMD↓ SSIM↑ PSNR↑ MS-SSIM↑(a) Main resultsQwen2.5-7B-Instruct7B65.000.18 12.734.102.0924.359.8529.137.9326.51InternLM3-8B-instruct8B68.130.57 29.912.084.4210.3915.2222.975.1417.91Llama-3.1-8B-Instruct8B58.750.24 36.392.141.6413.476.7024.716.5515.17CodeLlama-7B-hf7B55.000.09 30.165.330.2330.3210.6320.839.1810.58Qwen2.5-Coder-7B-Instruct7B73.130.77 10.692.155.5016.7112.0232.336.3721.18GPT-4o-96.252.407.151.141.189.314.0953.7612.1423.70Claude-3.7-Sonnet-91.562.288.563.650.089.817.2452.2511.9036.34Gemini-1.5 Pro-1.108.684.335.7510.405.4742.339.5723.25Lchart-Agent7B97.502.65 6.731.171.108.494.0655.5612.4936.55(b) Ablation studyw/o GPT-4o7B97.502.327.131.251.229.135.0852.7710.7633.52w/o Compiler7B90.632.268.021.371.149.226.1651.6711.5326.85w/o GPT-4o &amp; Compiler7B90.001.988.331.561.319.387.3248.799.8422.915.3 Error Analysis</p>
<p>Cvt-slr: Contrastive visual-textual transformation for sign language recognition with variational alignment. Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen, Stan Z Li, CVPR. 2023</p>
<p>Simvpv2: Towards simple yet powerful spatiotemporal predictive learning. Cheng Tan, Zhangyang Gao, Siyuan Li, Stan Z Li, IEEE Transactions on Multimedia. 2025</p>
<p>Openstl: A comprehensive benchmark of spatiotemporal predictive learning. Cheng Tan, Siyuan Li, Zhangyang Gao, Wenfei Guan, Zedong Wang, Zicheng Liu, Lirong Wu, Stan Z Li, Advances in Neural Information Processing Systems. 202336</p>
<p>Peek across: Improving multi-document modeling via cross-document questionanswering. Avi Caciularu, Matthew Peters, Jacob Goldberger, Ido Dagan, Arman Cohan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Compressed heterogeneous graph for abstractive multi-document summarization. Miao Li, Jianzhong Qi, Jey Han Lau, AAAI. 202337</p>
<p>Xiangxiang Zhang, Jingxuan Wei, Donghong Zhong, Qi Chen, Caijun Jia, Cheng Tan, Jinming Gu, Xiaobo Qin, Zhiping Liu, Liang Hu, Tong Sun, Yuchen Wu, Zewei Sun, Chenwei Lou, Hua Zheng, Tianyang Zhan, Changbao Wang, Shuangzhi Wu, Zefa Lin, Chang Guo, Sihang Yuan, Riwei Chen, Shixiong Zhao, Yingping Zhang, Bihui Gaowei, Jiahui Yu, Zhehui Wu, Qianqian Zhao, Ruofeng Liu, Xingyue Tang, Bing Huang, Zhao, Mengyang Zhang, and Youqiang Zhou. Structvrm: Aligning multimodal reasoning with structured and verifiable reward models. 2025</p>
<p>Text summarization with pretrained encoders. Yang Liu, Mirella Lapata, EMNLP-IJCNLP. 2019</p>
<p>Long-span summarization via local attention and content selection. Potsawee Manakul, Mark Gales, ACL. 2021</p>
<p>Hegel: Hypergraph transformer for long document summarization. Haopeng Zhang, Xiao Liu, Jiawei Zhang, EMNLP. 2022</p>
<p>Hierarchical attention graph for scientific document summarization in global and local level. Chenlong Zhao, Xiwen Zhou, Xiaopeng Xie, Yong Zhang, NAACL. 2024</p>
<p>Generating a structured summary of numerous academic papers: Dataset and method. Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen, arXiv:2302.045802023arXiv preprint</p>
<p>Chime: Llm-assisted hierarchical organization of scientific studies for literature review support. Chao-Chun, Erin Hsu, Jenna Bransom, Bailey Sparks, Chenhao Kuehl, David Tan, Lucy Lu Wadden, Aakanksha Wang, Naik, ACL. 2024</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, NeurIPS. 372024</p>
<p>Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Simin Niu, Shichao Song, Hanyu Wang, Bo Tang, Feiyu Xiong, arXiv:2502.14776Academic survey automation via large language models. 2025arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou, arXiv:2501.039162025arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Agentic reasoning: Reasoning llms with tools for the deep research. Junde Wu, Jiayuan Zhu, Yuyuan Liu, arXiv:2502.046442025arXiv preprint</p>
<p>Cheng Tan, Qi Chen, Jingxuan Wei, Gaowei Wu, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z Li, arXiv:2508.01237Sketchagent: Generating structured diagrams from hand-drawn sketches. 2025arXiv preprint</p>
<p>From words to structured visuals: A benchmark and framework for text-to-diagram generation and editing. Jingxuan Wei, Cheng Tan, Qi Chen, Gaowei Wu, Siyuan Li, Zhangyang Gao, Linzhuang Sun, Bihui Yu, Ruifeng Guo, CVPR. 2025</p>
<p>Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu, arXiv:2504.031602025arXiv preprint</p>
<p>Textrank: Bringing order into text. Rada Mihalcea, Paul Tarau, EMNLP. 2004</p>
<p>Lexrank: Graph-based lexical centrality as salience in text summarization. Günes Erkan, Dragomir R Radev, Journal of artificial intelligence research. 222004</p>
<p>An empirical survey on long document summarization: Datasets, models, and metrics. Yee Huan, Jiaxin Koh, Ming Ju, Shirui Liu, Pan, 2022ACM computing surveys55</p>
<p>Extractive summarization using extended textrank algorithm. N Ansh, Rinit Vora, Aastha Sanjeev Mayur Jain, Sheetal Shah, Sonawane, Proceedings of the 21st International Conference on Natural Language Processing (ICON). the 21st International Conference on Natural Language Processing (ICON)2024</p>
<p>Dependencybased discourse parser for single-document summarization. Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, Masaaki Nagata, EMNLP. 2014</p>
<p>Learning-based singledocument summarization with compression and anaphoricity constraints. Greg Durrett, Taylor Berg-Kirkpatrick, Dan Klein, ACL. 1998-2008, 2016</p>
<p>Single-document abstractive text summarization: A systematic literature review. Abishek Rao, Shivani Aithal, Sanjay Singh, ACM Computing Surveys. 5732024</p>
<p>Booookscore: A systematic exploration of book-length summarization in the era of llms. Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer, arXiv:2310.007852023arXiv preprint</p>
<p>Long document summarization with top-down and bottom-up inference. Bo Pang, Erik Nijkamp, Wojciech Kryściński, Silvio Savarese, Yingbo Zhou, Caiming Xiong, EACL. 2023</p>
<p>Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang, arXiv:2401.03203Hi-map: Hierarchical factorized radiance field for high-fidelity monocular dense mapping. 2024arXiv preprint</p>
<p>Skt5scisumm-revisiting extractivegenerative approach for multi-document scientific summarization. Quoc Huy, Ming To, Guangyan Liu, Hung-Nghiep Huang, Tran, ' Andr, Felix Greiner-Petter, Akiko Beierle, Aizawa, arXiv:2402.173112024arXiv preprint</p>
<p>Improving multi-document summarization through referenced flexible extraction with credit-awareness. Yun-Zhu Song, Yi-Syuan Chen, Hong-Han Shuai, ACL. 2022</p>
<p>Supervising the centroid baseline for extractive multi-document summarization. Simão Gonçalves, Gonçalo Correia, Diogo Pernes, Afonso Mendes, EMNLP. 202387</p>
<p>Enhancing multi-document summarization with cross-document graphbased information extraction. Zixuan Zhang, Heba Elfardy, Markus Dreyer, Kevin Small, Ji Heng, Mohit Bansal, EACL. 2023</p>
<p>Disentangling specificity for abstractive multi-document summarization. Congbo Ma, Wei Emma Zhang, Hu Wang, Haojie Zhuang, Mingyu Guo, IJCNN. IEEE2024</p>
<p>Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles. Kung-Hsiang Huang, Philippe Laban, Alexander Richard Fabbri, Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu, NAACL. 2024</p>
<p>From single to multi: How llms hallucinate in multidocument summarization. Catarina G Belem, Pouya Pezeskhpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka, arXiv:2410.139612024arXiv preprint</p>
<p>Assisting in writing wikipedia-like articles from scratch with large language models. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, Monica Lam, NAACL. 2024</p>
<p>Chatcite: Llm agent with human workflow guidance for comparative literature summary. Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen, COLING. 2025</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen, arXiv:2503.05592R1-searcher: Incentivizing the search capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Gpt-4o: Advancing multimodal ai with real-time audio, vision, and text integration. 2024OpenAITechnical Report</p>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, arXiv:2409.121865-coder technical report. 2024arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.172972024Internlm2 technical report. arXiv preprint</p>
<p>Llama 3: Open foundation for generative ai. A I Meta, 20248B parameter instruct-tuned model</p>
<p>open-foundation-models-for-code/, 2023. Code-specific adaptation of Llama 2. A I Meta, Codellama, Open foundation models for code. </p>
<p>The claude 3 model family: Opus, sonnet. Anthropic, 2024Technical Report</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
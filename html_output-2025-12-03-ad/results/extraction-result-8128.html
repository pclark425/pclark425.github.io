<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8128 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8128</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8128</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-279250842</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07824v2.pdf" target="_blank">Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Multi-digit addition is a clear probe of the computational power of large language models. To dissect the internal arithmetic processes in LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection. Inspired by the step-by-step manner in which humans perform addition, we propose and analyze a coherent four-stage trajectory in the forward pass:Formula-structure representations become linearly decodable first, while the answer token is still far down the candidate list.Core computational features then emerge prominently.At deeper activation layers, numerical abstractions of the result become clearer, enabling near-perfect detection and decoding of the individual digits in the sum.Near the output, the model organizes and generates the final content, with the correct token reliably occupying the top rank.This trajectory suggests a hierarchical process that favors internal computation over rote memorization. We release our code and data to facilitate reproducibility.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8128.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8128.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLAMA-3-8B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The primary model analyzed in this paper: a 32-layer, 8-billion-parameter instruction-tuned LLaMA-3 variant whose frozen weights were probed to map layer-wise representations during multi-digit base-10 addition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA-3-8B-INSTRUCT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer decoder model, 32 transformer blocks, ~8B parameters, instruction-tuned; model parameters kept frozen during all probing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>In-distribution base-10 multi-digit addition (1–6 digits), primary experiments on two- and three-digit additions with evaluation on held-out in-domain and some OOD lengths (1,3,4 digits).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Layered, digit-wise and position-specific representations that emerge sequentially: early encoding of formula structure (operand/operator layout), mid-layer explicit core computations (column sums and carry bits), deeper-layer result-level numerical abstractions (per-digit identities), and final output-aligned representations that drive token generation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Layerwise linear probes on the last-input-token hidden state (one linear classifier per layer per attribute) and logit lens projections (ℓ(l)=h_S^(l) W_U^T) to track gold-token rank across layers; probes trained with cross-entropy on frozen model activations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match addition accuracy (model output): 98.18% overall on the benchmark. Probe results (typical peaks): formula-structure probes reach high accuracy in early-to-mid layers (peaks often ≥95%), sum-range classification improves sharply around L16-L19 and plateaus high, carry detection approaches ceiling by ~L19, digit-wise decoding reaches stable high accuracy after ~L28; logit-lens: first top-1 gold-token for all examples occurs in L23-L32 with a peak at L30 (~45.4% of samples).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Paper notes dynamic 'low-high-dip' patterns where decodability peaks at intermediate layers then dips as representations are transformed; limitations include no causal interventions (correlation vs causation), scoped only to base-10 addition under short prompts, and potential failure to generalize outside studied distributions (long multiplication, mixed bases, borrowing subtraction not evaluated). Prior work (cited) reports OOD failures like commutativity violations and symbol perturbation sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise probe accuracy curves showing consistent ordering: structure → carries/sums → digit abstractions → output alignment across independent runs; carry probes (binary) rise from ~L14 and saturate by ~L19; sum-range classifiers show sharp rise L16-L19; digit probes (10-way) reach high accuracy after ~L28; logit lens shows final token selection in late layers — together consistent with stepwise, computation-like internal processing rather than pure rote memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Authors emphasize that linear decodability does not prove usage in forward computation; no causal tests (activation patching, ablation) were performed. The ordering persists across other models but exact layer indices vary (model specificity). The study is limited to in-distribution prompts and base-10 numerals, so generalization to other formats and adversarial perturbations remains untested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8128.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8128.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Four-stage trajectory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four-stage information-processing trajectory for addition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A descriptive mechanism introduced by the paper: addition-related signals become linearly decodable in four sequential stages across depth — (1) formula structure, (2) core computations (column sums & carry), (3) result-level numerical abstractions (digit identities), (4) output-aligned generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA-3-8B-INSTRUCT (primary example); qualitatively observed across other tested 7B models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Descriptive account applicable to transformer decoder LLMs studied (32-layer 8B and several 7B variants in appendix); not an implemented module but an observed ordering of linearly-decodable signals.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit base-10 addition (main), with digit-probe generalization tested on subtraction and multiplication for hundreds-digit.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Hierarchy of representational stages: syntactic/structural encoding precedes localized arithmetic computations (digit-wise sums and carry signals), which then consolidate into abstract per-digit identities used to generate tokens; representations are position-specific and become linearly accessible at different depths.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Operationalized via per-layer linear probes for structural labels, carry bits, sum-range bins, and per-digit classifiers; logit lens used to map output-alignment stage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Stage-wise probe peaks: formula-structure decodable in early-to-mid layers (~L8-L14); carry and sum-range signals rise around L14-L19 and saturate by ~L19; digit identities become highly decodable in late layers (post ~L25–L28). Probes often achieve ≥95% accuracy on in-domain tasks and up to 99% in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Stage-specific signals can transiently reduce in linear decodability after their peak (the low-high-dip phenomenon) as representations are transformed downstream; the paper cautions that decodability peaks may not correspond to causal necessity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent ordering of probe accuracy peaks across multiple diagnostic tasks and runs, cross-model qualitative replication (Appendix D), and alignment of logit-lens final token selection with the last stage support the staged trajectory interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No causal interventions performed to prove these stages are necessary; representation shifts and dips complicate exact stage boundaries; specialized math-optimized models exhibit layer shifts and different effect sizes, so ordering (not exact layers) is the main robust claim.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8128.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8128.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear probes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-layer linear probing of last-token hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The main diagnostic probing technique used: train a separate single-layer linear classifier (softmax) on the last-input-token hidden vector at each model layer to decode attributes like carry bits, digit values, and formula structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to LLAMA-3-8B-INSTRUCT and several 7B models (Mistral-7B-Instruct, Qwen2.5-Math-7B-Instruct, AceMath-7B-Instruct) in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probes are single linear layers (W^(l), b^(l)) trained with cross-entropy over stratified train/test splits (typically 80/20), trained for 10 epochs, model frozen; input is h_S^(l) (last token state).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to decode formula-structure labels (3-way), sum-range bins (10-way), carry bits (binary per decimal place), and per-digit identities (10-way per position).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Reveals linearly-decodable representations at different depths: early structural cues, mid-layer carry/sum signals, late-layer digit abstractions; supports the idea of position-specific digit encodings and a low-dimensional numerical subspace accessible by linear readouts.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Linear-probe training across L0-L32 on last-token vectors; reported mean accuracy over five runs with 95% CIs; balanced datasets for binary tasks and controlled sampling for multi-class tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Typical probe outcomes: formula-structure probes rise from ~baseline to high accuracy (peaks ≥95%); sum-range classification shows sharp rise near L16-L19; carry detection goes from ~0.5 baseline to near-ceiling by ~L19; digit-wise 10-class probes reach high accuracy (~>95%) in late layers (post ~L28).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Probes can overfit or detect incidental correlations; linear decodability does not prove causal usage. Some probes show a post-peak decrease in accuracy as representations are transformed, indicating dynamic reformatting rather than monotonic refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent layerwise accuracy curves across tasks and seeds, plus cross-operation generalization tests (hundreds-digit probe generalizes to subtraction ~0.9 and multiplication ~0.8) show probes reveal reusable numerical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Authors explicitly caution that probes are diagnostic (correlational) and recommend causal interventions (activation patching, ablation) which were not performed here; therefore probe-readability alone cannot confirm that the model uses those features in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8128.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8128.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logit lens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logit lens projection of intermediate hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic that projects intermediate last-token hidden states through the model's unembedding matrix to produce layer-specific logits and next-token rankings, used here to track when the gold answer token becomes top-ranked.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interpreting gpt: the logit lens</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA-3-8B-INSTRUCT (applied diagnostically)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Given unembedding W_U, compute ℓ(l)=h_S^(l) W_U^T and softmax(ℓ(l)) to estimate what the model would predict if layer l were final; used to record earliest layer where gold next-token attains rank-1.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Three-digit addition (1000 problems) used to measure earliest top-1 appearance of the correct first answer token across layers L0-L32.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Shows that decisive token selection is aligned with late-layer representations; logit lens is most informative in later layers because final token rank-1 occurs in L23-L32 (peak at L30), implying earlier layers reorganize internal features before output selection.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Projection of hidden states at each layer through fixed unembedding matrix (no fine-tuning); histogramming earliest layer where gold token becomes top-1 across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>All test prompts first reach top-1 in layers L23-L32; the most frequent earliest layer is L30 (~45.4% of samples over five runs); there were no 'never top-1' cases in the tested set.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Logit lens provides little insight into intermediate processing since the gold token becomes top-1 late; therefore it can mask intermediate representational stages. The method is correlational and does not reveal causal processing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Concordance between the late emergence of top-1 gold-token via logit lens and the late-layer saturation of digit-wise probes supports a pipeline where computation and abstraction occur prior to final token selection.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Because the logit lens shows final selection only late, it cannot by itself detect mid-layer computational structure; authors combine it with probes to reveal intermediate signals. Also, logit-lens ranks do not prove causal reliance on intermediate features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8128.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8128.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other evaluated models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct, AceMath-7B-Instruct, Qwen2.5-Math-7B-Instruct (comparative models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Additional 7B-scale and math-optimized models evaluated for comparative probe curves in Appendix D; used to check robustness of the four-stage ordering across architectures and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct; AceMath-7B-Instruct; Qwen2.5-Math-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter transformer models; AceMath and Qwen2.5 are math-optimized variants (post-training/fine-tuning). Probes were run with frozen model weights similarly to LLAMA-3-8B-INSTRUCT.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same base-10 multi-digit addition probing tasks (carry detection, digit decoding, sum-range classification, logit-lens) reported in Appendix D/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>All models qualitatively show the same four-stage ordering (structure → carries/sums → digit abstractions → output alignment) though exact layer indices shift and effect sizes vary; math-optimized checkpoints may show different signal clarity and slightly better raw exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same per-layer linear probes and logit lens diagnostics as for LLAMA-3-8B-INSTRUCT, reported in appendices and comparative figures (Figures 15–26 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported overall exact-match addition accuracies in Table 1: Mistral-7B-Instruct 96.12%, AceMath † 99.10%, Qwen2.5-Math-7B † 98.68% († indicates math-optimized). Probe curves qualitatively similar but with layer index shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Math-optimized training can introduce confounds for mechanistic attribution; layer indices for signal emergence vary across architectures; some numerical-range slices show anomalies (noted in appendices).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Appendix D shows the same four-stage ordering largely persists across models, supporting that the trajectory is not unique to the primary model though exact depths differ.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Specialized fine-tuning (math-optimized models) may change internal strategies and obscure mechanistic comparisons; paper does not perform causal interventions to test necessity across different models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Carrying over algorithm in transformers <em>(Rating: 2)</em></li>
                <li>Understanding addition in transformers <em>(Rating: 2)</em></li>
                <li>Interpreting gpt: the logit lens <em>(Rating: 2)</em></li>
                <li>Pre-trained large language models use fourier features to compute addition <em>(Rating: 2)</em></li>
                <li>Language models use trigonometry to do addition <em>(Rating: 2)</em></li>
                <li>Arithmetic without algorithms: Language models solve math with a bag of heuristics <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Interpreting arithmetic mechanism in large language models through comparative neuron analysis <em>(Rating: 2)</em></li>
                <li>Do phd-level llms truly grasp elementary addition? probing rule learning vs. memorization in large language models <em>(Rating: 1)</em></li>
                <li>Language models encode the value of numbers linearly <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8128",
    "paper_id": "paper-279250842",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "LLaMA-3-8B-Instruct",
            "name_full": "LLAMA-3-8B-INSTRUCT",
            "brief_description": "The primary model analyzed in this paper: a 32-layer, 8-billion-parameter instruction-tuned LLaMA-3 variant whose frozen weights were probed to map layer-wise representations during multi-digit base-10 addition.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLAMA-3-8B-INSTRUCT",
            "model_description": "Transformer decoder model, 32 transformer blocks, ~8B parameters, instruction-tuned; model parameters kept frozen during all probing experiments.",
            "arithmetic_task_type": "In-distribution base-10 multi-digit addition (1–6 digits), primary experiments on two- and three-digit additions with evaluation on held-out in-domain and some OOD lengths (1,3,4 digits).",
            "mechanism_or_representation": "Layered, digit-wise and position-specific representations that emerge sequentially: early encoding of formula structure (operand/operator layout), mid-layer explicit core computations (column sums and carry bits), deeper-layer result-level numerical abstractions (per-digit identities), and final output-aligned representations that drive token generation.",
            "probing_or_intervention_method": "Layerwise linear probes on the last-input-token hidden state (one linear classifier per layer per attribute) and logit lens projections (ℓ(l)=h_S^(l) W_U^T) to track gold-token rank across layers; probes trained with cross-entropy on frozen model activations.",
            "performance_metrics": "Exact-match addition accuracy (model output): 98.18% overall on the benchmark. Probe results (typical peaks): formula-structure probes reach high accuracy in early-to-mid layers (peaks often ≥95%), sum-range classification improves sharply around L16-L19 and plateaus high, carry detection approaches ceiling by ~L19, digit-wise decoding reaches stable high accuracy after ~L28; logit-lens: first top-1 gold-token for all examples occurs in L23-L32 with a peak at L30 (~45.4% of samples).",
            "error_types_or_failure_modes": "Paper notes dynamic 'low-high-dip' patterns where decodability peaks at intermediate layers then dips as representations are transformed; limitations include no causal interventions (correlation vs causation), scoped only to base-10 addition under short prompts, and potential failure to generalize outside studied distributions (long multiplication, mixed bases, borrowing subtraction not evaluated). Prior work (cited) reports OOD failures like commutativity violations and symbol perturbation sensitivity.",
            "evidence_for_mechanism": "Layerwise probe accuracy curves showing consistent ordering: structure → carries/sums → digit abstractions → output alignment across independent runs; carry probes (binary) rise from ~L14 and saturate by ~L19; sum-range classifiers show sharp rise L16-L19; digit probes (10-way) reach high accuracy after ~L28; logit lens shows final token selection in late layers — together consistent with stepwise, computation-like internal processing rather than pure rote memorization.",
            "counterexamples_or_challenges": "Authors emphasize that linear decodability does not prove usage in forward computation; no causal tests (activation patching, ablation) were performed. The ordering persists across other models but exact layer indices vary (model specificity). The study is limited to in-distribution prompts and base-10 numerals, so generalization to other formats and adversarial perturbations remains untested.",
            "uuid": "e8128.0",
            "source_info": {
                "paper_title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Four-stage trajectory",
            "name_full": "Four-stage information-processing trajectory for addition",
            "brief_description": "A descriptive mechanism introduced by the paper: addition-related signals become linearly decodable in four sequential stages across depth — (1) formula structure, (2) core computations (column sums & carry), (3) result-level numerical abstractions (digit identities), (4) output-aligned generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLAMA-3-8B-INSTRUCT (primary example); qualitatively observed across other tested 7B models",
            "model_description": "Descriptive account applicable to transformer decoder LLMs studied (32-layer 8B and several 7B variants in appendix); not an implemented module but an observed ordering of linearly-decodable signals.",
            "arithmetic_task_type": "Multi-digit base-10 addition (main), with digit-probe generalization tested on subtraction and multiplication for hundreds-digit.",
            "mechanism_or_representation": "Hierarchy of representational stages: syntactic/structural encoding precedes localized arithmetic computations (digit-wise sums and carry signals), which then consolidate into abstract per-digit identities used to generate tokens; representations are position-specific and become linearly accessible at different depths.",
            "probing_or_intervention_method": "Operationalized via per-layer linear probes for structural labels, carry bits, sum-range bins, and per-digit classifiers; logit lens used to map output-alignment stage.",
            "performance_metrics": "Stage-wise probe peaks: formula-structure decodable in early-to-mid layers (~L8-L14); carry and sum-range signals rise around L14-L19 and saturate by ~L19; digit identities become highly decodable in late layers (post ~L25–L28). Probes often achieve ≥95% accuracy on in-domain tasks and up to 99% in some settings.",
            "error_types_or_failure_modes": "Stage-specific signals can transiently reduce in linear decodability after their peak (the low-high-dip phenomenon) as representations are transformed downstream; the paper cautions that decodability peaks may not correspond to causal necessity.",
            "evidence_for_mechanism": "Consistent ordering of probe accuracy peaks across multiple diagnostic tasks and runs, cross-model qualitative replication (Appendix D), and alignment of logit-lens final token selection with the last stage support the staged trajectory interpretation.",
            "counterexamples_or_challenges": "No causal interventions performed to prove these stages are necessary; representation shifts and dips complicate exact stage boundaries; specialized math-optimized models exhibit layer shifts and different effect sizes, so ordering (not exact layers) is the main robust claim.",
            "uuid": "e8128.1",
            "source_info": {
                "paper_title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Linear probes",
            "name_full": "Per-layer linear probing of last-token hidden states",
            "brief_description": "The main diagnostic probing technique used: train a separate single-layer linear classifier (softmax) on the last-input-token hidden vector at each model layer to decode attributes like carry bits, digit values, and formula structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to LLAMA-3-8B-INSTRUCT and several 7B models (Mistral-7B-Instruct, Qwen2.5-Math-7B-Instruct, AceMath-7B-Instruct) in the paper.",
            "model_description": "Probes are single linear layers (W^(l), b^(l)) trained with cross-entropy over stratified train/test splits (typically 80/20), trained for 10 epochs, model frozen; input is h_S^(l) (last token state).",
            "arithmetic_task_type": "Used to decode formula-structure labels (3-way), sum-range bins (10-way), carry bits (binary per decimal place), and per-digit identities (10-way per position).",
            "mechanism_or_representation": "Reveals linearly-decodable representations at different depths: early structural cues, mid-layer carry/sum signals, late-layer digit abstractions; supports the idea of position-specific digit encodings and a low-dimensional numerical subspace accessible by linear readouts.",
            "probing_or_intervention_method": "Linear-probe training across L0-L32 on last-token vectors; reported mean accuracy over five runs with 95% CIs; balanced datasets for binary tasks and controlled sampling for multi-class tasks.",
            "performance_metrics": "Typical probe outcomes: formula-structure probes rise from ~baseline to high accuracy (peaks ≥95%); sum-range classification shows sharp rise near L16-L19; carry detection goes from ~0.5 baseline to near-ceiling by ~L19; digit-wise 10-class probes reach high accuracy (~&gt;95%) in late layers (post ~L28).",
            "error_types_or_failure_modes": "Probes can overfit or detect incidental correlations; linear decodability does not prove causal usage. Some probes show a post-peak decrease in accuracy as representations are transformed, indicating dynamic reformatting rather than monotonic refinement.",
            "evidence_for_mechanism": "Consistent layerwise accuracy curves across tasks and seeds, plus cross-operation generalization tests (hundreds-digit probe generalizes to subtraction ~0.9 and multiplication ~0.8) show probes reveal reusable numerical structure.",
            "counterexamples_or_challenges": "Authors explicitly caution that probes are diagnostic (correlational) and recommend causal interventions (activation patching, ablation) which were not performed here; therefore probe-readability alone cannot confirm that the model uses those features in generation.",
            "uuid": "e8128.2",
            "source_info": {
                "paper_title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Logit lens",
            "name_full": "Logit lens projection of intermediate hidden states",
            "brief_description": "A diagnostic that projects intermediate last-token hidden states through the model's unembedding matrix to produce layer-specific logits and next-token rankings, used here to track when the gold answer token becomes top-ranked.",
            "citation_title": "Interpreting gpt: the logit lens",
            "mention_or_use": "use",
            "model_name": "LLAMA-3-8B-INSTRUCT (applied diagnostically)",
            "model_description": "Given unembedding W_U, compute ℓ(l)=h_S^(l) W_U^T and softmax(ℓ(l)) to estimate what the model would predict if layer l were final; used to record earliest layer where gold next-token attains rank-1.",
            "arithmetic_task_type": "Three-digit addition (1000 problems) used to measure earliest top-1 appearance of the correct first answer token across layers L0-L32.",
            "mechanism_or_representation": "Shows that decisive token selection is aligned with late-layer representations; logit lens is most informative in later layers because final token rank-1 occurs in L23-L32 (peak at L30), implying earlier layers reorganize internal features before output selection.",
            "probing_or_intervention_method": "Projection of hidden states at each layer through fixed unembedding matrix (no fine-tuning); histogramming earliest layer where gold token becomes top-1 across samples.",
            "performance_metrics": "All test prompts first reach top-1 in layers L23-L32; the most frequent earliest layer is L30 (~45.4% of samples over five runs); there were no 'never top-1' cases in the tested set.",
            "error_types_or_failure_modes": "Logit lens provides little insight into intermediate processing since the gold token becomes top-1 late; therefore it can mask intermediate representational stages. The method is correlational and does not reveal causal processing steps.",
            "evidence_for_mechanism": "Concordance between the late emergence of top-1 gold-token via logit lens and the late-layer saturation of digit-wise probes supports a pipeline where computation and abstraction occur prior to final token selection.",
            "counterexamples_or_challenges": "Because the logit lens shows final selection only late, it cannot by itself detect mid-layer computational structure; authors combine it with probes to reveal intermediate signals. Also, logit-lens ranks do not prove causal reliance on intermediate features.",
            "uuid": "e8128.3",
            "source_info": {
                "paper_title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Other evaluated models",
            "name_full": "Mistral-7B-Instruct, AceMath-7B-Instruct, Qwen2.5-Math-7B-Instruct (comparative models)",
            "brief_description": "Additional 7B-scale and math-optimized models evaluated for comparative probe curves in Appendix D; used to check robustness of the four-stage ordering across architectures and fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct; AceMath-7B-Instruct; Qwen2.5-Math-7B-Instruct",
            "model_description": "7B-parameter transformer models; AceMath and Qwen2.5 are math-optimized variants (post-training/fine-tuning). Probes were run with frozen model weights similarly to LLAMA-3-8B-INSTRUCT.",
            "arithmetic_task_type": "Same base-10 multi-digit addition probing tasks (carry detection, digit decoding, sum-range classification, logit-lens) reported in Appendix D/figures.",
            "mechanism_or_representation": "All models qualitatively show the same four-stage ordering (structure → carries/sums → digit abstractions → output alignment) though exact layer indices shift and effect sizes vary; math-optimized checkpoints may show different signal clarity and slightly better raw exact-match accuracy.",
            "probing_or_intervention_method": "Same per-layer linear probes and logit lens diagnostics as for LLAMA-3-8B-INSTRUCT, reported in appendices and comparative figures (Figures 15–26 etc.).",
            "performance_metrics": "Reported overall exact-match addition accuracies in Table 1: Mistral-7B-Instruct 96.12%, AceMath † 99.10%, Qwen2.5-Math-7B † 98.68% († indicates math-optimized). Probe curves qualitatively similar but with layer index shifts.",
            "error_types_or_failure_modes": "Math-optimized training can introduce confounds for mechanistic attribution; layer indices for signal emergence vary across architectures; some numerical-range slices show anomalies (noted in appendices).",
            "evidence_for_mechanism": "Appendix D shows the same four-stage ordering largely persists across models, supporting that the trajectory is not unique to the primary model though exact depths differ.",
            "counterexamples_or_challenges": "Specialized fine-tuning (math-optimized models) may change internal strategies and obscure mechanistic comparisons; paper does not perform causal interventions to test necessity across different models.",
            "uuid": "e8128.4",
            "source_info": {
                "paper_title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Carrying over algorithm in transformers",
            "rating": 2,
            "sanitized_title": "carrying_over_algorithm_in_transformers"
        },
        {
            "paper_title": "Understanding addition in transformers",
            "rating": 2,
            "sanitized_title": "understanding_addition_in_transformers"
        },
        {
            "paper_title": "Interpreting gpt: the logit lens",
            "rating": 2,
            "sanitized_title": "interpreting_gpt_the_logit_lens"
        },
        {
            "paper_title": "Pre-trained large language models use fourier features to compute addition",
            "rating": 2,
            "sanitized_title": "pretrained_large_language_models_use_fourier_features_to_compute_addition"
        },
        {
            "paper_title": "Language models use trigonometry to do addition",
            "rating": 2,
            "sanitized_title": "language_models_use_trigonometry_to_do_addition"
        },
        {
            "paper_title": "Arithmetic without algorithms: Language models solve math with a bag of heuristics",
            "rating": 2,
            "sanitized_title": "arithmetic_without_algorithms_language_models_solve_math_with_a_bag_of_heuristics"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Interpreting arithmetic mechanism in large language models through comparative neuron analysis",
            "rating": 2,
            "sanitized_title": "interpreting_arithmetic_mechanism_in_large_language_models_through_comparative_neuron_analysis"
        },
        {
            "paper_title": "Do phd-level llms truly grasp elementary addition? probing rule learning vs. memorization in large language models",
            "rating": 1,
            "sanitized_title": "do_phdlevel_llms_truly_grasp_elementary_addition_probing_rule_learning_vs_memorization_in_large_language_models"
        },
        {
            "paper_title": "Language models encode the value of numbers linearly",
            "rating": 1,
            "sanitized_title": "language_models_encode_the_value_of_numbers_linearly"
        }
    ],
    "cost": 0.012597999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs
9 Sep 2025</p>
<p>Yao Yan yaoyan@stu.cqnu.edu.cn 
Chongqing Normal University Chongqing
China</p>
<p>College of Computer and Information Science
Chongqing Key Lab of Cognitive Intelligence and Intelligent Finance</p>
<p>Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs
9 Sep 2025041344E80C8F5A6204DF8CD58D75F25FarXiv:2506.07824v2[cs.AI]
Arithmetic offers a compact test of whether large language models compute or memorize.We study multi-digit addition in LLAMA-3-8B-INSTRUCT using linear probes and the logit lens, and find a consistent four-stage, layer-wise ordering of probe-decodable signal types across depth: (1) early layers encode formula structure (operand/operator layout) while the gold next token is still far from top-1; (2) mid layers expose digit-wise sums and carry indicators; (3) deeper layers express result-level numerical abstractions that support near-perfect digit decoding from hidden states; and (4) near the output, representations align with final sequence generation, with the correct next token reliably ranked first.Across experiments, each signal family becomes linearly decodable with high accuracy (stage-wise peaks typically ≥95% on in-domain multi-digit addition, and up to 99%).Taken together, these observations-in our setting-are consistent with a hierarchical, computation-first account rather than rote pattern matching, and help explain why logit lens inspection is most informative mainly in later layers.Code and data are available at https://github.com/YaoToolChest/addition-in-four-movements.git.</p>
<p>Introduction</p>
<p>Status.This paper has been accepted to EMNLP 2025.Generalizable reasoning remains a core challenge for large language models (LLMs).Arithmetic-and multi-digit addition in particularoffers a controlled probe of whether models execute algorithmic procedures or merely match surface patterns (Nikankin et al., 2024).Humans solve addition via a well-defined carry algorithm; an LLM could either approximate this stepwise computation Accepted to EMNLP 2025.2) core computations (column sums and carries), (3) result-level numerical abstractions (digit identities), and (4) output-aligned organization that drives final token generation.Each signal type is operationalized via a linear-probe task and becomes decodable at different depths.</p>
<p>(e.g., column-wise summation with carry propagation) or rely on shortcuts.Consistent, systematic performance across inputs is suggestive of internal computation, whereas systematic or contextspecific failures point to superficial heuristics.</p>
<p>Prior work advances two contrasting views.One line of evidence reports structured, algorithm-like behavior: analyses of GPT-style models reveal components that specialize in partial summation and carry handling (Yu and Ananiadou, 2024), and even minimal two-layer encoder-only Transformers can learn the carry-addition procedure (Kruthoff, 2024).Another line argues that correct answers often arise from simple heuristics (Nikankin et al., 2024), citing a "bag of heuristics" in neuron activations (Zhou et al., 2024) and out-of-distribution errors such as commutativity violations or symbol perturbations (Yan et al., 2025).Related analyses report Fourier-like representational structure that mixes low-and high-frequency features (Zhou et al., 2024).These disagreements leave open how mainstream LLMs implement addition.</p>
<p>We focus on LLaMA-3-8B-Instruct (Grattafiori et al., 2024) and ask: does it perform multi-digit addition through structured, algorithmic computation (e.g., digit-wise summation with carry propagation), or through non-algorithmic pattern extraction?</p>
<p>Our starting point is a logit lens analysis (Yu and Ananiadou, 2024).Consistent with observations that the lens is most informative in later layers (cf.(Katz et al., 2024)), we find that next-token probabilities largely reflect compressed late-stage representations and reveal little about intermediate processing.Because the logit lens is informative mainly in late layers-i.e., the gold next token tends to become rank-1 only late in the forward pass-we hypothesize a hierarchical, multi-stage computation rather than a simple accumulation of confidence on the correct token.In other words, intermediate layers appear to reorganize information before output selection, rather than merely amplifying the correct-token probability.</p>
<p>Guided by this observation, we articulatewithout presupposing discrete stage boundaries or causal transitions-a layer-wise organization of addition-related signals that become decodable in a consistent order as depth increases.We make each signal family explicit and testable:</p>
<ol>
<li>Formula-structure signals: encoding of operand/operator layout and delimiter positions; probed by predicting token-category labels and structural spans from hidden states.Using linear probes on LLaMA-3-8B-Instruct, we observe that these four signal families emerge in the above order across layers.The ordering supports a computation-first interpretation in our setting, reconciles component specialization with reusable numerical structure, and explains why logit lens inspection is most revealing in later layers.We emphasize that our analysis is diagnostic rather than interventional: decodability does not imply necessity, and we do not claim causal stage transitions; instead, we provide operational evidence for a structured information-processing pipeline during addition.</li>
</ol>
<p>Related Work</p>
<p>We organize prior work on LLM arithmetic competence into three lenses: (i) algorithmic computation, (ii) heuristic pattern matching, and (iii) abstract numerical representation.</p>
<p>Algorithmic evidence Toy models and mid-sized LLMs exhibit algorithm-like solutions: attention can implement DFT-style modular addition (Nanda et al., 2023), and helical/"clock" number encodings have been proposed for general addition (Kantamneni and Tegmark, 2025).In larger models, component specialization persists -carry-routing heads and layer-wise decompositions into column summation and carry propagation are reported in 7Bscale GPT/LLaMA variants (Yu and Ananiadou, 2024;Kruthoff, 2024); causal analyses further identify operand routers and parallel digit streams (Stolfo et al., 2023;Quirke and Barez, 2024).</p>
<p>Heuristic evidence An opposing view attributes success to a "bag of heuristics": neuron-level studies find sparse, template-like strategies (Nikankin et al., 2024), and controlled out-of-distribution tests reveal failures under digit permutations, novel symbols, or simple reorderings (Yan et al., 2025).</p>
<p>Abstract numerical representations Beyond this dichotomy, pre-trained LLMs appear to superimpose magnitude and parity in a Fourier-like basis (Zhou et al., 2024); digit values are linearly recoverable, suggesting a low-dimensional numerical subspace (Zhu et al., 2025).Group-structured arithmetic elicits basis vectors reminiscent of representation theory (Chughtai et al., 2023), tree-structured reasoning traces can be reconstructed from hidden states (Hou et al., 2023), and evidence supports digit-by-digit encoding with position-specific neuron groups (Levy and Geva, 2025).</p>
<p>Our position</p>
<p>We provide a layer-resolved account of multi-digit addition in LLaMA-3-8B by combining linear probes (Belinkov, 2022) with the logit lens (nostalgebraist, 2020).The observed four-stage trajectory-from formula recognition, through core computation, to numerical abstraction and output generation-bridges component specialization with the emergence of reusable numerical structure.</p>
<p>Methodology</p>
<p>Overview.We study how internal representations evolve when LLAMA-3-8B-INSTRUCT performs multi-digit addition.As illustrated in Fig. 2, for each input we extract the hidden state of the last input token (the position that predicts the first answer token) at every layer and apply two complementary diagnostics: (i) logit lens to observe the model's layer-wise next-token distribution, and (ii) linear probes to test whether specific arithmetic attributes are linearly decodable.This setup allows us to map where and when structure, intermediate computations, and result digits emerge across depth.</p>
<p>Indexing convention.We adopt a 0-indexed convention with layer states l ∈ {0, . . ., 32} for LLAMA-3-8B-INSTRUCT.Here, l=0 denotes the embedding state (before any Transformer block), and l=1, . . ., 32 denote the outputs after Transformer blocks 1-32.Unless otherwise noted, all results are reported using layer states L0-L32.</p>
<p>Model, Prompting, and Target Activations</p>
<p>We use a standardized prompt template "Calculate: num1+num2 = " (note the trailing space), which tokenizes to X = (x 1 , . . ., x S ).The Transformer yields a sequence of hidden states H (l) ∈ R S×d at each layer state l ∈ {0, . . ., 32}, with the last-token vector denoted h
(l) S ∈ R d .
We focus on h (l) S because it conditions the generation of the first answer token.Among several open-source LLMs we screened, LLAMA-3-8B-INSTRUCT offered the most pronounced and consistent layer-wise probe signals and stable performance on our addition benchmark (Table 1), so we use it as our primary subject.Results for other models, including math-optimized variants (Mistral-7B-Instruct (Jiang et al., 2023), Qwen2.5-Math-7B-Instruct(Yang et al., 2024), AceMath-7B-Instruct (Liu et al., 2025)), are reported in Appendix D.</p>
<p>Logit Lens</p>
<p>Given the model's unembedding matrix W U ∈ R |V|×d , the logit lens projects the last-token state</p>
<p>Model</p>
<p>Addition Accuracy LLAMA-3-8B-INSTRUCT 98.18% Mistral-7B-Instruct 96.12% AceMath † 99.10% Qwen2.5-Math-7B† 98.68% at layer l to layer-specific next-token logits
ℓ (l) = h (l) S W ⊤ U ∈ R |V| .
Interpreting softmax(ℓ (l) ) as the distribution the model would produce if layer l were final, we can track how the rank of the gold next token and the sharpness of the distribution evolve with depth.</p>
<p>Linear Probes: Design, Training, and Evaluation</p>
<p>To assess where arithmetic information becomes linearly accessible, we train a separate linear classifier at each layer l for each attribute (e.g., carry bits at each position, per-digit sums, result digits).</p>
<p>Each probe takes the last-token vector h (l)</p>
<p>S ∈ R d as input and produces K logits:
o (l) = W (l) h (l) S + b (l) ,(1)p(l) = softmax o (l) .(2)
Here W (l) ∈ R K×d and b (l) ∈ R K .We optimize the standard cross-entropy over a training set of size N :
L (l) = − 1 N N i=1 log p(l) i,y i = − 1 N N i=1 K k=1 ⊮[y i = k] log p(l) i,k .(3)
and report accuracy on a held-out test set.1 Layerwise accuracy curves reveal where each attribute becomes reliably decodable.S W ⊤ U , which gives the next-token logits just before the first answer token is generated; and (ii) a linear probe over the same vector, l) , trained to decode task-relevant attributes (e.g., carry bits, digit values).Scanning l localizes where arithmetic information becomes linearly accessible and visualizes how predictions sharpen across depth.
o (l) = W (l) h (l) S + b (</p>
<p>Data Handling and Evaluation Protocol</p>
<p>Dataset construction.We construct task-specific datasets for probe training and testing using the above prompt template (with minor variants when stated).Splits are disjoint at the instance level, near-duplicates are removed, and OOD sets (e.g., unseen operand ranges/formats) are included when applicable.Preprocessing and sampling.We standardize spacing and punctuation, control operand distributions (e.g., uniform within range), balance classes where relevant, check tokenization artifacts, and fix random seeds for reproducibility (Appendix B).Evaluation criteria.For each attribute and layer, we compute accuracy on held-out data.When accuracy is near-perfect and stable across seeds, we treat the corresponding representation as linearly accessible at that layer.Comparing the onset of high decodability across attributes yields a layer-wise ordering that characterizes the flow of arithmetic information.</p>
<p>Model selection rationale.Our goal is to diagnose how models compute, rather than to optimize leaderboard accuracy.We therefore prioritize a widely used, instruction-tuned baseline whose internal signals are clear and reproducible under linear probes.While math-optimized models (marked † in Table 1) can achieve slightly higher raw accuracy on our benchmark, their specialized training can introduce confounds for mechanistic attribution.In contrast, LLAMA-3-8B-INSTRUCT consistently exhibits a clean four-stage ordering across layers in our probes, making it a suitable primary case study.Appendix D shows that the same ordering largely persists across other models with minor depth shifts.</p>
<p>Experiments and Results</p>
<p>We conduct a series of experiments to diagnose how arithmetic features are represented within the LLAMA-3-8B-INSTRUCT model.Across all experiments, diagnostic probes are trained on the hidden states of all layer states L0-L32 (embedding + 32 Transformer blocks), while the pretrained model parameters remain frozen.We use a common default setup for datasets, prompts, preprocessing, and splits; experiment-specific settings are described in each subsection.</p>
<p>Formula structure representation</p>
<p>Arithmetic Structure Recognition Experiments:</p>
<p>We ask whether the model encodes the structure of an addition prompt-independently of the specific numbers-early in the forward pass.For each layer l, we extract the last-input-token vector h (l) S and fit a 3-way linear classifier to distinguish {a+b, b+a, a+a} (with a &gt; b).Training uses two-digit expressions; evaluation covers a heldout in-domain two-digit set and out-of-distribution (OOD) lengths (1-, 3-, and 4-digit).As shown in Fig. 3, performance starts near the 1/3 random baseline at shallow layers, increases with depth, and plateaus at high accuracy in mid-to-late layers.The narrowing gap between OOD and in-domain curves suggests that the network progressively abstracts operand-order and self-addition patterns into linearly accessible features before answer generation. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Layer Index(llama) S to predict the equation type.Probes are trained on two-digit expressions (in-domain) and evaluated on a disjoint test split comprising the in-domain two-digit set and out-of-distribution (OOD) lengths (1-, 3-, and 4-digit).Curves show mean accuracy over five independent runs; the dashed line marks the random baseline (1/3).Accuracy rises from near-baseline at shallow layers to stable, high values in early-to-mid layers, indicating that structural cues about operand order and self-addition become increasingly linearly readable with depth.</p>
<p>Emergence of core computational features</p>
<p>Sum-range classification ("arithmetic results" probes).We localize where the sum becomes linearly decodable by training layer-wise probes to classify problems into contiguous sum bins (e.g., 500-509), which yields a 10-way task with a 0.10 random baseline.Each class contains 400 samples; dataset generation and splits are detailed in Appendix C. As shown in Fig. 4, accuracy is near baseline in shallow layers (L0-L8), increases gradually from ∼L8, exhibits a sharp rise around L16-L19, and then plateaus at high accuracy from about L19 onward, indicating that aggregate result information consolidates in mid-to-late layers.</p>
<p>Carry-signal detection.To test when carry information emerges, we train independent binary probes (balanced carry/no-carry) for the ones, tens, and hundreds positions on three-digit addition.Accuracy rises from near the 0.50 random baseline in early layers, begins a steady climb around ∼L14, and approaches saturation for all positions by ∼L19 (Fig. 5).This alignment with the sumrange transition supports the view that carry computation is established in mid layers and consolidated before output formation.Mean probe accuracy by layer for ones/tens/hundreds positions (5 runs; balanced labels).Curves rise from the 0.50 random baseline, begin increasing consistently around ∼L14, and approach ceiling by ∼L19, indicating efficient carry processing in mid-to-late layers.</p>
<p>Numerical abstraction of results</p>
<p>Digit-wise decoding (numerical abstraction).</p>
<p>We probe when individual result digits become linearly accessible by training separate 10-way (digits 0-9) linear classifiers at each layer on the lastinput-token state h (l) S , one probe per position (ones, tens, hundreds).This yields a random baseline of 0.10 per task; dataset construction and splits are detailed in Appendix C.4.As shown in Fig. 6, ac-curacy rises from near-baseline in shallow layers, improves steadily from mid layers, and reaches a stable high plateau in late layers (e.g., after ∼L28).The three positions exhibit similar depth profiles, indicating that digit-level abstractions consolidate late in the forward pass.Mean probe accuracy (±95% CI over 5 runs) by layer for predicting the ones, tens, and hundreds digits from h (l) S .All three tasks improve with depth from near the 10-class chance level (0.10) and saturate at high accuracy in late layers (e.g., after ∼L28), indicating robust numerical abstraction across positions.</p>
<p>Cross-operation generalization.To assess whether these digit representations transfer across operations, we train the hundreds-digit probe on addition and evaluate it on held-out subtraction and multiplication.Figure 7 shows that generalization is strongest to subtraction (peaking near ∼0.9) and somewhat lower to multiplication (peaking near ∼0.8), with both tasks well above the 10-class chance level (0.10).This pattern suggests a shared numerical substrate with operation-specific components that are less aligned for multiplication than subtraction.</p>
<p>Logit lens: Organization and generation of output content</p>
<p>First-correct-token emergence (logit lens).We use the logit lens to identify the earliest layer at which the gold next token (the first digit of the sum) becomes the model's top-1 prediction.For each layer l, we project the last-input-token state h (l)</p>
<p>S through the unembedding matrix to obtain logits and record the first l where the gold token attains rank 1 (The dataset consists of addition problems where two 3-digit addends result in a 3-digit sum; details in Appendix C.5).As shown in Fig. 8, all test prompts first reach top-1 within late layer states (L23-L32), with a pronounced peak at L30 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Model Layer Index (llama)  (about 45.4% of samples across five runs) and no "never top-1" cases.This indicates that decisive arithmetic commitment and output selection are finalized in the concluding third of the network, consistent with mid-layer transitions seen in sumrange and carry probes but occurring later in the computation pipeline.</p>
<p>Analysis</p>
<p>The experimental results of this study systematically reveal that when the LLAMA-3-8B-INSTRUCT large language model processes addition operations, the evolution of its internal information representation presents a clear, sequential trajectory.</p>
<p>Firstly, in the shallower layers of the network (e.g., in the formula structure recognition experiment, probes trained on two-digit numbers, when processing four-digit comparisons, show peak accuracy around layer 14), the model can efficiently identify the structural type of addition expressions (such as a+b, b+a, a+a).At this point, the probe accuracy for decoding these structural features is high, especially its good generalization to different operand lengths, indicating that the model can capture the syntactic and basic semantic composition of the input arithmetic task at early computational depths.</p>
<p>Secondly, in the middle layers of the network (roughly from layer 14 to layer 18), the decodability of core arithmetic operation features begins to become prominent.The decoding accuracy for the representation of carry signals (whether for units, tens, or hundreds digits) significantly improves in this interval, reaching a near-perfect level around layer 19, forming a clear and easily decodable representation of the key steps of the operation.Almost simultaneously (starting with a sharp rise from layer 16 and stabilizing around layers 17-19), the prediction accuracy for the overall addition result (i.e., the "sum") also reaches saturation, suggesting that a stable internal representation of the result is largely formed by these layers.</p>
<p>Subsequently, in the deeper layers of the network (especially after layer 25), probe results indicate that the model appears to focus more on a detailed numerical abstraction of the operation result.Experiments show that in these deeper layers, the model can accurately distinguish and represent the individual digits composing the "sum" (such as the hundreds digit, tens digit, units digit), linking the operation result with its specific numerical attributes.Logit Lens analysis further indicates that the generation of the final answer token primarily occurs in the final stages of the network (layers 23 to 32, peaking at layer 30).This is sequentially consistent with the maturation point of internal numerical representations (after layer 25), suggesting a potential transformation process within the model from internal numerical concepts to the final text output.</p>
<p>Furthermore, in the "arithmetic result experiment," the probe accuracy for sums across different numerical ranges (500-509 to 900-909) shows a highly consistent trend with increasing layer depth, all reaching high accuracy at similar levels (around layers [16][17][18][19].This indicates that, from the per-spective of the evolution pattern of probe accuracy, these specific numerical range problems do not exhibit a significant difficulty gradient in terms of "when they become solvable" within the model's internal processing.</p>
<p>Discussion</p>
<p>The sequential evolution pattern of information representation observed in this experiment provides important clues for understanding how LLMs perform arithmetic operations.Several phenomena are worth further discussion:</p>
<p>Firstly, representations are dynamic, with their prominence peaking at specific positions in the processing trajectory, sometimes followed by a slight decrease in decodability.This 'low-highslight decrease' pattern was observed in several probe experiments.For instance, in the formula structure representation task (Fig. 3, focusing on probes trained on 2-digit addition), the accuracy for identifying '2-digit (In-Domain Train)' patterns peaks around layer 10 and then shows a gradual decline.Similarly, the '4-digit (OOD Test)' curve in the same figure peaks around layer 14 before decreasing.A similar trend is observable in the carry signal experiment (Fig. 5).For example, the 'hundreds place' carry detection accuracy sharply peaks around layer 19-20, then exhibits a noticeable decrease in subsequent layers before a slight rebound.The 'tens place' and 'ones place' also show peaks (around layer 19 and 18 respectively) followed by a dip.</p>
<p>This pattern suggests that initially, accuracy is low as the specific arithmetic representation has not yet clearly formed.In intermediate layers, this information becomes most linearly decodable, and its representation strength and accessibility reach their peak.Subsequently, as the model processes this information or integrates it for subsequent computational stages, it may shift focus.Newly accumulated representational information from later layers (via residual connections) or the transformation of these features for downstream tasks might slightly interfere with or partially obscure the previously formed representations that are no longer central to the immediate next step of computation.This reflects the dynamic evolutionary nature of the model's internal representations, where specific information (as detected by our probes) is most clearly and accessibly represented at particular depths, rather than being monotonically refined.</p>
<p>Secondly,Generalizability of representation.Generalization experiments on the three-digit number recognition task show that a probe trained on addition generalizes very well to subtraction (peaking near 90% accuracy) and strongly to multiplication (peaking near 80%), and that its generalization to subtraction is even stronger than to multiplication.The relatively superior generalization to subtraction (i.e., marginally better performance at similarly high levels) may indicate that in this model's representation learning, subtraction and addition share more direct or more readily transferable numerical representation elements.Although multiplication (conceptually a form of repeated addition, and likewise exhibiting good generalization here) also achieves a high degree of transfer, it falls slightly short of subtraction.This suggests that, despite substantial commonality, the computational mechanism the model learns for multiplication-or the representations it forms-retain some distinctive aspects that render its transfer a bit less effective than subtraction.Overall, this shows that the model's learned numerical representations combine a certain level of abstraction and cross-operation universality with features that are specific to each arithmetic operation.</p>
<p>Thirdly, the analogical nature of the model's "computation" process.The sequential emergence and progressive refinement of features revealed in this study bear resemblance to the cognitive processes humans use to solve arithmetic problems, which involve breaking down steps and calculating progressively.From identifying the structure of the expression, to processing carries, calculating the sum, and finally confirming and expressing each digit, the model internally appears to be executing a structured, computation-like process, rather than simple pattern matching or memory retrieval.</p>
<p>Conclusion</p>
<p>Through a series of carefully designed probe experiments on the LLAMA-3-8B-INSTRUCT model, and with comparative analyses on other representative models (detailed in Appendix D), this study systematically investigated their internal information processing mechanisms when performing addition operations.The main conclusions regarding LLAMA-3-8B-INSTRUCT are as follows:</p>
<p>Sequential Information Processing Trajectory: Our findings indicate that when LLAMA-3-8B-INSTRUCT processes addition operations, the decodability of its internal arithmetic features presents a coherent, sequential evolution pattern, outlining a multi-step information processing trajectory.Information at different levels, such as the recognition of formula structure, the emergence of features related to core operations (including carry and summation), and the numerical abstraction and output organization of the result, corresponds to representations that become clearly discernible at different, sequential depths of the network.</p>
<p>Formation of Explicit Representations: At various nodes of this processing trajectory (corresponding to different network depths), the model forms internal representations of specific arithmetic features (such as carries, sum values, individual digits) that are clear, stable, and easily decodable by linear probes, and the strength of these representations changes dynamically as processing proceeds.</p>
<p>Support for "Computation-like" Process over "Rote Memorization": The observed sequential emergence of features, the intrinsic connection between the operation results and their numerical attributes, the dynamic evolution of representations, and a certain degree of cross-task generalization capability collectively constitute strong evidence.This suggests that when LLAMA-3-8B-INSTRUCT encounters simple addition problems within its training distribution, it is more inclined to perform a structured, "computation-like" process rather than primarily relying on large-scale pattern memorization.</p>
<p>Furthermore, comparative analyses with Qwen2.5-Math-7B-Instruct(see D for full details) revealed that the observed sequential processing trajectory and computation-like mechanisms are largely consistent across different model architectures, though with variations in the specific layers at which features emerge / certain model-specific strategies in representing numerical attributes which helps to contextualize the findings from LLAMA-3-8B-INSTRUCT and underscores the broader relevance of these information processing principles in LLMs.</p>
<p>In summary, the findings of this study deepen the understanding of the arithmetic capabilities of LLMs and provide empirical evidence for their complex internal information processing mechanisms, particularly revealing an ordered, computation-like internal information representation evolution path.</p>
<p>While some math-optimized models achieve higher raw accuracy on our addition benchmark, the four-stage trajectory we document is robust across architectures (Appendix D) and is most cleanly expressed in LLAMA-3-8B-INSTRUCT, which motivates our focus on this model for mechanistic analysis.</p>
<p>Limitations</p>
<p>Our study offers a first layer-wise picture of how LLAMA-3-8B-INSTRUCT handles multi-digit addition, but several caveats temper the strength of our claims.</p>
<p>Correlation vs. causation.Our analysis uses correlational tools-linear probes and the Logit Lens.These reveal that certain attributes are linearly decodable from hidden states, but do not prove that the forward pass uses those attributes, nor that the model implements a specific carrypropagation algorithm.Decodability can arise from distributed or incidental codes, and linear probes can overfit if not carefully regularized.Establishing dependence on intermediate computations requires causal interventions (e.g., activation patching, targeted ablations, or circuit-level surgery), which we leave to future work.</p>
<p>Task scope.We study in-distribution, base-10 multi-digit addition under short prompts of the form "Calculate: num1+num2 = " (with a trailing space) and evaluate 1-6 digits using exactmatch metrics.This choice reflects data realism-decimal numerals dominate pretraining corpora-while avoiding confounds from distribution shift.We do not evaluate borrowing subtraction, long multiplication, mixed-base arithmetic, chainof-thought prompting, or adversarial perturbations, and we therefore scope claims to base-10 addition under short prompts.</p>
<p>Model specificity.Our main analyses focus on LLAMA-3-8B-INSTRUCT; we additionally report probe results for several 7B-scale models, including math-optimized variants, in Appendix D. While the four-stage ordering (structure → carries/sums → digit abstractions → output alignment) recurs qualitatively, the exact layer indices and effect sizes vary across architectures and checkpoints.Thus, our claims concern ordering rather than universal layer numbers.Broader generalization may be affected by architecture, scale, tokenizer, instruction tuning, and math-oriented fine-tuning.</p>
<p>Ethical Considerations</p>
<p>This research focuses on investigating the internal information processing pathways of Large Language Models (LLMs) when performing basic multi-digit addition.The core of the study involves analyzing internal model activations using techniques like linear probing and logit lens to understand their computational mechanisms, rather than developing new applications or models.</p>
<p>We assess that this specific research work itself does not present significant ethical risks, for the following reasons:</p>
<ol>
<li>Benign Nature of the Task: The problem under investigation (multi-digit addition) is a fundamental and well-defined arithmetic task that does not involve sensitive content, personal data, or socially charged issues that might introduce bias.</li>
</ol>
<p>Non-sensitive Data:</p>
<p>The datasets used in this study (e.g., prompts of the form "Calculate: num1+num2 = " and corresponding arithmetic problems) are synthetically constructed for the research purpose and do not contain any personally identifiable information or other sensitive information.</p>
<p>Transparency in Research</p>
<p>Aim: This study aims to enhance the understanding of the internal workings of LLMs, contributing to model interpretability and transparency.</p>
<p>While Large Language Models as a broader technology may raise various ethical considerations (such as bias, misuse, environmental impact, etc.), the scope of this research is limited to a mechanistic investigation of arithmetic capabilities in specific models (e.g., LLaMA-3-8B-Instruct).It does not directly engage with these broader ethical issues.Therefore, we believe the direct ethical risks associated with the current work are minimal and manageable.</p>
<p>Fangwei Zhu, Damai Dai, and Zhifang Sui.2025.Language models encode the value of numbers linearly.</p>
<p>In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 693-709.Association for Computational Linguistics.</p>
<p>A Detailed Model Performance Metrics</p>
<p>This appendix presents a detailed summary of the performance metrics for the different language models evaluated in this study.Table 2 offers a comparative analysis of these models based on their accuracy in specific numerical prediction tasks.</p>
<p>Each row in the table corresponds to a distinct language model, identified under the "Model ID" column.The "Overall Accuracy" column provides a general measure of each model's performance across all evaluated tasks.The subsequent columns, labeled from "1-digit" through "6-digit," specify the model's accuracy in correctly predicting numerical values composed of one to six digits, respectively.All accuracy figures presented in the table are expressed as percentages.This data provides insight into the capabilities and limitations of each model concerning tasks that require numerical precision and understanding of magnitude.</p>
<p>B General Experimental Setup</p>
<p>B.1 Language Model</p>
<p>All experiments used the LLaMA-3-8B-Instruct model.Model parameters were kept frozen for all probing runs.The network has 32 transformer layers.</p>
<p>B.2 Input Representation for Probes</p>
<p>For each of the 32 layers an independent diagnostic probe was trained.The probe input was the hidden state of the last token of the prompt Calculate: num1 + num2 =-namely the trailing space after the "=" sign.</p>
<p>B.3 Probe Architecture</p>
<p>Unless otherwise stated (see Appendix C), every probe consisted of • a single linear classification layer, followed by</p>
<p>• a SOFTMAX activation.</p>
<p>This yields a probability distribution over the taskspecific classes.</p>
<p>B.4 Training and Evaluation</p>
<p>• Data splitting: Each dataset was split 80 %/20 % into train/test using stratified sampling to preserve class ratios.Training data.</p>
<p>• Type 1: 5000 random samples Task.Predict the exact sum; each class is one sum value.</p>
<p>Dataset generation.</p>
<p>• 400 examples per class with unconstrained addends.</p>
<p>• Five groups of sums: 500-509, 600-609, 700-709, 800-809, 900-909.</p>
<p>C.3 Carry-Signal Experiment</p>
<p>Task.Binary classification-detect a carry in the units, tens, or hundreds place of three-digit additions.</p>
<p>Datasets.Three balanced sets of 1 000 problems each (one per decimal place).A single problem may appear in several sets if its carries differ by place.</p>
<p>Figure 14: AceMath: Layer-wise accuracy on arithmetic detection across numerical ranges (500-509 to 900-909).The same anomaly applies for the "a + b = 500" slice (correct ∼10%).</p>
<p>D.3 Carry Signal Experiment</p>
<p>Figure 1 :
1
Figure 1: Multi-stage information flow for addition in LLMs.As depth increases, internal representations sequentially expose (1) problem structure (operand/operator layout), (2) core computations (column sums and carries), (3) result-level numerical abstractions (digit identities), and (4) output-aligned organization that drives final token generation.Each signal type is operationalized via a linear-probe task and becomes decodable at different depths.</p>
<p>Figure 2 :
2
Figure2: Representation extraction sites and diagnostics for arithmetic reasoning.For an addition prompt ("Calculate: num1 + num2 = "),we record the hidden state of the last input token at every layer state (L0-L32) of LLAMA-3-8B-INSTRUCT.At layer l, we compute (i) a logit lens projection of the last-token activation, ℓ (l) = h (l)</p>
<p>Figure 3 :
3
Figure 3: Layer-wise decodability of formula structure (3-way classification: a+b, b+a, a+a with a &gt; b).At each layer state (L0-L32) of LLAMA-3-8B-INSTRUCT, we train a linear probe on the last-inputtoken state h (l)</p>
<p>Figure 5 :
5
Figure 5: Carry detection by position (3-digit addition).Mean probe accuracy by layer for ones/tens/hundreds positions (5 runs; balanced labels).Curves rise from the 0.50 random baseline, begin increasing consistently around ∼L14, and approach ceiling by ∼L19, indicating efficient carry processing in mid-to-late layers.</p>
<p>Figure 6 :
6
Figure 6: Digit-wise decodability of 3-digit sums.Mean probe accuracy (±95% CI over 5 runs) by layer for predicting the ones, tens, and hundreds digits from h</p>
<p>Figure 7 :
7
Figure7: Hundreds-digit probe generalization across operations.Mean accuracy (±95% CI over 5 runs) by layer when a probe trained on addition is tested on addition, subtraction, and multiplication.All curves exceed the 10-class chance level (0.10); subtraction generalizes more strongly than multiplication.</p>
<p>Figure 8 :
8
Figure 8: Earliest layer where the gold next token becomes top-1 (logit lens).Histogram over N =1000 three-digit additions showing, for each layer state in L23-L32, the mean number of samples whose gold token first becomes top-1.All examples first reach top-1 in this late-layer range, peaking at L30 (∼45.4% of samples).</p>
<p>Figure 15 :Figure 16 :LayerFigure 17 :
151617
Figure 15: Mistral-7B-Instruct: Carry detectionrecognition accuracy of ones, tens, and hundreds at each layer.</p>
<p>Figure 18 :
18
Figure 18: Mistral-7B-Instruct: Layer-wise recognition accuracy for ones, tens, and hundreds of the sum.</p>
<p>Figure 19 :
19
Figure 19: Qwen2.5-Math-7B:Layer-wise recognition accuracy for ones, tens, and hundreds of the sum.</p>
<p>Figure 20 :Figure 21 :Figure 23 :Figure 24 :
20212324
Figure 20: AceMath: Layer-wise recognition accuracy for ones, tens, and hundreds of the sum.</p>
<p>Figure 25: Qwen2.5-Math-7B:Logit Lens analysis (layers 23-32), distribution of first top-1 correct token.</p>
<p>Figure 26: AceMath: Logit Lens analysis, distribution of first top-1 correct token across the last 10 layers.</p>
<p>Table 1
1: Overall accuracy on 1-6 digit addition (exactmatch).  † denotes math-optimized models. Althougha math-optimized model attains the highest accuracy,LLAMA-3-8B-INSTRUCT exhibits the clearest andmost consistent layer-wise probe signals (Sections 4-6),so we focus on it for the mechanistic analyses. SeeAppendix A for evaluation details and Appendix D forper-model probe curves and results on the other models.
Unless otherwise noted, the base model is frozen; probes are trained for 10 epochs and the checkpoint is selected by validation accuracy. Implementation details are in Appendix B.
AcknowledgementsThis work was supported in part by the Chongqing Key Lab of Cognitive Intelligence and Intelligent Finance and the College of Computer and Information Science at Chongqing Normal University.Computational resources were provided by the Chongqing Key Lab of Cognitive Intelligence and Intelligent Finance.This research was funded by the Chongqing Graduate Student Research Innovation Project (Grant No. YKC24001).Model IDOverall Accuracy 1-digit 2-digit 3-digit 4-digit 5-digit 6-digitC.4 Numerical Abstraction of ResultsDigit-identification task.Given a sum that is three digits long, predict its hundreds, tens, and units digits (10-way classification each).The dataset contains 1 000 two-addend problems.Hundreds-digit generalization.A probe trained on hundreds-digit identification was also tested on subtraction and multiplication datasets whose results are mostly three-digit numbers, generated with the same principles.C.5 Logit Lens AnalysisTask.Find the earliest transformer layer whose output distribution ranks the correct sum token at top-1 (logit lens).Dataset.1000 randomly generated problems: two three-digit addends yielding a three-digit sum.Linear Decodability vs Depth (95% CI over 5 reps)D Model Experiment Results
Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, 10.1162/COLI_A_00422Comput. Linguistics. 4812022</p>
<p>Neural networks learn representation theory: Reverse engineering how networks perform group operations. Bilal Chughtai, Lawrence Chan, Neel Nanda, ICLR 2023 Workshop on Physics for Machine Learning. 2023</p>
<p>Alan Schelten, Alex Vaughan, and 1 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan, 10.18653/V1/2023.EMNLP-MAIN.299Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Language models use trigonometry to do addition. Subhash Kantamneni, Max Tegmark, ICLR 2025 Workshop on Building Trust in Language Models and Applications. 2025</p>
<p>Backward lens: Projecting language model gradients into the vocabulary space. Shahar Katz, Yonatan Belinkov, Mor Geva, Lior Wolf, 10.18653/V1/2024.EMNLP-MAIN.142Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>Carrying over algorithm in transformers. Jorrit Kruthoff, arXiv:2401.079932024Preprint</p>
<p>Language models encode numbers using digit representations in base 10. Amit Arnold, Levy , Mor Geva, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlbuquerque, New MexicoAssociation for Computational Linguistics20252Short Papers)</p>
<p>Acemath: Advancing frontier math reasoning with post-training and reward modeling. Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, 10.18653/v1/2025.findings-acl.206Findings of the Association for Computational Linguistics: ACL 2025. Vienna, AustriaAssociation for Computational Linguistics2025</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Arithmetic without algorithms: Language models solve math with a bag of heuristics. Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov, arXiv:2410.21272.nostalgebraist.2020Interpreting gpt: the logit lens. 2024Preprint</p>
<p>Understanding addition in transformers. Philip Quirke, Fazl Barez, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, 10.18653/V1/2023.EMNLP-MAIN.435Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Do phd-level llms truly grasp elementary addition? probing rule learning vs. memorization in large language models. Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan, arXiv:2504.052622025Preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, 10.48550/ARXIV.2409.12122CoRR, abs/2409.12122Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. </p>
<p>Interpreting arithmetic mechanism in large language models through comparative neuron analysis. Zeping Yu, Sophia Ananiadou, 10.18653/v1/2024.emnlp-main.193Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Pre-trained large language models use fourier features to compute addition. Tianyi Zhou, Deqing Fu, Sharan Vatsal, Robin Jia, GenAcc: 0.65) 600-609 (Mean GenAcc: 0.85) 700-709 (Mean GenAcc: 0.52) 800-809 (Mean GenAcc: 0.54) 900-909 (Mean GenAcc: 0.73) Baseline: 0.10Advances in Neural Information Processing Systems. Curran Associates, Inc202437</p>            </div>
        </div>

    </div>
</body>
</html>
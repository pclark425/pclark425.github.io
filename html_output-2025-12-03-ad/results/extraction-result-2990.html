<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2990 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2990</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2990</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-222341348</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2020.blackboxnlp-1.18.pdf" target="_blank">Probing for Multilingual Numerical Understanding in Transformer-Based Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Natural language numbers are an example of compositional structures, where larger numbers are composed of operations on smaller numbers. Given that compositional reasoning is a key to natural language understanding, we propose novel multilingual probing tasks tested on DistilBERT, XLM, and BERT to investigate for evidence of compositional reasoning over numerical data in various natural language number systems. By using both grammaticality judgment and value comparison classification tasks in English, Japanese, Danish, and French, we find evidence that the information encoded in these pretrained models’ embeddings is sufficient for grammaticality judgments but generally not for value comparisons. We analyze possible reasons for this and discuss how our tasks could be extended in further studies.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2990.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2990.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistilBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>distilbert-base-multilingual-cased (DistilBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled, multilingual transformer encoder used as a probe backbone in this study (≈134M parameters); pretrained with masked language modeling and evaluated for encoding of numerical compositional/symbolic information in its embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DistilBERT (distilbert-base-multilingual-cased)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder (DistilBERT) — distilled variant of BERT; paper reports ~134 million parameters; pretrained with masked language modeling on multilingual corpora (Wikipedia) and used here as a frozen-embedding probe and also fine-tuned for tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grammaticality judgment of spelled-out number words (compositional number formation) and value comparison (magnitude comparison) of spelled-out numbers across multiple languages (English, Japanese, Danish, French). No explicit arithmetic operations (addition/multiplication) were evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pretrained embeddings appear to encode surface-level/compositional syntactic regularities sufficient for grammaticality judgments (pattern matching/memorization of formation rules), but do not robustly encode semantic magnitude information necessary for magnitude comparison; model capacity can learn magnitude when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Probing with a frozen-language-model + single-hidden-layer MLP classifier: DistilBERT's pretrained embeddings yielded high/better-than-chance accuracy on Task 1 (grammaticality), in some cases reaching 100%, while Task 2 (value comparison) performance was mostly insufficient; performance on bare numbers > numbers embedded in sentences; fine-tuning the full model for the tasks yields >99% accuracy after sufficient epochs, showing capacity to learn mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The fact that full fine-tuning attains >99% accuracy indicates the model has the capacity to represent and compute magnitude-related distinctions, but these are not necessarily present in the pretrained embeddings; authors also note lack of direct interpretability analyses (no representational dissection) so the precise internal encoding is not demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Two interventions reported: (1) probing setup using an MLP probe on top of frozen pretrained DistilBERT embeddings; (2) full-model fine-tuning on the tasks. Data-variation intervention: comparing 'bare' number inputs vs numbers-in-sentence templates.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>MLP probe (frozen LM) — succeeded on grammaticality (Task 1) but largely failed on magnitude comparison (Task 2). Full fine-tuning — models (including DistilBERT) reach very high accuracy (>99% after 20 epochs) on both tasks, showing that supervised training can teach required mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probing (frozen embeddings + MLP): Task 1 (grammaticality) — 'better-than-chance' across languages; DistilBERT sometimes reached 100% on Task 1 in some settings (paper text). Task 2 (value comparison) — described as 'mostly insufficient' for pretrained embeddings. Fine-tuning: after 20 epochs models reach >99% validation accuracy on all languages/tasks except Danish sentence tasks (~95%). Dataset sizes used: train=30,000, val=10,000, test=10,000 per variant.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Pretrained embeddings fail to reliably encode numeric magnitude (value comparison) despite encoding grammatical/compositional form; performance degrades when numbers are embedded in sentences vs bare numbers; relies on surface cues rather than deep semantic magnitude; sensitivity to model size/architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct quantitative comparison to human performance or symbolic calculators was performed. The paper references psycholinguistic findings about human learning (language transparency effects on child counting) but does not compute explicit human-vs-model metrics or compare to algorithmic arithmetic methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2990.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2990.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (multilingual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (base-multilingual-cased)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multilingual BERT used as a backbone probe (≈110M parameters); evaluated for whether its pretrained embeddings encode compositional and semantic information about spelled-out numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (base-multilingual-cased)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base multilingual BERT encoder (~110M parameters) pretrained with masked language modeling on multilingual Wikipedia. Used both as a frozen-embedding probe (MLP classifier on top) and in fine-tuning experiments in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grammaticality judgement of spelled-out number words and value (magnitude) comparison between spelled-out numbers in multiple languages; no explicit arithmetic operations evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Like other transformers in the study, BERT's pretrained embeddings appear to encode surface-level regularities enabling grammaticality judgments (pattern recognition/memorization) but do not reliably encode magnitude semantics; model size/architecture influences performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Probing experiments (frozen embeddings + MLP) show BERT embeddings give better-than-chance performance on Task 1 but worse performance than DistilBERT and XLM, particularly poor in French where gaps exceed ~20 percentage points (reported qualitatively). Fine-tuning yields high accuracies, indicating the task is learnable.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Full fine-tuning leads to >99% accuracy, showing BERT can learn magnitude distinctions under supervised adaptation, which argues that lack of pretrained magnitude encoding is not a capacity limitation but an encoding/training-data limitation. The paper provides no representational/interpretability analysis to show how BERT internally represents numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>MLP probe on frozen BERT embeddings; full-model fine-tuning; input-format intervention (bare numbers vs sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>MLP probe: allowed grammatical judgments but poor magnitude comparison; full fine-tuning: rapid improvement to >99% accuracy across tasks/languages (except Danish sentence case ~95%). Embedding-freeze + probe isolates what is encoded in pretrained weights; fine-tuning demonstrates learning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probing (frozen embeddings + MLP): Task 1 performance 'better-than-chance' but generally worse than DistilBERT and XLM; Task 2 pretrained performance described as 'mostly insufficient.' In French, BERT performed markedly worse than other models (gap sometimes >20 points). Fine-tuning: >99% validation accuracy after 20 epochs in most settings; Danish sentences ~95%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Worse pretrained performance relative to larger models; poor in particular languages (French) on pretrained probes; struggles to encode magnitude without task-specific supervision; sentence contexts degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human/symbolic comparison performed. The study references prior human-focused work on number naming transparency but does not benchmark BERT against algorithmic arithmetic routines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2990.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2990.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLM (xlm-mlm-100-1280)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large cross-lingual masked-language-model (reported ~550M parameters) evaluated for whether pretrained embeddings encode numerical compositionality and magnitude across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Crosslingual language model pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLM (xlm-mlm-100-1280)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large cross-lingual transformer pretrained with masked language modeling (XLM) — reported here as ~550M parameters (inexact). Used as a frozen-embedding probe and fine-tuned on probing tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grammaticality judgement of spelled-out number words and value (magnitude) comparison across multiple languages; does not include arithmetic operations like addition.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pretrained XLM embeddings encode compositional syntactic information sufficient for grammaticality detection and tend to perform best or near-best on some tasks (often outperforming BERT), but like other models they do not robustly encode magnitude semantics in pretrained embeddings; larger parameter count correlates with better pretrained performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Probe results: XLM frequently outperforms other tested models on pretrained probes, particularly on English and Japanese in Task 2 (bare numbers), indicating larger models/parametric scale better capture some numerical information; however Task 2 is still described as mostly insufficient in pretrained embeddings overall.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite superior pretrained performance relative to BERT in many settings, XLM's pretrained embeddings still fail to reliably support magnitude comparisons without supervision; fine-tuning still required to reach >99% accuracy, indicating pretrained magnitude encoding remains weak.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Frozen-embedding + MLP probe; full-model fine-tuning experiments; bare vs sentence input variants.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Frozen embedding probe: performed well on grammaticality and in some cases best on bare-number Task 2, but overall magnitude-comparison capability from pretrained embeddings remains inadequate. Fine-tuning: yields >99% accuracy after training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probing (frozen embeddings + MLP): Task 1 — good, above chance; Task 2 — XLM often best among models for English and Japanese bare-number comparisons but overall described as 'mostly insufficient' without fine-tuning. Fine-tuning: >99% validation accuracy after 20 epochs (dataset sizes: train=30k, val=10k, test=10k).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Even large pretrained models with many parameters do not consistently encode magnitude semantics in embeddings; performance drops when numbers are in sentences vs bare; reliance on model size rather than principled representation of numeric meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison to human or symbolic arithmetic systems; paper notes that larger models perform better but does not equate this with algorithmic numeric reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2990.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2990.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probing setup (MLP probe & fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLP probing of frozen pretrained embeddings and full-model fine-tuning (this paper's probing methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's experimental paradigm: (A) an MLP probe (single hidden layer) trained on top of frozen pretrained LM embeddings to test what numerical information is already encoded, and (B) full-model fine-tuning on the same tasks to measure learnability under supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Probing pipeline: frozen-LM + MLP probe; and end-to-end fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probe: pretrained transformer embeddings (BERT/DistilBERT/XLM) are frozen and a single-hidden-layer MLP classifier is trained on top; Fine-tuning: the full transformer + classifier are trained on the tasks. Both variants tested with two input conditions: 'bare' spelled-out numbers and numbers embedded in short sentence templates. Datasets generated with num2words; training splits: 30k train, 10k val, 10k test.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two tasks: (1) Grammaticality judgment of spelled-out number words (syntactic/compositional structure) and (2) Value comparison (magnitude comparison) of pairs of spelled-out numbers. No arithmetic operations like addition/multiplication were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>The probe is intended to reveal whether pretrained embeddings contain (a) syntactic/compositional rules for number-word formation and (b) semantic magnitude representations. Results indicate pretrained embeddings hold syntactic/compositional cues but lack reliable magnitude representations; supervised fine-tuning can produce magnitude-sensitive behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Frozen-embedding + MLP probe: Task 1 success indicates syntactic/compositional cues are present in embeddings; Task 2 failure indicates magnitude semantics are not robustly encoded. Full fine-tuning: task performance reaches >99% accuracy, showing that supervised learning can instill the necessary mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Because full fine-tuning succeeds, the lack of magnitude knowledge in pretrained embeddings likely stems from pretraining data/distribution or objective (e.g., masked LM emphasizing grammaticality), not from an absolute inability of the architectures to represent numeric magnitude. The study did not perform representational dissection or causal interventions to localize numeric representations in the model.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Probe intervention (freeze LM weights, train MLP); full-model fine-tuning (train entire model); input-format intervention (bare vs sentence). No use of prompting, chain-of-thought, external calculators, or algorithmic augmentation was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Frozen-embedding probing isolates pretraining-encoded knowledge: shows syntactic knowledge but weak semantic magnitude. Full fine-tuning converts poor-pretrained-magnitude behavior into near-perfect magnitude discrimination (>99%), demonstrating learnability. Using sentence-context inputs consistently reduced probe performance relative to bare numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probe (frozen embeddings + MLP): Task 1 — better-than-chance across languages; Task 2 — mostly insufficient in pretrained embeddings. Fine-tuning: after 20 epochs validation accuracies >99% across languages/tasks (except Danish sentence tasks ~95%). Dataset: numbers in range 0–999 used for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Probing with frozen embeddings underestimates what models can learn under supervision; frozen probes reveal the lack of pretrained magnitude semantics but do not locate internal representations. Sentence contexts introduce noise and reduce pretrained-embedding probe accuracy; masked language modeling pretraining objective may bias toward grammatical prediction rather than semantic magnitude encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human-versus-model or symbolic-calculator comparison for arithmetic capability; the probing setup is diagnostic of what pretrained embeddings encode versus what can be learned with supervision, rather than an assessment of algorithmic arithmetic abilities comparable to calculators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do nlp models know numbers? probing numeracy in embeddings. <em>(Rating: 2)</em></li>
                <li>What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models <em>(Rating: 2)</em></li>
                <li>Probing for semantic evidence of composition by means of simple classification tasks <em>(Rating: 2)</em></li>
                <li>Analysis methods in neural language processing: A survey. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2990",
    "paper_id": "paper-222341348",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "DistilBERT",
            "name_full": "distilbert-base-multilingual-cased (DistilBERT)",
            "brief_description": "A distilled, multilingual transformer encoder used as a probe backbone in this study (≈134M parameters); pretrained with masked language modeling and evaluated for encoding of numerical compositional/symbolic information in its embeddings.",
            "citation_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
            "mention_or_use": "use",
            "model_name": "DistilBERT (distilbert-base-multilingual-cased)",
            "model_description": "Transformer encoder (DistilBERT) — distilled variant of BERT; paper reports ~134 million parameters; pretrained with masked language modeling on multilingual corpora (Wikipedia) and used here as a frozen-embedding probe and also fine-tuned for tasks.",
            "arithmetic_task_type": "Grammaticality judgment of spelled-out number words (compositional number formation) and value comparison (magnitude comparison) of spelled-out numbers across multiple languages (English, Japanese, Danish, French). No explicit arithmetic operations (addition/multiplication) were evaluated in this paper.",
            "reported_mechanism": "Pretrained embeddings appear to encode surface-level/compositional syntactic regularities sufficient for grammaticality judgments (pattern matching/memorization of formation rules), but do not robustly encode semantic magnitude information necessary for magnitude comparison; model capacity can learn magnitude when fine-tuned.",
            "evidence_for_mechanism": "Probing with a frozen-language-model + single-hidden-layer MLP classifier: DistilBERT's pretrained embeddings yielded high/better-than-chance accuracy on Task 1 (grammaticality), in some cases reaching 100%, while Task 2 (value comparison) performance was mostly insufficient; performance on bare numbers &gt; numbers embedded in sentences; fine-tuning the full model for the tasks yields &gt;99% accuracy after sufficient epochs, showing capacity to learn mappings.",
            "evidence_against_mechanism": "The fact that full fine-tuning attains &gt;99% accuracy indicates the model has the capacity to represent and compute magnitude-related distinctions, but these are not necessarily present in the pretrained embeddings; authors also note lack of direct interpretability analyses (no representational dissection) so the precise internal encoding is not demonstrated.",
            "intervention_type": "Two interventions reported: (1) probing setup using an MLP probe on top of frozen pretrained DistilBERT embeddings; (2) full-model fine-tuning on the tasks. Data-variation intervention: comparing 'bare' number inputs vs numbers-in-sentence templates.",
            "effect_of_intervention": "MLP probe (frozen LM) — succeeded on grammaticality (Task 1) but largely failed on magnitude comparison (Task 2). Full fine-tuning — models (including DistilBERT) reach very high accuracy (&gt;99% after 20 epochs) on both tasks, showing that supervised training can teach required mappings.",
            "performance_metrics": "Probing (frozen embeddings + MLP): Task 1 (grammaticality) — 'better-than-chance' across languages; DistilBERT sometimes reached 100% on Task 1 in some settings (paper text). Task 2 (value comparison) — described as 'mostly insufficient' for pretrained embeddings. Fine-tuning: after 20 epochs models reach &gt;99% validation accuracy on all languages/tasks except Danish sentence tasks (~95%). Dataset sizes used: train=30,000, val=10,000, test=10,000 per variant.",
            "notable_failure_modes": "Pretrained embeddings fail to reliably encode numeric magnitude (value comparison) despite encoding grammatical/compositional form; performance degrades when numbers are embedded in sentences vs bare numbers; relies on surface cues rather than deep semantic magnitude; sensitivity to model size/architecture.",
            "comparison_to_humans_or_symbolic": "No direct quantitative comparison to human performance or symbolic calculators was performed. The paper references psycholinguistic findings about human learning (language transparency effects on child counting) but does not compute explicit human-vs-model metrics or compare to algorithmic arithmetic methods.",
            "uuid": "e2990.0"
        },
        {
            "name_short": "BERT (multilingual)",
            "name_full": "BERT (base-multilingual-cased)",
            "brief_description": "Multilingual BERT used as a backbone probe (≈110M parameters); evaluated for whether its pretrained embeddings encode compositional and semantic information about spelled-out numbers.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "use",
            "model_name": "BERT (base-multilingual-cased)",
            "model_description": "Base multilingual BERT encoder (~110M parameters) pretrained with masked language modeling on multilingual Wikipedia. Used both as a frozen-embedding probe (MLP classifier on top) and in fine-tuning experiments in this study.",
            "arithmetic_task_type": "Grammaticality judgement of spelled-out number words and value (magnitude) comparison between spelled-out numbers in multiple languages; no explicit arithmetic operations evaluated.",
            "reported_mechanism": "Like other transformers in the study, BERT's pretrained embeddings appear to encode surface-level regularities enabling grammaticality judgments (pattern recognition/memorization) but do not reliably encode magnitude semantics; model size/architecture influences performance.",
            "evidence_for_mechanism": "Probing experiments (frozen embeddings + MLP) show BERT embeddings give better-than-chance performance on Task 1 but worse performance than DistilBERT and XLM, particularly poor in French where gaps exceed ~20 percentage points (reported qualitatively). Fine-tuning yields high accuracies, indicating the task is learnable.",
            "evidence_against_mechanism": "Full fine-tuning leads to &gt;99% accuracy, showing BERT can learn magnitude distinctions under supervised adaptation, which argues that lack of pretrained magnitude encoding is not a capacity limitation but an encoding/training-data limitation. The paper provides no representational/interpretability analysis to show how BERT internally represents numbers.",
            "intervention_type": "MLP probe on frozen BERT embeddings; full-model fine-tuning; input-format intervention (bare numbers vs sentences).",
            "effect_of_intervention": "MLP probe: allowed grammatical judgments but poor magnitude comparison; full fine-tuning: rapid improvement to &gt;99% accuracy across tasks/languages (except Danish sentence case ~95%). Embedding-freeze + probe isolates what is encoded in pretrained weights; fine-tuning demonstrates learning ability.",
            "performance_metrics": "Probing (frozen embeddings + MLP): Task 1 performance 'better-than-chance' but generally worse than DistilBERT and XLM; Task 2 pretrained performance described as 'mostly insufficient.' In French, BERT performed markedly worse than other models (gap sometimes &gt;20 points). Fine-tuning: &gt;99% validation accuracy after 20 epochs in most settings; Danish sentences ~95%.",
            "notable_failure_modes": "Worse pretrained performance relative to larger models; poor in particular languages (French) on pretrained probes; struggles to encode magnitude without task-specific supervision; sentence contexts degrade performance.",
            "comparison_to_humans_or_symbolic": "No direct human/symbolic comparison performed. The study references prior human-focused work on number naming transparency but does not benchmark BERT against algorithmic arithmetic routines.",
            "uuid": "e2990.1"
        },
        {
            "name_short": "XLM",
            "name_full": "XLM (xlm-mlm-100-1280)",
            "brief_description": "Large cross-lingual masked-language-model (reported ~550M parameters) evaluated for whether pretrained embeddings encode numerical compositionality and magnitude across languages.",
            "citation_title": "Crosslingual language model pretraining.",
            "mention_or_use": "use",
            "model_name": "XLM (xlm-mlm-100-1280)",
            "model_description": "A large cross-lingual transformer pretrained with masked language modeling (XLM) — reported here as ~550M parameters (inexact). Used as a frozen-embedding probe and fine-tuned on probing tasks in this study.",
            "arithmetic_task_type": "Grammaticality judgement of spelled-out number words and value (magnitude) comparison across multiple languages; does not include arithmetic operations like addition.",
            "reported_mechanism": "Pretrained XLM embeddings encode compositional syntactic information sufficient for grammaticality detection and tend to perform best or near-best on some tasks (often outperforming BERT), but like other models they do not robustly encode magnitude semantics in pretrained embeddings; larger parameter count correlates with better pretrained performance.",
            "evidence_for_mechanism": "Probe results: XLM frequently outperforms other tested models on pretrained probes, particularly on English and Japanese in Task 2 (bare numbers), indicating larger models/parametric scale better capture some numerical information; however Task 2 is still described as mostly insufficient in pretrained embeddings overall.",
            "evidence_against_mechanism": "Despite superior pretrained performance relative to BERT in many settings, XLM's pretrained embeddings still fail to reliably support magnitude comparisons without supervision; fine-tuning still required to reach &gt;99% accuracy, indicating pretrained magnitude encoding remains weak.",
            "intervention_type": "Frozen-embedding + MLP probe; full-model fine-tuning experiments; bare vs sentence input variants.",
            "effect_of_intervention": "Frozen embedding probe: performed well on grammaticality and in some cases best on bare-number Task 2, but overall magnitude-comparison capability from pretrained embeddings remains inadequate. Fine-tuning: yields &gt;99% accuracy after training.",
            "performance_metrics": "Probing (frozen embeddings + MLP): Task 1 — good, above chance; Task 2 — XLM often best among models for English and Japanese bare-number comparisons but overall described as 'mostly insufficient' without fine-tuning. Fine-tuning: &gt;99% validation accuracy after 20 epochs (dataset sizes: train=30k, val=10k, test=10k).",
            "notable_failure_modes": "Even large pretrained models with many parameters do not consistently encode magnitude semantics in embeddings; performance drops when numbers are in sentences vs bare; reliance on model size rather than principled representation of numeric meaning.",
            "comparison_to_humans_or_symbolic": "No direct comparison to human or symbolic arithmetic systems; paper notes that larger models perform better but does not equate this with algorithmic numeric reasoning.",
            "uuid": "e2990.2"
        },
        {
            "name_short": "Probing setup (MLP probe & fine-tuning)",
            "name_full": "MLP probing of frozen pretrained embeddings and full-model fine-tuning (this paper's probing methodology)",
            "brief_description": "The paper's experimental paradigm: (A) an MLP probe (single hidden layer) trained on top of frozen pretrained LM embeddings to test what numerical information is already encoded, and (B) full-model fine-tuning on the same tasks to measure learnability under supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Probing pipeline: frozen-LM + MLP probe; and end-to-end fine-tuning",
            "model_description": "Probe: pretrained transformer embeddings (BERT/DistilBERT/XLM) are frozen and a single-hidden-layer MLP classifier is trained on top; Fine-tuning: the full transformer + classifier are trained on the tasks. Both variants tested with two input conditions: 'bare' spelled-out numbers and numbers embedded in short sentence templates. Datasets generated with num2words; training splits: 30k train, 10k val, 10k test.",
            "arithmetic_task_type": "Two tasks: (1) Grammaticality judgment of spelled-out number words (syntactic/compositional structure) and (2) Value comparison (magnitude comparison) of pairs of spelled-out numbers. No arithmetic operations like addition/multiplication were performed.",
            "reported_mechanism": "The probe is intended to reveal whether pretrained embeddings contain (a) syntactic/compositional rules for number-word formation and (b) semantic magnitude representations. Results indicate pretrained embeddings hold syntactic/compositional cues but lack reliable magnitude representations; supervised fine-tuning can produce magnitude-sensitive behavior.",
            "evidence_for_mechanism": "Frozen-embedding + MLP probe: Task 1 success indicates syntactic/compositional cues are present in embeddings; Task 2 failure indicates magnitude semantics are not robustly encoded. Full fine-tuning: task performance reaches &gt;99% accuracy, showing that supervised learning can instill the necessary mappings.",
            "evidence_against_mechanism": "Because full fine-tuning succeeds, the lack of magnitude knowledge in pretrained embeddings likely stems from pretraining data/distribution or objective (e.g., masked LM emphasizing grammaticality), not from an absolute inability of the architectures to represent numeric magnitude. The study did not perform representational dissection or causal interventions to localize numeric representations in the model.",
            "intervention_type": "Probe intervention (freeze LM weights, train MLP); full-model fine-tuning (train entire model); input-format intervention (bare vs sentence). No use of prompting, chain-of-thought, external calculators, or algorithmic augmentation was evaluated.",
            "effect_of_intervention": "Frozen-embedding probing isolates pretraining-encoded knowledge: shows syntactic knowledge but weak semantic magnitude. Full fine-tuning converts poor-pretrained-magnitude behavior into near-perfect magnitude discrimination (&gt;99%), demonstrating learnability. Using sentence-context inputs consistently reduced probe performance relative to bare numbers.",
            "performance_metrics": "Probe (frozen embeddings + MLP): Task 1 — better-than-chance across languages; Task 2 — mostly insufficient in pretrained embeddings. Fine-tuning: after 20 epochs validation accuracies &gt;99% across languages/tasks (except Danish sentence tasks ~95%). Dataset: numbers in range 0–999 used for generation.",
            "notable_failure_modes": "Probing with frozen embeddings underestimates what models can learn under supervision; frozen probes reveal the lack of pretrained magnitude semantics but do not locate internal representations. Sentence contexts introduce noise and reduce pretrained-embedding probe accuracy; masked language modeling pretraining objective may bias toward grammatical prediction rather than semantic magnitude encoding.",
            "comparison_to_humans_or_symbolic": "No direct human-versus-model or symbolic-calculator comparison for arithmetic capability; the probing setup is diagnostic of what pretrained embeddings encode versus what can be learned with supervision, rather than an assessment of algorithmic arithmetic abilities comparable to calculators.",
            "uuid": "e2990.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do nlp models know numbers? probing numeracy in embeddings.",
            "rating": 2,
            "sanitized_title": "do_nlp_models_know_numbers_probing_numeracy_in_embeddings"
        },
        {
            "paper_title": "What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "rating": 2,
            "sanitized_title": "what_bert_is_not_lessons_from_a_new_suite_of_psycholinguistic_diagnostics_for_language_models"
        },
        {
            "paper_title": "Probing for semantic evidence of composition by means of simple classification tasks",
            "rating": 2,
            "sanitized_title": "probing_for_semantic_evidence_of_composition_by_means_of_simple_classification_tasks"
        },
        {
            "paper_title": "Analysis methods in neural language processing: A survey.",
            "rating": 1,
            "sanitized_title": "analysis_methods_in_neural_language_processing_a_survey"
        }
    ],
    "cost": 0.013760999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Probing for Multilingual Numerical Understanding in Transformer-Based Language Models
Association for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 20, 2020. 2020</p>
<p>Devin Johnson 
Department of Linguistics
University of Washington</p>
<p>Denise Mak 
Department of Linguistics
University of Washington</p>
<p>Drew Barker 
Department of Linguistics
University of Washington</p>
<p>Lexi Loessberg-Zahl lexilz@uw.edu 
Department of Linguistics
University of Washington</p>
<p>Probing for Multilingual Numerical Understanding in Transformer-Based Language Models</p>
<p>Online
the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPAssociation for Computational LinguisticsNovember 20, 2020. 2020184
Natural language numbers are an example of compositional structures, where larger numbers are composed of operations on smaller numbers. Given that compositional reasoning is a key to natural language understanding, we propose novel multilingual probing tasks tested on DistilBERT, XLM, and BERT to investigate for evidence of compositional reasoning over numerical data in various natural language number systems. By using both grammaticality judgment and value comparison classification tasks in English, Japanese, Danish, and French, we find evidence that the information encoded in these pretrained models' embeddings is sufficient for grammaticality judgments but generally not for value comparisons. We analyze possible reasons for this and discuss how our tasks could be extended in further studies.</p>
<p>Introduction</p>
<p>In recent years, transformer-based language models such as BERT (Devlin et al., 2018), XLM (Lample and Conneau, 2019), and DistilBERT (Sanh et al., 2020) have achieved unprecedented results on a wide array of natural language understanding tasks, even when such models are trained on other tasks (i.e. transfer learning). In light of this success, there has been increased interest in investigating what particular information transformer-based language models encode in their word embeddings during pretraining that allows them to perform well in transfer learning experiments. Put into the context of this paper, we may ask: do such models gain certain linguistic/compositional understanding from pretraining? Attempts to assess such phenomena are commonly referred to as probing experiments. In this paper, we introduce a novel probing task using targeted datasets and classification tasks aimed at evaluating models' compositional reasoning capabilities in relation to numbers 1 in multiple natural languages.</p>
<p>We choose both grammaticality judgment and value comparison tasks (Section 3) to assess multilingual DistilBERT, XLM, and BERT 2 over various number systems. We argue that high performance on these tasks indicates some ability of reasoning over compositional structures, particularly over the rules generating valid compositional structures (task 1) and the resultant meanings of the structures (task 2). After probing the selected models on our tasks, we discuss explanations for the performance of models, as well as possible future extensions to this probing task schema. Additionally, studies in cognitive psychology such as Miller et al. (1995) assert that children learning more transparent number systems (i.e. those exhibiting more regularity in their surface forms such as Japanese) have a greater counting proficiency in several tasks compared to those learning less transparent (opaque) systems, such as English or French. Although it is not the focus of our work, given the multilingual setting, we will refer to the idea of number system transparency when analyzing possible explanations of results. 3</p>
<p>Related Work</p>
<p>Our approach is informed by previous linguistically-motivated probing studies such 1 For our purposes, numbers are spelled out, i.e. written out as words such as "ninety".</p>
<p>2 Models were chosen for their varied sizes (num. parameters) as well as our access to computing resources. 3 Number system complexity could be the subject of its own paper. However, as a small example, we can look at "thirty" in English and " 三 三 三 十 十 十" in Japanese. In English, there is no previous number such as "three" that appears (unchanged) in the word "thirty". In Japanese, however, the word consists of the kanji for 3 (" 三 三 三") and the kanji for 10 (" 十 十 十"). If we continue comparing in this way, we would see compositionality more clearly and regularly in Japanese's surface forms, thus forming our intuitions. at those discussed in Belinkov and Glass (2019) and Ettinger (2020). Though Ettinger (2020) discusses important findings on psycholinguistic probing experiments of BERT, we find Ettinger et al. (2016) particularly useful for our study due to its clear explanations of linguistic probing experiment setup. In their study, Ettinger et al. present methods for constructing linguisticallytargeted datasets (including example sentences, as we use) and classification tasks for probing word embeddings for semantic knowledge. As one of our tasks also seeks to probe for semantic knowledge, we were able to use this setup as a rough guideline. In addition, the authors are careful to create linguistic data with sufficient diversity as not to give potentially helpful cues to their classifier which are not related to the knowledge they wish to probe for. We thus carefully create our task data in a similar manner by limiting the distribution of our data (described more later) and forbidding duplicates.</p>
<p>To our knowledge, there have been few studies conducted on investigating numerical understanding specifically in transformer-based language models with a multilingual approach. However, one particularly relevant study on English comes from Wallace et al. (2019). Wallace et al. probe the embeddings of various models (BERT, ELMo, word2vec, etc.) using three tasks: find the maximum of a list, decode a number word to its numerical form, and add number words to produce a numerical form. In their results, the authors note that all embeddings contain some understanding, though standard embeddings perform particularly well and character-level models perform best. Although we investigate similar phenomena as Wallace et al., our methodology includes several key differences:</p>
<p>• Our focus is first and foremost to present a novel probing task schema/data and test it on a selection of transformer-based models -not to compare performance of differing language model architectures on previouslymade tasks.</p>
<p>• We seek to draw conclusions from our task performance about model weaknesses over varied languages and suggest ways in which further probing experiments in this area can be designed in the future.</p>
<p>• We assert that the inclusion of other lan-guages besides English is an important addition to probing experiments, as variation in language structure may help point to previously-unseen weaknesses in pretrained models.</p>
<p>• We include spelled-out numbers above 100 (up to 1000), which were not used in Wallace et al. We believe having a larger range of numbers might highlight weaknesses of models in handling multiple identical tokens in one word.</p>
<p>• We use only spelled-out number words, and do not include tasks where both Arabic numerals and spelled-out words might be used. Our reasoning for this choice is our desire to leave out the possibility of models merely learning a mapping from number words to numerals in order to perform well on tasks.</p>
<p>In this way, we hope to make our tasks/data as restrictive as possible in order that they require a certain compositional/linguistic understanding.</p>
<p>Methods</p>
<p>We propose and perform two classification tasks in English, Danish, Japanese, and French. Task 1 is a probe for underlying syntactic information encoded in pretrained word embeddings, while task 2 is a probe of underlying semantic information. We run our tasks on all three models over two different datasets which we have generated: one where number words are inserted into sentences (e.g. "There are seven hundred books in the library.") and one with numbers alone (e.g. "seven-hundred"). Our probing model features a multilayer perceptron (a NN with a single hidden layer) classifier on top of the existing transformer language model architecture. In this manner, pretrained word embeddings from the language model (BERT, DistilBERT, XLM) are fed as input to the MLP classifier which itself is then trained on our tasks. A depiction of the probing model structure is shown in Figure 1.</p>
<p>Task 1: Grammaticality Judgment</p>
<p>We specify the first task as follows:</p>
<p>• Let v ∈ {bare, sentence} specify the variant of our task. If v = bare, then only training examples with numbers not inserted into sentences will be used for grammaticality judgments. Otherwise, only training examples with number inserted into sentences are used. A mixture of two input data types is never used.</p>
<p>• Let the training set of task 1, T , be defined by
pairs t 0 ...t n where t i = (x t i , y t i )
• Let x t i be the input of the i'th training example t i such that x t i = s. s is a string consisting of a number word such as "thirty-two" or a sentence containing a number word such as "He could eat thirty-two oranges" (depending on the value of v)
• Let y t i ∈ {0, 1} be the corresponding label of input x t i of training sample t i . y t i = 1 if the input string s of x t i is ungrammatical, otherwise y t i = 0.
We argue high accuracy on this task is evidence of some understanding of the underlying compositional/syntactic rules for the process which generates natural language numbers.</p>
<p>Figure 1: Probing model; Inputs can vary from sentences with numbers to standalone numbers. All weights are fixed except the connections to the MLP layer As an example, "two hundred three and fifty" is not a number in the English language, while "five hundred" is. We also assert that the importance of using spelled-out numbers comes into play since no string of Arabic numerals is ungrammatical except a small amount of strings such as "0112". Often in written text, Arabic numerals are used in place of number words; However, given human ability to generalize compositional rules from other structures in order to learn new structures, even if it were the case that fewer number words were seen in pretraining, we would hope to see a similar compositional generalization capability present in pretrained language models' embeddings which would allow them to perform well on this task.</p>
<p>Task 2: Value Comparison</p>
<p>We specify the second task as follows:</p>
<p>• Let v ∈ {bare, sentence} specify the variant of our task. If v = bare, then only training examples with numbers not inserted into sentences will be used for value comparsion. Otherwise, only training examples with number inserted into sentences are used. A mixture of two input data types is never used.</p>
<p>• Let the training set of task 2, U , be defined by
pairs u 0 ...u n where u i = (x u i , y u i )
• Let x u i be the input of the i'th training example u i such that x u i = (s 0 , s 1 ). s 0 and s 1 represent bare number words or number words inserted into sentences (depending on the value of v).</p>
<p>• Let y u i ∈ {0, 1} be the corresponding label of input x u i of training sample u i . y u i = 0 if for s 0 and s 1 of x u i , s 0 refers to a value larger than that of s 1 . y u i = 1 if for s 0 and s 1 of x u i , s 0 refers to a value smaller than that of s 1 .</p>
<p>With this task, we take high accuracy as evidence of some understanding of the compositional semantic information carried by the number, i.e. its magnitude. For example, given the pair (s 0 ="twelve", s 1 ="fifteen") the correct output should be 1, since the first number in the pair is less than the second. This task is similar in form to the list maximum task of Wallace et al. (2019); However, notable differences include our usage of number words in sentences, our inclusion of number words above 100, and languages other than English.</p>
<p>Task (v)</p>
<p>Input Output Task 1 (sentence) "He could eat three hundred and two oranges" 0 (s grammatical) Task 1 (bare) "seventy-four six hundred and thirty-eight" 1 (s ungrammatical) Task 2 (sentence) "There are seven hundred and eighty-six books in the library" 0 (s 0 greater) "There are thirty-eight books in the library" Task 2 (bare) "five hundred" 1 (s 0 less) "six hundred" </p>
<p>Data</p>
<p>Separate datasets for each variant of each task were made, each with a 60-20-20 train-validationtest split. To generate number words to create training example inputs, the python package num2words (Ogawa) was used for text conversion from numerical to standard spelled-out numbers. For task 1, it was necessary to create ungrammatical numbers in each language. These ungrammatical number words were created by randomly appending grammatical number words (or parts of them) together and controlling for length. Lastly, it was also necessary to create custom sentences to insert our number words into. Eleven sentence templates were used, translated into each of our languages and verified by native speakers. A sample of our data can be seen in Table 1. Detailed information on dataset statistics and data generation techniques can be found in appendix B.</p>
<p>Results</p>
<p>The following sections show probing results on both tasks. Before probing and as a precaution, we fine-tuned our models on both tasks. As we expected, we find that both tasks are learnable to accuracies above 95%. A further discussion of finetuning results is left for appendix A.</p>
<p>Task 1 Results</p>
<p>Our results on task 1 (Figure 2) show that the pretrained embeddings of multilingual BERT, Dis-tilBERT and XLM seem to have sufficient information to be able to determine grammaticality of number words in Japanese, English, Danish, and French at better-than-chance performance. We thus argue that these results suggest that the pretrained embeddings of these models contain some understanding of the compositional rules for generating number words in the languages we've selected.</p>
<p>There are a few patterns worth noting in the results. Firstly, accuracy on bare numbers was always better than accuracy on numbers in sentences. Though accuracy on numbers in sentences was not extremely poor, our initial prediction was that they would perform better as they would resemble the type of data which the models were pretrained on (Wikipedia). Secondly, in terms of overall model performance, DistilBERT performs best (sometimes even at 100% accuracy) in the majority of cases, followed by XLM and BERT. A further analysis of these patterns is left for our discussion (6).</p>
<p>Task 2 Results</p>
<p>On our second task, we find that overall, the information in the pretrained embeddings of BERT, DistilBERT, and XLM is mostly insufficient for comparing number magnitudes in our tested languages with high accuracy. This suggests that these pretrained embeddings may struggle with understanding the compositional semantics of number words (i.e. how compositional elements in number words form to create meaning).</p>
<p>As for patterns in these results, we can again see that bare number performance is always equal to or better than when numbers are inserted into sentences. Looking at bare results, DistilBERT again performs well, but this time XLM performs best on Japanese and English. A further analysis of these patterns is also left for our discussion (6). </p>
<p>Discussion</p>
<p>Pretraining Data</p>
<p>Since this probing experiment is using pretrained transformer-based language models (Wolf et al., 2019), we can briefly discuss pretraining methods in order to ascertain potential effects on results. Each model was pretrained using masked language modelling and all except XLM (unspecified) were pretrained on multilingual Wikipedia data. With this, one may ask whether this method of training should be expected to encode the information we probe for. Particularly, one may point to the usage of Arabic numerals in Wikipedia text and less-frequent usage of spelled-out numbers as a cause for poor performance on task 2. In imagining alternatives to Wikipedia, a consideration of Arabic numerals must still be present, which serves as a reminder that there is no perfect pretraining set.</p>
<p>However, we do not believe this would prevent language models from capturing the information necessary to complete our task 2. We assert that even if these models have seen fewer spelled-out numbers, they could still learn compositional rules from other linguistic structures and generalize to our probing task. If Arabic numerals proved to be an issue, we would expect to see our results be worse across the board, not only on task 2. Thus, from our results, it seems that pretraining on Wikipedia was certainly not sufficient for encoding a highly accurate sense of number magnitude for any of the models/languages, but this was likely not due to pretraining methods.</p>
<p>Worse Performance on Task 2</p>
<p>Why might models have more difficulty in ascertaining magnitude of number words? For one, we believe this task is naturally more difficult than the first because of the deep semantic information necessary to succeed on it. In the first task, it may have been possible to leverage at least some of the surface level characteristics of grammatical and ungrammatical words, whereas in the second task there is no such leveraging possible. That is to say, in the first task, a model can learn syntactic information more directly from surface level patterns. Instead, in task 2, the models need to have encoded some semantic information about the magnitude of number words, where the surface forms of these words gives less indication of their underlying meaning (except for the possibility of longer words having larger quantities, though this is not always the case). Given this is true, this may point to a weakness in models to make finegrained semantic distinctions regarding quantities, especially when quantities are used in sentences and not left bare.</p>
<p>Language Transparency</p>
<p>In terms of number system transparency (as mentioned in our introduction), we loosely presumed that accuracies might follow the order of Japanese &gt; English/Danish &gt; French, with Japanese performing best given its higher transparency and French the worst due to its vigesimal number system. Again, we choose not to formally define transparency, as such a formal definition is an indepth topic of its own. To our slight surprise, our results did not match these predictions, with rankings of performance by language varying by task and task variation.</p>
<p>We argue there could be many reasons (such as pretraining data) for the unexpected results pattern. However, since there is more of a consistent performance pattern across models than there is across languages, we believe it is far more likely that differences in performance are not necessarily due to language transparency, but rather model architecture and therefore that the structural differences between these languages is not a significant contributing factor to performance patterns. If this were true (which we think is probable), this is a good sign for these models, since language structure differences are not proving to be a challenge to performance, but rather some other factor.</p>
<p>Model Architecture and Performance</p>
<p>One clear pattern we can see is that multilingual BERT's embeddings consistently perform much worse than both XLM and DistilBERT. This is especially clear in French results, where the gap between BERT and the other two models is sometimes more than 20 points. A relatively simple explanation for this is the size of each model in terms of number of parameters. Indeed, as model size increases, performance on both tasks increases (BERT -&gt; DistilBERT -&gt; XLM). Of course, correlation is no evidence of causation; However, if this were true, it is quite consistent with other trends in recent NLP studies. In the case of this study, we can say that bigger is (almost) always better, with XLM mostly performing best, followed by DistilBERt and BERT. Though, this is somewhat undesirable on a larger scale since, ideally, we would hope that it would not require such a large model to encode the information we probe for.</p>
<p>Bare Number Words Perform Better</p>
<p>On both tasks, our results also show that bare numbers performed equal to or better than numbers in sentences. We propose that this is due to the sentences creating noisiness, thus creating more difficulty for a model to know exactly where it should be looking for the necessary information to complete the tasks. This is very much the case for task 2, where we believe it would be harder to know the magnitude a sentence is referring to than merely if the sentence is grammatical. We argue that, besides adding noise, this method of probing exploits a possible weakness in masked language modeling as a pretraining method. That is, given that masked language modeling's task it to predict appropriate (grammatical) words, there may be less emphasis on learning the underlying semantics of those words, thus the better performance on task 1 sentences and worse performance on task 2 sentences.</p>
<p>Further Work</p>
<p>As this work is an exploration of a new probing method for state-of-the-art language model architecture, there are surely a number of ways to extend from it.</p>
<p>Though we discussed it briefly here, exploring the architectural reasons for the shortcomings of these pretrained embeddings, especially in the case of task 2 and with sentences is an important area for future work. Indeed, in a similar task from Wallace et al. (2019), BERT was also found to have poor performance. In the future, several more specific probes could be designed to test for understanding of magnitude in various linguistic contexts to find strengths and weaknesses of transformer-based models. A particularly interesting case would be in testing magnitude comprehension in sentences of varying structures. Our sentence templates used in this study are few, and experimenting with other varieties could prove to be insightful.</p>
<p>Our experiment also made use of the idea of language transparency. We also find this to be a topic for possible further work. Namely, is there a method to reliably measure transparency of languages to predict performance on numerical understanding tasks such as these? We believe this may be possible through measuring complexities of grammars which generate number words in each language. Overall, in future extensions of this study, there is room for more languages, sentence types, task renditions, and models.</p>
<p>Conclusion</p>
<p>In this paper, we introduced methods for probing the multilingual compositional reasoning capabilities of transformer-based models' pretrained embeddings over natural language numbers. From our experiments, we've shown that these pretrained embeddings show some capabilities in making grammatical judgments of number words, though they are less capable of making value com-parisons. In addition, we find that results generally follow a trend based upon model size. Our results are in accord with previous work such as Wallace et al. (2019); However, we have also highlighted further model weaknesses through our probing methods. Therefore, the opportunities for future work, especially with a multilingual focus, are plenty.</p>
<p>A Fine-Tuning</p>
<p>The fine-tuning results shown in (Figure 3) are validation accuracies after one epoch of training. When trained for 20 epochs (Figure 4), the models reach over 99% accuracy on all languages and tasks except for Danish which reaches 95% on sentence tasks.   The parameters below were used to generate data for each language per each model per each task variation:</p>
<p>• Data Gen. Seed: 1</p>
<p>• Data Gen. Number Range: [0-999]</p>
<p>• Train Set Size: 30,000</p>
<p>• Validation Set Size: 10,000</p>
<p>• Test Set Size: 10,000</p>
<p>• Shuffle = True</p>
<p>B.2 Task 1 Data</p>
<p>For both variants of task 1 (sentences/bare), grammatical data are generated by creating random numbers then converting them to text through the num2words (Ogawa) package. For the ungrammatical data, two grammatical numbers are randomly generated, both converted to text, then appended together to create an ungrammatical number. For example the ungrammatical number "fifty-five two hundred" is the combination of "fifty-five" and "two hundred". Another example made from the same original elements could be: "two fifty-five hundred". Grammatical numbers used in splits, however, were only split such that the resulting elements were grammatical words themselves. So, for example, an non-continuous number word string like "fi-tfy te nnine" would never occur.</p>
<p>Since generating numbers with appendage can naturally occur in ungrammatical number words being longer than grammatical, we control for length by limiting our set of ungrammatical number words to words that are at most as long as the longest grammatical number. Lastly, we ensure no grammatical numbers are accidentally created in this process by keeping a list of known grammatical numbers in text form (generated by num2words) that ranges from number sufficiently higher than our generation range. For example, if our number word generation ranges from 0-1000, we would make this list of known grammatical numbers from 1-100,000,000. These number words are finally either left bare or inserted into sentences to form our x inputs and are labeled 0 if grammatical and 1 if ungrammatical.</p>
<p>B.3 Task 2 Data</p>
<p>The data for the semantic task are generated by creating pairs of random (grammatical) number words and labeling the pair with one of two categories: 0 if the first numbers is larger than the second and 1 if the second is larger than the first. Through our process of data generation, we ensure that there are never two pairs using the same number. They are then converted to text form for input to a model. These number words are finally either left bare or inserted into sentences to form our x inputs. When numbers are used in sentence templates, it is ensured that the numbers are used in the same template. For example, given the number pair "five" and "six", we could compare the sentences: "There are five apples." and "There are six apples.".</p>
<p>C Modeling</p>
<p>C.1 Pytorch Hugging Face Transformers</p>
<p>We use the configurations below of transformers from Hugging Face Transformers (Wolf et al., 2019) in Pytorch on all of our reported experimental runs. Average runtimes were all around 1 hour or less. </p>
<p>C.2 Hyperparameters</p>
<p>All experiments which produced our final results shown in the paper were run with the following hyperparemeters which were selected manually by tuning for accuracy over a validation set: </p>
<p>C.4 Code Repository</p>
<p>Our Github repository can be found here. Code is subject to change after publishing of this paper. Refer to the Github README for latest information.</p>
<p>Figure 2 :
2Task test accuracy on probing per language/model.</p>
<p>Figure 3 :
3Fine-tuned task validation accuracy per language/model, when trained for 1 epoch.</p>
<p>Figure 4 :
4Fine-tuned task validation accuracy per language/model, when trained for 20 epochs.</p>
<p>-
Class: DistilBertForSequenceClassification -Config: distilbert-base-multilingual-cased -Tokenizer: DistilBertTokenizer -Num. Parameters: 134 million total • BERT -Class: BertForSequenceClassification -Config: base-multilingual-cased -Tokenizer: BertTokenizer -Num. Parameters: 110 million total • XLM -Class: XLMForSequenceClassification -Config: xlm-mlm-100-1280 -Tokenizer: XLMTokenizer -Num. Paremeters: ∼550 million total (inexact)</p>
<p>•
Epochs</p>
<p>Table 1 :
1Sample of generated English data for our tasks
AcknowledgementsThank you to all anonymous reviewers for your helpful comments. Thank you to Professor Shane Steinert-Threlkeld for your guidance and throughout all stages of this paper. Thank you to Professor Luke Zettlemoyer for your advice on our paper in its earlier stages. Thank you to the several volunteer native speakers for grammaticality judgments.-This work was facilitated through the use of advanced computational, storage, and networking infrastructure provided by the Hyak supercomputer system and funded by the STF at the University of Washington.-
Analysis methods in neural language processing: A survey. Yonatan Belinkov, James R Glass, Transactions of the Association for Computational Linguistics. 7Yonatan Belinkov and James R. Glass. 2019. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. 2018. Bert: Pre-training of deep bidirectional trans- formers for language understanding.</p>
<p>What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Allyson Ettinger, 10.1162/tacl_a_00298Transactions of the Association for Computational Linguistics. 8Allyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for lan- guage models. Transactions of the Association for Computational Linguistics, 8:34-48.</p>
<p>Probing for semantic evidence of composition by means of simple classification tasks. Allyson Ettinger, Ahmed Elgohary, Philip Resnik, 10.18653/v1/W16-2524Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. the 1st Workshop on Evaluating Vector-Space Representations for NLPBerlin, GermanyAssociation for Computational LinguisticsAllyson Ettinger, Ahmed Elgohary, and Philip Resnik. 2016. Probing for semantic evidence of composition by means of simple classification tasks. In Proceed- ings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 134-139, Berlin, Germany. Association for Computational Linguis- tics.</p>
<p>Crosslingual language model pretraining. Guillaume Lample, Alexis Conneau, abs/1901.07291CoRRGuillaume Lample and Alexis Conneau. 2019. Cross- lingual language model pretraining. CoRR, abs/1901.07291.</p>
<p>Preschool origins of cross-national differences in mathematical competence: The role of number-naming systems. K F Miller, C M Smith, J Zhu, H Zhang, 10.1111/j.1467-9280.1995.tb00305.xPsychological Science. 6K. F. Miller, C. M. Smith, J. Zhu, and H. Zhang. 1995. Preschool origins of cross-national differ- ences in mathematical competence: The role of number-naming systems. Psychological Science, 6:56-60.</p>
<p>. T Ogawa, 2T. Ogawa. num2words.</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.</p>
<p>Do nlp models know numbers? probing numeracy in embeddings. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, 10.18653/v1/d19-1534Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLPEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know num- bers? probing numeracy in embeddings. Proceed- ings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th In- ternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP).</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R&apos;emi Louf, Morgan Funtowicz, Jamie Brew, abs/1910.03771ArXiv. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R'emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface's trans- formers: State-of-the-art natural language process- ing. ArXiv, abs/1910.03771.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1789 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1789</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1789</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-f23a0e443fe931aa2fed932421bf47c1a4fcf619</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f23a0e443fe931aa2fed932421bf47c1a4fcf619" target="_blank">CodeBLEU: a Method for Automatic Evaluation of Code Synthesis</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a new automatic evaluation metric, dubbed CodeBLEU, which absorbs the strength of BLEU in the n-gram match and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow and can achieve a better correlation with programmer assigned scores compared with BLEu and accuracy.</p>
                <p><strong>Paper Abstract:</strong> Evaluation metrics play a vital role in the growth of an area as it defines the standard of distinguishing between good and bad models. In the area of code synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but they are not suitable enough to evaluate codes, because BLEU is originally designed to evaluate the natural language, neglecting important syntactic and semantic features of codes, and perfect accuracy is too strict thus it underestimates different outputs with the same semantic logic. To remedy this, we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the strength of BLEU in the n-gram match and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow. We conduct experiments by evaluating the correlation coefficient between CodeBLEU and quality scores assigned by the programmers on three code synthesis tasks, i.e., text-to-code, code translation, and code refinement. Experimental results show that our proposed CodeBLEU can achieve a better correlation with programmer assigned scores compared with BLEU and accuracy.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1789.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1789.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeBLEU (weighted syntactic and semantic BLEU for code synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation metric for code synthesis that combines standard BLEU, a weighted n-gram match (keywords weighted), an AST subtree match (syntactic), and a data-flow match (semantic) into a weighted sum to better reflect programmers' judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metrics</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated code outputs from code synthesis systems)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Java (text-to-code, code refinement) and Java↔C# (code translation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Overall code quality (1-5 Likert by human judges), implicitly capturing correctness, syntactic/semantic correctness and general quality/readability</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>10 human judges familiar with Java and C# rated 4 system outputs per input on a 1 (very bad) to 5 (very good) Likert scale; judges used a UI showing 50 randomly sampled inputs each paired with 4 outputs (200 input-output pairs total), order randomized across outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Familiar with Java and C# (programmers); described as human judges familiar with the languages</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation coefficient between metric scores and mean human Likert ratings; also linear regression R^2 reported, and statistical reliability via mean/stddev on 20 blocks and paired t-statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Pearson r(CodeBLEU vs human) = 0.977 (text-to-code), 0.970 (code translation), 0.979 (code refinement); R^2 reported in regressions (figure) and block statistics: mean/stddev and paired t-statistics used to assess significance of metric differences.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Tasks where syntactic and semantic matches matter (text-to-code and code translation) — higher weights on AST and data-flow components improved alignment; tuning weights (e.g., α,β,γ,δ = 0.1,0.1,0.4,0.4) increased Pearson correlation; when outputs are semantically similar but differ in variable naming, AST/DF components keep high agreement with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Surface-token-only matching (standard BLEU) underestimates quality when variable names change or semantics preserved (example: variable renaming); perfect accuracy (exact match) underestimates equivalently correct but syntactically different outputs; BLEU gives high scores to token-overlap candidates with logical errors.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Agreement depends on task/type: for code refinement (outputs often unique/few valid fixes), exact-match (accuracy) correlates especially well with human scores; for tasks involving more syntactic/semantic variability (text-to-code, translation), syntactic/semantic components (AST, data-flow) improve agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Tuning the metric composition (clarity of which components are emphasized) changes alignment: increasing weights of syntactic (AST) and semantic (data-flow) components reliably increased Pearson correlation with human judgments across tasks; thus more explicit semantic/syntactic criteria improved alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Automatic metrics computed on 500 randomly chosen test samples per task; human evaluation performed on a subset of 50 inputs (200 input-output pairs since 4 system outputs per input). Metrics were also computed on 20 blocks of 25 sentences to estimate variance.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>CodeBLEU showed higher Pearson correlation with human Likert scores than BLEU across all three tasks and higher than exact-match accuracy for text-to-code and code translation; for code refinement, accuracy correlated highest (Acc r=0.999), while CodeBLEU r=0.979, BLEU r=0.923. Direct inter-human vs proxy comparison not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>CodeBLEU hyper-parameters (α,β,γ,δ) were systematically varied and evaluated against human scores to find combinations that maximize correlation (e.g., recommendation α,β,γ,δ = 0.1,0.1,0.4,0.4); no training of metric on human labels beyond this tuning is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) CodeBLEU yields higher correlation with programmer-assigned Likert scores than standard BLEU across text-to-code, code translation, and code refinement tasks (reported Pearson r improvements: e.g., text-to-code BLEU→0.967 to CodeBLEU→0.977). 2) AST and data-flow components individually show very high correlations with human ratings (Match_ast and Match_df often ≥0.97 for tasks), indicating syntactic and semantic matching are strong predictors of human judgments. 3) Standard BLEU underestimates correct outputs when variable renaming occurs and can miss semantic errors; perfect accuracy is too strict but correlates strongly when outputs are unique (code refinement). 4) Metric differences are statistically reliable — block-based variance and paired t-tests indicate significance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human evaluation subset is relatively small (50 inputs, 10 judges); inter-rater agreement among humans is not reported; human judges described as 'familiar with' languages not necessarily highly experienced experts; CodeBLEU relies on language-specific parsers/AST and predefined keyword lists and its hyper-parameters may need retuning per domain; computational accuracy (another automated approach) is noted as lacking universality/practicality; some errors (e.g., semantic equivalence beyond data-flow) may still be missed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeBLEU: a Method for Automatic Evaluation of Code Synthesis', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1789.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1789.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU (Bilingual Evaluation Understudy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-level n-gram overlap metric originally developed for machine translation that measures precision of n-grams with a brevity penalty; commonly used previously for code synthesis evaluation but known to ignore syntactic and semantic correctness in code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metrics (n-gram overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated code outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Java and Java→C# code synthesis tasks in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Token-level n-gram overlap between candidate and reference (surface similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same human Likert-style setup as used for CodeBLEU: 10 judges, 50 sampled inputs, 4 outputs per input, rated 1-5.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Familiar with Java and C#</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation coefficient between BLEU and mean human Likert ratings; regression R^2 visualized; variance and significance via block split and paired t-tests for metric reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Pearson r(BLEU vs human) = 0.967 (text-to-code), 0.940 (code translation), 0.923 (code refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>BLEU can align well with human scores when surface token overlap reflects semantic/syntactic correctness, and when tasks have less variability in naming/structure.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>BLEU underperforms when variable renaming or other semantically-neutral surface changes occur (Example 2) and when semantic/logical correctness diverges from token overlap (gives high BLEU even with logic errors).</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>BLEU correlation decreases when semantics matter more than token overlap (semantic substitutions, logical errors); in highly-unique-output tasks (refinement) BLEU correlation is lower than accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>As a surface metric, BLEU lacks explicit syntactic/semantic criteria; adding explicit components (AST, data-flow) in CodeBLEU improved alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Computed on 500-sample subsets per task; human comparisons on 50-sample subset.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>BLEU correlates strongly with human judgments but less than CodeBLEU in the studied tasks; specific numeric comparisons provided (see CodeBLEU entry).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No calibration of BLEU against human judgments in this work; BLEU used as standard baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BLEU remains a strong predictor of human judgments but has systematic failure modes on code (variable renaming, syntactic/semantic errors) which CodeBLEU addresses by adding syntactic and semantic matching.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>BLEU was designed for natural language and ignores code-specific features (keywords importance, AST tree structure, data-flow semantics), causing under- or over-estimation in many code cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeBLEU: a Method for Automatic Evaluation of Code Synthesis', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1789.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1789.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy (Exact Match)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perfect accuracy (exact-match percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation that computes the percentage of predicted programs that are exactly identical to the reference program; used as a strict automated metric for code synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metrics (exact-match / perfect accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated code outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Java and Java↔C# code synthesis tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact string/token match between candidate and reference (100% identical required to count as correct)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same Likert human rating setup (10 judges, 50 sampled inputs, 4 outputs per input) used for correlation analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Familiar with Java and C#</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation coefficient between exact-match accuracy and human Likert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Pearson r(Acc vs human) = 0.912 (text-to-code), 0.968 (code translation), 0.999 (code refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When the correct output is usually unique (e.g., small bug fixes / code refinement), exact-match accuracy aligns very strongly with human judgments (code refinement: r=0.999).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>When multiple semantically-equivalent implementations exist or variable naming differs, exact-match underestimates human-perceived correctness (low agreement in more generative tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Exact-match performs well (high agreement) for tasks with unique outcomes and low implementation variability (code refinement) but poorly for tasks allowing many semantically correct outputs (text-to-code, translation).</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Exact-match enforces a very strict criterion (identity), which increases agreement only when humans also expect identical outputs; otherwise its strictness reduces alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Computed on 500-sample subsets per task; human comparisons on 50-sample subset.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Exact-match outperforms BLEU and in one case outperforms CodeBLEU on code refinement because human judges reward the unique correct fix; in other tasks CodeBLEU exceeds accuracy in correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exact-match is overly strict in general but correlates extremely well with human judgments when the task has a unique ground-truth (code refinement); it underestimates quality in more open-ended generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Too strict: penalizes semantically equivalent but syntactically different solutions; not robust to variable renaming or equivalent refactorings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeBLEU: a Method for Automatic Evaluation of Code Synthesis', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised Translation of Programming Languages <em>(Rating: 2)</em></li>
                <li>Mapping Language to Code in Programmatic Context <em>(Rating: 1)</em></li>
                <li>An empirical study on learning bug-fixing patches in the wild via neural machine translation <em>(Rating: 2)</em></li>
                <li>BLEU: a method for automatic evaluation of machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1789",
    "paper_id": "paper-f23a0e443fe931aa2fed932421bf47c1a4fcf619",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "CodeBLEU",
            "name_full": "CodeBLEU (weighted syntactic and semantic BLEU for code synthesis)",
            "brief_description": "An automatic evaluation metric for code synthesis that combines standard BLEU, a weighted n-gram match (keywords weighted), an AST subtree match (syntactic), and a data-flow match (semantic) into a weighted sum to better reflect programmers' judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated metrics",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code (generated code outputs from code synthesis systems)",
            "artifact_domain": "Java (text-to-code, code refinement) and Java↔C# (code translation)",
            "evaluation_criteria": "Overall code quality (1-5 Likert by human judges), implicitly capturing correctness, syntactic/semantic correctness and general quality/readability",
            "human_evaluation_setup": "10 human judges familiar with Java and C# rated 4 system outputs per input on a 1 (very bad) to 5 (very good) Likert scale; judges used a UI showing 50 randomly sampled inputs each paired with 4 outputs (200 input-output pairs total), order randomized across outputs.",
            "human_expert_count": "10",
            "human_expert_expertise": "Familiar with Java and C# (programmers); described as human judges familiar with the languages",
            "agreement_metric": "Pearson correlation coefficient between metric scores and mean human Likert ratings; also linear regression R^2 reported, and statistical reliability via mean/stddev on 20 blocks and paired t-statistics.",
            "agreement_score": "Pearson r(CodeBLEU vs human) = 0.977 (text-to-code), 0.970 (code translation), 0.979 (code refinement); R^2 reported in regressions (figure) and block statistics: mean/stddev and paired t-statistics used to assess significance of metric differences.",
            "high_agreement_conditions": "Tasks where syntactic and semantic matches matter (text-to-code and code translation) — higher weights on AST and data-flow components improved alignment; tuning weights (e.g., α,β,γ,δ = 0.1,0.1,0.4,0.4) increased Pearson correlation; when outputs are semantically similar but differ in variable naming, AST/DF components keep high agreement with humans.",
            "low_agreement_conditions": "Surface-token-only matching (standard BLEU) underestimates quality when variable names change or semantics preserved (example: variable renaming); perfect accuracy (exact match) underestimates equivalently correct but syntactically different outputs; BLEU gives high scores to token-overlap candidates with logical errors.",
            "artifact_complexity_effect": "Agreement depends on task/type: for code refinement (outputs often unique/few valid fixes), exact-match (accuracy) correlates especially well with human scores; for tasks involving more syntactic/semantic variability (text-to-code, translation), syntactic/semantic components (AST, data-flow) improve agreement.",
            "criteria_clarity_effect": "Tuning the metric composition (clarity of which components are emphasized) changes alignment: increasing weights of syntactic (AST) and semantic (data-flow) components reliably increased Pearson correlation with human judgments across tasks; thus more explicit semantic/syntactic criteria improved alignment.",
            "sample_size": "Automatic metrics computed on 500 randomly chosen test samples per task; human evaluation performed on a subset of 50 inputs (200 input-output pairs since 4 system outputs per input). Metrics were also computed on 20 blocks of 25 sentences to estimate variance.",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "CodeBLEU showed higher Pearson correlation with human Likert scores than BLEU across all three tasks and higher than exact-match accuracy for text-to-code and code translation; for code refinement, accuracy correlated highest (Acc r=0.999), while CodeBLEU r=0.979, BLEU r=0.923. Direct inter-human vs proxy comparison not reported.",
            "calibration_or_training": "CodeBLEU hyper-parameters (α,β,γ,δ) were systematically varied and evaluated against human scores to find combinations that maximize correlation (e.g., recommendation α,β,γ,δ = 0.1,0.1,0.4,0.4); no training of metric on human labels beyond this tuning is reported.",
            "key_findings": "1) CodeBLEU yields higher correlation with programmer-assigned Likert scores than standard BLEU across text-to-code, code translation, and code refinement tasks (reported Pearson r improvements: e.g., text-to-code BLEU→0.967 to CodeBLEU→0.977). 2) AST and data-flow components individually show very high correlations with human ratings (Match_ast and Match_df often ≥0.97 for tasks), indicating syntactic and semantic matching are strong predictors of human judgments. 3) Standard BLEU underestimates correct outputs when variable renaming occurs and can miss semantic errors; perfect accuracy is too strict but correlates strongly when outputs are unique (code refinement). 4) Metric differences are statistically reliable — block-based variance and paired t-tests indicate significance.",
            "limitations_noted": "Human evaluation subset is relatively small (50 inputs, 10 judges); inter-rater agreement among humans is not reported; human judges described as 'familiar with' languages not necessarily highly experienced experts; CodeBLEU relies on language-specific parsers/AST and predefined keyword lists and its hyper-parameters may need retuning per domain; computational accuracy (another automated approach) is noted as lacking universality/practicality; some errors (e.g., semantic equivalence beyond data-flow) may still be missed.",
            "uuid": "e1789.0",
            "source_info": {
                "paper_title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "BLEU (Bilingual Evaluation Understudy)",
            "brief_description": "A token-level n-gram overlap metric originally developed for machine translation that measures precision of n-grams with a brevity penalty; commonly used previously for code synthesis evaluation but known to ignore syntactic and semantic correctness in code.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated metrics (n-gram overlap)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code (generated code outputs)",
            "artifact_domain": "Java and Java→C# code synthesis tasks in the paper",
            "evaluation_criteria": "Token-level n-gram overlap between candidate and reference (surface similarity)",
            "human_evaluation_setup": "Same human Likert-style setup as used for CodeBLEU: 10 judges, 50 sampled inputs, 4 outputs per input, rated 1-5.",
            "human_expert_count": "10",
            "human_expert_expertise": "Familiar with Java and C#",
            "agreement_metric": "Pearson correlation coefficient between BLEU and mean human Likert ratings; regression R^2 visualized; variance and significance via block split and paired t-tests for metric reliability.",
            "agreement_score": "Pearson r(BLEU vs human) = 0.967 (text-to-code), 0.940 (code translation), 0.923 (code refinement).",
            "high_agreement_conditions": "BLEU can align well with human scores when surface token overlap reflects semantic/syntactic correctness, and when tasks have less variability in naming/structure.",
            "low_agreement_conditions": "BLEU underperforms when variable renaming or other semantically-neutral surface changes occur (Example 2) and when semantic/logical correctness diverges from token overlap (gives high BLEU even with logic errors).",
            "artifact_complexity_effect": "BLEU correlation decreases when semantics matter more than token overlap (semantic substitutions, logical errors); in highly-unique-output tasks (refinement) BLEU correlation is lower than accuracy.",
            "criteria_clarity_effect": "As a surface metric, BLEU lacks explicit syntactic/semantic criteria; adding explicit components (AST, data-flow) in CodeBLEU improved alignment.",
            "sample_size": "Computed on 500-sample subsets per task; human comparisons on 50-sample subset.",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "BLEU correlates strongly with human judgments but less than CodeBLEU in the studied tasks; specific numeric comparisons provided (see CodeBLEU entry).",
            "calibration_or_training": "No calibration of BLEU against human judgments in this work; BLEU used as standard baseline.",
            "key_findings": "BLEU remains a strong predictor of human judgments but has systematic failure modes on code (variable renaming, syntactic/semantic errors) which CodeBLEU addresses by adding syntactic and semantic matching.",
            "limitations_noted": "BLEU was designed for natural language and ignores code-specific features (keywords importance, AST tree structure, data-flow semantics), causing under- or over-estimation in many code cases.",
            "uuid": "e1789.1",
            "source_info": {
                "paper_title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Accuracy (Exact Match)",
            "name_full": "Perfect accuracy (exact-match percentage)",
            "brief_description": "An evaluation that computes the percentage of predicted programs that are exactly identical to the reference program; used as a strict automated metric for code synthesis.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated metrics (exact-match / perfect accuracy)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code (generated code outputs)",
            "artifact_domain": "Java and Java↔C# code synthesis tasks",
            "evaluation_criteria": "Exact string/token match between candidate and reference (100% identical required to count as correct)",
            "human_evaluation_setup": "Same Likert human rating setup (10 judges, 50 sampled inputs, 4 outputs per input) used for correlation analysis.",
            "human_expert_count": "10",
            "human_expert_expertise": "Familiar with Java and C#",
            "agreement_metric": "Pearson correlation coefficient between exact-match accuracy and human Likert ratings.",
            "agreement_score": "Pearson r(Acc vs human) = 0.912 (text-to-code), 0.968 (code translation), 0.999 (code refinement).",
            "high_agreement_conditions": "When the correct output is usually unique (e.g., small bug fixes / code refinement), exact-match accuracy aligns very strongly with human judgments (code refinement: r=0.999).",
            "low_agreement_conditions": "When multiple semantically-equivalent implementations exist or variable naming differs, exact-match underestimates human-perceived correctness (low agreement in more generative tasks).",
            "artifact_complexity_effect": "Exact-match performs well (high agreement) for tasks with unique outcomes and low implementation variability (code refinement) but poorly for tasks allowing many semantically correct outputs (text-to-code, translation).",
            "criteria_clarity_effect": "Exact-match enforces a very strict criterion (identity), which increases agreement only when humans also expect identical outputs; otherwise its strictness reduces alignment.",
            "sample_size": "Computed on 500-sample subsets per task; human comparisons on 50-sample subset.",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Exact-match outperforms BLEU and in one case outperforms CodeBLEU on code refinement because human judges reward the unique correct fix; in other tasks CodeBLEU exceeds accuracy in correlation.",
            "calibration_or_training": null,
            "key_findings": "Exact-match is overly strict in general but correlates extremely well with human judgments when the task has a unique ground-truth (code refinement); it underestimates quality in more open-ended generation tasks.",
            "limitations_noted": "Too strict: penalizes semantically equivalent but syntactically different solutions; not robust to variable renaming or equivalent refactorings.",
            "uuid": "e1789.2",
            "source_info": {
                "paper_title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised Translation of Programming Languages",
            "rating": 2
        },
        {
            "paper_title": "Mapping Language to Code in Programmatic Context",
            "rating": 1
        },
        {
            "paper_title": "An empirical study on learning bug-fixing patches in the wild via neural machine translation",
            "rating": 2
        },
        {
            "paper_title": "BLEU: a method for automatic evaluation of machine translation",
            "rating": 1
        }
    ],
    "cost": 0.013427999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CodeBLEU: a Method for Automatic Evaluation of Code Synthesis</h1>
<p>Shuo Ren ${ }^{1}$, Daya Guo ${ }^{2}$, Shuai $\mathbf{L u}^{3}$, Long Zhou ${ }^{4}$, Shujie Liu ${ }^{4}$, Duyu Tang ${ }^{4}$, Neel Sundaresan ${ }^{4}$, Ming Zhou ${ }^{4}$, Ambrosio Blanco ${ }^{4}$, Shuai Ma ${ }^{1}$<br>${ }^{1}$ SKLSDE Lab, Beihang University; Beijing Advanced Innovation Center for Big Data and Brain Computing<br>${ }^{2}$ Sun Yat-sen University ${ }^{3}$ Peking University ${ }^{4}$ Microsoft<br>${ }^{1}{$ shuoren, mashuai $} @$ buaa.edu.cn ${ }^{2}$ guody5@mail2.sysu.edu.cn ${ }^{3}$ lushuai96@pku.edu.cn<br>${ }^{4}$ {Long.Zhou, shujliu, dutang, neels, mingzhou, ambrob}@microsoft.com</p>
<h4>Abstract</h4>
<p>Evaluation metrics play a vital role in the growth of an area as it defines the standard of distinguishing between good and bad models. In the area of code synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but they are not suitable enough to evaluate codes, because BLEU is originally designed to evaluate natural language, neglecting important syntactic and semantic features of codes, and perfect accuracy is too strict thus it underestimates different outputs with the same semantic logic. To remedy this, we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the strength of BLEU in the n-gram match, and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow. We conduct experiments by evaluating the correlation coefficient between CodeBLEU and quality scores assigned by the programmers on three code synthesis tasks, i.e., text-to-code, code translation, and code refinement. Experimental results show that, our proposed CodeBLEU can achieve a better correlation with programmer assigned scores compared with BLEU and accuracy.</p>
<h2>1 Introduction</h2>
<p>A suitable evaluation metric is important to push forward the research of an area, such as BLEU (Papineni et al. 2002) and ROUGE (Lin 2004) for machine translation and text summarization. Along with the rapid progress of code synthesis such as text-to-code synthesis, code translation and code change prediction (Karaivanov, Raychev, and Vechev 2014; Oda et al. 2015; Barone and Sennrich 2017; Chen, Liu, and Song 2018; Kanade et al. 2019; Husain et al. 2019; Feng et al. 2020; Dinella et al. 2020; Lachaux et al. 2020), different automatic evaluation methods for code synthesis are leveraged, including n-gram accuracy (Karaivanov, Raychev, and Vechev 2014), perfect accuracy (Chen, Liu, and Song 2018), and computational accuracy (Lachaux et al. 2020). The n-gram accuracy (e.g. 4-gram BLEU) is the most popular evaluation method for code synthesis (Karaivanov, Raychev, and Vechev 2014; Barone and Sennrich 2017), based on the token overlapping between the hypothesis and the reference. The perfect accuracy calculates the percentage of the predicted target programs that are exactly the same as the ground truth (Chen, Liu, and Song 2018). The</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>recently proposed computational accuracy (Lachaux et al. 2020), evaluates whether the hypothesis function generates the same outputs as the reference given the same inputs.</p>
<p>However, the above evaluation approaches still face many drawbacks. First, the n-gram accuracy does not take into account the grammatical and logical correctness, resulting in favoring candidates with high n-gram accuracy and serious logical errors. Second, the perfect accuracy is too strict, and underestimates different outputs with the same semantic logic. Third, the computational accuracy is weak in universality and practicability, since it should be designed for different programming languages, as well as specific compilers and the desired computing resource.</p>
<p>In order to deal with that, in this paper, we propose a new evaluation metric CodeBLEU, considering information from not only the shallow (n-gram) match, but also the syntactic match and the semantic match. More specifically, the n-gram match assigns different weights for different n-grams, the syntactic match considers the abstract syntax tree (AST) information in the evaluation score by matching the sub-trees, and the semantic match uses data-flow structure to measure the semantic similarity. CodeBLEU is a weighted combination of the original BLEU, the weighted n-gram match, the syntactic AST match, and the semantic data-flow match.</p>
<p>We conduct massive experiments to evaluate the effectiveness of CodeBLEU and the correlation coefficient between CodeBLEU scores and human evaluation scores in three code synthesis tasks including text-to-code synthesis, code translation, and code refinement. Experimental results demonstrate that CodeBLEU can significantly differentiate the systems' performance and achieve better correlation with the quality scores given by programmers than the popularly used BLEU. We hope that our proposed CodeBLEU can accelerate the R\&amp;D cycle of code synthesis tasks.</p>
<h2>2 Why not BLEU?</h2>
<p>In this section we will briefly introduce BLEU, and analyze its merits and demerits when applying it to code synthesis.</p>
<h3>2.1 BLEU for Machine Translation</h3>
<p>Machine translation, which uses computers to realize automatic translation between languages, is first proposed by Warren Weaver as early as 1949 (Weaver 1955). Since then, machine translation quality has not significantly improved</p>
<p>until the automatic evaluation metric (BLEU) is proposed in 2002 (Papineni et al. 2002). The appearance of BLEU makes it possible to automatically train and optimize the machine translation systems and speeds up the research process of machine translation.</p>
<p>BLEU measures how well a candidate translation matches a set of translation references by calculating the percentage of n-grams overlapped between them. Besides, the brevity penalty is introduced to punish the candidates with a very short length, so it is hard for the MT system to cheat the evaluation metric by finding a way to change the output that the BLEU score goes up, but the translation quality doesn’t.</p>
<h3>2.2 Code vs Natural Language</h3>
<p>Although the BLEU achieves great success in the evaluation of machine translation and greatly encourages the research in this area, BLEU is not suitable for the evaluation of code synthesis without considering the characteristics of the programming language. A natural language is any language that has evolved naturally in humans through use and repetition, but code is artificially designed to produce various kinds of output. There are three big differences between them.
(1) Limited keywords vs. millions of words. Different from natural languages with a huge vocabulary, code is designed by humans and uses a small number of keywords, i.e., the reserved words of programming languages. Intuitively, keywords are more important than other words and the keywords match should gain a higher score.
(2) Tree structure vs. sequential structure. Humans usually speak and write from left to right, and the current mainstream models usually process natural languages as a sequence (Zhou et al. 2019), such as end-to-end neural machine translation (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014; Vaswani et al. 2017). In contrast, code has a natural tree structure and needs to be compiled according to their abstract syntax tree (Rabinovich, Stern, and Klein 2017). Therefore, how to evaluate the syntactic structure of code becomes particularly important.
(3) Unique instructions vs. ambiguous semantic. Word sense disambiguation is a basic research problem in natural language processing, because natural languages usually have ambiguous and variable semantic. However, code design is required to be unique, standardized and systematic, with unique and fixed instructions. This feature makes it possible to evaluate the semantics of the code.</p>
<p>In summary, code is significantly different from natural languages, and BLEU is not suitable for code synthesis evaluation only considering the token match and ignoring the importance of keywords, syntactic accuracy, and semantic correctness. Therefore, we propose a new evaluation metric CodeBLEU, which will be introduced in the following.</p>
<h2>3 CodeBLEU</h2>
<p>In order to pay attention to the keywords, leverage the tree structure and consider the semantic logic information, we propose a new evaluation metric CodeBLEU defined as the weighted combination of four parts as shown in Figure 1:</p>
<p>$$
\begin{aligned}
\operatorname{CodeBLEU} &amp; =\alpha \cdot \mathrm{BLEU}+\beta \cdot \mathrm{BLEU}<em _ast="{ast" _text="\text">{\text {weight }} \
&amp; +\gamma \cdot \operatorname{Match}</em>
\end{aligned}
$$}}+\delta \cdot \operatorname{Match}_{\mathrm{df}</p>
<p>where BLEU is calculated by standard BLEU (Papineni et al. 2002), $\mathrm{BLEU}<em _ast="{ast" _text="\text">{\text {weight }}$ is the weighted n-gram match, obtained by comparing the hypothesis code and the reference code tokens with different weights (Sec. 3.1), $\mathrm{Match}</em>$ is the semantic dataflow match, considering the semantic similarity between the hypothesis and the reference (Sec. 3.3). The weighted ngram match and the syntactic AST match are used to measure grammatical correctness, and the semantic data-flow match is used to calculate logic correctness.}}$ is the syntactic AST match, exploring the syntactic information of code (Sec. 3.2), and $\mathrm{Match}_{\mathrm{df}</p>
<h3>3.1 Weighted N-Gram Match</h3>
<p>The original BLEU compares n-grams between the candidate and the reference, and calculates the ratio of matched n-grams. Compared with natural languages which a huge vocabulary and a free word order, programming languages are manually designed and have only a few keywords such as "int", "public" and so on. Applying the traditional BLEU directly to code synthesis will ignore the importance of the keywords. Hence, we introduce the weighted n-gram match to assign different weights for different n-grams, so that the keywords may have higher weights, as shown in Figure 1.</p>
<p>The weighted n-gram match precision is computed as:</p>
<p>$$
p_{n}=\frac{\sum_{C \in \text { Candidates }} \sum_{i=1}^{l} \mu_{n}^{i} \cdot \operatorname{Count}<em C_prime="C^{\prime">{\text {clip }}(C(i, i+n))}{\sum</em>
$$} \in \text { Candidates }} \sum_{i=1}^{l} \mu_{n}^{i} \cdot \operatorname{Count}\left(C^{\prime}(i, i+n)\right)</p>
<p>where $n$ means the length of the n-gram, $C(i, i+n)$ is the n-gram from the position $i$ to the position $i+n$, and $\operatorname{Count}<em n="n">{\text {clip }}(C(i, i+n))$ is the maximum number of n-grams co-occurring in a candidate code and a set of reference codes. $\mu</em>$ of the keywords is 5 times the weights of other tokens. Next, following the brevity penalty of original BLEU, we also compute the brevity penalty BP:}^{i}$ denotes the weights of different keywords or ngram. In this paper, $\mu_{n}^{i</p>
<p>$$
\mathrm{BP}= \begin{cases}1 &amp; \text { if } c&gt;r \ e^{1-r / c} &amp; \text { if } c \leq r\end{cases}
$$</p>
<p>where $c$ is the length of the candidate code and $r$ is the effective reference corpus length. The weighted n-gram match score is calculated as:</p>
<p>$$
\mathrm{BLEU}<em n="1">{\text {weight }}=\mathrm{BP} \cdot \exp \left(\sum</em>\right)
$$}^{N} w_{n} \log p_{n</p>
<p>In our paper, the keywords are only considered in the unigrams, so N and $w_{n}$ are equal to 1 . Note that a keywords list is predefined for each programming language.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>CodeBLEU := $\cdot$ N - Gram Match (BLEU) + $\boldsymbol{\beta}$ $\cdot$ Weighted N-Gram Match + $\boldsymbol{\gamma}$ $\cdot$ Syntactic AST Match + $\boldsymbol{\delta}$ $\cdot$ Semantic Data-flow Match</p>
<p>Figure 1: The proposed CodeBLEU, a weighted syntactic and semantic BLEU for code synthesis evaluation, consists of the original BLEU, the weighted n-gram match, the syntactic AST match, and the semantic data-flow match.</p>
<h3>3.2 Syntactic AST Match</h3>
<p>In addition to the sequence-level matching, we also consider the syntactic information in CodeBLEU by matching the tree structure. Different from natural language, programming language has natural tree structures, such as the abstract syntax tree (AST). AST is a tree representation of the abstract syntactic structure of programming languages. We can obtain all the sub-trees of the tree-sitter parsing result<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup>, then calculate the accuracy by comparing the candidate and reference sub-trees. In AST, each node denotes a construct occurring in the source code. The leaves of AST represent the names of the function and all the variables. However, we just want to use the syntactic structure of the codes, and the naming is not important, thus we leave out all the leave nodes in the original AST trees.</p>
<p>As shown in the middle part of Figure 1, we extract all the sub-trees of the candidate and the reference ASTs respectively. Then we calculate the syntactic AST match score as:</p>
<p>$$
\text{Match}<em _text_clip="\text{clip">{\text{ast}} = \text{Count}</em>}}\left(\text{T<em _text_ref="\text{ref">{\text{cand}}\right) / \text{Count}\left(\text{T}</em>
$$}}\right) \tag{4</p>
<p>where Count( $\text{T}<em _text_clip="\text{clip">{\text{ref}}$) is the total number of the reference sub-trees, and Count${}</em>\right)$ is the number of the candidate subtrees that are matched the reference. This score can evaluate code quality from a syntactic perspective, because grammatical errors such as token missing, data type errors can be captured by the difference between their ASTs.}}\left(\text{T}_{\text{cand}</p>
<h3>3.3 Semantic Data-flow Match</h3>
<p>In programming languages, the semantic of source code is highly relevant to the dependency relations among variables. Taking Figure 2 as an example, the function is to calculate the mean value of an array. Although the difference between the candidate and the reference is subtle (return $y \rightarrow$ return $x$), their semantics are completely different. However, the weighted n-gram match and the syntactic AST match still give a high score since the two pieces of codes have the same AST and their tokens are highly overlapped. Therefore, we also consider the semantic information in CodeBLEU. We use data-flow (Guo et al. 2020) to represent a source code as a graph, in which nodes represent variables and edges represent where the value of each variable comes from. Unlike AST, data-flows of the two codes are different in Figure 2 since their return values come from $x$ and $y$ respectively. Such a semantic graph can be used to measure the semantic match between the candidate and the reference.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: BLEU: 95.47; Match ${ }_{\text{ast}}$: 100.</p>
<p>Based on the above, there are three steps to compute the semantic data-flow match score.</p>
<p>Step 1: Obtain the data-flow graphs for the candidate and the reference. Based on AST, we first utilize the leaves to identify variable sequence, denoted as $V = {v_0, v_1, \ldots, v_m}$. We then take each variable as a node of the graph and a directed edge $\epsilon = \langle v_i, v_j \rangle$ from $v_i$ to $v_j$ refers that the value of $j$-th variable comes from $i$-th variable. The graph $\mathcal{G}(C) = (V; E)$ is used to represent relations among variables of the code $C$, as shown by the red arrows in Figure 1.</p>
<p>Step 2: Normalize data-flow items. For simplicity and unity, we ignore the variable position and normalize their names. We collect all the variables in the data-flow items and rename them var_$i$, where $i$ is the order of the variables appearing in all data-flow items.</p>
<p>Step 3: Calculate the semantic data-flow match score as:</p>
<p>$$
\text{Match}<em _text_clip="\text{clip">{\text{df}} = \text{Count}</em>}}\left(\text{DF<em _text_ref="\text{ref">{\text{cand}}\right) / \text{Count}\left(\text{DF}</em>
$$}}\right) \tag{5</p>
<p>where Count( $\text{DF}<em _text_clip="\text{clip">{\text{ref}}$) is the total number of the reference data-flows, and Count${}</em>\right)$ is the number of}}\left(\text{DF}_{\text{cand}</p>
<table>
<thead>
<tr>
<th>[Candidate]:</th>
<th>[Reference]:</th>
</tr>
</thead>
<tbody>
<tr>
<td>public static int Sign ( double d )</td>
<td>public static int Sign ( double d )</td>
</tr>
<tr>
<td>{ return ( float ) ( ( d == 0 ) ? 0 : ( c &lt; 0.0 ) ? -1 : 1 );</td>
<td>{ return ( int ) ( ( d == 0 ) ? 0 : ( d &lt; 0 ) ? -1 : 1 ); }</td>
</tr>
</tbody>
</table>
<p>Figure 3: Example 1. BLEU: 75.43; CodeBLEU: 69.73.</p>
<h3>3.4 Two Examples</h3>
<p>Here we will give two toy examples to show how to calculate CodeBLEU. Meanwhile, we show the qualitative advantages of CodeBLEU compared with the traditional BLEU score.</p>
<p>Example 1 The output candidate of a code synthesis system and the according reference are shown in Figure 3.</p>
<p>In this example, there are four differences between the candidate and the reference, which are stressed with the red color. They are (1) the conversion type of the return value ("float" vs. "int"); (2) the variable naming ("c" vs. "d"); (3) the type of a constant ("0.0" and "0"); (4) the missing token ("}") in the candidate. This toy example is designed based on the background that the data type, the variable naming and the token missing tend to cause problems in reality.</p>
<p>The CodeBLEU is calculated as follows: (1) First, we calculate the n-gram match score (BLEU, which is 75.43) given the candidate and the reference. (2) Then, we calculate the weighted n-gram match score for it. The weight assigned to the keywords "public, static, int, return, double" in the reference are 4 times more than that of the rest tokens. The resulting score is 74.91, lower than the BLEU score, penalizing the keyword error ("float" vs. "int"). (3) The number of all sub-trees of the reference AST generated by treesitter is 21 and the hit number for the candidate is 13, so the syntactic AST match score is $13 / 21 * 100=61.90(\%)$. The data type errors in the candidate are penalized by the AST mismatch. (4) Three data-flows can be extracted from the reference AST, which are "[('var_0', 'comesFrom', []), ('var_0', 'comesFrom', ['var_0'])], ('var_0', 'comesFrom', ['var_0'])]", corresponding to the three variables "d" in the reference. The first "d" comes from no parent because it is in the parameter list. The second and the third "d" come from the first "d". The variable names are normalized and their positions are ignored according to Section 3.3. However, we can only extract two data-flows from the candidate AST , i.e., "[('var_0', 'comesFrom', []), ('var_0', 'comesFrom', ['var_0'])]" corresponding to the two "d"s in this code. The variable "c" is used before declaration so no data-flow is extracted for it. Therefore the data-flow match score is $2 / 3 *$ $100=66.67(\%)$. With $\alpha, \beta, \gamma, \delta=0.25,0.25,0.25,0.25$, the final CodeBLEU score is 69.73 , which is lower than BLEU because CodeBLEU penalizes the keyword and semantic errors for the programming languages.</p>
<p>Example 2 As shown in Figure 4, in this example, there is no difference between the candidate and the reference except for the names of the local variables ("c" vs. "d"). In the real scenario, the candidate is correct without doubt, and a human expert would give a score of 100 . However, its</p>
<table>
<thead>
<tr>
<th>[Candidate]:</th>
<th>[Reference]:</th>
</tr>
</thead>
<tbody>
<tr>
<td>public static int Sign ( double c )</td>
<td>public static int Sign ( double d )</td>
</tr>
<tr>
<td>{ return ( int ) ( ( c == 0 ) ? 0 : ( c &lt; 0 ) ? -1 : 1 ); }</td>
<td>{ return ( int ) ( ( d == 0 ) ? 0 : ( d &lt; 0 ) ? -1 : 1 ); }</td>
</tr>
</tbody>
</table>
<p>Figure 4: Example 2. BLEU: 68.14; CodeBLEU: 83.97.</p>
<p>BLEU score is only 75.71 , which underestimates the quality of the candidate. With CodeBLEU, we have the weight ngram match score of 76.46 , the syntactic AST match score of 100 and the semantic data-flow match score of 100 , the final CodeBLEU score being 88.04 , which makes up for the underestimation of BLEU.</p>
<p>From the two examples, we find that in some typical scenarios, CodeBLEU gives more reasonable scores than BLEU to evaluate the code synthesis output. In the experiment section, we will give the quantitative analysis, further showing the effectiveness of CodeBLEU.</p>
<h2>4 Experiments</h2>
<p>We conduct experiments on three code synthesis tasks, i.e., text-to-code (Java), code translation (from Java to C#) and code refinement (Java). Previous work of these tasks uses BLEU or perfect accuracy (exactly match) for evaluation. In this paper, we will take the proposed CodeBLEU as the evaluation metric to see if CodeBLEU is more reasonable. For each task, we calculate the Pearson correlation coefficient to check the correlation between the scores given by our proposed CodeBLEU and the scores assigned by programmers (human evaluation scores). In the following subsections, we will first introduce the three tasks we used. Then we will give details of our experiment settings. Next, the experimental results will be shown and discussed. Finally, we will do an ablation study and investigate the influence of different components of CodeBLEU to the final results.</p>
<h3>4.1 Task Introduction</h3>
<p>The three tasks we choose for the experiment are text-tocode, code translation, and code refinement.</p>
<p>Text-to-code Text-to-code (Iyer et al. 2018) is the task of generating class member functions given the function documentation and the programmatic context. The inputs are the natural language documentation, and the class environment the code resides in. The environment comprises two lists of entities: (1) class member variable names with their data types, and (2) member function names together with their return types. The output is a piece of code of the desired class member function. We use the same dataset released by Iyer et al. (2018), which consists of 100k training samples, 2 k validation samples and 2 k test samples.</p>
<p>Code Translation Code translation aims to migrate legacy software from one programming language in a platform to another. Following Nguyen, Nguyen, and Nguyen (2015) and Chen, Liu, and Song (2018), we conduct experiments on a dataset crawled from several open-source projects, i.e.,</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Text-to-code</th>
<th>Code translation</th>
<th>Code refinement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sys1</td>
<td>Seq2Seq</td>
<td>PBSMT</td>
<td>LSTM</td>
</tr>
<tr>
<td>Sys2</td>
<td>Seq2Action+MAML</td>
<td>Transformer</td>
<td>Transformer</td>
</tr>
<tr>
<td>Sys3</td>
<td>GPT2</td>
<td>Transformer+CodeBERT</td>
<td>Transformer+CodeBERT</td>
</tr>
<tr>
<td>Sys4</td>
<td>CodeGPT</td>
<td>Human</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 1: The systems we choose for each task. Note that “Human” in this table means the output is given by human programming experts. ^{1} (Guo et al. 2019); ^{2} Fine-tune with GPT-2 (Radford et al. 2019); ^{3} Pre-trained GPT-2 with the Java data of Codesearchnet (Husain et al. 2019) and then fine-tuning; ^{4} Fine-tune with CodeBERT (Feng et al. 2020).</p>
<p>Lucene^{2}, POI^{3}, JGit^{4}, and Antlr^{5}. Those projects have both Java and C# implementation. We paired the methods in the two languages based on their file names and method names. After removing duplication, the total number of method pairs is 11.8k, and we split 0.5k pairs from them as the development set and another 1k pairs for test. We will release the code translation dataset with our scripts.</p>
<p>Code Refinement Code refinement aims to automatically fix bugs in the code, which can contribute to reducing the cost of bug-fixing for developers. We use the dataset released by Tufano et al. (2019). The source is buggy Java functions while the target is the according fixed ones. Their dataset contains two subsets ( i.e. small and medium) based on the code length. For the small dataset, the function numbers of training, development and test samples are 46,680, 5,835 and 5,835. For the medium dataset, the function numbers are 52,364, 6,545 and 6,545 respectively.</p>
<h3>4.2 Settings</h3>
<p>For each task, we prepare 3 to 4 standard systems as shown in Table 1. We randomly choose 500 samples from each test set for evaluation. As for human evaluation, we have a group of human judges consisting of 10 people who are familiar with Java and C#. The humans judge our four systems on a subset of 50 samples extracted randomly from our test set. We pair each input with its 4 outputs, resulting in a total of 200 pairs of the given inputs and the output codes. We prepare a UI software with these input-output pairs randomly ordered to disperse the 4 outputs of each input. All judges use this same software and see the pairs in the same order. They rated each output from 1 (very bad) to 5 (very good).</p>
<h3>4.3 Results</h3>
<p>Main Results The main results are shown in Table 2. In this table, we calculate BLEU scores, perfect accuracy, CodeBLEU and human evaluation scores for all systems of each task on the selected test set. Note that the former three metrics are ranging from 0 to 100 and the last one is ranging from 1 (very bad) to 5 (very good). We find that some of the systems are very close in terms of BLEU and CodeBLEU scores. Hence, some questions are raised.</p>
<table>
<thead>
<tr>
<th>Text-to-code</th>
</tr>
</thead>
<tbody>
<tr>
<td>System</td>
</tr>
<tr>
<td>Sys1</td>
</tr>
<tr>
<td>Sys2</td>
</tr>
<tr>
<td>Sys3</td>
</tr>
<tr>
<td>Sys4</td>
</tr>
<tr>
<td>Code translation</td>
</tr>
<tr>
<td>System</td>
</tr>
<tr>
<td>Sys1</td>
</tr>
<tr>
<td>Sys2</td>
</tr>
<tr>
<td>Sys3</td>
</tr>
<tr>
<td>Sys4</td>
</tr>
<tr>
<td>Code refinement</td>
</tr>
<tr>
<td>System</td>
</tr>
<tr>
<td>Sys1</td>
</tr>
<tr>
<td>Sys2</td>
</tr>
<tr>
<td>Sys3</td>
</tr>
</tbody>
</table>
<p>Table 2: The results of all baselines of the given three tasks evaluated by BLEU, accuracy (exactly match), CodeBLEU and human evaluation scores.</p>
<ul>
<li>Is the difference in CodeBLEU metric reliable?</li>
<li>What is the variance of the CodeBLEU score?</li>
<li>Is CodeBLEU more correlated with human scores than BLEU and accuracy?</li>
</ul>
<p>To answer these questions, first, following Papineni et al. (2002), we divided the test set into 20 blocks of 25 sentences each, and computed CodeBLEU on these blocks individually. We thus have 20 samples of these metrics for each system. We computed the means, variances, and paired t-statistics for them, which is displayed in Table 3.</p>
<p>From Table 3, as expected, these two sets of results are close for each system and differ only by small finite block size effects. Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems’ scores are statistically very significant. The reported variance on 25-sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus. Therefore, we conclude that the difference in the CodeBLEU metric is reliable, and the variance of it is within a reasonable range.</p>
<p>Next, we compare the correlation of BLEU, accuracy and</p>
<p>| | Text-to-code | | | Code translation | | | Code refinement | | |
| System | Mean | StdDev | t | Mean | StdDev | t | Mean | StdDev | t |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Sys1 | 17.93 | 1.8 | - | 44.62 | 5.2 | - | 79.21 | 5.6 | - |
| Sys2 | 20.67 | 2.9 | 7.4 | 60.04 | 5.8 | 30 | 81.04 | 5.8 | 2.1 |
| Sys3 | 23.92 | 3.4 | 7 | 81.55 | 6.1 | 38 | 82.52 | 6.4 | 3.4 |
| Sys4 | 30.13 | 4.2 | 12 | 83.26 | 6.7 | 5.2 | - | - | - |</p>
<p>Table 3: The mean, standard deviation and paired t-statistic of all baselines of the given three tasks. The t-statistic compares each system with the neighbor above it in the table.</p>
<table>
<thead>
<tr>
<th></th>
<th>Text-to-code</th>
<th>Code trans</th>
<th>Code ref</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLEU &amp; human</td>
<td>0.967</td>
<td>0.940</td>
<td>0.923</td>
</tr>
<tr>
<td>Acc &amp; human</td>
<td>0.912</td>
<td>0.968</td>
<td>$\mathbf{0.999}$</td>
</tr>
<tr>
<td>CodeBLEU &amp; human</td>
<td>$\mathbf{0.977}$</td>
<td>$\mathbf{0.970}$</td>
<td>0.979</td>
</tr>
<tr>
<td></td>
<td>(+1.0)</td>
<td>(+3.0)</td>
<td>(+5.6)</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of the Pearson correlation coefficients between human evaluation scores and three different metrics. The numbers in the brackets in the last row are the improvements in percent compared with BLEU.</p>
<p>CodeBLEU to human evaluation scores respectively. The Pearson correlation coefficients are listed in Table 4.</p>
<p>From the table, we see CodeBLEU scores are more correlated with human evaluation scores in all the three tasks. The improvements are significant compared with the traditional MT metric BLEU. The results verify the effectiveness of our proposed metric. For text-to-code and code translation tasks, CodeBLEU scores are also more correlated with human scores than accuracy (Acc), but there is an exception that the Acc is more correlated for code refinement. This is because the data of refinement task is just fixing small bugs in a given Java function. The output is usually unique, and the humans score the outputs based on the unique refinement way, so that the Acc here correlates more with human evaluation scores. However, we also believe that in the more general code synthesis scenarios, CodeBLEU is more reasonable in terms of the correlation with human scores.</p>
<p>Figure 5 shows the comparable regression results for each metric to human scores on the text-to-code and code translation tasks. The $R^{2}$ values of the linear regression are also shown in the figure. From the figure, we find CodeBLEU is more linearly correlated with human evaluation scores than BLEU, which is consistent with the results in Table 4.</p>
<p>Based on the above results and analysis, we conclude that:</p>
<ul>
<li>The difference in CodeBLEU metric is reliable. CodeBLEU is capable to differentiate code synthesis systems.</li>
<li>CodeBLEU is reliable, and its variance is within a reasonable range.</li>
<li>CodeBLEU is more correlated with human evaluation scores than traditional BLEU scores on all the three tasks, and more correlated than Acc on the two tasks.</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: BLEU and CodeBLEU predict human evaluation scores. (a) Text-to-code; (b) Code translation.</p>
<p>Ablation Study To investigate the influence of the different components of CodeBLEU, we conduct the following experiment to calculate the respective Pearson correlation between the human evaluation scores and the scores given by different components. The results are reported in Table 5.</p>
<table>
<thead>
<tr>
<th>Components</th>
<th>Text-to-code</th>
<th>Code trans</th>
<th>Code ref</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLEU</td>
<td>0.967</td>
<td>0.940</td>
<td>0.923</td>
</tr>
<tr>
<td>BLEU ${ }_{\text {weight }}$</td>
<td>0.960</td>
<td>0.934</td>
<td>0.985</td>
</tr>
<tr>
<td>Match $_{\text {ast }}$</td>
<td>0.985</td>
<td>0.977</td>
<td>0.967</td>
</tr>
<tr>
<td>Match $_{\text {df }}$</td>
<td>0.978</td>
<td>0.974</td>
<td>0.983</td>
</tr>
<tr>
<td>CodeBLEU</td>
<td>0.977</td>
<td>0.970</td>
<td>0.979</td>
</tr>
</tbody>
</table>
<p>Table 5: The Pearson correlation coefficients between different components of CodeBLEU and humans.</p>
<p>From the table, we find that, for the text-to-code and code translation tasks, the scores of the last two components, i.e., syntactic AST match and semantic data-flow match, are more relevant to human evaluation scores compared with the n-gram and weight n-gram match scores. For the code refinement task, the scores given by the weighted n-gram match and the semantic data-flow are more relevant to human evaluation. This may be because many bugs in the refinement training data are wrong variable naming or keywords errors,</p>
<p>while the weighted n-gram and semantic data-flow match scores could evaluate them better. The above result verifies the effectiveness of our three proposed components, i.e., weighted n-gram match, syntactic AST match and semantic data-flow match, for code synthesis evaluation. Besides, the results are inspiring for us to change the hyper-parameters $\alpha, \beta, \gamma, \delta$ in Eq. (1) to get better evaluation whose results are more correlated with humans. For example, to achieve this, we can increase $\gamma$ and $\delta$ to improve the weights of the last two components in the final CodeBLEU scores. In the next section, we will conduct experiments to investigate the influence of the four hyper-parameters.</p>
<h3>4.4 Influence of hyper-parameters</h3>
<p>In the above subsection, we find different components have a different influence on the final results of CodeBLEU in terms of the correlation with human evaluation scores. Therefore, we can change the weights of those components to achieve a higher correlation between CodeBLEU and human evaluation. We gradually increase the weights of the last two components (as in Table 6) and record the correlation coefficients between CodeBLEU and human evaluation scores for the three tasks. The results are shown in Figure 6.</p>
<p>From the figure, we find that increasing the weights of the last two components improves the correlation between CodeBLEU and human scores for all of the three tasks. The performance starts to converge after the combination [4] and the combination [7], i.e., $\alpha, \beta, \gamma, \delta=0.1,0.1,0.4,0.4$, achieves the best result among all the combinations in Figure 6 (0.981, 0.975, 0.980 for the three tasks respectively). Of course, [7] is not the best combination all the time. For example, $\alpha, \beta, \gamma, \delta=0.1,0.4,0.1,0.4$ achieves the better result (the correlation coefficient is 0.984 ) than the combination [7] (the correlation coefficient is 0.980 ) for the code refinement task. In spite of this, we recommend to choose the combination [7] when calculating CodeBLEU for general code synthesis tasks, because the last two components are more likely to be more correlated with human evaluation scores from the instinct given by Table 4.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: The correlation coefficients between CodeBLEU and human scores with different hyper-parameters. The hyper-parameter setting of each combination is in Table 6.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Combination</th>
<th style="text-align: center;">$\alpha, \beta, \gamma, \delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[1]</td>
<td style="text-align: center;">$0.40,0.40,0.10,0.10$</td>
</tr>
<tr>
<td style="text-align: center;">[2]</td>
<td style="text-align: center;">$0.35,0.35,0.15,0.15$</td>
</tr>
<tr>
<td style="text-align: center;">[3]</td>
<td style="text-align: center;">$0.30,0.30,0.20,0.20$</td>
</tr>
<tr>
<td style="text-align: center;">[4]</td>
<td style="text-align: center;">$0.25,0.25,0.25,0.25$</td>
</tr>
<tr>
<td style="text-align: center;">[5]</td>
<td style="text-align: center;">$0.20,0.20,0.30,0.30$</td>
</tr>
<tr>
<td style="text-align: center;">[6]</td>
<td style="text-align: center;">$0.15,0.15,0.35,0.35$</td>
</tr>
<tr>
<td style="text-align: center;">[7]</td>
<td style="text-align: center;">$0.10,0.10,0.40,0.40$</td>
</tr>
</tbody>
</table>
<p>Table 6: The settings of each combination in Figure 6.</p>
<h2>5 Related Work</h2>
<p>As code artificial intelligence receives more and more attention (Allamanis et al. 2015; Yin and Neubig 2017; Allamanis et al. 2018; Monperrus 2018; Alon et al. 2019; Svyatkovskiy et al. 2020), the evaluation of code synthesis becomes critical to promote its development. Although there are several automatic evaluation methods, which can be used to evaluate code synthesis (Karaivanov, Raychev, and Vechev 2014; Chen, Liu, and Song 2018; Lachaux et al. 2020), these approaches still suffer from many weakness and are not suitable to evaluate code.</p>
<p>The widely used 4-gram BLEU (Papineni et al. 2002) evaluates the code quality by using the relative overlap between the tokens in the hypothesis and reference (Karaivanov, Raychev, and Vechev 2014; Barone and Sennrich 2017). Nevertheless, BLEU ignores the grammatical correctness and logic correctness. The perfect accuracy (Rabinovich, Stern, and Klein 2017; Chen, Liu, and Song 2018) is too strict and it is an underestimation of the true accuracy based on semantic equivalence. Additionally, the computational accuracy (Lachaux et al. 2020), evaluating whether the hypothesis function generates the same outputs given the same inputs by performing code, locks universality and practicability. To overcome the limitation, our proposed simple and effective CodeBLEU can not only consider the surface match similar with the original BLEU, but can also consider the grammatical correctness and the logic correctness.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we propose a novel metric CodeBLEU for code synthesis evaluation. CodeBLEU evaluates the candidate code pieces considering not only the shallow match, but also the syntactic match and the semantic match. The results of three real-world tasks, i.e. text-to-code, code translation and code refinement, demonstrate the rationality and effectiveness of CodeBLEU by analyzing the correlation with human evaluation scores from different granularity. In the future work, we will delve more into the evaluation of syntactic and semantic match and try more tasks with CodeBLEU to show its practicality.</p>
<h2>References</h2>
<p>Allamanis, M.; Barr, E. T.; Devanbu, P.; and Sutton, C. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR) 51(4): 1-37.</p>
<p>Allamanis, M.; Tarlow, D.; Gordon, A.; and Wei, Y. 2015. Bimodal modelling of source code and natural language. In International conference on machine learning, 2123-2132.
Alon, U.; Zilberstein, M.; Levy, O.; and Yahav, E. 2019. code2vec: Learning distributed representations of code. Proceedings of the ACM on Programming Languages 3(POPL): $1-29$.
Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .
Barone, A. V. M.; and Sennrich, R. 2017. A parallel corpus of Python functions and documentation strings for automated code documentation and code generation. arXiv preprint arXiv:1707.02275 .
Chen, X.; Liu, C.; and Song, D. 2018. Tree-to-tree neural networks for program translation. In Advances in neural information processing systems, 2547-2557.
Dinella, E.; Dai, H.; Li, Z.; Naik, M.; Song, L.; and Wang, K. 2020. Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs. In International Conference on Learning Representations.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.; Shou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 .
Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Liu, S.; Zhou, L.; Duan, N.; Yin, J.; Jiang, D.; et al. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. arXiv preprint arXiv:2009.08366 .
Guo, D.; Tang, D.; Duan, N.; Zhou, M.; and Yin, J. 2019. Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing. arXiv preprint arXiv:1906.07108 .
Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and Brockschmidt, M. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 .
Iyer, S.; Konstas, I.; Cheung, A.; and Zettlemoyer, L. 2018. Mapping Language to Code in Programmatic Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 1643-1652.
Kanade, A.; Maniatis, P.; Balakrishnan, G.; and Shi, K. 2019. Pre-trained contextual embedding of source code. arXiv preprint arXiv:2001.00059 .
Karaivanov, S.; Raychev, V.; and Vechev, M. 2014. Phrasebased statistical translation of programming languages. In Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \&amp; Software, 173-184.
Lachaux, M.-A.; Roziere, B.; Chanussot, L.; and Lample, G. 2020. Unsupervised Translation of Programming Languages. arXiv preprint arXiv:2006.03511 .
Lin, C.-Y. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out,</p>
<p>74-81. Barcelona, Spain: Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W041013.</p>
<p>Monperrus, M. 2018. Automatic software repair: a bibliography. ACM Computing Surveys (CSUR) 51(1): 1-24.
Nguyen, A. T.; Nguyen, T. T.; and Nguyen, T. N. 2015. Divide-and-conquer approach for multi-phase statistical migration for source code (t). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), 585-596. IEEE.
Oda, Y.; Fudaba, H.; Neubig, G.; Hata, H.; Sakti, S.; Toda, T.; and Nakamura, S. 2015. Learning to generate pseudocode from source code using statistical machine translation (t). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), 574-584. IEEE.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 311-318.
Rabinovich, M.; Stern, M.; and Klein, D. 2017. Abstract syntax networks for code generation and semantic parsing. arXiv preprint arXiv:1704.07535 .
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsupervised multitask learners. OpenAI Blog 1(8): 9.
Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104-3112.
Svyatkovskiy, A.; Deng, S. K.; Fu, S.; and Sundaresan, N. 2020. IntelliCode Compose: Code Generation Using Transformer. arXiv preprint arXiv:2005.08025 .
Tufano, M.; Watson, C.; Bavota, G.; Penta, M. D.; White, M.; and Poshyvanyk, D. 2019. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on Software Engineering and Methodology (TOSEM) 28(4): 1-29.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, 6000-6010.
Weaver, W. 1955. Translation. Machine translation of languages 14(15-23): 10.
Yin, P.; and Neubig, G. 2017. A Syntactic Neural Model for General-Purpose Code Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 440-450. Vancouver, Canada: Association for Computational Linguistics.
Zhou, L.; Zhang, J.; Zong, C.; and Yu, H. 2019. Sequence generation: From both sides to the middle. In Proceedings of IJCAI 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright (c) 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>https://github.com/tree-sitter/tree-sitter&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3476 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3476</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3476</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-80.html">extraction-schema-80</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <p><strong>Paper ID:</strong> paper-247187974</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.00600v1.pdf" target="_blank">Dual Embodied-Symbolic Concept Representations for Deep Learning</a></p>
                <p><strong>Paper Abstract:</strong> Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs. Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space. Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space. The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing. As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning. To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching. Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration. We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3476.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3476.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual embodied-symbolic model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual embodied-symbolic concept representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional-level theory positing that human concepts are represented at two interacting levels: embodied (modality-specific feature vectors derived from sensorimotor experience) and symbolic (amodal, language-specific concept/knowledge-graph embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>dual embodied-symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is represented simultaneously in two complementary formats: (1) embodied representations — modality-specific feature vectors that encode sensorimotor and perceptual features of concepts; and (2) symbolic representations — amodal, language-derived graph/word embeddings (concept graphs) capturing distributional and relational semantic structure; the two levels interact to drive conceptual processing.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Behavioral and neurocognitive findings cited in the paper: (a) category production studies where both sensorimotor similarity and linguistic proximity predict order/frequency of produced category members; (b) neuroimaging and cognitive evidence for modality-specific, multimodal, and amodal hub regions for concrete concepts; (c) multimodal models (visual + affective + linguistic) better capture human behavior for many concepts; computational use-cases in deep learning (knowledge distillation for few-shot CIL, fused image-concept representations for image-text matching) provide practical support for the representational utility of dual formats.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Paper notes that in some tasks people rely predominantly on linguistic distributional information (the 'linguistic shortcut') when it suffices, and that symbol-only representations can sometimes predict behavior; grounding abstract concepts remains challenging and requires extra mechanisms (emotional, social, linguistic information). The paper does not present direct neural-level causal evidence that the two formats are always both instantiated for every concept.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Explicitly contrasted with pure embodied (grounded) theories and pure symbolic (distributional) theories: dual model is presented as a hybrid that reconciles these approaches by assigning complementary roles and allowing interaction (linguistic shortcuts vs sensorimotor simulation when needed).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How the two levels are implemented and interact neurally (mechanistic mapping), how to represent and ground highly abstract concepts robustly, and how to integrate the dual formats in unified learning architectures remain open; empirical constraints on the dynamics of switching between linguistic shortcut and sensorimotor simulation are underspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3476.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual-coding framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual-coding framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical framework proposing that knowledge is coded in two complementary systems — sensorimotor-based and language-based — which together support conceptual processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dual Coding of Knowledge in the Human Brain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>dual-coding framework</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented in two distinct but interacting codes: imagery/sensorimotor-based codes that derive from perceptual/motor experience, and verbal/amodal codes derived from language; both codes contribute to semantic representation and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Cited evidence that concrete concepts show involvement of both sensorimotor and linguistic systems; computational and behavioral studies where both modalities predict performance in tasks like category production.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Explaining abstract concepts that lack direct sensorimotor referents challenges a naive dual-coding account; must be extended to incorporate metaphor, emotion, and social/linguistic grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as a precursor and conceptual ally to the paper's advocated dual embodied-symbolic model; contrasts with single-format symbolic-only or purely embodied-only accounts.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Details on how representations from the two codes integrate and the boundary conditions for reliance on one code over the other (e.g., task demands) are not fully specified in the surveyed work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3476.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linguistic shortcut hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linguistic shortcut hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hypothesis that people will use computationally cheaper linguistic distributional information for conceptual tasks when it suffices, resorting to sensorimotor simulation only when more detailed representation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>linguistic shortcut hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>When a task can be solved using distributional (linguistic) cues, the cognitive system preferentially uses those cheaper cues rather than invoking costly sensorimotor simulations; sensorimotor simulation is used when higher-fidelity conceptual information is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Category production study cited: both sensorimotor similarity and linguistic proximity predict category production metrics, with linguistic proximity explaining variance beyond sensorimotor similarity; many everyday tasks can be accomplished using distributional cues alone.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>There are tasks (or concept types) where distributional cues are insufficient (e.g., perceptual discriminations, fine-grained semantic distinctions) and sensorimotor information must be recruited; the hypothesis does not by itself specify the decision rule or neural mechanism for when the switch occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Frames distributional and embodied sources as competing but complementary information sources; contrasts with views that sensorimotor simulation is always primary for meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Mechanistic characterization of the 'cost' computation and selection process, and empirical boundary conditions determining when linguistic shortcuts are used, are open issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3476.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid (modality-specific + amodal hub)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid models combining modality-specific systems and amodal semantic hubs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional models positing that conceptual content is stored partly in distributed modality-specific areas and partly in amodal/multimodal hub regions that integrate across modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Varieties of Abstract Concepts and Their Grounding in Perception or Action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>hybrid hub-and-spoke model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is distributed across 'spokes' (modality-specific sensorimotor and affective systems encoding component features) and an amodal 'hub' or multimodal regions that integrate and code higher-order, supramodal conceptual information.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Neurocognitive findings cited: evidence that modality-specific and multimodal regions represent conceptual features while amodal hubs code supramodal conceptual information; this pattern fits both concrete and abstract concept data better than purely modality-specific or purely amodal accounts.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Precise localization and functional role of hubs versus spokes, and how abstract concepts recruit non-sensorimotor modalities (emotion, social cognition), remain debated; some models differ in whether hubs are strictly amodal or merely highly multimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as superior to purely embodied or purely amodal symbolic models for accommodating both concrete and abstract concept phenomena; the hybrid model explicitly reconciles distributional/linguistic and sensorimotor contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How hub representations are learned and the computational transformations between spoke information and hub codes, and how hubs support flexible context-sensitive conceptualization, are unresolved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3476.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied / grounded cognition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied (grounded) cognition theories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of theories proposing that conceptual knowledge is grounded in sensorimotor and bodily systems — concepts are represented via reenactments or simulations of perceptual, motor, and affective states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Varieties of Abstract Concepts and Their Grounding in Perception or Action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>embodied (grounded) cognition</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are functionally represented by modality-specific sensorimotor and affective systems such that understanding a concept involves partially reenacting perception, action, or internal states associated with that concept.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Behavioral and imaging findings showing modality-specific activations for concrete concepts; improvements in computational models when perceptual/affective features are included (multimodal models outperform purely linguistic ones for many concept-related tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Hard to account for abstract concepts lacking clear sensorimotor referents; purely sensorimotor accounts struggle to explain behaviors predicted by distributional linguistic information and rapid generalization based on linguistic context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with symbolic/distributional theories; paper argues embodied theories must be extended and combined with symbolic/linguistic information to account for abstract concept representation and many behavioral patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Mechanisms for grounding abstract concepts (role of metaphor, emotion, social cognition, language) and the degree to which sensorimotor simulation is necessary versus epiphenomenal are open questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3476.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional linguistic model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional linguistic (distributional semantics) model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional account where word meaning derives from linguistic context statistics: words that occur in similar contexts have similar meanings (distributional hypothesis).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual and Affective Multimodal Models of Word Meaning in Language and Mind</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>distributional semantics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts (words) are represented as vectors derived from patterns of co-occurrence in large language corpora; semantic similarity arises from proximity in this distributional semantic space.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Distributional models (e.g., Word2Vec, GloVe) account for many linguistic phenomena and predict human behavior in some semantic tasks; in the cited category-production study, linguistic proximity predicted category production above sensory measures.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Purely corpus-derived embeddings fail to capture perceptual, affective, and grounded aspects of meaning; they may miss structured relational or commonsense knowledge unless augmented with other sources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with embodied/grounded accounts; paper advocates multimodal combinations (distributional + perceptual/affective) as better predictors of human conceptual representations than distributional-only models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to integrate distributional vectors with sensorimotor/affective feature representations and how distributional cues are used by humans versus when sensorimotor simulation is required are open questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3476.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word association model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word association network model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model treating word meaning as a distribution over associative links measured from human association data (semantic network of associative strengths).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual and Affective Multimodal Models of Word Meaning in Language and Mind</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>word association model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented by the pattern and weights of associative links to other words in a large semantic network (derived from human association tasks), capturing associative relatedness beyond corpus co-occurrence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Cited comparative modeling study shows that word-association and multimodal (vis + affect) models sometimes better capture human conceptual behavior than purely distributional models, particularly for certain concept classes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Association networks depend on available association norms and may not scale or capture structured relational/commonsense knowledge as explicitly as knowledge-graph based representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as an alternative linguistic-based representation to distributional corpus-derived vectors; contrasts in how semantic relations are encoded (associative links vs contextual co-occurrence).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How associative network representations combine with perceptual/affective information and how they generalize to novel concepts are open issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3476.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exemplar-based representation (CSSL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exemplar-based contrastive self-supervised learning (CSSL) for embodied representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recommended practical approach for learning concept-centered embodied representations that stores or emphasizes exemplars (instances) and uses contrastive objectives to produce class-centric feature vectors suited for class-incremental learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>exemplar-based representation (functional)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally, concepts are represented via stored exemplars (or exemplar-derived feature prototypes); learned embeddings emphasize instance-level similarity and discriminability so that new classes can be integrated incrementally without catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Paper argues exemplar-based CSSL is concept-centric and supports class incremental learning; cited experiments/approaches (their own and related work) show reduced catastrophic forgetting and improved few-shot generalization when exemplar-style multi-embedding and semantic guidance are used.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Exemplar approaches can be memory intensive and need mechanisms (e.g., grouping/attention, semantic guidance) to avoid overfitting in few-shot settings; exemplar storage/selection and scaling to large taxonomies are challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasts with prototype- or purely parametric-classifiers by emphasizing retention of instance-level structure; in the paper exemplar-style learning is combined with symbolic semantic vectors to leverage both embodied exemplars and amodal semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How exemplar-based storage maps onto neural substrates (if at all), how exemplars should be selected/condensed, and how to optimally combine exemplar representations with symbolic graph embeddings remain open.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3476.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3476.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge graph embedding (KGE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge graph embeddings for symbolic concept graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of computational representations embedding nodes (concepts) and relations of a knowledge graph into continuous vector spaces to capture relational and structural semantics of concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph Embeddings and Explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>knowledge-graph embedding (symbolic representation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Symbolic conceptual knowledge (facts, relations) is functionally represented as low-dimensional vectors for concepts and relations such that graph structure and relational patterns are preserved (e.g., translation models like TransE or tensor/matrix-based semantic matching models).</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Cited success of KGE methods in encoding structured relational knowledge and enabling downstream tasks (link prediction, reasoning); paper uses KG embeddings as the symbolic level in dual representations and as a bridge in scene-graph / commonsense-graph integration (GB-Net).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Some KGE methods that treat triples independently neglect global intrinsic associations and deeper semantics; earlier translation-based and semantic-matching models have limits handling multi-relational, multi-hop, or contextualized knowledge without graph-aware neural extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with distributional linguistic embeddings: KGEs aim to capture explicit relational/graph structure rather than purely distributional co-occurrence statistics; the paper proposes combining KGEs (symbolic) with embodied modality-specific embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to ground purely symbolic KG nodes to perceptual data (symbol grounding), how to incorporate multimodal attributes, and how to scale KGE methods to richer commonsense and multimodal knowledge remain active questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dual Coding of Knowledge in the Human Brain <em>(Rating: 2)</em></li>
                <li>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production <em>(Rating: 2)</em></li>
                <li>Varieties of Abstract Concepts and Their Grounding in Perception or Action <em>(Rating: 2)</em></li>
                <li>Visual and Affective Multimodal Models of Word Meaning in Language and Mind <em>(Rating: 2)</em></li>
                <li>Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning <em>(Rating: 2)</em></li>
                <li>Knowledge Graph Embeddings and Explainable AI <em>(Rating: 2)</em></li>
                <li>Bridging Knowledge Graphs to Generate Scene Graphs <em>(Rating: 2)</em></li>
                <li>Knowledge Aware Semantic Concept Expansion for Image-Text Matching <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3476",
    "paper_id": "paper-247187974",
    "extraction_schema_id": "extraction-schema-80",
    "extracted_data": [
        {
            "name_short": "Dual embodied-symbolic model",
            "name_full": "Dual embodied-symbolic concept representation",
            "brief_description": "A functional-level theory positing that human concepts are represented at two interacting levels: embodied (modality-specific feature vectors derived from sensorimotor experience) and symbolic (amodal, language-specific concept/knowledge-graph embeddings).",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "dual embodied-symbolic representation",
            "theory_description": "Conceptual knowledge is represented simultaneously in two complementary formats: (1) embodied representations — modality-specific feature vectors that encode sensorimotor and perceptual features of concepts; and (2) symbolic representations — amodal, language-derived graph/word embeddings (concept graphs) capturing distributional and relational semantic structure; the two levels interact to drive conceptual processing.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Behavioral and neurocognitive findings cited in the paper: (a) category production studies where both sensorimotor similarity and linguistic proximity predict order/frequency of produced category members; (b) neuroimaging and cognitive evidence for modality-specific, multimodal, and amodal hub regions for concrete concepts; (c) multimodal models (visual + affective + linguistic) better capture human behavior for many concepts; computational use-cases in deep learning (knowledge distillation for few-shot CIL, fused image-concept representations for image-text matching) provide practical support for the representational utility of dual formats.",
            "counter_evidence_or_challenges": "Paper notes that in some tasks people rely predominantly on linguistic distributional information (the 'linguistic shortcut') when it suffices, and that symbol-only representations can sometimes predict behavior; grounding abstract concepts remains challenging and requires extra mechanisms (emotional, social, linguistic information). The paper does not present direct neural-level causal evidence that the two formats are always both instantiated for every concept.",
            "comparison_to_other_theories": "Explicitly contrasted with pure embodied (grounded) theories and pure symbolic (distributional) theories: dual model is presented as a hybrid that reconciles these approaches by assigning complementary roles and allowing interaction (linguistic shortcuts vs sensorimotor simulation when needed).",
            "notable_limitations_or_open_questions": "How the two levels are implemented and interact neurally (mechanistic mapping), how to represent and ground highly abstract concepts robustly, and how to integrate the dual formats in unified learning architectures remain open; empirical constraints on the dynamics of switching between linguistic shortcut and sensorimotor simulation are underspecified.",
            "uuid": "e3476.0",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Dual-coding framework",
            "name_full": "Dual-coding framework",
            "brief_description": "A theoretical framework proposing that knowledge is coded in two complementary systems — sensorimotor-based and language-based — which together support conceptual processing.",
            "citation_title": "Dual Coding of Knowledge in the Human Brain",
            "mention_or_use": "mention",
            "theory_name": "dual-coding framework",
            "theory_description": "Concepts are represented in two distinct but interacting codes: imagery/sensorimotor-based codes that derive from perceptual/motor experience, and verbal/amodal codes derived from language; both codes contribute to semantic representation and retrieval.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Cited evidence that concrete concepts show involvement of both sensorimotor and linguistic systems; computational and behavioral studies where both modalities predict performance in tasks like category production.",
            "counter_evidence_or_challenges": "Explaining abstract concepts that lack direct sensorimotor referents challenges a naive dual-coding account; must be extended to incorporate metaphor, emotion, and social/linguistic grounding.",
            "comparison_to_other_theories": "Presented as a precursor and conceptual ally to the paper's advocated dual embodied-symbolic model; contrasts with single-format symbolic-only or purely embodied-only accounts.",
            "notable_limitations_or_open_questions": "Details on how representations from the two codes integrate and the boundary conditions for reliance on one code over the other (e.g., task demands) are not fully specified in the surveyed work.",
            "uuid": "e3476.1",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Linguistic shortcut hypothesis",
            "name_full": "Linguistic shortcut hypothesis",
            "brief_description": "A hypothesis that people will use computationally cheaper linguistic distributional information for conceptual tasks when it suffices, resorting to sensorimotor simulation only when more detailed representation is required.",
            "citation_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "mention_or_use": "mention",
            "theory_name": "linguistic shortcut hypothesis",
            "theory_description": "When a task can be solved using distributional (linguistic) cues, the cognitive system preferentially uses those cheaper cues rather than invoking costly sensorimotor simulations; sensorimotor simulation is used when higher-fidelity conceptual information is necessary.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Category production study cited: both sensorimotor similarity and linguistic proximity predict category production metrics, with linguistic proximity explaining variance beyond sensorimotor similarity; many everyday tasks can be accomplished using distributional cues alone.",
            "counter_evidence_or_challenges": "There are tasks (or concept types) where distributional cues are insufficient (e.g., perceptual discriminations, fine-grained semantic distinctions) and sensorimotor information must be recruited; the hypothesis does not by itself specify the decision rule or neural mechanism for when the switch occurs.",
            "comparison_to_other_theories": "Frames distributional and embodied sources as competing but complementary information sources; contrasts with views that sensorimotor simulation is always primary for meaning.",
            "notable_limitations_or_open_questions": "Mechanistic characterization of the 'cost' computation and selection process, and empirical boundary conditions determining when linguistic shortcuts are used, are open issues.",
            "uuid": "e3476.2",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Hybrid (modality-specific + amodal hub)",
            "name_full": "Hybrid models combining modality-specific systems and amodal semantic hubs",
            "brief_description": "Functional models positing that conceptual content is stored partly in distributed modality-specific areas and partly in amodal/multimodal hub regions that integrate across modalities.",
            "citation_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "mention_or_use": "mention",
            "theory_name": "hybrid hub-and-spoke model",
            "theory_description": "Conceptual knowledge is distributed across 'spokes' (modality-specific sensorimotor and affective systems encoding component features) and an amodal 'hub' or multimodal regions that integrate and code higher-order, supramodal conceptual information.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Neurocognitive findings cited: evidence that modality-specific and multimodal regions represent conceptual features while amodal hubs code supramodal conceptual information; this pattern fits both concrete and abstract concept data better than purely modality-specific or purely amodal accounts.",
            "counter_evidence_or_challenges": "Precise localization and functional role of hubs versus spokes, and how abstract concepts recruit non-sensorimotor modalities (emotion, social cognition), remain debated; some models differ in whether hubs are strictly amodal or merely highly multimodal.",
            "comparison_to_other_theories": "Presented as superior to purely embodied or purely amodal symbolic models for accommodating both concrete and abstract concept phenomena; the hybrid model explicitly reconciles distributional/linguistic and sensorimotor contributions.",
            "notable_limitations_or_open_questions": "How hub representations are learned and the computational transformations between spoke information and hub codes, and how hubs support flexible context-sensitive conceptualization, are unresolved.",
            "uuid": "e3476.3",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Embodied / grounded cognition",
            "name_full": "Embodied (grounded) cognition theories",
            "brief_description": "A class of theories proposing that conceptual knowledge is grounded in sensorimotor and bodily systems — concepts are represented via reenactments or simulations of perceptual, motor, and affective states.",
            "citation_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "mention_or_use": "mention",
            "theory_name": "embodied (grounded) cognition",
            "theory_description": "Concepts are functionally represented by modality-specific sensorimotor and affective systems such that understanding a concept involves partially reenacting perception, action, or internal states associated with that concept.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Behavioral and imaging findings showing modality-specific activations for concrete concepts; improvements in computational models when perceptual/affective features are included (multimodal models outperform purely linguistic ones for many concept-related tasks).",
            "counter_evidence_or_challenges": "Hard to account for abstract concepts lacking clear sensorimotor referents; purely sensorimotor accounts struggle to explain behaviors predicted by distributional linguistic information and rapid generalization based on linguistic context.",
            "comparison_to_other_theories": "Contrasted with symbolic/distributional theories; paper argues embodied theories must be extended and combined with symbolic/linguistic information to account for abstract concept representation and many behavioral patterns.",
            "notable_limitations_or_open_questions": "Mechanisms for grounding abstract concepts (role of metaphor, emotion, social cognition, language) and the degree to which sensorimotor simulation is necessary versus epiphenomenal are open questions.",
            "uuid": "e3476.4",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Distributional linguistic model",
            "name_full": "Distributional linguistic (distributional semantics) model",
            "brief_description": "A functional account where word meaning derives from linguistic context statistics: words that occur in similar contexts have similar meanings (distributional hypothesis).",
            "citation_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "mention_or_use": "mention",
            "theory_name": "distributional semantics",
            "theory_description": "Concepts (words) are represented as vectors derived from patterns of co-occurrence in large language corpora; semantic similarity arises from proximity in this distributional semantic space.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Distributional models (e.g., Word2Vec, GloVe) account for many linguistic phenomena and predict human behavior in some semantic tasks; in the cited category-production study, linguistic proximity predicted category production above sensory measures.",
            "counter_evidence_or_challenges": "Purely corpus-derived embeddings fail to capture perceptual, affective, and grounded aspects of meaning; they may miss structured relational or commonsense knowledge unless augmented with other sources.",
            "comparison_to_other_theories": "Contrasted with embodied/grounded accounts; paper advocates multimodal combinations (distributional + perceptual/affective) as better predictors of human conceptual representations than distributional-only models.",
            "notable_limitations_or_open_questions": "How to integrate distributional vectors with sensorimotor/affective feature representations and how distributional cues are used by humans versus when sensorimotor simulation is required are open questions.",
            "uuid": "e3476.5",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Word association model",
            "name_full": "Word association network model",
            "brief_description": "A model treating word meaning as a distribution over associative links measured from human association data (semantic network of associative strengths).",
            "citation_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "mention_or_use": "mention",
            "theory_name": "word association model",
            "theory_description": "Concepts are represented by the pattern and weights of associative links to other words in a large semantic network (derived from human association tasks), capturing associative relatedness beyond corpus co-occurrence statistics.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Cited comparative modeling study shows that word-association and multimodal (vis + affect) models sometimes better capture human conceptual behavior than purely distributional models, particularly for certain concept classes.",
            "counter_evidence_or_challenges": "Association networks depend on available association norms and may not scale or capture structured relational/commonsense knowledge as explicitly as knowledge-graph based representations.",
            "comparison_to_other_theories": "Presented as an alternative linguistic-based representation to distributional corpus-derived vectors; contrasts in how semantic relations are encoded (associative links vs contextual co-occurrence).",
            "notable_limitations_or_open_questions": "How associative network representations combine with perceptual/affective information and how they generalize to novel concepts are open issues.",
            "uuid": "e3476.6",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Exemplar-based representation (CSSL)",
            "name_full": "Exemplar-based contrastive self-supervised learning (CSSL) for embodied representations",
            "brief_description": "A recommended practical approach for learning concept-centered embodied representations that stores or emphasizes exemplars (instances) and uses contrastive objectives to produce class-centric feature vectors suited for class-incremental learning.",
            "citation_title": "Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning",
            "mention_or_use": "use",
            "theory_name": "exemplar-based representation (functional)",
            "theory_description": "Functionally, concepts are represented via stored exemplars (or exemplar-derived feature prototypes); learned embeddings emphasize instance-level similarity and discriminability so that new classes can be integrated incrementally without catastrophic forgetting.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Paper argues exemplar-based CSSL is concept-centric and supports class incremental learning; cited experiments/approaches (their own and related work) show reduced catastrophic forgetting and improved few-shot generalization when exemplar-style multi-embedding and semantic guidance are used.",
            "counter_evidence_or_challenges": "Exemplar approaches can be memory intensive and need mechanisms (e.g., grouping/attention, semantic guidance) to avoid overfitting in few-shot settings; exemplar storage/selection and scaling to large taxonomies are challenges.",
            "comparison_to_other_theories": "Contrasts with prototype- or purely parametric-classifiers by emphasizing retention of instance-level structure; in the paper exemplar-style learning is combined with symbolic semantic vectors to leverage both embodied exemplars and amodal semantics.",
            "notable_limitations_or_open_questions": "How exemplar-based storage maps onto neural substrates (if at all), how exemplars should be selected/condensed, and how to optimally combine exemplar representations with symbolic graph embeddings remain open.",
            "uuid": "e3476.7",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Knowledge graph embedding (KGE)",
            "name_full": "Knowledge graph embeddings for symbolic concept graphs",
            "brief_description": "A family of computational representations embedding nodes (concepts) and relations of a knowledge graph into continuous vector spaces to capture relational and structural semantics of concepts.",
            "citation_title": "Knowledge Graph Embeddings and Explainable AI",
            "mention_or_use": "use",
            "theory_name": "knowledge-graph embedding (symbolic representation)",
            "theory_description": "Symbolic conceptual knowledge (facts, relations) is functionally represented as low-dimensional vectors for concepts and relations such that graph structure and relational patterns are preserved (e.g., translation models like TransE or tensor/matrix-based semantic matching models).",
            "level_of_analysis": "functional",
            "supporting_evidence": "Cited success of KGE methods in encoding structured relational knowledge and enabling downstream tasks (link prediction, reasoning); paper uses KG embeddings as the symbolic level in dual representations and as a bridge in scene-graph / commonsense-graph integration (GB-Net).",
            "counter_evidence_or_challenges": "Some KGE methods that treat triples independently neglect global intrinsic associations and deeper semantics; earlier translation-based and semantic-matching models have limits handling multi-relational, multi-hop, or contextualized knowledge without graph-aware neural extensions.",
            "comparison_to_other_theories": "Contrasted with distributional linguistic embeddings: KGEs aim to capture explicit relational/graph structure rather than purely distributional co-occurrence statistics; the paper proposes combining KGEs (symbolic) with embodied modality-specific embeddings.",
            "notable_limitations_or_open_questions": "How to ground purely symbolic KG nodes to perceptual data (symbol grounding), how to incorporate multimodal attributes, and how to scale KGE methods to richer commonsense and multimodal knowledge remain active questions.",
            "uuid": "e3476.8",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dual Coding of Knowledge in the Human Brain",
            "rating": 2,
            "sanitized_title": "dual_coding_of_knowledge_in_the_human_brain"
        },
        {
            "paper_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "rating": 2,
            "sanitized_title": "linguistic_distributional_knowledge_and_sensorimotor_grounding_both_contribute_to_semantic_category_production"
        },
        {
            "paper_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "rating": 2,
            "sanitized_title": "varieties_of_abstract_concepts_and_their_grounding_in_perception_or_action"
        },
        {
            "paper_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "rating": 2,
            "sanitized_title": "visual_and_affective_multimodal_models_of_word_meaning_in_language_and_mind"
        },
        {
            "paper_title": "Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning",
            "rating": 2,
            "sanitized_title": "exemplarbased_contrastive_selfsupervised_learning_with_fewshot_class_incremental_learning"
        },
        {
            "paper_title": "Knowledge Graph Embeddings and Explainable AI",
            "rating": 2,
            "sanitized_title": "knowledge_graph_embeddings_and_explainable_ai"
        },
        {
            "paper_title": "Bridging Knowledge Graphs to Generate Scene Graphs",
            "rating": 2,
            "sanitized_title": "bridging_knowledge_graphs_to_generate_scene_graphs"
        },
        {
            "paper_title": "Knowledge Aware Semantic Concept Expansion for Image-Text Matching",
            "rating": 1,
            "sanitized_title": "knowledge_aware_semantic_concept_expansion_for_imagetext_matching"
        }
    ],
    "cost": 0.0141065,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dual Embodied-Symbolic Concept Representations for Deep Learning</p>
<p>Daniel T Chang dtchang43@gmail.com 
Dual Embodied-Symbolic Concept Representations for Deep Learning
C9D883FB721C375593C778749DBE936F
Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs.Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching.Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
<p>Introduction</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations [1][2]: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concepts in the form of concept graphs.For concrete concepts, the two levels are associated / connected.The embodied level corresponds to sensorimotor-derived knowledge representations of the dual-coding framework [4]; the symbolic level corresponds to language-derived knowledge representations of that framework.</p>
<p>Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.</p>
<p>Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.Traditionally, they are learned separately and used independently.For example, deep learning for computer vision learns embodied concept representations in the form of visual feature vectors for image classification, whereas deep learning for natural language processing learns symbolic concept representations in the form of semantic word vectors for sentiment analysis.The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5][6][7].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system.Further, they typically interact to drive conceptual processing.</p>
<p>As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for fewshot class incremental learning (CIL) and embodied-symbolic fused representation for image-text matching.The first use case [16] demonstrates that embodied-symbolic knowledge distillation mitigates both the catastrophic forgetting problem for CIL and the overfitting problem for few-shot learning.The second use case [17] demonstrates that embodied-symbolic fused representation closes the semantic gap between images and text leading to improved performance when matching an image with text.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.The first example [19] presents a unified formulation of scene graphs (SGs) and commonsense knowledge graphs (CSKGs), where a SG is seen as an image-conditioned embodiment of a CSKG.The second example [21] grounds symbols in knowledge graphs (KGs) to corresponding image, text, sound and video data and maps symbols to their corresponding referents with meanings in the physical world, which is a key step towards the realization of human-level AI.</p>
<p>Human Conceptual System: Symbolic and Embodied</p>
<p>The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing.</p>
<p>In the following we discuss three recent findings in cognitive science that demonstrate the crucial role of dual embodiedsymbolic concept representations in human cognition.</p>
<p>Category Production</p>
<p>A common way of testing how concepts are structured and accessed from long-term memory is with a category production task, whereby a participant is presented with a category name such as 'animal', and asked to name concepts belonging to that category.In a category production study [5], measures of sensorimotor similarity between a category and member concept (based on an 11-dimensional representation of sensorimotor strength) and linguistic proximity between the category name and member-concept name (based on word co-occurrence derived from a large corpus) are used to test category production performance.Both measures predict the order and frequency of category production, but linguistic proximity has an effect above and beyond sensorimotor similarity.</p>
<p>Sensorimotor and linguistic distributional information, therefore, are found to offer an explanation to the mechanisms driving responses in category production tasks [5].In terms of linguistic distributional information (i.e., symbolic information), it is evident that the shared linguistic contexts between member-concept names and category name (e.g., between cat / dog and 'animal') in corpus-derived linguistic space is an effective predictor of category membership.In terms of sensorimotor information (i.e., embodied information), as suggested by many theories of conceptual structure, categorical distinctions emerge from common features (e.g., fur, four-legged) of member concepts that we perceive when interacting with the world.Furthermore, sensorimotor and linguistic distributional information typically interact to drive conceptual processing, and the linguistic shortcut hypothesis is the process by which people arrive at the most-frequently-named and first-named member concepts of a category.</p>
<p>Grounding of Concrete and Abstract Concepts</p>
<p>Concrete concepts (e.g., animal) have perceivable referents.In general, their semantic content can be clearly characterized, and their conceptual taxonomies can be unequivocally defined.Evidence obtained with concrete concepts [6] suggests interplay between modality-specific, multimodal and amodal semantic hub regions.Modality-specific and multimodal regions represent conceptual features, whereas amodal semantic hubs code conceptual information in an overarching supramodal fashion.The available evidence is most consistent with hybrid models of conceptual representations combining modality-specific and multimodal circuits with amodal conceptual hubs, i.e., combining embodied and symbolic representations.</p>
<p>Abstract concepts (e.g., justice) have referents which cannot be directly perceived, like mental or emotional states, abstract ideas, social constellations and scientific theories.Their semantic content is highly variable across individuals and contexts.As such, abstract concepts are a particular challenge for embodied / grounded cognition theories [6] because at the first glance, it is hard to imagine how abstract concepts, without a referent which can be perceived or acted upon, could be grounded in the sensorimotor brain systems.Embodied / grounded cognition theories, however, have been refined [6] in order to account for the representation of abstract concepts.Besides sensorimotor information due to metaphoric mapping or due to relations to classes of situations, the relevance of emotional, introspective, social and linguistic information has been stressed.</p>
<p>As is the case with concrete concepts, the findings with regard to abstract concepts [6] can be accommodated best by hybrid models of conceptual representations assuming an interaction between modality-specific, multimodal and amodal hub areas.Modality-specific systems include the sensorimotor systems, but also the modal systems involved in the processing of emotions, introspections, mentalizing, social constellations, and language.The latter modal systems are probably more important for abstract than for concrete concepts.</p>
<p>Meaning in Language and Mind</p>
<p>There are different theories on how much linguistic and sensorimotor representations contribute to meaning [7].In embodied theories, meaning is based on the relation between words and internal bodily states corresponding to multiple modalities, such as vision, olfaction, and perhaps even internal affective states.By contrast, symbolic theories suggest that the meaning of word (e.g., rose) can be derived in a recursive fashion by considering its relation to the meaning of words in its linguistic context (e.g., red, flower).Current theories of semantics tend to posit that both symbolic and embodied information contribute to meaning.</p>
<p>A study [7] is made to evaluate how well different kinds of models account for people's representations of both concrete and abstract concepts.The models include unimodal linguistic models as well as multimodal models which combine linguistic with perceptual or affective information.There are two types of linguistic models considered: the distributional linguistic model which derives word meaning from word co-occurrences derived from large language corpora, and the word association model which measures the meaning of a word as a distribution of respectively weighted associative links encoded in a large semantic network.The study demonstrates that both visual and affective multimodal models better capture behavior that reflects human representations, especially for basic-level concepts that belong to the same superordinate category.The conclusion is that multimodal information (i.e., symbolic and embodied information) is important for capturing both abstract and concrete concepts.</p>
<p>Symbolic Concept Representations</p>
<p>Embodied concept representations (i.e., modality-specific concept-oriented feature representations) have been discussed previously [1][2][3].In particular, we recommend their learning using exemplar-based contrastive self-supervised learning (CSSL) [3] since it is concept (class) centric and it supports class incremental learning.</p>
<p>In the following we discuss symbolic concept representations (i.e., amodal, language-specific concept graphs) since they are an integral part of dual embodied-symbolic concept representations, but we have not discussed them previously.They come in two forms: word embedding for representing concepts (namely, word semantics in natural language corpora or texts), and knowledge graph embedding for representing concept graphs (a.k.a.knowledge graphs), i.e., conceptual knowledge (e.g.commonsense knowledge).</p>
<p>Note that we will not discuss either word embedding learning or knowledge-graph embedding learning since they are of secondly importance to the focus of this paper.They are discussed in [9][10] and [12][13][14] respectively.</p>
<p>Word Embedding</p>
<p>Word embeddings [8][9] are dense representations of words in semantic vector spaces generated from language corpora or texts, in which semantically similar words have similar embedding vectors.Word embeddings have played an important role for tasks of natural language processing including complex and pertinent ones such as information retrieval and sentiment analysis.They are efficient to learn, highly scalable for large corpora (thousands of millions of words).</p>
<p>Two most widely used word embedding models [8][9][10] are Word2Vec and GloVe.Word2Vec employs two model architectures: Continuous Bag-of-Words (CBOW) model which aims to predict the occurrence of a word given other words that constitute its context, and Skip Gram (SG) model which deals with predicting a context given the word.Word2Vec considers only local word co-occurrences.GloVe is based on a model that reduces the dimensionality of a global cooccurrence matrix of the word-word type in a corpus, with the statistics of the entire corpus captured directly by the model.</p>
<p>GloVe focuses on global word co-occurrences.</p>
<p>For deep-learning based natural language processing [10] word embedding is the basic building block that maps words in the input sentences into continuous space vectors, and usually used (pretrained) in the first layer of a neural network.</p>
<p>Based on the word embedding, complex networks such as recurrent neural networks (RNNs) can be used for feature extraction and build, for example, context-aware word embedding or phrase / sentence embedding.</p>
<p>Note that the methods that generate word embedding based purely on information in a language corpus or text fail to take advantage of the semantic relational structure that exits between words in concurrent contexts.To overcome this limitation, the corpus or text is enhanced with extra morphological, syntactic, semantic and domain knowledge from knowledge sources (e.g., Wikipedia, Wordnet) to generate knowledge-aware word embedding [11].</p>
<p>Knowledge Graph Embedding</p>
<p>The knowledge graph (KG) [12][13][14][15] (a.k.a.concept graph) is a representation of conceptual knowledge (specifically, structured relational information) in the form of concepts and relations between them.We can view a KG as a set of statements (facts) having the form of subject-predicate-object triples, using the notation (h, r, t) (head, relation, tail) to identify a statement.We can also view a KG as a directed labeled graph, where nodes represent concepts and edges represent relations between concepts.</p>
<p>Knowledge graph embedding (KGE) [12][13][14][15] is a widely adopted approach to KG representation in which concepts and relations are embedded in low-dimensional continuous vector spaces.Most methods create a vector for each concept and each relation.These embeddings are generated in a way to capture latent properties of the semantics in the KG: similar concepts and similar relations will be represented with similar vectors.KGE offers precise, effective and structural representation of symbolic conceptual information.</p>
<p>Most of the KGE approaches rely mainly on the use of the subject-predicate-object triples present in the KG to generate the vector representations, (h, r, t), for (head, relation, tail) [12][13][14][15].These approaches can be broadly classified into two groups: translation-based models and semantic matching models.Translation-based models (e.g., TransE) are based on learning the translations from the head concept to the tail concept.They use distance-based measures to generate the similarity score for a pair of concepts and their relations.The training objective is to achieve, mathematically, h + r ~ t.</p>
<p>Semantic matching models (e.g., RESCAL) use a multiplicative approach and represent the relations as matrices / tensors in the vector space.For example, RESCAL relies on a tensor factorization approach upon the 3-dimensional tensor generated by considering subject-predicate-object as the 3 dimensions of the tensor.Note that these models only consider each individual fact, while their intrinsic associations are neglected, which is not sufficient for capturing deeper semantics for better embedding.They, therefore, cannot meet the requirements of KGE.</p>
<p>New approaches leverage the graph nature of KG, and use neural network (NN) based models for various tasks [14][15].</p>
<p>When treated as a graph, KG can be seen as a heterogeneous graph, with the logical relations of more importance than the graph structure.NN-based models can consider the type of concept or relation, neighborhood / substructure information, path information, and temporal information.The use of convolutional neural networks (CNNs) or attention mechanisms also helps to generate better embeddings.Examples of CNN-based models include ConvE and ConvKB [14].</p>
<p>Graph neural networks (GNNs) are neural networks that can be directly applied to graphs, and provide an easy way to generate node-level, edge-level, and graph-level embeddings.They have a somewhat universal architecture in common, referred to as Graph Convolutional Networks (GCNs) which use deep, multi-layer processing known as message passing.</p>
<p>Examples of GCN-based models include RGCN and SACN [14].</p>
<p>GCNs have a strong ability to mine the underlying semantics of KGs [14].In general, GCN-based models can incorporate additional information, such as node types, relation types, node attributes and substructures, to generate better embeddings.For example, if the node information of a multi-hop domain can be aggregated, the accuracy of the model in specific tasks can be greatly improved.A knowledge distillation approach suitable for few-shot CIL is proposed in [16], which is based on the use of dual To mitigate the overfitting problem, multiple visual embeddings for classes are generated, where each is designed specifically for a group of classes.The semantic word vectors are used to separate classes into several groups.The number of groups / visual embeddings is defined by the superclass (cluster) knowledge obtained from the semantic word vector space.</p>
<p>In the semantic space, there is a semantic word vector for each class.The set of superclasses is attained from the semantic word vector space representations of the base classes, and are then held fixed.</p>
<p>For the first (base) task, involving base classes, the steps for obtaining groups / visual embeddings are:</p>
<p> Train the network backbone on the base classes, which is then kept frozen.</p>
<p> Apply k-means clustering, where k = N, on base semantic word vectors and assign a superclass (cluster) label to each base class.</p>
<p> Train N embeddings on the base task using superclass labels as group identity.</p>
<p>For other subsequent (novel) task, involving novel classes, the cluster centers (obtained in the base task) are used to assign superclass labels to novel classes.To assign a superclass label to novel classes, the minimum Euclidean distance between the semantic word vector of a novel class and cluster centers is used.Hence, given a novel class, there is a selection of groups / visual embeddings that each may be more or less suited.</p>
<p>An attention module is used to merge multiple visual embeddings of a class to generate its final visual embedding, i.e., visual vector.A mapping module is then used to project the visual vector from the visual space into the semantic space to align the visual vector with its associated semantic word vector.For the first (base) task, training involves attention loss and classification loss; for other subsequent (novel) task, training involves attention loss, distillation loss (i.e., alignment loss) and classification loss.</p>
<p>Embodied-Symbolic Fused Representation for Image-Text Matching</p>
<p>Image and text matching [17] is an important vision-language cross-modality task for many applications including image retrieval and caption.Before calculating the similarity between an image and text, a matching model needs to obtain a rich representation of the image (and text) first.Most of the current image-text matching models utilize pre-trained neural networks to extract feature embeddings as the representation of images.The image embeddings, however, fail to extract highlevel semantic information.So the semantic gap between images and text leads to limited performance when matching an image with text.</p>
<p>Learning semantic concepts is useful to enhance image representation and can significantly improve the performance of both image-to-text and text-to-image retrieval.Frequently co-occurred concepts in the same image (scene), e.g.bedroom and bed, can provide commonsense knowledge to discover other semantic-related concepts.[17] uses a Scene Concept Graph (SCG) to support this by aggregating image scene graphs and extracting frequently co-occurred concepts as commonsense knowledge.Moreover, it proposes a novel model to incorporate this knowledge to improve image-text matching.Specifically, semantic concepts are detected from images and then expanded by the SCG to include commonly-related concepts (which may be occluded or long-tailed).Afterwards, it fuses their representations with the image embeddings, as semantic-enhanced image embeddings, to use (with text embeddings) for image-text matching.</p>
<p>The model uses dual embodied-symbolic concept representations.The visual embodied representation exists in the form of image embeddings.There are two symbolic representations.The text symbolic representation exists in the form of (context-aware) word embeddings.The concept symbolic (semantic) representation for images exists in the form of concept embeddings.Finally, there is an embodied-symbolic (image-concept) fused representation existing in the form of conceptenhanced image embeddings.</p>
<p>The scene graph [17] of an image is a graph consisting of concepts and relations between them.It can be represented as a set of triples of <subject, relation, object>.The SCG is constructed from scene graphs by aggregating co-occurred <subject, object> pairs from scene graphs of all images.To generate the concept embeddings for an image, first, a concept detection module (a multi-label image classification model) is used to extract semantic concepts from the image on a small concept vocabulary.With the SCG in hand, a concept expansion module is then used to expand the semantic concepts to include commonly-related concepts.Finally, a concept prediction module is used to predict relevant concepts from these and generate the concept embeddings.The concept embeddings of an image are fused with its visual embedding, by the imageconcept fusion module, to generate a concept-enhanced image representation.</p>
<p>The image-text matching problem is formulated as a ranking model.Given the input image and the text, the output is the similarity score of matching their respective visual and language representations, with the visual representation being an image-concept (embodied-symbolic) fused representation and the language representation being word embeddings.To learn image and text matching as well as image-relevant semantic concepts jointly in an end-to-end fashion, the loss function consists of two parts: image-text matching loss and concept prediction loss.</p>
<p>Deep Learning and Symbolic AI Integration</p>
<p>Symbolic AI and deep learning both have strength and weakness, which tend to be each other's opposites.A significant challenge today [18] is to effect a reconciliation.Symbolic AI is based on manipulation of abstract compositional representations whose elements stand for concepts and relations.Therefore, to facilitate reconciliation, a key objective for deep learning is to develop architectures capable of discovering concepts and relations in raw data, and learning how to represent them.</p>
<p>An excellent example for doing this for image data has been discussed in 5 Embodied-Symbolic Fused Representation for Image-Text Matching.(Note that only co-occurred concepts are considered in [17].However, their relations will also be considered in future work.)As discussed there, dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.</p>
<p>In the following, we discuss two additional important examples of deep learning and symbolic AI integration.</p>
<p>Scene Graph Generation with Knowledge Graph Bridging</p>
<p>Scene graphs (SGs) are powerful representations that extract semantic concepts and their relations from images, which facilitate visual comprehension and reasoning.(For an example usage, see 5 Embodied-Symbolic Fused Representation for Image-Text Matching.)A SG can be represented as a set of triples of <subject, relation, object>.On the other hand, commonsense knowledge graphs (CSKGs) [20] are rich repositories that encode how the world is structured (i.e., commonsense knowledge), and how common concepts are related and interact.</p>
<p>GB-Net (Graph Bridging Network) [19] presents a unified formulation of these two constructs, where a SG is seen as an image-conditioned embodiment of a CSKG.Based on this perspective, the SG generation is formulated as the inference of a bridge between the SG and CSKG, where each concept or relation in the SG must be linked to its corresponding concept or relation in the CSKG.Specifically, both SG and CSKG are defined as special types of knowledge graph (KG): GB-Net fuses the SG embedding and CSKG embedding through a dynamic message passing and bridging algorithm using a graph neural networks (GNN).The method iteratively propagates messages to update nodes, then compares nodes to update bridge edges, and repeats until the two graphs are carefully connected.This results in the SG embedding with bridges to the CSKG embedding.</p>
<p>Multimodal Knowledge Graphs</p>
<p>Knowledge graphs (KGs) have found great use in a wide range of applications including text understanding, recommendation system, natural language question answering, and image understanding (see 5 Embodied-Symbolic Fused Representation for Image-Text Matching and 6.1 Scene Graph Generation with Knowledge Graph Bridging).More and more KGs have been created, covering common sense knowledge [20] (see 6.1 Scene Graph Generation with Knowledge Graph Bridging), lexical knowledge, encyclopedia knowledge, taxonomic knowledge, and geographic knowledge.</p>
<p>Most of the existing KGs are represented with pure symbols, denoted in the form of text, without grounding to the physical world experience.However, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing (see 2 Human Conceptual System: Symbolic and Embodied).Therefore, it is necessary to ground symbols in KGs to corresponding image, text, sound and video data and map symbols to their corresponding referents with meanings in the physical world.That is, the multi-modalization</p>
<p>Conclusion</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs (a.k.a.knowledge graphs).The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.That is, deep learning should learn from data not only modality-specific embodied representations such as image embeddings, text embeddings, etc., but also the corresponding amodal symbolic (semantic) representation as knowledge graph embeddings, with links to commonsense knowledge graphs.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration, which is an important direction for deep learning and AI since their integration reinforces each other's strength, compensates each other's weakness, and takes a major step toward human-level AI (e.g., grounding in experience / data, understanding, reasoning, explanation, etc.).</p>
<p>4</p>
<p>Embodied-Symbolic Knowledge Distillation for Few-Shot CIL In CIL [2], a model learns tasks continually, with each task containing a batch of new classes.In few-shot CIL [3], only the training set of the first (base) task may have large-scale training data for base classes, while other subsequent (novel) task just contains few-shot instances for novel classes.Few-shot CIL requires transferring knowledge (i.e., knowledge distillation) from old classes to new classes in solving the catastrophic forgetting problem, which is generic to CIL.The additional challenge of few-shot learning is the overfitting problem.</p>
<p>embodied-symbolic concept (class) representations.The embodied (visual) representation exists in the form of visual vectors.Semantic word vectors (Word2Vec and GloVe) are used as the symbolic (semantic) representation to facilitate knowledge distillation, which are generated from unsupervised learning on an unannotated text corpus.The semantically guided network does not add new parameters while adding new classes incrementally.The knowledge distillation process only includes semantic word vectors of novel classes in addition to the base classes, which are used to help the network remembering base class training, generalizing to novel classes, and generating well-separated embodied representation (visual vectors) of classes.Note that though the network may not have had the opportunity to see instances of novel classes, nevertheless, novel classes may very well share semantic properties with base classes it has seen.For example, if hyena is a novel class, many typical hyena attributes like 'face', 'body', etc., may have been seen by the network from base classes.</p>
<p></p>
<p>A KG is a set of nodes of type concept (C) or relation (R), and a set of directed, weighted edges (Ε) between the nodes. A CSKG is a type of KG with commonsense concept (CC) nodes and commonsense relation (CR) nodes.Commonsense edges are of four types: CC-&gt;CC, CC-&gt;CR, CR-&gt;CC, and CR-&gt;CR. A SG is a type of KG with scene concept (SC) nodes and scene relation (SR) nodes.Scene edges are of four types: SC-&gt;SR (subjectOf), SC-&gt;SR (objectOf), SR-&gt;SC (hasSubject), and SR-&gt;SC (hasObject). The SG and CSKG are connected through four types of bridge edges: SC-&gt;CC, SR-&gt;CR, CC-&gt;SC and CR-&gt;SR.GB-Net uses dual embodied-symbolic concept representations.The visual embodied representation for images exists in the form of image embeddings.There are two symbolic concept representations.The CSKG symbolic representation exists in the form of CSKG embedding (of commonsense nodes and edges).The SG symbolic (semantic) representation for images exists in the form of SG embedding (of scene nodes and edges).The CSKG embedding and the SG embedding are interconnected through the bridge edges.</p>
<p>Acknowledgement: Thanks to my wife Hedy (郑期芳) for her support.of KGs[21]is an inevitable key step towards the realization of human-level AI, which results in Multimodal Knowledge Graphs (MMKGs)[12,15,21].To support symbol grounding in MMKG[21], one can take multimodal data as particular attribute values of concepts or relations.This can be denoted in a triple (s, r, d), where s denotes a concept or relation, d denotes one of its corresponding multimodal data, and the relation r is, e.g., "hasImage" when d is an image.Symbol grounding, therefore, can be divided into concept grounding and relation grounding.As an example, concept grounding aims to find representative, discriminative and diverse images for visual concepts.A major challenge is to find representative images for a visual concept from a group of relevant images.The representativeness and discriminativeness of images can be scored in terms of results of cluster-based methods, such as K-means, based on visual embeddings.The captions of images can also be utilized to evaluate the representativeness and discriminativeness of images, at the semantic level, based on text embeddings.IKRL and DKRL are two well-known examples of MMKG. IKRL (Image-embodied Knowledge RepresentationLearning)[12,15]provides a method to integrate images inside the scoring function of the KG embedding model (TransE).Essentially, IKRL uses multiple images for each concept and use the AlexNet CNN to generate embeddings for the images.These embeddings are then selected and combined with the use of attention to be finally projected in the KG embedding space.DKRL (Description-Embodied Knowledge Representation Learning)[12,15], on the other hand, includes the description of concepts in the representation.It uses a CNN to encode the concept description into a vector representation and uses this representation in the loss function.DKRL learns two embeddings for each concept, one that is structure-based (i.e., KG, like TransE) and one that is based on the concept descriptions.The two embeddings are interconnected / integrated.The above discussion, though brief, shows that MMKG uses dual embodied-symbolic concept representations.The KG symbolic representation exists in the form of KG embedding.There are various embodied representations, depending on the multimodal data involved.For image data, the visual embodied representation exists in the form of image embedding.For text (descriptions), the textual embodied representation exists in the form of text embedding.The KG embedding and the modality-specific embedding(s) are interconnected / integrated.
Concept-Oriented Deep Learning. T Daniel, Chang, arXiv:1806.017562018arXiv preprint</p>
<p>Concept Representation Learning with Contrastive Self-Supervised Learning. T Daniel, Chang, arXiv:2112.056772021arXiv preprint</p>
<p>Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning. T Daniel, Chang, arXiv:2202.026012022arXiv preprint</p>
<p>Dual Coding of Knowledge in the Human Brain. Y Bi, Trends in Cognitive Sciences. 2510October 2021</p>
<p>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production. B Banks, C Wingfield, L Connell, Cogn. Sci. 45e130552021</p>
<p>Varieties of Abstract Concepts and Their Grounding in Perception or Action. M Kiefer, M Harpaintner, Open Psychol. 20202</p>
<p>Visual and Affective Multimodal Models of Word Meaning in Language and Mind. S D Deyne, D Navarro, G Collell, A Perfors, Cogn, Sci. 45e129222021</p>
<p>A Systematic Literature Review on Word Embeddings. L Gutiérrez, B Keith, Proc. Int. Conf. Softw. Process Improvement. Int. Conf. Softw. ess Improvement2018</p>
<p>U Naseem, I Razzak, S K Khan, M Prasad, arXiv:2010.15036A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-art Word Representation Language Models. 2020</p>
<p>M Zhou, N Duan, S Liu, H.-Y Shum, Progress in Neural NLP: Modeling, Learning, and Reasoning," in Engineering. 20206</p>
<p>Incorporating Extra Knowledge to Enhance Word Embedding. A Roy, S Pan, Proceedings of the 29th International Joint Conference on Artificial Intelligence. the 29th International Joint Conference on Artificial Intelligence202020</p>
<p>Knowledge Graph Embeddings and Explainable AI. Federico Bianchi, Gaetano Rossiello, Luca Costabello, Matteo Palmonari, Pasquale Minervini, arXiv:2004.148432020arXiv preprint</p>
<p>A Survey of Knowledge Graph Embedding and Their Applications. Shivani Choudhary, Tarun Luthra, Ashima Mittal, Rajat Singh, arXiv:2107.078422021arXiv preprint</p>
<p>A Survey on Knowledge Graph Embeddings for Link Prediction. Meihong Wang, Linling Qiu, Xiaoli Wang, Symmetry. 202113485</p>
<p>Application and Evaluation of Knowledge Graph Embeddings in Biomedical Data. Mona Alshahrani, Maha A Thafar, Magbubah Essack, PeerJ Computer Science. 72021</p>
<p>Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning. Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, Mehrtash Harandi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2021</p>
<p>Knowledge Aware Semantic Concept Expansion for Image-Text Matching. B Shi, L Ji, P Lu, Z Niu, N Duan, IJCAI. 2019</p>
<p>Reconciling Deep Learning with Symbolic Artificial Intelligence: Representing Objects and Relations. Marta Garnelo, Murray Shanahan, Current Opinion in Behavioral Sciences. 292019</p>
<p>Bridging Knowledge Graphs to Generate Scene Graphs. A Zareian, S Karaman, S F Chang, arXiv:2001.023142020arXiv preprint</p>
<p>CSKG: The Commonsense Knowledge Graph. F Ilievski, P Szekely, B Zhang, Extended Semantic Web Conference (ESWC). 2020</p>
<p>Multi-Modal Knowledge Graph Construction and Application: A Survey. Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, Nicholas Jing Yuan, arXiv:2202.057862022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
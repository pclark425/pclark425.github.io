<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5663 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5663</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5663</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-265609730</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.01032" target="_blank">Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Designing high-quality educational questions is a challenging and time-consuming task. In this work, we propose a novel approach that utilizes prompt-based techniques to generate descriptive and reasoning-based questions. However, current question-answering (QA) datasets are inadequate for conducting our experiments on prompt-based question generation (QG) in an educational setting. Therefore, we curate a new QG dataset called EduProbe for school-level subjects, by leveraging the rich content of NCERT textbooks. We carefully annotate this dataset as quadruples of 1) Context: a segment upon which the question is formed; 2) Long Prompt: a long textual cue for the question (i.e., a longer sequence of words or phrases, covering the main theme of the context); 3) Short Prompt: a short textual cue for the question (i.e., a condensed representation of the key information or focus of the context); 4) Question: a deep question that aligns with the context and is coherent with the prompts. We investigate several prompt-based QG methods by fine-tuning pre-trained transformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and BART. Moreover, we explore the performance of two general-purpose pre-trained LLMs such as Text-Davinci-003 and GPT-3.5-Turbo without any further training. By performing automatic evaluation, we show that T5 (with long prompt) outperforms all other models, but still falls short of the human baseline. Under human evaluation criteria, TextDavinci-003 usually shows better results than other models under various prompt settings. Even in the case of human evaluation criteria, QG models mostly fall short of the human baseline. Our code and dataset are available at: https://github.com/my625/PromptQG</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5663.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5663.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Davinci (Text-Davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Davinci-003 (OpenAI GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose pre-trained LLM by OpenAI used zero-shot with explicit instruction prompts; evaluated under three prompt formats (long prompt, short prompt, without prompt) for educational question generation on EduProbe.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text-Davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question Generation (EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate deep, school-level questions from a context (text segment) optionally guided by a long or short prompt (hints/phrases) using the EduProbe NCERT-derived dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Three prompt formats passed as input strings to the LLM: (a) With Long Prompt: "Given the context <Context> and the long prompt <Long Prompt>, generate a Question"; (b) With Short Prompt: "Given the context <Context> and the short prompt <Short Prompt>, generate a Question"; (c) Without Prompt: "Given the context <Context>, generate a Question".</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Long prompt vs Short prompt vs Without prompt (the paper compares all three formats for each model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics (selected): ROUGE-2 F1: long 0.491, short 0.319, without 0.327; ROUGE-L F1: long 0.603, short 0.529, without 0.506; METEOR long 0.443, short 0.313, without 0.283. Human evaluation (summary): Davinci scored highest among models on grammaticality, appropriateness, relevance, complexity and novelty across prompt settings and often approached but did not surpass human baseline except it matched/beat humans on some complexity/novelty criteria (see paper narrative).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Davinci shows substantially higher automatic scores in the long-prompt setting than short/none (ROUGE-2 F1: +0.172 long vs short; ROUGE-2 F1: +0.164 long vs without). On human judgments Davinci outperforms other models in most criteria across formats; in novelty (long prompt) and complexity (without prompt) Davinci (and ChatGPT) exceeded the human baseline according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ROUGE-2 F1: +0.172 absolute (0.491 vs 0.319) long vs short; ROUGE-L F1: +0.074 absolute (0.603 vs 0.529) long vs short. Human-eval direction: long prompt improved grammaticality/appropriateness/relevance relative to short/none per aggregated human scores (precise numeric human scores are in paper Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>long prompt improved automated overlap metrics and generally improved human-judged grammaticality/appropriateness/relevance; without prompt sometimes produced more complex/novel questions according to human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper hypothesizes that providing explicit cues (long prompts) guides the model to focus on salient concepts in the context, producing outputs closer to the gold-standard phrasing (hence higher automatic overlap metrics). Conversely, no-prompt outputs from Davinci can be more creative/novel and require higher cognitive effort (higher complexity), which human raters rewarded despite lower automatic overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Although long prompts improved automatic metrics, Davinci (and ChatGPT) generated the most complex and novel questions under the without-prompt setting in some cases — i.e., no prompt sometimes increased complexity/novelty even while reducing automatic overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5663.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5663.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose conversational LLM (GPT-3.5-Turbo) evaluated zero-shot with the same three prompt templates for question generation on EduProbe; compared to fine-tuned models via automatic and human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>154B (approx. as reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question Generation (EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same EduProbe QG task: generate educational questions from a context with optional long or short prompt hints.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same three input templates: With Long Prompt, With Short Prompt, Without Prompt; prompt text examples provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Long prompt vs Short prompt vs Without prompt (direct comparison in both automatic and human evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics (selected): ROUGE-2 F1: long 0.476, short 0.304, without 0.319; ROUGE-L F1: long 0.592, short 0.522, without 0.492; METEOR long 0.423, short 0.309, without 0.266. Human evaluation (summary): ChatGPT rated second-best after Davinci across human criteria and in some settings produced complexity above human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Automatic metrics are higher in the long-prompt setting (ROUGE-2 F1 +0.172 long vs short). Human metrics: ChatGPT improved with prompts for grammaticality/appropriateness/relevance; without prompt there was an increase in complexity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ROUGE-2 F1: +0.172 absolute (0.476 vs 0.304) long vs short; ROUGE-L F1: +0.070 absolute (0.592 vs 0.522) long vs short.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>long prompt improved automatic overlap and many human-judged quality metrics; absence of prompt sometimes increased perceived complexity/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue prompts act as guidance that focuses generation and increases similarity to reference questions (hence better automatic scores), while unconstrained generation (no prompt) allows more divergent, novel or complex questions favored by human raters for complexity/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Despite higher automatic scores with long prompts, ChatGPT sometimes produced lower novelty than long-prompt Davinci; also automatic metrics penalize divergence even when human raters prefer outputs (i.e., format improvements in human judgment don't always align with automatic metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5663.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5663.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-large (fine-tuned on EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer encoder-decoder (T5-large) fine-tuned on EduProbe under three input formatting choices; produced top automatic-metric performance, especially with long prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question Generation (EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tuned seq2seq QG model producing questions from Context + optional Prompt (long/short) matching EduProbe gold questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>During fine-tuning/testing the input was formatted as: With Long Prompt: [CLS] Context [SEP] Long Prompt [SEP] -> Question; With Short Prompt: [CLS] Context [SEP] Short Prompt [SEP] -> Question; Without Prompt: [CLS] Context [SEP] -> Question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Long prompt vs Short prompt vs Without prompt (comparison of automated metrics primarily; human eval also performed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics: With long prompt T5 achieved ROUGE-2 F1 0.575, ROUGE-L F1 0.668, METEOR 0.503, CHrF 74.40%, BLEU 42.97%, BERTScore 0.818; With short prompt ROUGE-2 F1 0.368, ROUGE-L F1 0.542; Without prompt ROUGE-2 F1 0.368, ROUGE-L F1 0.539. Human evaluation: T5 scored well on automated-aligned criteria but was behind Davinci/ChatGPT on human judgments of novelty/complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>T5 shows large gains in automated metrics with long prompt vs short/none: ROUGE-2 F1 increase +0.207 (0.575 vs 0.368) long vs short; ROUGE-L F1 +0.126 (0.668 vs 0.542) long vs short. Little difference between short and without prompt on automated metrics (short and without have nearly equal ROUGE-2 F1 0.368).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ROUGE-2 F1: +0.207 absolute long vs short; BERTScore: 0.818 long vs 0.742 short (+0.076).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Long prompt substantially improved automated overlap metrics for T5; short vs no-prompt had minimal difference for T5 on those automated measures.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Fine-tuned models like T5 learn to replicate the style/phrasing of the gold questions; introducing a long prompt during training/test helps the model attend to key theme phrases and produce outputs closer to references, increasing automatic metric scores. The similarity-oriented automated metrics favor the fine-tuned phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Although long prompts strongly improve automatic metrics, T5 still lags behind human baseline on human-judged appropriateness/relevance/novelty; short vs without prompt produced similar automated results, suggesting short prompts (very brief cues) may be insufficient to change model outputs for fine-tuned T5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5663.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5663.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART-large (fine-tuned on EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A denoising seq2seq transformer fine-tuned on EduProbe; performed strongly in many automated metrics and showed sensitivity to prompt format, particularly favoring long prompts for automated overlap and short prompts for some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>406M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question Generation (EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tuned model mapping Context (+ optional Prompt) to school-level questions; evaluated under long, short, and no-prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same formatting as other fine-tuned models: Context with appended Long Prompt or Short Prompt, or Context alone for without-prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Long prompt vs Short prompt vs Without prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics: With long prompt ROUGE-2 F1 0.573, ROUGE-L F1 0.666, METEOR 0.443, CHrF 73.72%, BLEU 36.47%, BERTScore 0.809. With short prompt ROUGE-2 F1 0.377, ROUGE-L F1 0.549, METEOR 0.346, CHrF 59.52%, BLEU 21.82%, BERTScore 0.756. Without prompt ROUGE-2 F1 0.341, ROUGE-L F1 0.516, METEOR 0.319.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BART's automated scores were much higher with long prompts than with short or no prompt (ROUGE-2 F1 +0.196 long vs short; ROUGE-L F1 +0.117 long vs short). Short prompt outperformed no prompt modestly in some metrics (e.g., ROUGE-2 F1 0.377 vs 0.341).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ROUGE-2 F1: +0.196 absolute (0.573 vs 0.377) long vs short; METEOR +0.097 (0.443 vs 0.346) long vs short.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Long prompt improved automated overlap metrics substantially; short prompt gave intermediate performance; without-prompt the weakest on automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Like other fine-tuned models, BART benefits from explicit long prompts that emphasize relevant phrases; this reduces model uncertainty and produces outputs more similar to reference questions. The authors note automated metrics reward closeness to gold, which fine-tuned models achieve better when guided by longer prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Despite higher automated performance with long prompts, human evaluators preferred some outputs from general-purpose LLMs for novelty/complexity; thus BART's long-prompt gains in automatic metrics did not translate to top human-judged novelty/complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5663.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5663.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pegasus (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pegasus-large (fine-tuned on EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A summarization-focused encoder-decoder transformer (Pegasus) fine-tuned for QG on EduProbe and evaluated under long/short/none prompt formats; shows prompt sensitivity but overall lower automated scores than T5/BART.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pegasus-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>568M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question Generation (EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transform context (+ optional prompt) into a question; Pegasus is repurposed from summarization to QG and fine-tuned on EduProbe.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same three formats (Context+Long Prompt, Context+Short Prompt, Context only) during fine-tuning/testing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Long prompt vs Short prompt vs Without prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics: With long prompt ROUGE-2 F1 0.453, ROUGE-L F1 0.552, METEOR 0.411, CHrF 67.95%, BLEU 27.78%; With short prompt ROUGE-2 F1 0.312, ROUGE-L F1 0.477; Without prompt ROUGE-2 F1 0.280, ROUGE-L F1 0.449.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Pegasus improved with long prompts compared to short/none (ROUGE-2 F1 +0.141 long vs short; ROUGE-L F1 +0.075 long vs short).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ROUGE-2 F1: +0.141 (0.453 vs 0.312) long vs short; METEOR +0.08 (0.411 vs 0.331 long vs short).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Long prompt improved automated overlap metrics; short prompt provided moderate improvements over no prompt in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that models pre-trained for summarization (Pegasus) can leverage long prompts (topic cues) to better extract and reformulate salient information into questions, improving overlap with reference questions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Pegasus still trails T5/BART on automated metrics even with long prompts; thus prompt length cannot fully substitute for architecture/pretraining differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5663.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5663.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBART (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MBART-large-50 (fine-tuned on EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilingual pre-trained seq2seq model fine-tuned for EduProbe QG and evaluated under long/short/none prompt conditions; shows moderate sensitivity to prompt format with better performance in long-prompt settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MBART-large-50</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>610M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question Generation (EduProbe)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tuned seq2seq model to generate school-level questions from Context +/- prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Inputs formatted as Context plus Long Prompt, Context plus Short Prompt, or Context only, analogous to other fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Long prompt vs Short prompt vs Without prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Automatic metrics: With long prompt ROUGE-2 F1 0.526, ROUGE-L F1 0.640, METEOR 0.417, CHrF 71.05%, BLEU 33.55%, BERTScore 0.786; With short prompt ROUGE-2 F1 0.308, ROUGE-L F1 0.483; Without prompt ROUGE-2 F1 0.281, ROUGE-L F1 0.464.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>MBART shows clear gains for long prompt vs short: ROUGE-2 F1 +0.218 (0.526 vs 0.308); ROUGE-L F1 +0.157 (0.640 vs 0.483).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ROUGE-2 F1: +0.218 absolute long vs short; BERTScore long 0.786 vs short 0.718 (+0.068).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Long prompt improved automated metrics markedly; short prompt yielded intermediate but notably lower scores; without prompt lowest.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The long prompt supplies rich topical cues that MBART can use to generate questions aligned to reference phrasing; shorter prompts are insufficient to trigger the same level of alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Despite long-prompt gains, MBART's human-evaluation results lag behind general-purpose LLMs (Davinci/ChatGPT) on novelty and complexity, indicating format improvements do not close the gap to human-like creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>KHANQ: A Dataset for Generating Deep Questions in Education <em>(Rating: 2)</em></li>
                <li>Type-dependent Prompt CycleQAG : Cycle Consistency for Multi-hop Question Generation <em>(Rating: 2)</em></li>
                <li>Exploring Question-Specific Rewards for Generating Deep Questions <em>(Rating: 1)</em></li>
                <li>LearningQ: A Large-Scale Dataset for Educational Question Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5663",
    "paper_id": "paper-265609730",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Davinci (Text-Davinci-003)",
            "name_full": "Text-Davinci-003 (OpenAI GPT-3 family)",
            "brief_description": "A general-purpose pre-trained LLM by OpenAI used zero-shot with explicit instruction prompts; evaluated under three prompt formats (long prompt, short prompt, without prompt) for educational question generation on EduProbe.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Text-Davinci-003",
            "model_size": "175B",
            "task_name": "Question Generation (EduProbe)",
            "task_description": "Generate deep, school-level questions from a context (text segment) optionally guided by a long or short prompt (hints/phrases) using the EduProbe NCERT-derived dataset.",
            "problem_format": "Three prompt formats passed as input strings to the LLM: (a) With Long Prompt: \"Given the context &lt;Context&gt; and the long prompt &lt;Long Prompt&gt;, generate a Question\"; (b) With Short Prompt: \"Given the context &lt;Context&gt; and the short prompt &lt;Short Prompt&gt;, generate a Question\"; (c) Without Prompt: \"Given the context &lt;Context&gt;, generate a Question\".",
            "comparison_format": "Long prompt vs Short prompt vs Without prompt (the paper compares all three formats for each model).",
            "performance": "Automatic metrics (selected): ROUGE-2 F1: long 0.491, short 0.319, without 0.327; ROUGE-L F1: long 0.603, short 0.529, without 0.506; METEOR long 0.443, short 0.313, without 0.283. Human evaluation (summary): Davinci scored highest among models on grammaticality, appropriateness, relevance, complexity and novelty across prompt settings and often approached but did not surpass human baseline except it matched/beat humans on some complexity/novelty criteria (see paper narrative).",
            "performance_comparison": "Davinci shows substantially higher automatic scores in the long-prompt setting than short/none (ROUGE-2 F1: +0.172 long vs short; ROUGE-2 F1: +0.164 long vs without). On human judgments Davinci outperforms other models in most criteria across formats; in novelty (long prompt) and complexity (without prompt) Davinci (and ChatGPT) exceeded the human baseline according to the authors.",
            "format_effect_size": "ROUGE-2 F1: +0.172 absolute (0.491 vs 0.319) long vs short; ROUGE-L F1: +0.074 absolute (0.603 vs 0.529) long vs short. Human-eval direction: long prompt improved grammaticality/appropriateness/relevance relative to short/none per aggregated human scores (precise numeric human scores are in paper Table 5).",
            "format_effect_direction": "long prompt improved automated overlap metrics and generally improved human-judged grammaticality/appropriateness/relevance; without prompt sometimes produced more complex/novel questions according to human raters.",
            "explanation_or_hypothesis": "The paper hypothesizes that providing explicit cues (long prompts) guides the model to focus on salient concepts in the context, producing outputs closer to the gold-standard phrasing (hence higher automatic overlap metrics). Conversely, no-prompt outputs from Davinci can be more creative/novel and require higher cognitive effort (higher complexity), which human raters rewarded despite lower automatic overlap.",
            "counterexample_or_null_result": "Although long prompts improved automatic metrics, Davinci (and ChatGPT) generated the most complex and novel questions under the without-prompt setting in some cases — i.e., no prompt sometimes increased complexity/novelty even while reducing automatic overlap.",
            "uuid": "e5663.0",
            "source_info": {
                "paper_title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ChatGPT (GPT-3.5-Turbo)",
            "name_full": "GPT-3.5-Turbo (ChatGPT)",
            "brief_description": "A general-purpose conversational LLM (GPT-3.5-Turbo) evaluated zero-shot with the same three prompt templates for question generation on EduProbe; compared to fine-tuned models via automatic and human metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_size": "154B (approx. as reported in paper)",
            "task_name": "Question Generation (EduProbe)",
            "task_description": "Same EduProbe QG task: generate educational questions from a context with optional long or short prompt hints.",
            "problem_format": "Same three input templates: With Long Prompt, With Short Prompt, Without Prompt; prompt text examples provided in paper.",
            "comparison_format": "Long prompt vs Short prompt vs Without prompt (direct comparison in both automatic and human evaluations).",
            "performance": "Automatic metrics (selected): ROUGE-2 F1: long 0.476, short 0.304, without 0.319; ROUGE-L F1: long 0.592, short 0.522, without 0.492; METEOR long 0.423, short 0.309, without 0.266. Human evaluation (summary): ChatGPT rated second-best after Davinci across human criteria and in some settings produced complexity above human baseline.",
            "performance_comparison": "Automatic metrics are higher in the long-prompt setting (ROUGE-2 F1 +0.172 long vs short). Human metrics: ChatGPT improved with prompts for grammaticality/appropriateness/relevance; without prompt there was an increase in complexity scores.",
            "format_effect_size": "ROUGE-2 F1: +0.172 absolute (0.476 vs 0.304) long vs short; ROUGE-L F1: +0.070 absolute (0.592 vs 0.522) long vs short.",
            "format_effect_direction": "long prompt improved automatic overlap and many human-judged quality metrics; absence of prompt sometimes increased perceived complexity/novelty.",
            "explanation_or_hypothesis": "Authors argue prompts act as guidance that focuses generation and increases similarity to reference questions (hence better automatic scores), while unconstrained generation (no prompt) allows more divergent, novel or complex questions favored by human raters for complexity/novelty.",
            "counterexample_or_null_result": "Despite higher automatic scores with long prompts, ChatGPT sometimes produced lower novelty than long-prompt Davinci; also automatic metrics penalize divergence even when human raters prefer outputs (i.e., format improvements in human judgment don't always align with automatic metrics).",
            "uuid": "e5663.1",
            "source_info": {
                "paper_title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "T5 (fine-tuned)",
            "name_full": "T5-large (fine-tuned on EduProbe)",
            "brief_description": "A transformer encoder-decoder (T5-large) fine-tuned on EduProbe under three input formatting choices; produced top automatic-metric performance, especially with long prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large",
            "model_size": "770M",
            "task_name": "Question Generation (EduProbe)",
            "task_description": "Fine-tuned seq2seq QG model producing questions from Context + optional Prompt (long/short) matching EduProbe gold questions.",
            "problem_format": "During fine-tuning/testing the input was formatted as: With Long Prompt: [CLS] Context [SEP] Long Prompt [SEP] -&gt; Question; With Short Prompt: [CLS] Context [SEP] Short Prompt [SEP] -&gt; Question; Without Prompt: [CLS] Context [SEP] -&gt; Question.",
            "comparison_format": "Long prompt vs Short prompt vs Without prompt (comparison of automated metrics primarily; human eval also performed).",
            "performance": "Automatic metrics: With long prompt T5 achieved ROUGE-2 F1 0.575, ROUGE-L F1 0.668, METEOR 0.503, CHrF 74.40%, BLEU 42.97%, BERTScore 0.818; With short prompt ROUGE-2 F1 0.368, ROUGE-L F1 0.542; Without prompt ROUGE-2 F1 0.368, ROUGE-L F1 0.539. Human evaluation: T5 scored well on automated-aligned criteria but was behind Davinci/ChatGPT on human judgments of novelty/complexity.",
            "performance_comparison": "T5 shows large gains in automated metrics with long prompt vs short/none: ROUGE-2 F1 increase +0.207 (0.575 vs 0.368) long vs short; ROUGE-L F1 +0.126 (0.668 vs 0.542) long vs short. Little difference between short and without prompt on automated metrics (short and without have nearly equal ROUGE-2 F1 0.368).",
            "format_effect_size": "ROUGE-2 F1: +0.207 absolute long vs short; BERTScore: 0.818 long vs 0.742 short (+0.076).",
            "format_effect_direction": "Long prompt substantially improved automated overlap metrics for T5; short vs no-prompt had minimal difference for T5 on those automated measures.",
            "explanation_or_hypothesis": "Fine-tuned models like T5 learn to replicate the style/phrasing of the gold questions; introducing a long prompt during training/test helps the model attend to key theme phrases and produce outputs closer to references, increasing automatic metric scores. The similarity-oriented automated metrics favor the fine-tuned phrasing.",
            "counterexample_or_null_result": "Although long prompts strongly improve automatic metrics, T5 still lags behind human baseline on human-judged appropriateness/relevance/novelty; short vs without prompt produced similar automated results, suggesting short prompts (very brief cues) may be insufficient to change model outputs for fine-tuned T5.",
            "uuid": "e5663.2",
            "source_info": {
                "paper_title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "BART (fine-tuned)",
            "name_full": "BART-large (fine-tuned on EduProbe)",
            "brief_description": "A denoising seq2seq transformer fine-tuned on EduProbe; performed strongly in many automated metrics and showed sensitivity to prompt format, particularly favoring long prompts for automated overlap and short prompts for some metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART-large",
            "model_size": "406M",
            "task_name": "Question Generation (EduProbe)",
            "task_description": "Fine-tuned model mapping Context (+ optional Prompt) to school-level questions; evaluated under long, short, and no-prompt formats.",
            "problem_format": "Same formatting as other fine-tuned models: Context with appended Long Prompt or Short Prompt, or Context alone for without-prompt.",
            "comparison_format": "Long prompt vs Short prompt vs Without prompt.",
            "performance": "Automatic metrics: With long prompt ROUGE-2 F1 0.573, ROUGE-L F1 0.666, METEOR 0.443, CHrF 73.72%, BLEU 36.47%, BERTScore 0.809. With short prompt ROUGE-2 F1 0.377, ROUGE-L F1 0.549, METEOR 0.346, CHrF 59.52%, BLEU 21.82%, BERTScore 0.756. Without prompt ROUGE-2 F1 0.341, ROUGE-L F1 0.516, METEOR 0.319.",
            "performance_comparison": "BART's automated scores were much higher with long prompts than with short or no prompt (ROUGE-2 F1 +0.196 long vs short; ROUGE-L F1 +0.117 long vs short). Short prompt outperformed no prompt modestly in some metrics (e.g., ROUGE-2 F1 0.377 vs 0.341).",
            "format_effect_size": "ROUGE-2 F1: +0.196 absolute (0.573 vs 0.377) long vs short; METEOR +0.097 (0.443 vs 0.346) long vs short.",
            "format_effect_direction": "Long prompt improved automated overlap metrics substantially; short prompt gave intermediate performance; without-prompt the weakest on automated metrics.",
            "explanation_or_hypothesis": "Like other fine-tuned models, BART benefits from explicit long prompts that emphasize relevant phrases; this reduces model uncertainty and produces outputs more similar to reference questions. The authors note automated metrics reward closeness to gold, which fine-tuned models achieve better when guided by longer prompts.",
            "counterexample_or_null_result": "Despite higher automated performance with long prompts, human evaluators preferred some outputs from general-purpose LLMs for novelty/complexity; thus BART's long-prompt gains in automatic metrics did not translate to top human-judged novelty/complexity.",
            "uuid": "e5663.3",
            "source_info": {
                "paper_title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Pegasus (fine-tuned)",
            "name_full": "Pegasus-large (fine-tuned on EduProbe)",
            "brief_description": "A summarization-focused encoder-decoder transformer (Pegasus) fine-tuned for QG on EduProbe and evaluated under long/short/none prompt formats; shows prompt sensitivity but overall lower automated scores than T5/BART.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pegasus-large",
            "model_size": "568M",
            "task_name": "Question Generation (EduProbe)",
            "task_description": "Transform context (+ optional prompt) into a question; Pegasus is repurposed from summarization to QG and fine-tuned on EduProbe.",
            "problem_format": "Same three formats (Context+Long Prompt, Context+Short Prompt, Context only) during fine-tuning/testing.",
            "comparison_format": "Long prompt vs Short prompt vs Without prompt.",
            "performance": "Automatic metrics: With long prompt ROUGE-2 F1 0.453, ROUGE-L F1 0.552, METEOR 0.411, CHrF 67.95%, BLEU 27.78%; With short prompt ROUGE-2 F1 0.312, ROUGE-L F1 0.477; Without prompt ROUGE-2 F1 0.280, ROUGE-L F1 0.449.",
            "performance_comparison": "Pegasus improved with long prompts compared to short/none (ROUGE-2 F1 +0.141 long vs short; ROUGE-L F1 +0.075 long vs short).",
            "format_effect_size": "ROUGE-2 F1: +0.141 (0.453 vs 0.312) long vs short; METEOR +0.08 (0.411 vs 0.331 long vs short).",
            "format_effect_direction": "Long prompt improved automated overlap metrics; short prompt provided moderate improvements over no prompt in some cases.",
            "explanation_or_hypothesis": "Authors suggest that models pre-trained for summarization (Pegasus) can leverage long prompts (topic cues) to better extract and reformulate salient information into questions, improving overlap with reference questions.",
            "counterexample_or_null_result": "Pegasus still trails T5/BART on automated metrics even with long prompts; thus prompt length cannot fully substitute for architecture/pretraining differences.",
            "uuid": "e5663.4",
            "source_info": {
                "paper_title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MBART (fine-tuned)",
            "name_full": "MBART-large-50 (fine-tuned on EduProbe)",
            "brief_description": "A multilingual pre-trained seq2seq model fine-tuned for EduProbe QG and evaluated under long/short/none prompt conditions; shows moderate sensitivity to prompt format with better performance in long-prompt settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MBART-large-50",
            "model_size": "610M",
            "task_name": "Question Generation (EduProbe)",
            "task_description": "Fine-tuned seq2seq model to generate school-level questions from Context +/- prompts.",
            "problem_format": "Inputs formatted as Context plus Long Prompt, Context plus Short Prompt, or Context only, analogous to other fine-tuned models.",
            "comparison_format": "Long prompt vs Short prompt vs Without prompt.",
            "performance": "Automatic metrics: With long prompt ROUGE-2 F1 0.526, ROUGE-L F1 0.640, METEOR 0.417, CHrF 71.05%, BLEU 33.55%, BERTScore 0.786; With short prompt ROUGE-2 F1 0.308, ROUGE-L F1 0.483; Without prompt ROUGE-2 F1 0.281, ROUGE-L F1 0.464.",
            "performance_comparison": "MBART shows clear gains for long prompt vs short: ROUGE-2 F1 +0.218 (0.526 vs 0.308); ROUGE-L F1 +0.157 (0.640 vs 0.483).",
            "format_effect_size": "ROUGE-2 F1: +0.218 absolute long vs short; BERTScore long 0.786 vs short 0.718 (+0.068).",
            "format_effect_direction": "Long prompt improved automated metrics markedly; short prompt yielded intermediate but notably lower scores; without prompt lowest.",
            "explanation_or_hypothesis": "The long prompt supplies rich topical cues that MBART can use to generate questions aligned to reference phrasing; shorter prompts are insufficient to trigger the same level of alignment.",
            "counterexample_or_null_result": "Despite long-prompt gains, MBART's human-evaluation results lag behind general-purpose LLMs (Davinci/ChatGPT) on novelty and complexity, indicating format improvements do not close the gap to human-like creativity.",
            "uuid": "e5663.5",
            "source_info": {
                "paper_title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "KHANQ: A Dataset for Generating Deep Questions in Education",
            "rating": 2,
            "sanitized_title": "khanq_a_dataset_for_generating_deep_questions_in_education"
        },
        {
            "paper_title": "Type-dependent Prompt CycleQAG : Cycle Consistency for Multi-hop Question Generation",
            "rating": 2,
            "sanitized_title": "typedependent_prompt_cycleqag_cycle_consistency_for_multihop_question_generation"
        },
        {
            "paper_title": "Exploring Question-Specific Rewards for Generating Deep Questions",
            "rating": 1,
            "sanitized_title": "exploring_questionspecific_rewards_for_generating_deep_questions"
        },
        {
            "paper_title": "LearningQ: A Large-Scale Dataset for Educational Question Generation",
            "rating": 1,
            "sanitized_title": "learningq_a_largescale_dataset_for_educational_question_generation"
        }
    ],
    "cost": 0.014638,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models
2 Dec 2023</p>
<p>Subhankar Maity subhankar.ai@kgpian.iitkgp.ac.in 0009-0001-1358-9534
Aniket Deroy 
Sudeshna Sarkar sudeshna@cse.iitkgp.ac.in </p>
<p>IIT Kharagpur West Bengal
India</p>
<p>IIT Kharagpur West Bengal
India</p>
<p>India</p>
<p>Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models
2 Dec 20235A2FB88B3F0C328DB9716841A5E5D59B10.1145/3632754.3632755arXiv:2312.01032v1[cs.CL]EducationQuestion GenerationPromptLarge Language Models (LLMs)
Designing high-quality educational questions is a challenging and time-consuming task.In this work, we propose a novel approach that utilizes prompt-based techniques to generate descriptive and reasoning-based questions.However, current question-answering (QA) datasets are inadequate for conducting our experiments on prompt-based question generation (QG) in an educational setting.Therefore, we curate a new QG dataset called EduProbe for schoollevel subjects, by leveraging the rich content of NCERT textbooks.We carefully annotate this dataset as quadruples of 1) Context: a segment upon which the question is formed; 2) Long Prompt: a long textual cue for the question (i.e., a longer sequence of words or phrases, covering the main theme of the context); 3) Short Prompt: a short textual cue for the question (i.e., a condensed representation of the key information or focus of the context); 4) Question: a deep question that aligns with the context and is coherent with the prompts.We investigate several prompt-based QG methods by fine-tuning pre-trained transformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and BART.Moreover, we explore the performance of two general-purpose pre-trained LLMs such as Text-Davinci-003 and GPT-3.5-Turbowithout any further training.By performing automatic evaluation, we show that T5 (with long prompt) outperforms all other models, but still falls short of the human baseline.Under human evaluation criteria, Text-Davinci-003 usually shows better results than other models under various prompt settings.Even in the case of human evaluation criteria, QG models mostly fall short of the human baseline.Our code and dataset are available at: https://github.com/my625/PromptQGCCS CONCEPTS• Computing methodologies → Natural language generation; Language resources; • Applied computing → Education.</p>
<p>INTRODUCTION</p>
<p>The primary objective of the automated question generation task (AQG) is to automatically produce questions based on textual or knowledge data.Prompt-based QG refers to the approach of generating questions using a prompt or stimulus text, which enables providing more information while producing questions [8,15].</p>
<p>Previous studies in AQG [11,30,35] employ single-hop questionanswering (QA) datasets such as SQuAD [27], which are representative of QA research, as well as multi-hop QA datasets such as HotpotQA [32].But they were considered unsuitable for this particular study, which specifically focuses on educational materials like textbooks for real-life classroom scenarios.</p>
<p>Our proposed dataset EduProbe is sourced from NCERT1 textbooks and covers a wide range of subjects and grade levels from 6 ℎ to 12 ℎ standard.We carefully annotate this dataset as quadruples of 1) Context: a segment upon which the question is formed; 2) Long Prompt: a long textual cue for the question (i.e., a longer sequence of words or phrases, covering the main theme of the context); 3) Short Prompt: a short textual cue for the question (i.e., a condensed representation of the key information or focus of the context).Primarily, we select a noun phrase from the beginning portion of the context to serve as a short prompt; 4) Question: a deep question that aligns with the context and is coherent with the prompts.After annotation, we gather a total of 3,502 <Context, Long Prompt, Short Prompt, Question> quadruples to form the EduProbe dataset.An example is given in Table 1.</p>
<p>Table 1: One instance within our EduProbe dataset.</p>
<p>Context: Purchasing power parity (PPP) is an economic indicator that signifies the purchasing power of the currencies of various nations of the world against each other.It helps in comparing living standards between different countries and estimating economic productivity.</p>
<p>Long Prompt: purchasing power parity helps Short Prompt: purchasing power Gold Standard Question : What does purchasing power parity do?</p>
<p>To go into the diversity and depth of questions in our created dataset, we classify questions based on the first two words in the question and make comparisons to other frequently used QA datasets, such as SQuAD, and HotpotQA.The corresponding table is provided as Table 2.As stated in [6], questions phrased with "Why is" and "How do" are often considered to be deep questions.Table 2 shows that our proposed dataset, EduProbe consists of more deep questions (starting with "Why is", and "How do") compared to SQuAD, and HotpotQA.Following the criteria in [3], we further categorize these questions based on their reasoning type.It turns out that 46.14% of questions in EduProbe include deep reasoning.In this work, we fine-tune four transformer-based large language models (LLMs) such as Pegasus [33], T5 [26], MBART [18], and BART [16] on our proposed dataset EduProbe.With the recent advancement of general-purpose LLMs2 such as Text-Davinci-003 and GPT-3.5-Turbo, the question arises of whether these generalpurpose LLMs can be used for QG without any further training.So we also explore pre-trained LLMs such as Text-Davinci-003 and GPT-3.5-Turbo for prompt-based QG.Through a comprehensive evaluation, we observe that T5 (with long prompt) outperforms other models in all automated evaluation metrics (see Section 8).In the case of automated metrics, the QG models still fall short of the human baseline.Regarding human evaluation (see Section 9), Text-Davinci-003 (with long prompt) shows the best proficiency in generating questions in terms of grammaticality.Furthermore, Text-Davinci-003 (with short prompt) demonstrates commendable proficiency in generating questions in terms of appropriateness, and relevance.On the other hand, Text-Davinci-003 (without prompt) distinguishes itself by generating questions that are novel and complex.Although in the case of manual evaluation criteria, the QG models still fall short of the human baseline except for Text-Davinci-003 and GPT-3.5-Turbounder novelty and complexity criteria.</p>
<p>In summary, the contributions of this paper are as follows: (i) Developing a dataset called EduProbe, for school-level subjects, namely History, Geography, Economics, Environmental Studies, and Science, which to the best of our knowledge is not present; (ii) We perform an in-depth comparative study of prompt-based (e.g., long and short prompt) techniques, and without prompt-based technique utilizing state-of-the-art (SOTA) LLMs on our proposed dataset, EduProbe.We evaluate them using automated metrics; (iii) Since automated metrics have their own limitations, in terms of evaluating deep questions, we also perform the human evaluation of the generated questions with the help of school-level students and teachers, thereby drawing various meaningful insights.</p>
<p>The remainder of the paper is organized as follows.We discuss the relevant literature in Section 2. We present the motivation of our work in Section 3. We then define the task in Section 4, and discuss the dataset in Section 5. We describe the methodology in Section 6, and the experimental setting in Section 7. We present the automated and human evaluation metrics, results in Section 8, and Section 9 respectively.We have a general discussion on the analysis of the results in Section 10.Finally, we conclude our work in Section 11.</p>
<p>RELATED WORK</p>
<p>Previous works of QG utilize sequence-to-sequence (Seq2Seq) models [11,35], to produce questions based on various aspects of the sentence, including its focus, type, and specific relationships.A model is proposed by Pan et al. [22] which is made up of four components: a document encoder that encodes the input document, a semantic graph encoder that embeds the document-level semantic graph using a gated graph neural network based on attention, a content selector that identifies important information from the semantic graph suitable for generating questions, and a question decoder that generates questions based on the enhanced document representation.Xie et al. [31] introduce a framework consisting of two main parts: the question generator and QG-specific rewards.The question generator utilizes a Seq2Seq framework with attention, copying, and coverage mechanisms, similar to existing neural QG works.During training, the model learns by maximizing the likelihood of correct questions.However, this basic question generator faces a problem called exposure bias.To address this issue, they introduce three QG-specific rewards to assess the quality of the questions generated by the basic model.These rewards focus on assessing the fluency, relevance, and answerability of the questions.</p>
<p>Prompt-based QG</p>
<p>Current works [8,15] explore a few prompt-based techniques for QG.Gong et al. [8] build a large dataset called KHANQ by annotating each data sample as a triple of <Context, Prompt, Question> and explore prompt-based QG with LLMs such as BERT Generation [29], BART [16], GPT2 [25], T5 [26], and UniLM [7].The prompts used in KHANQ have been designed on the basis of the learner's background knowledge and understanding of the subject.Prompt-based fine-tuning is employed by Lee and Lee [15] to create multi-hop questions.The methodology for this task involves a series of tasks starting with QG, followed by QA, which is performed repeatedly in cycles to develop a robust methodology for the QG task.They use T5 to train both the QG and the QA models.Also, question paraphrasing is being performed, which adds to the robustness of the method.Finally, prompt-based fine-tuning is performed to generate quality questions.They generate a prompt by selecting relevant words associated with the correct answer.</p>
<p>Use of LLMs for QG</p>
<p>Here, we discuss the following LLMs, which we explore for the task of QG in this study: Text-Davinci-003 (abbreviated as Davinci) is a model by OpenAI which has 175 billion parameters.During the training process, a combination of supervised and unsupervised learning methods is used.The specific details of the training data sources used for Davinci have not been publicly disclosed by OpenAI.However, training data are known to consist of a diverse range of sources, including web pages, books, scientific articles, and various other forms of human-written text.The maximum input length supported by Davinci is 4,097 tokens.Davinci is available at: https: //platform.openai.com/docs/models/gpt-3-5.GPT-3.5-Turbo(abbreviated as ChatGPT), an extension of the GPT-3 architecture, which has around 154 billion parameters and underwent training using various text sources such as web pages, books, scientific articles, and more.Its training methods included supervised and reinforcement learning techniques.The primary focus of its optimization was to improve speed, performance, and resource efficiency.It has a maximum input token limit of 4,096.ChatGPT is available at: https://platform.openai.com/docs/models/gpt-3-5.T5 [26] is based on the transformer architecture and trained using a large-scale dataset consisting of diverse text sources.T5-large has a huge number of parameters, specifically around 770 million, enabling it to capture intricate patterns and relationships in text data.The maximum input length supported by T5-large is 512 tokens.The extensive pre-training and fine-tuning process of T5 makes it a powerful tool for generating high-quality questions and producing accurate natural language output.Here, we use T5-large (https://huggingface.co/t5-large) for the QG task.Pegasus [33] is a transformer-based model designed for text summarization, using an encoder-decoder architecture with self-attention mechanisms to capture long-range dependencies.With approximately 568 million parameters, Pegasus-large can handle complex summarization tasks and generate high-quality summaries.Its input limit is 1,024 tokens.It has the potential to be utilized for the QG task.Here, we utilize Pegasus-large (https://huggingface.co/ google/pegasus-large) for the purpose of QG.MBART [18] (Multilingual Bidirectional Auto-Regressive Transformers) incorporates a transformer-based architecture, which allows it to effectively capture contextual information and generate high-quality translations.It leverages pre-training on a large multilingual corpus to learn cross-lingual representations.In terms of the number of parameters, MBART-large-50 has around 610 million parameters.It has a maximum input token limit of 1,024.It can be utilized for the QG task.Here, we utilize MBART-large (https://huggingface.co/facebook/MBART-large-50) for QG task.BART <a href="Bidirectional Auto-Regressive Transformers">16</a> comprises a bidirectional encoder and a left-to-right decoder.During pretraining, it shuffles the order of sentences and uses a unique method of infilling where sections of text are replaced with a mask token.BART-large has around 406 million parameters.The maximum input length supported by BART is 1,024 tokens.It can be used for QG tasks.Here, we use BART-large (https://huggingface.co/facebook/ BART-large) for the QG task.</p>
<p>Datasets used for QG</p>
<p>Due to the data-centric nature of QG, the QG methods mentioned above leverage the availability of large-scale QA datasets, such as SQuAD, HotpotQA, TriviaQA [9], Natural Questions corpus [12], QuAC [5], OpenBookQA [19], etc.According to Cao and Wang [3], these corpora are limited to the generation of simple fact-based questions.Furthermore, as stated in [20,21], the majority of these QA datasets are borrowed or crowd-sourced from open-source platforms such as Wikipedia articles, and the questions generally do not incorporate multiple sentences as their basis.There is a notable QG dataset for educational purposes called LearningQ [4], which utilizes complete articles or videos as contexts, resulting in a substantial portion of sentences within the contexts being irrelevant to the specific target question.In contrast, we utilize explanatory answers that contain comprehensive knowledge points relevant to the question.</p>
<p>MOTIVATION</p>
<p>The creation of high-quality questions is a fundamental task for educators seeking to foster deep understanding and critical thinking in students.However, the process of designing educational questions manually is often burdensome and time-consuming [8].Through this research effort, our aim is to provide a valuable tool that empowers educators to create descriptive and reasoning-based questions more efficiently.By allowing teachers to allocate more time to classroom interactions and student participation, our proposed QG method has the potential to positively impact teaching practices and enhance learning outcomes.</p>
<p>Previous research in QG [11,30,35] predominantly emphasizes the generation of fact-based questions that relate to a single piece of information derived from a single sentence.Moreover, current QA datasets such as SQuAD, HotpotQA, TriviaQA, QuAC, Open-BookQA, etc. do not align with the requirements of generating school-level educational questions since they do not have deep questions that are reasoning-based and descriptive in nature.</p>
<p>So, we curate a new dataset called EduProbe to tackle the complexities of school-level educational QG.The proposed dataset serves as the foundation for our investigation into the efficacy of promptbased methods for QG, which involves providing the system with explicit hints or triggers to generate deep questions.The promptbased AQG process offered by our approach not only reduces the time required for question development, but also improves the overall quality and variety of questions generated.</p>
<p>Our work has the following three key differences from previous works on QG: (i) Our created dataset EduProbe is geared towards creating questions that are more educationally oriented in the context of school-level subjects; (ii) We explore different types of prompt-based techniques (e.g., long prompt, short prompt, and without prompt) with SOTA LLMs (e.g., Text-Davinci-003, GPT-3.5-Turbo,etc.) to provide the QG models additional guidance on what information to emphasize more when generating questions that still have not been explored earlier in a detailed manner; (iii) Our proposed prompt-based approaches are capable of generating a wide variety of questions from a single context which has not been explored in previous works.</p>
<p>TASK DEFINITION</p>
<p>In this section, we define three different prompt settings explored in our study: Firstly, the annotators are instructed to go through the selected chapters line by line and generate question-answer pairs by considering only the relevant portions of the information from the NCERT textbooks.To establish the context, we instruct the annotators to review the generated answers.Upon analysis, we observe that the majority of the answers consist of comprehensive explanations related to the questions' pertinent knowledge points.Hence, these answers prove suitable for serving as the context for the question.Secondly, we instruct the annotators to select a sequence of words or phrases which cover the main theme of the context to serve as a long prompt for the question.Lastly, annotators are asked to pick up a noun phrase from the beginning portion of the context to act as a short prompt.In this manner, each question-answer pair in EduProbe is carefully annotated as a quadruple of <Context, Long Prompt, Short Prompt, Question>.</p>
<p>Two annotators (two graduate students) with adequate subject knowledge and experience were involved in manually annotating the data samples.</p>
<p>Data Statistics: We carefully curate 3,502 question-answer pairs, of which 858 pairs are related to History, 861 pairs are related to Geography, 802 pairs are related to Economics, 606 pairs are related to Environmental Studies, and 375 pairs are related to Science.On average, the length of the context, long prompt, short prompt, and question are 55.27 words, 6.80 words, 2.15 words, and 7.16 words, respectively.</p>
<p>Comparing the KHANQ dataset and our proposed dataset EduProbe we observe that the average length of the long prompt and short prompt in our dataset is 6.80 and 2.15 words, respectively.But in the KHANQ dataset, the average prompt length is 14.12 words.However, the KHANQ dataset is not publicly available for research purposes.</p>
<p>Question Types: In order to gain a deeper understanding of the question attributes, we conduct a detailed manual analysis of a subset of 65 distinct questions randomly sampled from the EduProbe dataset.These questions were categorized according to the criteria specified in [3].Here, we present a summary of the most commonly found question types in EduProbe and their corresponding examples.</p>
<p>• Procedural Questions: Out of the questions we examine, 18.46% of them are about the procedures or methods used to achieve a specific outcome.Most of these questions begin with "How" and are followed by a modal verb, an auxiliary verb, or "to".</p>
<p>-How did Pandita Ramabai break stereotypes?-How did Brahmo Samaj reform Indian society?• Cause Questions: We observe that 15.38% of the questions examined are focused on the reason or cause behind a concept or event.Most of these questions began with the word "Why" and were subsequently followed by a modal verb, an auxiliary verb, or their negative counterparts.</p>
<p>-Why is the Ganges river dolphin blind?-Why is urban waste disposal a serious problem in India?• Verification Questions: We discover that 9.23% of the examined questions are concerned with verifying the trustworthiness of a concept or event.Most of these questions are formulated as general questions that originate with verbs, modal verbs, or auxiliary verbs.</p>
<p>-Does universal basic income (UBI) reduce poverty?-Are Vedas older than Puranas?• Consequence Questions: We find that 12.30% of the questions analyzed focused on the consequences or outcomes resulting from a particular event.Most of these questions used phrases such as "What happens", "How does it affect", etc.</p>
<p>-What happens if oceans acidify?-How does the government deficit affect the economy?</p>
<p>According to the findings of [3], there are four categories of questions that require profound reasoning, namely cause, consequence, judgemental, and procedural.Relating these categories to EduProbe, we observe that the procedural questions, the cause questions, and the consequence questions are three specific categories that involve deep reasoning, representing 46.14%.</p>
<p>METHODOLOGY</p>
<p>We experiment with various prompt-based settings with LLMs.There are two main categories of LLMs explored in our study: (1) Pre-trained General-purpose LLMs; (2) Fine-tuned Domain-specific LLMs.</p>
<p>Pre-trained General-purpose LLMs</p>
<p>We try Text-Davinci-003 (abbreviated as Davinci) and GPT-3.5-Turbo(abbreviated as ChatGPT) model using OpenAI API 3 .For the pre-trained general-purpose LLMs, we have to pass a prompt as input to the LLM which will consist of the instruction for the task of QG, along with the context and prompt (if needed).The LLM will generate text based on the given prompt, which will be our desired output.For each model, we try three different variations based on prompts that are as follows: a) With Long Prompt: The prompt we use is "Given the context <Context> and the long prompt <Long Prompt>, generate a Question".b) With Short Prompt: The prompt we apply is "Given the context <Context> and the short prompt <Short Prompt>, generate a Question".c) Without Prompt: The prompt we utilize is "Given the context <Context>, generate a Question".</p>
<p>Fine-tuned Domain-specific LLMs</p>
<p>We also fine-tune four pre-trained transformer-based QG models (or LLMs), namely Pegasus [33], T5 [26], MBART [18], and BART [16] obtained from the open-source library 4</p>
<p>EXPERIMENTAL SETTINGS</p>
<p>In our experiment, we randomly sample 80% of the data in EduProbe for training and the rest for testing.The experiments are run on an NVIDIA Tesla P100 16GB GPU and the models are optimized using the Adam optimizer [10].The specific hyperparameter configurations of the LLMs used in our experiments are given in Table 3.</p>
<p>AUTOMATIC EVALUATION</p>
<p>In this section, we present the main results in EduProbe, using the methodology explained in Section 6.</p>
<p>Evaluation Metrics</p>
<p>We use the following popular metrics that compare a QG modelgenerated question with the gold standard question: Rouge [17] (Recall-Oriented Understudy for Gisting Evaluation) is widely utilized as a metric to assess the quality of generated summaries by summarization models.Here, we compute Rouge-2 precision, Rouge-2 recall, and Rouge-2 F1 score to evaluate the bigram overlap between the QG model-generated questions and the reference gold standard questions.Furthermore, Rouge-L precision, Rouge-L recall, and Rouge-L F1 scores are calculated to measure the longest common subsequence-based match between the generated questions and the gold standard questions.</p>
<p>Meteor [14] (Metric for Evaluation of Translation with Explicit ORdering) calculates the harmonic mean of unigram precision and recall, which is commonly used to evaluate machine translation results.In our case, we apply this metric to measure the unigram overlap between a QG model-generated question and the reference gold standard question.</p>
<p>CHrF [24] (Character n-gram F-score) is a metric that evaluates the similarity between the generated output and the reference summaries at the character level.Here, CHrF calculates the F-score based on the precision and recall of the matching character n-grams between the QG model-generated questions and the reference goldstandard questions.BLEU <a href="Bilingual Evaluation Understudy">23</a> is a widely used metric to evaluate the quality of a machine-generated text.Here, we calculate the overlap between the QG model-generated question and the reference gold question based on n-gram matches.BLEU calculates a score ranging from 0 to 1, with higher scores indicating better quality.</p>
<p>BERTScore [34] is a metric that we utilize to measure the similarity between the generated question and the reference gold question using contextual embeddings from a pre-trained BERT model.It computes a score based on the cosine similarity of the embeddings, capturing both lexical and contextual similarities.We utilize the implementations of the aforementioned metrics from the SummEval package 5 .</p>
<p>In order to compare with human performance, we appoint two high school teachers who were not involved in the annotation process and request them to undertake the same task as the models, generating questions based on the given context and prompt settings for all data samples from the test set.To minimize subjective variations, they were instructed to collaborate and reach a consensus while formulating their responses.</p>
<p>Results</p>
<p>We present the results of automated evaluation metrics for different models, in order to investigate the influence of prompts.Table 4 represents the results of automatic evaluation of LLMs under different prompt settings for QG.T5 performs the best across all metrics in the long prompt setting.When the prompt is short, T5 achieves the highest scores in ROUGE-2 precision and ROUGE-L recall.BART obtains the best results in ROUGE-2 Recall, ROUGE-2 F1, ROUGE-L Precision, ROUGE-L F1, METEOR, CHrF, BLEU, and BERTScore under the short prompt setting.Furthermore, T5 outperforms the other models in terms of all metrics, except for BERTScore, in the without prompt setting and BART achieves the highest BERTScore.</p>
<p>Compared to other models, T5 and BART exhibit superior performance in automatic evaluation across various prompt settings (e.g., long prompt, short prompt, and without prompt).However, human references consistently achieve the highest scores across all automated metrics and prompt settings.This observation emphasizes the fact that SOTA LLMs still have not reached the level of human performance on our EduProbe dataset.</p>
<p>HUMAN EVALUATION</p>
<p>Considering the limitations associated with automated metrics in the field of text generation research [1, 2, 28], we also conduct a human evaluation by appointing two high school teachers and three high school students who were not engaged in the annotation process and the generation of human baseline questions.Every human evaluator was asked to rate a total of 1,800 questions, taking into account six models and three prompt settings.The rating scale used ranged from 1 (worst) to 5 (best) based on five criteria: Grammaticality, which measures the grammatical correctness of the generated question, regardless of the context or prompt; Appropriateness, which examines the semantic correctness of the question irrespective of the context or prompt; Relevance, which measures the degree to which the generated question is pertinent and aligned with the given context or prompt; Complexity, which estimates the level of reasoning or cognitive effort required to answer the generated question; Novelty, which measures the originality and distinctiveness of the generated question in comparison to the gold standard question for the given context.Model-wise Evaluation: We report the human evaluation results for different models under different prompt settings on the EduProbe dataset in Table 5. Davinci demonstrates superior performance compared to other models in human evaluation in different prompt settings (e.g., long prompt, short prompt, and without prompt) in generating questions that exhibit impressive grammaticality, appropriateness, relevance, complexity, and novelty.However, human references achieve the highest scores on most human criteria, except for the novelty under the long prompt setting and complexity in the without prompt setting.This also suggests that SOTA LLMs still fall short of reaching the level of human performance in most cases.Although Davinci and ChatGPT overtake human-level performance in terms of producing complex questions.Inter-annotator Agreement: In order to assess the level of agreement among the five annotators assigned to each generated question, we use Fleiss's kappa as a metric for inter-annotator agreement.Our calculations yield agreement scores of 0.49, 0.43, 0.44, 0.39, and 0.32 for grammaticality, appropriateness, relevance, complexity, and novelty, respectively.The kappa values in grammaticality, appropriateness, relevance indicate a moderate agreement [13], while the kappa results for complexity and novelty indicate a fair level of agreement.</p>
<p>ANALYSIS</p>
<p>Both Davinci and ChatGPT generate questions that differ from the gold standard questions in terms of character, unigram, bigram, or longest common subsequence-based overlap [14,17,23,24].Therefore, in terms of automated metrics (see Table 4), these two LLMs cannot show good performance.However, the questions generated by both these general-purpose LLMs (e.g., Davinci and ChatGPT) are of good quality, as shown by the results of the human evaluation criteria in Table 5. Davinci and ChatGPT show superior performance in terms of human criteria like grammaticality, relevance, appropriateness, complexity, and novelty.However, Davinci and ChatGPT fall short of the human baseline in terms of grammaticality, appropriateness, and relevance under the different prompt settings.But it has been observed that Davinci and ChatGPT can generate novel questions that are not present in the gold standard, thus giving them high scores for the novelty metric under the long prompt setting.Overall, Davinci emerges as the best performer, followed by ChatGPT, based on most human evaluation criteria.Furthermore, it has also been observed that Davinci and ChatGPT can generate complex questions in the without prompt setting.The amount of cognitive effort required to answer a question generated by Davinci and ChatGPT in the without prompt setting is significantly higher than the human baseline.Fine-tuned domain-specific LLMs like T5 and BART show good performance in terms of automated metrics because these LLMs generate questions that are closer to the gold standard questions in terms of character, unigram, bigram, and longest common subsequence matches.However, these fine-tuned domain-specific LLMs fall short of Davinci and ChatGPT in terms of human evaluation criteria.We observe different sets of results under different prompt settings and a broad and diverse range of questions generated from the same context, thereby showcasing the utility of prompt-based QG techniques.It suggests that prompts definitely help to vary the quality of the generated questions which is observed both in the case of pre-trained general-purpose LLMs (e.g., Davinci and ChatGPT) and fine-tuned domain-specific LLMs (e.g., Pegasus, T5, MBART, and BART).</p>
<p>Table 6 and Table 7 show two data samples from our EduProbe test set and the corresponding questions generated by different LLMs under various prompt settings.</p>
<p>CONCLUSIONS AND FUTURE WORKS</p>
<p>We introduced EduProbe, a dataset to create deep and diverse questions that are more educationally oriented in the context of schoollevel subjects.We explored different types of prompt-based techniques (e.g., long prompt, short prompt, and without prompt) to provide QG models additional guidance on what information to emphasize more when generating questions.The experiments demonstrate that T5 surpasses other models in all automated metrics.Pre-trained general-purpose LLMs such as Davinci exhibit superior proficiency in generating questions that excel in terms of grammaticality, appropriateness, relevance, novelty, and complexity.Furthermore, Davinci and ChatGPT surpass the human baseline in terms of generating complex questions, though they fall short of the human baseline in terms of generating grammatical, appropriate, relevant, and novel questions.</p>
<p>We aim to explore even larger language models (e.g., GPT-4) for QG in the future on our proposed EduProbe dataset.Currently, we are creating manual prompts through our annotators that can be replaced by automatic keyphrase or span detection models.Additionally, there is a need to develop better automated metrics for measuring the quality of generated questions, as current metrics cannot fully capture the quality of generated questions.We also plan to fine-tune general-purpose LLMs like Davinci in the future.Although LLMs have shown good performance in generating questions, they are still not able to reach human-level performance in most cases.Therefore, further research is required in this direction.</p>
<p>Figure 1 :
1
Figure 1: A diagrammatic representation of the pipeline process utilized to generate questions.</p>
<p>named Huggingface, in our proposed dataset EduProbe.For every model, we try the three different prompt-based techniques which are as follows: a) With Long Prompt: During fine-tuning, we format the input sequence as:( [CLS] Context [SEP] Long Prompt [SEP], [CLS] Question [SEP]) pair.We provide [CLS] Context [SEP] Long Prompt [SEP] as input to predict the Question during test time, as shown in Figure 2.</p>
<p>Figure 2 :
2
Figure 2: With Long Prompt QG Model.</p>
<p>Figure 3 :
3
Figure 3: With Short Prompt QG Model.</p>
<p>Figure 4 :
4
Figure 4: Without Prompt QG Model.</p>
<p>Table 3 :
3
Hyperparameters of the LLMs used in our work.Model Hyperparameters Davinci max tokens: 50, presence penalty: 1.0, frequency penalty: 0.0, temperature: 0.7 ChatGPT max tokens: 50, temperature: 0.7 Pegasus learning rate: 2e-3, epochs: 6, batch size: 1, input length: 512 T5 learning rate: 2e-3, epochs: 6, batch size: 2, input length: 512 MBART learning rate: 1e-3, epochs: 8, batch size: 1, input length: 512 BART learning rate: 1e-3, epochs: 8, batch size: 1, input length: 512</p>
<p>Table 2 :
2
Leading bigrams that occur most frequently in EduProbe, SQuAD, and HotpotQA.
EduProbe%SQuAD% HotpotQA %What is17.70What is8.5What is5.0What are 15.10 What was 5.3Who was2.1Why is7.31 How many 4.9 What was 2.0What was2.94When did 3.1In what1.8Which is2.79In what2.9 When was 1.7How many 2.34What did 2.8Who is1.6How do2.08 When was 2.1 How many 1.0Who was1.91Who was 2.1In which0.9Who is1.91 What does 1.7 What year 0.9What do1.88What are 1.7Are both0.9</p>
<p>Given a dataset , where each data point  ∈  is represented as a 4-tuple <Context, Long Prompt, Short Prompt, Question>, our task is to learn a probabilistic model P(Question | Context, Short Prompt) that can generate a relevant question in the context of the given information.• Without Prompt: Given a dataset , where each data point  ∈  is represented as a 4-tuple <Context, Long Prompt, Short Prompt, Question>, our task is to learn a probabilistic model P(Question | Context) that can generate a relevant question in the context of the given information.There is no public dataset available to conduct our experiments on prompt-based QG in an educational setting.Therefore, we produce a dataset called EduProbe by manually creating question-answer pairs from segments of varying lengths taken from a diverse set of chapters present in the National Council of Educational Research and Training (NCERT) textbooks on History, Geography, Economics, Environmental Studies, and Science from the 6 ℎ standard to the 12 ℎ standard.
Figure 1 illustrates the schematic of the pipeline process used forQG.5 DATASETData Collection and Annotation:
• With Long Prompt: Given a dataset , where each data point  ∈  is represented as a 4-tuple <Context, Long Prompt, Short Prompt, Question>, our task is to learn a probabilistic model P(Question | Context, Long Prompt) that can generate a relevant question in the context of the given information.•With Short Prompt:</p>
<p>Table 4 :
4
Automatic evaluation results for different LLMs in EduProbe with ROUGE2-Precision, ROUGE2-Recall, ROUGE2-F1, ROUGEL-Precision, ROUGEL-Recall, ROUGEL-F1, METEOR, CHrF, BLEU, and BERTScore.The highest value for any metric in long prompt, short prompt, and without prompt setting achieved by any model is shown in blue.The highest value for any metric achieved by any model is underlined.
ModelROUGE-2 PrecisionROUGE-2 RecallROUGE-2 F1ROUGE-L PrecisionROUGE-L RecallROUGE-L F1METEORCHrF (%)BLEU (%)BERTScoreWith Long PromptHuman Baseline0.5170.8430.6260.5880.8910.6950.53176.30 46.570.860Pre-trained General-purpose LLMsDavinci0.4090.7260.4910.4990.8120.6030.44368.2323.240.803ChatGPT0.3910.7060.4760.4840.7930.5920.42366.3622.440.790Fine-tuned Domain-specific LLMsPegasus0.3290.7980.4530.4130.8850.5520.41167.9527.780.770T50.4830.8000.5750.5660.8880.6680.50374.40 42.970.818MBART0.4240.7500.5260.5270.8600.6400.41771.0533.550.786BART0.4600.7940.5730.5480.8870.6660.44373.7236.470.809With Short PromptHuman Baseline0.3240.5790.4180.4680.7580.5630.37761.52 24.920.778Pre-trained General-purpose LLMsDavinci0.2630.4920.3190.4290.7230.5290.31355.6220.860.739ChatGPT0.2600.4860.3040.4180.7200.5220.30954.8319.540.720Fine-tuned Domain-specific LLMsPegasus0.2400.4960.3120.3740.7080.4770.33154.4118.540.723T50.3010.5300.3680.4480.7420.5420.34158.7621.150.742MBART0.2370.4640.3080.3850.6800.4830.30753.5617.640.718BART0.3000.5410.3770.4490.7400.5490.34659.52 21.820.756Without PromptHuman Baseline0.3230.5320.3900.4660.7230.5530.35558.23 23.490.758Pre-trained General-purpose LLMsDavinci0.2730.4620.3270.4130.6660.5060.28354.8620.440.729ChatGPT0.2660.4420.3190.4010.6450.4920.26652.8319.440.710Fine-tuned Domain-specific LLMsPegasus0.2140.4650.2800.3460.6930.4490.30751.4816.240.702T50.3060.5010.3680.4550.7060.5390.32257.03 21.590.718MBART0.2190.4140.2810.3730.6420.4640.29350.4617.340.706BART0.2750.4770.3410.4250.6880.5160.31954.9620.050.742</p>
<p>Table 6 :
6
A data sample related to Economics in our EduProbe dataset, along with corresponding questions generated by different LLMs using various prompt variations.Purchasing power parity (PPP) is an economic indicator that signifies the purchasing power of the currencies of various nations of the world against each other.It helps in comparing living standards between different countries and estimating economic productivity.In what ways can the utilization of purchasing power parity (PPP) as an economic indicator be expanded beyond its traditional applications to promote sustainable development, address socio-economic disparities, and foster international cooperation for the betterment of global economic systems?ChatGPT (with long prompt): What does purchasing power parity (PPP) indicate and how does it assist in comparing living standards and estimating economic productivity among different nations?ChatGPT (with short prompt): What does purchasing power parity (PPP) represent and how does it facilitate the comparison of living standards and the estimation of economic productivity across different nations?ChatGPT (without prompt): How does purchasing power parity (PPP) serve as an economic indicator to measure the purchasing power of currencies across different nations, and what are its implications for comparing living standards and estimating economic productivity?Pegasus (With long prompt): What is the importance of purchasing power parity?How does it help us understand the situation of the world economy?Pegasus (With short prompt): Why is it important to have a good knowledge of purchasing power in order to make informed decisions?
Long Prompt: purchasing power parity helpsShort Prompt: purchasing powerGold Standard Question : What does purchasing power parity do?Davinci (with long prompt): How can the concept of purchasing power parity(PPP) be leveraged to not only compare living standards and estimate economicproductivity between different countries but also address global income inequalityand promote equitable economic development on a global scale?Davinci (with short prompt): How can the concept of purchasing power be har-nessed to foster international collaboration and create a more equitable globaleconomic system that promotes shared prosperity and uplifts disadvantaged na-tions?Davinci (without prompt): Pegasus (Without prompt): What is the relation between living standards andeconomic productivity?T5 (With long prompt): How does purchasing power parity help in economics?Read more about how does purchasing power parity help in economics?T5 (With short prompt): What is the purpose of purchasing power?T5 (Without prompt): What is the importance of a country's development index?How does it help the poor?MBART (With long prompt): What does rupee purchasing power parity do forpeople living in economics?MBART (With short prompt): What is the importance of purchasing power indaily living standards between two countries?MBART (Without prompt): Why is it important to read a country-based economypattern? Give example of poverty.BART (With long prompt): How does purchasing power parity help improveeconomy parity? and why is it good?BART (With short prompt): What is the importance of purchasing power in dailyliving? Purchasing power is important?BART (Without prompt): What is the importance of ease of living standards insimple words, and why is it important?
Context:</p>
<p>Table 7 :
7
A data sample related to History in our EduProbe dataset, along with corresponding questions generated by different LLMs using various prompt variations.During the medieval period in India, Islamic rulers held significant power, leading to the blending of Indian and Islamic cultures, which can still be observed in the architecture and artwork created at that time.The country was governed and administered by notable rulers who made exceptional contributions in the fields of art, culture, literature, and architecture during this period.Long Prompt: the contribution of the medieval period to Indian history Short Prompt: medieval period Gold Standard Question: What is the contribution of the medieval period to Indian history?Davinci (with long prompt): How did the convergence of Indian and Islamic cultures during the medieval period in India influence not only the realms of art, culture, literature, and architecture but also the broader socio-political fabric and historical trajectory of the nation?Davinci (with short prompt): How did the dynamic interplay between Indian and Islamic influences during the medieval period shape not only the artistic and architectural landscape but also the social, religious, and intellectual fabric of India, leaving a lasting impact on its history and identity?Davinci (without prompt): How did the cross-cultural exchange between Indian and Islamic civilizations during the medieval period in India result in a unique synthesis of artistic, architectural, literary, and cultural expressions, and what enduring legacies did these contributions leave on the country's cultural heritage?ChatGPT (with long prompt): What notable contributions were made to Indian history during the medieval period in terms of art, culture, literature, and architecture, reflecting the blending of Indian and Islamic influences?ChatGPT (with short prompt): What impact did the significant influence of Islamic rulers during the medieval period have on the architecture, artwork, governance, and cultural development in India?ChatGPT (without prompt): What were the key cultural and artistic developments that emerged during the medieval period in India under the influence of Islamic rulers, and how did these contributions shape the architecture, artwork, literature, and overall cultural landscape of the time?What is the contribution of the medieval period to Indian history?and what is its significance in Indian history?BART (with short prompt): What was the medieval period in India? and what was its importance for shaping public opinion?BART (without prompt): What was the main difference between the medieval period in India and the Chalcolithic period?</p>
<p>Context: Pegasus (with long prompt): What is the contribution of the medieval period to Indian history?Pegasus (with short prompt): What was the impact of the medieval period on Indian culture, politics and economy?Give examples.Pegasus (without prompt): What were the major events and developments that took place in the medieval period in India?Give examples.T5 (with long prompt): What is the contribution of the medieval period to Indian history?Have you ever wondered what is the contribution of the medieval period to Indian history?T5 (with short prompt): What were the main events of the medieval period in India that took place during the Islamic period?T5 (without prompt): What happened in the medieval period in India, which saw a strong control of Islamic rulers?MBART (with long prompt): What is the contribution of the medieval period to Indian history?What is the contribution of the medieval period?MBART (with short prompt): What were the major events in the medieval period in India that took place during the ancient period?MBART (without prompt): What were the major events that took place in India during the Second World War (WW2)?BART (with long prompt):</p>
<p>https://en.wikipedia.org/wiki/National_Council_of_Educational_Research_and_ Training
Details of the two LLMs are available at: https://platform.openai.com/docs/models/
https://platform.openai.com/docs/api-reference/completions
https://huggingface.co/models
https://github.com/Yale-LILY/SummEval</p>
<p>Human evaluation results for different LLMs in EduProbe on grammaticality, appropriateness, relevance, complexity, and novelty. The highest value for any metric in the long prompt, short prompt, and without prompt setting achieved by any model is shown in blue. The highest value for any metric achieved by any model is underlined. Model Grammaticality Appropriateness Relevance Complexity Novelty With Long Prompt Human Baseline. Table. 595</p>
<p>The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification. Fernando Alva-Manchego, Carolina Scarton, Lucia Specia, 10.1162/coli_a_00418Computational Linguistics. 472021. 12 2021</p>
<p>Re-evaluating Evaluation in Text Summarization. Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, 10.18653/v1/2020.emnlp-main.751Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>Controllable Open-ended Question Generation with A New Question Type Ontology. Shuyang Cao, Lu Wang, 10.18653/v1/2021.acl-long.502Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>LearningQ: A Large-Scale Dataset for Educational Question Generation. Guanliang Chen, Jie Yang, Claudia Hauff, Geert-Jan Houben, 10.1609/icwsm.v12i1.14987Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social Media2018. Jun. 201812</p>
<p>QuAC: Question Answering in Context. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-Tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, 10.18653/v1/D18-1241Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Overhearing dialogues and monologues in virtual tutoring sessions: Effects on quesioning and vicarious learning. Scotty D Craig, Barry Gholson, Matthew Ventura, Arthur C Graesser, International Journal of Artificial Intelligence in Education. 112000. 2000</p>
<p>Unified Language Model Pre-Training for Natural Language Understanding and Generation. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, 2019Curran Associates IncRed Hook, NY, USA</p>
<p>KHANQ: A Dataset for Generating Deep Questions in Education. Huanli Gong, Liangming Pan, Hengchang Hu, Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics. the 29th International Conference on Computational Linguistics. International Committee on Computational LinguisticsGyeongju, Republic of Korea2022</p>
<p>TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Adam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, ICLR 20153rd International Conference on Learning Representations. Yann Bengio, Lecun, San Diego, CA, USA2015. May 7-9, 2015Conference Track Proceedings</p>
<p>Generating Question-Answer Hierarchies. Kalpesh Krishna, Mohit Iyyer, 10.18653/v1/P19-1224Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Natural Questions: A Benchmark for Question Answering Research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019. 2019</p>
<p>The Measurement of Observer Agreement for Categorical Data. J , Richard Landis, Gary G Koch, Biometrics. 331977. 1977</p>
<p>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. Alon Lavie, Abhaya Agarwal, Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics. the Second Workshop on Statistical Machine Translation. Association for Computational LinguisticsPrague, Czech Republic2007</p>
<p>Type-dependent Prompt CycleQAG : Cycle Consistency for Multi-hop Question Generation. Seungyeon Lee, Minho Lee, Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics. the 29th International Conference on Computational Linguistics. International Committee on Computational LinguisticsGyeongju, Republic of Korea2022</p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Multilingual Denoising Pre-training for Neural Machine Translation. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, 10.1162/tacl_a_00343Transactions of the Association for Computational Linguistics. 82020. 2020</p>
<p>Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, 10.18653/v1/D18-1260Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Automatic Generation of Multiple-Choice Test Items from Paragraphs Using Deep Neural Networks. Ruslan Mitkov, Halyna Maslak, Tharindu Ranasinghe, Vilelmini Sosoni, Advancing Natural Language Processing in Educational Assessment. Routledge2023</p>
<p>Automatic question generation: a review of methodologies, datasets, evaluation metrics, and applications. Nikahat Mulla, Prachi Gharpure, Progress in Artificial Intelligence. 122023. 2023</p>
<p>Semantic Graphs for Generating Deep Questions. Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan, 10.18653/v1/2020.acl-main.135Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>Bleu: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>chrF: character n-gram F-score for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 192019. 2019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020. 2020</p>
<p>Know What You Don't Know: Unanswerable Questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20182Short Papers)</p>
<p>A Structured Review of the Validity of BLEU. Ehud Reiter, 10.1162/coli_a_00322Computational Linguistics. 4432018. 2018</p>
<p>Leveraging Pretrained Checkpoints for Sequence Generation Tasks. Sascha Rothe, Shashi Narayan, Aliaksei Severyn, 10.1162/tacl_a_00313Transactions of the Association for Computational Linguistics. 82020. 2020</p>
<p>Capturing Greater Context for Question Generation. Anh Luu, Darsh Tuan, Regina Shah, Barzilay, 10.1609/aaai.v34i05.6440Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020. Apr. 202034</p>
<p>Exploring Question-Specific Rewards for Generating Deep Questions. Yuxi Xie, Liangming Pan, Dongzhe Wang, Min-Yen Kan, Yansong Feng, 10.18653/v1/2020.coling-main.228Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics. the 28th International Conference on Computational Linguistics. International Committee on Computational LinguisticsBarcelona, Spain (Online2020</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu, International Conference on Machine Learning. PMLR2020</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020</p>
<p>Question-type Driven Question Generation. Wenjie Zhou, Minghua Zhang, Yunfang Wu, 10.18653/v1/D19-1622Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
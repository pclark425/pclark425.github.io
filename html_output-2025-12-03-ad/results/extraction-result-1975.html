<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1975 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1975</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1975</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-282102125</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.13237v1.pdf" target="_blank">Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions. Despite this progress, their adversarial robustness remains underexplored. In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models. We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view. In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator. EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs. Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task. To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs. Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation. The codebase is accessible via the homepage at https://edpa-attack.github.io/.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1975.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1975.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (vision-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLA model that encodes images into token-level patch embeddings and language into token embeddings, concatenates them and feeds them to an LVLM backbone to autoregressively produce action tokens decoded to robot actions; used as the primary victim model for attack and defense experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Visual encoder Ev produces a sequence of d-dimensional image patch embeddings [p1..pN]; language encoder Et produces token embeddings [w1..wM]; both are concatenated and processed by an LVLM backbone f(·) (large vision-language model) which outputs action tokens mapped to robot actions. Grounding is implemented via joint processing of image patch embeddings and language token embeddings inside the LVLM, with attention from language tokens over image patches.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Patch-based visual encoder producing image patch embeddings (Ev); specific architecture not specified in paper (described abstractly as patch embeddings projected into LVLM input space).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not precisely specified here; paper indicates OpenVLA's visual encoder was primarily trained/fine-tuned on third-person robotic datasets for LIBERO downstream tasks (pretraining details referenced to OpenVLA prior work, not given numerically in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Concatenation of image patch embeddings and language token embeddings processed by an LVLM backbone; language tokens attend to image patch embeddings (cross-attention-like mechanism inside LVLM). Paper operationalizes semantic grounding in embedding space via cosine-similarity alignment between image patch embeddings and language token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Token-level image patch embeddings (patch-level, i.e., multi-patch token representation).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit only (no explicit 3D coordinates or depth maps used); multi-camera inputs (primary third-person and wrist camera) are supported in some variants but exact spatial coordinate representations are not used/derived within grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Instruction-conditioned robotic manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO benchmark (task suites: Spatial, Object, Goal, Long)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (LIBERO simulator); primary = third-person camera views and optional wrist (egocentric) camera views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Failure Rate (FR; = 1 - Success Rate) averaged across trials</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Original OpenVLA under EDPA: near 100% failure rate on LIBERO task suites in white-box setting (e.g., EDPA: 100.0% FR on Spatial/Object/Goal reported); under random-noise patches ~34.8%–74.9% FR depending on suite; after adversarial finetuning of visual encoder FR reduced substantially (examples: Spatial EDPA FR reduced to 39.4% ±1.0; Object EDPA FR reduced to 58.6% ±0.6; Goal EDPA FR reduced to 73.9% ±1.1; overall average reduction ~34.2% against EDPA).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Not applicable (no direct ablation isolating grounding component); adversarial fine-tuning of visual encoder (defense) reduces EDPA-induced failure by ~34.2% on average for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper compares OpenVLA (single-camera) to OpenVLA-OFT and π0 (multi-camera variants / different pretraining). OpenVLA is the least robust to EDPA; OpenVLA-OFT and π0 show lower failure rates under the same attacks (π0 notably more robust, attributed to wrist-camera data included during pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper identifies overfitting of the visual encoder to the robotic arm appearance (robot arm present in most training images) and attention hijacking by adversarial patches (patch attracts disproportionate attention at expense of task-relevant objects). Also notes inability to align observations across multiple moving camera viewpoints in real time as a limitation for multi-camera attack optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Key failure modes: (1) attention hijacking — adversarial patch draws linguistic-token attention toward patch location and away from objects/robot arm (visualized in first and last attention layers); (2) occlusion — patch can occlude important objects leading to failures; (3) overfitting to arm appearance — models memorize arm features and are sensitive to perturbations that mimic arm structure. Quantitatively, EDPA drives OpenVLA to ~100% FR in white-box; OpenVLA-OFT FR increases by ~62.0% on average and π0 by ~31.4% under EDPA (paper reports per-suite FRs in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Defense: adversarial fine-tuning of the visual encoder to align adversarially perturbed and clean latent representations (minimize L2 between E_v(v⊕δ) and original E_v(v)); π0 handles domain shift better due to inclusion of diverse wrist-camera data in pretraining. No explicit domain-adaptation layer beyond finetuning; transferability experiments (cross-dataset and cross-model) show EDPA transfers moderately well.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper evaluates finetuned downstream VLA variants (OpenVLA, OpenVLA-OFT, π0) and also presents adversarial finetuning of the visual encoder; adversarial finetuning yields robustness gains against patches with only a small degradation under clean conditions (~+1.6% FR increase under clean for OpenVLA). No explicit frozen-vs-finetuned encoder numeric comparison is provided for general pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper hypothesizes that small scale of robotic task datasets and lack of diverse viewpoints cause visual-encoder overfitting to robot arm appearance; no quantitative scaling curve is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early fusion by concatenation of image patch embeddings and language token embeddings into the LVLM input; cross-attention inside LVLM allows language tokens to attend over image patch tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>OpenVLA-style grounding uses patch-token embeddings + LVLM cross-attention; this grounding is fragile to localized adversarial patches that distort the image–instruction embedding alignment and hijack attention; multi-view inputs (wrist + primary) and pretraining that includes diverse camera views reduce fragility; adversarially fine-tuning the visual encoder to align adversarial and clean embeddings is an effective mitigation with modest cost to clean performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1975.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1975.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA-OFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA-OFT (OpenVLA with OFT variant / multi-camera fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuned variant of OpenVLA that incorporates wrist-camera inputs (multi-camera setting) and/or different fine-tuning; evaluated for robustness to adversarial patches and shown to be more robust than the base OpenVLA in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA-OFT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same high-level LVLM architecture as OpenVLA: Ev produces patch embeddings, Et produces language token embeddings; model ingests multi-camera inputs (primary and wrist camera) each encoded by Ev and concatenated into LVLM. Grounding realized by LVLM attention across concatenated multi-view patch tokens and language tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Patch-based visual encoder (same family as OpenVLA) — pre-existing OpenVLA encoder fine-tuned with extra data (OFT variant); not specified to a concrete backbone in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not fully specified here; paper notes OpenVLA-OFT is based on OpenVLA and was fine-tuned with wrist-camera data for LIBERO task suites (fine-tuning details referenced to prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Concatenation of multi-camera image patch embeddings with language tokens processed by LVLM; cross-attention enables language to ground over multi-view image patches.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Patch-token level per camera; multi-view token concatenation (multi-level only in the sense of multiple camera views).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit spatial cues only; multi-camera observations provide additional viewpoint information but not explicit 3D coordinate grounding in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Instruction-conditioned robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO benchmark (Spatial, Object, Goal, Long)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (LIBERO) with primary third-person and wrist camera (egocentric) views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Failure Rate (FR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Under EDPA (white-box patches applied independently to each camera): reported FRs for OpenVLA-OFT in Table 3 — Spatial: 39.7% ±0.9, Object: 52.3% ±0.8, Goal: 80.8% ±0.4, Long: 86.4% ±1.97 (clean FRs are very low: e.g., clean Spatial FR 1.4% ±0.4). EDPA increases FR strongly but less than OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Multi-camera input and diverse fine-tuning reduce susceptibility: compared to single-camera OpenVLA, OpenVLA-OFT exhibits substantially lower FR under transferred and cross-dataset attacks (paper reports ~62% average increase for OpenVLA-OFT under EDPA vs. larger increases for OpenVLA).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>OpenVLA-OFT is more robust than OpenVLA; still less robust than π0 in several suites. Improvement attributed to additional wrist-camera fine-tuning but limited because base OpenVLA encoder had already overfitted.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper suggests OpenVLA-OFT inherits some pretraining overfitting from OpenVLA's visual encoder; even with wrist-camera fine-tuning, residual overfitting and patch-induced attention shifts remain failure sources.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>EDPA applied to each camera increases FR substantially; multi-camera views mitigate but do not eliminate attention-hijacking and occlusion failure modes. Cross-camera alignment inability limits attack optimization (limitation for attacker) and also constrains some defense dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>OpenVLA-OFT uses additional fine-tuning on wrist-camera views for LIBERO; this helps but if encoder pretraining overfitted to third-person views, robustness gains are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>OpenVLA-OFT is a finetuned variant; paper indicates finetuning with wrist-camera data improves robustness relative to base OpenVLA but does not provide a simple frozen-versus-finetuned numeric breakdown within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper discusses that limited scale/diversity of robot datasets can cause encoder overfitting; OpenVLA-OFT partly mitigates this via additional data but residual effects remain.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Concatenation of multi-camera patch tokens with language tokens; LVLM cross-attention over concatenated tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Adding wrist-camera views and fine-tuning reduces vulnerability by providing more diverse visual evidence for grounding, but if the base visual encoder was pre-trained on narrow third-person data, fragility remains; multi-view grounding reduces but does not eliminate attention hijacking by adversarial patches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1975.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1975.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0 (pi-zero: vision-language-action flow model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA model trained with multi-camera data (including wrist-camera during pretraining) that exhibits comparatively stronger robustness to EDPA adversarial patches, attributed to more diverse pretraining and multi-view grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action flow model that encodes visual inputs (multi-camera) into patch tokens and language into token embeddings; LVLM backbone processes concatenated multi-view visual tokens + language tokens to produce action tokens. Pretraining incorporates wrist-camera views leading to increased visual diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Patch-token visual encoder (architecture details inherited from π0 prior work; not specified in this paper), trained on multi-view/ wrist-camera data</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Paper states π0 incorporated wrist-camera data during pretraining (increasing diversity) — exact datasets/scale not specified here (referenced to π0 original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Concatenated multi-view patch-token + language token inputs to LVLM; cross-attention inside LVLM grounds language to visual patches across multiple camera views.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Patch-token (multi-camera concatenated) representation level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit multi-view information used; no explicit 3D coordinate representation described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Instruction-conditioned manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO benchmark (Spatial, Object, Goal, Long)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (LIBERO) with both primary and wrist camera views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Failure Rate (FR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Under EDPA applied per-camera (Table 3): π0 FRs — Spatial: 29.8% ±1.6, Object: 39.5% ±1.7, Goal: 44.3% ±2.0, Long: 70.7% ±1.6 (clean FRs: Spatial 3.5% ±0.3, Object 2.3% ±0.5, Goal 12.0% ±1.6, Long 40.8% ±1.6). π0 shows lower relative increases in FR under EDPA compared to OpenVLA and OpenVLA-OFT.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>π0's pretraining with wrist-camera data appears to improve grounding robustness vs single-camera OpenVLA; EDPA-induced FR increases are smaller for π0 than for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>π0 is the most robust among the evaluated VLA variants in this paper, attributed to wrist-camera inclusion during pretraining and thus more diverse visual representations that reduce overfitting to robotic-arm appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>π0 is less affected by robot-arm overfitting, but still shows substantial FR increases on long-horizon tasks (Long suite) under EDPA—indicating that other perception and long-horizon planning challenges remain.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>EDPA still raises FR notably on long-horizon tasks for π0 (e.g., Long FR up to ~70.7% under attack), indicating that multi-view grounding mitigates but does not remove vulnerability, and that long-horizon tasks remain more brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>π0's inclusion of wrist-camera views during pretraining is presented as an effective strategy to handle view-distribution shift between pretraining and downstream robot observations.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>π0 is provided as a model fine-tuned for LIBERO in experiments; no explicit frozen vs fine-tuned encoder comparison is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper attributes π0's improved robustness qualitatively to more diverse pretraining (multi-view) rather than larger scale per se; no numerical pretraining-scale ablation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Multi-view patch-token concatenation fed to LVLM; language-to-visual grounding via LVLM attention over multi-view tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Pretraining that includes diverse camera viewpoints (wrist-camera) produces visual encoders that ground language more robustly under adversarial patch perturbations; nonetheless long-horizon tasks still show vulnerability, indicating remaining grounding/perception bottlenecks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1975.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1975.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EDPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding Disruption Patch Attack (EDPA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic adversarial patch attack that optimizes a physicalizable patch to disrupt semantic alignment between visual patch embeddings and language token embeddings and to maximize embedding deviation between clean and perturbed visual inputs; requires only access to visual encoder parameters (not LVLM or robot platform).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EDPA (adversarial patch attack)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generates a universal adversarial patch δ by maximizing a weighted sum of two embedding-space objectives: (1) image–instruction alignment disruption loss (L_patch) that reduces semantic alignment between Ev(v) and Ev(v⊕δ) with respect to Et(t), and (2) patch contrastive loss (L_align) that maximizes discrepancy between Ev(v) and Ev(v⊕δ) measured relative to language embeddings. The patch is optimized via gradient steps on δ given access to the visual encoder Ev parameters. The generated patch is placed physically in camera view (applied via mask) to create v⊕δ.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Attack requires access to the visual encoder Ev parameters (patch-token producing encoder); the attack is agnostic to exact encoder architecture but optimizes in that encoder's embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not required for attack design; attacker uses available Ev parameters (either victim encoder or accessible base model) — the paper evaluates cross-model transfer when only base model is available.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>EDPA explicitly targets vision-language grounding by disrupting the semantic alignment between image patch embeddings and language token embeddings (i.e., attacking the embedding-level grounding used by LVLM). It leverages cosine-similarity based alignment and InfoNCE-inspired contrastive objectives to cause mis-grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Embedding-level (latent patch-token embeddings) — attacks internal representation used for grounding rather than pixel-level loss on action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Patch is spatially localized (50×50 px default) applied via a 2D mask to image (top-left corner in visualizations); attack does not use explicit 3D spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Robotic manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO benchmark (Spatial, Object, Goal, Long)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (LIBERO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Failure Rate (FR) increase induced by patch</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>EDPA substantially increases FR: OpenVLA (white-box) FR ≈ 100% across suites; OpenVLA-OFT average FR increases ≈ 62.0%; π0 average FR increases ≈ 31.4% (paper reports per-suite FRs in Tables 2 and 3). Compared with random-noise patches, EDPA increases average FR by ~50.5% (OpenVLA-OFT) and ~26.5% (π0) respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Paper includes ablations related to EDPA objectives: varying α1 (weighting between L_patch and L_align) and patch size. EDPA remains effective across α1 values on OpenVLA, but π0 is more sensitive to α1 (higher α1 => stronger L_patch contribution increases EDPA effectiveness on π0). Larger patch sizes systematically increase FR for all models (tested sizes: 2%, 4%, 8%, 10% of image area).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>N/A (this is an attack). The ablations show that attacks that disrupt image–instruction alignment (L_patch) are especially effective on some VLA models; both loss terms contribute to attack performance depending on the target model.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>EDPA is transferable across models and datasets: cross-dataset transfer (patches generated on LIBERO-Spatial applied to other suites) still causes large FR increases (e.g., OpenVLA increases ~74.7% on average), and cross-model transfer (patches generated on base model then applied to downstream variant) yields reduced but still significant FR increases (e.g., OpenVLA ~49.8% avg FR increase, OpenVLA-OFT ~26.98%, π0 ~9.3% across suites).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>EDPA exploits that grounding occurs at embedding level and that encoders over-attend to repeated visual elements (robot arm); by perturbing embeddings the attack causes attention shifts and mis-grounding. Multi-camera misalignment and occlusion are noted as limitations that also shape attack effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Failure modes produced by EDPA: complete task failure (OpenVLA ~100% FR), attention hijacking (tokens concentrate on patch), and misinterpreted instructions leading to incorrect action generation. Sensitivity analyses show larger patch sizes and particular α1 weighting amplify these failures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>EDPA remains effective under cross-dataset and cross-model transfer, indicating attacks found on one dataset or base model generalize to downstream variants though with reduced potency; attack generation does not require downstream task data in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>EDPA evaluated against finetuned downstream models (OpenVLA, OpenVLA-OFT, π0) and remains effective, though potency differs by target model pretraining/fine-tuning; adversarially fine-tuned encoders are more robust to EDPA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>EDPA effectiveness varies with pretraining diversity: models pretrained with more diverse views (π0) are less susceptible; paper links limited robotic dataset scale to encoder overfitting which EDPA exploits.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Targets embedding-level alignment of concatenated image patch tokens and language tokens inside LVLM (cross-attention pathway), rather than explicit fusion-layer manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Attacking embedding-space grounding (disrupting image–instruction alignment and maximizing embedding drift) is a highly effective, model-agnostic means to break VLA grounding in embodied tasks; attacks transfer across datasets and models to varying degrees and are mitigated by encoder robustness (e.g., multi-view pretraining, adversarial finetuning).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1975.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1975.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adv-Finetune</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial fine-tuning of visual encoder (defense)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A defense that fine-tunes the visual encoder using adversarially perturbed images (patches from EDPA) to encourage the encoder to produce similar embeddings for clean and adversarial inputs, while preserving embeddings for clean inputs via a consistency regularizer to the original encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adversarial fine-tuning (visual encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tune Ev to minimize a weighted sum of L2 reconstruction losses: keep Ev(v) close to E_orig_v(v) for clean inputs and encourage Ev(v⊕δ) to match E_orig_v(v) for perturbed inputs. Training uses intermediate EDPA patches (reset periodically) and optimizes Ev while keeping LVLM and language encoder fixed, integrating updated Ev back into the VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Same encoder architecture as victim but weights updated (fine-tuned) adversarially; attack/defense operates in encoder embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Defensive fine-tuning is applied to pre-existing visual encoder (pretrained/fine-tuned on LIBERO or pretraining dataset); paper uses original Ev as reference E_orig_v.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Defense operates by stabilizing or re-aligning image patch embeddings so that the LVLM receives similar embeddings for clean and adversarially perturbed images, thus preserving correct grounding between visual tokens and language tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Embedding-level (latent patch-token alignment preserved between clean and perturbed inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit; defense does not add explicit spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Robotic manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (LIBERO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Failure Rate (FR) after defense under various attacks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Adversarial fine-tuning reduces OpenVLA average FR by ~34.2% against EDPA (e.g., Spatial EDPA: FR reduced from 100.0% to 39.4% ±1.0), reduces FR by 21.5% against random noise baseline, and by 19.1% and 36.0% for UADA and UPA respectively. Clean-condition FR impact is small (~+1.6% FR increase).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>No explicit 'grounding removed' ablation; the defense ablation evaluates hyperparameters (α2 balancing clean vs adversarial embedding consistency, patch reset frequency φ) and reports sensitivity results in appendix but does not isolate grounding removal.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Adversarial fine-tuning of Ev significantly improves preservation of correct grounding under adversarial patches (reduces EDPA-induced FR by ~34.2% on OpenVLA on average) with small clean performance degradation (~1.6% FR increase).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Defense directly addresses the identified bottleneck that grounding uses fragile image patch embeddings; by enforcing embedding consistency between clean and attacked inputs the encoder becomes less sensitive to localized patch perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>After defense, EDPA and other patch attacks are less effective but not eliminated; some failure cases persist, particularly in multi-camera alignment-limited scenarios and where patches occlude crucial objects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Defense is a fine-tuning procedure on downstream data (LIBERO) using adversarial patches generated during training—this is a localized domain-robustness technique rather than a general domain adaptation algorithm. It improves robustness under both white-box and transferred attack settings according to reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>This defense specifically fine-tunes the visual encoder while keeping LVLM fixed; results show improved robustness compared to original (frozen) encoder in the face of EDPA at small clean-cost.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Not directly evaluated; defense effectiveness is demonstrated empirically on finetuned models for LIBERO only.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Defense does not change fusion mechanism; it stabilizes the image patch embeddings so the existing LVLM cross-attention can ground correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Defense uses adversarial patches generated during EDPA optimization (intermediate patches) with batch-size 16 and T up to 50,000 iterations in experiments; no small-sample efficiency claims made.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Adversarial fine-tuning of the visual encoder to align embeddings of clean and patched views is an effective mitigation for attacks that target embedding-level grounding, substantially reducing failure rates while preserving most clean-task performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1975.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1975.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UADA/UPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UADA & UPA (Untargeted Action Discrepancy Attack & Untargeted Position-aware Attack)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two prior untargeted adversarial patch attacks (Wang et al., 2024) tailored to OpenVLA and a 7-DoF robotic arm that maximize discrepancy in action-token outputs or in position components of action vectors; used here as baselines and shown to be effective but less general than EDPA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UADA / UPA (baseline patch attacks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>UADA exploits numerical differences in OpenVLA language-model-derived action tokens to force maximal deviation from ground-truth action tokens; UPA directly optimizes the first three components of the decoded action vector (3D directional movements) to deviate from intended movements. Both are pixel/trajectory-targeted attacks that require full model/robot knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Pixel/patch-level adversarial patch methods; rely on full access to victim model to compute gradients to action/trajectory losses (not model-agnostic).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Not primarily designed to attack grounding; they operate on action-token / action-vector outputs exploiting mapping from LVLM outputs to robot control tokens, and thus are tightly coupled to OpenVLA’s decoding of language model tokens into actions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Operate on action-token / action-vector level and pixel-level perturbations; their objective is to maximize action discrepancy rather than directly disrupt image-language embedding alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>UPA explicitly manipulates the first three components of action vector (3D directional movement); attack requires knowledge of robot kinematics and action encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Robotic manipulation (7-DoF arm in prior study; evaluated here on LIBERO via OpenVLA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO (evaluated as baselines on OpenVLA in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (LIBERO) / prior OpenVLA experiments on robotic arm trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Failure Rate (FR) when attacks applied</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On OpenVLA (Table 2): UADA FRs are very high (e.g., Spatial UADA 98.9% FR original, reduced to 65.4% after adversarial finetuning), UPA FR similarly high (e.g., Spatial UPA 99.1% FR original). EDPA performance comparable or slightly stronger while being more model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>N/A (attacks). Compared to EDPA, UADA/UPA require stronger attacker assumptions (victim model and robot access) and are less generalizable across different VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>These attacks induce incorrect action trajectories leading to failures; their failure patterns are highly effective on OpenVLA but rely on white-box access and robot specifics, limiting transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not designed for transfer across models; paper notes UADA/UPA are difficult to transfer to other VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>UADA and UPA show that directly optimizing for erroneous action outputs is effective in white-box settings for OpenVLA, but because they do not target embedding-level grounding and require detailed model/robot information, they are less general than embedding-space attacks like EDPA.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1975.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1975.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AttentionVis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention visualization / analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attention-weight visualizations from language tokens to image patches (first and last layers) that demonstrate how adversarial patches shift linguistic attention toward the patch and away from task-relevant objects, providing direct evidence of grounding disruption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Attention visualization (OpenVLA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Paper computes averaged attention weights from linguistic tokens to primary-camera patch tokens across all heads in both first and last LVLM layers and visualizes attention maps under clean, random-noise patch, and EDPA conditions to reveal how grounding/attention changes.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Visualization relies on the patch-token based Ev used in OpenVLA; method inspects LVLM attention maps.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>This analysis inspects the cross-attention grounding pathway: language-token → image-patch attention distributions to reveal where grounding focuses under different perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Attention over patch-token level; visualization of token-to-token attention.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D image patch grid attention maps (no explicit 3D used).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Robotic manipulation / instruction following (diagnostic visualization on LIBERO examples)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LIBERO (example episodes used for visualization)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation (LIBERO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative attention maps; diagnostic for Failure Rate causes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not a numeric performance metric; demonstrates that under EDPA, language tokens' attentions concentrate on patch region rather than true objects, correlating with high FRs (OpenVLA EDPA → ~100% FR).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Visualization shows that with no perturbation or random noise, attention correctly highlights robot arm and salient objects; under EDPA attention concentrates on patch, indicating grounding disruption. No numeric 'without grounding' performance other than FRs reported elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Visualization provides direct evidence that adversarial patches hijack language-to-vision attention and thereby break grounding—this is identified as a key perception bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Attention hijacking is directly visualized and temporally persistent from early to last LVLM layers; correlates with incorrect action generation and task failure.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention inside LVLM as inspected via attention weights.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Attention-weight visualizations provide causal evidence that EDPA causes language tokens to mis-attend to adversarial patches, thereby disrupting the model's visual-language grounding and leading to high task failure rates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openvla: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>π0: A vision-language-action flow model for general robot control <em>(Rating: 2)</em></li>
                <li>Exploring the adversarial vulnerabilities of vision-language-action models in robotics <em>(Rating: 2)</em></li>
                <li>Libero: Benchmarking knowledge transfer for lifelong robot learning <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1975",
    "paper_id": "paper-282102125",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (vision-language-action model)",
            "brief_description": "An open-source VLA model that encodes images into token-level patch embeddings and language into token embeddings, concatenates them and feeds them to an LVLM backbone to autoregressively produce action tokens decoded to robot actions; used as the primary victim model for attack and defense experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "Visual encoder Ev produces a sequence of d-dimensional image patch embeddings [p1..pN]; language encoder Et produces token embeddings [w1..wM]; both are concatenated and processed by an LVLM backbone f(·) (large vision-language model) which outputs action tokens mapped to robot actions. Grounding is implemented via joint processing of image patch embeddings and language token embeddings inside the LVLM, with attention from language tokens over image patches.",
            "visual_encoder_type": "Patch-based visual encoder producing image patch embeddings (Ev); specific architecture not specified in paper (described abstractly as patch embeddings projected into LVLM input space).",
            "visual_encoder_pretraining": "Not precisely specified here; paper indicates OpenVLA's visual encoder was primarily trained/fine-tuned on third-person robotic datasets for LIBERO downstream tasks (pretraining details referenced to OpenVLA prior work, not given numerically in this paper).",
            "grounding_mechanism": "Concatenation of image patch embeddings and language token embeddings processed by an LVLM backbone; language tokens attend to image patch embeddings (cross-attention-like mechanism inside LVLM). Paper operationalizes semantic grounding in embedding space via cosine-similarity alignment between image patch embeddings and language token embeddings.",
            "representation_level": "Token-level image patch embeddings (patch-level, i.e., multi-patch token representation).",
            "spatial_representation": "Implicit only (no explicit 3D coordinates or depth maps used); multi-camera inputs (primary third-person and wrist camera) are supported in some variants but exact spatial coordinate representations are not used/derived within grounding.",
            "embodied_task_type": "Instruction-conditioned robotic manipulation / instruction following",
            "embodied_task_name": "LIBERO benchmark (task suites: Spatial, Object, Goal, Long)",
            "visual_domain": "Photorealistic simulation (LIBERO simulator); primary = third-person camera views and optional wrist (egocentric) camera views",
            "performance_metric": "Failure Rate (FR; = 1 - Success Rate) averaged across trials",
            "performance_value": "Original OpenVLA under EDPA: near 100% failure rate on LIBERO task suites in white-box setting (e.g., EDPA: 100.0% FR on Spatial/Object/Goal reported); under random-noise patches ~34.8%–74.9% FR depending on suite; after adversarial finetuning of visual encoder FR reduced substantially (examples: Spatial EDPA FR reduced to 39.4% ±1.0; Object EDPA FR reduced to 58.6% ±0.6; Goal EDPA FR reduced to 73.9% ±1.1; overall average reduction ~34.2% against EDPA).",
            "has_grounding_ablation": false,
            "performance_without_grounding": "",
            "grounding_improvement": "Not applicable (no direct ablation isolating grounding component); adversarial fine-tuning of visual encoder (defense) reduces EDPA-induced failure by ~34.2% on average for OpenVLA.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper compares OpenVLA (single-camera) to OpenVLA-OFT and π0 (multi-camera variants / different pretraining). OpenVLA is the least robust to EDPA; OpenVLA-OFT and π0 show lower failure rates under the same attacks (π0 notably more robust, attributed to wrist-camera data included during pretraining).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper identifies overfitting of the visual encoder to the robotic arm appearance (robot arm present in most training images) and attention hijacking by adversarial patches (patch attracts disproportionate attention at expense of task-relevant objects). Also notes inability to align observations across multiple moving camera viewpoints in real time as a limitation for multi-camera attack optimization.",
            "failure_mode_analysis": "Key failure modes: (1) attention hijacking — adversarial patch draws linguistic-token attention toward patch location and away from objects/robot arm (visualized in first and last attention layers); (2) occlusion — patch can occlude important objects leading to failures; (3) overfitting to arm appearance — models memorize arm features and are sensitive to perturbations that mimic arm structure. Quantitatively, EDPA drives OpenVLA to ~100% FR in white-box; OpenVLA-OFT FR increases by ~62.0% on average and π0 by ~31.4% under EDPA (paper reports per-suite FRs in Table 3).",
            "domain_shift_handling": "Defense: adversarial fine-tuning of the visual encoder to align adversarially perturbed and clean latent representations (minimize L2 between E_v(v⊕δ) and original E_v(v)); π0 handles domain shift better due to inclusion of diverse wrist-camera data in pretraining. No explicit domain-adaptation layer beyond finetuning; transferability experiments (cross-dataset and cross-model) show EDPA transfers moderately well.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Paper evaluates finetuned downstream VLA variants (OpenVLA, OpenVLA-OFT, π0) and also presents adversarial finetuning of the visual encoder; adversarial finetuning yields robustness gains against patches with only a small degradation under clean conditions (~+1.6% FR increase under clean for OpenVLA). No explicit frozen-vs-finetuned encoder numeric comparison is provided for general pretraining.",
            "pretraining_scale_effect": "Paper hypothesizes that small scale of robotic task datasets and lack of diverse viewpoints cause visual-encoder overfitting to robot arm appearance; no quantitative scaling curve is provided.",
            "fusion_mechanism": "Early fusion by concatenation of image patch embeddings and language token embeddings into the LVLM input; cross-attention inside LVLM allows language tokens to attend over image patch tokens.",
            "sample_efficiency": null,
            "key_findings_grounding": "OpenVLA-style grounding uses patch-token embeddings + LVLM cross-attention; this grounding is fragile to localized adversarial patches that distort the image–instruction embedding alignment and hijack attention; multi-view inputs (wrist + primary) and pretraining that includes diverse camera views reduce fragility; adversarially fine-tuning the visual encoder to align adversarial and clean embeddings is an effective mitigation with modest cost to clean performance.",
            "uuid": "e1975.0"
        },
        {
            "name_short": "OpenVLA-OFT",
            "name_full": "OpenVLA-OFT (OpenVLA with OFT variant / multi-camera fine-tuning)",
            "brief_description": "A finetuned variant of OpenVLA that incorporates wrist-camera inputs (multi-camera setting) and/or different fine-tuning; evaluated for robustness to adversarial patches and shown to be more robust than the base OpenVLA in many cases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenVLA-OFT",
            "model_description": "Same high-level LVLM architecture as OpenVLA: Ev produces patch embeddings, Et produces language token embeddings; model ingests multi-camera inputs (primary and wrist camera) each encoded by Ev and concatenated into LVLM. Grounding realized by LVLM attention across concatenated multi-view patch tokens and language tokens.",
            "visual_encoder_type": "Patch-based visual encoder (same family as OpenVLA) — pre-existing OpenVLA encoder fine-tuned with extra data (OFT variant); not specified to a concrete backbone in this paper.",
            "visual_encoder_pretraining": "Not fully specified here; paper notes OpenVLA-OFT is based on OpenVLA and was fine-tuned with wrist-camera data for LIBERO task suites (fine-tuning details referenced to prior work).",
            "grounding_mechanism": "Concatenation of multi-camera image patch embeddings with language tokens processed by LVLM; cross-attention enables language to ground over multi-view image patches.",
            "representation_level": "Patch-token level per camera; multi-view token concatenation (multi-level only in the sense of multiple camera views).",
            "spatial_representation": "Implicit spatial cues only; multi-camera observations provide additional viewpoint information but not explicit 3D coordinate grounding in paper.",
            "embodied_task_type": "Instruction-conditioned robotic manipulation",
            "embodied_task_name": "LIBERO benchmark (Spatial, Object, Goal, Long)",
            "visual_domain": "Simulation (LIBERO) with primary third-person and wrist camera (egocentric) views",
            "performance_metric": "Failure Rate (FR)",
            "performance_value": "Under EDPA (white-box patches applied independently to each camera): reported FRs for OpenVLA-OFT in Table 3 — Spatial: 39.7% ±0.9, Object: 52.3% ±0.8, Goal: 80.8% ±0.4, Long: 86.4% ±1.97 (clean FRs are very low: e.g., clean Spatial FR 1.4% ±0.4). EDPA increases FR strongly but less than OpenVLA.",
            "has_grounding_ablation": false,
            "performance_without_grounding": "",
            "grounding_improvement": "Multi-camera input and diverse fine-tuning reduce susceptibility: compared to single-camera OpenVLA, OpenVLA-OFT exhibits substantially lower FR under transferred and cross-dataset attacks (paper reports ~62% average increase for OpenVLA-OFT under EDPA vs. larger increases for OpenVLA).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "OpenVLA-OFT is more robust than OpenVLA; still less robust than π0 in several suites. Improvement attributed to additional wrist-camera fine-tuning but limited because base OpenVLA encoder had already overfitted.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper suggests OpenVLA-OFT inherits some pretraining overfitting from OpenVLA's visual encoder; even with wrist-camera fine-tuning, residual overfitting and patch-induced attention shifts remain failure sources.",
            "failure_mode_analysis": "EDPA applied to each camera increases FR substantially; multi-camera views mitigate but do not eliminate attention-hijacking and occlusion failure modes. Cross-camera alignment inability limits attack optimization (limitation for attacker) and also constrains some defense dynamics.",
            "domain_shift_handling": "OpenVLA-OFT uses additional fine-tuning on wrist-camera views for LIBERO; this helps but if encoder pretraining overfitted to third-person views, robustness gains are limited.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "OpenVLA-OFT is a finetuned variant; paper indicates finetuning with wrist-camera data improves robustness relative to base OpenVLA but does not provide a simple frozen-versus-finetuned numeric breakdown within this work.",
            "pretraining_scale_effect": "Paper discusses that limited scale/diversity of robot datasets can cause encoder overfitting; OpenVLA-OFT partly mitigates this via additional data but residual effects remain.",
            "fusion_mechanism": "Concatenation of multi-camera patch tokens with language tokens; LVLM cross-attention over concatenated tokens.",
            "sample_efficiency": null,
            "key_findings_grounding": "Adding wrist-camera views and fine-tuning reduces vulnerability by providing more diverse visual evidence for grounding, but if the base visual encoder was pre-trained on narrow third-person data, fragility remains; multi-view grounding reduces but does not eliminate attention hijacking by adversarial patches.",
            "uuid": "e1975.1"
        },
        {
            "name_short": "π0",
            "name_full": "π0 (pi-zero: vision-language-action flow model)",
            "brief_description": "A VLA model trained with multi-camera data (including wrist-camera during pretraining) that exhibits comparatively stronger robustness to EDPA adversarial patches, attributed to more diverse pretraining and multi-view grounding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "π0",
            "model_description": "Vision-language-action flow model that encodes visual inputs (multi-camera) into patch tokens and language into token embeddings; LVLM backbone processes concatenated multi-view visual tokens + language tokens to produce action tokens. Pretraining incorporates wrist-camera views leading to increased visual diversity.",
            "visual_encoder_type": "Patch-token visual encoder (architecture details inherited from π0 prior work; not specified in this paper), trained on multi-view/ wrist-camera data",
            "visual_encoder_pretraining": "Paper states π0 incorporated wrist-camera data during pretraining (increasing diversity) — exact datasets/scale not specified here (referenced to π0 original paper).",
            "grounding_mechanism": "Concatenated multi-view patch-token + language token inputs to LVLM; cross-attention inside LVLM grounds language to visual patches across multiple camera views.",
            "representation_level": "Patch-token (multi-camera concatenated) representation level",
            "spatial_representation": "Implicit multi-view information used; no explicit 3D coordinate representation described in this paper.",
            "embodied_task_type": "Instruction-conditioned manipulation",
            "embodied_task_name": "LIBERO benchmark (Spatial, Object, Goal, Long)",
            "visual_domain": "Simulation (LIBERO) with both primary and wrist camera views",
            "performance_metric": "Failure Rate (FR)",
            "performance_value": "Under EDPA applied per-camera (Table 3): π0 FRs — Spatial: 29.8% ±1.6, Object: 39.5% ±1.7, Goal: 44.3% ±2.0, Long: 70.7% ±1.6 (clean FRs: Spatial 3.5% ±0.3, Object 2.3% ±0.5, Goal 12.0% ±1.6, Long 40.8% ±1.6). π0 shows lower relative increases in FR under EDPA compared to OpenVLA and OpenVLA-OFT.",
            "has_grounding_ablation": false,
            "performance_without_grounding": "",
            "grounding_improvement": "π0's pretraining with wrist-camera data appears to improve grounding robustness vs single-camera OpenVLA; EDPA-induced FR increases are smaller for π0 than for OpenVLA.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "π0 is the most robust among the evaluated VLA variants in this paper, attributed to wrist-camera inclusion during pretraining and thus more diverse visual representations that reduce overfitting to robotic-arm appearance.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "π0 is less affected by robot-arm overfitting, but still shows substantial FR increases on long-horizon tasks (Long suite) under EDPA—indicating that other perception and long-horizon planning challenges remain.",
            "failure_mode_analysis": "EDPA still raises FR notably on long-horizon tasks for π0 (e.g., Long FR up to ~70.7% under attack), indicating that multi-view grounding mitigates but does not remove vulnerability, and that long-horizon tasks remain more brittle.",
            "domain_shift_handling": "π0's inclusion of wrist-camera views during pretraining is presented as an effective strategy to handle view-distribution shift between pretraining and downstream robot observations.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "π0 is provided as a model fine-tuned for LIBERO in experiments; no explicit frozen vs fine-tuned encoder comparison is reported in this paper.",
            "pretraining_scale_effect": "Paper attributes π0's improved robustness qualitatively to more diverse pretraining (multi-view) rather than larger scale per se; no numerical pretraining-scale ablation provided.",
            "fusion_mechanism": "Multi-view patch-token concatenation fed to LVLM; language-to-visual grounding via LVLM attention over multi-view tokens.",
            "sample_efficiency": null,
            "key_findings_grounding": "Pretraining that includes diverse camera viewpoints (wrist-camera) produces visual encoders that ground language more robustly under adversarial patch perturbations; nonetheless long-horizon tasks still show vulnerability, indicating remaining grounding/perception bottlenecks.",
            "uuid": "e1975.2"
        },
        {
            "name_short": "EDPA",
            "name_full": "Embedding Disruption Patch Attack (EDPA)",
            "brief_description": "A model-agnostic adversarial patch attack that optimizes a physicalizable patch to disrupt semantic alignment between visual patch embeddings and language token embeddings and to maximize embedding deviation between clean and perturbed visual inputs; requires only access to visual encoder parameters (not LVLM or robot platform).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "EDPA (adversarial patch attack)",
            "model_description": "Generates a universal adversarial patch δ by maximizing a weighted sum of two embedding-space objectives: (1) image–instruction alignment disruption loss (L_patch) that reduces semantic alignment between Ev(v) and Ev(v⊕δ) with respect to Et(t), and (2) patch contrastive loss (L_align) that maximizes discrepancy between Ev(v) and Ev(v⊕δ) measured relative to language embeddings. The patch is optimized via gradient steps on δ given access to the visual encoder Ev parameters. The generated patch is placed physically in camera view (applied via mask) to create v⊕δ.",
            "visual_encoder_type": "Attack requires access to the visual encoder Ev parameters (patch-token producing encoder); the attack is agnostic to exact encoder architecture but optimizes in that encoder's embedding space.",
            "visual_encoder_pretraining": "Not required for attack design; attacker uses available Ev parameters (either victim encoder or accessible base model) — the paper evaluates cross-model transfer when only base model is available.",
            "grounding_mechanism": "EDPA explicitly targets vision-language grounding by disrupting the semantic alignment between image patch embeddings and language token embeddings (i.e., attacking the embedding-level grounding used by LVLM). It leverages cosine-similarity based alignment and InfoNCE-inspired contrastive objectives to cause mis-grounding.",
            "representation_level": "Embedding-level (latent patch-token embeddings) — attacks internal representation used for grounding rather than pixel-level loss on action tokens.",
            "spatial_representation": "Patch is spatially localized (50×50 px default) applied via a 2D mask to image (top-left corner in visualizations); attack does not use explicit 3D spatial reasoning.",
            "embodied_task_type": "Robotic manipulation / instruction following",
            "embodied_task_name": "LIBERO benchmark (Spatial, Object, Goal, Long)",
            "visual_domain": "Simulation (LIBERO)",
            "performance_metric": "Failure Rate (FR) increase induced by patch",
            "performance_value": "EDPA substantially increases FR: OpenVLA (white-box) FR ≈ 100% across suites; OpenVLA-OFT average FR increases ≈ 62.0%; π0 average FR increases ≈ 31.4% (paper reports per-suite FRs in Tables 2 and 3). Compared with random-noise patches, EDPA increases average FR by ~50.5% (OpenVLA-OFT) and ~26.5% (π0) respectively.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Paper includes ablations related to EDPA objectives: varying α1 (weighting between L_patch and L_align) and patch size. EDPA remains effective across α1 values on OpenVLA, but π0 is more sensitive to α1 (higher α1 =&gt; stronger L_patch contribution increases EDPA effectiveness on π0). Larger patch sizes systematically increase FR for all models (tested sizes: 2%, 4%, 8%, 10% of image area).",
            "grounding_improvement": "N/A (this is an attack). The ablations show that attacks that disrupt image–instruction alignment (L_patch) are especially effective on some VLA models; both loss terms contribute to attack performance depending on the target model.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "EDPA is transferable across models and datasets: cross-dataset transfer (patches generated on LIBERO-Spatial applied to other suites) still causes large FR increases (e.g., OpenVLA increases ~74.7% on average), and cross-model transfer (patches generated on base model then applied to downstream variant) yields reduced but still significant FR increases (e.g., OpenVLA ~49.8% avg FR increase, OpenVLA-OFT ~26.98%, π0 ~9.3% across suites).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "EDPA exploits that grounding occurs at embedding level and that encoders over-attend to repeated visual elements (robot arm); by perturbing embeddings the attack causes attention shifts and mis-grounding. Multi-camera misalignment and occlusion are noted as limitations that also shape attack effectiveness.",
            "failure_mode_analysis": "Failure modes produced by EDPA: complete task failure (OpenVLA ~100% FR), attention hijacking (tokens concentrate on patch), and misinterpreted instructions leading to incorrect action generation. Sensitivity analyses show larger patch sizes and particular α1 weighting amplify these failures.",
            "domain_shift_handling": "EDPA remains effective under cross-dataset and cross-model transfer, indicating attacks found on one dataset or base model generalize to downstream variants though with reduced potency; attack generation does not require downstream task data in many cases.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "EDPA evaluated against finetuned downstream models (OpenVLA, OpenVLA-OFT, π0) and remains effective, though potency differs by target model pretraining/fine-tuning; adversarially fine-tuned encoders are more robust to EDPA.",
            "pretraining_scale_effect": "EDPA effectiveness varies with pretraining diversity: models pretrained with more diverse views (π0) are less susceptible; paper links limited robotic dataset scale to encoder overfitting which EDPA exploits.",
            "fusion_mechanism": "Targets embedding-level alignment of concatenated image patch tokens and language tokens inside LVLM (cross-attention pathway), rather than explicit fusion-layer manipulation.",
            "sample_efficiency": null,
            "key_findings_grounding": "Attacking embedding-space grounding (disrupting image–instruction alignment and maximizing embedding drift) is a highly effective, model-agnostic means to break VLA grounding in embodied tasks; attacks transfer across datasets and models to varying degrees and are mitigated by encoder robustness (e.g., multi-view pretraining, adversarial finetuning).",
            "uuid": "e1975.3"
        },
        {
            "name_short": "Adv-Finetune",
            "name_full": "Adversarial fine-tuning of visual encoder (defense)",
            "brief_description": "A defense that fine-tunes the visual encoder using adversarially perturbed images (patches from EDPA) to encourage the encoder to produce similar embeddings for clean and adversarial inputs, while preserving embeddings for clean inputs via a consistency regularizer to the original encoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adversarial fine-tuning (visual encoder)",
            "model_description": "Fine-tune Ev to minimize a weighted sum of L2 reconstruction losses: keep Ev(v) close to E_orig_v(v) for clean inputs and encourage Ev(v⊕δ) to match E_orig_v(v) for perturbed inputs. Training uses intermediate EDPA patches (reset periodically) and optimizes Ev while keeping LVLM and language encoder fixed, integrating updated Ev back into the VLA.",
            "visual_encoder_type": "Same encoder architecture as victim but weights updated (fine-tuned) adversarially; attack/defense operates in encoder embedding space.",
            "visual_encoder_pretraining": "Defensive fine-tuning is applied to pre-existing visual encoder (pretrained/fine-tuned on LIBERO or pretraining dataset); paper uses original Ev as reference E_orig_v.",
            "grounding_mechanism": "Defense operates by stabilizing or re-aligning image patch embeddings so that the LVLM receives similar embeddings for clean and adversarially perturbed images, thus preserving correct grounding between visual tokens and language tokens.",
            "representation_level": "Embedding-level (latent patch-token alignment preserved between clean and perturbed inputs).",
            "spatial_representation": "Implicit; defense does not add explicit spatial representations.",
            "embodied_task_type": "Robotic manipulation / instruction following",
            "embodied_task_name": "LIBERO benchmark",
            "visual_domain": "Simulation (LIBERO)",
            "performance_metric": "Failure Rate (FR) after defense under various attacks",
            "performance_value": "Adversarial fine-tuning reduces OpenVLA average FR by ~34.2% against EDPA (e.g., Spatial EDPA: FR reduced from 100.0% to 39.4% ±1.0), reduces FR by 21.5% against random noise baseline, and by 19.1% and 36.0% for UADA and UPA respectively. Clean-condition FR impact is small (~+1.6% FR increase).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "No explicit 'grounding removed' ablation; the defense ablation evaluates hyperparameters (α2 balancing clean vs adversarial embedding consistency, patch reset frequency φ) and reports sensitivity results in appendix but does not isolate grounding removal.",
            "grounding_improvement": "Adversarial fine-tuning of Ev significantly improves preservation of correct grounding under adversarial patches (reduces EDPA-induced FR by ~34.2% on OpenVLA on average) with small clean performance degradation (~1.6% FR increase).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Defense directly addresses the identified bottleneck that grounding uses fragile image patch embeddings; by enforcing embedding consistency between clean and attacked inputs the encoder becomes less sensitive to localized patch perturbations.",
            "failure_mode_analysis": "After defense, EDPA and other patch attacks are less effective but not eliminated; some failure cases persist, particularly in multi-camera alignment-limited scenarios and where patches occlude crucial objects.",
            "domain_shift_handling": "Defense is a fine-tuning procedure on downstream data (LIBERO) using adversarial patches generated during training—this is a localized domain-robustness technique rather than a general domain adaptation algorithm. It improves robustness under both white-box and transferred attack settings according to reported results.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "This defense specifically fine-tunes the visual encoder while keeping LVLM fixed; results show improved robustness compared to original (frozen) encoder in the face of EDPA at small clean-cost.",
            "pretraining_scale_effect": "Not directly evaluated; defense effectiveness is demonstrated empirically on finetuned models for LIBERO only.",
            "fusion_mechanism": "Defense does not change fusion mechanism; it stabilizes the image patch embeddings so the existing LVLM cross-attention can ground correctly.",
            "sample_efficiency": "Defense uses adversarial patches generated during EDPA optimization (intermediate patches) with batch-size 16 and T up to 50,000 iterations in experiments; no small-sample efficiency claims made.",
            "key_findings_grounding": "Adversarial fine-tuning of the visual encoder to align embeddings of clean and patched views is an effective mitigation for attacks that target embedding-level grounding, substantially reducing failure rates while preserving most clean-task performance.",
            "uuid": "e1975.4"
        },
        {
            "name_short": "UADA/UPA",
            "name_full": "UADA & UPA (Untargeted Action Discrepancy Attack & Untargeted Position-aware Attack)",
            "brief_description": "Two prior untargeted adversarial patch attacks (Wang et al., 2024) tailored to OpenVLA and a 7-DoF robotic arm that maximize discrepancy in action-token outputs or in position components of action vectors; used here as baselines and shown to be effective but less general than EDPA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "UADA / UPA (baseline patch attacks)",
            "model_description": "UADA exploits numerical differences in OpenVLA language-model-derived action tokens to force maximal deviation from ground-truth action tokens; UPA directly optimizes the first three components of the decoded action vector (3D directional movements) to deviate from intended movements. Both are pixel/trajectory-targeted attacks that require full model/robot knowledge.",
            "visual_encoder_type": "Pixel/patch-level adversarial patch methods; rely on full access to victim model to compute gradients to action/trajectory losses (not model-agnostic).",
            "visual_encoder_pretraining": "",
            "grounding_mechanism": "Not primarily designed to attack grounding; they operate on action-token / action-vector outputs exploiting mapping from LVLM outputs to robot control tokens, and thus are tightly coupled to OpenVLA’s decoding of language model tokens into actions.",
            "representation_level": "Operate on action-token / action-vector level and pixel-level perturbations; their objective is to maximize action discrepancy rather than directly disrupt image-language embedding alignment.",
            "spatial_representation": "UPA explicitly manipulates the first three components of action vector (3D directional movement); attack requires knowledge of robot kinematics and action encoding.",
            "embodied_task_type": "Robotic manipulation (7-DoF arm in prior study; evaluated here on LIBERO via OpenVLA)",
            "embodied_task_name": "LIBERO (evaluated as baselines on OpenVLA in this paper)",
            "visual_domain": "Simulation (LIBERO) / prior OpenVLA experiments on robotic arm trajectories",
            "performance_metric": "Failure Rate (FR) when attacks applied",
            "performance_value": "On OpenVLA (Table 2): UADA FRs are very high (e.g., Spatial UADA 98.9% FR original, reduced to 65.4% after adversarial finetuning), UPA FR similarly high (e.g., Spatial UPA 99.1% FR original). EDPA performance comparable or slightly stronger while being more model-agnostic.",
            "has_grounding_ablation": false,
            "performance_without_grounding": "",
            "grounding_improvement": "N/A (attacks). Compared to EDPA, UADA/UPA require stronger attacker assumptions (victim model and robot access) and are less generalizable across different VLA models.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "These attacks induce incorrect action trajectories leading to failures; their failure patterns are highly effective on OpenVLA but rely on white-box access and robot specifics, limiting transferability.",
            "domain_shift_handling": "Not designed for transfer across models; paper notes UADA/UPA are difficult to transfer to other VLA models.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "",
            "pretraining_scale_effect": "",
            "fusion_mechanism": "",
            "sample_efficiency": null,
            "key_findings_grounding": "UADA and UPA show that directly optimizing for erroneous action outputs is effective in white-box settings for OpenVLA, but because they do not target embedding-level grounding and require detailed model/robot information, they are less general than embedding-space attacks like EDPA.",
            "uuid": "e1975.5"
        },
        {
            "name_short": "AttentionVis",
            "name_full": "Attention visualization / analysis",
            "brief_description": "Attention-weight visualizations from language tokens to image patches (first and last layers) that demonstrate how adversarial patches shift linguistic attention toward the patch and away from task-relevant objects, providing direct evidence of grounding disruption.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Attention visualization (OpenVLA)",
            "model_description": "Paper computes averaged attention weights from linguistic tokens to primary-camera patch tokens across all heads in both first and last LVLM layers and visualizes attention maps under clean, random-noise patch, and EDPA conditions to reveal how grounding/attention changes.",
            "visual_encoder_type": "Visualization relies on the patch-token based Ev used in OpenVLA; method inspects LVLM attention maps.",
            "visual_encoder_pretraining": "",
            "grounding_mechanism": "This analysis inspects the cross-attention grounding pathway: language-token → image-patch attention distributions to reveal where grounding focuses under different perturbations.",
            "representation_level": "Attention over patch-token level; visualization of token-to-token attention.",
            "spatial_representation": "2D image patch grid attention maps (no explicit 3D used).",
            "embodied_task_type": "Robotic manipulation / instruction following (diagnostic visualization on LIBERO examples)",
            "embodied_task_name": "LIBERO (example episodes used for visualization)",
            "visual_domain": "Simulation (LIBERO)",
            "performance_metric": "Qualitative attention maps; diagnostic for Failure Rate causes",
            "performance_value": "Not a numeric performance metric; demonstrates that under EDPA, language tokens' attentions concentrate on patch region rather than true objects, correlating with high FRs (OpenVLA EDPA → ~100% FR).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Visualization shows that with no perturbation or random noise, attention correctly highlights robot arm and salient objects; under EDPA attention concentrates on patch, indicating grounding disruption. No numeric 'without grounding' performance other than FRs reported elsewhere.",
            "grounding_improvement": "",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Visualization provides direct evidence that adversarial patches hijack language-to-vision attention and thereby break grounding—this is identified as a key perception bottleneck.",
            "failure_mode_analysis": "Attention hijacking is directly visualized and temporally persistent from early to last LVLM layers; correlates with incorrect action generation and task failure.",
            "domain_shift_handling": "",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "",
            "pretraining_scale_effect": "",
            "fusion_mechanism": "Cross-attention inside LVLM as inspected via attention weights.",
            "sample_efficiency": null,
            "key_findings_grounding": "Attention-weight visualizations provide causal evidence that EDPA causes language tokens to mis-attend to adversarial patches, thereby disrupting the model's visual-language grounding and leading to high task failure rates.",
            "uuid": "e1975.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openvla: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "π0: A vision-language-action flow model for general robot control",
            "rating": 2
        },
        {
            "paper_title": "Exploring the adversarial vulnerabilities of vision-language-action models in robotics",
            "rating": 2
        },
        {
            "paper_title": "Libero: Benchmarking knowledge transfer for lifelong robot learning",
            "rating": 2
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 1
        }
    ],
    "cost": 0.02447,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MODEL-AGNOSTIC ADVERSARIAL ATTACK AND DE-FENSE FOR VISION-LANGUAGE-ACTION MODELS
15 Oct 2025</p>
<p>Haochuan Xu 
The University of Auckland</p>
<p>Yun Sing Koh 
The University of Auckland</p>
<p>Shuhuai Huang 
The University of Auckland</p>
<p>Zirun Zhou 
The University of Auckland</p>
<p>Di Wang 
King Abdullah University of Science and Technology</p>
<p>Jun Sakuma 
Tokyo University of Science</p>
<p>RIKEN Center for Advanced Intelligence Project</p>
<p>Jingfeng Zhang jingfeng.zhang@auckland.ac.nz 
The University of Auckland</p>
<p>King Abdullah University of Science and Technology</p>
<p>RIKEN Center for Advanced Intelligence Project</p>
<p>MODEL-AGNOSTIC ADVERSARIAL ATTACK AND DE-FENSE FOR VISION-LANGUAGE-ACTION MODELS
15 Oct 20253585EDE36496075E9B956CFFC9F65695arXiv:2510.13237v1[cs.CV]
Vision-Language-Action (VLA) models have achieved revolutionary progress in robot learning, enabling robots to execute complex physical robot tasks from natural language instructions.Despite this progress, their adversarial robustness remains underexplored.In this work, we propose both adversarial patch attack and corresponding defense strategies for VLA models.We first introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic adversarial attack that generates patches directly placeable within the camera's view.In comparison to prior methods, EDPA can be readily applied to different VLA models without requiring prior knowledge of the model architecture, or the controlled robotic manipulator.EDPA constructs these patches by (i) disrupting the semantic alignment between visual and textual latent representations, and (ii) maximizing the discrepancy of latent representations between adversarial and corresponding clean visual inputs.Through the optimization of these objectives, EDPA distorts the VLA's interpretation of visual information, causing the model to repeatedly generate incorrect actions and ultimately result in failure to complete the given robotic task.To counter this, we propose an adversarial fine-tuning scheme for the visual encoder, in which the encoder is optimized to produce similar latent representations for both clean and adversarially perturbed visual inputs.Extensive evaluations on the widely recognized LIBERO robotic simulation benchmark demonstrate that EDPA substantially increases the task failure rate of cutting-edge VLA models, while our proposed defense effectively mitigates this degradation.The codebase is accessible via the homepage at https://edpa-attack.github.io/.</p>
<p>INTRODUCTION</p>
<p>Vision-Language-Action (VLA) models (Zitkovich et al., 2023;Team et al., 2024;Kim et al., 2024;Black et al., 2024) built on vision-language foundation models (Touvron et al., 2023;Beyer et al., 2024;Achiam et al., 2023;Liu et al., 2023b) have recently emerged to enable robots to perform complex physical tasks from high-level instructions.Through integrating vision and language understanding, VLAs leverage powerful perceptual and reasoning abilities, allowing robots to generalize to previously unseen environments (Zitkovich et al., 2023).</p>
<p>As the increasing attention on VLA models, concerns about their reliability become particularly urgent, since failures in VLAs deployed on physical robotic platforms can lead to tangible consequences such as robots mishandling objects and resulting in property damage or performing incorrect actions that endanger human safety.Adversarial robustness (Carlini et al., 2019;Goodfellow et al., 2014;Madry et al., 2017) has long been recognized as a critical security challenge in computer vision.Given the VLA models reliance on visual inputs captured from camera, these models are likely to inherit similar vulnerabilities.While adversarial robustness has been extensively investigated in traditional deep learning models, its implications for VLA models remain largely underexplored.</p>
<p>A recent study (Wang et al., 2024) highlighted the vulnerability of VLA models to adversarial attacks.The authors proposed several adversarial patch methods, each employing loss functions tailored to the robotic arm's action trajectory for the OpenVLA (Kim et al., 2024) model controlling a 7-degree-of-freedom (DoF) robotic arm (Zitkovich et al., 2023).Their experiments showed that OpenVLA exhibited almost no resistance to such attacks.However, these attacks depend on stringent requirements: the attacker must have prior knowledge of the victim model, and full access to all model parameters to compute gradients for generating adversarial patches.These constraints substantially limit the practicality of the attacks in real-world scenarios (see Section 2.2).</p>
<p>To address this limitation, we propose the Embedding Disruption Patch Attack (EDPA), designed to generate adversarial patches that disrupt a VLA's interpretation of visual information.In contrast to prior attacks, EDPA requires only access to the VLA's encoder parameters and does not rely on knowledge of the VLA's architecture or the controlled robot platform.EDPA optimizes patches with two complementary objectives: (i) disrupting the semantic alignment between the visual latent representation and the corresponding instruction's language latent representation, and (ii) maximizing the deviation between the latent representations of adversarial and corresponding clean visual inputs.Through jointly optimizing these objectives, EDPA produces adversarial patches that markedly distort visual understanding in VLAs, leading to a substantial reduction in the success rate of robotic tasks across the latest VLA models.</p>
<p>In addition, we introduce a complementary adversarial fine-tuning scheme for the visual encoder to enhance the robustness of VLA models against such attacks.Specifically, all adversarial patches generated during the EDPA optimization process are applied to construct adversarial visual samples, which are then used to fine-tune the visual encoder.This method encourages the encoder to produce latent representations for adversarial visual inputs that match those of the corresponding clean inputs, while simultaneously ensuring that the fine-tuned encoder preserves performance for clean inputs by maintaining latent representations similar to those produced by the original encoder.Due to our experimental results showed that OpenVLA exhibited the weakest robustness against EDPA, it was chosen as the primary model for defense evaluation.The results demonstrate that adversarial fine-tuning not only strengthens OpenVLA's resistance to EDPA but also significantly improves its robustness against previously proposed untargeted adversarial patch attacks (Wang et al., 2024).</p>
<p>RELATED WORK</p>
<p>VLA FOR EMBODIMENTS</p>
<p>The concept of embodied AI was introduced by Machinery (1950) to examine whether agents can demonstrate intelligence through interaction with and navigation in complex physical environments, rather than just solving abstract problems in the digital world.In the early stages of developing generalist robots, prevailing approaches (Silva et al., 2021;Nair et al., 2022;Jang et al., 2022;Brohan et al., 2023) primarily relied on reinforcement learning or traditional imitation learning paradigms to acquire task-specific policies.</p>
<p>In recent years, the advent of Large-scale Vision-Language Models (LVLMs) has shifted the paradigm toward leveraging these models to enhance generalization and language grounding in embodied agents, enabling robots to execute tasks directly from natural language instructions.Generalist robotic models such as RT-2 (Zitkovich et al., 2023), Octo (Team et al., 2024), OpenVLA (Kim et al., 2024), and π 0 (Black et al., 2024), typically built upon LVLMs and commonly referred to as Vision-Language-Action (VLA) models, demonstrate strong generalization across diverse scenes and tasks, facilitating effective transfer to previously unseen scenarios.</p>
<p>ADVERSARIAL ROBUSTNESS IN VLA</p>
<p>Adversarial robustness is a fundamental challenge in securing deep learning models in computer vision.It concerns the resilience of these models to malicious input, known as adversarial attacks.Although primitive adversarial attacks (Goodfellow et al., 2014;Madry et al., 2017;Croce &amp; Hein, 2020;Carlini &amp; Wagner, 2017) demonstrate strong effectiveness in deceiving models through im-Figure 1: Overview of OpenVLA architecture and patch attack requirements.Given a visual observation and a language instruction, the OpenVLA model first encodes the inputs into token-level latent representations.These representations are processed by the LVLM to produce action tokens, which are subsequently decoded into executable actions for the robotic platform.The colored dashed lines highlight the prior knowledge and/or access required by different patch attacks for various modules within the VLA: green for EDPA, purple for UADA, and red for UPA.perceptible pixel-level additive noise, they are often impractical in physical-world applications, as adversaries typically have limited access to the resources (Sharma et al., 2022) (e.g., adversary may not be able to directly modify the pixel of image).In this context, adversarial patch attacks (Brown et al., 2017;Karmon et al., 2018;Liu et al., 2018;Li et al., 2019) focus on manipulating a contiguous region of an image with perceptible but implementable perturbations (e.g., printed as stickers).Given that VLA models incorporate visual input, their adversarial robustness raises concerns analogous to those observed in other computer vision tasks, but studies in this field remain limited.</p>
<p>Category</p>
<p>Requirement UADA (Wang et al., 2024) UPA (Wang et al., 2024) EDPA (Ours)
Knowledge Victim Model ✓ ✗ ✗ Robotic Manipulator ✗ ✓ ✗ Access Encoder Parameters ✓ ✓ ✓ LVLM Parameters ✓ ✓ ✗
Table 1: Comparison of attack requirements between UADA, UPA, and EDPA.</p>
<p>To the best of our knowledge, Wang et al. (2024) conducted the first systematic study on the robustness of VLA models against adversarial patch attacks.They proposed two untargeted adversarial patch attacks to explore the adversarial robustness of VLA: the Untargeted Action Discrepancy Attack (UADA) and the Untargeted Position-aware Attack (UPA).These attacks generate adversarial patches that aim to cause OpenVLA to produce action trajectories that deviate from the intended trajectories when controlling a 7-DoF robotic arm.UADA exploits the fact that OpenVLA uses part of the language model's output tokens as action tokens that can be mapped to physical actions, and the differences in the numerical values of action tokens are correlated with differences in action magnitudes.Utilizing this property, UADA induces OpenVLA to output action tokens that deviate maximally from the ground-truth action tokens.In contrast, UPA directly operates on the first three components of the action vector.These components represent the 3D directional movements of the 7-DoF robotic arm, with UPA forcing OpenVLA to produce action vectors that deviate from the intended ones along these dimensions.</p>
<p>These attacks exhibit limited generality, as they are specifically tailored to the unique characteristics of OpenVLA and the 7-DoF robotic arm, making them difficult to transfer to other VLA models or embodied agents.Furthermore, their execution relies on stringent conditions: the attacker must possess prior knowledge of the victim model design and the robotic manipulator, as well as access to all model parameters to compute gradients.In comparison, our proposed EDPA can operate without detailed knowledge of the victim model or the controlled robotic manipulator, relying solely on access to the encoder parameters, which makes it more practical for real-world scenarios (see Table 1 and Figure 1).</p>
<p>METHODOLOGY 3.1 PRELIMINARIES</p>
<p>Vision-language Action Models.The architecture of SOTA VLA models built on top of largescaled LVLM commonly consist of three main components:</p>
<p>(1) A visual encoder E v (•) that transforms the visual input v (i.e., an image) captured by a camera into a sequence of image patch embeddings and projects them into the input space of the language model:
E v (v) = [ p 1 , p 2 , . . . , p N ], p i ∈ R d
, where p i denotes the d-dimensional embedding of the i-th image patch, and N is the total number of patches.</p>
<p>(2) A language encoder E t (•) that tokenizes the natural language instruction t into a sequence of textual tokens and encodes them into language tokens embeddings:
E t (t) = [ w 1 , w 2 , . . . , w M ], w j ∈ R d ,
where w j denotes the d-dimensional embedding of the j-th token, and M is the total number of tokens in the instruction.</p>
<p>(3) The LVLM backbone f (•) processes the concatenated image patch embeddings and language token embeddings to generate an action vector:
A = f E v (v), E t (t)
where A denotes the generated action vector can be executed by robot manipulator.</p>
<p>Adversarial Patch Attack.An adversarial patch attack is a specific type of adversarial attack in which the perturbation is applied to a localized area of the image, known as an adversarial patch.The patch is typically of a fixed shape (e.g., square or arbitrary) and can be randomly placed at any location within the image to mislead the model.</p>
<p>Formally, given a clean visual input v ∈ [0, 1] H×W ×C and an adversarial patch δ ∈ [0, 1] h×w×C , the adversarial patch attack applies the patch δ to v by replacing the pixel values within the patch region:
v ⊕ δ = (1 − p) ⊙ v + p ⊙ δ,(1)
where p is a binary mask indicating the patch's shape and location, and h and w are the height and width of the patch.The operator ⊙ denotes Hadamard element-wise multiplication.</p>
<p>EMBEDDING DISRUPTION PATCH ATTACK</p>
<p>A number of studies (Zhang et al., 2022;Zhao et al., 2023;Bagdasaryan et al., 2024;Lu et al., 2023;Zhang et al., 2025) have shown that adversarial attacks targeting embedding representations are generally highly effective against LVLMs.Since VLAs are built upon LVLMs, they likely inherit similar vulnerabilities.As these attacks typically rely on traditional pixel-level perturbations, directly applying them to VLAs that interact with the physical environment is impractical.Motivated by this insight, we propose an untargeted adversarial patch method, Embedding Disruption Patch Attack (EDPA), which specifically targets latent representations within VLA models.This method requires no access to the VLM backbone or prior knowledge of the model design and is agnostic to the type of robotic manipulator.The learning objective of the EDPA comprises two components: (1) disrupting the original semantic alignment between the image patch embeddings of v and the language token embeddings of the language instruction t, and (2) maximizing the discrepancy between the image patch embeddings of the clean visual input v and the adversarial visual input v ′ .To this end, we introduce the corresponding loss functions: the image-instruction alignment loss and the patch contrastive loss.</p>
<p>Image-Instruction Alignment Loss.The semantic alignment between visual and language information is crucial for a model to correctly understand and execute language instructions.We aim to introduce our generated adversarial patch to alter the alignment between the visual and language in the embedding space, thereby disrupting the model's perception of their semantic correspondence and interfering with its understanding and execution of the instruction.To quantify this effect, we define the image-instruction alignment loss, which measures the change in semantic alignment between the image patch embeddings and the language token embeddings of the corresponding language instruction.Formally, given a language instruction t corresponding to the visual input v, we measure the change in alignment between the image patch embeddings E v (v) and E v (v ′ ) with respect to the language token embeddings E t (t).To this end, we define the loss function as follow:
L patch (E v (v), E v (v ′ )) = − 1 N N i=1 log exp cos p i , p ′ i /τ N j=1 exp cos p i , p ′ j /τ ,(2)
where p i and p ′ i denote the i-th image patch embeddings in E v (v) and E v (v ′ ), respectively.Here, τ is a scalar hyperparameter, and cos(•, •) is the cosine similarity function.</p>
<p>Patch Contrastive Loss.However, some VLA models exhibit limited capability in understanding the alignment between visual and language information (Kim et al., 2025).Therefore, we also aim to directly use an adversarial patch to induce the visual encoder to generate latent representations that substantially deviate from those produced in the absence of the patch, thereby disrupting the model's understanding of essentially identical visual information and altering its outputs.To explicitly quantify this effect, we introduce the patch contrastive loss, which measures the discrepancy between the image patch embeddings of the clean and perturbed visual inputs.Specifically, given a clean input visual input v and its perturbed counterpart v ′ = v ⊕ δ, we measure the embedding deviation between E v (v) and E v (v ′ ).Inspired by the InfoNCE (Oord et al., 2018), we define the loss function as follows:
L align (E v (v), E v (v ′ ), E t (t)) = 1 N × M N i=1 M j=1 cos p i , w j − cos p ′ i , w j ,(3)
where w j denotes the j-th language token embedding in E t (t).</p>
<p>Adversarial Patch Generation.To construct a universal adversarial patch δ, we formulate an optimization problem that jointly maximizes the patch contrastive loss (equation 2) and the imageinstruction alignment loss (equation 3) as following:
δ * = arg max δ E v∼D [α 1 • L patch (E v (v), E v (v ⊕ δ)) + (1 − α 1 ) • L align (E v (v), E v (v ⊕ δ), E t (t))] ,(4)
where D is the data distribution of visual inputs and α 1 ∈ [0, 1] is a hyperparameter controlling the relative contributions of each loss function.In practice, we find this objective effective in constructing adversarial patches that degrade the ultimate performance of the VLA model.</p>
<p>Algorithm 1: Adversarial Finetuning on Visual Encoder</p>
<p>Input: Original visual encoder E orig v , language encoder E t , robotic dataset D, hyperparameter α 1 , hyperparameter α 2 , step size η δ , patch reset frequency φ, inner attack iterations K, learning rate η, max training iterations T Output:
visual encoder E * v Initialize E v ← E orig v , δ ∼ U (0, 1); for i = 1 to T do Sample minibatch (v, t) ⊂ D; if i mod φ = 0 then Reinitialize δ ∼ U (0, 1); for k = 1 to K do J ← α 1 • L patch (E v (v), E v (v ⊕ δ)) + (1 − α 1 ) • L align (E v (v), E v (v ⊕ δ), E t (t)); δ ← clip(δ + η δ • sign(∇ δ J ), 0, 1) L ← α 2 • ∥E v (v) − E orig v (v)∥ 2 2 + (1 − α 2 ) • ∥E v (v ⊕ δ) − E orig v (v)∥ 2 2
; Update E v by gradient descent with learning rate η; return E v ;</p>
<p>ADVERSARIAL FINETUNING ON VISUAL ENCODER</p>
<p>In the Section 3.2, we demonstrate that an effective adversarial patch δ can be derived by directly targeting the embedding space of the VLA.In this section, we present a complementary adversarial finetuning scheme aimed at improving the robustness of the visual encoder E v (•) within the VLA.</p>
<p>The finetuning scheme incorporates adversarial visual inputs constructed from adversarial patches δ generated by EDPA.Instead of relying solely on the final optimized δ, the training process utilizes all intermediate patches produced during the EDPA training.Throughout the process, the current δ is applied to the visual inputs to generate perturbed samples (refer to equation 1), which are then used to optimize the visual encoder.In addition, we adopt a fixed reset frequency for δ, periodically reinitializing the patch during training to prevent overfitting to a specific patch and to ensure that the visual encoder is exposed to a diverse set of adversarial patches.</p>
<p>The learning objective of the finetuning process mainly consists of two complementary objectives: (i) to encourage the fine-tuned visual encoder to produce latent representations of adversarially perturbed visual inputs that are close to those of the corresponding clean visual inputs produced by the original visual encoder, thereby enhancing the visual encoder's robustness against adversarial patches; and (ii) to ensure that the latent representations generated by the fine-tuned visual encoder on clean visual inputs remain consistent with those generated by the original visual encoder, thereby preserving fidelity on clean visual inputs.As a result, the fine-tuned visual encoder can be directly integrated into the VLA without any modification or further fine-tuning of the LVLM backbone, mitigating the impact of adversarial patches while preserving overall performance.</p>
<p>To formalize the learning objective of the finetuning scheme, let E orig v (•) denote the original visual encoder without being adversarial finetuned.The objective is to optimize the parameters of the updated visual encoder by solving the following optimization problem:
E * v = arg min Ev E v∼D α 2 • E v (v) − E orig v (v) 2 2 + (1 − α 2 ) • E v (v ⊕ δ) − E orig v (v) 2 2
(5)</p>
<p>where δ denotes an adversarial patch generated by EDPA during its training procedure, and α 2 ∈ [0, 1] controls the relative contributions of the two learning objectives.The pseudocode of our finetuning scheme is shown in Algorithm 1.</p>
<p>EXPERIMENT</p>
<p>EXPERIMENT SETTINGS</p>
<p>Dataset.The adversarial patch generation through EDPA are conduct on LIBERO (Liu et al., 2023a), a simulation dataset specifically designed for robotic manipulation.The datasets comprises four distinct task suites: Spatial, Object, Goal, and Long.</p>
<p>Victim Models.</p>
<p>We evaluate recent open-source VLA models, including OpenVLA Kim et al.</p>
<p>(2024), OpenVLA-OFT (Kim et al., 2025), and π 0 (Black et al., 2024), all of which provide finetuned variants for LIBERO.Specifically, OpenVLA and OpenVLA-OFT each offer separate finetuned models for individual task suites, whereas π 0 provides a single model fine-tuned across all task suites.</p>
<p>Baseline Method.Given the limited research in this domain, no existing baseline directly matches our experimental setting.The most relevant untargeted adversarial patch attacks are UADA and UPA.However, these attacks are difficult to transfer to models other than OpenVLA due to their stringent application requirements.Therefore, we compare EDPA with UADA and UPA in terms of attack performance only on the OpenVLA model, while also evaluating the effectiveness of our defense method against them.Details of both experiments are provided in Section 4.2.For general experiments, we use a random noise baseline following Wang et al. (2024), where patches are sampled from a Gaussian distribution N (0, 1) and evaluated under the same settings as EDPA.</p>
<p>Hyperparameter Settings.In all experiments, the sizes of both adversarial and noise patches are fixed at 50×50 pixels, following Wang et al. (2024), while VLAs commonly receive visual inputs at a resolution of 224×224.During the generation of adversarial patches with EDPA, the hyperparameter α 1 , which controls the relative contribution of each loss, is set to 0.8, and the number of inner attack iterations K is fixed at 1. Since the two losses may operate on different scales, exponential moving average (EMA) normalization is applied to each loss to ensure that α 1 accurately governs their relative contributions.The step size η δ is set to 2/255, and training proceeds for a maximum of T = 50,000 iterations with a batch size of 16.For the adversarial fine-tuning scheme, we set α 2 = 0.5 to balance the two learning objectives, and the patch reset frequency φ is set to 1000.The visual encoder is optimized using the Adam optimizer with a learning rate of η = 1 × 10 −5 .The sensitivity to some of these hyperparameter settings are reported in Appendix C.</p>
<p>Simulation and Evaluation Metric.We evaluate our methods on all four task suites of the LIBERO simulation benchmark, with each suite comprising 10 tasks and 50 executions per task.Performance is measured using the Failure Rate (FR) metric following Wang et al. (2024), representing the proportion of tasks that were not successfully completed after a certain number of steps (defined as Failure Rate = 1 -Success Rate).To account for stochastic variability, reported FRs are averaged over three experiments with different random seeds, following Kim et al. (2024).</p>
<p>EVALUATING ON SINGLE-CAMERA VLA</p>
<p>In this subsection, we focus on evaluating the performance of our attack and defense methods on a single-camera VLA.We adopt OpenVLA as the representative model, which relies solely on visual input from the primary camera.In our evaluation, we measure the performance of OpenVLA before and after adversarial fine-tuning of the visual encoder when subjected to various patch-based attacks in the libero simulation benchmark.</p>
<p>As shown in Table 2, the OpenVLA without adversarial fine-tuned visual encoder demonstrates almost no resilience to the adversarial patch attacks designed to mislead the model.Compared to common baselines, the adversarial patches generated via EDPA causes OpenVLA to increase its average failure rate on LIBERO tasks by approximately around 74.7% relative to the clean condition and by 53.0% relative to the random noise patch.For comparison with untargeted adversarial patch attacks specifically designed for OpenVLA, we also evaluated adversarial patches generated by UADA and UPA on the LIBERO dataset.The results show that UADA, UPA, and EDPA differ only marginally in effectiveness, as OpenVLA demonstrates minimal resilience against such adversarial patches.</p>
<p>We then evaluate OpenVLA models integrated with an adversarially fine-tuned visual encoder against patch attacks.The results in Table 2 demonstrate such models exhibit substantially re- 100.0 ± 0.0 91.2 ± 0.5 duced failure rates, with average decreases of 34.2% against EDPA and 21.5% against random noise patches in the LIBERO simulation environment.In parallel, the results demonstrate that adversarially fine-tuning the visual encoder not only mitigates the impact of EDPA but also confers improved robustness to OpenVLA against adversarial patches produced by other methods, with reductions in failure rates of 19.1% for UADA and 36.0%for UPA.Importantly, this improvement results in only a minor 1.6% increase in failure rate under clean conditions, reflecting the well-known trade-off between robustness and standard performance.</p>
<p>EVALUATING ON MULTI-CAMERA VLA</p>
<p>In this subsection, we evaluate the effectiveness of EDPA attacks on multi-camera VLAs.We use OpenVLA-OFT and π 0 as the victim models, both of which rely on visual inputs from the primary camera as well as the wrist camera.In contrast to the primary camera, the wrist camera's viewpoint changes substantially as the robot arm moves.Since real-time alignment of primary and wrist camera observations for the same patch is not feasible, we apply separate adversarial patches to each camera independently for evaluation.As UADA and UPA are difficult to transfer to models other than OpenVLA due to their stringent application requirements, we do not include them in the comparisons in this subsection.</p>
<p>As shown in Table 3, EDPA increases the average failure rate on LIBERO tasks by approximately 62.0% for OpenVLA-OFT and 31.4% for π 0 .Compared with random noise, EDPA induces markedly higher increases in average failure rates of around 50.5%, and 26.5% for OpenVLA-OFT, and π 0 , respectively.These results indicate that EDPA remains highly effective against other (SOTA) multi-camera VLA models.Although the differences between OpenVLA, OpenVLA-OFT, and π 0 extend beyond camera settings, the results suggest that VLAs processing multiple camera views may exhibit improved robustness to adversarial patches, potentially due to the additional visual information provided by multiple viewpoints.</p>
<p>PATCH VISUALIZATION AND DISCUSSION</p>
<p>The Figure 2 illustrates representative adversarial patches produced by various patch-based attack methods on the LIBERO dataset.Specifically, Figure 2a shows patches generated by EDPA on OpenVLA, OpenVLA-OFT, and π 0 , while Figure 2b displays patches produced by UADA and UPA on OpenVLA (Wang et al., 2024).An interesting observation is that all generated patches consistently exhibit structural patterns reminiscent of a robotic arm.In particular, EDPA patches targeting OpenVLA and OpenVLA-OFT more closely resemble a robotic manipulator than those targeting π 0 .</p>
<p>In light of these observations, we propose a hypothesis that slightly differs from Wang et al. (2024).</p>
<p>We posit that the visual encoder of VLA models overfits to the appearance of robotic arms.This overfitting may be attributed to two factors: (1) the datasets used for robotic learning are much smaller in scale compared to internet-scale datasets;</p>
<p>(2) the visual samples used for training are primarily captured from third-person camera viewpoints, which causes the robotic arm to appear in every visual sample and occupy a substantial portion of it.</p>
<p>This hypothesis is also supported by our experimental findings.OpenVLA-OFT and π 0 demonstrate greater robustness to EDPA compared to OpenVLA, likely because OpenVLA was trained primarily on visual input from the third-person camera.Although both OpenVLA-OFT and π 0 can process multiple camera views, π 0 exhibits superior robustness.This is likely because OpenVLA-OFT, even with wrist camera data added during fine-tuning, is based on the original OpenVLA model whose visual encoder had already overfitted during pretraining.In contrast, π 0 incorporates wrist camera data from the pretraining stage, increasing the diversity of visual training data and thereby mitigating overfitting.</p>
<p>LIMITATION</p>
<p>Despite the effectiveness of our methods, there are still some limitations that need to be acknowledged: (i) in multi-camera settings, we cannot compute the alignment of observations from different camera views for the same patch in real time.This limits EDPA's ability to optimize patches under conditions that fully reflect their physical observation, potentially reducing attack effectiveness; and</p>
<p>(ii) object position information cannot be directly obtained from static data, meaning the adversarial patch may occasionally occlude important objects.In such cases, our adversarial fine-tuning scheme on the visual encoder could potentially have a negative impact on the encoder's performance.</p>
<p>CONCLUSION</p>
<p>In this study, we investigated the robustness of VLA models against adversarial patch attacks, a critical yet underexplored threat to their reliability.We first introduced a novel patch generation method, targeting the latent representation space of VLA models.We then proposed an adversarial fine-tuning strategy to enhance VLA robustness against such attacks.Our empirical results reveal significant vulnerabilities in current SOTA models and demonstrate that the proposed defense can effectively mitigate these threats.We hope this work will inspire future research efforts toward developing robust and secure vision-language embodied agents capable of safe deployment in complex real-world environments.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>Following the details provided in the main text, we ensure the reproducibility of our results by providing the loss functions, pseudocode, and the specific hyperparameters used in our experiments.</p>
<p>Researchers can replicate our experiments and verify our findings using these descriptions and settings.While the results may not be exactly identical due to randomness factor in the experiments, they are expected to fall within a reasonable range.The codebase have released on github already.</p>
<p>ETHICS STATEMENT</p>
<p>This work demonstrates a potential security threat in VLAs: carefully crafted adversarial patches can significantly degrade the performance of a VLA when placed within the camera's view, causing the embodied agent to misinterpret visual information in the physical environment.This not only reduces the agent's task success rate but also results in unexpected movement trajectories during task execution, posing hazards such as object mishandling, property damage, or actions endangering human safety.In this work, we do provide a method for generating such adversarial patches.We acknowledge that similar methods could be misused maliciously, but our intention is to highlight potential vulnerabilities in VLAs and to encourage the development of defenses methods against such attacks.Our goal is to promote the creation of embodied AI systems that are more robust, secure, and reliable.regions within the image.However, when adversarial patches derived from EDPA are introduced, a clear change in the attention distribution emerges: the patch location receives disproportionately high attention, while focus on originally important objects is markedly diminished.</p>
<p>In summary, these visualizations demonstrate that adversarial patches generated by EDPA can substantially distort the attention distribution of linguistic tokens over the visual input, thereby undermining model performance.</p>
<p>B TRANSFERABILITY OF EDPA</p>
<p>In practice, an adversary may not have full access to the finetuned model or the dataset of downstream task.Here, we evaluate the transferability of EDPA under two scenarios: (1) cross-dataset transferability, where the adversary lacks access to downstream task data; and (2) cross-model transferability, where the adversary has access only to the victim's base model.</p>
<p>To evaluate cross-dataset transferability, we generate adversarial patches on LIBERO-Spatial and apply them to the other three task suites within the LIBERO simulation benchmark.As shown in Table 4, EDPA demonstrates strong dataset-level transferability, substantially increasing the average failure rates about 74.7%, 52.4%, and 33.7% for OpenVLA, OpenVLA-OFT, and π 0 , respectively.Interestingly, the attack performance of EDPA is comparable to that observed under fully white-box settings, suggesting that EDPA requires minimal knowledge of the training data and that an adver-  71.9 ± 0.9 sary can effectively compromise VLA performance even without access to data from the targeted task.</p>
<p>In parallel, we examine the cross-model transferability of EDPA in a scenario where the adversary has no access to the victim model but can leverage its corresponding base model to generate adversarial patches, which are subsequently transferred to the downstream variant.As summarized in Table 5, these transferred patches increase the average failure rates around 49.8%, 26.98%, and 9.3% for OpenVLA, OpenVLA-OFT, and π 0 , respectively, evaluated across the four task suites within the LIBERO simulation benchmark.These findings indicate that, although the cross-model transferability of EDPA is generally lower than that observed under fully white-box settings or in cross-dataset  In our primary experiments, we fix the patch size at 50 × 50 pixels, consistent with prior work, to enable a fair comparison with baseline.In this section, we further investigate how varying the size of adversarial patches generated by EDPA influences the performance of VLA models.In our experimental setup, we generate adversarial patches using EDPA with sizes determined as 2%, 4%, 8%, and 10% of the total visual observation area, study their impact on different VLA model performance.We report the average failure rate across four task suites in the LIBERO simulation benchmark for each model under different patch sizes in Figure 5.</p>
<p>The experimental results demonstrate that the performance of all VLA models is consistently affected by the size of adversarial patches.As the patch size increases, the failure rate of all models in executing robotic tasks also rises.This trend suggests that larger adversarial patches lead to stronger degradation of the model's performance.</p>
<p>C.2 ANALYZING THE IMPACT OF α 1 ON EDPA'S EFFECTIVENESS IN VLAS</p>
<p>To investigate how the hyperparameter α 1 influences the effectiveness of EDPA, we conduct a series of experiments by systematically varying α 1 while keeping all other settings fixed.This hyperparameter controls the relative contribution of the two loss functions during patch optimization and is expected to impact the failure rate of VLAs in robotic task execution.In the main experiments, we set α 1 to 0.8.We further evaluate EDPA with α 1 set to 0, 0.2, 0.5, 0.8, and 1 to examine how different trade-offs between these objectives affect the performance of various VLAs in the LIBERO simulation benchmark.The results are summarized in Figure 6.EDPA generates effective adversarial patches for OpenVLA across all α 1 settings, indicating that both loss terms contribute to attacks that the model cannot resist.On OpenVLA-OFT, the highest failure rate occurs at α 1 = 0.2, but variations in α 1 do not result in substantial differences in attack performance.However, the failure rate of π 0 increases clearly with higher α 1 , suggesting that a larger contribution of L patch enhances the effectiveness of EDPA on this model.In summary, the effectiveness of EDPA varies across different VLA models.</p>
<p>The sensitivity of each model to the hyperparameter α 1 differs, suggesting that the two loss functions have varying effectiveness depending on the target VLA.</p>
<p>D LARGE LANGUANGE MODEL USAGE STATEMENT</p>
<p>The large language model was only used to polish the language during the preparation of this manuscript.Specifically, we used the well-known LLM ChatGPT1 for this purpose.The model was not used to generate any technical content, ideas, or analyses.</p>
<p>(a) EDPA patches on OpenVLA, OpenVLA-OFT, and π0.(b) Patches generated through UADA and UPA on OpenVLA.</p>
<p>Figure 2 :
2
Figure 2: Visualization of Patch.All the patches are generated on LIBERO dataset.</p>
<p>Figure 3 :
3
Figure 3: Average attention weights of each linguistic token to the primary camera input in the first layer of OpenVLA.</p>
<p>Figure 4 :
4
Figure 4: Average attention weights of each linguistic token to the primary camera input in the last layer of OpenVLA.</p>
<p>Figure 5 :
5
Figure 5: Impact of patch size.The figure shows how varying patch sizes of EDPA affect the average failure rate (FR) across different VLA models on the LIBERO simulation benchmark.</p>
<p>Figure 6 :
6
Figure 6: Impact of hyperparameter α 1 .The figure illustrates how varying α 1 affects the performance of EDPA in terms of average failure rate (FR) across different VLA models on the LIBERO simulation benchmark.</p>
<p>Table 2 :
2
Attack and defense performance on OpenVLA.Average failure rates (FR) of OpenVLA models across four task suites in the LIBERO benchmark under different attacks, reported before and after adversarial fine-tuning.
SourceMethodFailure Rate (FR ↑)OriginalAdversarial FinetunedClean14.1 ± 0.517.9 ± 0.8Random34.8 ± 1.119.4 ± 1.4SpatialUADA98.9 ± 0.165.4 ± 1.0UPA99.1 ± 0.346.6 ± 1.0EDPA (Ours)100.0 ± 0.039.4 ± 1.0Clean12.0 ± 0.417.3 ± 0.7Random39.2 ± 1.416.0 ± 0.9ObjectUADA92.5 ± 0.758.8 ± 1.4UPA92.1 ± 0.843.9 ± 1.4EDPA (Ours)100.0 ± 0.058.6 ± 0.6Clean26.9 ± 1.522.8 ± 0.4Random37.9 ± 0.723.0 ± 1.1GoalUADA98.6 ± 0.191.6 ± 0.4UPA98.9 ± 0.268.3 ± 1.7EDPA (Ours)100.0 ± 0.073.9 ± 1.1Clean48.1 ± 1.949.0 ± 0.3Random74.9 ± 2.450.2 ± 0.5LongUADA99.6 ± 0.297.4 ± 0.4UPA99.6 ± 0.386.7 ± 0.9EDPA (Ours)</p>
<p>Table 3 :
3
Attack performance on Other VLAs.The average failure rates (FR) of various finetuned VLA models on the four task suites in the LIBERO simulation benchmark under different perturbation levels.
SourceMethodFailure Rate (FR) ↑OpenVLA-OFTπ 0Clean1.4 ± 0.43.5 ± 0.3SpatialRandom8.1 ± 2.14.0 ± 0.9EDPA39.7 ± 0.929.8 ± 1.6Clean2.0 ± 0.02.3 ± 0.5ObjectRandom15.9 ± 0.44.3 ± 0.3EDPA52.3 ± 0.839.5 ± 1.7Clean2.8 ± 0.712.0 ± 1.6GoalRandom5.1 ± 0.117.5 ± 1.3EDPA80.8 ± 0.444.3 ± 2.0Clean4.9 ± 0.640.8 ± 1.6LongRandom28.1 ± 2.451.9 ± 0.8EDPA86.4 ± 1.970.7 ± 1.6</p>
<p>Table 4 :
4SourceFailure Rate (FR)OpenVLAOpenVLA-OFTπ 0Object100.0 ± 0.032.2 ± 1.030.3 ± 1.2Goal100.0 ± 0.071.8 ± 0.942.7 ± 2.2Long100.0 ± 0.061.4 ± 1.6
The average failure rates (FR) of different fine-tuned VLA models on the other three task suites in the LIBERO simulation benchmark on EDPA adversarial patches derived from LIBERO-Spatial.</p>
<p>Table 5 :
5
The average failure rates (FR) of different fine-tuned VLA models on the four task suites in the LIBERO simulation benchmark on EDPA adversarial patches derived from the base model.
SourceFailure Rate (FR)OpenVLAOpenVLA-OFTπ 0Spatial71.5 ± 1.412.3 ± 0.67.1 ± 0.3Object69.4 ± 1.731.6 ± 1.610.0 ± 1.4Goal67.4 ± 1.732.8 ± 0.222.7 ± 0.8Long91.9 ± 1.842.3 ± 3.255.8 ± 1.1transfer scenarios, it nonetheless maintains significant attack effectiveness against downstream VLAvariants.C ABLATION STUDY
C.1 IMPACT OF ADVERSARIAL PATCH SIZE ON VLA PERFORMANCE</p>
<p>https://chatgpt.com
APPENDIX A ATTENTION VISUALIZATIONTo gain deeper insight into how EDPA disrupts the performance of VLAs, we examine the average attention weights from linguistic tokens to visual input locations.Specifically, we use OpenVLA as the victim model and visualize the averaged attention weights across all heads in both the first and last layers under three perturbation conditions: clean, random, and EDPA.Here, clean denotes unperturbed inputs, random corresponds to patches generated from Gaussian noise, and EDPA refers to adversarial patches produced by our method (Section 3.2).To isolate their effects, all perturbation patches are fixed at the top-left corner of the visual input.In Figure3, we present the first-layer average attentions of OpenVLA for three sampled linguistic tokens with respect to their corresponding image regions.For clean samples, the tokens correctly attend to the robot arm and other salient objects visible in the camera view.Under random perturbations, the attention distributions remain largely stable.In contrast, when adversarial patches generated by EDPA are applied, the first-layer attentions shift dramatically: the tokens concentrate predominantly on the patch location, while their focus on the originally relevant objects and the robot arm is markedly reduced.Figure4shows that this phenomenon also persists in the final attention layer of OpenVLA.Compared to the first layer, the linguistic tokens in the last layer exhibit more localized focus on specific
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Adversarial illusions in {Multi-Modal} embeddings. Eugene Bagdasaryan, Rishi Jha, Vitaly Shmatikov, Tingwei Zhang, 33rd USENIX Security Symposium (USENIX Security 24). 2024</p>
<p>Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, arXiv:2407.07726A versatile 3b vlm for transfer. 2024arXiv preprint</p>
<p>π0: A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, ARXIV.2410.2416420242024arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on robot learning. PMLR2023</p>
<p>. Dandelion Tom B Brown, Aurko Mané, Martín Roy, Justin Abadi, Gilmer, arXiv:1712.096652017Adversarial patch. arXiv preprint</p>
<p>Towards evaluating the robustness of neural networks. Nicholas Carlini, David Wagner, 2017 ieee symposium on security and privacy (sp). Ieee2017</p>
<p>Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, Alexey Kurakin, arXiv:1902.06705On evaluating adversarial robustness. 2019arXiv preprint</p>
<p>Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. Francesco Croce, Matthias Hein, International conference on machine learning. PMLR2020</p>
<p>Explaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, arXiv:1412.65722014arXiv preprint</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>Lavan: Localized and visible adversarial noise. Danny Karmon, Daniel Zoran, Yoav Goldberg, International conference on machine learning. PMLR2018</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Fine-tuning vision-language-action models: Optimizing speed and success. Jin Moo, Chelsea Kim, Percy Finn, Liang, arXiv:2502.196452025arXiv preprint</p>
<p>Adversarial camera stickers: A physical camerabased attack on deep learning systems. Juncheng Li, Frank Schmidt, Zico Kolter, International conference on machine learning. PMLR2019</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone, Advances in Neural Information Processing Systems. 2023a36</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2023b36</p>
<p>Dpatch: An adversarial patch attack on object detectors. Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, Yiran Chen, arXiv:1806.022992018arXiv preprint</p>
<p>Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models. Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, Feng Zheng, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Computing machinery and intelligence-am turing. Computing Machinery, Mind. 592364331950</p>
<p>Towards deep learning models resistant to adversarial attacks. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, arXiv:1706.060832017arXiv preprint</p>
<p>Learning languageconditioned robot behavior from offline data and crowd-sourced annotation. Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>Representation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.037482018arXiv preprint</p>
<p>Adversarial patch attacks and defences in vision-based tasks: A survey. Abhijith Sharma, Yijun Bian, Phil Munz, Apurva Narayan, arXiv:2206.083042022arXiv preprint</p>
<p>Lancon-learn: Learning with language to enable generalization in multi-task manipulation. Andrew Silva, Nina Moorman, William Silva, Zulfiqar Zaidi, Nakul Gopalan, Matthew Gombolay, IEEE Robotics and Automation Letters. 722021</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Exploring the adversarial vulnerabilities of vision-language-action models in robotics. Taowen Wang, Cheng Han, James Chenhao Liang, Wenhao Yang, Dongfang Liu, Luna Xinyu Zhang, Qifan Wang, Jiebo Luo, Ruixiang Tang, arXiv:2411.135872024arXiv preprint</p>
<p>Towards adversarial attack on vision-language pre-training models. Jiaming Zhang, Qi Yi, Jitao Sang, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022</p>
<p>Anyattack: Towards large-scale self-supervised adversarial attacks on visionlanguage models. Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Yunhao Chen, Jitao Sang, Dit-Yan Yeung, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>On evaluating adversarial robustness of large vision-language models. Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Man Ngai-Man, Min Cheung, Lin, Advances in Neural Information Processing Systems. 202336</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Conference on Robot Learning. PMLR2023</p>            </div>
        </div>

    </div>
</body>
</html>
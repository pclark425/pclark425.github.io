<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6812 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6812</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6812</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-271745051</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.03350v3.pdf" target="_blank">miniCTX: Neural Theorem Proving with (Long-)Contexts</a></p>
                <p><strong>Paper Abstract:</strong> Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce miniCTX, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for miniCTX, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as miniF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting and annotating theorem proving data, making it easy to add new projects into miniCTX to ensure that contexts are not seen during training. miniCTX offers a challenging and realistic evaluation of neural theorem provers.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6812.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6812.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (API-based OpenAI model as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art API-based transformer LLM used in the paper for both full-proof generation (few-shot) and context-augmented prompting; evaluated on generating complete Lean proofs (pass@8) with/without file context and retrieved premises.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer language model (API service); exact architecture and parameter count not specified in the paper; used via few-shot prompting to generate full proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper (pretrained on broad web/code corpora typical of OpenAI models); no task-specific fine-tuning reported in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Full-proof generation via few-shot prompting (8 examples) with optional appended preceding file context and/or retrieved premises; generation evaluated by verifying produced proofs in Lean.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Generated proofs are checked using the Lean verifier (Lean REPL) during evaluation, but GPT-4o is not integrated with a theorem-proving tool during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniCTX (primary), miniF2F (also evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniCTX: benchmark of theorems from real Lean projects with long in-file and cross-file context; miniF2F: competition-style standalone problems not requiring extra context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Full-proof generation (generate complete Lean proof), evaluated also on context-dependent proving (in-file and cross-file premises).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@8 proof success rate (percentage of problems for which at least one of 8 generated proofs verifies in Lean)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Without preceding file context: 11.72% (avg on miniCTX-test); With preceding file context: 27.08% (avg on miniCTX-test); With context + retrieved premises: 26.82% (avg on miniCTX-test). Also miniF2F: 13.52% (full-proof few-shot baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Providing preceding file context improves GPT-4o by +15.36 percentage points absolute (11.72% -> 27.08%) on miniCTX; premise selection helps on high cross-file-dependency splits (improves per-split performance) but does not uniformly improve average performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conditioning GPT-4o on in-file context dramatically increases proof success; retrieved cross-file premises can help on splits with high cross-file dependency but can also interfere when excessive or mismatched.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Absolute performance remains low despite context; sensitive to length and size of retrieved premises (long premises relative to context harm success); relies on external Lean automation tactics in many proofs (see automation reliance experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'miniCTX: Neural Theorem Proving with (Long-)Contexts', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6812.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6812.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llemma-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llemma-7b (math-specialized open model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open model specialized for mathematical tasks, evaluated here as the state-tactic prompting model that outputs a tactic given a proof state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llemma: An open language model for mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llemma-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter transformer LLM specialized for mathematics (as cited); used here for state-to-tactic prompting (generate next tactic from proof state).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; the cited Llemma work describes math-focused pretraining/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>State-tactic prompting: model is prompted with a proof state and asked to output the next tactic; search (best-first) composes tactics into full proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Tactics proposed by the model are executed/checked using Lean; best-first search is used to assemble tactic sequences into complete proofs within the proof assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniCTX (primary), miniF2F (also evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniCTX tests context-dependent theorem proving (in-file and cross-file dependencies); miniF2F contains standalone competition problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Tactic prediction and proof search (state -> tactic repeated to build full proof).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof success rate (percentage of problems solved by the tactic-prediction + search pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>State-tactic prompting baseline: average 14.58% on miniCTX-test; on miniF2F prompting achieves 28.28% (per Table 3 rows).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Prompting (no fine-tuning) performs worse than state-tactic fine-tuning (+~4.95 percentage points when fine-tuned on state-tactic pairs: 14.58% -> 19.53% average).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>State-to-tactic prompting can produce useful tactics but benefits substantially from fine-tuning on (state, tactic) pairs; vanilla prompting is weaker on context-dependent problems that require new definitions/lemmas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As typically framed, state-tactic models are unaware of new file-level context unless that context appears in the proof state; they struggle with cross-file/context-level generalization unless augmented with retrieval or re-training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'miniCTX: Neural Theorem Proving with (Long-)Contexts', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6812.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6812.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-Coder-1.3b (state-tactic tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-Coder-1.3b fine-tuned for state-tactic prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.3B-parameter code-oriented transformer LLM used as the fine-tuning base for state-tactic prediction; fine-tuned on human-extracted (proof state, tactic) pairs from Mathlib using NTP-TOOLKIT and evaluated with best-first search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-coder: When the large language model meets programming -the rise of code intelligence.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Coder-1.3b (state-tactic tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>1.3B transformer model pre-trained for programming/code tasks and further fine-tuned on extracted (state, tactic) pairs for tactic prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on Mathlib-derived (state, tactic) pairs extracted with NTP-TOOLKIT (307k extraction from a 2023 snapshot reported; training splits: 583k train examples for full-tuning pipelines referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>State-tactic fine-tuning: model maps proof states to next-tactic predictions; best-first search constructs full proofs at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Tactic actions are executed and verified in Lean; evaluation and search operate within the Lean REPL environment.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniCTX (primary), miniF2F</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniCTX: context-dependent proofs from real Lean projects; miniF2F: standalone competition problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Tactic prediction & proof search (state -> tactic repeated)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof success rate (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>State-tactic tuned DeepSeek-Coder-1.3b: average 19.53% on miniCTX-test; 32.79% on miniF2F (per Table 3 row 'State-tactic tuning').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Fine-tuning improves over prompting (state-tactic prompting 14.58% -> tuning 19.53% avg on miniCTX); however, file-tuning yields much larger gains on context-dependent problems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning on (state,tactic) pairs improves tactic generation; still limited on context-dependent theorems that require leveraging new file-level definitions/lemmas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lacks mechanism to consume long preceding file context at inference time; performance degrades on problems that require novel context-level generalization; relies on Lean automation tactics for many solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'miniCTX: Neural Theorem Proving with (Long-)Contexts', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6812.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6812.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>File-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>File-tuning (fine-tuning on (state, context, tactic) triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training methodology that fine-tunes a language model to generate tactics conditioned not only on the proof state but also on preceding file contents (in-file context), enabling context-dependent theorem proving over long repository contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>File-tuned DeepSeek-Coder-1.3b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeepSeek-Coder-1.3b fine-tuned on (proof state, preceding file contents, next tactic) triples extracted by NTP-TOOLKIT; training uses random truncation strategies to handle long files.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Human-written (state, context, tactic) triples extracted from Mathlib and other Lean projects via NTP-TOOLKIT (paper cites extraction counts and training splits: 583k train, 15k dev, 15k test across their extraction pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Context-conditioned tactic generation: model synthesizes tactics using both local proof state and preceding file context (definitions, lemmas, comments); best-first search composes tactics into proofs. Long-context handling via truncation to token budgets (random truncation strategy: middle or preceding 1024 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Generated tactics are executed and validated in the Lean REPL; NTP-TOOLKIT extracts contexts and premises; optional premise retrieval (LeanDojo retriever) can be appended to context.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniCTX (primary), miniF2F (comparative)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniCTX emphasizes context-dependent theorem proving from real Lean projects with long in-file/cross-file dependencies; miniF2F contains standalone competition problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Context-aware tactic prediction and proof generation (uses full preceding file content as input).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof success rate (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>File-tuned model average on miniCTX-test: 35.94% (per Table 3 'File tuning' avg); on miniF2F the gain is small: 33.61% vs the state-tactic tuned 32.79%. On certain splits (e.g., PNT) much larger per-split gains observed (see table and text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially outperforms state-tactic tuned baseline on miniCTX (19.53% -> 35.94%, +16.41 percentage points), while improving only slightly on miniF2F (32.79% -> 33.61%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>File-tuning enables models to use new in-file definitions, lemmas, and previous proofs to materially improve context-dependent theorem proving performance; helps across difficulty levels and longer contexts; models can learn proof patterns from previous proofs in the file.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Current implementation truncates contexts to token budgets (1024) which discards long-range information; inconsistent gains when combined with premise selection (cross-file premises) and can be harmed by large retrieved premise blocks; limited improvement on very challenging proofs (e.g., long PFR proofs); performance drops when Lean automation tactics are disabled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'miniCTX: Neural Theorem Proving with (Long-)Contexts', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6812.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6812.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Premise selection (LeanDojo retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Premise selection via LeanDojo retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval component that identifies and returns the most relevant definitions/lemmas from imported modules (top-20) to augment a model's context for cross-file dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LeanDojo: Theorem proving with retrieval-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LeanDojo premise retriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embedding-based premise retriever (as used in LeanDojo) that ranks candidate premises from imported modules and returns top-k relevant items to append to the model input context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>embedding-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Repository lemmas and definitions (module-level corpora); specifics come from LeanDojo implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Retrieval-augmented reasoning: retrieved premises are appended to in-file context so the generator can use cross-file lemmas during proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Retriever provides premises that are then verified/executed within the Lean REPL; integration is via context concatenation before model input.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniCTX (used to simulate cross-file context)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniCTX includes metadata for imported modules to support premise retrieval; PFR cross split intentionally contains many cross-file premises to evaluate this mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Cross-file premise retrieval to support formal proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Impact on proof success rate (percentage) when premises are appended to model input</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Effect is mixed: for GPT-4o, premise selection improves performance on high cross-file-dependency splits (e.g., PFR, PFR cross, SciLean); for the file-tuned model, appending retrieved premises does not consistently improve and can worsen performance on some splits (see Table 3 and analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Helps GPT-4o on high cross-file-dependency splits; does not consistently help and can interfere with file-tuned model performance (sometimes reduces proof success).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Premise retrieval can capture useful cross-file context and help certain models and splits, but naive concatenation of large retrieved premise blocks can overwhelm or distract models from in-file context and harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Retrieved premises can be long relative to the remainder of the context and harm attention/focus; mismatches between retrieved premises and in-file context can disrupt proof generation; better integration strategies are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'miniCTX: Neural Theorem Proving with (Long-)Contexts', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6812.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6812.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automation tactic reliance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reliance on Lean automation tactics (e.g., simp, linarith)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that models often rely on Lean's powerful automation tactics to discharge routine reasoning steps; disabling these tactics causes substantial performance drops in tasks requiring explicit step-by-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but an observed behavior/limitation across evaluated models: many generated proofs invoke automation tactics rather than explicit symbolic reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Models often exploit availability of automation tactics in Lean to produce short proofs; when automation is disabled (as in Math2001 dataset), models must produce explicit step-by-step reasoning which is harder.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Automation tactics (simp, linarith) are part of the Lean environment and are invoked by generated proofs; disabling them in evaluation shows reliance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math2001 (textbook-style split used for the analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Math2001 is a Lean textbook dataset where common automation tactics are disabled for pedagogical reasons, requiring explicit proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Full-proof generation under constraints (automation disabled) / explicit step-by-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative drop in proof success rate when automation tactics are disabled</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Substantial performance drops observed when common automation tactics are disabled (exact percentages not reproduced in the main text; reported as large in Table 6 and analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>When automation is enabled, models obtain higher success rates by invoking tactics; disabling automation reveals much lower explicit-reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Current LLM-based provers depend heavily on automated tactics â€” disabling them uncovers a significant gap in explicit symbolic/logical reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Shows that current successes can reflect reliance on proof assistant automation rather than explicit logical step-by-step reasoning; progress needed to improve fine-grained symbolic reasoning without automation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'miniCTX: Neural Theorem Proving with (Long-)Contexts', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative language modeling for automated theorem proving <em>(Rating: 2)</em></li>
                <li>LeanDojo: Theorem proving with retrieval-augmented language models <em>(Rating: 2)</em></li>
                <li>Llemma: An open language model for mathematics <em>(Rating: 2)</em></li>
                <li>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs <em>(Rating: 2)</em></li>
                <li>Baldur: Whole-proof generation and repair with large language models <em>(Rating: 2)</em></li>
                <li>miniF2F: a cross-system benchmark for formal olympiad-level mathematics <em>(Rating: 1)</em></li>
                <li>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. <em>(Rating: 2)</em></li>
                <li>Graph2tac: Online representation learning of formal math concepts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6812",
    "paper_id": "paper-271745051",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (API-based OpenAI model as used in experiments)",
            "brief_description": "A state-of-the-art API-based transformer LLM used in the paper for both full-proof generation (few-shot) and context-augmented prompting; evaluated on generating complete Lean proofs (pass@8) with/without file context and retrieved premises.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Large pre-trained transformer language model (API service); exact architecture and parameter count not specified in the paper; used via few-shot prompting to generate full proofs.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": "Not specified in paper (pretrained on broad web/code corpora typical of OpenAI models); no task-specific fine-tuning reported in this work.",
            "reasoning_method": "Full-proof generation via few-shot prompting (8 examples) with optional appended preceding file context and/or retrieved premises; generation evaluated by verifying produced proofs in Lean.",
            "external_tool_used": false,
            "external_tool_description": "Generated proofs are checked using the Lean verifier (Lean REPL) during evaluation, but GPT-4o is not integrated with a theorem-proving tool during generation.",
            "benchmark_name": "miniCTX (primary), miniF2F (also evaluated)",
            "benchmark_description": "miniCTX: benchmark of theorems from real Lean projects with long in-file and cross-file context; miniF2F: competition-style standalone problems not requiring extra context.",
            "task_type": "Full-proof generation (generate complete Lean proof), evaluated also on context-dependent proving (in-file and cross-file premises).",
            "performance_metric": "pass@8 proof success rate (percentage of problems for which at least one of 8 generated proofs verifies in Lean)",
            "performance_value": "Without preceding file context: 11.72% (avg on miniCTX-test); With preceding file context: 27.08% (avg on miniCTX-test); With context + retrieved premises: 26.82% (avg on miniCTX-test). Also miniF2F: 13.52% (full-proof few-shot baseline).",
            "comparison_with_baseline": "Providing preceding file context improves GPT-4o by +15.36 percentage points absolute (11.72% -&gt; 27.08%) on miniCTX; premise selection helps on high cross-file-dependency splits (improves per-split performance) but does not uniformly improve average performance.",
            "key_findings": "Conditioning GPT-4o on in-file context dramatically increases proof success; retrieved cross-file premises can help on splits with high cross-file dependency but can also interfere when excessive or mismatched.",
            "limitations": "Absolute performance remains low despite context; sensitive to length and size of retrieved premises (long premises relative to context harm success); relies on external Lean automation tactics in many proofs (see automation reliance experiments).",
            "uuid": "e6812.0",
            "source_info": {
                "paper_title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Llemma-7b",
            "name_full": "Llemma-7b (math-specialized open model)",
            "brief_description": "A 7B-parameter open model specialized for mathematical tasks, evaluated here as the state-tactic prompting model that outputs a tactic given a proof state.",
            "citation_title": "Llemma: An open language model for mathematics",
            "mention_or_use": "use",
            "model_name": "Llemma-7b",
            "model_description": "7B-parameter transformer LLM specialized for mathematics (as cited); used here for state-to-tactic prompting (generate next tactic from proof state).",
            "model_size": "7B",
            "architecture_type": "transformer",
            "training_data": "Not specified in this paper; the cited Llemma work describes math-focused pretraining/fine-tuning.",
            "reasoning_method": "State-tactic prompting: model is prompted with a proof state and asked to output the next tactic; search (best-first) composes tactics into full proofs.",
            "external_tool_used": true,
            "external_tool_description": "Tactics proposed by the model are executed/checked using Lean; best-first search is used to assemble tactic sequences into complete proofs within the proof assistant.",
            "benchmark_name": "miniCTX (primary), miniF2F (also evaluated)",
            "benchmark_description": "miniCTX tests context-dependent theorem proving (in-file and cross-file dependencies); miniF2F contains standalone competition problems.",
            "task_type": "Tactic prediction and proof search (state -&gt; tactic repeated to build full proof).",
            "performance_metric": "Proof success rate (percentage of problems solved by the tactic-prediction + search pipeline)",
            "performance_value": "State-tactic prompting baseline: average 14.58% on miniCTX-test; on miniF2F prompting achieves 28.28% (per Table 3 rows).",
            "comparison_with_baseline": "Prompting (no fine-tuning) performs worse than state-tactic fine-tuning (+~4.95 percentage points when fine-tuned on state-tactic pairs: 14.58% -&gt; 19.53% average).",
            "key_findings": "State-to-tactic prompting can produce useful tactics but benefits substantially from fine-tuning on (state, tactic) pairs; vanilla prompting is weaker on context-dependent problems that require new definitions/lemmas.",
            "limitations": "As typically framed, state-tactic models are unaware of new file-level context unless that context appears in the proof state; they struggle with cross-file/context-level generalization unless augmented with retrieval or re-training.",
            "uuid": "e6812.1",
            "source_info": {
                "paper_title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DeepSeek-Coder-1.3b (state-tactic tuned)",
            "name_full": "DeepSeek-Coder-1.3b fine-tuned for state-tactic prediction",
            "brief_description": "A 1.3B-parameter code-oriented transformer LLM used as the fine-tuning base for state-tactic prediction; fine-tuned on human-extracted (proof state, tactic) pairs from Mathlib using NTP-TOOLKIT and evaluated with best-first search.",
            "citation_title": "Deepseek-coder: When the large language model meets programming -the rise of code intelligence.",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Coder-1.3b (state-tactic tuned)",
            "model_description": "1.3B transformer model pre-trained for programming/code tasks and further fine-tuned on extracted (state, tactic) pairs for tactic prediction.",
            "model_size": "1.3B",
            "architecture_type": "transformer",
            "training_data": "Fine-tuned on Mathlib-derived (state, tactic) pairs extracted with NTP-TOOLKIT (307k extraction from a 2023 snapshot reported; training splits: 583k train examples for full-tuning pipelines referenced).",
            "reasoning_method": "State-tactic fine-tuning: model maps proof states to next-tactic predictions; best-first search constructs full proofs at test time.",
            "external_tool_used": true,
            "external_tool_description": "Tactic actions are executed and verified in Lean; evaluation and search operate within the Lean REPL environment.",
            "benchmark_name": "miniCTX (primary), miniF2F",
            "benchmark_description": "miniCTX: context-dependent proofs from real Lean projects; miniF2F: standalone competition problems.",
            "task_type": "Tactic prediction & proof search (state -&gt; tactic repeated)",
            "performance_metric": "Proof success rate (percentage)",
            "performance_value": "State-tactic tuned DeepSeek-Coder-1.3b: average 19.53% on miniCTX-test; 32.79% on miniF2F (per Table 3 row 'State-tactic tuning').",
            "comparison_with_baseline": "Fine-tuning improves over prompting (state-tactic prompting 14.58% -&gt; tuning 19.53% avg on miniCTX); however, file-tuning yields much larger gains on context-dependent problems.",
            "key_findings": "Fine-tuning on (state,tactic) pairs improves tactic generation; still limited on context-dependent theorems that require leveraging new file-level definitions/lemmas.",
            "limitations": "Lacks mechanism to consume long preceding file context at inference time; performance degrades on problems that require novel context-level generalization; relies on Lean automation tactics for many solutions.",
            "uuid": "e6812.2",
            "source_info": {
                "paper_title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "File-tuning",
            "name_full": "File-tuning (fine-tuning on (state, context, tactic) triples)",
            "brief_description": "A training methodology that fine-tunes a language model to generate tactics conditioned not only on the proof state but also on preceding file contents (in-file context), enabling context-dependent theorem proving over long repository contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "File-tuned DeepSeek-Coder-1.3b",
            "model_description": "DeepSeek-Coder-1.3b fine-tuned on (proof state, preceding file contents, next tactic) triples extracted by NTP-TOOLKIT; training uses random truncation strategies to handle long files.",
            "model_size": "1.3B",
            "architecture_type": "transformer",
            "training_data": "Human-written (state, context, tactic) triples extracted from Mathlib and other Lean projects via NTP-TOOLKIT (paper cites extraction counts and training splits: 583k train, 15k dev, 15k test across their extraction pipeline).",
            "reasoning_method": "Context-conditioned tactic generation: model synthesizes tactics using both local proof state and preceding file context (definitions, lemmas, comments); best-first search composes tactics into proofs. Long-context handling via truncation to token budgets (random truncation strategy: middle or preceding 1024 tokens).",
            "external_tool_used": true,
            "external_tool_description": "Generated tactics are executed and validated in the Lean REPL; NTP-TOOLKIT extracts contexts and premises; optional premise retrieval (LeanDojo retriever) can be appended to context.",
            "benchmark_name": "miniCTX (primary), miniF2F (comparative)",
            "benchmark_description": "miniCTX emphasizes context-dependent theorem proving from real Lean projects with long in-file/cross-file dependencies; miniF2F contains standalone competition problems.",
            "task_type": "Context-aware tactic prediction and proof generation (uses full preceding file content as input).",
            "performance_metric": "Proof success rate (percentage)",
            "performance_value": "File-tuned model average on miniCTX-test: 35.94% (per Table 3 'File tuning' avg); on miniF2F the gain is small: 33.61% vs the state-tactic tuned 32.79%. On certain splits (e.g., PNT) much larger per-split gains observed (see table and text).",
            "comparison_with_baseline": "Substantially outperforms state-tactic tuned baseline on miniCTX (19.53% -&gt; 35.94%, +16.41 percentage points), while improving only slightly on miniF2F (32.79% -&gt; 33.61%).",
            "key_findings": "File-tuning enables models to use new in-file definitions, lemmas, and previous proofs to materially improve context-dependent theorem proving performance; helps across difficulty levels and longer contexts; models can learn proof patterns from previous proofs in the file.",
            "limitations": "Current implementation truncates contexts to token budgets (1024) which discards long-range information; inconsistent gains when combined with premise selection (cross-file premises) and can be harmed by large retrieved premise blocks; limited improvement on very challenging proofs (e.g., long PFR proofs); performance drops when Lean automation tactics are disabled.",
            "uuid": "e6812.3",
            "source_info": {
                "paper_title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Premise selection (LeanDojo retriever)",
            "name_full": "Premise selection via LeanDojo retriever",
            "brief_description": "A retrieval component that identifies and returns the most relevant definitions/lemmas from imported modules (top-20) to augment a model's context for cross-file dependencies.",
            "citation_title": "LeanDojo: Theorem proving with retrieval-augmented language models",
            "mention_or_use": "use",
            "model_name": "LeanDojo premise retriever",
            "model_description": "Embedding-based premise retriever (as used in LeanDojo) that ranks candidate premises from imported modules and returns top-k relevant items to append to the model input context.",
            "model_size": null,
            "architecture_type": "embedding-based retrieval",
            "training_data": "Repository lemmas and definitions (module-level corpora); specifics come from LeanDojo implementation.",
            "reasoning_method": "Retrieval-augmented reasoning: retrieved premises are appended to in-file context so the generator can use cross-file lemmas during proof generation.",
            "external_tool_used": true,
            "external_tool_description": "Retriever provides premises that are then verified/executed within the Lean REPL; integration is via context concatenation before model input.",
            "benchmark_name": "miniCTX (used to simulate cross-file context)",
            "benchmark_description": "miniCTX includes metadata for imported modules to support premise retrieval; PFR cross split intentionally contains many cross-file premises to evaluate this mechanism.",
            "task_type": "Cross-file premise retrieval to support formal proof generation",
            "performance_metric": "Impact on proof success rate (percentage) when premises are appended to model input",
            "performance_value": "Effect is mixed: for GPT-4o, premise selection improves performance on high cross-file-dependency splits (e.g., PFR, PFR cross, SciLean); for the file-tuned model, appending retrieved premises does not consistently improve and can worsen performance on some splits (see Table 3 and analysis).",
            "comparison_with_baseline": "Helps GPT-4o on high cross-file-dependency splits; does not consistently help and can interfere with file-tuned model performance (sometimes reduces proof success).",
            "key_findings": "Premise retrieval can capture useful cross-file context and help certain models and splits, but naive concatenation of large retrieved premise blocks can overwhelm or distract models from in-file context and harm performance.",
            "limitations": "Retrieved premises can be long relative to the remainder of the context and harm attention/focus; mismatches between retrieved premises and in-file context can disrupt proof generation; better integration strategies are needed.",
            "uuid": "e6812.4",
            "source_info": {
                "paper_title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Automation tactic reliance",
            "name_full": "Reliance on Lean automation tactics (e.g., simp, linarith)",
            "brief_description": "Empirical finding that models often rely on Lean's powerful automation tactics to discharge routine reasoning steps; disabling these tactics causes substantial performance drops in tasks requiring explicit step-by-step reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "n/a",
            "model_description": "Not a model but an observed behavior/limitation across evaluated models: many generated proofs invoke automation tactics rather than explicit symbolic reasoning steps.",
            "model_size": null,
            "architecture_type": "n/a",
            "training_data": "n/a",
            "reasoning_method": "Models often exploit availability of automation tactics in Lean to produce short proofs; when automation is disabled (as in Math2001 dataset), models must produce explicit step-by-step reasoning which is harder.",
            "external_tool_used": true,
            "external_tool_description": "Automation tactics (simp, linarith) are part of the Lean environment and are invoked by generated proofs; disabling them in evaluation shows reliance.",
            "benchmark_name": "Math2001 (textbook-style split used for the analysis)",
            "benchmark_description": "Math2001 is a Lean textbook dataset where common automation tactics are disabled for pedagogical reasons, requiring explicit proofs.",
            "task_type": "Full-proof generation under constraints (automation disabled) / explicit step-by-step reasoning",
            "performance_metric": "Relative drop in proof success rate when automation tactics are disabled",
            "performance_value": "Substantial performance drops observed when common automation tactics are disabled (exact percentages not reproduced in the main text; reported as large in Table 6 and analysis).",
            "comparison_with_baseline": "When automation is enabled, models obtain higher success rates by invoking tactics; disabling automation reveals much lower explicit-reasoning ability.",
            "key_findings": "Current LLM-based provers depend heavily on automated tactics â€” disabling them uncovers a significant gap in explicit symbolic/logical reasoning capability.",
            "limitations": "Shows that current successes can reflect reliance on proof assistant automation rather than explicit logical step-by-step reasoning; progress needed to improve fine-grained symbolic reasoning without automation.",
            "uuid": "e6812.5",
            "source_info": {
                "paper_title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative language modeling for automated theorem proving",
            "rating": 2,
            "sanitized_title": "generative_language_modeling_for_automated_theorem_proving"
        },
        {
            "paper_title": "LeanDojo: Theorem proving with retrieval-augmented language models",
            "rating": 2,
            "sanitized_title": "leandojo_theorem_proving_with_retrievalaugmented_language_models"
        },
        {
            "paper_title": "Llemma: An open language model for mathematics",
            "rating": 2,
            "sanitized_title": "llemma_an_open_language_model_for_mathematics"
        },
        {
            "paper_title": "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs",
            "rating": 2,
            "sanitized_title": "draft_sketch_and_prove_guiding_formal_theorem_provers_with_informal_proofs"
        },
        {
            "paper_title": "Baldur: Whole-proof generation and repair with large language models",
            "rating": 2,
            "sanitized_title": "baldur_wholeproof_generation_and_repair_with_large_language_models"
        },
        {
            "paper_title": "miniF2F: a cross-system benchmark for formal olympiad-level mathematics",
            "rating": 1,
            "sanitized_title": "minif2f_a_crosssystem_benchmark_for_formal_olympiadlevel_mathematics"
        },
        {
            "paper_title": "Deepseek-coder: When the large language model meets programming -the rise of code intelligence.",
            "rating": 2,
            "sanitized_title": "deepseekcoder_when_the_large_language_model_meets_programming_the_rise_of_code_intelligence"
        },
        {
            "paper_title": "Graph2tac: Online representation learning of formal math concepts",
            "rating": 1,
            "sanitized_title": "graph2tac_online_representation_learning_of_formal_math_concepts"
        }
    ],
    "cost": 0.01966975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>miniCTX: NEURAL THEOREM PROVING WITH (LONG-) CONTEXTS
4 Mar 2025</p>
<p>Jiewen Hu 
Carnegie Mellon University</p>
<p>Thomas Zhu 
Carnegie Mellon University</p>
<p>Sean Welleck 
Carnegie Mellon University</p>
<p>miniCTX: NEURAL THEOREM PROVING WITH (LONG-) CONTEXTS
4 Mar 2025AEBF20B6CCDAC2A7A41122A320BB6EA4arXiv:2408.03350v3[cs.AI]
Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information.We introduce miniCTX, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training.miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens.Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof.As a baseline for miniCTX, we tested fine-tuning and prompting methods that condition theorem proving on preceding context.Both approaches substantially outperform traditional methods that rely solely on state information.We found that this ability to use context is not captured by previous benchmarks such as miniF2F.Alongside miniCTX, we offer NTP-TOOLKIT for automatically extracting and annotating theorem proving data, making it easy to add new projects into miniCTX to ensure that contexts are not seen during training.miniCTX offers a challenging and realistic evaluation of neural theorem provers. 1</p>
<p>INTRODUCTION</p>
<p>Formal theorem proving in interactive theorem provers (ITPs) provides a testbed for evaluating the reasoning capabilities of large language models (LLMs).Theorem proving capabilities can then directly translate to automation for mathematicians, such as via tools that complete or formalize proofs (Welleck &amp; Saha, 2023;Song et al., 2024;Welleck, 2024;Agrawal et al., 2022).However, despite their promise, we see a gap between the evaluation of current language model-based provers and the complexity of real-world theorem proving.</p>
<p>Our motivating observation is that theorems and proofs depend on various forms of context, such as newly-defined definitions and lemmas.For instance, to prove results about a square, one might first formalize a definition of a rectangle, prove some results about rectangles, then specialize them to a newly-defined square (Kontorovich, 2024b) (Figure 1).However, existing methods for training and evaluating LLM-based theorem provers often fail to incorporate the full range of contextual information available in real-world projects.For example, benchmarks often focus on proving standalone competition problems (e.g., miniF2F (Zheng et al., 2022)) or theorems from a library that the model has trained on (e.g., Mathlib (Han et al., 2022;Yang et al., 2023)), and state-of-the-art LLM-based provers are trained to accept only a proof state as input, making them unaware of new theorems and definitions (Polu &amp; Sutskever, 2020;Ying et al., 2024;Xin et al., 2024).While some existing work, including premise selection techniques (Jiang et al., 2022;MikuÅ‚a et al., 2023;Yang et al., 2023) and datasets like CoqGym (Yang &amp; Deng, 2019), have explored theorem proving based on information beyond the current state, they often focus only on providing relevant premises-lemmas that can assist proof construction-which are only a subset of the available information.</p>
<p>Building on these foundations, we propose miniCTX: a benchmark that seeks to expand the scope of context used in theorem proving.We extend beyond traditional premise selection explored in prior benchmarks (e.g., Yang et al. (2023); Yang &amp; Deng (2019)) by incorporating a more comprehensive set of contextual elements.This includes premises, prior proofs, comments, notation, and structural  (Azerbayev et al., 2023) Lean âœ— âœ“ âœ“ âœ— LeanDojo (Yang et al., 2023) Lean âœ“ âœ— âœ— âœ— LeanStep (Han et al., 2022) Lean âœ“ âœ— âœ“ âœ— CoqGym (Yang &amp; Deng, 2019) Coq âœ“ âœ— âœ“ âœ— PISA (Jiang et al., 2021) Isabelle
âœ— âœ— âœ“ âœ— miniCTX (Ours) Lean âœ“ âœ“ âœ“ âœ“
components like imports and declarations.By doing so, miniCTX aims to drive the development of methods that understand and work with context that occurs in complex, real-world theorem proving tasks.We compare miniCTX with several popular theorem proving datasets to highlight its unique contributions in terms of contextual dependency and real-world applicability (see Table 1).Additionally, considering the common use of pre-trained language models we mitigate potential data contamination by continually and automatically updating miniCTX with new Lean projects, so that evaluated theorems are not seen during training.Our key contributions are:</p>
<p>miniCTX Benchmark: We introduce miniCTX, the first benchmark designed specifically to evaluate theorem proving in real-world settings where proofs depend on in-file definitions, lemmas, and context from formal projects.miniCTX presents a unique challenge by requiring models to reason over long contexts and handle dependencies that arise in real-world theorem proving tasks.</p>
<p>NTP-TOOLKIT: To facilitate the automatic updating of miniCTX, we developed the NTP-TOOLKIT, which automatically extracts relevant theorems and contexts from Lean projects.Additionally, we provide a Lean REPL wrapper that enables simpler evaluation on miniCTX.</p>
<p>Baseline Evaluations: We evaluate miniCTX on several existing baseline models, including different fine-tuning and prompting strategies, as well as methods with premise selection.We also propose filetuning, a strong baseline method for training models using full file contexts, where both the theorem statements and their surrounding context are provided during training.This approach establishes a robust baseline for future work on context-dependent theorem proving.</p>
<p>Figure 1: Many state of the art provers are trained on a static dataset of theorems and proofs, then evaluated on standalone problems such as competition problems (left).We argue that neural provers must also operate in the realistic context-dependent setting, in which results depend on working with new mathematical objects and their facts, notations, and the structural elements of the project (imports, variables, etc.) (right).</p>
<p>THEOREM PROVING WITH CONTEXT</p>
<p>For language model-based provers to function as useful tools in this real-world setting, they need to be able to work with new information such as new definitions or lemmas.For example, a system suggesting proofs in the Prime Number Theorem project should be familiar with the project's definition of "square".Many current language model-based provers are trained on a static snapshot of data, and are hence unaware of any context that was created after the model was trained (Figure 1).They are often evaluated on either context-less standalone competition problems (Zheng et al., 2022;Azerbayev et al., 2023) that do not reflect realistic settings, or existing Mathlib proofs (Yang et al., 2023) which risks data contamination.As a result, it remains unclear whether these models can work with new information, which is necessary for using them as an assistant in a real proof development, and how to enable this capability.</p>
<p>Context-dependent proving.We study context-dependent theorem proving, where the goal is for a model to generate proofs y for new theorems x, based on a context c that includes background information such as definitions, lemmas, or natural language comments.Formally, the problem is
maximize M E (x,c)âˆ¼p E yâˆ¼M (â€¢|x,c) v(x, c, y),
(1) where (x, c) âˆ¼ p denotes a (theorem, context) pair from a context distribution p, M is a model that produces a proof y, and v returns 1 if the proof is correct and 0 otherwise.</p>
<p>Evaluating context-dependent proving.We choose Lean (Moura &amp; Ullrich, 2021) as the verifier v, because of the large body of recent theorems in Lean that can be used as evaluation data, and the abundance of proving methods in Lean that we use as baselines.We treat a Lean repository as the distribution p.Each context c is a subset of the repository, including new definitions, lemmas, notations, imports, and comments that are relevant to the theorem.</p>
<p>Given a language model, we can test three kinds of generalization by ensuring the following:</p>
<p>â€¢ Theorem-level generalization: the proof must not occur in the model's training data.</p>
<p>â€¢ Context-level generalization: the code c and proof must not occur in the training data.</p>
<p>â€¢ Project-level generalization: the entire repository must not occur in the training data.</p>
<p>For baseline evaluations, we investigate the in-file context case where c is the source code that precedes the theorem x in a file, as well as cross-file context where c includes both preceding code and relevant premises retrieved from imported modules through premise selection.</p>
<p>miniCTX: A BENCHMARK FOR THEOREM PROVING WITH CONTEXT</p>
<p>We develop miniCTX, a Lean 4 theorem proving benchmark of theorems that depend on newlydefined lemmas, definitions, and proofs from within a project.miniCTX is currently based on 762 theorems from six projects: (1) Prime Number Theorem (PNT) (Kontorovich, 2024a), (2) Polynomial Freiman-Ruzsa Conjecture and its extension with high cross-file dependency (PFR) (Tao, 2023), (3) recent results from the standard mathematical library (Mathlib), (4) an introductory text on theorem proving (HTPI) (Macbeth, 2023), (5) high energy physics formalization in HepLean (HEP) (Tooby-Smith, 2024), and (6) scientific computing formalizations (SciLean) (SkÅ™ivan, 2021).These theorems are equally split into 381 validation and 381 test theorems.Table 2 shows an overview of the dataset.Each theorem in miniCTX consists of the theorem statement, preceding file contents up to the theorem statement, and metadata, in JSON (see Â§A.Using our benchmark, users can easily reconstruct the complete context for each theorem, including both in-file and cross-file context.The in-file context is provided directly by preceding file contents, while the cross-file context can be reconstructed using the metadata, which includes information on imported modules.We open-source the dataset and evaluation code.HTPI.HTPI contains the Lean code for the book How to Prove It (HTPI) (Velleman, 2019), which explains a systematic approach to constructing mathematical proofs with Lean.It covers topics like elementary logic and number theory, and proving techniques like induction.The files in HTPI typically start with basic definitions and lemmas that might be used throughout the entire file, followed by exercises and several example problems.Therefore, models can utilize definitions, lemmas, and proof structures from example problems to solve exercises, making it an effective benchmark for testing context-dependent theorem-proving models.</p>
<p>HEP. HepLean (Tooby-Smith, 2024) is an open-source project that digitalizes definitions, theorems, proofs, and calculations in high energy physics using Lean.HepLean aims to facilitate discovery, automate new findings, verify correctness, and provide educational tools in physics.We selected files from the space and time section, which introduces several new definitions, including 4D Euclidean spacetime models.These files contain high in-file and cross-file dependencies, ideal for evaluating context-dependent theorem proving.This split represents an area outside of mathematics, further expanding the generalization of the benchmark.</p>
<p>Recent SciLean Commits.SciLean (SkÅ™ivan, 2021) is an open-source project designed to formalize concepts in scientific computing using Lean 4. SciLean aims to provide formalized tools and frameworks for efficiently representing and proving properties of numerical methods, differential equations, and optimization problems, bridging applied mathematics and computer science.Given Scilean has been created for 3 years and accessed by multiple projects, similar to our Mathlib split, we selected 46 test and 46 validation theorems added to SciLean since March 2024 in order to ensure the data is more likely unseen for evaluating models.The newly added problems are mostly supplements to existing files, so they evaluate the model's ability to apply and learn from previous lemmas.</p>
<p>PROBLEM SELECTION METHODOLOGY</p>
<p>The problem selection process for different splits in miniCTX follows three main criteria: (1) ensuring new problems are less likely to have appeared in training data, (2) utilizing an automated selection process, and (3) promoting generalization.Depending on the properties and goals of each split, we adopted three primary approaches:</p>
<ol>
<li>
<p>Recency-based selection: For popular libraries such as Mathlib and SciLean, which are highly likely to be used for training, we aim to select newly added theorems.This is achieved by sorting theorems based on the timestamp of when the theorem was first added to the project, which is extracted by NTP-TOOLKIT.This helps mitigate data contamination.</p>
</li>
<li>
<p>Random selection: For more recent projects, such as the PFR split, where the risk of data contamination is lower, we randomly select proved theorems to ensure a representative sample of the entire project.This approach maintains generality of the selected problems.</p>
</li>
</ol>
<p>Dependency-based selection:</p>
<p>To explicitly evaluate models' performance in contextdependent proving, we selected files based on the in-file and cross-file premise labels available in our benchmark.For the PNT split, we chose the file with the highest number of in-file dependencies, while for PFR crossfile , we selected files with the highest cross-file dependencies.For HEP, we selected files with a balance of both types.The level of dependency is also extracted automatically by NTP-TOOLKIT.</p>
<p>Although human inspection and expertise are involved in ensuring that the selected problems are valid and sufficiently general for evaluating models across diverse settings, all selection processes are ultimately automated by using the labels extracted through our toolkit.This ensures consistency across the benchmark and scalability to future updates.</p>
<p>KEY FEATURES AND CHALLENGES</p>
<p>miniCTX introduces several key features that distinguish it from other theorem proving benchmarks, addressing challenges that have not been tackled by previous benchmarks:</p>
<p>Real-world theorem proving.Unlike popular benchmarks (e.g., miniF2F (Zheng et al., 2022), ProofNet (Azerbayev et al., 2023), FIMO (Liu et al., 2023)) that focus on isolated competition problems, real-world research-level theorem proving is heavily dependent on rich mathematical contexts.Therefore, miniCTX includes real-world, complex theorems from a variety of ongoing Lean projects, such as Prime Number Theorem (PNT) and Polynomial Freiman-Ruzsa Conjecture (PFR).They rigorously test a model's ability in real-world formalization projects.This diversity contrasts with the LeanDojo benchmark (Yang et al., 2023), which focuses solely on Mathlib, enabling miniCTX to better test a model's generalization in different settings.</p>
<p>Contextual evaluation.Proving a theorem often depends on new definitions, lemmas, or other contextual information, which a model may not have seen during training.miniCTX includes theorems along with this new context.During evaluation, the model is expected to leverage the provided new context to help prove the theorem.</p>
<p>Beyond previous datasets like LeanDojo (Yang et al., 2023) and CoqGym (Yang &amp; Deng, 2019), which include relevant definitions and theorems, miniCTX includes additional useful contextual information that may make some theorems easier to prove compared to standalone theorems.For instance, Lean source code can have natural language comments that may help constrain the space of possible proofs.Moreover, some proofs within a file often have analogous patterns or structure, which may make subsequent theorems easier to prove (see Â§A.2).These additional forms of context occur in the real-world process of formalization, yet their use in neural theorem proving is underexplored.</p>
<p>Automatically updating the benchmark.Most modern neural theorem provers use a large language model as a backbone.Therefore, it is crucial to ensure that evaluation content is not seen during (pre-)training, a problem not addressed by previous benchmarks (see Â§G).We plan to update miniCTX periodically to include theorems beyond a certain cut-off date (see our project page).Future updates will be automatically extracted from new Lean projects using NTP-TOOLKIT ( Â§3.4).See Figure 2.</p>
<p>EXPERIMENTS</p>
<p>We evaluate several baselines on miniCTX, demonstrating the importance of context in real-world theorem proving.We study performance along various axes, including premise dependency, context length, difficulty, and the type of information in the context.Our experiments show that file-tuned models, which can utilize context at evaluation time, improve drastically over traditional state-tactic models in the context dependent setting.Moreover, we discover that this real-world performance boost cannot be readily measured by existing benchmarks such as miniF2F.</p>
<p>BASELINES</p>
<p>We select several baselines that we evaluate on miniCTX, as follows:</p>
<p>Prompting LLMs.We first test the ability of a state-of-the-art API-based model, GPT-4o, to generate the complete proof given a theorem statement in Lean, with several few-shot examples provided for guidance.We generate 8 such proof samples and measure pass@8 proof rate.We also test whether adding context in the form of preceding file contents or retrieved premises improves the proof rate.</p>
<p>State-tactic prompting.Another common approach to theorem proving using language models is to let the model generate a tactic given the current proof state (Han et al., 2022;Yang et al., 2023;Polu &amp; Sutskever, 2020;Lample et al., 2022).Therefore, we test the state-tactic prompting setting, which prompts a model specialized for mathematical tasks, Llemma-7b (Azerbayev et al., 2024), to output a tactic given a proof state.At test time, the model generates one tactic at a time, and we use a best-first search to construct full proofs (Han et al., 2022;Yang et al., 2023;Polu &amp; Sutskever, 2020).</p>
<p>State-tactic tuning.We can further improve tactic generation by fine-tuning language models on human-written (state x t , tactic y t ) pairs.We follow this state-tactic framework and fine-tune a state-tactic tuned model from DeepSeek-Coder-1.3b (Guo et al., 2024) to input proof states and output tactics, trained on human-written tactics in Mathlib, the main mathematical library in Lean, extracted by NTP-TOOLKIT.Similarly, we use best-first search at test time.</p>
<p>File-tuning.A drawback to the setups above is that they only consider static (state, tactic) pairs, and do not take into account new context, including comments, definitions, lemmas, and file structure, encountered during test time.Therefore, we test whether supplying context c, in the form of preceding file contents, to the model improves performance.Similar to state-tactic tuning, we finetune DeepSeek-Coder-1.3b on human-written (state x t , context c, tactic y t ) triples to generate tactics based on the current context and proof state, resulting in the file-tuned model (see Figure 3).</p>
<p>Premise selection.While in-file context offers valuable information to models, it still overlooks external resources like imported modules, which are often crucial in real-world interactive theorem proving.To better simulate a complete context and evaluate on project-level generalization, we apply premise selection to extract relevant premises from imported files within the same repository.Specifically, we use the premise retriever provided by LeanDojo (Yang et al., 2023) to identify the top 20 most relevant definitions or lemmas from imported modules and append them to the in-file context.Given that the models we are evaluating are trained on Mathlib, and they exhibit a strong ability to apply Mathlib lemmas, we did not include Mathlib in the potential premises to ensure that the models gain as much information as possible from the provided context (both infile and crossfile).All potential premises are automatically extracted using our toolkit, ensuring an efficient and automated process.</p>
<p>RESULTS</p>
<p>Context-dependent methods improve theorem proving.Figure 4: Model performance by dependency on premises.For each theorem in miniCTX, we record as metadata whether its human-written proof depends on other definitions or theorems in the same file ("in-file") or in other files ("cross-file"), and test the performance of baselines on each type.</p>
<p>the preceding file context, which includes definitions and lemmas, to GPT-4o results in dramatic improvement compared to using just the proof state (27.08% vs. 11.72%).These findings highlight the importance of providing models with rich contextual information beyond the immediate proof state, also demonstrating that miniCTX is able to measure this ability of context-dependent proving.</p>
<p>Premise selection improves performance on high cross-file dependency splits.The results in Table 3 indicate that premise selection has a mixed impact on model performance.For the GPT-4o, premise selection improves performance on high cross-file dependency splits, such as PFR, PFR cross , and SciLean.This suggests that premise selection helps capture the cross-file context, enabling GPT-4o to make better use of cross-file information.However, for the file-tuned model, premise selection does not consistently improve results, and even performs worse on the PFR cross split, which was designed to evaluate the effective use of cross-file premises.This suggests that the retrieved premises differ significantly from the in-file context.Therefore, developing methods that effectively support the integration of cross-file context (e.g., premise selection) alongside in-file context remains an interesting open research direction for improving performance on the miniCTX benchmark.</p>
<p>Evaluation on miniF2F.We evaluate baselines on miniF2F, a standard benchmark based on competition problems that do not require context.We use import statements as context for the file-tuned model.The file-tuned model improves very little beyond the state-tactic model (33.61% vs. 32.79%),showing that the dramatic difference in context-dependent proving abilities seen on miniCTX cannot be captured by miniF2F.</p>
<p>ANALYSIS</p>
<p>We analyze the baseline models on miniCTX further along several axes, including the kinds of contextual dependencies, the difficulty, and the content made available in the context.</p>
<p>File-tuning especially helps on problems with infile dependencies.We use the miniCTX metadata to categorize theorems based on their in-file dependencies.Figure 6 shows the performance of state-tactic tuned model and file-tuned model on problems with in-file dependencies compared to those without.We also show miniF2F as an additional reference point for problems without in-file dependencies.The file-tuned model shows a marked improvement over the state-tactic tuned model, especially in problems that have dependencies on context.We conclude that file-tuning specifically helps in the realistic setting of theorem proving with new definitions and theorems in context.</p>
<p>Premise selection helps but may interfere with in-file context.We use miniCTX metadata to categorize problems based on their cross-file dependencies, evaluating the impact of premise selection across the entire dataset.As shown in Figure 4, GPT-4o benefits significantly from premise selection on problems with high cross-file dependencies, showing improved performance when leveraging relevant premises from imported files.However, we also observe that premise selection can interfere with in-file context, leading to inconsistent results, particularly when the available in-file context is relatively short.This suggests that adding cross-file premises may sometimes disrupt the model's ability to focus on the in-file information.Further analysis of this interference is included in Â§D.3.This highlights the need for more sophisticated integration strategies that can balance both in-file and cross-file contexts effectively.Models can learn from previous proofs in the file context.To determine the contribution of different components in the in-file context, we conducted an ablation study on the PFR.ForMathlib.Entropy.Basic file, which contains numerous co-related lemmas and rich natural language comments, making it an ideal candidate to investigate the influence of different context components.In this ablation, we systematically removed specific parts of the in-file context and evaluated the model's ability to generate proofs under these modified conditions.As shown in Table 4, both the file-tuned model and GPT-4o benefit from the inclusion of previous proofs in the file context.This indicates that models are capable of learning proof strategies from existing proofs in the file and effectively applying them to new problems (see Â§D.4 for more examples).</p>
<p>Natural language comments contribute in certain settings.Our ablation also explored the effect of natural language comments in the in-file context.Though the impact was not dramatic, comments written in natural language were found to be helpful in certain settings.In scenarios where proofs were excluded from the context, adding comments resulted in slight performance gains for both models.For the file-tuned model, these gains were further amplified when proofs were included alongside comments, demonstrating the value of combining formal context with explanatory natural language.However, for GPT-4o, the presence of comments when proofs were included led to a slight decrease in performance, suggesting that effective context selection may vary depending on the model architecture and underlying training characteristics.</p>
<p>File-tuning improves across all difficulty levels and context lengths.Finally, Appendix Â§D.2 shows performance on problems categorized by the length of the human-written proof (when available), which we take as a rough proxy of the problem difficulty.The file-tuned model improved on all three difficulty categories.Appendix Â§D.2 also shows that file-tuning had improved accuracy across context lengths, particularly for problems with longer contexts.Longer contexts may imply more dependencies, suggesting that these problems can benefit more from file-tuning.</p>
<p>Models rely on common symbolic automation.</p>
<p>To demonstrate an additional kind of contextdependence, we perform an additional analysis on Math2001 (Macbeth, 2023), which is another Lean textbook setting. 2 In particular, the textbook code disables powerful automation tactics including simp and linarith to promote manual reasoning, akin to traditional textbook exercises.For example, Math2001 includes numerous arithmetic problems that are trivial with automation tactics (e.g., linarith) but are challenging for models to explicitly prove with step-by-step reasoning (e.g., via calc).In Table 6 we evaluate models with the automation disabled, and observe substantial performance drops, confirming the reliance on automation tactics.We also find that the state-tactic tuned model relies on simp for unseen definitions, making it performing similarly well to the file-tuned model on theorems that only rely on new definitions ( Â§D.6).</p>
<p>DISCUSSION AND FUTURE CHALLENGES</p>
<p>In addition to general improvements in performance, we comment on some specific open challenges.</p>
<p>Making better use of long-contexts.Our file-tuning method simply truncates contexts to be within a token budget (1024), which can discard useful contextual information.We found gains in providing GPT-4o 8,000 tokens of context compared to not providing it context, but its absolute performance was still low.There are several possible strategies that can be explored in future work, including feeding in the entire context, retrieval, or mixtures of the two.</p>
<p>Repository-level context.We focused on evaluating in-file context in this paper.As shown in Â§D.1, many problems require using context outside of the current file.Although we incorporated premise selection as a means of leveraging cross-file context, our experiments indicate that it does not consistently improve performance, even on datasets with high cross-file dependencies.This suggests a need to further investigate how to better integrate premise selection with in-file context.miniCTX provides sufficient metadata to reconstruct the entire environment, allowing for comprehensive investigation into premise selection and other potential methods for leveraging cross-file context.</p>
<p>Challenging proofs.Using context through file tuning did not improve performance on the challenging PFR proofs.Moreover, performance is relatively low (19%) on proofs that had a human-written proof of longer than five lines (see Â§D.2).Proving these kinds of theorems remains an open problem.</p>
<p>Working with constraints.As shown in Table 6, model performance drops when the proof cannot use powerful automation tactics.Models have a tendency to invoke these powerful tactics, and struggle with more explicit step-by-step proofs.Improving performance in this setting of miniCTX is an interesting future direction.</p>
<p>RELATED WORK</p>
<p>Formal theorem proving with language models.GPT-f (Polu &amp; Sutskever, 2020) pioneered the use of language models for theorem proving via next tactic prediction given the current proof state, a technique adopted by many subsequent methods (Jiang et al., 2021;Han et al., 2022;Lample et al., 2022;Polu et al., 2022;Welleck &amp; Saha, 2023;Azerbayev et al., 2024).ReProver (Yang et al., 2023) conditions each generation on retrieved premises, while Draft-sketch-prove (Jiang et al., 2023) conditions each generation on an informal proof.Baldur (First et al., 2023) fine-tunes a model with 50 lines of the preceding file content as context, but unlike file-tuning trains the model to generate a full proof without proof states.More broadly, machine learning for formal theorem proving is an active research area; see Lu et al. (2023); Li et al. (2024) for surveys.</p>
<p>Theorem proving data extraction.Several tools extract training data from interactive theorem provers, including CoqGym (Yang &amp; Deng, 2019) for Coq, PISA (Jiang et al., 2021) for Isabelle, LeanStep (Han et al., 2022) for Lean 3, and LeanDojo (Yang et al., 2023) for Lean 3 and 4. Recently, lean-training-data (Morrison, 2023) provides tools for extracting proof states and other information using Lean 4 metaprogramming, which we anecdotally found to be easiest to modify and fastest among Lean 4 data extraction tools.Our NTP-TOOLKIT adds 3 new tools on top of this code, along with a pipeline for running on any Lean projects and instruction tuning.</p>
<p>Theorem proving benchmarks.Theorem proving methods are typically evaluated in two settings: (1) standalone competition problems (Zheng et al., 2022) or textbook (Azerbayev et al., 2023) problems;</p>
<p>(2) holding out theorems from a mathematical library that the model is trained on, such as Mathlib for Lean (Han et al., 2022;Polu et al., 2022;Yang et al., 2023) or the Archive of Formal Proofs for Isabelle (Jiang et al., 2021;First et al., 2023).The first does not test the use of context, while the second tests only theorem-level generalization.miniCTX is designed to test the use of context as well as theorem-level, context-level, and project-level generalization across several mathematical domains.</p>
<p>Premise selection.Premise selection is an extensively studied class of methods for handling context in theorem proving.These methods retrieve useful lemmas from previously proved results, to help prove the current theorem.Many recent premise retrievers embed theorems and premises to a common space, and then taking a similarity measure (Yang et al., 2023;MikuÅ‚a et al., 2023), or use a classifier to determine if a premise is relevant (Irving et al., 2016;Han et al., 2022).Graph2Tac (Blaauwbroek et al., 2024) proposes an online learning method that can learn from both proofs and premises in a new context, which is particularly relevant to our work since it tackles project-level generalization.However, such methods cannot combine with more advanced pre-training-based methods without risking data contamination during evaluation.miniCTX aims to fill this gap by our temporal split.</p>
<p>We collect its data into miniCTX, formatted in JSON as follows:</p>
<p>{ # Preceding file content "srcContext": "importâ£Mathlib.Data.Real.Basic\n\n/-!\n#â£Squareâ£function\nWeâ£defineâ£theâ£squaringâ£functionâ£<code>sâ£:â£\\u211dâ£\\u2192â£\\u211d</code>â£toâ£beâ£<code>sâ£xâ£:=â£xâ£*â£ x</code>.\n-/\n\ndefâ£sâ£(xâ£:â£\u211d)â£:â£\u211dâ£:=â£xâ£*â£x\n\n", # Theorem statement "theoremStatement": "lemmaâ£s_eq_pow_twoâ£{xâ£:â£\u211d}â£:â£sâ£xâ£=â£xâ£^â£2", # Fully qualified theorem name "theoremName": "s_eq_pow_two", # Temporal metadata "fileCreated": "(gitâ£commit)", "theoremCreated": "(gitâ£commit)", # Source metadata "file": "MyProject/Square.lean","module": "MyProject.Square", "positionMetadata": { # Line number the theorem is on "lineInFile": 10, # Number of tokens before the theorem "tokenPositionInFile": 152, # Number of premises (definitions, theorems) before the theorem "theoremPositionInFile": 1 }, # Dependency metadata "dependencyMetadata": { # Number of definitions or lemmas defined in this file that the theorem uses "inFilePremises": true, "numInFilePremises": 1, # Number of definitions or lemmas defined in this repository that the theorem uses (including in-file ones) "repositoryPremises": true "numRepositoryPremises": 1, # Number of total premises (in file, repository, or otherwise) "numPremises": 2, # Modules imported in the current file "importedModules": ["Mathlib.Data.Real.Basic", ...] }, # Proof metadata "proofMetadata": { "hasProof": true, "proof": "by\nâ£â£rwâ£[s,â£pow_two]", "proofType": "tactic", "proofLengthLines": 2, "proofLengthTokens": 20 } }</p>
<p>In additional to individual entries, we also record the version (git commit) of the repository.</p>
<p>A.2 PRIME NUMBER THEOREM EXAMPLE</p>
<p>We collect theorems from the Rectangle.leanfile in PrimeNumberTheoremAnd.The following excerpt from Rectangle.leandemonstrates the scenario that often arises in a theorem proving environment where context is critical to producing a proof: When proving the final lemma symm_re, a model can benefit much from the preceding file contents, which include (1) the existing imports from Mathlib, variable declarations, and open namespaces that provide a syntactic context for this theorem, (2) the new definition Rectangle in the context, which the model has not seen in training, (3) natural language and LaTeX documentation of the file and Rectangle definition, (4) the analogous (in this case identical) proof of the preceding theorem symm.We demonstrate that performance on Rectangle.lean is indeed much higher when preceding file contents are given as context to a model.</p>
<p>For future data added to miniCTX that specifically test the preceding file contents as context, we will ensure it is standalone like Rectangle.lean,i.e. it does not import any other unseen files from the same repository, so the preceding file contents already contain all important information relevant to the proof.</p>
<p>B ADDITIONAL DATASETS</p>
<p>In addition to problems in miniCTX, we also evaluated other datasets that are not included due to copyright reasons.</p>
<p>B.1 MATH2001</p>
<p>Math2001 (Macbeth, 2023) contains the Lean code for the book The Mechanics of Proof by Heather Macbeth, an introductory text on mathematical theorem proving with accompanying Lean code.Each chapter of The Mechanics of Proof covers an introductory topic and walks through how to write the associated mathematics in Lean, along with exercises.The topics include proofs by calculation, proofs with structure, parity and divisibility, logic, induction, number theory, functions, sets, and relations.A unique aspect of Math2001 is that it disables common Lean automation for pedagogical purposes.For example, a student must write out an equality proof in detail, with each step justified.It also defines new tactics and definitions separate from the common Lean libraries.Typically a file in the textbook will show examples of such proofs, followed by exercises for a student to complete.We can view this as a form of contextual adaptation: a model must prove the theorem according to the constraints of the textbook.Math2001 has 41 files that include examples and exercises.We selected 1 to 2 theorems from each file (depending on the length of the file), for a total of 50 theorems.Of these, 31 have no proof in the Math2001 repository, hence testing theorem-level generalization.</p>
<p>Context-aware models surpass state-based models Below we show the inputs and outputs for file-tuning and state-tactic tuning.In the paper we refer to the natural language description at the beginning of the input as an "instruction", and refer to a set of inputs and outputs as described below as "instruction-tuning data".</p>
<p>C.2.1 FILE TUNING.</p>
<p>Given an example containing a state, next-tactic, and preceding file contents (srcUpToTactic), the data is formatted as:
Input: /-</p>
<p>D.5 EXAMPLE OF USING THEOREMS FROM CONTEXT</p>
<p>The file-tuned model is able to utilize the stated theorems in the context.Here is an example of the model using the previously defined theorem in the proof: The state-tactic tuned model is able to utilize the unseen definitions that appear in the proof state.In the following example Set.uIoo is a newly defined definition, which is never seen for state-tactic tuned model:</p>
<p>Input: Output:</p>
<p>simp [uIoo, h] This leads the state-tactic tuned model to perform similarly well as the file-tuned model on problems that only rely on new definitions, and not new theorems (see Figure 6).</p>
<p>E DATASET HOSTING AND MAINTENANCE</p>
<p>miniCTX is released on HuggingFace: l3lab/miniCTX, distributed under the Apache 2.0 license.Data extraction tool NTP-TOOLKIT is released on GitHub: cmu-l3/ntp-toolkit, under the MIT license.We note that the underlying data for the individual splits of miniCTX are also released under the Apache 2.0 license.We include the licensing information in the dataset repository.We plan to regularly update and maintain the dataset to include examples from new projects.For information about future updates such as miniCTX-v2, please refer to our project page: https: //cmu-l3.github.io/minictx.</p>
<p>F NTP-TOOLKIT GUIDELINE</p>
<p>We introduced NTP-TOOLKIT in Â§3.4.With the NTP-TOOLKIT, users can extract and annotate new theorems and proofs from any valid Lean project, in miniCTX format.The extracted data can be used either as updates to miniCTX, or as training data (for which we also provide instruction tuning utilities).We also develop a lightweight evaluation framework for easy evaluation on miniCTX.</p>
<p>F.1 PRELIMINARY</p>
<p>The evaluation code relies heavily on the Lean REPL (Lean Prover Community, 2024), which operates within the project environment.Therefore, it is essential that the project builds without any errors.Additionally, the version of Lean used in the project should match the version supported by the REPL.While the Lean REPL supports versions â‰¥ 4.3.0,for the best experience with data extraction and evaluation, we recommend evaluating projects that use Lean version 4.7.0 or higher (all miniCTX theorems are in 4.7.0).We plan to continuously update NTP-TOOLKIT to support newer versions.</p>
<p>F.2 USING THE NTP-TOOLKIT</p>
<p>The NTP-TOOLKIT is designed to easily extract and annotate theorem proving data from Lean projects, by simply providing the project URL.To use the NTP-TOOLKIT for data extraction, follow these steps:</p>
<ol>
<li>
<p>Installation: Clone the NTP-TOOLKIT repository from GitHub to your local machine.Currently, to use NTP-TOOLKIT for Lean version 4.7, switch to the lean-v4.7.0 branch; for 4.8.0 or above, use the main branch.Ensure that you have the required dependencies installed, as listed in the repository's README file.</p>
</li>
<li>
<p>Configuration: Supply GitHub URL, commit hash, and root modules of your Lean project in a JSON configuration file.Make sure that your project is using a compatible version of Lean.NTP-TOOLKIT will extract data from all modules imported by the root modules.</p>
</li>
<li>
<p>Data extraction: Run the data extraction script provided by the toolkit.Specify the --full_proof_training_data and --premises options to extract miniCTX-style data, which will be stored in an minictx.jsonloutput file.Specify the --declarations option to additionally extract the premises in each module, for premise retrieval.The full_proof_training_data outputs can be additionally used for fine tuning (assuming the extracted data is dated before the current temporal split of miniCTX).</p>
</li>
</ol>
<p>For detailed commands and additional options, please refer to the README file in the NTP-TOOLKIT repository.</p>
<p>F.3 miniCTX EVALUATION</p>
<p>We provide a comprehensive evaluation pipeline in the miniCTX-eval repository, supporting both tactic-prediction and full-proof generation tasks.Users should place the extracted JSONL file from the NTP-TOOLKIT into the data folder.To run an evaluation task, execute the task script by specifying the dataset path, the corresponding project path, and the path to the Lean REPL.This setup ensures that the evaluation is conducted within the correct environment and with the necessary data inputs.</p>
<p>G DATA CONTAMINATION IN EXISTING BENCHMARKS</p>
<p>One of the contributions of miniCTX is the temporal split: using NTP-TOOLKIT, miniCTX only includes theorems created after a certain cut-off date.This ensures LLMs trained before this date have not seen problems in miniCTX.We claimed that existing benchmarks face significant risks of data contamination, and here we provide some evidence to this claim.</p>
<p>With regards to contamination, existing benchmarks can be largely categorized as either extracted from a standard mathematical library (e.g.Mathlib), or curated as competition-style problems.For the former category, evaluation benchmarks include Mathlib test problems extracted by LeanDojo (Yang et al., 2023) or PACT (Han et al., 2022), as well as datasets like CoqGym (Yang &amp; Deng, 2019) and PISA (Jiang et al., 2021) in other formal languages.They are inherently at risk of contamination, because they are statically sourced from projects on GitHub, many of which have existed for years (e.g.Mathlib has existed since 2017).On the other hand, virtually all modern language models are pre-trained on public GitHub code, and have therefore seen the proofs.This partially invalidates any evaluation results using these benchmarks.</p>
<p>On the other hand, we have empirically found many solutions of datasets like miniF2F (Zheng et al., 2022) online.The original repository of miniF2F already contains full solutions to 72 of the 244 test problems (in Lean 3, which is directly translatable to Lean 4)3 .Moreover, of the remaining problems, 12 IMO problems have full solutions in Mathlib4 and Compfiles (a catalog of Lean proofs of competition problems)5 , at the time of writing.In total, at least 84 of the 244 problems in miniF2F-test, including 12 of the 19 IMO problems, have full solutions on GitHub.This may be an inherent issue of small competition-style benchmarks, as the theorem-proving community itself has significant interest in formalizing competition problems and solutions.The scarcity of problems from competitions like IMO each year, as well as the bias to only formalizing some categories (geometry problems are often not formalized due to difficulty) also contribute to this issue.Whether temporal split and other mitigation methods can be applied to competition-style benchmarks may also be an interesting future direction of research.</p>
<p>file contents up to the theorem statement, 3. Metadata, including: (a) File name, (b) Project commit and version, (c) Commit and time at which the theorem and its file was added, (d) Position of the theorem in the file and number of premises preceding it, (e) Number of in-file premises and cross-file premises used by the statement or proof, (f) Imported modules (for premise selection support), (g) Proof length and type.</p>
<p>Figure 2 :
2
Figure 2: miniCTX is automatically updated with Lean projects to stay ahead of LLM training cutoff dates, making it a suitable benchmark for real-world theorem proving for pre-trained models.Figure3: State-tactic vs. file tuning.</p>
<p>Figure 2: miniCTX is automatically updated with Lean projects to stay ahead of LLM training cutoff dates, making it a suitable benchmark for real-world theorem proving for pre-trained models.Figure3: State-tactic vs. file tuning.</p>
<p>import Mathlib.Analysis.Complex.CauchyIntegral import Mathlib.Analysis.Complex.Convex open Complex Set Topology open scoped Interval variable {z w : C} {c : R} /-%% \begin{definition}\label{Rectangle}\lean{Rectangle}\leanok A Rectangle has corners $z$ and $w \in \C$.\end{definition} %%-/ /--A <code>Rectangle</code>has corners <code>z</code>and <code>w</code>.-/ def Rectangle (z w : C) : Set C := [[z.re, w.re]] Ã—C [[z.im, w.im]] namespace Rectangle lemma symm : Rectangle z w = Rectangle w z := by simp [Rectangle, uIcc_comm] lemma symm_re : Rectangle (w.re + z.im * I) (z.re + w.im * I) = Rectangle z w := by simp [Rectangle, uIcc_comm]</p>
<p>lemma RectSubRect {x0 x1 x2 x3 y0 y1 y2 y3 : R} (x0_le_x1 : x0 â‰¤ x1) (x1_le_x2 : x1 â‰¤ x2) (x2_le_x3 : x2 â‰¤ x3) (y0_le_y1 : y0 â‰¤ y1) (y1_le_y2 : y1 â‰¤ y2) (y2_le_y3 : y2 â‰¤ y3) : Rectangle (x1 + y1 * I) (x2 + y2 * I) âŠ† Rectangle (x0 + y0 * I) (x3 + y3 * I) := by rw [rect_subset_iff, mem_Rect, mem_Rect] refine âŸ¨âŸ¨?<em>, ?</em>, ?<em>, ?</em>âŸ©, ?<em>, ?</em>, ?<em>, ?</em>âŸ© all_goals simpa using by linarith lemma RectSubRect' {z0 z1 z2 z3 : C} (x0_le_x1 : z0.re â‰¤ z1.re) (x1_le_x2 : z1.re â‰¤ z2.re) (x2_le_x3 : z2.re â‰¤ z3.re) (y0_le_y1 : z0.im â‰¤ z1.im) (y1_le_y2 : z1.im â‰¤ z2.im) (y2_le_y3 : z2.im â‰¤ z3.im) : Rectangle z1 z2 âŠ† Rectangle z0 z3 := by Output: simpa using RectSubRect x0_le_x1 x1_le_x2 x2_le_x3 y0_le_y1 y1_le_y2 y2_le_y3 D.6 EXAMPLE OF USING UNSEEN DEFINITIONS</p>
<p>theorem uIoo_of_le {Î± : Type*} [Lattice Î±] {a b : Î±} (h : a â‰¤ b) : Set.uIoo a b = Ioo a b :</p>
<p>Table 1 :
1
Comparison of theorem proving benchmarks across several key features.
BenchmarkLanguage Premise Full Context Multi-source Temporal SplitminiF2F (Zheng et al., 2022)Multipleâœ—âœ—âœ—âœ—ProofNet</p>
<p>Table 2 :
2
(Zheng et al., 2022)n miniF2F(Zheng et al., 2022)and miniCTX.
SplitProblems Context SizeIn-File PremisesRepo. PremisesProof Sizevalid + test(tokens)(premises / 100 tokens) (premises / 100 tokens)(lines)miniF2F244 + 244153<em>--3.0  â€ PNT85 + 8510,8581.870.303.3PFR51 + 5118,0590.651.1027.2PFR cross43 + 434,3510.442.752.7miniCTXMathlib50 + 5014,440--6.1HTPI45 + 4565,0822.850.0010.7  â€ HEP61 + 613,5855.654.253.1SciLean46 + 466,2492.089.721.8All381 + 38118,6901.942.638.5</em>Only counting library imports and definitions.â€  Excluding theorems without proofs.3.1 miniCTX SOURCESPNT. PrimeNumberTheoremAnd (Kontorovich, 2024a) is a project started in January 2024 thatformalizes the prime number theorem in Lean as well as related concepts, such as counter integralon rectangles in C. We find the files Rectangle.lean and ResidueCalcOnRectangles.leansuitable for our purpose of testing context-dependent theorem proving, especially when we usepreceding file content as context, as each file is self-contained within the project and contains newdefinitions (rectangles, squares) and many interdependent lemmas. See  Â§A.2 for an illustration ofsuch lemmas.PFR. PFR (Tao, 2023) is a project started in November 2023 that formalizes a proof of the PolynomialFreiman-Ruzsa (PFR) conjecture. We included 51 validation and 51 test theorems from PFR. Wefind that proofs of theorems in PFR tend to be much more monolithic and longer in length than thosein Mathlib or other libraries. PFR also defines custom mathematical concepts and notations (such asRuzsa distance) and a proof typically depends on many lemmas in PFR outside the current file.PFR crossfile . PFR crossfile is an extension of the PFR split, which includes additional problems to furtherevaluate cross-file dependencies. To evaluate models' performance on problems with extensive cross-file dependencies, we added 43 test theorems from Entropy.Group and Entropy.Kernel.Group,which have the highest cross-file dependencies in the project. These problems contain three timesthe number of cross-file premises compared to other math splits, making them a strong candidate forevaluating a model's ability to utilize cross-file premises. 43 validation theorems are chosen similarly.Recent Mathlib Commits. Mathlib (Mathlib Community, 2020), is Lean's largest community-maintained repository, encompassing a wide range of mathematical concepts, programming APIs,and common tactics. It is commonly used for training theorem-proving models and is frequentlyupdated with new definitions, theorems, and refactorings. Therefore, to avoid data contamination, weincluded 50 test and 50 validation theorems newly added to Mathlib since March 2024, by filteringrecent Mathlib commits to ones that only add new theorems. Assuming that the model was trainedprior to April 2024, the Mathlib split guarantees the evaluation of theorem-level generalization.</p>
<p>We open-source our training data for both file-tuning and state-tactic tuning, split into 583k train, 15k dev, and 15k test examples.</p>
<p>Our investigation reveals several open challenges that we discuss in Â§5.Training data.We ran NTP-TOOLKIT's next-tactic extraction on a 2023 snapshot of Mathlib, yielding 307,049 examples.We then ran NTP-TOOLKIT's instruction tuning script on these examples, yielding training examples and for state-tactic and file-tuning (see Â§4.1).For the file-tuning examples, as an initial method for handling the long Lean files, we either truncate the middle of an input file so that the file contents is 1024 tokens, or take only the preceding 1024 tokens, with the strategy selected at random for each example.</p>
<p>Table 3 :
3
Performance comparison (%) of different models on miniF2F-test and miniCTX-test.
miniF2FminiCTX-testMethodTestPrime PFR PFR cross Mathlib HTPI HEP SciLean Avg.GPT-4o (full proof)13.527.061.856.9814.0013.33 31.156.5211.72+ context-31.765.5634.8826.0017.78 49.1817.3927.08+ context + premise-29.417.4139.53-15.56 44.2621.7426.82State-tactic prompting28.2820.005.560.0016.000.0031.1519.5714.58State-tactic tuning32.7917.655.560.0022.0011.11 52.4619.5719.53File tuning33.6140.005.5644.1934.0015.56 60.6645.6535.94+ premise-42.35 11.1116.28-8.8950.8232.6130.21</p>
<p>Table 4 :
4
Ablation study on different context components for theorem proving.
Environment DefinitionsLemmaLemma Natural Language File-tuning GPT-4oStatementProofCommentsâœ—âœ—âœ—âœ—âœ—14.12%8.24%âœ“âœ—âœ—âœ—âœ—25.88%2.35%âœ“âœ“âœ—âœ—âœ—24.71%9.41%âœ“âœ“âœ“âœ—âœ—27.06%22.35%âœ“âœ“âœ“âœ“âœ—32.94%34.12%âœ“âœ“âœ“âœ—âœ“28.24%23.53%âœ“âœ“âœ“âœ“âœ“35.29%31.76%</p>
<p>Table 5 shows the performance comparison of different models.Both the GPT-4o model, which includes context in the input, and the file-tuned C.2 INPUT-OUTPUT FORMATTING.</p>
<p>You are proving a theorem in Lean 4.You are given the following information: -The file contents up to the current tactic, inside [CTX]...[/CTX] -The current proof state, inside [STATE]...[/STATE] Now here is your exercise.There is no need to restate the problem.If needed, think through the proof using comments.-/{theoremstatement}For full proof generation task with additional infile context, we use the following prompt:Your task is to generate complete proofs for problems stated in Lean4.For each problem, you will be provided with the context from the file in which the theorem is stated.This context includes useful external libraries, along with important definitions and theorems that are relevant to the proof.You are encouraged to use any tactics, definitions, lemmas, or theorems defined within this context to construct your proof.Please pay careful attention to indentation and formatting to ensure that the proof adheres to Lean4 syntax standards.Here are some examples:
{nextTactic} rintro âŸ¨h1, h2âŸ© [/TAC] exact (right_lt_sup.mp h2) (le_of_not_le (inf_lt_right.mp h1))C.2.3 GPT-4O PROMPTFor full proof generation task with only theorem statement, we use the following prompt:Your task is to generate complete proofs for problems stated in Lean4. You may use anytactics available in Mathlib, but no additional context, definitions, or theorems from theYour task is to generate the next tactic in the proof. Put the next tactic inside [TAC]...[/TAC] -/ [CTX] {srcUpToTactic} [/CTX] [STATE] problem's file will be provided. Focus on crafting proofs using general knowledge and techniques applicable in Lean4. Here are some examples: lemma deriv_scale {f : CS (n + 1) E} : (f.scale R).deriv = R âˆ’1 â€¢ f.deriv.scale R := by ext v ; by_cases hR : R = 0 &lt;;&gt; simp [hR, scale] â€¢ simp [deriv, smul] ; exact deriv_const _ _ â€¢ exact ((f.hasDerivAt (R âˆ’1 â€¢ v)).scomp v (by simpa using (hasDerivAt_id v).const_smul R âˆ’1 )).deriv theorem mul_dvd_mul_left (a : Î±) (h : b | c) : a * b | a * c := by obtain âŸ¨d, rflâŸ© := h use d rw [mul_assoc] import Mathlib.Analysis.Calculus.Deriv.Support import Mathlib.Analysis.Distribution.SchwartzSpace import Mathlib.Order.Filter.ZeroAndBoundedAtFilter open Real Complex MeasureTheory Filter Topology BoundedContinuousFunction SchwartzMap BigOperators variable {E : Type<em>} [NormedAddCommGroup E] [NormedSpace R E] {{n : N}} @[ext] structure CS (n : N) (E : Type</em>) [NormedAddCommGroup E] [NormedSpace R E] where toFun : R â†’ E h1 : ContDiff R n toFun /-#Context: h2 : HasCompactSupport toFun{state} noncomputable def scale (g : CS n E) (R : R) : CS n E := by[/STATE] by_cases h : R = 0[TAC] â€¢ exact âŸ¨0, contDiff_const, by simp [HasCompactSupport, tsupport]âŸ©â€¢ refine âŸ¨fun x =&gt; funscale g R x, ?<em>, ?</em>âŸ©â€¢ exact g.h1.comp (contDiff_const.smul contDiff_id)Output:â€¢ exact g.h2.comp_smul (inv_ne_zero h)/-Truncated -/{nextTactic} /-Now here is your exercise. There is no need to restate the problem. If[/TAC] needed, think through the proof using comments. -/#Context:{}C.2.2 STATE-TACTIC TUNING. #Problem:Given an example containing a state and next-tactic, the data is formatted as: {}Input: {theorem statement}/-You are proving a theorem in Lean 4.You are given the following information:-The current proof state, inside [STATE]...[/STATE]Your task is to generate the next tactic in the proof.Put the next tactic inside [TAC]...[/TAC]-/[STATE]{state}[/STATE][TAC]Output:
Project page: https://cmu-l3.github.io/minictx. Please refer to our project page for our dataset and evaluation links, and future updates including miniCTX-v2.
See Appendix B.1 for further details on Math2001. Due to licensing we do not include it in miniCTX.
https://github.com/openai/miniF2F
https://github.com/leanprover-community/mathlib4/tree/master/Archive/Imo
https://github.com/dwrensha/compfiles
CONCLUSIONWe studied the realistic setting of proving theorems that depend on new information and project constraints, and formulated an evaluation framework for testing generalization using real Lean projects.We built miniCTX, and found that the predominant method for training neural theorem provers fails to enable context dependent proving.Our file tuning method provides a strong starting point for the new challenges opened by our investigation into theorem proving with context.ACKNOWLEDGEMENTS Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Program for their support.AppendixA miniCTX EXAMPLES Here we give some examples of the miniCTX and its sources to illustrate the format of the data and how and why we collect certain theorems.A.1 EXAMPLE ENTRY An entry in the miniCTX dataset consists of the theorem statement, preceding file contents, and metadata information.For example, given the following theorem s_eq_pow_two in context:   model perform significantly better than the other models.This demonstrates the importance of context information in context-dependent textbook-style problems.Models rely on common symbolic automation.The Math2001 split originally disables powerful automation tactics including simp and nlinarith to promote manual reasoning, akin to traditional textbook exercises.In Table6we evaluate models with the automation disabled, and observe substantial performance drops, confirming a heavy reliance of current models on these automation tactics.An examination of the training corpus further revealed a general dependency on automated tactics within real Lean projects, indicating that our models have learned to rely on these tactics.{ "state": # tactic state , "nextTactic": # pretty-printed next tactic, "srcUpToTactic": # source code in the file up to the tactic invocation, "decl": # declaration without proof (e.g., statement of a theorem), "declUpToTactic": # source code in the declaration up to the tactic invocation, "declId": # unique identifier of the declaration }The full proof data is suitable for making evaluation examples of the form (context, theorem, proof):{ "srcUpToDecl": # source code in the file up to the declaration, "decl": # declaration without proof (e.g., statement of a theorem), "declId": # unique identifier of the declaration, "proof": # proof } Full proof data is also suitable for training a model to directly generate a full proof, and NTP-TOOLKIT also provides Lean source with proof states interleaved, both of which we do not explore in this work.D ADDITIONAL RESULTS AND ANALYSISWe present additional analysis on the composition of miniCTX and additional quantitative and qualitative evaluation of performance on miniCTX.All tests in this section are done on miniCTX-test.For each theorem in miniCTX, we record as metadata whether its human-written proof depends on other definitions or theorems in the same file, and test the performance of baselines on each type.File-tuned models substantially outperform state-tactic tuned models on theorems with definition and/or theorem dependencies.D.3 INTERFERENCE BETWEEN IN-FILE CONTEXT AND RETRIEVED PREMISESIn our experiments, we attempted to supply both in-file context (in the form of preceding code) and premise context (in the form of retrieved premises) to GPT-4o for proving a theorem.In Figure8, we present an analysis of the impact of the length of retrieved premises on the resulting proof success rate.Longer retrieved premises hurt performance.The results indicate that problems with a lower premise-to-context length ratio tend to have higher success rates.Specifically, successful problems often feature relatively shorter premises as proportion of the full context length.This suggests that models are better able to utilize and focus on relevant in-file context when the cross-file premises are proportionally smaller.Conversely, when the length of the premises becomes relatively large File-tuning substantially improves theorem-proving abilities across all cases, but especially when the theorem is easier and the context is longer.compared to the full context, it may overwhelm or distract the model, reducing its ability to effectively utilize the in-file information.This finding highlights the importance of ensuring a balanced integration of premises with the in-file context to maintain model focus and improve proof generation performance.0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Total size of retrieved premises, as proportion of promptPerformance by size of retrieved premises Success FailureFigure8: Impact of length of retrieved premises on GPT-4o model performance.A higher premiseto-context length ratio is correlated with lower success rates, suggesting that too much premise in context overwhelms the model.D.4 EXAMPLE OF LEARNING PROOFS FROM CONTEXTThe file-tuned model is able to utilize the proofs in the context.Here is an example of the model making minimal modification to the proofs from the context: Input:... Output:
Towards a mathematics formalisation assistant using large language models. Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni Narayanan, Anand Tadipatri, arXiv:2211.07524arXiv:2302.12433Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers2022. 2023arXiv preprint</p>
<p>Llemma: An open language model for mathematics. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Graph2tac: Online representation learning of formal math concepts. Lasse Blaauwbroek, Miroslav OlÅ¡Ã¡k, Jason Rute, Fidel Ivan Schaposnik, Jelle Massolo, Vasily Piepenbrock, Pestun, 2024</p>
<p>Baldur: Whole-proof generation and repair with large language models. Emily First, Markus N Rabe, Talia Ringer, Yuriy Brun, 2023</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, 2024</p>
<p>Proof artifact cotraining for theorem proving with language models. Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, Stanislas Polu, International Conference on Learning Representations. 2022</p>
<p>Deepmath-deep sequence models for premise selection. Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas EÃ©n, FranÃ§ois Chollet, Josef Urban, Advances in neural information processing systems. 292016</p>
<p>Albert Qiaochu Jiang, Wenda Li, Jesse Han, Yuhuai Wu, Lisa, Language models of isabelle proofs. 2021</p>
<p>Thor: Wielding hammers to integrate language models and automated theorem provers. Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz OdrzygÃ³ÅºdÅº, Piotr MiÅ‚oÅ›, Yuhuai Wu, Mateja Jamnik, Advances in Neural Information Processing Systems. 202235</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, Yuhuai Wu, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. Alex Kontorovich, 2024a</p>
<p>. Alex Kontorovich, 2024brectangle.lean</p>
<p>Hypertree proof search for neural theorem proving. Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, Xavier Martinet, Advances in neural information processing systems. 202235</p>
<p>Lean Prover Community. repl. 2024</p>
<p>A survey on deep learning for theorem proving. Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si, 2024</p>
<p>Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, arXiv:2309.04295A challenge formal dataset for automated theorem proving. 2023arXiv preprint</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, 10.18653/v1/2023.acl-long.817Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Mathlib Community. The lean mathematical library. Heather Macbeth, 10.1145/3372885.3373824Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL '20. the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL '20ACM2023. January 2020The mechanics of proof</p>
<p>Magnushammer: A transformerbased approach to premise selection. Maciej MikuÅ‚a, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, Åukasz KuciÅ„ski, Piotr MiÅ‚oÅ›, Yuhuai Wu, arXiv:2303.044882023arXiv preprint</p>
<p>Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. Kim Morrison, Automated Deduction-CADE 28: 28th International Conference on Automated Deduction, Virtual Event. Proceedings. Springer2023. July 12-15, 2021. 202128lean-training-data</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, 2020</p>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. 2022</p>
<p>TomÃ¡Å¡ SkÅ™ivan, arXiv: Arxiv-2404.12534Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large language models as copilots for theorem proving in Lean. 2021. 2024arXiv preprintScientific computing in lean</p>
<p>. Terence Tao, Pfr, 2023</p>
<p>Joseph Tooby, - Smith, Heplean, arXiv:2405.08863Digitalising high energy physics. 2024arXiv preprint</p>
<p>How to Prove It: A Structured Approach. J Daniel, Velleman, 2019Cambridge University Press3 edition</p>
<p>. Sean Welleck, Llmlean, 2024</p>
<p>Sean Welleck, Rahul Saha, arXiv:2310.18457Llmstep: Llm proofstep suggestions in lean. 2023arXiv preprint</p>
<p>Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. Z Z Huajian Xin, Junxiao Ren, Zhihong Song, Wanjia Shao, Haocheng Zhao, Bo Wang, Liyue Liu, Xuan Zhang, Qiushi Lu, Wenjun Du, Qihao Gao, Dejian Zhu, Zhibin Yang, Z F Gou, Fuli Wu, Chong Luo, Ruan, arXiv:2408.081522024arXiv preprint</p>
<p>Learning to prove theorems via interacting with proof assistants. Kaiyu Yang, Jia Deng, 2019</p>
<p>LeanDojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, arXiv:2402.06332Internlm-math: Open math large language models toward verifiable reasoning. 2024arXiv preprint</p>
<p>miniF2F: a cross-system benchmark for formal olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
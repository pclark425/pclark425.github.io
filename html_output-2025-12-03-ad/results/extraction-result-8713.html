<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8713 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8713</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8713</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-247187974</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.00600v1.pdf" target="_blank">Dual Embodied-Symbolic Concept Representations for Deep Learning</a></p>
                <p><strong>Paper Abstract:</strong> Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs. Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space. Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space. The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing. As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning. To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching. Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration. We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8713.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8713.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual embodied-symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual embodied-symbolic concept representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid two-level functional model in which concepts have modality-specific embodied feature representations (e.g., visual vectors) and amodal language-specific symbolic representations (e.g., word / knowledge-graph embeddings); both levels interact to drive conceptual processing and are advocated as the target representation for deep learning systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>hybrid embodied-symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented at two interacting functional levels: (1) embodied representations — modality-specific feature vectors encoding sensorimotor/perceptual properties; (2) symbolic representations — amodal, language-oriented graph or vector encodings (word embeddings, KG embeddings) capturing relational and distributional information. Interaction between the two supports tasks requiring fast linguistic heuristics or detailed simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid (distributed + symbolic / feature-based + graph-based)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Category production, grounding of concrete and abstract concepts, language understanding, few-shot class-incremental learning, image-text matching, scene graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper summarizes cognitive-science results showing both sensorimotor (embodied) and linguistic-distributional (symbolic) information predict category-production behavior (linguistic proximity adds variance beyond sensorimotor similarity). Hybrid models best accommodate empirical data for both concrete and abstract concepts; in engineering use-cases, combining embodied and symbolic representations (a) reduces catastrophic forgetting and overfitting in few-shot CIL [described use case], and (b) reduces the semantic gap and improves image-text matching performance in image retrieval / captioning tasks when image embeddings are fused with concept/KG-based semantic embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contrasted with purely embodied (modality-only) or purely symbolic (distributional-only) formats: evidence and engineering use-cases favor the hybrid approach. The paper cites that distributional-only models are efficient but miss perceptual features; embodied-only accounts struggle with abstract concepts. The hybrid model is thus favored by both cognitive findings and applied results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>None fatal in the paper, but it notes challenges: abstract concepts still pose grounding difficulties and require affective/social/linguistic signals in addition to sensorimotor mappings; engineering implementations need good alignment mechanisms and external knowledge (e.g., KG linking) to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>The dual-level format is proposed as the functional foundation for integrating deep learning with symbolic AI: it supports fast linguistic heuristics (linguistic shortcut) and slower sensorimotor simulation, enables better generalization and continual learning, and is necessary for symbol grounding, reasoning and explanation in human-level AI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8713.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied vectors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied concept representations (modality-specific feature vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Modality-specific distributed feature vectors (e.g., visual embeddings for images) that represent perceptual/sensorimotor features of concepts and are used functionally to simulate experience when detailed conceptual information is required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>embodied (feature-vector) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge is represented as modality-specific continuous feature vectors derived from sensorimotor experience (e.g., CNN image embeddings for visual concepts); those vectors encode perceptual attributes and are used for concept recognition, fine-grained discrimination, and sensorimotor simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed, feature-based</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Category production (sensorimotor similarity measures), grounding of concrete concepts, perceptual categorization, image recognition and image-text matching</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sensorimotor similarity (an 11-dimensional sensorimotor strength representation in cited work) predicts order and frequency of items in category production; embodied vectors provide detailed, precise concept representations required when linguistic shortcuts are insufficient. In applied tasks, visual embeddings form the embodied input to fused representations that improve downstream image-text matching.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared to symbolic distributional representations, embodied vectors better capture perceptual/common-feature structure that drives categorical distinctions; however, they are less effective for purely linguistic or abstract-concept tasks and can be computationally heavier (hence linguistic shortcuts).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Embodied-only accounts have difficulty explaining representation of abstract concepts that lack direct perceivable referents; perceptual embeddings alone leave a semantic gap when matching to language.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Embodied vectors functionally support sensorimotor simulation and fine-grained perceptual discrimination, and must be combined with symbolic representations for broad conceptual competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8713.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word embedding / distributional representations (Word2Vec, GloVe, word-association models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Amodal, language-derived continuous vector representations of words learned from corpora (distributional statistics or association networks) that capture lexical co-occurrence and semantic proximity used functionally as symbolic concept encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>distributional / symbolic word representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Words/concepts are represented as dense vectors in a semantic space derived from large text corpora (e.g., Word2Vec local co-occurrence, GloVe global co-occurrence) or as weighted associative link distributions (word-association models); similarity in this space indicates linguistic/contextual relatedness and functional substitutability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / distributed (amodal language-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Category production (linguistic proximity measures), language understanding, semantic prediction, few-shot CIL semantic guidance, image-text matching (as text-side representation)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Linguistic proximity derived from word embeddings predicts category-production ordering and frequency above and beyond sensorimotor similarity; semantic word vectors are used to form superclasses and guide knowledge distillation in few-shot CIL, enabling grouping and transfer even when visual examples are scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Distributional models are computationally cheap and effective at capturing linguistic regularities; contrasted with embodied/perceptual representations, they lack perceptual content but suffice for many tasks (linguistic shortcut hypothesis). Combining them with embodied features yields better performance on multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Corpus-only embeddings fail to exploit explicit semantic relational structures (e.g., logical relations, commonsense links) unless augmented with external knowledge; purely distributional representations also struggle with perceptually-grounded or abstract-emotive nuances without multimodal information.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, word embeddings provide fast, efficient symbolic proxies for concepts and are used by humans as linguistic shortcuts; they should be integrated with embodied representations for tasks requiring grounding or fine-grained perceptual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8713.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge graph embedding (KGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that embed nodes (concepts) and relations from structured concept graphs (knowledge graphs) into low-dimensional continuous vector spaces so that graph semantics and relational structure are captured functionally for tasks like link prediction and concept alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>knowledge-graph / graph-structured symbolic representation (embedded)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge is stored as directed labeled graphs of triples (h, r, t); KGE maps concepts and relations to vectors/matrices in a continuous space to preserve relational structure (e.g., TransE translation h + r ≈ t; RESCAL multiplicative factorization; GCN/GNN approaches incorporate neighborhood and type information).</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic (graph-structured) / distributed embedding of symbols</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Commonsense reasoning, scene-graph grounding, concept-relation inference, knowledge bridging between symbolic graphs and scene graphs, multimodal grounding</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>KGE provides precise structural representation of symbolic conceptual information; translation-based models capture simple translational relations while semantic-matching and GNN-based models capture richer neighborhood/path/substructure information; GCN-based models can incorporate node/relation types and attributes to improve task accuracy. The paper argues KGE is necessary for linking symbolic knowledge with embodied data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Triple-only embedding methods (e.g., TransE) are contrasted with multiplicative/semantic-matching methods (RESCAL) and graph-neural approaches; the latter better capture intrinsic associations and substructure semantics. KGE complements distributional word embeddings and perceptual vectors by explicitly representing relational/structural knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Methods that only consider individual (h,r,t) triples neglect broader intrinsic associations (multi-hop, subgraph patterns) and so can miss deeper semantics; KGs are often symbolic and ungrounded unless multimodal grounding is added.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, KG embeddings provide amodal symbolic scaffolding that can be bridged to embodied data to enable reasoning, disambiguation, and transfer in multimodal and continual learning systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8713.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Image-concept fused</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied-symbolic fused (image-concept) representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fused multimodal representation formed by combining image (visual) embeddings with symbolic concept embeddings expanded via a Scene Concept Graph (SCG), used to close the semantic gap between images and text for image-text matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Aware Semantic Concept Expansion for Image-Text Matching</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>multimodal fused representation (image + concept embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>An image's perceptual embedding is augmented by detected semantic concepts which are expanded through a scene-concept graph (SCG) to include commonly-related concepts; the concept embeddings are fused (e.g., concatenation/attention fusion) with image embeddings to produce a concept-enhanced visual vector used in cross-modal similarity/ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid multimodal fused (distributed + symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Image-text matching, image retrieval, captioning, handling occlusion and long-tailed concepts in scenes</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting image embeddings with SCG-expanded concept embeddings reduces the semantic gap and improves image-text matching performance; common-cooccurence concept expansion helps recover occluded or long-tailed concepts and yields more semantically aligned image representations for ranking losses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared to raw image-only embeddings, fused representations better align with text-side word embeddings; compared to naive concept-detection, SCG expansion supplies commonsense co-occurrence relations that enrich the concept set and improve matching.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Current SCG approach considers only co-occurrence (subject-object pairs) and not full relations in some implementations; fusion quality depends on concept detection accuracy and SCG coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally demonstrates how embodied perceptual vectors can be augmented with symbolic, graph-derived semantics to enable better cross-modal alignment and support retrieval and reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8713.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scene Concept Graph (SCG) / Scene Graph (SG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graphical representations of image scenes: scene graphs are per-image triples <subject, relation, object> capturing detected concepts and relations; Scene Concept Graphs aggregate co-occurring subject-object pairs across images to form commonsense-like concept co-occurrence knowledge used for expanding concept sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Aware Semantic Concept Expansion for Image-Text Matching</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>scene graph / aggregated scene-concept graph</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>A scene graph represents an image as a set of labeled nodes (scene concepts and relations) and directed edges (<subject, relation, object>). The SCG is built by aggregating co-occurrence pairs across many scene graphs to yield a commonsense co-occurrence graph used for concept expansion and semantic enrichment.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic graph-structured (multimodal grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Visual scene understanding, image-text matching, concept expansion for occlusion/long-tail recovery, scene graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregating co-occurrence across images provides commonsense signals (e.g., bedroom↔bed) that improve detection of semantically-related but occluded or long-tailed concepts when expanding detected concepts; using SCG-expanded concepts as symbolic augmentation improves multimodal matching.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>SCG provides complementary relational/co-occurrence structure not present in flat concept lists or pure perceptual embeddings; bridging SGs to external commonsense KGs further integrates image-conditioned semantics with broader world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>SCG in cited work focuses on co-occurrence and may not encode richer relation types; its effectiveness depends on the quality and coverage of aggregated scene graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally positions per-image scene graphs as image-conditioned embodiments of commonsense KGs, enabling alignment between visual instances and amodal symbolic knowledge for richer multimodal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8713.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GB-Net bridging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Bridging Network (GB-Net) for SG–CSKG bridging</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that frames scene graph generation as inference of a bridge between an image-conditioned scene graph (SG) and a larger commonsense knowledge graph (CSKG), iteratively aligning embeddings and creating bridge edges between corresponding nodes/relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging Knowledge Graphs to Generate Scene Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>bridged graph representation (SG linked to CSKG)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Both the scene graph and commonsense KG are treated as knowledge graphs and embedded; a dynamic message-passing and bridging algorithm (via GNNs) iteratively updates node embeddings and bridge edges so that SG nodes/relations are connected to corresponding CSKG concepts/relations, producing an SG embedding that is grounded in CSKG semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid graph-bridging / symbolic + distributed graph embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Scene graph generation, visual commonsense inference, grounding of image concepts in external knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Viewing SG as an image-conditioned embodiment of CSKG enables iterative bridging using GNN message-passing to enhance SG generation with commonsense knowledge; the method produces SG embeddings interconnected with CSKG embeddings, facilitating grounded scene understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Bridging SG and CSKG is contrasted with standalone SG generation; interconnected embeddings exploit broader commonsense structure and outperform approaches that rely only on local image evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Bridging requires reliable alignment signals and sufficiently overlapping ontologies between SG vocabularies and CSKG; bridging complexity and noise in CSKG can affect SG generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally proposes scene graphs as image-conditioned realizations of larger symbolic KGs and provides a mechanism for integrating embodied visual data with structured symbolic commonsense knowledge to improve visual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8713.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Knowledge Graphs (MMKGs) and symbol grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Knowledge graphs augmented with multimodal data (images, text, audio, video) as attributes for concepts/relations, enabling grounding of symbolic nodes to perceptual referents and supporting multimodal representation learning (e.g., IKRL, DKRL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-Modal Knowledge Graph Construction and Application: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>multimodal grounded symbolic graph</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>KG triples are extended to include modality-specific data as attribute values (e.g., (s, 'hasImage', image_id) or (s, r, d)) so that each symbol in the KG can be grounded to example images/audio/text; embeddings for KG structure and modality encodings are learned jointly or projected into a shared space to enable symbol grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic (graph) + embodied (modality-specific embeddings) integrated</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Symbol grounding, multimodal retrieval, knowledge-augmented reasoning, concept image selection, descriptor-based concept representation</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MMKG approaches like IKRL and DKRL integrate images or textual descriptions into KG embedding objectives: IKRL uses multiple images per concept and attention to combine image embeddings projected into KG space; DKRL encodes textual descriptions into vectors and jointly learns description- and structure-based embeddings. Such integration enables better grounded KG representations and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>MMKGs bridge the gap between purely symbolic KGs and modality-only embeddings; they outperform pure-structure KGE on tasks requiring grounding and multimodal retrieval, and provide richer representations than image-only or text-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Challenges include selecting representative/discriminative images for concepts, noisy multimodal attributes, and integrating heterogeneous modalities; representativeness requires clustering/semantic filtering and caption-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, MMKGs are necessary for symbol grounding: linking amodal symbols to perceptual exemplars supports referent mapping, multimodal inference, and is a prerequisite for human-level AI capabilities such as explanation and grounded reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8713.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8713.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional vs association</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional linguistic models vs word-association models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two classes of symbolic linguistic representations: distributional models deriving meaning from corpus co-occurrence (e.g., Word2Vec, GloVe) and word-association models deriving meaning from human associative links; both are used to model human conceptual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>distributional vs associative symbolic representations</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Distributional models represent words by vectors learned from statistical co-occurrence patterns in large corpora; word-association models represent words as weighted distributions over associative links derived from human association data, capturing psychologically-realized associative structure.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / distributional (corpus-derived) and symbolic / associative (network-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Behavioral similarity judgments, category production, modeling concrete vs abstract concept representations</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Empirical comparison reported that multimodal models combining distributional linguistic information with perceptual/affective data better capture human representations (especially for basic-level concepts). The paper reports that both unimodal linguistic models and word-association models were evaluated, and multimodal (linguistic + perceptual/affective) models outperform unimodal ones.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Distributional and association models both capture linguistic-level structure; multimodal combinations (symbolic + embodied) outperform either alone. Word-association models offer a psychologically-grounded alternative to corpus-only distributional embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Corpus-derived distributional models miss explicit semantic relations and perceptual/affective content; association networks require human-collected data and may not scale.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, both distributional and association-based symbolic representations contribute to human conceptual representations, but must be supplemented with embodied/perceptual information to capture full semantic content, particularly for basic-level and some abstract concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production <em>(Rating: 2)</em></li>
                <li>Dual Coding of Knowledge in the Human Brain <em>(Rating: 2)</em></li>
                <li>Visual and Affective Multimodal Models of Word Meaning in Language and Mind <em>(Rating: 2)</em></li>
                <li>Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning <em>(Rating: 2)</em></li>
                <li>Knowledge Aware Semantic Concept Expansion for Image-Text Matching <em>(Rating: 2)</em></li>
                <li>Bridging Knowledge Graphs to Generate Scene Graphs <em>(Rating: 2)</em></li>
                <li>CSKG: The Commonsense Knowledge Graph <em>(Rating: 2)</em></li>
                <li>Multi-Modal Knowledge Graph Construction and Application: A Survey <em>(Rating: 2)</em></li>
                <li>Knowledge Graph Embeddings and Explainable AI <em>(Rating: 1)</em></li>
                <li>A Survey of Knowledge Graph Embedding and Their Applications <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8713",
    "paper_id": "paper-247187974",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "Dual embodied-symbolic",
            "name_full": "Dual embodied-symbolic concept representations",
            "brief_description": "A hybrid two-level functional model in which concepts have modality-specific embodied feature representations (e.g., visual vectors) and amodal language-specific symbolic representations (e.g., word / knowledge-graph embeddings); both levels interact to drive conceptual processing and are advocated as the target representation for deep learning systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "hybrid embodied-symbolic representation",
            "representational_format_description": "Concepts are represented at two interacting functional levels: (1) embodied representations — modality-specific feature vectors encoding sensorimotor/perceptual properties; (2) symbolic representations — amodal, language-oriented graph or vector encodings (word embeddings, KG embeddings) capturing relational and distributional information. Interaction between the two supports tasks requiring fast linguistic heuristics or detailed simulation.",
            "format_type": "hybrid (distributed + symbolic / feature-based + graph-based)",
            "cognitive_task_or_phenomenon": "Category production, grounding of concrete and abstract concepts, language understanding, few-shot class-incremental learning, image-text matching, scene graph generation",
            "key_findings": "Paper summarizes cognitive-science results showing both sensorimotor (embodied) and linguistic-distributional (symbolic) information predict category-production behavior (linguistic proximity adds variance beyond sensorimotor similarity). Hybrid models best accommodate empirical data for both concrete and abstract concepts; in engineering use-cases, combining embodied and symbolic representations (a) reduces catastrophic forgetting and overfitting in few-shot CIL [described use case], and (b) reduces the semantic gap and improves image-text matching performance in image retrieval / captioning tasks when image embeddings are fused with concept/KG-based semantic embeddings.",
            "comparison_with_other_formats": "Contrasted with purely embodied (modality-only) or purely symbolic (distributional-only) formats: evidence and engineering use-cases favor the hybrid approach. The paper cites that distributional-only models are efficient but miss perceptual features; embodied-only accounts struggle with abstract concepts. The hybrid model is thus favored by both cognitive findings and applied results.",
            "limitations_or_counter_evidence": "None fatal in the paper, but it notes challenges: abstract concepts still pose grounding difficulties and require affective/social/linguistic signals in addition to sensorimotor mappings; engineering implementations need good alignment mechanisms and external knowledge (e.g., KG linking) to be effective.",
            "theoretical_claims_or_implications": "The dual-level format is proposed as the functional foundation for integrating deep learning with symbolic AI: it supports fast linguistic heuristics (linguistic shortcut) and slower sensorimotor simulation, enables better generalization and continual learning, and is necessary for symbol grounding, reasoning and explanation in human-level AI.",
            "uuid": "e8713.0",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Embodied vectors",
            "name_full": "Embodied concept representations (modality-specific feature vectors)",
            "brief_description": "Modality-specific distributed feature vectors (e.g., visual embeddings for images) that represent perceptual/sensorimotor features of concepts and are used functionally to simulate experience when detailed conceptual information is required.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "embodied (feature-vector) representation",
            "representational_format_description": "Conceptual knowledge is represented as modality-specific continuous feature vectors derived from sensorimotor experience (e.g., CNN image embeddings for visual concepts); those vectors encode perceptual attributes and are used for concept recognition, fine-grained discrimination, and sensorimotor simulation.",
            "format_type": "distributed, feature-based",
            "cognitive_task_or_phenomenon": "Category production (sensorimotor similarity measures), grounding of concrete concepts, perceptual categorization, image recognition and image-text matching",
            "key_findings": "Sensorimotor similarity (an 11-dimensional sensorimotor strength representation in cited work) predicts order and frequency of items in category production; embodied vectors provide detailed, precise concept representations required when linguistic shortcuts are insufficient. In applied tasks, visual embeddings form the embodied input to fused representations that improve downstream image-text matching.",
            "comparison_with_other_formats": "Compared to symbolic distributional representations, embodied vectors better capture perceptual/common-feature structure that drives categorical distinctions; however, they are less effective for purely linguistic or abstract-concept tasks and can be computationally heavier (hence linguistic shortcuts).",
            "limitations_or_counter_evidence": "Embodied-only accounts have difficulty explaining representation of abstract concepts that lack direct perceivable referents; perceptual embeddings alone leave a semantic gap when matching to language.",
            "theoretical_claims_or_implications": "Embodied vectors functionally support sensorimotor simulation and fine-grained perceptual discrimination, and must be combined with symbolic representations for broad conceptual competence.",
            "uuid": "e8713.1",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Word embeddings",
            "name_full": "Word embedding / distributional representations (Word2Vec, GloVe, word-association models)",
            "brief_description": "Amodal, language-derived continuous vector representations of words learned from corpora (distributional statistics or association networks) that capture lexical co-occurrence and semantic proximity used functionally as symbolic concept encodings.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "distributional / symbolic word representation",
            "representational_format_description": "Words/concepts are represented as dense vectors in a semantic space derived from large text corpora (e.g., Word2Vec local co-occurrence, GloVe global co-occurrence) or as weighted associative link distributions (word-association models); similarity in this space indicates linguistic/contextual relatedness and functional substitutability.",
            "format_type": "symbolic / distributed (amodal language-derived)",
            "cognitive_task_or_phenomenon": "Category production (linguistic proximity measures), language understanding, semantic prediction, few-shot CIL semantic guidance, image-text matching (as text-side representation)",
            "key_findings": "Linguistic proximity derived from word embeddings predicts category-production ordering and frequency above and beyond sensorimotor similarity; semantic word vectors are used to form superclasses and guide knowledge distillation in few-shot CIL, enabling grouping and transfer even when visual examples are scarce.",
            "comparison_with_other_formats": "Distributional models are computationally cheap and effective at capturing linguistic regularities; contrasted with embodied/perceptual representations, they lack perceptual content but suffice for many tasks (linguistic shortcut hypothesis). Combining them with embodied features yields better performance on multimodal tasks.",
            "limitations_or_counter_evidence": "Corpus-only embeddings fail to exploit explicit semantic relational structures (e.g., logical relations, commonsense links) unless augmented with external knowledge; purely distributional representations also struggle with perceptually-grounded or abstract-emotive nuances without multimodal information.",
            "theoretical_claims_or_implications": "Functionally, word embeddings provide fast, efficient symbolic proxies for concepts and are used by humans as linguistic shortcuts; they should be integrated with embodied representations for tasks requiring grounding or fine-grained perceptual detail.",
            "uuid": "e8713.2",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "KGE",
            "name_full": "Knowledge graph embedding (KGE)",
            "brief_description": "A family of methods that embed nodes (concepts) and relations from structured concept graphs (knowledge graphs) into low-dimensional continuous vector spaces so that graph semantics and relational structure are captured functionally for tasks like link prediction and concept alignment.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "knowledge-graph / graph-structured symbolic representation (embedded)",
            "representational_format_description": "Conceptual knowledge is stored as directed labeled graphs of triples (h, r, t); KGE maps concepts and relations to vectors/matrices in a continuous space to preserve relational structure (e.g., TransE translation h + r ≈ t; RESCAL multiplicative factorization; GCN/GNN approaches incorporate neighborhood and type information).",
            "format_type": "symbolic (graph-structured) / distributed embedding of symbols",
            "cognitive_task_or_phenomenon": "Commonsense reasoning, scene-graph grounding, concept-relation inference, knowledge bridging between symbolic graphs and scene graphs, multimodal grounding",
            "key_findings": "KGE provides precise structural representation of symbolic conceptual information; translation-based models capture simple translational relations while semantic-matching and GNN-based models capture richer neighborhood/path/substructure information; GCN-based models can incorporate node/relation types and attributes to improve task accuracy. The paper argues KGE is necessary for linking symbolic knowledge with embodied data.",
            "comparison_with_other_formats": "Triple-only embedding methods (e.g., TransE) are contrasted with multiplicative/semantic-matching methods (RESCAL) and graph-neural approaches; the latter better capture intrinsic associations and substructure semantics. KGE complements distributional word embeddings and perceptual vectors by explicitly representing relational/structural knowledge.",
            "limitations_or_counter_evidence": "Methods that only consider individual (h,r,t) triples neglect broader intrinsic associations (multi-hop, subgraph patterns) and so can miss deeper semantics; KGs are often symbolic and ungrounded unless multimodal grounding is added.",
            "theoretical_claims_or_implications": "Functionally, KG embeddings provide amodal symbolic scaffolding that can be bridged to embodied data to enable reasoning, disambiguation, and transfer in multimodal and continual learning systems.",
            "uuid": "e8713.3",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Image-concept fused",
            "name_full": "Embodied-symbolic fused (image-concept) representation",
            "brief_description": "A fused multimodal representation formed by combining image (visual) embeddings with symbolic concept embeddings expanded via a Scene Concept Graph (SCG), used to close the semantic gap between images and text for image-text matching.",
            "citation_title": "Knowledge Aware Semantic Concept Expansion for Image-Text Matching",
            "mention_or_use": "use",
            "representational_format_name": "multimodal fused representation (image + concept embeddings)",
            "representational_format_description": "An image's perceptual embedding is augmented by detected semantic concepts which are expanded through a scene-concept graph (SCG) to include commonly-related concepts; the concept embeddings are fused (e.g., concatenation/attention fusion) with image embeddings to produce a concept-enhanced visual vector used in cross-modal similarity/ranking.",
            "format_type": "hybrid multimodal fused (distributed + symbolic)",
            "cognitive_task_or_phenomenon": "Image-text matching, image retrieval, captioning, handling occlusion and long-tailed concepts in scenes",
            "key_findings": "Augmenting image embeddings with SCG-expanded concept embeddings reduces the semantic gap and improves image-text matching performance; common-cooccurence concept expansion helps recover occluded or long-tailed concepts and yields more semantically aligned image representations for ranking losses.",
            "comparison_with_other_formats": "Compared to raw image-only embeddings, fused representations better align with text-side word embeddings; compared to naive concept-detection, SCG expansion supplies commonsense co-occurrence relations that enrich the concept set and improve matching.",
            "limitations_or_counter_evidence": "Current SCG approach considers only co-occurrence (subject-object pairs) and not full relations in some implementations; fusion quality depends on concept detection accuracy and SCG coverage.",
            "theoretical_claims_or_implications": "Functionally demonstrates how embodied perceptual vectors can be augmented with symbolic, graph-derived semantics to enable better cross-modal alignment and support retrieval and reasoning tasks.",
            "uuid": "e8713.4",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "SCG",
            "name_full": "Scene Concept Graph (SCG) / Scene Graph (SG)",
            "brief_description": "Graphical representations of image scenes: scene graphs are per-image triples &lt;subject, relation, object&gt; capturing detected concepts and relations; Scene Concept Graphs aggregate co-occurring subject-object pairs across images to form commonsense-like concept co-occurrence knowledge used for expanding concept sets.",
            "citation_title": "Knowledge Aware Semantic Concept Expansion for Image-Text Matching",
            "mention_or_use": "use",
            "representational_format_name": "scene graph / aggregated scene-concept graph",
            "representational_format_description": "A scene graph represents an image as a set of labeled nodes (scene concepts and relations) and directed edges (&lt;subject, relation, object&gt;). The SCG is built by aggregating co-occurrence pairs across many scene graphs to yield a commonsense co-occurrence graph used for concept expansion and semantic enrichment.",
            "format_type": "symbolic graph-structured (multimodal grounding)",
            "cognitive_task_or_phenomenon": "Visual scene understanding, image-text matching, concept expansion for occlusion/long-tail recovery, scene graph generation",
            "key_findings": "Aggregating co-occurrence across images provides commonsense signals (e.g., bedroom↔bed) that improve detection of semantically-related but occluded or long-tailed concepts when expanding detected concepts; using SCG-expanded concepts as symbolic augmentation improves multimodal matching.",
            "comparison_with_other_formats": "SCG provides complementary relational/co-occurrence structure not present in flat concept lists or pure perceptual embeddings; bridging SGs to external commonsense KGs further integrates image-conditioned semantics with broader world knowledge.",
            "limitations_or_counter_evidence": "SCG in cited work focuses on co-occurrence and may not encode richer relation types; its effectiveness depends on the quality and coverage of aggregated scene graphs.",
            "theoretical_claims_or_implications": "Functionally positions per-image scene graphs as image-conditioned embodiments of commonsense KGs, enabling alignment between visual instances and amodal symbolic knowledge for richer multimodal inference.",
            "uuid": "e8713.5",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "GB-Net bridging",
            "name_full": "Graph Bridging Network (GB-Net) for SG–CSKG bridging",
            "brief_description": "A method that frames scene graph generation as inference of a bridge between an image-conditioned scene graph (SG) and a larger commonsense knowledge graph (CSKG), iteratively aligning embeddings and creating bridge edges between corresponding nodes/relations.",
            "citation_title": "Bridging Knowledge Graphs to Generate Scene Graphs",
            "mention_or_use": "use",
            "representational_format_name": "bridged graph representation (SG linked to CSKG)",
            "representational_format_description": "Both the scene graph and commonsense KG are treated as knowledge graphs and embedded; a dynamic message-passing and bridging algorithm (via GNNs) iteratively updates node embeddings and bridge edges so that SG nodes/relations are connected to corresponding CSKG concepts/relations, producing an SG embedding that is grounded in CSKG semantics.",
            "format_type": "hybrid graph-bridging / symbolic + distributed graph embeddings",
            "cognitive_task_or_phenomenon": "Scene graph generation, visual commonsense inference, grounding of image concepts in external knowledge",
            "key_findings": "Viewing SG as an image-conditioned embodiment of CSKG enables iterative bridging using GNN message-passing to enhance SG generation with commonsense knowledge; the method produces SG embeddings interconnected with CSKG embeddings, facilitating grounded scene understanding.",
            "comparison_with_other_formats": "Bridging SG and CSKG is contrasted with standalone SG generation; interconnected embeddings exploit broader commonsense structure and outperform approaches that rely only on local image evidence.",
            "limitations_or_counter_evidence": "Bridging requires reliable alignment signals and sufficiently overlapping ontologies between SG vocabularies and CSKG; bridging complexity and noise in CSKG can affect SG generation quality.",
            "theoretical_claims_or_implications": "Functionally proposes scene graphs as image-conditioned realizations of larger symbolic KGs and provides a mechanism for integrating embodied visual data with structured symbolic commonsense knowledge to improve visual reasoning.",
            "uuid": "e8713.6",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "MMKG",
            "name_full": "Multimodal Knowledge Graphs (MMKGs) and symbol grounding",
            "brief_description": "Knowledge graphs augmented with multimodal data (images, text, audio, video) as attributes for concepts/relations, enabling grounding of symbolic nodes to perceptual referents and supporting multimodal representation learning (e.g., IKRL, DKRL).",
            "citation_title": "Multi-Modal Knowledge Graph Construction and Application: A Survey",
            "mention_or_use": "use",
            "representational_format_name": "multimodal grounded symbolic graph",
            "representational_format_description": "KG triples are extended to include modality-specific data as attribute values (e.g., (s, 'hasImage', image_id) or (s, r, d)) so that each symbol in the KG can be grounded to example images/audio/text; embeddings for KG structure and modality encodings are learned jointly or projected into a shared space to enable symbol grounding.",
            "format_type": "symbolic (graph) + embodied (modality-specific embeddings) integrated",
            "cognitive_task_or_phenomenon": "Symbol grounding, multimodal retrieval, knowledge-augmented reasoning, concept image selection, descriptor-based concept representation",
            "key_findings": "MMKG approaches like IKRL and DKRL integrate images or textual descriptions into KG embedding objectives: IKRL uses multiple images per concept and attention to combine image embeddings projected into KG space; DKRL encodes textual descriptions into vectors and jointly learns description- and structure-based embeddings. Such integration enables better grounded KG representations and downstream tasks.",
            "comparison_with_other_formats": "MMKGs bridge the gap between purely symbolic KGs and modality-only embeddings; they outperform pure-structure KGE on tasks requiring grounding and multimodal retrieval, and provide richer representations than image-only or text-only approaches.",
            "limitations_or_counter_evidence": "Challenges include selecting representative/discriminative images for concepts, noisy multimodal attributes, and integrating heterogeneous modalities; representativeness requires clustering/semantic filtering and caption-based evaluation.",
            "theoretical_claims_or_implications": "Functionally, MMKGs are necessary for symbol grounding: linking amodal symbols to perceptual exemplars supports referent mapping, multimodal inference, and is a prerequisite for human-level AI capabilities such as explanation and grounded reasoning.",
            "uuid": "e8713.7",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Distributional vs association",
            "name_full": "Distributional linguistic models vs word-association models",
            "brief_description": "Two classes of symbolic linguistic representations: distributional models deriving meaning from corpus co-occurrence (e.g., Word2Vec, GloVe) and word-association models deriving meaning from human associative links; both are used to model human conceptual representations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representational_format_name": "distributional vs associative symbolic representations",
            "representational_format_description": "Distributional models represent words by vectors learned from statistical co-occurrence patterns in large corpora; word-association models represent words as weighted distributions over associative links derived from human association data, capturing psychologically-realized associative structure.",
            "format_type": "symbolic / distributional (corpus-derived) and symbolic / associative (network-derived)",
            "cognitive_task_or_phenomenon": "Behavioral similarity judgments, category production, modeling concrete vs abstract concept representations",
            "key_findings": "Empirical comparison reported that multimodal models combining distributional linguistic information with perceptual/affective data better capture human representations (especially for basic-level concepts). The paper reports that both unimodal linguistic models and word-association models were evaluated, and multimodal (linguistic + perceptual/affective) models outperform unimodal ones.",
            "comparison_with_other_formats": "Distributional and association models both capture linguistic-level structure; multimodal combinations (symbolic + embodied) outperform either alone. Word-association models offer a psychologically-grounded alternative to corpus-only distributional embeddings.",
            "limitations_or_counter_evidence": "Corpus-derived distributional models miss explicit semantic relations and perceptual/affective content; association networks require human-collected data and may not scale.",
            "theoretical_claims_or_implications": "Functionally, both distributional and association-based symbolic representations contribute to human conceptual representations, but must be supplemented with embodied/perceptual information to capture full semantic content, particularly for basic-level and some abstract concepts.",
            "uuid": "e8713.8",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "rating": 2,
            "sanitized_title": "linguistic_distributional_knowledge_and_sensorimotor_grounding_both_contribute_to_semantic_category_production"
        },
        {
            "paper_title": "Dual Coding of Knowledge in the Human Brain",
            "rating": 2,
            "sanitized_title": "dual_coding_of_knowledge_in_the_human_brain"
        },
        {
            "paper_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "rating": 2,
            "sanitized_title": "visual_and_affective_multimodal_models_of_word_meaning_in_language_and_mind"
        },
        {
            "paper_title": "Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning",
            "rating": 2,
            "sanitized_title": "semanticaware_knowledge_distillation_for_fewshot_classincremental_learning"
        },
        {
            "paper_title": "Knowledge Aware Semantic Concept Expansion for Image-Text Matching",
            "rating": 2,
            "sanitized_title": "knowledge_aware_semantic_concept_expansion_for_imagetext_matching"
        },
        {
            "paper_title": "Bridging Knowledge Graphs to Generate Scene Graphs",
            "rating": 2,
            "sanitized_title": "bridging_knowledge_graphs_to_generate_scene_graphs"
        },
        {
            "paper_title": "CSKG: The Commonsense Knowledge Graph",
            "rating": 2,
            "sanitized_title": "cskg_the_commonsense_knowledge_graph"
        },
        {
            "paper_title": "Multi-Modal Knowledge Graph Construction and Application: A Survey",
            "rating": 2,
            "sanitized_title": "multimodal_knowledge_graph_construction_and_application_a_survey"
        },
        {
            "paper_title": "Knowledge Graph Embeddings and Explainable AI",
            "rating": 1,
            "sanitized_title": "knowledge_graph_embeddings_and_explainable_ai"
        },
        {
            "paper_title": "A Survey of Knowledge Graph Embedding and Their Applications",
            "rating": 1,
            "sanitized_title": "a_survey_of_knowledge_graph_embedding_and_their_applications"
        }
    ],
    "cost": 0.01557825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dual Embodied-Symbolic Concept Representations for Deep Learning</p>
<p>Daniel T Chang dtchang43@gmail.com 
Dual Embodied-Symbolic Concept Representations for Deep Learning
C9D883FB721C375593C778749DBE936F
Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs.Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching.Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
<p>Introduction</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations [1][2]: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concepts in the form of concept graphs.For concrete concepts, the two levels are associated / connected.The embodied level corresponds to sensorimotor-derived knowledge representations of the dual-coding framework [4]; the symbolic level corresponds to language-derived knowledge representations of that framework.</p>
<p>Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.</p>
<p>Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.Traditionally, they are learned separately and used independently.For example, deep learning for computer vision learns embodied concept representations in the form of visual feature vectors for image classification, whereas deep learning for natural language processing learns symbolic concept representations in the form of semantic word vectors for sentiment analysis.The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5][6][7].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system.Further, they typically interact to drive conceptual processing.</p>
<p>As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for fewshot class incremental learning (CIL) and embodied-symbolic fused representation for image-text matching.The first use case [16] demonstrates that embodied-symbolic knowledge distillation mitigates both the catastrophic forgetting problem for CIL and the overfitting problem for few-shot learning.The second use case [17] demonstrates that embodied-symbolic fused representation closes the semantic gap between images and text leading to improved performance when matching an image with text.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.The first example [19] presents a unified formulation of scene graphs (SGs) and commonsense knowledge graphs (CSKGs), where a SG is seen as an image-conditioned embodiment of a CSKG.The second example [21] grounds symbols in knowledge graphs (KGs) to corresponding image, text, sound and video data and maps symbols to their corresponding referents with meanings in the physical world, which is a key step towards the realization of human-level AI.</p>
<p>Human Conceptual System: Symbolic and Embodied</p>
<p>The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing.</p>
<p>In the following we discuss three recent findings in cognitive science that demonstrate the crucial role of dual embodiedsymbolic concept representations in human cognition.</p>
<p>Category Production</p>
<p>A common way of testing how concepts are structured and accessed from long-term memory is with a category production task, whereby a participant is presented with a category name such as 'animal', and asked to name concepts belonging to that category.In a category production study [5], measures of sensorimotor similarity between a category and member concept (based on an 11-dimensional representation of sensorimotor strength) and linguistic proximity between the category name and member-concept name (based on word co-occurrence derived from a large corpus) are used to test category production performance.Both measures predict the order and frequency of category production, but linguistic proximity has an effect above and beyond sensorimotor similarity.</p>
<p>Sensorimotor and linguistic distributional information, therefore, are found to offer an explanation to the mechanisms driving responses in category production tasks [5].In terms of linguistic distributional information (i.e., symbolic information), it is evident that the shared linguistic contexts between member-concept names and category name (e.g., between cat / dog and 'animal') in corpus-derived linguistic space is an effective predictor of category membership.In terms of sensorimotor information (i.e., embodied information), as suggested by many theories of conceptual structure, categorical distinctions emerge from common features (e.g., fur, four-legged) of member concepts that we perceive when interacting with the world.Furthermore, sensorimotor and linguistic distributional information typically interact to drive conceptual processing, and the linguistic shortcut hypothesis is the process by which people arrive at the most-frequently-named and first-named member concepts of a category.</p>
<p>Grounding of Concrete and Abstract Concepts</p>
<p>Concrete concepts (e.g., animal) have perceivable referents.In general, their semantic content can be clearly characterized, and their conceptual taxonomies can be unequivocally defined.Evidence obtained with concrete concepts [6] suggests interplay between modality-specific, multimodal and amodal semantic hub regions.Modality-specific and multimodal regions represent conceptual features, whereas amodal semantic hubs code conceptual information in an overarching supramodal fashion.The available evidence is most consistent with hybrid models of conceptual representations combining modality-specific and multimodal circuits with amodal conceptual hubs, i.e., combining embodied and symbolic representations.</p>
<p>Abstract concepts (e.g., justice) have referents which cannot be directly perceived, like mental or emotional states, abstract ideas, social constellations and scientific theories.Their semantic content is highly variable across individuals and contexts.As such, abstract concepts are a particular challenge for embodied / grounded cognition theories [6] because at the first glance, it is hard to imagine how abstract concepts, without a referent which can be perceived or acted upon, could be grounded in the sensorimotor brain systems.Embodied / grounded cognition theories, however, have been refined [6] in order to account for the representation of abstract concepts.Besides sensorimotor information due to metaphoric mapping or due to relations to classes of situations, the relevance of emotional, introspective, social and linguistic information has been stressed.</p>
<p>As is the case with concrete concepts, the findings with regard to abstract concepts [6] can be accommodated best by hybrid models of conceptual representations assuming an interaction between modality-specific, multimodal and amodal hub areas.Modality-specific systems include the sensorimotor systems, but also the modal systems involved in the processing of emotions, introspections, mentalizing, social constellations, and language.The latter modal systems are probably more important for abstract than for concrete concepts.</p>
<p>Meaning in Language and Mind</p>
<p>There are different theories on how much linguistic and sensorimotor representations contribute to meaning [7].In embodied theories, meaning is based on the relation between words and internal bodily states corresponding to multiple modalities, such as vision, olfaction, and perhaps even internal affective states.By contrast, symbolic theories suggest that the meaning of word (e.g., rose) can be derived in a recursive fashion by considering its relation to the meaning of words in its linguistic context (e.g., red, flower).Current theories of semantics tend to posit that both symbolic and embodied information contribute to meaning.</p>
<p>A study [7] is made to evaluate how well different kinds of models account for people's representations of both concrete and abstract concepts.The models include unimodal linguistic models as well as multimodal models which combine linguistic with perceptual or affective information.There are two types of linguistic models considered: the distributional linguistic model which derives word meaning from word co-occurrences derived from large language corpora, and the word association model which measures the meaning of a word as a distribution of respectively weighted associative links encoded in a large semantic network.The study demonstrates that both visual and affective multimodal models better capture behavior that reflects human representations, especially for basic-level concepts that belong to the same superordinate category.The conclusion is that multimodal information (i.e., symbolic and embodied information) is important for capturing both abstract and concrete concepts.</p>
<p>Symbolic Concept Representations</p>
<p>Embodied concept representations (i.e., modality-specific concept-oriented feature representations) have been discussed previously [1][2][3].In particular, we recommend their learning using exemplar-based contrastive self-supervised learning (CSSL) [3] since it is concept (class) centric and it supports class incremental learning.</p>
<p>In the following we discuss symbolic concept representations (i.e., amodal, language-specific concept graphs) since they are an integral part of dual embodied-symbolic concept representations, but we have not discussed them previously.They come in two forms: word embedding for representing concepts (namely, word semantics in natural language corpora or texts), and knowledge graph embedding for representing concept graphs (a.k.a.knowledge graphs), i.e., conceptual knowledge (e.g.commonsense knowledge).</p>
<p>Note that we will not discuss either word embedding learning or knowledge-graph embedding learning since they are of secondly importance to the focus of this paper.They are discussed in [9][10] and [12][13][14] respectively.</p>
<p>Word Embedding</p>
<p>Word embeddings [8][9] are dense representations of words in semantic vector spaces generated from language corpora or texts, in which semantically similar words have similar embedding vectors.Word embeddings have played an important role for tasks of natural language processing including complex and pertinent ones such as information retrieval and sentiment analysis.They are efficient to learn, highly scalable for large corpora (thousands of millions of words).</p>
<p>Two most widely used word embedding models [8][9][10] are Word2Vec and GloVe.Word2Vec employs two model architectures: Continuous Bag-of-Words (CBOW) model which aims to predict the occurrence of a word given other words that constitute its context, and Skip Gram (SG) model which deals with predicting a context given the word.Word2Vec considers only local word co-occurrences.GloVe is based on a model that reduces the dimensionality of a global cooccurrence matrix of the word-word type in a corpus, with the statistics of the entire corpus captured directly by the model.</p>
<p>GloVe focuses on global word co-occurrences.</p>
<p>For deep-learning based natural language processing [10] word embedding is the basic building block that maps words in the input sentences into continuous space vectors, and usually used (pretrained) in the first layer of a neural network.</p>
<p>Based on the word embedding, complex networks such as recurrent neural networks (RNNs) can be used for feature extraction and build, for example, context-aware word embedding or phrase / sentence embedding.</p>
<p>Note that the methods that generate word embedding based purely on information in a language corpus or text fail to take advantage of the semantic relational structure that exits between words in concurrent contexts.To overcome this limitation, the corpus or text is enhanced with extra morphological, syntactic, semantic and domain knowledge from knowledge sources (e.g., Wikipedia, Wordnet) to generate knowledge-aware word embedding [11].</p>
<p>Knowledge Graph Embedding</p>
<p>The knowledge graph (KG) [12][13][14][15] (a.k.a.concept graph) is a representation of conceptual knowledge (specifically, structured relational information) in the form of concepts and relations between them.We can view a KG as a set of statements (facts) having the form of subject-predicate-object triples, using the notation (h, r, t) (head, relation, tail) to identify a statement.We can also view a KG as a directed labeled graph, where nodes represent concepts and edges represent relations between concepts.</p>
<p>Knowledge graph embedding (KGE) [12][13][14][15] is a widely adopted approach to KG representation in which concepts and relations are embedded in low-dimensional continuous vector spaces.Most methods create a vector for each concept and each relation.These embeddings are generated in a way to capture latent properties of the semantics in the KG: similar concepts and similar relations will be represented with similar vectors.KGE offers precise, effective and structural representation of symbolic conceptual information.</p>
<p>Most of the KGE approaches rely mainly on the use of the subject-predicate-object triples present in the KG to generate the vector representations, (h, r, t), for (head, relation, tail) [12][13][14][15].These approaches can be broadly classified into two groups: translation-based models and semantic matching models.Translation-based models (e.g., TransE) are based on learning the translations from the head concept to the tail concept.They use distance-based measures to generate the similarity score for a pair of concepts and their relations.The training objective is to achieve, mathematically, h + r ~ t.</p>
<p>Semantic matching models (e.g., RESCAL) use a multiplicative approach and represent the relations as matrices / tensors in the vector space.For example, RESCAL relies on a tensor factorization approach upon the 3-dimensional tensor generated by considering subject-predicate-object as the 3 dimensions of the tensor.Note that these models only consider each individual fact, while their intrinsic associations are neglected, which is not sufficient for capturing deeper semantics for better embedding.They, therefore, cannot meet the requirements of KGE.</p>
<p>New approaches leverage the graph nature of KG, and use neural network (NN) based models for various tasks [14][15].</p>
<p>When treated as a graph, KG can be seen as a heterogeneous graph, with the logical relations of more importance than the graph structure.NN-based models can consider the type of concept or relation, neighborhood / substructure information, path information, and temporal information.The use of convolutional neural networks (CNNs) or attention mechanisms also helps to generate better embeddings.Examples of CNN-based models include ConvE and ConvKB [14].</p>
<p>Graph neural networks (GNNs) are neural networks that can be directly applied to graphs, and provide an easy way to generate node-level, edge-level, and graph-level embeddings.They have a somewhat universal architecture in common, referred to as Graph Convolutional Networks (GCNs) which use deep, multi-layer processing known as message passing.</p>
<p>Examples of GCN-based models include RGCN and SACN [14].</p>
<p>GCNs have a strong ability to mine the underlying semantics of KGs [14].In general, GCN-based models can incorporate additional information, such as node types, relation types, node attributes and substructures, to generate better embeddings.For example, if the node information of a multi-hop domain can be aggregated, the accuracy of the model in specific tasks can be greatly improved.A knowledge distillation approach suitable for few-shot CIL is proposed in [16], which is based on the use of dual To mitigate the overfitting problem, multiple visual embeddings for classes are generated, where each is designed specifically for a group of classes.The semantic word vectors are used to separate classes into several groups.The number of groups / visual embeddings is defined by the superclass (cluster) knowledge obtained from the semantic word vector space.</p>
<p>In the semantic space, there is a semantic word vector for each class.The set of superclasses is attained from the semantic word vector space representations of the base classes, and are then held fixed.</p>
<p>For the first (base) task, involving base classes, the steps for obtaining groups / visual embeddings are:</p>
<p> Train the network backbone on the base classes, which is then kept frozen.</p>
<p> Apply k-means clustering, where k = N, on base semantic word vectors and assign a superclass (cluster) label to each base class.</p>
<p> Train N embeddings on the base task using superclass labels as group identity.</p>
<p>For other subsequent (novel) task, involving novel classes, the cluster centers (obtained in the base task) are used to assign superclass labels to novel classes.To assign a superclass label to novel classes, the minimum Euclidean distance between the semantic word vector of a novel class and cluster centers is used.Hence, given a novel class, there is a selection of groups / visual embeddings that each may be more or less suited.</p>
<p>An attention module is used to merge multiple visual embeddings of a class to generate its final visual embedding, i.e., visual vector.A mapping module is then used to project the visual vector from the visual space into the semantic space to align the visual vector with its associated semantic word vector.For the first (base) task, training involves attention loss and classification loss; for other subsequent (novel) task, training involves attention loss, distillation loss (i.e., alignment loss) and classification loss.</p>
<p>Embodied-Symbolic Fused Representation for Image-Text Matching</p>
<p>Image and text matching [17] is an important vision-language cross-modality task for many applications including image retrieval and caption.Before calculating the similarity between an image and text, a matching model needs to obtain a rich representation of the image (and text) first.Most of the current image-text matching models utilize pre-trained neural networks to extract feature embeddings as the representation of images.The image embeddings, however, fail to extract highlevel semantic information.So the semantic gap between images and text leads to limited performance when matching an image with text.</p>
<p>Learning semantic concepts is useful to enhance image representation and can significantly improve the performance of both image-to-text and text-to-image retrieval.Frequently co-occurred concepts in the same image (scene), e.g.bedroom and bed, can provide commonsense knowledge to discover other semantic-related concepts.[17] uses a Scene Concept Graph (SCG) to support this by aggregating image scene graphs and extracting frequently co-occurred concepts as commonsense knowledge.Moreover, it proposes a novel model to incorporate this knowledge to improve image-text matching.Specifically, semantic concepts are detected from images and then expanded by the SCG to include commonly-related concepts (which may be occluded or long-tailed).Afterwards, it fuses their representations with the image embeddings, as semantic-enhanced image embeddings, to use (with text embeddings) for image-text matching.</p>
<p>The model uses dual embodied-symbolic concept representations.The visual embodied representation exists in the form of image embeddings.There are two symbolic representations.The text symbolic representation exists in the form of (context-aware) word embeddings.The concept symbolic (semantic) representation for images exists in the form of concept embeddings.Finally, there is an embodied-symbolic (image-concept) fused representation existing in the form of conceptenhanced image embeddings.</p>
<p>The scene graph [17] of an image is a graph consisting of concepts and relations between them.It can be represented as a set of triples of <subject, relation, object>.The SCG is constructed from scene graphs by aggregating co-occurred <subject, object> pairs from scene graphs of all images.To generate the concept embeddings for an image, first, a concept detection module (a multi-label image classification model) is used to extract semantic concepts from the image on a small concept vocabulary.With the SCG in hand, a concept expansion module is then used to expand the semantic concepts to include commonly-related concepts.Finally, a concept prediction module is used to predict relevant concepts from these and generate the concept embeddings.The concept embeddings of an image are fused with its visual embedding, by the imageconcept fusion module, to generate a concept-enhanced image representation.</p>
<p>The image-text matching problem is formulated as a ranking model.Given the input image and the text, the output is the similarity score of matching their respective visual and language representations, with the visual representation being an image-concept (embodied-symbolic) fused representation and the language representation being word embeddings.To learn image and text matching as well as image-relevant semantic concepts jointly in an end-to-end fashion, the loss function consists of two parts: image-text matching loss and concept prediction loss.</p>
<p>Deep Learning and Symbolic AI Integration</p>
<p>Symbolic AI and deep learning both have strength and weakness, which tend to be each other's opposites.A significant challenge today [18] is to effect a reconciliation.Symbolic AI is based on manipulation of abstract compositional representations whose elements stand for concepts and relations.Therefore, to facilitate reconciliation, a key objective for deep learning is to develop architectures capable of discovering concepts and relations in raw data, and learning how to represent them.</p>
<p>An excellent example for doing this for image data has been discussed in 5 Embodied-Symbolic Fused Representation for Image-Text Matching.(Note that only co-occurred concepts are considered in [17].However, their relations will also be considered in future work.)As discussed there, dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.</p>
<p>In the following, we discuss two additional important examples of deep learning and symbolic AI integration.</p>
<p>Scene Graph Generation with Knowledge Graph Bridging</p>
<p>Scene graphs (SGs) are powerful representations that extract semantic concepts and their relations from images, which facilitate visual comprehension and reasoning.(For an example usage, see 5 Embodied-Symbolic Fused Representation for Image-Text Matching.)A SG can be represented as a set of triples of <subject, relation, object>.On the other hand, commonsense knowledge graphs (CSKGs) [20] are rich repositories that encode how the world is structured (i.e., commonsense knowledge), and how common concepts are related and interact.</p>
<p>GB-Net (Graph Bridging Network) [19] presents a unified formulation of these two constructs, where a SG is seen as an image-conditioned embodiment of a CSKG.Based on this perspective, the SG generation is formulated as the inference of a bridge between the SG and CSKG, where each concept or relation in the SG must be linked to its corresponding concept or relation in the CSKG.Specifically, both SG and CSKG are defined as special types of knowledge graph (KG): GB-Net fuses the SG embedding and CSKG embedding through a dynamic message passing and bridging algorithm using a graph neural networks (GNN).The method iteratively propagates messages to update nodes, then compares nodes to update bridge edges, and repeats until the two graphs are carefully connected.This results in the SG embedding with bridges to the CSKG embedding.</p>
<p>Multimodal Knowledge Graphs</p>
<p>Knowledge graphs (KGs) have found great use in a wide range of applications including text understanding, recommendation system, natural language question answering, and image understanding (see 5 Embodied-Symbolic Fused Representation for Image-Text Matching and 6.1 Scene Graph Generation with Knowledge Graph Bridging).More and more KGs have been created, covering common sense knowledge [20] (see 6.1 Scene Graph Generation with Knowledge Graph Bridging), lexical knowledge, encyclopedia knowledge, taxonomic knowledge, and geographic knowledge.</p>
<p>Most of the existing KGs are represented with pure symbols, denoted in the form of text, without grounding to the physical world experience.However, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing (see 2 Human Conceptual System: Symbolic and Embodied).Therefore, it is necessary to ground symbols in KGs to corresponding image, text, sound and video data and map symbols to their corresponding referents with meanings in the physical world.That is, the multi-modalization</p>
<p>Conclusion</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs (a.k.a.knowledge graphs).The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.That is, deep learning should learn from data not only modality-specific embodied representations such as image embeddings, text embeddings, etc., but also the corresponding amodal symbolic (semantic) representation as knowledge graph embeddings, with links to commonsense knowledge graphs.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration, which is an important direction for deep learning and AI since their integration reinforces each other's strength, compensates each other's weakness, and takes a major step toward human-level AI (e.g., grounding in experience / data, understanding, reasoning, explanation, etc.).</p>
<p>4</p>
<p>Embodied-Symbolic Knowledge Distillation for Few-Shot CIL In CIL [2], a model learns tasks continually, with each task containing a batch of new classes.In few-shot CIL [3], only the training set of the first (base) task may have large-scale training data for base classes, while other subsequent (novel) task just contains few-shot instances for novel classes.Few-shot CIL requires transferring knowledge (i.e., knowledge distillation) from old classes to new classes in solving the catastrophic forgetting problem, which is generic to CIL.The additional challenge of few-shot learning is the overfitting problem.</p>
<p>embodied-symbolic concept (class) representations.The embodied (visual) representation exists in the form of visual vectors.Semantic word vectors (Word2Vec and GloVe) are used as the symbolic (semantic) representation to facilitate knowledge distillation, which are generated from unsupervised learning on an unannotated text corpus.The semantically guided network does not add new parameters while adding new classes incrementally.The knowledge distillation process only includes semantic word vectors of novel classes in addition to the base classes, which are used to help the network remembering base class training, generalizing to novel classes, and generating well-separated embodied representation (visual vectors) of classes.Note that though the network may not have had the opportunity to see instances of novel classes, nevertheless, novel classes may very well share semantic properties with base classes it has seen.For example, if hyena is a novel class, many typical hyena attributes like 'face', 'body', etc., may have been seen by the network from base classes.</p>
<p></p>
<p>A KG is a set of nodes of type concept (C) or relation (R), and a set of directed, weighted edges (Ε) between the nodes. A CSKG is a type of KG with commonsense concept (CC) nodes and commonsense relation (CR) nodes.Commonsense edges are of four types: CC-&gt;CC, CC-&gt;CR, CR-&gt;CC, and CR-&gt;CR. A SG is a type of KG with scene concept (SC) nodes and scene relation (SR) nodes.Scene edges are of four types: SC-&gt;SR (subjectOf), SC-&gt;SR (objectOf), SR-&gt;SC (hasSubject), and SR-&gt;SC (hasObject). The SG and CSKG are connected through four types of bridge edges: SC-&gt;CC, SR-&gt;CR, CC-&gt;SC and CR-&gt;SR.GB-Net uses dual embodied-symbolic concept representations.The visual embodied representation for images exists in the form of image embeddings.There are two symbolic concept representations.The CSKG symbolic representation exists in the form of CSKG embedding (of commonsense nodes and edges).The SG symbolic (semantic) representation for images exists in the form of SG embedding (of scene nodes and edges).The CSKG embedding and the SG embedding are interconnected through the bridge edges.</p>
<p>Acknowledgement: Thanks to my wife Hedy (郑期芳) for her support.of KGs[21]is an inevitable key step towards the realization of human-level AI, which results in Multimodal Knowledge Graphs (MMKGs)[12,15,21].To support symbol grounding in MMKG[21], one can take multimodal data as particular attribute values of concepts or relations.This can be denoted in a triple (s, r, d), where s denotes a concept or relation, d denotes one of its corresponding multimodal data, and the relation r is, e.g., "hasImage" when d is an image.Symbol grounding, therefore, can be divided into concept grounding and relation grounding.As an example, concept grounding aims to find representative, discriminative and diverse images for visual concepts.A major challenge is to find representative images for a visual concept from a group of relevant images.The representativeness and discriminativeness of images can be scored in terms of results of cluster-based methods, such as K-means, based on visual embeddings.The captions of images can also be utilized to evaluate the representativeness and discriminativeness of images, at the semantic level, based on text embeddings.IKRL and DKRL are two well-known examples of MMKG. IKRL (Image-embodied Knowledge RepresentationLearning)[12,15]provides a method to integrate images inside the scoring function of the KG embedding model (TransE).Essentially, IKRL uses multiple images for each concept and use the AlexNet CNN to generate embeddings for the images.These embeddings are then selected and combined with the use of attention to be finally projected in the KG embedding space.DKRL (Description-Embodied Knowledge Representation Learning)[12,15], on the other hand, includes the description of concepts in the representation.It uses a CNN to encode the concept description into a vector representation and uses this representation in the loss function.DKRL learns two embeddings for each concept, one that is structure-based (i.e., KG, like TransE) and one that is based on the concept descriptions.The two embeddings are interconnected / integrated.The above discussion, though brief, shows that MMKG uses dual embodied-symbolic concept representations.The KG symbolic representation exists in the form of KG embedding.There are various embodied representations, depending on the multimodal data involved.For image data, the visual embodied representation exists in the form of image embedding.For text (descriptions), the textual embodied representation exists in the form of text embedding.The KG embedding and the modality-specific embedding(s) are interconnected / integrated.
Concept-Oriented Deep Learning. T Daniel, Chang, arXiv:1806.017562018arXiv preprint</p>
<p>Concept Representation Learning with Contrastive Self-Supervised Learning. T Daniel, Chang, arXiv:2112.056772021arXiv preprint</p>
<p>Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning. T Daniel, Chang, arXiv:2202.026012022arXiv preprint</p>
<p>Dual Coding of Knowledge in the Human Brain. Y Bi, Trends in Cognitive Sciences. 2510October 2021</p>
<p>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production. B Banks, C Wingfield, L Connell, Cogn. Sci. 45e130552021</p>
<p>Varieties of Abstract Concepts and Their Grounding in Perception or Action. M Kiefer, M Harpaintner, Open Psychol. 20202</p>
<p>Visual and Affective Multimodal Models of Word Meaning in Language and Mind. S D Deyne, D Navarro, G Collell, A Perfors, Cogn, Sci. 45e129222021</p>
<p>A Systematic Literature Review on Word Embeddings. L Gutiérrez, B Keith, Proc. Int. Conf. Softw. Process Improvement. Int. Conf. Softw. ess Improvement2018</p>
<p>U Naseem, I Razzak, S K Khan, M Prasad, arXiv:2010.15036A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-art Word Representation Language Models. 2020</p>
<p>M Zhou, N Duan, S Liu, H.-Y Shum, Progress in Neural NLP: Modeling, Learning, and Reasoning," in Engineering. 20206</p>
<p>Incorporating Extra Knowledge to Enhance Word Embedding. A Roy, S Pan, Proceedings of the 29th International Joint Conference on Artificial Intelligence. the 29th International Joint Conference on Artificial Intelligence202020</p>
<p>Knowledge Graph Embeddings and Explainable AI. Federico Bianchi, Gaetano Rossiello, Luca Costabello, Matteo Palmonari, Pasquale Minervini, arXiv:2004.148432020arXiv preprint</p>
<p>A Survey of Knowledge Graph Embedding and Their Applications. Shivani Choudhary, Tarun Luthra, Ashima Mittal, Rajat Singh, arXiv:2107.078422021arXiv preprint</p>
<p>A Survey on Knowledge Graph Embeddings for Link Prediction. Meihong Wang, Linling Qiu, Xiaoli Wang, Symmetry. 202113485</p>
<p>Application and Evaluation of Knowledge Graph Embeddings in Biomedical Data. Mona Alshahrani, Maha A Thafar, Magbubah Essack, PeerJ Computer Science. 72021</p>
<p>Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning. Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, Mehrtash Harandi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2021</p>
<p>Knowledge Aware Semantic Concept Expansion for Image-Text Matching. B Shi, L Ji, P Lu, Z Niu, N Duan, IJCAI. 2019</p>
<p>Reconciling Deep Learning with Symbolic Artificial Intelligence: Representing Objects and Relations. Marta Garnelo, Murray Shanahan, Current Opinion in Behavioral Sciences. 292019</p>
<p>Bridging Knowledge Graphs to Generate Scene Graphs. A Zareian, S Karaman, S F Chang, arXiv:2001.023142020arXiv preprint</p>
<p>CSKG: The Commonsense Knowledge Graph. F Ilievski, P Szekely, B Zhang, Extended Semantic Web Conference (ESWC). 2020</p>
<p>Multi-Modal Knowledge Graph Construction and Application: A Survey. Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, Nicholas Jing Yuan, arXiv:2202.057862022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
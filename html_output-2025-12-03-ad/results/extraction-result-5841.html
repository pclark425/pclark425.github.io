<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5841 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5841</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5841</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-265697818</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.04556v2.pdf" target="_blank">Large Language Models for Mathematicians</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LLMs to change how mathematicians work.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5841",
    "paper_id": "paper-265697818",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00368775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Mathematicians
2 Apr 2024</p>
<p>Simon Frieder 
Department of Computer Science
University of Oxford
OxfordUK</p>
<p>Julius Berner 
Department of Computing and Mathematical Sciences
Caltech, PasadenaUS</p>
<p>Philipp Petersen 
Faculty of Mathematics and Research Network Data Science
University of Vienna
ViennaAustria</p>
<p>Thomas Lukasiewicz 
Institute of Logic and Computation
Department of Computer Science
Vienna University of Technology
ViennaAustria</p>
<p>University of Oxford
OxfordUK</p>
<p>Large Language Models for Mathematicians
2 Apr 2024EE3C96763D94F399E4B133BB9499B8CAarXiv:2312.04556v2[cs.CL]
Large language models (LLMs) such as ChatGPT have received immense interest for their generalpurpose language understanding and, in particular, their ability to generate high-quality text or computer code.For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work.In this note, we discuss to what extent they can aid professional mathematicians.We first provide a mathematical description of the transformer model used in all modern language models.Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models.Finally, we shed light on the potential of LLMs to change how mathematicians work.</p>
<p>Introduction</p>
<p>Large language models (LLMs) recently revolutionized the field of natural language processing, bringing forth unparalleled progress for applications such as conversational bots or text generation.Advances in deep-learning algorithms, computational resources, as well as substantial engineering efforts, allowed to train language models with billions of parameters and on massive datasets, such as the 800GB large The Pile [14].Being trained on increasingly larger datasets, language models infer essential aspects of the structure of language 1 , paving the way from task-specific to general-purpose language understanding.As such, these models can also parse mathematical questions and solve (sometimes surprisingly complex) mathematical problems and tasks-however, they also make embarrassing mistakes.</p>
<p>Let us have a look at two examples that ChatGPT (web version of 30-January-2023) produced 2 .Both examples below were taken from [13]; the input to the model, as well as its output, consists of L A T E X code, which we have compiled below for better readability:</p>
<p>" prompt " : " Find a f u n c t i o n f : R → R t h a t i s c o n t i n u o u s a t p r e c i s e l y one p o i n t ." , " output " : "One example o f such a f u n c t i o n i s
f (x) = x 2 if x ̸ = 0 1 if x = 0.
This f u n c t i o n i s c o n t i n u o u s a t x = 0 but not a t any o t h e r p o i n t ."</p>
<p>" prompt " : " Let µ be a measure , ∥f ∥∞ = inf{M : µ({x : |f The first response is nonsensical on all levels, whereas the second is correct and helpful.With these examples in mind, we might ask ourselves:</p>
<p>How can large language models assist human mathematicians in their work?</p>
<p>To address this question, this article proceeds in the following way: First, we provide an overview of modern language models.We clarify the theory of how language models work, how their main building block-the transformer architecture-is set up, and why these models can perform sufficient mathematics to assist mathematicians in their daily work.Understanding this architecture will also highlight how an LLM produces an answer to a mathematical question -which differs tremendously from how a mathematician arrives at an answer.Then, we present empirical evidence attesting to the abilities of language models, in particular state-of-the-art models, such as ChatGPT and GPT-4.We end with an outlook on the potential future impacts on mathematicians and mathematics in general.</p>
<p>Overview of Modern Language Models</p>
<p>The concept of language models has a long history.One of the pioneering achievements, dating back to the year 2000, presented one of the initial instances of what we now refer to as word embeddings within the framework of neural networks [3]; see Section 3 for a definition.</p>
<p>Most previous approaches were rooted in estimating probabilities over trigrams (or, more general, n-grams).</p>
<p>An n-gram is a sequence of n adjacent elements from a string of word pieces, so-called tokens, which could be syllables, letters, words, or base pairs according to the context.In the sentence "The quick brown fox jumps over the lazy dog", the sequence "quick brown fox" is an example of a trigram.Models based on n-grams had severe limitations: For example, if a trigram does not appear in the training corpus (or contains words that were not in the vocabulary of the corpus), no meaningful way of estimating its probability existed.By using a form of word embeddings, these problems are circumvented.The model proposed by [3] dominated all other pure n-gram models.The authors note that improvements can be made regarding the "architecture, computational efficiency, and taking advantage of prior knowledge".</p>
<p>The introduction of the transformer architecture [40] in 2017 marked the most striking advancement in terms of neural network architectures: On the one hand, the attention mechanism modeled the structure of the language more faithfully; on the other hand, it was an architecture that was easily parallelizable on modern hardware (see Section 3 for details).This led to a series of further milestones and improvements:</p>
<p>In 2018, the Bidirectional Encoder Representations from Transformers (BERT) model [7] was introduced, a successor to the original transformer, which inspired a vast number of successors on its own, such as RoBERTa [22], or DistilBERT [32].BERT (and its successors) were notable because classical pipelines (e.g., defining text representations, carrying out parts-of-speech tagging) were all subsumed by BERT-type models [36], which could easily be fine-tuned to specific tasks.At roughly the same time as the BERT model, the Generative Pre-Trained Transformer (GPT) model was introduced by OpenAI [28].This was a further variation on the original transformer architecture and is the first version of the model that underlies ChatGPT, which was released in 2022 [25], and is closely related to InstructGPT [27].</p>
<p>The last milestone consists of the LLaMA [38] and LLaMA2 models [39] introduced in 2023, months after GPT-4 [26].Their importance lies in being the first publicly released models, the code and weights of which were easily accessible and rivaled the performance of GPT-4; in the technical report associated with GPT-4 it is stated: "this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar".The LLaMA models led to a democratization of language models and to a large number of further successors, such as Stanford's Alpaca3 model, or the Vicuna4 model, which have since been used in a wide array of contexts.As these models evolved, the number of their parameters, as well as the sizes of the dataset on which they were trained, kept increasing, from the order of millions of parameters (in case of [3,40,7]), to billions [39,38], to trillions [8,30], see Figure 1.While the main trend indicates increasing model sizes, there is a countertrend to make the models smaller while retaining the performance.The DistilBERT model is an example of this.Scaling the size of the architectures and the amount of training data enabled unprecedented capabilities for the resulting LLMs, eliminating the need for fine-tuning for specific tasks.</p>
<p>Figure 1: A selection of representative, modern language models is presented along with their parameter counts (in billions), which are displayed above.The y-axis is a log-axis, and model ranges are displayed (two horizontal dots), where available, for each model.We observe that a wide range of parameters appears, between 28 million and 1.2 trillion.For ChatGPT, exact parameter counts are not available but are taken from InstructGPT, which is a sibling model on which ChatGPT is based.For GPT-4, parameter counts are not available.</p>
<p>Technical Background</p>
<p>In the following, we seek to give a brief introduction to the inner workings of LLMs.We refer to [45,23] for surveys and further details.We do not strive to present state-of-the-art models and techniques in natural language processing but focus on a conceptual understanding of the functionality of LLMs.In particular, we will restrict the presentation to one of the most popular architectures, the transformer [40] (see Section 2 for context regarding the importance of this model).Our description below is a simplified, mathematical summary loosely based on the (open-source) code5 of GPT-2 [29], see also Figure 2 for an overview.</p>
<p>Transformer Architecture</p>
<p>Let us explore how a transformer architecture predicts text based on some provided input, often referred to as prompt.In line with successful models, such as the GPT and LLaMA series, we focus on the setting, where the model iteratively predicts the next word pieces (i.e., tokens) based on a given sequence of tokens.This procedure is coined autoregressive since the prediction of new tokens is only based on previous tokens.Such conditional sequence generation tasks using autoregressive transformers are often referred to as decoder-only settings.Although there is a maximum context length in practice, we will work with sequences of arbitrary length for ease of presentation.We define the shorthand notation S * := n∈N S n for a set S to denote the set of sequences s = (s (i) ) n i=1 ⊂ S with arbitrary length n ∈ N.For a function F : S 1 → S 2 , we denote by F * : S * 1 → S * 2 the entrywise applied mapping given by
F * (s) := (F(s (i) )) n i=1 . (1)
Tokenization K. First, we want to clarify how we define word pieces, i.e., tokens.Mathematically speaking, we seek an injective mapping K : A * → T * from the given text, i.e., a sequence a = (a (i) ) N i=1 of characters in an alphabet A to a sequence of n ≤ N tokens (t (i) ) n i=1 , where typically T := {1, 2, . . ., M }.</p>
<p>Tokenization Embedding + Positional encoding Prediction head Transformer layers Sampling</p>
<p>Prove that pi Prove that pi is
(Pro) (ve) ( that)( pi)
( is)
('s) ( has) Normalization Self-Attention Normalization Multiplayer Perceptron</p>
<p>Skip connection Skip connection Skip connection</p>
<p>Figure 2: Illustration of the operations of an LLM for the input text "Prove that pi".The token indices, as well as the probabilities for the next token, are taken from GPT-2 [29] using the implementation in the transformers library [42].The highest probability for the next token is assigned to 318 corresponding to the word "is".</p>
<p>To represent text on a computer, we could encode every character a (i) individually, similar to Unicode.While this would lead to a small vocabulary T of tokens, it yields long sequences where individual tokens do not capture any linguistic information.For LLMs, one often employs subword tokenization [34], which creates a vocabulary of subwords by analyzing large text corpora and iteratively merging frequently occurring sequences of characters.Akin to a compression problem, one balances the length n of the sequences and the size6 M of the vocabulary.As an example, the GPT-4 tokenizer7 splits the word "discontinuity" into the subwords "dis" (prefix), "contin" (subword capturing the root of "continuous"), and "uity" (suffix).Another example can be found in Figure 2.</p>
<p>Embedding E. To use these tokens (given by the indices of the subwords in the vocabulary) in a neural network, we embed each token t (i) into the same Euclidean space E := R d .Intuitively, we seek a map E : T → E, such that the distance ∥E(t (i) ) − E(t (j) )∥ corresponds to the linguistic similarity of the subwords represented by the tokens t (i) and t (j) .In practice, such an embedding is often initialized with a sequence of M random initial embeddings and learned jointly with the transformer model from data.</p>
<p>Positional Encoding P. Since E operates on each token t (i) independently, the embeddings E(t (i) ) do not contain information on the position i of the (sub)words within a sentence8 .Thus, one typically adds so-called positional encodings, which can be described by a mapping P :
E * → E * . A commonly used choice is of the form P((e (i) ) n i=1 ) := (e (i) + p(i)) n i=1 ,(2)
where p : N → E can be a prescribed injective function, e.g., a sinusoid [40], or learned (similar to the embedding E) [28].</p>
<p>In summary, tokenization K, followed by an application of E to each token and the positional encoding P, maps the text a ∈ A * to a sequence of embeddings
e := (P • E * • K) (a) ∈ E * . (3)
where the length of e depends on a and the tokenization algorithm.</p>
<p>Transformer T .The transformer can be represented as a neural network T : E * → E * .It is trained to map a sequence of embeddings e to another sequence of the same length containing contextual information.</p>
<p>Based on the desired autoregressive structure, where the prediction of the next token only depends on the previous tokens, we want the i-th element of T (e) to contain information about all the embeddings (e (j) ) j≤i , however, to be independent of (e (j) ) j&gt;i .</p>
<p>The transformer is typically defined by a composition of L ∈ N blocks, consisting of self-attention maps A ℓ , entrywise applied normalizing layers N A,ℓ , N M,ℓ , and feed-forward multiplayer perceptrons M ℓ , i.e.,
T := Id + M * L • N * M,L • Id + A L • N * A,L • • • • • Id + M * 1 • N * M,1 • Id + A 1 • N * A,1 .(4)
In the above, Id denotes the identity mapping, commonly known as a skip or residual connection, and the addition is understood entrywise.The indices of the layers N , M, and A in ( 4) indicate the use of different trainable parameters in each of the layers.Let us describe these layers in more detail below.</p>
<p>Layers: Normalization N .The normalizing layer can be interpreted as a re-parametrization with a learnable mean and standard deviation to stabilize training.For instance, using layer normalization
N : E → E, we compute N (e) = diag(s) σ (e − µ) + m,(5)
where 2 are the mean and variance of e ∈ E, and s, m ∈ E are learnable parameters [1].
µ = 1 d d i=1 e i and σ 2 = 1 d d i=1 (e i − µ)
Layers: Multilayer Perceptrons N .The Multilayer perception (MLP) is a standard feed-forward neural network consisting of compositions of affine mappings and nonlinear activation functions.Let us define by L (m,n) : R m → R n an affine mapping L (m,n) (x) := W x + b, where the weight matrix W ∈ R n×m and the bias vector b ∈ R m are learnable.Moreover, let ϱ : R → R be an activation function, e.g., the GELU activation function ϱ(x) := x Φ(x), where Φ is the standard Gaussian cumulative distribution function [17].A typical MLP M : E → E used in transformers is then given by
M := L (d,D) • ϱ * • L (D,d) , (6)
where D ∈ N with D ≥ d.</p>
<p>Layers: Self-Attention A. As can be seen in ( 4) and Figure 2, the self-attention layer A : E * → E * is the only layer that combines embeddings of different tokens; in other words, it attends to other tokens.</p>
<p>Let us denote the input to the layer by (e (i) ) n i=1 and focus on the i-th output.We first compute the (normalized) inner products
s (i) j = 1 √ k L (k,d) query (e (i) ), L (k,d)
key (e (j) ) , j = 1, . . ., i, (7) with given k ∈ N. On a high level, we can interpret s (i) = (s
(i) j ) i j=1 ⊂ R as similarities between the embedding L (k,d)
query (e (i) ) of the i-th token (i.e., the so-called query) and the embeddings L (k,d) key (e (j) ) of the other tokens (i.e., keys); to satisfy the autoregressive structure, we only consider j ≤ i.To normalize s (i)  to probabilities, we can further use a softmax layer softmax : R * → R * given by softmax(s (i) ) j := exp s
(i) j i k=1 exp s (i) k , j = 1, . . . , i. (8)
We can now interpret softmax(s (i) ) j as the probability for the i-th query to "attend" to the j-th key.</p>
<p>The self-attention layer A can then be defined as
A(e) i := L (k,d)   i j=1 softmax(s (i) ) j L (k,d) value (e (j) )   , i = 1, . . . , n,(9)
where the outputs of
L (k,d)
value are often referred to as the values of the token embeddings e (j) , and where the learnable affine layer L (k,d) maps the weighted average of values back to E = R d .</p>
<p>Note that in practice, one typically considers a sum of h ∈ N such attention layers (so-called heads), each with dimension k = d/h [40,21].Moreover, instead of considering vectors of variable length i, a mask enforces the autoregressive structure so that all operations can be efficiently batched.</p>
<p>Prediction Head H.The prediction head or un-embedding layer can be represented as a mapping H : E * → ∆ M , where
∆ M := P ∈ [0, 1] M : M i=1 P i = 1 (10)
denotes the probability simplex in R M .It maps the sequence of transformed embeddings (ẽ (i) ) n i=1 := T (e) to a vector P ∈ ∆ M , where P i describes the probability of predicting i ∈ T as the next token.Since the transformed embedding of the last token, i.e., ẽ(n) , contains information about the whole input text, a simple approach is to use a linear mapping composed with a softmax layer and define
P := (softmax • L (M,d) )(ẽ (n) ). (11)
Sampling S.There are multiple sampling strategies S : ∆ M → T to arrive at the final prediction for the next token t (n+1) , see, e.g., [18]; the arguably simplest one, so-called greedy sampling, predicts the token with the highest probability, i.e., t (n+1) = S(P ) := arg max i=1,...,M
P i ,(12)
see Figure 2. One can then apply the same operations to the extended sequence t = (t (i) ) n+1 i=1 , i.e.,
t (n+2) := (S • H • T • P • E * ) (t)(13)
to iteratively compute further tokens 9 .Due to the autoregressive structure, this can efficiently be done by caching the previous (intermediate) results and only considering the computations for the new token.</p>
<p>Training</p>
<p>During training, we transform text corpora into sequences of tokens, such that, for a given sequence (t i ) n i=1 , we already know the next token t n+1 based on the underlying text.One can thus compute the deviation D between the predicted probabilities P of the next token and the ground-truth t n+1 , typically using a cross-entropy loss; in practice, this procedure can be parallelized to compute average losses across many predictions.Using automatic-differentiation, one then computes the derivative ∇ θ D of the average loss D with respect to the learnable parameters θ ∈ R p of the transformer T , the embedding E, the prediction head H (and the positional encoding P if it is trainable).Updating the parameter by subtracting a sufficiently small multiple λ ∈ (0, ∞) of the derivative, i.e., θ k+1 = θ k − λ∇ θ D, one can iteratively minimize the loss-a method known as stochastic gradient descent.This is the essential mechanism by which word occurrence probabilities are estimated by training from raw data.With substantial engineering efforts, more elaborate versions of such training schemes can be parallelized on large GPU clusters and scaled to immense amounts of data.To get an idea of the dimensions, the largest LLaMA2 model with p = 70 • 10 9 parameters was trained for more than 1.7 million GPU hours on about 2 trillion tokens of data from publicly available sources [39].</p>
<p>Training Costs and Emissions</p>
<p>Training LLMs, as described in the previous section, is a computationally very intensive process and, therefore, costly to carry out in terms of electricity usage (assuming all the hardware would be in place).However, information about training costs and CO 2 emissions is not consistently provided in the literature.Notable exceptions include the LaMDA model.The authors [37] report that a total of 451MWh was consumed during training, and, as a result, approximately 26 tons of CO 2 were emitted.Using historic US prices10 of 0.148 dollars per kWh, this amounts to a cost of 66, 748 dollars.We note that costs may vary by country and by the energy source used to produce energy [35].The GLaM model consumes, when trained on the largest dataset, similarly 456MWh and emits 40.2 tons of CO 2 , which places it thus in a similar category to the LaMDA model in terms of cost and emission.</p>
<p>However, more modern LLMs incur significantly more energy consumption and emissions.For instance, the training of LLaMA2 (using 1.7 million hours on GPUs with a power consumption of about 400W) emitted more than 291 tons of Carbon dioxide equivalent (CO 2 -eq) [39].LLM vendors (such as OpenAI) typically do not release information about the costs (either in terms of consumed megawatt-hours or (rented) GPU-hours) of training their models, so only vague estimates are possible, which are nonetheless staggering.For example, training the older-generation GPT-3 model [4] was estimated, using GPU-hour prices from that time, to run up costs of approximately 4.6 million dollars [20].</p>
<p>LLMs for Mathematics</p>
<p>With the foundations of LLMs now well-established, we turn our attention to their application in supporting professional mathematicians.While mathematicians engage in a broad spectrum of mathematical activities, such as performing simulations, modeling, and computation, we focus on the arguably most important task: the capacity of LLMs to generate mathematical proofs.In this sense, our article differs from [41], which reviews other general deep-learning approaches to mathematics.</p>
<p>When using an LLM to assist in the task of theorem proving, the simplest way is to directly prompt the model to prove the statement instead of using it for individual steps or other tasks that will be described below.However, many issues have been found with this approach.A primary concern is that mathematical arguments hinge on the precision of logic; a single wrong statement very likely invalidates the entire proof.Assuming that LLMs have a non-negligible, independent probability of error with each predicted word, the likelihood of producing a correct proof diminishes exponentially with increasing text length.This was also empirically observed in [16], where an LLM turned out to have a higher accuracy in solving computation tasks if it was asked to skip intermediate steps.</p>
<p>The autoregressive nature of LLMs introduces another critical issue.Once a statement is made, LLMs typically do not revisit or revise their arguments.This process diverges significantly from the methodologies of most mathematicians.Rarely does a mathematician draft a complete and detailed proof in a single attempt.Instead, the process often involves crafting a rough sketch, omitting small steps in which we have confidence, iterating, and refining until the proof reaches completion.Furthermore, LLMs may construct entirely valid proofs for questions different from those posed.Such an instance was exemplified in the introduction when the LLM was prompted for a function that is continuous at only one point.Given the prevalence of similar but distinct problems in training datasets, LLMs are likely to respond to the more commonly encountered variations of a question.In this case, a question for a function that is discontinuous at only one point.Similarly, they may prove a theorem under stronger assumptions than stated without making this explicit.</p>
<p>Finally, being based solely on statistical relations of language, LLMs struggle considerably with arithmetic problems: These often occur when an LLM has to complete a task, such as carrying out addition or multiplication (in particular, if the involved numbers are large).The reason for this is that no numerical solver is built into LLMs.Steps towards overcoming this have recently been made by employing a Toolformer approach [33].An instance of this is the WolframAlpha plugin that is available for GPT-4.</p>
<p>In summary, when LLMs are used to prove theorems, they are susceptible to a range of errors.These errors were examined in [13], to be discussed in the following chapter.Consequently, a more collaborative approach, incorporating human expertise, is advisable.The following strategies appear to be sensible:</p>
<p>• Literature/search engine: The LLM can be prompted to explain a definition, find the established name for a vaguely described concept, or find references for a certain statement.In this context, two crucial considerations arise.First, LLMs are known for generating plausible yet fictitious content.This phenomenon is often referred to as hallucinations.Therefore, its answer to our queries needs to be verified.Second, the LLM may exacerbate biases in research.This can occur when an LLM overlooks inadequately cited work, effectively burying it while disproportionately recommending over-hyped articles.</p>
<p>• Brainstorming/Idea Generation: An LLM can be asked to provide a high-level idea of how to prove a theorem.While this will not produce the full result, it closely resembles the style of a mathematician.There is, however, no guarantee that this idea will be very insightful or lead to something.Being trained on a large corpus of mathematical arguments, an LLM will likely be biased towards recommending the most standard ideas.This may not be helpful for a mathematician who is already an expert in a specific field.However, it could be very valuable for a mathematician trying to enter a new area.</p>
<p>• Proof-checking: An LLM can be asked to find mistakes in a given proof.While there is no guarantee whatsoever that it will find all errors, the ones it finds can often be immediately confirmed as actual mistakes by a mathematician.This is helpful but not reliable.The LLM will likely focus on syntactical correctness over semantic correctness and hence overlook complex errors.</p>
<p>• Collaborative writing: An LLM can be asked to provide parts or a sketch of a proof and then, after taking feedback from the user, improve parts, repair errors, and add more details.In [5], the interactive performance of three LLMs (InstructGPT, ChatGPT, and GPT-4) on mathematical queries has been measured on a cohort of users.It was attempted to solve mathematical problems by using these models as an assistant.Asking definitions, general mathematical questions (not strictly related to the problem), and proof steps were the three most common use cases.It is important to keep in mind that this approach still is susceptible to introducing errors.The study found that self-assessment of individuals-whether they had correctly solved the problem using an LLM-was not always correct.</p>
<p>The approaches above are sensible for the successful employment of modern multi-purpose LLMs.In addition, we anticipate that LLMs specifically designed to prove theorems will be developed in the future.One avenue to achieve this is by combining LLM-generated proofs with interactive theorem provers [6,2].First steps in this direction have already been taken and appear to be very promising [12,11,43,44].</p>
<p>Measuring LLM Performance on Mathematics</p>
<p>In [13], an empirical study was carried out to study the mathematical reasoning abilities of three LLMs which were considered to be state-of-the-art in terms of general performance: Two ChatGPT versions (9-January-2023 and 30-January-2023) and GPT-4.The prompts used to carry out the evaluation correspond to some of the use cases discussed in Section 4.</p>
<p>(PP) Producing proofs: Exercises from well-known textbooks (Probability Theory by R. Durret [9], Topology by J. R. Munkres [24], and Functional Analysis by W. Rudin [31]) were fed to the LLMs.</p>
<p>(FH) Filling holes: Proofs with a gap were given to the LLMs, and they were asked to fill the gap.</p>
<p>(SE) Acting as a mathematical search engine: The LLMs were asked to give a definition of concepts such as: "What is a Banach space?".In addition, they were asked to provide the name of definitions, as in "How is a complete normed vector space called?".Finally, they were prompted to provide proof ideas used in famous theorems.</p>
<p>(CO) Computation: The LLMs were given mathematical tasks in which quantities had to be computed.</p>
<p>To carry out this analysis, the GHOSTS dataset was introduced; see Table 1 for a more detailed description of its subdatasets (which make up the acronym "GHOSTS"), as well as how they correspond to the use cases listed above.It consists of 709 prompts, each of which was given to the three considered models.The responses of the LLMs were rated by professional mathematicians 11 on a scale between one (failure to understand the query) and five (perfect or almost perfect output).The obtained ratings are shown in Figure 3.We can make the following observations:</p>
<p>Table 1: A summary of all the files from all the subdatasets comprising the GHOSTS dataset, together with their size, i.e., the number of prompts and their associated attribute tags.</p>
<p>Subdataset Name Size Type</p>
<p>Grad-Text</p>
<p>GPT-4</p>
<p>Figure 3: Average rating for each file in each subdataset (bold) of GHOSTS for the studied versions of ChatGPT and GPT-4.The maximal ranking is 5, and the minimal ranking, where the question was at least understood, is 2; the rating of 1 indicates that the answer completely misses the question.Thus, a reasonable passing grade, i.e., 50% of points, corresponds to a score of 3.5, indicated by the dotted line.</p>
<p>The error bars represent 95% confidence intervals.</p>
<p>GPT-4</p>
<p>Figure 4: A Sankey diagram of how the ratings evolve from the 9-January-2023 version of ChatGPT to the 30-January-2023 version ChatGPT, and subsequently to GPT-4 (from top to bottom), with all models evaluated on a representative subset of GHOSTS.Each score is color-coded, and the line widths are proportional to the number of ratings.</p>
<p>•The LLMs work well as a search engine: ChatGPT and GPT-4 achieved an excellent score when we asked for definitions of a concept or the name of a theorem or definition.</p>
<p>• ChatGPT and GPT-4 struggle with hard questions: No version of the tested LLMs achieved satisfactory results on the hardest problem set-Olympiad-Problem-Solving.Similarly, on the functional analysis questions from Rudin (Chapter 2)-arguably the second most advanced set of questions in the datasetthe results were underwhelming.The ratings were substantially better for more straightforward questions, such as the exercises in topology, which only ask for simple set theory and Boolean logic.</p>
<p>• Good results for simple computations: Despite not having a built-in numerical solver, GPT-4 performed reasonably well on questions requiring simple computation.For more sophisticated computation involved in Symbolic-Integration, ChatGPT failed and GPT-4 barely achieved a passing grade.</p>
<p>•User input can have a positive effect: On the Holes-in-Proofs subdataset, we see excellent results in some of the problems.It appears that the additional context given by the user helps the LLMs to produce more truthful solutions.A similar observation was made in a separate experiment where carefully crafted prompts (so-called prompt engineering) slightly increased the score of the LLM on the Olympiad-Problem-Solving subdataset [13].</p>
<p>The improvement in rating, as the models become more sophisticated, is indicated in the Sankey diagram in Figure 4, which shows how ratings change from one model version to another.We use a representative subset of GHOSTS to advise the Sankey diagram.We observe that between the 9-January-2023 version and the 30-January-2023 version, scores are approximately shuffled, and no substantial increase of the net score occurs.For the newer generation, i.e., GPT-4, we can observe a significant improvement in the ratings.This supports the general trend that more modern models also perform better on challenging mathematical reasoning tasks.</p>
<p>In [13], a subdivision of each question type (as indicated in Figure 3) was made, and a much more fine-grained benchmark was introduced that also differentiates potential failure modes.We refer the reader to [13] for further information on more detailed analyses.</p>
<p>Conclusion</p>
<p>Our study has highlighted how LLMs possess a remarkable capacity to assist mathematicians in various ways, from detecting and filling gaps in theorems to acting as search engines and finding definitions from descriptions of mathematical objects.We have shed light on the inner mechanism of the core piece of architecture that powers modern LLMs, the transformer, and how the way they produce an answer to mathematical questions differs starkly from human reasoning.</p>
<p>The ability of LLMs to interact with users in a natural language format has made mathematics more accessible, allowing for a broader range of individuals to engage with mathematical research and education.While the full potential of LLMs in automating mathematics is yet to be realized, our findings suggest a promising synergy between human mathematicians and artificial intelligence.High exposure of the work of mathematicians to the effects of LLMs has also been reported in [10].However, we want to caution that, currently, LLMs are not on a trajectory to replace mathematicians.In [13], it was shown that even the best-performing model has trouble with mathematics on upper-undergraduate difficulties, such as when it is tasked to solve exercises from W. Rudin's Functional Analysis [31].The performance of LLMs has been reported to be below that of humans also in related domains, such as coding challenges in computer science [19].Nonetheless, we anticipate that the emergence of LLMs will be a challenge for education and research.Simple exercises or homework and individual steps in mathematical research will be gradually supported by automation or become obsolete.</p>
<p>(x)| &gt; M }) = 0} and ∥g∥1 = |g|dµ .Prove t h a t |f g|dµ ≤ ∥g∥1∥f ∥∞ ." , " output " : " Let M = ∥f ∥∞ and ϵ &gt; 0 .Then , by t h e d e f i n i t i o n o f ∥f ∥∞ , we have µ({x : |f (x)| &gt; M + ϵ}) = 0 .T h e r e f o r e , |f g|dµ = |f |≤M +ϵ |f g|dµ + |f |&gt;M +ϵ |f g|dµ ≤ (M + ϵ) |g|dµ + |f |&gt;M +ϵ |f ||g|dµ.The s e c o n d term on t h e r i g h t s i d e i s z e r o b e c a u s e µ(x : |f (x)| &gt; M + ϵ) = 0 .So we have |f g|dµ ≤ (M + ϵ) |g|dµ = (M + ϵ)∥g∥1 .As ϵ can be a r b i t r a r i l y s m a l l , we can l e t i t go t o z e r o and we g e t |f g|dµ ≤ M ∥g∥1 = ∥f ∥∞∥g∥1 .Hence proved ."</p>
<p>https://github.com/tatsu-lab/stanford_alpaca
https://lmsys.org/blog/2023-03-30-vicuna/
https://github.com/openai/gpt-2
The LLaMA and LLaMA2 models employ vocabularies T with M = 32000 tokens[39]. GPT-2 uses M = 50257, and other models in the GPT series, e.g., GPT-3.5-turbo and GPT-4, even use M = 100277 tokens, see https://github.com/ openai/tiktoken.
See https://platform.openai.com/tokenizer.
In some settings, where the transformer architecture is permutation invariant, positional encodings are strictly necessary. In our decoder-only setting, this is not the case[15]; however, the encodings still seem to improve performance
There is usually a stopping criterion based, e.g., on a special token or the entropy of P .
https://data.bls.gov/timeseries/APU000072610
The authors of the present paper form a subset of the evaluators.
AcknowledgementsS. Frieder and T. Lukasiewicz were partially supported by the AXA Research Fund.
J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450Layer normalization. 2016arXiv preprint</p>
<p>The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria. B Barras, S Boutin, C Cornes, J Courant, J.-C Filliatre, E Gimenez, H Herbelin, G Huet, C Munoz, C Murthy, 1997</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, Advances in Neural Information Processing Systems. 200013</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Evaluating language models for mathematics through interactions. K M Collins, A Q Jiang, S Frieder, L Wong, M Zilka, U Bhatt, T Lukasiewicz, Y Wu, J B Tenenbaum, W Hart, arXiv:2306.016942023arXiv preprint</p>
<p>The Lean theorem prover (system description). L De Moura, S Kong, J Avigad, F Van Doorn, J Raumer, Automated Deduction-CADE-25: 25th International Conference on Automated Deduction. 2015</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>GLaM: Efficient scaling of language models with mixture-of-experts. N Du, Y Huang, A M Dai, S Tong, D Lepikhin, Y Xu, M Krikun, Y Zhou, A W Yu, O Firat, International Conference on Machine Learning. PMLR2022</p>
<p>Probability: Theory and Examples. R Durrett, Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press2019</p>
<p>T Eloundou, S Manning, P Mishkin, D Rock, arXiv:2303.10130GPTs are GPTs: An early look at the labor market impact potential of large language models. 2023arXiv preprint</p>
<p>Baldur: whole-proof generation and repair with large language models. E First, M N Rabe, T Ringer, Y Brun, arXiv:2303.049102023arXiv preprint</p>
<p>LLM vs ITP. S Frieder, M Alawadhi, Trimmel, K Rashid, Gy, The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23. 2023</p>
<p>Mathematical capabilities of ChatGPT. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P C Petersen, A Chevalier, J Berner, Advances in Neural Information Processing Systems. 202336</p>
<p>The Pile: An 800GB dataset of diverse text for language modeling. L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, arXiv:2101.000272020arXiv preprint</p>
<p>Transformer language models without positional encodings still learn positional information. A Haviv, O Ram, O Press, P Izsak, O Levy, arXiv:2203.166342022arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the MATH dataset. 2021arXiv preprint</p>
<p>D Hendrycks, K Gimpel, arXiv:1606.08415Gaussian error linear units (GELUs). 2016arXiv preprint</p>
<p>A Holtzman, J Buys, L Du, M Forbes, Y Choi, arXiv:1904.09751The curious case of neural text degeneration. 2019arXiv preprint</p>
<p>A Koubaa, B Qureshi, A Ammar, Z Khan, W Boulila, L Ghouti, arXiv:2305.06934Humans are still better than ChatGPT: Case of the IEEEXtreme competition. 2023arXiv preprint</p>
<p>OpenAI's GPT-3 language model: A technical overview. C Li, 2020</p>
<p>Multi-head or single-head? an empirical comparison for transformer training. L Liu, J Liu, J Han, arXiv:2106.096502021arXiv preprint</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized BERT pretraining approach. 2019arXiv preprint</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. B Min, H Ross, E Sulem, A P B Veyseh, T H Nguyen, O Sainz, E Agirre, I Heintz, D Roth, ACM Computing Surveys. 5622023</p>
<p>. J R Munkres, Topology, 2000Prentice-Hall</p>
<p>. OpenAI. Introducing ChatGPT. 2022</p>
<p>2303.0877OpenAI. GPT-4 technical report. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019</p>
<p>PanGu-Σ: Towards trillion parameter language model with sparse heterogeneous computing. X Ren, P Zhou, X Meng, X Huang, Y Wang, W Wang, P Li, X Zhang, A Podolskiy, G Arshinov, arXiv:2303.108452023arXiv preprint</p>
<p>Functional analysis. W Rudin, 1991McgGraw-Hill</p>
<p>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. V Sanh, L Debut, J Chaumond, T Wolf, arXiv:1910.011082019arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, Toolformer, arXiv:2302.04761Language models can teach themselves to use tools. 2023arXiv preprint</p>
<p>R Sennrich, B Haddow, A Birch, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>Energy and policy considerations for deep learning in NLP. E Strubell, A Ganesh, A Mccallum, arXiv:1906.022432019arXiv preprint</p>
<p>I Tenney, D Das, E Pavlick, arXiv:1905.05950BERT rediscovers the classical NLP pipeline. 2019arXiv preprint</p>
<p>R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Is deep learning a useful tool for the pure mathematician?. G Williamson, arXiv:2304.126022023arXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2020</p>
<p>Learning to prove theorems via interacting with proof assistants. K Yang, J Deng, International Conference on Machine Learning. PMLR2019</p>
<p>K Yang, A M Swope, A Gu, R Chalamala, P Song, S Yu, S Godil, R Prenger, A Anandkumar, arXiv:2306.15626LeanDojo: Theorem proving with retrieval-augmented language models. 2023arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8643 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8643</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8643</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273186052</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.04223v1.pdf" target="_blank">Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning</a></p>
                <p><strong>Paper Abstract:</strong> While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8643.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8643.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llamole</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llamole: Multimodal Large Language Model for Molecular Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal autoregressive system that integrates a base LLM with pre-trained graph modules (Graph DiT and GNN) and a graph encoder to interleave text, molecular-graph generation, and reaction prediction; uses LLM-derived heuristics plus A* search for retrosynthetic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llamole (system integrating base LLM + Graph DiT + GNN + GIN graph encoder + A* with LLM heuristics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multimodal autoregressive LLM (transformer) + graph diffusion model + graph neural network (GIN) + symbolic search (A*)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies by base LLM (studied with 7B and 8B bases such as Llama-3.1-8B, Mistral-7B, Qwen2-7B; overall system includes pre-trained Graph DiT ~574M, GNN predictor ~632M, graph encoder ~43M)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Multimodal instruction dataset MolQA (graph-text QA pairs derived from USPTO reactions, Enamine purchasable set, property annotations) for SFT; MolPair (graph-text and reaction-text pairs ~600K) to pre-train graph modules; LoRA used for parameter-efficient fine-tuning of base LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery (small molecules) and materials (polymers / gas-separation membranes); general inverse molecular design with synthesizability (retrosynthesis) constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Interleaved generation: base LLM analyzes instruction and emits trigger tokens that activate Graph DiT for conditional graph (molecule) generation or GNN for reaction-template prediction; graph encoder embeds generated molecules back into LLM; A* search over AND-OR retrosynthesis tree with LLM-computed heuristic scores guides multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Designed molecules are produced by Graph DiT conditioned on textual and property constraints from MolQA/MolPair; novelty assessed implicitly via similarity and oracle property evaluation — Llamole produced many valid novel candidate molecules (paper reports >10K retrosynthesis success cases and multi-step solutions) but explicit percent-not-in-training-set is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generates molecules conditioned on multi-property constraints (8 benchmark properties including HIV, BACE, BBBP and five material properties) and structural descriptions; evaluates synthesizability using retrosynthetic planning against Enamine purchasable set and uses oracle property predictors to measure property compliance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Chemical validity, fingerprint similarity (Morgan), text metrics (BLEU-4, ROUGE-L), property deviation (MAE for continuous properties, Balanced Accuracy for categorical drug properties), retrosynthesis success rate (target -> purchasable building blocks), synthesis-route length and time.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Llamole substantially outperformed 14 baseline LLMs and GraphGA across 12 controllability & planning metrics: improved chemical validity by ~14.6%, structure controllability by ~4.4%, text generation by ~11.9–15.9%, and property controllability by 32–80%; retrosynthesis success increased from ~5% (best baseline LLM) to ~35% for drug tasks and to 17.9% for polymers; many successful cases required multi-step (40.48% two-or-more-step) planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against text-only LLMs (ICL and SFT) and GraphGA: Llamole outperforms text-only LLMs (which struggle with graph-formatting, planning, hallucination) and surpasses or matches GraphGA on many controllability metrics; GraphGA remains a strong domain-specific baseline but Llamole provides superior integrated text+graph+planning performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires pre-trained graph modules and curated multimodal datasets (MolQA, MolPair); A* planning is sensitive to heuristics and time limits (30s/300 iterations default) — longer runtime improves success at cost of latency; Graph DiT may not perfectly follow fine-grained textual instructions (subtle structural mismatches reported); occasional fallback to LLM-generated molecules if Graph DiT fails; overall chemical designs remain conceptual until wet-lab validation; scalability and dependence on quality of template database and property oracles are additional constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8643.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8643.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph DiT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete graph diffusion model implemented with Transformer layers and adaptive layer normalization (AdaLN) that performs multi-conditional molecular graph generation conditioned on categorical/continuous properties and text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inverse molecular design with multi-conditional diffusion guidance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph DiT (Graph Diffusion Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Discrete diffusion model over graphs implemented with Transformer (diffusion transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>approximately 574 million parameters (depth 28, hidden 1024, 16 heads, MLP 4096)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on ~600K graph-text pairwise data from MolPair (small molecules and polymers) with multiple property annotations (including the eight benchmark properties); SciBERT used to embed text conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Inverse molecular design for drug-like small molecules and polymer monomers under multi-property constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Predictor-free discrete denoising diffusion guided by summed condition embeddings (categorical/continuous/property/text); used as the molecule generator within Llamole activated by <design> trigger tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates molecules conditioned on properties/text; novelty measured implicitly by similarity metrics in evaluation (Morgan fingerprint similarity); explicit out-of-training-set percentages not reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Handles multi-property conditioning including drug and material utilities; text embeddings from SciBERT are integrated enabling text-driven structural guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Chemical validity, structure similarity to reference (Morgan fingerprints), property deviation (MAE/BA), and synthesis success when combined with retrosynthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Graph DiT, when integrated with LLM cues, enabled significantly better controllable generation than text-only LLMs; ablation shows text conditions strongly improve Graph DiT outputs and Llamole leverages Graph DiT’s conditional generation to meet multi-conditional requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms sequence-based LLM generation (SMILES) for structure/property controllability; compared against GraphGA and other graph methods, Graph DiT is a competitive generative component within Llamole.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>May not perfectly adhere to very subtle textual instructions (e.g., exact placement of functional groups) and relies on the quality/coverage of property labels and text embeddings; pretraining requires large curated graph-text datasets and compute (one week on A100 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8643.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8643.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN predictor (reaction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN-based one-step reaction template predictor (GIN architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Graph Isomorphism Network (GIN)-based classifier that predicts reaction templates (labels) from product molecular graphs and text conditions to enable one-step retrosynthetic decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GNN reaction predictor (GIN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph Neural Network (GIN) multi-class classifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>pre-trained model described with ~632 million parameters (three-layer GIN hidden size 512 reported for template predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on reaction-text pairs from USPTO and MolPair; templates extracted (~300K templates across USPTO), ~1.6M training examples for template prediction (MolPair reaction-text pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Retrosynthetic planning: one-step reverse reaction prediction to derive reactants from products.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Given a product graph and text condition embedding (SciBERT), predicts a reaction template label r; predicted templates are then instantiated to derive reactants and expand A* search tree.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Does not itself generate novel templates beyond USPTO-derived space; enables generation of reactant sets by applying predicted templates — novelty thus limited by template coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioned on textual context to prefer templates aligned with textual synthesis descriptions; used to reduce hallucination in textual reaction generation by producing concrete template-based reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Negative log-likelihood loss during training; downstream evaluation via retrosynthesis success rate (product -> purchasable building blocks) and correctness/format of reaction generation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using the GNN predictor within Llamole avoids many hallucination and formatting errors seen in text-only LLMs (ICL: 68.4% invalid reaction-format rate; SFT: 57.6%); GNN outputs allow direct template application yielding higher success and more reliable reaction nodes for A* planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>More reliable than directly prompting LLMs to generate reaction text/templates (which often hallucinate or produce incorrect templates); performance depends on template coverage extracted from USPTO.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Template-space is finite and limited to extracted USPTO templates; coverage gaps may prevent synthesis of some novel products; quality depends on text-condition embedding and template extraction correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8643.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8643.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph encoder (GIN CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph encoder: GIN-based graph-text contrastive encoder (CLIP-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GIN graph encoder trained with a CLIP-like contrastive loss to project molecular graphs into the same embedding space as text, used to feed molecule embeddings into the LLM decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GIN-based graph encoder (CLIP loss)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph neural network (GIN) trained with contrastive (CLIP) objective</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>approximately 43 million parameters (five-layer GIN, hidden size 768 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on ~600K graph-text pairs (MolPair) for contrastive alignment with SciBERT text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular understanding within multimodal LLM; enables integration of graph tokens into LLM token stream for interleaved generation and context.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encodes generated/previous molecules into vectors that the LLM can consume (used at <molecule> token positions), enabling the LLM to condition subsequent text/reaction-generation on explicit molecular graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not a generator; enables better multimodal grounding and consistency between textual descriptions and graph structures.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Improves LLM’s molecular understanding and supports property-conditioned generation and retrosynthetic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Contrastive alignment losses during pretraining; downstream impact measured via improved text coherence, molecule-guided reactions, and higher retrosynthesis success in Llamole.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Graph encoder provides robust molecule embeddings that let the LLM reason about previously generated molecules during planning; contributes to higher planning success and lower hallucination compared to text-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Offers better graph-text alignment than using only SMILES text tokens; enables replacing SMILES segments with structured embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Pretraining requires large graph-text corpora and careful contrastive sampling; frozen during Llamole SFT (with a tunable linear layer), so adaptation capacity is limited unless retrained.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8643.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8643.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline LLMs (ICL/SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline large language models used with In-Context Learning (ICL) and Supervised Fine-Tuning (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of off-the-shelf LLMs (Llama family, Mistral, Qwen, Granite, Flan-T5) evaluated in ICL and LoRA-based SFT modes to generate text and SMILES/SMILES-like molecular outputs for design and retrosynthesis tasks; generally struggle on graph-structured chemical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Examples: Llama-2 (7B, 13B, 70B), Llama-3 (8B, 70B, 3.1-8B), Mistral-7B, Qwen2-7B, Granite-13B, Flan-T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLMs (autoregressive and instruction-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>range reported in paper: 7B to 70B parameters (specific models enumerated above)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on general web/scientific corpora (not chemistry-specialized); fine-tuned or prompted on MolQA exemplars and/or SFT conversion where molecules converted into SMILES strings for LoRA fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Attempted applications: text-based molecular design, property-conditioned SMILES generation, and reaction-text generation for retrosynthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Two modes: (1) In-Context Learning (ICL) using demonstrations in prompts to elicit SMILES and synthesis text; (2) supervised fine-tuning (SFT) via LoRA with text-only graph converted to SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Often generate SMILES-like outputs; novelty not systematically quantified — however many outputs are invalid or not synthesizable; GraphGA and Llamole outperform them on controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Attempt to meet property/structure constraints via prompt or SFT, but struggle to reliably satisfy multi-conditional constraints and synthesizability requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as Llamole's: chemical validity, fingerprint similarity, BLEU/ROUGE for reaction texts, property deviation (MAE/BA), retrosynthesis success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Text-only LLMs with ICL or SFT often underperform: best ICL or SFT models still inferior to Llamole on multi-conditional controllability; ICL frequently produces invalid reaction formats (68.4% invalid format), SFT reduces but does not eliminate errors; retrosynthesis success for best LLMs was ~5% (material and drug tasks) compared to Llamole's ~35%. Larger model size alone weakly correlated with improved chemical design (Pearson ~0.366).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Often outperformed by GraphGA (a graph-based optimization baseline) and substantially outperformed by Llamole which leverages explicit graph modules and planning; SFT can improve property control but sometimes harms planning (example: Llama-3-8B SFT reduced retrosynthesis success from 5.5% to <1%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Poor handling of graph-structured molecular data when used as sequential SMILES text; formatting and instruction-adherence failures for reaction generation; hallucination of reaction templates and incorrect retrosynthetic steps; limited planning capability compared to search-guided systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8643.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8643.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGA: Graph-based genetic algorithm for molecular optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based optimization approach used as a strong domain-specific baseline for controllable molecular design and property optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphGA (graph genetic algorithm / graph-based optimization method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph-based evolutionary optimization / search</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not applicable (algorithmic method rather than parameterized model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Domain molecule libraries and property oracles used for optimization; not a pretrained neural model in this paper's comparison context.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Inverse molecular design and property optimization for drug and materials tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Graph-level optimization (evolutionary search) that manipulates molecular graphs directly to optimize properties and synthesizability heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Produces novel graphs via graph genetic operations; served as a competitive baseline — in many cases GraphGA outperformed plain LLMs without graph modules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed specifically for molecular objectives and constrained optimization; does not produce textual retrosynthesis outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same downstream metrics: validity, fingerprint similarity, property deviations, and other design metrics used in MolQA benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GraphGA is a strong baseline that challenges many LLMs in generating molecules meeting multi-conditional targets; Llamole surpasses GraphGA on averaged controllability metrics in the paper but GraphGA remains competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Stronger than text-only LLMs in controllable graph generation; Llamole's multimodal design yields better average performance across text+graph+planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Typically does not output retrosynthetic textual descriptions; bottom-up action-space approaches may limit flexibility versus top-down template-driven retrosynthesis; may require domain-specific heuristics and tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inverse molecular design with multi-conditional diffusion guidance <em>(Rating: 2)</em></li>
                <li>Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design <em>(Rating: 2)</em></li>
                <li>Retro*: learning retrosynthetic planning with neural guided a* search <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
                <li>Score-based generative modeling of graphs via the system of stochastic differential equations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8643",
    "paper_id": "paper-273186052",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Llamole",
            "name_full": "Llamole: Multimodal Large Language Model for Molecular Discovery",
            "brief_description": "A multimodal autoregressive system that integrates a base LLM with pre-trained graph modules (Graph DiT and GNN) and a graph encoder to interleave text, molecular-graph generation, and reaction prediction; uses LLM-derived heuristics plus A* search for retrosynthetic planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llamole (system integrating base LLM + Graph DiT + GNN + GIN graph encoder + A* with LLM heuristics)",
            "model_type": "Multimodal autoregressive LLM (transformer) + graph diffusion model + graph neural network (GIN) + symbolic search (A*)",
            "model_size": "varies by base LLM (studied with 7B and 8B bases such as Llama-3.1-8B, Mistral-7B, Qwen2-7B; overall system includes pre-trained Graph DiT ~574M, GNN predictor ~632M, graph encoder ~43M)",
            "training_data": "Multimodal instruction dataset MolQA (graph-text QA pairs derived from USPTO reactions, Enamine purchasable set, property annotations) for SFT; MolPair (graph-text and reaction-text pairs ~600K) to pre-train graph modules; LoRA used for parameter-efficient fine-tuning of base LLM.",
            "application_domain": "Drug discovery (small molecules) and materials (polymers / gas-separation membranes); general inverse molecular design with synthesizability (retrosynthesis) constraints.",
            "generation_method": "Interleaved generation: base LLM analyzes instruction and emits trigger tokens that activate Graph DiT for conditional graph (molecule) generation or GNN for reaction-template prediction; graph encoder embeds generated molecules back into LLM; A* search over AND-OR retrosynthesis tree with LLM-computed heuristic scores guides multi-step planning.",
            "novelty_of_chemicals": "Designed molecules are produced by Graph DiT conditioned on textual and property constraints from MolQA/MolPair; novelty assessed implicitly via similarity and oracle property evaluation — Llamole produced many valid novel candidate molecules (paper reports &gt;10K retrosynthesis success cases and multi-step solutions) but explicit percent-not-in-training-set is not reported.",
            "application_specificity": "Generates molecules conditioned on multi-property constraints (8 benchmark properties including HIV, BACE, BBBP and five material properties) and structural descriptions; evaluates synthesizability using retrosynthetic planning against Enamine purchasable set and uses oracle property predictors to measure property compliance.",
            "evaluation_metrics": "Chemical validity, fingerprint similarity (Morgan), text metrics (BLEU-4, ROUGE-L), property deviation (MAE for continuous properties, Balanced Accuracy for categorical drug properties), retrosynthesis success rate (target -&gt; purchasable building blocks), synthesis-route length and time.",
            "results_summary": "Llamole substantially outperformed 14 baseline LLMs and GraphGA across 12 controllability & planning metrics: improved chemical validity by ~14.6%, structure controllability by ~4.4%, text generation by ~11.9–15.9%, and property controllability by 32–80%; retrosynthesis success increased from ~5% (best baseline LLM) to ~35% for drug tasks and to 17.9% for polymers; many successful cases required multi-step (40.48% two-or-more-step) planning.",
            "comparison_to_other_methods": "Compared against text-only LLMs (ICL and SFT) and GraphGA: Llamole outperforms text-only LLMs (which struggle with graph-formatting, planning, hallucination) and surpasses or matches GraphGA on many controllability metrics; GraphGA remains a strong domain-specific baseline but Llamole provides superior integrated text+graph+planning performance in experiments.",
            "limitations_and_challenges": "Requires pre-trained graph modules and curated multimodal datasets (MolQA, MolPair); A* planning is sensitive to heuristics and time limits (30s/300 iterations default) — longer runtime improves success at cost of latency; Graph DiT may not perfectly follow fine-grained textual instructions (subtle structural mismatches reported); occasional fallback to LLM-generated molecules if Graph DiT fails; overall chemical designs remain conceptual until wet-lab validation; scalability and dependence on quality of template database and property oracles are additional constraints.",
            "uuid": "e8643.0",
            "source_info": {
                "paper_title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Graph DiT",
            "name_full": "Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation",
            "brief_description": "A discrete graph diffusion model implemented with Transformer layers and adaptive layer normalization (AdaLN) that performs multi-conditional molecular graph generation conditioned on categorical/continuous properties and text embeddings.",
            "citation_title": "Inverse molecular design with multi-conditional diffusion guidance",
            "mention_or_use": "use",
            "model_name": "Graph DiT (Graph Diffusion Transformer)",
            "model_type": "Discrete diffusion model over graphs implemented with Transformer (diffusion transformer)",
            "model_size": "approximately 574 million parameters (depth 28, hidden 1024, 16 heads, MLP 4096)",
            "training_data": "Pre-trained on ~600K graph-text pairwise data from MolPair (small molecules and polymers) with multiple property annotations (including the eight benchmark properties); SciBERT used to embed text conditions.",
            "application_domain": "Inverse molecular design for drug-like small molecules and polymer monomers under multi-property constraints.",
            "generation_method": "Predictor-free discrete denoising diffusion guided by summed condition embeddings (categorical/continuous/property/text); used as the molecule generator within Llamole activated by &lt;design&gt; trigger tokens.",
            "novelty_of_chemicals": "Generates molecules conditioned on properties/text; novelty measured implicitly by similarity metrics in evaluation (Morgan fingerprint similarity); explicit out-of-training-set percentages not reported in paper.",
            "application_specificity": "Handles multi-property conditioning including drug and material utilities; text embeddings from SciBERT are integrated enabling text-driven structural guidance.",
            "evaluation_metrics": "Chemical validity, structure similarity to reference (Morgan fingerprints), property deviation (MAE/BA), and synthesis success when combined with retrosynthesis planning.",
            "results_summary": "Graph DiT, when integrated with LLM cues, enabled significantly better controllable generation than text-only LLMs; ablation shows text conditions strongly improve Graph DiT outputs and Llamole leverages Graph DiT’s conditional generation to meet multi-conditional requirements.",
            "comparison_to_other_methods": "Outperforms sequence-based LLM generation (SMILES) for structure/property controllability; compared against GraphGA and other graph methods, Graph DiT is a competitive generative component within Llamole.",
            "limitations_and_challenges": "May not perfectly adhere to very subtle textual instructions (e.g., exact placement of functional groups) and relies on the quality/coverage of property labels and text embeddings; pretraining requires large curated graph-text datasets and compute (one week on A100 reported).",
            "uuid": "e8643.1",
            "source_info": {
                "paper_title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GNN predictor (reaction)",
            "name_full": "GNN-based one-step reaction template predictor (GIN architecture)",
            "brief_description": "A Graph Isomorphism Network (GIN)-based classifier that predicts reaction templates (labels) from product molecular graphs and text conditions to enable one-step retrosynthetic decomposition.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GNN reaction predictor (GIN)",
            "model_type": "Graph Neural Network (GIN) multi-class classifier",
            "model_size": "pre-trained model described with ~632 million parameters (three-layer GIN hidden size 512 reported for template predictor)",
            "training_data": "Trained on reaction-text pairs from USPTO and MolPair; templates extracted (~300K templates across USPTO), ~1.6M training examples for template prediction (MolPair reaction-text pairs).",
            "application_domain": "Retrosynthetic planning: one-step reverse reaction prediction to derive reactants from products.",
            "generation_method": "Given a product graph and text condition embedding (SciBERT), predicts a reaction template label r; predicted templates are then instantiated to derive reactants and expand A* search tree.",
            "novelty_of_chemicals": "Does not itself generate novel templates beyond USPTO-derived space; enables generation of reactant sets by applying predicted templates — novelty thus limited by template coverage.",
            "application_specificity": "Conditioned on textual context to prefer templates aligned with textual synthesis descriptions; used to reduce hallucination in textual reaction generation by producing concrete template-based reactions.",
            "evaluation_metrics": "Negative log-likelihood loss during training; downstream evaluation via retrosynthesis success rate (product -&gt; purchasable building blocks) and correctness/format of reaction generation.",
            "results_summary": "Using the GNN predictor within Llamole avoids many hallucination and formatting errors seen in text-only LLMs (ICL: 68.4% invalid reaction-format rate; SFT: 57.6%); GNN outputs allow direct template application yielding higher success and more reliable reaction nodes for A* planning.",
            "comparison_to_other_methods": "More reliable than directly prompting LLMs to generate reaction text/templates (which often hallucinate or produce incorrect templates); performance depends on template coverage extracted from USPTO.",
            "limitations_and_challenges": "Template-space is finite and limited to extracted USPTO templates; coverage gaps may prevent synthesis of some novel products; quality depends on text-condition embedding and template extraction correctness.",
            "uuid": "e8643.2",
            "source_info": {
                "paper_title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Graph encoder (GIN CLIP)",
            "name_full": "Graph encoder: GIN-based graph-text contrastive encoder (CLIP-style)",
            "brief_description": "A GIN graph encoder trained with a CLIP-like contrastive loss to project molecular graphs into the same embedding space as text, used to feed molecule embeddings into the LLM decoder.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GIN-based graph encoder (CLIP loss)",
            "model_type": "Graph neural network (GIN) trained with contrastive (CLIP) objective",
            "model_size": "approximately 43 million parameters (five-layer GIN, hidden size 768 reported)",
            "training_data": "Pre-trained on ~600K graph-text pairs (MolPair) for contrastive alignment with SciBERT text embeddings.",
            "application_domain": "Molecular understanding within multimodal LLM; enables integration of graph tokens into LLM token stream for interleaved generation and context.",
            "generation_method": "Encodes generated/previous molecules into vectors that the LLM can consume (used at &lt;molecule&gt; token positions), enabling the LLM to condition subsequent text/reaction-generation on explicit molecular graph embeddings.",
            "novelty_of_chemicals": "Not a generator; enables better multimodal grounding and consistency between textual descriptions and graph structures.",
            "application_specificity": "Improves LLM’s molecular understanding and supports property-conditioned generation and retrosynthetic planning.",
            "evaluation_metrics": "Contrastive alignment losses during pretraining; downstream impact measured via improved text coherence, molecule-guided reactions, and higher retrosynthesis success in Llamole.",
            "results_summary": "Graph encoder provides robust molecule embeddings that let the LLM reason about previously generated molecules during planning; contributes to higher planning success and lower hallucination compared to text-only approaches.",
            "comparison_to_other_methods": "Offers better graph-text alignment than using only SMILES text tokens; enables replacing SMILES segments with structured embeddings.",
            "limitations_and_challenges": "Pretraining requires large graph-text corpora and careful contrastive sampling; frozen during Llamole SFT (with a tunable linear layer), so adaptation capacity is limited unless retrained.",
            "uuid": "e8643.3",
            "source_info": {
                "paper_title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Baseline LLMs (ICL/SFT)",
            "name_full": "Baseline large language models used with In-Context Learning (ICL) and Supervised Fine-Tuning (SFT)",
            "brief_description": "A collection of off-the-shelf LLMs (Llama family, Mistral, Qwen, Granite, Flan-T5) evaluated in ICL and LoRA-based SFT modes to generate text and SMILES/SMILES-like molecular outputs for design and retrosynthesis tasks; generally struggle on graph-structured chemical tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Examples: Llama-2 (7B, 13B, 70B), Llama-3 (8B, 70B, 3.1-8B), Mistral-7B, Qwen2-7B, Granite-13B, Flan-T5-XXL",
            "model_type": "Transformer-based LLMs (autoregressive and instruction-tuned variants)",
            "model_size": "range reported in paper: 7B to 70B parameters (specific models enumerated above)",
            "training_data": "Pretrained on general web/scientific corpora (not chemistry-specialized); fine-tuned or prompted on MolQA exemplars and/or SFT conversion where molecules converted into SMILES strings for LoRA fine-tuning.",
            "application_domain": "Attempted applications: text-based molecular design, property-conditioned SMILES generation, and reaction-text generation for retrosynthesis.",
            "generation_method": "Two modes: (1) In-Context Learning (ICL) using demonstrations in prompts to elicit SMILES and synthesis text; (2) supervised fine-tuning (SFT) via LoRA with text-only graph converted to SMILES strings.",
            "novelty_of_chemicals": "Often generate SMILES-like outputs; novelty not systematically quantified — however many outputs are invalid or not synthesizable; GraphGA and Llamole outperform them on controllability.",
            "application_specificity": "Attempt to meet property/structure constraints via prompt or SFT, but struggle to reliably satisfy multi-conditional constraints and synthesizability requirements.",
            "evaluation_metrics": "Same as Llamole's: chemical validity, fingerprint similarity, BLEU/ROUGE for reaction texts, property deviation (MAE/BA), retrosynthesis success rate.",
            "results_summary": "Text-only LLMs with ICL or SFT often underperform: best ICL or SFT models still inferior to Llamole on multi-conditional controllability; ICL frequently produces invalid reaction formats (68.4% invalid format), SFT reduces but does not eliminate errors; retrosynthesis success for best LLMs was ~5% (material and drug tasks) compared to Llamole's ~35%. Larger model size alone weakly correlated with improved chemical design (Pearson ~0.366).",
            "comparison_to_other_methods": "Often outperformed by GraphGA (a graph-based optimization baseline) and substantially outperformed by Llamole which leverages explicit graph modules and planning; SFT can improve property control but sometimes harms planning (example: Llama-3-8B SFT reduced retrosynthesis success from 5.5% to &lt;1%).",
            "limitations_and_challenges": "Poor handling of graph-structured molecular data when used as sequential SMILES text; formatting and instruction-adherence failures for reaction generation; hallucination of reaction templates and incorrect retrosynthetic steps; limited planning capability compared to search-guided systems.",
            "uuid": "e8643.4",
            "source_info": {
                "paper_title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GraphGA",
            "name_full": "GraphGA: Graph-based genetic algorithm for molecular optimization",
            "brief_description": "A graph-based optimization approach used as a strong domain-specific baseline for controllable molecular design and property optimization.",
            "citation_title": "Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design",
            "mention_or_use": "use",
            "model_name": "GraphGA (graph genetic algorithm / graph-based optimization method)",
            "model_type": "Graph-based evolutionary optimization / search",
            "model_size": "not applicable (algorithmic method rather than parameterized model)",
            "training_data": "Domain molecule libraries and property oracles used for optimization; not a pretrained neural model in this paper's comparison context.",
            "application_domain": "Inverse molecular design and property optimization for drug and materials tasks.",
            "generation_method": "Graph-level optimization (evolutionary search) that manipulates molecular graphs directly to optimize properties and synthesizability heuristics.",
            "novelty_of_chemicals": "Produces novel graphs via graph genetic operations; served as a competitive baseline — in many cases GraphGA outperformed plain LLMs without graph modules.",
            "application_specificity": "Designed specifically for molecular objectives and constrained optimization; does not produce textual retrosynthesis outputs.",
            "evaluation_metrics": "Same downstream metrics: validity, fingerprint similarity, property deviations, and other design metrics used in MolQA benchmark.",
            "results_summary": "GraphGA is a strong baseline that challenges many LLMs in generating molecules meeting multi-conditional targets; Llamole surpasses GraphGA on averaged controllability metrics in the paper but GraphGA remains competitive.",
            "comparison_to_other_methods": "Stronger than text-only LLMs in controllable graph generation; Llamole's multimodal design yields better average performance across text+graph+planning tasks.",
            "limitations_and_challenges": "Typically does not output retrosynthetic textual descriptions; bottom-up action-space approaches may limit flexibility versus top-down template-driven retrosynthesis; may require domain-specific heuristics and tuning.",
            "uuid": "e8643.5",
            "source_info": {
                "paper_title": "Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inverse molecular design with multi-conditional diffusion guidance",
            "rating": 2,
            "sanitized_title": "inverse_molecular_design_with_multiconditional_diffusion_guidance"
        },
        {
            "paper_title": "Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design",
            "rating": 2,
            "sanitized_title": "amortized_tree_generation_for_bottomup_synthesis_planning_and_synthesizable_molecular_design"
        },
        {
            "paper_title": "Retro*: learning retrosynthetic planning with neural guided a* search",
            "rating": 2,
            "sanitized_title": "retro_learning_retrosynthetic_planning_with_neural_guided_a_search"
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        },
        {
            "paper_title": "Score-based generative modeling of graphs via the system of stochastic differential equations",
            "rating": 1,
            "sanitized_title": "scorebased_generative_modeling_of_graphs_via_the_system_of_stochastic_differential_equations"
        }
    ],
    "cost": 0.017792999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning</p>
<p>Gang Liu 
University of Notre Dame</p>
<p>Michael Sun 
MIT CSAIL</p>
<p>Wojciech Matusik wojciech@csail.mit.edu 
MIT CSAIL</p>
<p>Meng Jiang mjiang2@nd.edu 
University of Notre Dame</p>
<p>Jie Chen chenjie@us.ibm.com 
MIT-IBM Watson AI Lab</p>
<p>MIT-IBM Watson AI Lab
IBM Research</p>
<p>Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning
749BE36BBCBB415ADBB9A8C3BBCFC8A2
While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design.This difficulty stems from the need for coherent autoregressive generation across texts and graphs.To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning.Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules.Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning.We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning.Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.</p>
<p>Large language models (LLMs) are transforming machine learning across many domains [Achiam et al., 2023].Their success in natural language processing has extended to areas handling not only text but also image and speech data [Dong et al., 2023, Wu et al., 2024], thanks to the development of multimodal capabilities.Recently, the potential of LLMs for molecular discovery has been actively explored [Jablonka et al., 2023].However, LLMs struggle in the chemical domain, exhibiting poor generation quality and planning capability [Guo et al., 2023].This is due to the unique, graph structures of molecular data, which are challenging for LLMs that typically handle sequential texts.</p>
<p>Inverse molecular design requires LLMs to be controllable for generating molecular structures that meet multi-property and synthesizability requirements [Chen et al., 2020, Gao et al., 2021].These requirements can be detailed as questions for LLM input, as shown in Figure 2. Answering these questions demands a comprehensive understanding of molecular structures and their relationship to properties.However, sequence-based LLMs struggle with this because they are pre-trained or fine-tuned solely on arXiv:2410.04223v1[cs.LG] 5 Oct 2024 "Can you design a molecule that inhibits both HIV and Beta-Secretase 1, with a molecular weight around 284.33, and 2 rings including 1 aromatic and 1 aliphatic ring, and outline its synthesis pathway?" text representations of molecules, e.g., SMILES [Weininger, 1988].To illustrate this, we investigate 14 LLMs for molecular generation in Figure 1 across 10K drug and material questions: ten using in-context learning (ICL) and four with supervised fine-tuning (SFT).LLMs generate molecular structures based on the questions, and their properties are obtained through oracles to assess the differences between requirements and generated outputs.Details of the experimental set-ups and results can be found in Section 5.In summary, even the best LLMs perform worse than GraphGA [Gao et al., 2022], a simple yet effective graph-based method, in designing molecules with satisfactory properties.</p>
<p>As illustrated in Figure 2, practical answers for molecular design are more complex than what can be achieved by using graph methods or LLMs alone.The generation begins with a paragraph describing the intended molecule for multi-conditional generation, followed by retrosynthetic planning, detailing each synthesis step-one reaction per paragraph-in reverse order, from the target molecule to purchasable reactants.Thus, multimodal LLMs (MLLMs) are essential, with LLMs handling text generation and graph models managing molecular design.</p>
<p>In this work, we propose the multimodal Large language model for molecular discovery (Llamole).As shown in Figure 2 (c), the model seamlessly integrates LLMs and graph models within a multimodal autoregressive framework, enabling the interleaved generation of text, molecules, and reactions.It predicts the next token across both word and chemical spaces, framed as multi-class prediction tasks for word vocabulary, atom/bond types, and reaction templates.For retrosynthetic planning, Llamole integrates A* search to efficiently identify synthesis pathways for the designed molecule.</p>
<p>To implement Llamole, we augment a base LLM with two pre-trained graph modules: the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecule generation [Liu et al., 2024c] and a GNN for reaction template prediction.The base LLM controls the generation flow using a trigger-query-prediction approach with two sets of trigger tokens for the Graph DiT and GNN, respectively.Upon predicting a trigger token, one or a few query tokens summarize the prior text as vectors, activating the corresponding graph modules and generating molecules or predicting reaction templates.Afterward, the base LLM can resume text generation, aided by a graph encoder that encodes the previously generated molecule.In retrosynthetic planning, the LLM computes heuristics to efficiently assist the A* search in navigating the vast reaction space for multi-step generation.</p>
<p>Our work has several highlights.First, Llamole is the first MLLM for molecular design, capable of interleaved generation of text and graphs.Second, we curated a dataset along with fine-tuning instructions to benchmark complex yet realistic molecular design outcomes, including human conversation.Third, we present compelling experimental results that demonstrate the competitiveness of Llamole against 14 LLMs and GraphGA, as shown in Figure 1.With details in Tables 1 and 2, Llamole improves LLM performance by up to 80.9% across 12 metrics for controllable molecular generation and increases the success rate for retrosynthetic planning from 5.5% to 35%.</p>
<p>Preliminaries</p>
<p>Autoregressive Language Modeling</p>
<p>Given a sequence of word tokens W = {w 1 , w 2 , . . ., w L } of length L from the vocabulary W, LLMs parameterized by θ 1 decompose the joint distribution as p θ1 (W ) = L i=1 p θ1 (w i |W &lt;i ), where W &lt;i represents the tokens preceding the i-th position.These models are optimized by minimizing the negative log-likelihood between their predictions and the empirical data distribution, resulting in:
L LM = i − log p θ1 (w i |W &lt;i ).
(1)</p>
<p>Molecular Design with Graph Diffusion Models</p>
<p>Molecular graphs can be modeled through diffusion in discrete spaces [Austin et al., 2021, Vignac et al., 2022, Liu et al., 2024c].Given a one-hot encoded data point x ∈ R F with F categories (e.g., a node or an edge), discrete models perform diffusion using a transition matrix Q, where
[Q t ] ij = q(x t j | x t−1 i ) for i, j ∈ [1, F ].
The forward diffusion with Q is: q(x t | x t−1 ) = Cat(x t ; p = x t−1 Q t ), where Cat(x; p) denotes the categorical distribution over x with probabilities given by p. Starting from the original data point x = x 0 , we have q(x t | x 0 ) = Cat x t ; p = x 0 Qt , where Qt = i≤t Q i .The forward diffusion gradually corrupts data points.When the total timestep T is large enough, q(x T ) converges to a stationary distribution.The reverse process samples from q(x T ) and gradually removes noise.The posterior distribution q(x t−1 | x t ) is calculated as q(x t−1 |x t , x 0 ) ∝ x t (Q t ) ⊤ ⊙ x 0 Qt−1 .Using a denoising model parameterized by θ 2 , this posterior can be approximated by p θ2 (x t−1 |x t , x 0 ).For inverse molecular design with multi-property constraints, the denoising model can be optimized by minimizing the negative log-likelihood for x 0 :
L DM = E q(x 0 ) E q(x t |x 0 ) − log p θ2 x 0 | c 1 , c 2 , . . . , c M , c text , x t ,(2)
where M molecular properties are denoted by {c i } M i=1 , and the text embedding is c text .These conditions can be handled by Graph DiT [Liu et al., 2024c] without introducing additional predictors for guidance [Ho and Salimans, 2022].</p>
<p>One-Step Reaction Prediction with Graph Neural Networks</p>
<p>Retrosynthesis needs to predict the reverse of a synthetic reaction, which decomposes chemical products into reactants.A GNN parameterized by θ 3 takes the product G product to predict the label r ∈ R in the reaction space R.This label is interpreted as the template and determines the reactants.</p>
<p>With the text condition c text , we minimize the negative log-likelihood of the label distribution q(r):
L predictor = E q(r) [− log p θ3 (r | c text , G product )] .
(3)</p>
<p>Retrosynthetic Planning with A* Search</p>
<p>Given molecules from the structure space G, a subset G avail represents available molecular structures that can be purchased as building blocks for synthesis.For any target G target , one-step prediction of the reversed reaction may not yield reactants within G avail .Thus, retrosynthesis typically requires multi-step planning to find pathways from building blocks to the target in reverse order.The search space of chemical reactions can be navigated using A* on an AND-OR tree T , with G target as the root.</p>
<p>Reaction nodes follow an "AND" relation, requiring all child reactants, while molecule nodes follow an "OR" relation, meaning the product can be synthesized by any child reaction [Chen et al., 2020].</p>
<p>Selection: We select nodes from the frontier F(T ) containing unexplored molecule nodes to expand the tree.Given an oracle cost function J(•), the next node is selected as G next = arg min G∈F (T ) J(G) to minimize the cost.A well-designed J(•) improves search efficiency and aids in global optimality.</p>
<p>Details of Active Modules</p>
<p>"<s>To satisfy…It is <design><query>" "To synthesize   …<retro><query>" "To satisfy…It is <design>"
(a) (a) (a) (a) (c) "To" (b) (d)
"synthesize …<retro>" Vector Prob.</p>
<p>Llamole: Multimodal Large Language Model for Molecular Discovery  Update and Cost: After expanding G next , the tree becomes T ′ .We update the nodes in T ′ for the next iteration.A* selects the path that minimizes J(•) = J current (•) + J heuristic (•), which includes the cost from the start to the current node J current (•) and a heuristic estimate of the cost to the goal J heuristic (•).
𝑮 𝟏 𝑅 " 𝑅 # 𝐺 # 𝐺 $ 𝐺 % 𝑮 𝟓 Expansion Selection 𝑅 Reaction Node 𝐺 Molecule Node 𝑮 𝟏 𝑅 " 𝑅 # 𝐺 # 𝐺 $ 𝐺 % 𝐺 ' 𝐺 ! ∉ 𝐺 "#"$% …𝐺 &amp; ∈ 𝐺 "#"$% Sample(
With the GNN predictor, the negative log-likelihood of the reaction can be used to compute path cost J current (•) to the leaf molecule node, we design J heuristic (•) with the LLM in Llamole.</p>
<p>3 Llamole: Multimodal Large Language Model for Molecular Discovery</p>
<p>Multimodal Autoregressive Modeling</p>
<p>In molecular discovery, the sequence may include molecular structures G and retrosynthetic reactions R with each molecule or reaction tokenized.The sequence Y = {y 1 , y 2 , . . ., y N }, where y i ∈ W ∪ G ∪ R, combines these tokens.The sequence is interleaved with tokens in different spaces.</p>
<p>Suppose the molecule appears at position i; then, we typically see:
. . . , Y i ∈ G, Y i+1:i+L ∈ W, Y i+L+1 ∈ R, . . .L Llamole = L LM + L DM + L predictor = i − log p Θ (y i |Y &lt;i ).(4)
L DM interprets Y &lt;i as the input conditions, including desirable molecular properties and text con-
ditions {c i } M i=1 ∪ {c text } for the autoregression of Y i in G.
In L predictor , Y &lt;i represents G product and c text .Here, G product is generated from previous diffusion models or as intermediate G / ∈ G avail in retrosynthesis.The autoregression for the label Y i is performed in the reaction space R.</p>
<p>We present an overview of multimodal autoregression with Llamole in Figure 3, divided into controllable molecular generation and retrosynthetic planning.The base LLMs perform multiple roles: generating text, controlling the switch of active modules, and providing cost functions for A* search.</p>
<p>Augmented with the graph models, the overall parameters in Llamole are Θ = {θ 1 , θ 2 , θ 3 , ϕ 1 , ϕ 2 , ϕ 3 }, where ϕ 1 and ϕ 2 project text into c text for the Graph DiT and GNN predictor, respectively.The graph encoder with ϕ 3 projects molecule tokens into the LLM.Next, we detail the design space of Llamole.</p>
<p>Llamole Design Space</p>
<p>Llamole consists of a base LLM and two pre-trained graph modules: the Graph DiT for molecule generation and the GNN for one-step reaction prediction.The base LLM employs a trigger-queryprediction approach using two sets of special tokens to switch between modules.</p>
<p>Trigger Tokens.Llamole defines two special trigger tokens to augment the word vocabulary W: <design> for switching between the LLM and Graph DiT, and <retro> for switching between the LLM and GNN predictor.When a trigger token is predicted, Llamole activates the corresponding graph model.After molecule generation or reaction prediction, the active modules revert to the LLM.</p>
<p>Query Tokens.We introduce another set of special tokens, named query tokens <query> automatically placed after triggers.They use the LLM to query previous tokens and output hidden states as c hidden .A linear layer is applied: c text = Linear(c hidden ), adjusting the input size for the graph models.We use different query tokens for different triggers.Query tokens allow us to share parameters ϕ 1 and ϕ 2 with θ 1 , enhancing both efficiency and effectiveness.We can apply ensemble methods by repeating the query tokens multiple times and averaging the c hidden values [Dong et al., 2023].</p>
<p>Besides the special tokens, Llamole enhances molecule understanding with a graph encoder and uses the LLM to provide the cost function in A* search for retrosynthetic planning.</p>
<p>Graph Encoder.The graph encoder parameterized by ϕ 3 replaces the word encoder in the LLM tokenizer for molecule tokens.The LLM decoder takes molecule embeddings from the graph encoder, along with text embeddings from the tokenizer, into the Transformer layers for next token generation.We use a pre-trained Graph Isomorphism Network (GIN) [Xu et al., 2018] as the graph encoder, optimized via molecule-text contrastive learning similar to CLIP [Radford et al., 2021].</p>
<p>A* Cost Function with LLM.We define J heuristic as a multi-choice problem, where each choice, assigned a score, represents synthesis complexity, from few to many steps.The LLM estimates the remaining synthesis steps for the leaf molecule node G ∈ F(T ) \ G avail in the search tree T .It outputs probabilities for each choice, and J heuristic is computed as the weighted score by averaging the scores with their probabilities.For G ∈ F(T ) ∩ G avail , J heuristic = 0.</p>
<p>End-to-End Model Fine-Tuning and Generation</p>
<p>Supervised Fine-Tuning.We use multimodal SFT to connect the base LLM and other graph modules in Llamole [Ouyang et al., 2022].Specifically, we freeze the parameters for the graph modules (θ 2 and θ 3 ) and fine-tune the LLM parameters θ 1 , the learnable special tokens, and the linear layers for the query tokens (ϕ 1 and ϕ 2 ).We freeze the parameters of the pre-trained graph encoder (ϕ 3 ) and add a tunable linear layer between it and the LLM decoder.The optimization can be conducted end-to-end with Eq. ( 4).The SFT aligns the LLM with domain-specific graph models.To maintain generality in the base LLM, we employ parameter-efficient LoRA [Hu et al., 2021].</p>
<p>Interleaved Generation.Given a question as shown in Figure 2, Llamole performs controllable and synthesizable molecular designs, as presented in Figure 3.For the controllable generation, Llamole uses the base LLM to analyze the requirements and switches to the Graph DiT for generating G target when the trigger is predicted.For the synthesizable generation, Llamole plans synthesis routes for G target .A<em> search on the AND-OR tree T aids in multi-step generation, interleaving molecule and reaction nodes, with G target as the root.During each selection-expansion iteration, A</em> selects G next = arg min G∈F (T ) J(G) from the leaf nodes F(T ).The graph encoder embeds molecule tokens into the LLM, which generates reaction conditions until the token <retro> is triggered, activating the GNN predictor.The predictor then predicts the top-50 templates as reaction nodes, along with corresponding reactants as molecule nodes for the next iteration.A* stops after finding a route from G target to G avail with satisfying all AND-OR constraints, or if it fails after 30 seconds or 300 iterations.Upon success, the text with the corresponding reaction along the route is returned for retrosynthesis; otherwise, the base LLM directly generates texts.</p>
<p>"3-methyl-5-nitro-isoquinoline-2oxide (12 g) was added to phosphorus oxychloride (60 ml) and the mixture was refluxed with stirring for 1 hour, …This product was then recrystallized twice from acetone to yield 1-chloro-3-methyl-5-nitroisoquinoline melting at 112°C."</p>
<p>Benchmarking for Multimodal Molecular Design</p>
<p>To train Llamole, we need instruction data that provide detailed language supervision and evaluation covering synthetic complexity, drug and material utility, and reaction conditions.However, existing data based on PubChem [Kim et al., 2021] are only usable for small molecules and lack such details.Thus, we create MolQA, a large-scale graph-text multimodal instruction dataset for systematic LLM benchmarking used in Section 5. We also create MolPair with graph-text and reaction-text pairwise data to pre-train graph modules, as detailed in appendix C. To this end, we first collect multisource molecule data (Figure 4), with details in appendix B. Then we create MolQA and MolPair.</p>
<p>MolQA: Instruction Data Creation.USPTO reactions include text descriptions.We use Enamine's 1.3 million small molecules as G avail .The depth-first search identifies routes from reaction products (i.e.target molecules) to molecules within G avail , resulting in about 139K routes with lengths ranging from 1 to 10.We sample around 11K routes (750 for materials and 9986 for drugs) for testing and use the rest for instruction tuning.We focus on eight popular properties for benchmarking (i.e, M = 8 for {c i } M 1 in Eq. ( 2)).They include three drug-related categorical properties [Wu et al., 2018]: (1) HIV virus replication inhibition (HIV), (2) blood-brain barrier permeability (BBBP), and (3) human β-secretase 1 inhibition (BACE) and five continuous material properties [Thornton et al., 2012]: (4) CO 2 permeability, (5) N 2 permeability, (6) O 2 permeability, (7) fractional free volume (FFV), and (8) thermal conductivity (TC).Not all target molecules have these properties.To enrich properties and texts, two supervised GNNs predict drug and material properties with confidence scores as in Liu et al. [2022Liu et al. [ , 2023a]].Only high-confident predictions are selected for annotation.Llama-3-70B then generates descriptions using a template that incorporates these properties with structural and synthesis information from toolkits like RDKit.There are no polymerization reactions; we consider the monomer structure of the polymer as the synthesis target.We assemble molecule descriptions, text, and reactions from synthesis routes as answer data.Then Llama-3-70B is prompted to generate questions, resulting in MolQA with the example as shown in Figure 2. Details are in appendix B.2.</p>
<p>MolPair: Pairwise Data Creation.After excluding the target molecules from the instruction data, we use the remaining text-reaction data from USPTO to pre-train the GNN reaction predictor.Similarly, we utilize all other small molecules and polymers to pre-train the Graph DiT and graph encoder.For generalization, we expand beyond the eight properties used in the instruction data.For drug utility, we train another GNN to predict 41 properties, including toxicity, safety, enzyme interaction, absorption, distribution, metabolism, excretion (ADME), and biological activity [Swanson et al., 2024].For material utility, we consider 14 properties, such as thermal, physical, thermodynamic, permeability, solubility, and dielectric properties.Llama-3-70B generates related texts for these properties, incorporating structural and synthetic information.Finally, there are around 600K graphtext pairs for both small molecules and polymers to support pre-training.Details are in appendix B.3.</p>
<p>Experiment</p>
<p>We conduct a systematic evaluation to demonstrate Llamole's superior performance in controllable and synthesizable molecular design (RQ1).We investigate Llamole's performance in controllable molecular generation through ablation and case studies (RQ2).We analyze retrosynthetic performance of LLMs, focusing on error analysis and the efficiency and effectiveness of Llamole (RQ3).</p>
<p>Set-ups:</p>
<p>We include LLM baselines from 7B to 70B, such as Llama, Mistral, Qwen, Granite, and Flan-T5, using either ICL or LoRA-based SFT.The MolQA test set contains 9,986 QA pairs for material design and 750 for drug design.LLMs are prompted with questions to generate responses for texts, molecules, and reactions.For controllability, we evaluate up to 12 metrics across four aspects: (1) chemical validity, (2) similarity to the reference based on Morgan fingerprints [Rogers and Hahn, 2010], (3) BLEU-4 and ROUGE-L scores against reference texts, and (4) deviation from desired properties.We follow Gao et al. [2022] to use well-trained random forests as the oracle functions for obtaining properties of designed molecules.We focus on three drug-related categorical properties assessed by balanced accuracy (BA) and five continuous material properties assessed by mean absolute error (MAE).For retrosynthesis, we evaluate the success rate of designed molecules against those available in G avail from Enamine.Details are in appendix D.1.</p>
<p>RQ1: LLMs for Controllable and Synthesizable Molecular Design
Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Llama-3-70B Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B Mistral-7B Qwen2-7B Llama-3.1-8B</p>
<p>In-Context
Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Llama-3-70B Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B Mistral-7B Qwen2-7B Llama-3.1-8B
In  In-Context Learning (2) SFT improves molecular design but may not always enhance retrosynthesis.According to Figure 5 and Table 1, SFT enables 7B LLMs to achieve chemical validity, structure, and property control comparable to 70B LLMs with ICL.However, it offers minimal improvement in planning ability for the generated target molecule.A notable example is Llama-3-8B from Table 2, where SFT reduces its retrosynthesis planning success from 5.5% to below 1%.Except for Llama-3-8B, we connect LLM performance with the same baseline but different learning methods in Figure 5.The results show that SFT methods still outperform ICL with the same base 7B models in most cases.
Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Drug (%) 0.1 0.2 0.0 5.5 0.4 0.6 1.2 1.6 1.0 Material (%) 0.3 0.4 0.0 4.8 0.8 1.6 1.2 1.7 0.8 Supervised Fine-tuning Llamole Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B Mistral-7B Qwen2-7B Llama-3.1-8B Drug (%) 1.
(3) Larger models without domain-specific adaptation do not necessarily perform better in molecular designs.We calculate the average Pearson correlation coefficient between model size and molecular design metrics, yielding a value of 0.366, indicating a weak correlation (below 0.5) between size and performance.We also compare LLM performance with GraphGA, which has been shown to be simple yet powerful [Gao et al., 2022, Liu et al., 2024c].Our observations confirm that GraphGA serves as a strong molecular design baseline, challenging most LLM models with ICL and SFT in generating molecules with precise multi-condition control.</p>
<p>RQ2: Discussion on Controllable Molecular Generation</p>
<p>Ablation Studies on LLM and Graph DiT Synergy</p>
<p>We investigate the synergy effect of Graph DiT and LLM in Llamole for molecule controllability.We first remove text conditions c text .In this case, Graph DiT uses a learned "null" embedding to represent the dropped condition c text = ∅.Next, we remove the drug or material property conditions {c i } M i associated with the question.Results in Figure 6 show that text instructions enhance the chemical structure understanding ability of Graph DiT, while Llamole leverages Graph DiT's capabilities with property inputs to generate molecules with desirable properties."To satisfy the requirements: The molecule has a complex structure with two aromatic rings connected to aliphatic chains, featuring a carboxyl group and an amino group, which contributes to its moderate synthetic complexity score.Therefore, the designed molecule is <molecule_1>.""To synthesize <molecule_1>, follow these procedures: The electrolyte solution was prepared by dissolving 2.2 g of 1,4-bis (3,5-dimethylphenyl)benzene (compound 14) in 20 ml of toluene.The solution was stirred at room temperature for 3 hours.The solvent was evaporated under reduced pressure, and the residue was purified by column chromatography (hexane/ethyl acetate=1/1) to give 1.8 g (88%) of the title compound as a colorless oil.The applied reaction is <reaction_1> with the <template_1> which requires the reactants: <molecule_2> (available), <molecule_3>.""To synthesize <molecule_3>, follow these procedures: 100 g of meldrum acid were dissolved in 100 ml of methanol.100 g of 3-hydroxybenzaldehyde were added and the mixture was stirred for 1 hour at room temperature.100 ml of saturated sodium bicarbonate solution were added and the mixture was extracted with 100 ml of ethyl acetate.The organic phase was dried over sodium sulphate, filtered and concentrated to dryness.The residue was purified by chromatography on silica gel with hexane/ethyl acetate as eluent, yielding 100 g of 5-benzyl-2-oxo-2,3-dihydroindoline-3-carboxylic acid as an oil.The applied reaction is: <reaction_2> with the template <template_2> which requires the reactants: <molecule_4> (available), <molecule_5>(available)."</p>
<p>Case Studies for Property and Structure Controllability</p>
<p>In Figure 7, Llamole can design a satisfactory molecule that meets both functional and structural constraints.Functionally, the oracle function confirms that the properties of BACE and HIV align with the criteria.Structurally, all key criteria are satisfied, including molecular weight, "two aromatic rings," and "connected to aliphatic chains."Llamole also adds details for structure design, such as a carboxyl ( -COOH) group and an amino group ( -NH 2 ).While the amino group is present in the structure, it is connected to the carbonyl group ( -C( --O) -) instead of the carboxyl group.This subtle difference may require precise control based on the text condition.More results are in appendix D.3.(1) one-step reaction generation and (2) multistep planning.Table 2 highlights the weaknesses of LLMs with ICL and SFT in overall planning ability and the promise of Llamole.We examine the failure reasons in LLMs and the synergy between the GNN and LLMs to avoid them.</p>
<p>RQ3: Discussion on Retrosynthetic Planning</p>
<p>One-step Reaction Generation</p>
<p>We conduct error analysis for LLMs in reaction generation.Results in Figure 8 average performance across all LLMs using ICL or SFT methods.We identify five types of errors related to instruction adherence, format compliance, and template matching.We find that LLMs using ICL frequently fail to follow instructions for generating reactions in text format, with a high probability (68.4%) of not producing valid formats and templates.In contrast, LLMs with SFT reduce this probability to 57.6%.However, neither ICL nor SFT guarantees that the templates are correct or match the generated reactions.In comparison, Llamole avoids these errors by using GNN predictors, which estimate probabilities for over 300K templates derived from USPTO reactions.This enables Llamole to apply templates directly to derive reactions in retrosynthesis, avoiding hallucination.</p>
<p>Multi-step Retrosynthetic Planning</p>
<p>From the success cases in  Llamole is influenced by two factors for retrosynthesis: (1) the size of the search space and (2) the quality of the cost J heuristic .The results reported in Table 2 limited the total planning time to 30 seconds (on an A6000 card).We remove this time constraint and report comparisons for material tasks in Table 3.We find that success rates for all base LLMs significantly improve, but this comes at the cost of long inference time.While there is a trade-off between efficiency and effectiveness, extending response time by a few minutes is often acceptable to improve the success rate of finding synthesis paths for a single designed molecule.In Table 3, we also compare the J heuristics designed by LLMs (default) with the domain model trained from Chen et al. [2020].we find that LLMs are competitive with these domain models in providing the cost function for A*, contrasting with previous observations where LLMs struggled with retrosynthetic planning.</p>
<p>Related Work</p>
<p>Since the emergence of ChatGPT [Achiam et al., 2023], LLMs [Dubey et al., 2024] have become foundation models for text-based problems and are revolutionizing domains like vision and speech [Dong et al., 2023, Wu et al., 2024].These advancements extend to chemistry, biology, and material sciences, focusing on molecules [Guo et al., 2023, Jablonka et al., 2023, Jin et al., 2023].Prior work explores LLMs in molecular generation, property prediction, and one-step reaction prediction in retrosynthesis [Guo et al., 2023, Jablonka et al., 2023].A key lesson is the limitation of LLMs in sequential modeling of molecules (e.g., SMILES or SELFIES) [Guo et al., 2023].Multimodal LMs have been developed in combination with GNNs [Zhao et al., 2023, Liu et al., 2023b], but they have not been scaled up to leverage LLMs with at least 7 billion parameters.Additionally, LLMs struggle with planning tasks [Kambhampati et al., 2024], which are essential for retrosynthesis.We address these issues using graph-text multimodal LLMs, augmented by A* for efficient planning.</p>
<p>Domain-specific molecular design methods have evolved from sequential models [Segler et al., 2018] to graph diffusion models [Jo et al., 2022, Vignac et al., 2022, Liu et al., 2024c].Studies show that older graph-based methods like GraphGA remain competitive [Gao et al., 2022].To incorporate property constraints, one can use molecular optimization approaches such as Bayesian optimization or REINFORCE [Gao et al., 2022], or employ diffusion models with or without predictor guidance [Vignac et al., 2022, Liu et al., 2024c].For synthesizable molecular design, prior work has focused on bottom-up methods [Gao et al., 2021, Sun et al., 2024].These methods explore a chemical space defined by a discrete action space of reaction templates and purchasable starting materials, which may limit flexibility.Thus, retrosynthesis algorithms [Chen et al., 2020] are also studied as separate solutions to find synthesis routes for generated molecules in a top-down manner.</p>
<p>Conclusion</p>
<p>We have presented the first graph-text MLLM, Llamole, for multi-conditional molecular generation and retrosynthetic planning.By integrating a base LLM with specialized graph modules, Llamole interleaved the generation of text, molecular graphs, and reactions, enabling controllable and synthesizable designs.Extensive benchmarking against 14 LLMs revealed their limitations in controlling molecular structures and planning synthesis routes.In contrast, Llamole significantly outperformed these LLMs.These findings underscored the value of multimodal approaches in molecular discovery and highlighted Llamole's potential to connect text and chemical structures.The new benchmarking dataset also laid the groundwork for future MLLM research in molecular applications.</p>
<p>A Additional Details for Llamole</p>
<p>A.1 Details of Special Tokens</p>
<p>In total, there are nine special tokens divided into three groups.These tokens augment the word vocabulary W, enabling flexible control of the generation flow:</p>
<p>• Trigger and Query tokens: <design start>, <design body>, <design end>, <retro start>, <retro body>, <retro end> • Molecule token: <molecule> • Callback tokens: <callback start>, <callback end></p>
<p>The tokens <design start> and <retro start> switch between the LLM and the Graph DiT or GNN, respectively.The tokens <design body> and <retro body> serve as query tokens, repeated eight times.After tokenization, the LLM takes their embeddings as input and outputs a vector from the last layer.The tokens <design end> and <retro end> indicate the end of these switches.</p>
<p>The <molecule> token marks the position of the molecular graph where the graph encoder is applied.In the instruction dataset, the segment "<mol start>SMILES<mol end>" denotes the position and identity of the molecule.SMILES will be converted to molecular graphs using RDKit, and this segment will be replaced by the <molecule> token for Llamole inputs.</p>
<p>Finally, callback tokens control the LLM to generate backup results as complements to the specialized graph modules.For instance, if the Graph DiT fails to produce a valid molecule, the base LLM can generate an alternative, regardless of validity.</p>
<p>A.2 Details of LLM-based A* Heuristics</p>
<p>Llamole models J heuristics in A* search as a multi-choice problem, filling in information from the molecule node, its parent reaction nodes and siblings using the template below.Parameters such as step, reaction template, and reactants are optional.</p>
<p>Estimate remaining steps for the target { smiles } given the following parameters : Current step { step } , Current template : { template } , Reactants : { reactants }.Consider the following factors : 1. Intermediate complexity 2. Reagent availability 3. Side reactions 4. Stereochemistry challenges .</p>
<p>Using this question to estimate remaining steps, we input the text into the base LLM and formulate five choices with corresponding scores:</p>
<p>A .All readily available // Score : 0 B .Some commercial , some need 1 -2 steps // Score : 1 C .Mix of commercial and multi -step synthesis // Score : 2.5 D .Mostly require complex synthesis // Score : 4.5 E .All require extensive multi -step synthesis // Score : 7</p>
<p>The LLM outputs logits for the next token, which we average for each choice to obtain overall probabilities.The J heuristics is calculated as the weighted score using these probabilities.</p>
<p>B Additional Benchmarking and Datasets Details</p>
<p>We collect small drug molecules from PubChem [Kim et al., 2021], MoleculeNet [Wu et al., 2018], ChEMBL [Zdrazil et al., 2024], and ZINC [Sterling and Irwin, 2015].Polymers are macromolecules with one repeating unit called monomers.We collect polymers from PI1M [Ma and Luo, 2020],</p>
<p>the Membrane Society of Australia (MSA) [Thornton et al., 2012], and others [Liu et al., 2024b].Additionally, we collect 3.8 million patent chemical reactions with descriptions from USPTO [Lowe, 2017], spanning from 1976 to 2016.</p>
<p>B.1 Details of Quality Control</p>
<p>After collecting molecules and polymers from various sources, we deduplicate and merge the label information for identical molecules.We use RDKit to obtain canonical SMILES.For small molecules, we calculate the first 14 characters of the InChIKey as the unique identifier, while for polymers, where the polymerization point is represented by "*", we use the canonical SMILES directly.</p>
<p>For drug-like small molecules, we apply the following rules to filter out alert structures, known as the Rule of Five (Ro5):</p>
<p>• Molecular Weight (MW): Must be ≤ 500 Da.</p>
<p>• Hydrogen Bond Acceptors (HBA): Must not exceed 10.</p>
<p>• Hydrogen Bond Donors (HBD): Must not exceed 5.</p>
<p>• LogP: Must be ≤ 5, indicating lipophilicity.</p>
<p>A molecule passes the Ro5 test if at least three of these four conditions are met, indicating potential oral bioavailability.</p>
<p>We also apply 15 filter rules from the RDKit package, including the following from the Fil-terCatalogs Class: BRENK, CHEMBL, CHEMBL BMS, CHEMBL Dundee, CHEMBL Glaxo, CHEMBL Inpharmatica, CHEMBL LINT, CHEMBL MLSMR, CHEMBL SureChEMBL, NIH, PAINS, PAINS A, PAINS B, PAINS C, and ZINC.</p>
<p>B.2 Details on the Creation of MolQA</p>
<p>B.2.1 Creation of Synthesis Routes</p>
<p>The USPTO has 3.7 million reactions.There are approximately 1.3 million unique product molecules.The purchasable compounds come from the Enamine Building Block (June 2024 version), supplemented with other common ions and starting materials, totaling around 1.3 million.We check each product from USPTO as a target molecule in the retrosynthesis task, exploring whether they can be synthesized using existing USPTO reactions through depth-first search (DFS).Ultimately, we identify about 139K target molecules with synthesis routes, supporting the creation of MolQA.</p>
<p>Since there are no polymerization reactions, we consider only monomer structures by replacing the * point with hydrogen.Among the 139K small molecules with synthesis routes, 2196 fit the monomer structures and serve as target molecules for polymer retrosynthesis.The length of synthesis routes ranges from 1 to 10.For each length of the routes, we split half of the molecules into the testing set, with a maximum of 3000, while the remainder is retained in the training set.</p>
<p>It results in around 11K routes (750 for materials and 9986 for drugs) for testing and 126K target molecules for training.</p>
<p>B.2.2 Creation of Property Annotations</p>
<p>We focus on eight benchmarking properties: three drug-related categorical properties [Wu et al., 2018]-(1) HIV virus replication inhibition (HIV), (2) blood-brain barrier permeability (BBBP), and (3) human β-secretase 1 inhibition (BACE)-and five continuous material properties [Thornton et al., 2012]-(4) CO 2 permeability (CO 2 Perm), (5) N 2 permeability (N 2 Perm), (6) O 2 permeability (O 2 Perm), (7) fractional free volume (FFV), and (8) thermal conductivity (TC).</p>
<p>First, we check existing sources for annotations of these properties.To enrich the label space, we use well-trained GNN models [Liu et al., 2022] to generate confident pseudo-labels, following the method in [Liu et al., 2023a].We collect all labeled data to train two supervised multi-task GIN models for drug and material property annotation.The GIN models employ rationalization techniques [Liu et al., 2024a] to split the molecular graph into rationale and environment subgraphs in the latent space, predicting labels from the rationale subgraph.The confidence score is computed by combining the rationale subgraph with various environment subgraphs, using the reciprocal of prediction variance.We annotate properties when prediction confidence exceeds the median threshold.</p>
<p>B.2.3 Creation of Text Data for Molecular Description</p>
<p>In addition to property annotations, we consider structural and synthesis information of the molecules using RDKit and heuristic complexity estimation scores.First, for any molecule, we extract the following structural information:</p>
<p>• Scaffold: Extracted scaffold from the molecule structure.</p>
<p>• Molecular Weight: Calculated using the molecular weight descriptor.</p>
<p>• Number of Rings: Total number of rings in the molecule.</p>
<p>• Number of Aromatic Rings: Total number of aromatic rings in the molecule.</p>
<p>• Number of Aliphatic Rings: Total number of aliphatic rings in the molecule.</p>
<p>• Number of Rotatable Bonds: Total number of rotatable bonds in the molecule.</p>
<p>• Number of Hydrogen Bond Donors: Total number of hydrogen bond donors.</p>
<p>• Number of Hydrogen Bond Acceptors: Total number of hydrogen bond acceptors.</p>
<p>Next, we compute the synthetic accessibility score (SAScore) [Ertl and Schuffenhauer, 2009] and SCScore [Coley et al., 2018].Based on this information, we use the following template:</p>
<p>Generate a summary description that starts directly with " The molecule / polymer ..." based on the predicted chemical properties , synthetic complexity scores , and structural information for the molecule with SMILES : {{ smiles }}.Use your own knowledge , focus on functions , and avoid using numbers , redundant words , or mentioning SMILES .Ensure the output sentence is complete and ends with a period .This is for Drug / Material Utility of a Molecule / Polymer :</p>
<p>The structural context of a molecule includes its scaffold , which is the core structure around which the molecule is built .Key structural features include the presence of aromatic rings , aliphatic chains , and common functional groups such as hydroxyl , carboxyl , and amino groups .The pre-defined utility context for the small molecule is as follows:</p>
<p>The drug utility of a molecule is assessed based on its potential to serve as a therapeutic agent .Key properties considered include pharmacokinetics , which encompasses absorption , distribution , metabolism , excretion ( ADME ) , and toxicity .Bioactivity is another critical factor , measured by the molecule ' s ability to interact with biological targets , typically through binding affinity .Additionally , drug -likeness , which refers to the molecule ' s adherence to established rules such as Lipinski ' s Rule of Five , is essential .This rule evaluates molecular weight , hydrogen bond donors and acceptors , and lipophilicity to predict a molecule ' s suitability as an oral drug .</p>
<p>The pre-defined utility context for the polymer is as follows:</p>
<p>The material utility of a molecule , particularly for creating polymeric materials , is evaluated based on properties like mechanical strength , flexibility , and thermal and electrical behavior .For polymer membranes used in gas separation , crucial factors include gas permeability , which determines the efficiency of gas diffusion , and chemical stability , ensuring resistance to degradation .Additionally , thermal properties such as melting point and thermal conductivity are vital , as they affect the material ' s performance under various temperature conditions .Electrical properties , such as conductivity and dielectric constant , may also be significant depending on the intended application .</p>
<p>For the property variable, we include the property name with values, as well as the minimum, maximum, and percentile among the labels in the template.We repeat all annotated properties in the property variable.The estimated synthesis complexity scores are included among them.</p>
<p>We also prompt Llama-3-70B to generate short responses of 50-70 words, producing a molecular description for each molecule based on its properties, structures, and synthesis estimation.If a molecule has a description from PubChem [Kim et al., 2021], we concatenate these descriptions.</p>
<p>The generated texts may not always be meaningful or valid.We can establish filter rules based on patterns observed in poorly generated texts to remove them.We then regenerate texts for these items.After several iterations, we obtain the final text data for molecular utility descriptions, improving overall text quality.We also apply this strategy to other steps that involves prompting LLMs for synthetic data creation.</p>
<p>B.2.4 Creation of Question Answering Data</p>
<p>After annotating molecular description texts from appendix B.2.3, we combine them with reaction descriptions, including the reaction formula and template from synthesis routes in appendix B.2.1.This forms the answer data in a QA data pair.</p>
<p>Next, we prompt Llame-3-70B to generate questions for each answer based on the following template.</p>
<p>I ' m creating a question -answer dataset for LLM fine -tuning .The question is about designing a molecule / polymer with these properties : { property_info } and the following structure information : { structure_info }.The expected answer for the question is : { answer } Generate a SINGLE question about designing and synthesizing such a molecule / polymer that meets these criteria :</p>
<p>(1) Start with ' Question : '; (2) End with a question mark ;</p>
<p>(3) Sound natural ; (4) Be diverse ; (5) Avoid redundancy and introductory words ( like ' Here is a question that meets the criteria : ') (6) Do not include the answer ; (7) Do not include incorrect information .</p>
<p>Example questions :</p>
<p>(1) How can I design and synthesize a molecule with X , Y , and Z properties ?(2) What is the best way to create a polymer with X , Y , and Z characteristics ?</p>
<p>(3) How to design a molecule with X , Y , and Z features and synthesize it ?(4) I want a molecule with X , Y properties and Z structures .Please design it and describe the synthesis path .</p>
<p>The template is applied to any answer with the corresponding structure, property information, and complete answer texts.</p>
<p>B.3 Details on the Creation of MolPair</p>
<p>MolPair consists of two parts: reaction-text pairs and graph-text pairs.We curate reaction-text pairs from USPTO [Lowe, 2017], pairing each reaction with its corresponding description of the reaction conditions.We first deduplicate product molecules in reactions, obtaining input data as the product molecule alongside the reaction condition texts.Next, we extract reaction templates from the reaction formula using rdchiral [Coley et al., 2019], resulting in approximately 300K templates, which will serve as labels for predictions.Finally, we have approximately 1.6 million training examples.</p>
<p>For the graph-text pairs, we use small molecules and polymers from the multisource collection, excluding those in MolQA.We follow the same pipeline used to create property and text annotations for the MolQA data, focusing on broader properties that describe drug-related utility with 41 small molecule properties [Swanson et al., 2024].Besides the three used in MolQA, others include:</p>
<p>• Toxicity and Safety: AMES, Carcinogens Lagunin, ClinTox, DILI, Skin Reaction, hERG • Permeability Properties: Gas diffusion coefficient (D) [cm 2 /s]; Gas permeability coefficient (P ) [cm 3 (STP)•cm/(cm 2 •s•Pa)]; Oxygen (O 2 ) Gas Permeability (Barrer); Nitrogen (N 2 ) Gas Permeability (Barrer); Carbon Dioxide (CO 2 ) Gas Permeability (Barrer)</p>
<p>• Solubility Properties: Gas solubility coefficient (S) [cm 3 (STP)•cm/(cm 2 •s•Pa)]</p>
<p>• Dielectric &amp; Optical Properties: Dielectric constant.</p>
<p>We train two multi-task GIN models based on the rationalization method [Liu et al., 2022] using all existing labeled data for drug and material property prediction, respectively.We use these models to predict properties for millions of small molecules and polymers, retaining the top ten thousand predictions by confidence score for each property.These are then used to prompt Llama-3-70B to create molecular descriptions, using the same prompt template as in appendix B.2.3.Additionally, we apply the same strategy as in appendix B.2.3 to annotate labels for the eight studied properties, which can serve as input for pretraining the multi-conditional Graph DiT.Finally, we have approximately 300K graph-text pairs for small molecules and 300K graph-text pairs for polymers.</p>
<p>C Additional Pre-training and Fine-tuning Details</p>
<p>We pre-train three graph models including Graph DiT [Liu et al., 2024c] for multi-conditional molecular generation, a GIN-based GNN predictor for reaction template prediction, and a GIN-based graph encoder for molecule understanding [Xu et al., 2018].</p>
<p>C.1 Pre-training of Graph Diffusion Transformers</p>
<p>Suppose the node has F V categories and the edge has F E categories (including non-bond).Graph DiT models the node token by concatenating all its edge configurations to other nodes.For each node x ∈ R F , we have
F = F V + N G × F E ,
where N G denotes the graph size.This facilitates defining the transition matrix Q for the joint distribution of nodes and edges [Liu et al., 2024c].Graph DiT uses Transformer layers, replacing layer normalization with adaptive layer normalization (AdaLN):
AdaLN (h, c) = γ θ (c) ⊙ h − µ (h) σ (h) + β θ (c),
where h denotes the hidden state of x and c is the vector representing the input conditions.</p>
<p>Given multiple conditions with categorical, continuous properties, and text, Graph DiT uses one-hot encoding for categorical properties and a clustering-based approach with Linear (Softmax (Linear(c))) to embed continuous condition values c.We employ pre-trained SciBERT [Beltagy et al., 2019] to embed input texts into a 768-dimensional vector by averaging the representations of all text tokens in the sentence, then using a linear layer to adjust the dimension for Graph DiT.For each condition, the model also learns a drop embedding.The drop embedding is used when no values are provided.Finally, the model sums the representation vectors of different conditions as input for c.In the reverse diffusion process, the denoising model uses predictor-free guidance to sample molecular graphs given multiple conditions.We pre-train the denoising model with the loss function in Eq. ( 2) using 600K graph-text pairwise data and the eight properties defined in appendix B.3.The model employs the following hyperparameters: depth of 28, hidden size of 1024, 16 heads, and MLP hidden size of 4096.The total model size is around 574 million parameters.We pre-train the model for 45 epochs, which takes approximately one week on a single A100 card.</p>
<p>C.2 Pre-training of GNNs</p>
<p>We pre-train a three-layer GIN to predict reaction templates among 30,124 labels, using a hidden size of 512.Reaction template prediction is a multi-class classification task.Given reaction-text pairs from MolPair, we extract the product molecular graph from the reaction formula, using the reaction condition text as input.SciBERT [Beltagy et al., 2019] is used as the text encoder with frozen parameters.We average the text representations to obtain a sentence-level representation.The prediction target is the reaction template extracted from the reaction [Coley et al., 2019].GIN naturally uses molecular graphs, employing the AdaLN approach as the normalization layer added after each message-passing layer to incorporate text conditions.We pre-train the model for 5 epochs on a single V100 card, with 632 million parameters.This model serves as the reaction predictor to suggest reaction templates for Llamole.</p>
<p>For molecular understanding, we pre-train a five-layer GIN model with a hidden size of 768.SciB-ERT [Beltagy et al., 2019] is used as the text encoder with frozen parameters.We average the text representations to obtain a sentence-level representation, while the GIN model uses sum pooling to produce the graph representation.For each graph-text pair from MolPair, we optimize the graph encoder using the CLIP loss [Radford et al., 2021] for 40 epochs.The CLIP loss consists of two contrastive losses: it first computes the similarity score between graph-text pairs, then contrasts it with all other similarity scores by pairing the graph with other texts and pairing the text with other graphs as negative pairs.The model has around 43 million parameters.The model can be pre-trained on a single V100 card in a few days.This graph encoder will replace the word encoder in the LLM tokenizer module for molecules indicated by the token <molecule> as shown in appendix A.</p>
<p>C.3 Fine-tuning of Llamole</p>
<p>Llamole is fine-tuned on graph-text multimodal instruction data, freezing the parameters of the Graph DiT, GNN predictor, and graph encoder.It automatically adds eight query tokens to the sequence once the trigger tokens are predicted, allowing the base LLM to continue autoregression and output vectors for all eight query tokens.We average these output vectors as queries for prior generated texts and use them as input text vectors for the subsequent Graph DiT or GNN predictor module via a tunable linear layer.For the <molecule> token, we add a tunable linear layer on top of the token embedding after the graph encoder outputs it.Without loss of generality, we study three variants of Llamole with different base LLMs: Llama-3.1-8B[Dubey et al., 2024], Mistral-7B [Jiang et al., 2023], and Qwen2-7B [Yang et al., 2024].All LLMs are fine-tuned using LoRA [Hu et al., 2021] for four epochs, taking approximately two days on a single A100 card.</p>
<p>D Additional Experimental Details and Discussions</p>
<p>D.1 Additional Details on Experimental Set-ups</p>
<p>In Tables 1 and 2 and Figures 1, 5a and 5b, Llamole is compared with fourteen LLMs with sizes ranging from 7B to 70B, including Llama [Dubey et al., 2024], Mistral [Jiang et al., 2023], Qwen [Yang et al., 2024], Granite [Abdelaziz et al., 2024], and Flan-T5 [Chung et al., 2024].We prefer the instruct version of the model when available.</p>
<p>Using the MolQA training set, previous work can implement these LLMs in two ways: in-context learning (ICL) and text-only supervised fine-tuning (SFT).For ICL, we retrieve five closest QA pairs from the training set based on the average property difference from desired properties.The template used to construct the prompt with demonstrations is:</p>
<p>I ' m working on designing and synthesizing molecules .Here are some example questions and answers about molecular requirements , design , and synthesis : {{ examples }} Now , based on these examples , please answer the following question about molecular design and synthesis : {{ question }}</p>
<p>For SFT, we fine-tune the LLMs with LoRA after converting molecules into SMILES strings.</p>
<p>The MolQA test set contains 9,986 QA pairs for small molecules in drug applications and 750 pairs for polymeric materials.The questions serve as input to prompt the LLMs to generate responses.</p>
<p>For the controllability of multi-conditional molecular generation, we evaluate up to 12 metrics across four aspects: (1) chemical validity, (2) similarity to the truth based on Morgan fingerprints, (3) BLEU-4 and ROUGE-L scores compared to reference texts, and (4) deviation from desired properties.For polymer validity, we further examine whether the generated molecular structures contain at least two polymerization points ("*").To obtain the properties of the designed structure, we define an oracle function based on well-trained random forests from all annotated molecules, following previous work [Gao et al., 2022, Liu et al., 2024c].We evaluate three drug-related categorical properties using balanced accuracy (BA) and five continuous material properties using mean absolute error (MAE).As a baseline, we consider GraphGA [Gao et al., 2022] to reference the performance of LLMs compared to domain-specific methods.</p>
<p>For retrosynthesis, we evaluate the success rate from the designed molecule to those available in G avail , purchasable from the Enamine Building Block (June 2024 version), supplemented with other common ions and starting materials, totaling around 1.3 million.</p>
<p>D.1.1 Set-ups for Figure 1</p>
<p>For Figure 1, we average the balanced accuracy for three drug-related properties and five MAEs for the polymeric material properties.We then select the model with the best performance in each category based on these average metrics.For drug tasks, the best ICL model is Llama-3-8B-ICL, the best SFT model is Mistral-7B-SFT, and the best Llamole variant is based on Qwen2-7B.For material tasks, the best ICL model is Llama-3-70B-ICL, the best SFT model is Llama-3-8B-SFT, and the best Llamole variant is based on Llama-3.1-8B.Their average performance is visualized in Figure 1 in comparison with GraphGA.In-Context Learning ICL or SFT-based LLMs generate free-form text that includes both natural language and SMILESrepresented molecular structures.We need a method to automatically extract SMILES strings from LLM outputs for evaluation.Practically, one can observe generation patterns to summarize rules for regular expressions to accomplish this.In the MolQA training set, the designed molecular structures typically follow the phrase "the designed molecule is:" as shown in examples Figures 9 and 10.LLMs may not always adhere strictly to this pattern, so we may need to extend this rule to cover more cases.In the future, more sophisticated regular expressions could be developed to extract SMILES strings from text directly.However, these will still need to be combined with additional rules to identify the designed molecules, as LLMs may generate intermediate SMILES strings before and after the designed molecule.Compared to them, Llamole uses <design start> or <retro start> to indicate the position of generated molecular structures.
Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Llama-2-</p>
<p>D.2 Additional Discussion on One-Step Generation</p>
<p>We further examine the text generation results for reaction conditions.Since the answer represents just one possibility in retrosynthesis, we use the template to retrieve the best-matching reaction condition descriptions as references for Table 4, based on the available templates within the USPTO reaction space.One template may correspond to thousands of reactions, so we limit our search to five items to manage costs while identifying the best matching generated and reference pairs.</p>
<p>The results of generating reaction texts are shown in Table 4, where Llamole achieves the highest ROUGE-L but low BLEU-4 scores.The best ROUGE-L score for Llamole indicates its capacity to understand and maintain the overall structure of the answer after fine-tuning.The lower BLEU-4 scores may result from the A* search nature in Llamole, which explores a vast space (300K) of possible reactions, leading to fewer exact n-gram matches with reference sentences.The manyto-many relationships between products and reactants, along with various conditions for the same reaction, diminish BLEU-4's effectiveness in evaluating Llamole's capabilities.Overall, Llamole is not merely memorizing reaction conditions but actively exploring possibilities, yielding more contextually coherent and meaningful outputs.</p>
<p>D.3 Additional Discussion on Case Studies</p>
<p>We present case studies for baseline LLMs using the same question as in Figure 7. Results are shown in Figure 7.The reference indicates one possible ground truth for molecular design with retrosynthetic pathways, noting that many alternatives exist.Compared to the reference, results in Figure 7 demonstrate that Llamole designs another molecule with similar structures, properties, and shorter synthesis routes, showcasing its potential for controllability and generating synthesizable molecules.Using ICL, Qwen2-7B fails to generate meaningful responses, despite indicating it possesses rich knowledge about molecular design.SFT allows Qwen2-7B to more strictly follow instructions, producing meaningful responses.However, text-only generation leads to hallucinations, as the generated templates do not yield expected products in retrosynthetic planning.</p>
<p>Another example based on Llama-3.1/3-8B is provided in Figure 10.The ICL method may copy from the demonstrations to get the SMILES string CC(=O)C=Cc1cc(Cl)ccc1Cl.It also includes one SMILES string before the designed molecule, such as CN(C)c1ccc(C=NNc2ccc(I)cc2)cc1.However, it does not follow the instruction pattern and is therefore not automatically extracted for evaluation, as illustrated in appendix D.1.2.SFT follows the instructions through fine-tuning, using the pattern "the designed molecule is:" but generates invalid structures with meaninglessly repeated sentences.In contrast, Llamole generates meaningful and valid molecular structures that generally satisfy the question's requirements.During text generation for molecular design, Llamole analyzes the question and includes more details about desirable structures, such as "aromatic rings" and "aliphatic chains".Some functional groups, like hydroxyl, may not be precisely represented in the structure.This indicates a need for enhanced text instruction adherence in Graph DiT.</p>
<p>In addition to small molecules, we present a polymer inverse design case in Figure 11 based on Qwen2-7B.The polymer has polymerization points denoted by "*" in the reference structure.Since polymerization reactions are not considered, we focus on the retrosynthetic routes to the monomer structures by replacing polymerization points with hydrogen atoms.In this case, ICL-based Qwen2-7B fails molecular design due to the same issue as in Figure 9, not following instructions to generate polymer structures.SFT-based Qwen2-7B generates a polymer in SMILES format but invalid in chemical space.In contrast, Llamole successfully generates valid molecular structures through Graph DiT, satisfying the requirements of "high CO 2 permeability and low permeability to N 2 and O 2 ," and suggests a two-step retrosynthetic pathway for the monomer structure.Please note that this is a conceptual design and synthesis pathway.Actual molecular design and synthesis require detailed planning, access to necessary reagents, equipment, and expertise in organic synthesis.Additionally, the synthesis of drugs that target multiple pathways is complex and typically involves a multidisciplinary approach combining medicinal chemistry, biology, and pharmacology."</p>
<p>(Error: Molecular design failed due to not following instructions)</p>
<p>Reference Answer</p>
<p>"To satisfy the requirements: The molecule has a moderately complex structure, consisting of aromatic rings connected to aliphatic chains, and featuring carboxyl and hydroxyl functional groups.Therefore, the designed molecule is: <molecule_1>.This is step 1 of 3 in the synthesis process.To synthesize <molecule_1>, follow these procedures: A solution of (R)-acetic acid 3-(toluene-4-sulfonyloxy)-butyl ester (5.89 g, 21 mmol) and 4-chloro-2-phenoxy-phenol (5.0 g, 23 mmol) in DMF (50 mL) is treated with cesium carbonate (7.4 g, 23 mmol).The solution is heated to 60\u00b0 C. and stirred overnight.The reaction is cooled and quenched with 1N HCl.The solution is partitioned in EtOAc and water.The organic is separated, washed with brine, and dried over sodium sulfate.The organic is filtered, and the solvent is removed to afford acetic acid 3-(4-chloro-2-phenoxy-phenoxy)-butyl ester, which is then diluted in methanol (100 mL) and treated with potassium carbonate (5.68 g, 40 mmol).The reaction is stirred for 2 hours at rt.The reaction is then partitioned in EtOAc and water.The organic layer is separated, washed with brine, and dried over sodium sulfate.The organic is filtered and the solvent is removed to afford the crude product.The crude is purified by silica gel column chromatography using 1/1 hexanes/EtOAc to elute the pure product.Llamole "To satisfy the requirements: The molecule is characterized by the presence of aromatic rings, aliphatic chains, and common functional groups such as hydroxyl, carboxyl, and amino groups, which contribute to its moderate synthetic complexity.Therefore, the designed molecule is: <molecule_1> .To synthesize <molecule_1>, follow these procedures: 5 g of 3,4-dihydro-2H-1,4-benzoxazin-4-one (prepared according to the procedure of Example 1, step 1) was dissolved in 30 ml of ethanol and 1.0 g of 10% Pd/C was added.The mixture was hydrogenated under atmospheric pressure at room temperature for 24 h.The catalyst was filtered off and the filtrate was evaporated under reduced pressure.The residue was crystallized from ethanol to give 2.5 g of the title compound as a yellow solid.The applied reaction is: <reaction_1> with the template <template_1> which requires the reactants: <molecule_2>.</p>
<p>To synthesize <molecule_2>, follow these procedures: 3-[(2-Methoxy-5-methylphenyl)amino]-1-propanol (3.0 g, 14.6 mmol) and triethylamine (2.8 g, 28 mmol) were dissolved in methanol (20 ml) and cooled to 0\u00b0 C. 3-Bromopropanoic acid (1.0 g, 7.3 mmol) was added and the reaction stirred at 0\u00b0 C. for 2 h.The reaction was then allowed to warm to room temperature and stirred for 18 h.The reaction was concentrated and the residue was diluted with ethyl acetate (100 ml) and washed with 1N HCl (100 ml) and brine (100 ml).The organic layer was dried (MgSO4) and concentrated to give 3-[(2-methoxy-5-methylphenyl)amino]propanoic acid (2.4 g, 12.2 mmol, 84% yield).The applied reaction is: <reaction_2> with the template <template_2> which requires the reactants: <molecule_3> (available), <molecule_4> (available)."(Error: Molecular design failed due to not following instructions)</p>
<p>Reference Answer</p>
<p>"To satisfy the requirements: The polymer, featuring an aromatic ring and functional groups such as carboxyl and hydroxyl, exhibits a moderate synthetic complexity score and high gas permeability, indicating potential for use in gas separation membranes with good efficiency.Therefore, the designed molecule is: <polymer_1>.This is step 1 of 1 in the synthesis process.To synthesize <polymer_1>, follow these procedures: Ethanol (3.2 mL, 54 mmol) and 4-dimethylamino pyridine (DMAP) (54 mg, 0.45 mmol) were added to a mixture of pyrrole-3-carboxylic acid (500 mg, 4.5 mmol) and dicyclohexylcarbodiimide (1.11 g, 5.4 mmol) (Aldrich, Milwaukee, Wis.) in tetrahydrofuran (THF) (15 mL).After heating at 60\u00b0 C. for 10 hours, the reaction was cooled.</p>
<p>The precipitate was filtered off, washed with ethyl acetate, the combined filtrate was concentrated and purified on a silica gel column to give 500 mg (81%) of 1H-pyrrole-3 carboxylic acid ethyl ester as a colorless oil.The applied reaction is: <reaction_1> with the template <template_1>, which requires the reactants: <molecule_2> (available), <molecule_3> (available), and <molecule_4> (available)" To synthesize <molecule_2>, follow these procedures: 2.0 g (0.004 mol) of 2,3-dihydro-1H-indol-4-ylboronic acid were suspended in 100 ml of ethanol and 0.5 ml of acetic acid were added.The reaction mixture was heated at reflux for 24 hours.The reaction mixture was cooled to room temperature and concentrated under reduced pressure.The residue was purified by column chromatography on silica gel (eluent: ethyl acetate/hexane 1:1) to give 1.0 g (51%) of 2,3-dihydro-1H-indol-4-ylboronic acid as a colorless powder.The applied reaction is: <reaction_2> with the <template_2> which requires the reactants: <molecule_3> (available), <molecule_4> (available)."</p>
<p>SFT</p>
<p>""Figure 2 :
2
Figure 2: Three LLM-based methods for molecular design.The question outlines requirements for properties, structures, and synthesis, addressed as follows: (a) In-Context Learning and (b) Supervised Fine-Tuning use text-only data for demonstrations and instruction tuning, respectively.(c) The proposed Llamole uses graph-text multimodal data to fine-tune the LLM, integrating parameterfrozen graph models for interleaved text and graph generation with reaction inference.</p>
<p>Figure 3 :
3
Figure 3: Overview of Llamole: Trigger tokens (<design> and <retro>) switch active modules from the base LLM to the respective graph component.The subsequent <query> token utilizes output vectors from the LLM to summarize past texts as conditions.Using these, Llamole generates molecules and predicts one-step reactions.Enhanced with a graph encoder and A* search, Llamole efficiently plans synthesis routes through selection and expansion iterations on the AND-OR Tree.</p>
<p>where L is the length of the text following the molecule at position i.The sequence starts with text.If position i denotes the first molecule in the sequence, then Y &lt;i ∈ W; otherwise, y i−1 ∈ R. To handle non-word tokens, we integrate domain-specific Graph DiT and GNN with the LLM, forming a multimodal LLM, i.e., Llamole.Parameterized by Θ, Llamole unifies the cross-entropy losses from Eqs. (1) to (3) into autoregressive modeling:</p>
<p>Figure 4 :
4
Figure 4: Creation of MolQA and MolPair: MolQA comprises two sets: a training set for ICL and (multimodal) SFT, and a test set for evaluation.MolPair consists of graph-text and reaction-text pairs, with red highlights indicating synthetic complexity, structure, and properties information.</p>
<p>Figure 5 :
5
Figure 5: Overall Comparison of LLMs for Controllability and Synthesizability: Performance is ranked by averaged BA/MAE (x-axis) and retrosynthesis success rate (y-axis).Circle size indicates model size.LLMs with ICL, SFT, and Llamole are highlighted in blue, orange, and red, respectively.</p>
<p>Figure 6 :
6
Figure 6: Ablation Studies for the Graph DiT Module in Llamole: First, we remove the text conditions from the input, i.e., c text = ∅.Next, we remove both text and property conditions, {c i } M i ∪ c text .There are learned embeddings that represent the "null" value for different conditions.</p>
<p>"Figure 7 :
7
Figure 7: Interleaved generation with the base Qwen2-7B: Red indicates positions where molecules and reactions (with templates) are generated, forming three parts.The properties of the designed molecules are obtained from the oracle.Reference and other LLM responses are shown in Figure 9.</p>
<p>"</p>
<p>Can you design and synthesize a molecule that inhibits both Human Immunodeficiency Virus and Beta-Secretase 1, with a molecular weight of around 335 and two aromatic rings connected to aliphatic chains?"Question ICL "Designing and synthesizing a molecule that inhibits both Human Immunodeficiency Virus (HIV) and Beta-Secretase 1 (BACE1) would involve creating a complex structure with specific functional groups that target each enzyme.The goal would be to incorporate a scaffold that can interact with the active sites of both HIV and BACE1.Here's a conceptual design and synthesis strategy for such a molecule: ### Molecular Design #### Requirements: -<strong>Molecular Weight</strong>: Around 335 -<strong>Two Aromatic Rings</strong>: Connected to aliphatic chains -<strong>Functional Groups</strong>: Likely to include groups that can bind to viral structures (for HIV inhibition) and to the catalytic sites of BACE1 (for BACE1 inhibition) -<strong>Hydrophobicity</strong>: Adequate for intracellular penetration -<strong>Aromaticity</strong>: To ensure stability and interactions with the target enzymes -<strong>Rotatable Bonds</strong>: For flexibility aiding in optimal binding #### Potential Molecular Structure A possible structure could contain two aromatic rings, one serving as a main binding site for HIV and the other for BACE1.The aromatic rings might be connected to branched aliphatic chains to balance lipophilicity and flexibility.### Synthesis Route <strong>Step 1: Synthesis of Aromatic Core</strong> -<strong>Objective</strong>: Prepare a stable aromatic ring system with appropriate substituents for targeted enzyme interaction.-<strong>Method</strong>: Use a multi-step synthesis involving condensation reactions, halogenation, or other functional group manipulations to create the necessary aromatic core.For example, starting with alkene or alkyne compounds, perform a Diels-Alder reaction or electrophilic addition reactions to introduce the first aromatic ring.Subsequent steps could involve further modifications such as halogenation, introduction of functional groups like carboxylic acids, and amine functionalities.<strong>Step 2: Functionalization and Linkage</strong> -<strong>Objective</strong>: Introduce functional groups that can interact with HIV and BACE1.-<strong>Method</strong>: Use nucleophilic substitution reactions, addition reactions, or direct coupling methods (e.g., click chemistry) to attach the required functional groups to the aromatic cores.This could involve attaching groups like sulfonamides for HIV inhibition and benzyl esters or acetyl groups for BACE1 inhibition.<strong>Step 3: Connecting Aliphatic Chains</strong> -<strong>Objective</strong>: Join the two aromatic systems via aliphatic chains to ensure spatial configurations suitable for enzyme binding.-<strong>Method</strong>: Carry out a series of aldol reactions, Diels-Alder reactions, or conjugated addition reactions to connect the functionalized aromatic rings through aliphatic bridges.The choice of aliphatic chains will depend on balancing the desired hydrophobicity and flexibility.<strong>Step 4: Protection and De-protection</strong> -<strong>Objective</strong>: Protect sensitive functional groups during synthesis and deprotect them just before final purification.-<strong>Method</strong>: Use standard protecting group strategies to safeguard functional groups that might react undesirably during synthesis.Common protecting groups include t-Bu, Fmoc, or Boc groups, depending on the functional groups involved.<strong>Step 5: Purification and Characterization</strong> -<strong>Objective</strong>: Isolate the target molecule and confirm its purity and structure.-<strong>Method</strong>: Employ techniques such as column chromatography, HPLC, or preparative TLC to purify the molecule.Use spectral analysis (NMR, MS) to verify the structure and confirm the molecular weight.### Notes: -<strong>Validation</strong>: Post-synthesis, validate the antiviral and enzyme inhibitory activities through biochemical assays targeting HIV replication and BACE1 activity.-<strong>Toxicology Studies</strong>: Conduct preliminary toxicological assessments to ensure safety profiles for desired in vitro and potentially in vivo applications.-<strong>Optimization</strong>: Iterate the design and synthesis based on biological activity data, modifying the structure to enhance potency and specificity.</p>
<p>"Figure 9 :
9
Figure 9: Reference Answer with Generation Results from ICL or SFT Using the base LLM Qwen2-7B.The question is the same as in Figure 7.</p>
<p>Figure 10: A Case Study for the Small Molecule: We include the reference answer and the generation results from ICL with Llama-3-8B and SFT and Llamole with Llama-3.1-8B.</p>
<p>What is the optimal molecular design and synthesis route for a polymer with high CO2 gas permeability and low permeability to N2 and O2, featuring an aromatic ring and specific functional groups?Question ICL "To design and synthesize a polymer with high CO2 gas permeability and low permeability to N2 and O2, featuring an aromatic ring and specific functional groups, we should aim for a structure that allows for easy CO2 transport but restricts N2 and O2 passage.This usually involves the use of polymers with flexible structures that allow for easier diffusion through the polymer matrix, combined with appropriate functional groups that could either enhance CO2 binding or hinder the diffusion of smaller gases like N2 and O2.Given the specific molecular characteristics needed ( such as high CO2 permeability and low permeability to N2 and O2, along with the presence of an aromatic ring, the following molecular design and synthesis route could be considered: <strong>Molecular Design:</strong> 1. <strong>Polymer Backbone:</strong> Select an aromatic ring as the backbone, which can improve the stability of the polymer due to its inherent rigidity and electronic properties.2. <strong>Functional Groups:</strong> Introduce functional groups that can enhance CO2 permeability while minimizing permeability for other gases.This might include groups like carbonyl ( (COOH, &gt;C=O, etc.), which have been shown to enhance CO2 permeability in some materials.Additionally, incorporating \u03c0-\u03c0 stacking interactions through the aromatic rings or conjugated groups could help trap CO2 effectively without being as effective for smaller, nonpolar N2 and O2 molecules.3. <strong>Cross-linking and Branching:</strong> Adding cross-links and branching in the polymer matrix can modify the morphology and reduce the tortuosity, thereby enhancing permeability while possibly limiting the diffusion paths for N2 and O2.<strong>Synthesis Route:</strong> 1. <strong>Preparation of Monomers:</strong> Synthesize or obtain monomers that contain the desired functional groups and are compatible with an aromatic backbone.For example, monomers containing carboxylic acid (-COOH), aldehyde (&gt;C=O), or other reactive sites that can form the desired structure after polymerization.2. <strong>Polymerization:</strong> Use a suitable polymerization method such as radical, anionic, or condensation polymerization depending on the monomer types and desired polymer properties.For instance, if carboxylic acids are used, a condensation polymerization might be employed to introduce the aromatic ring backbone and functional groups simultaneously.3. <strong>Post-Polymerization Functionalization:</strong> If necessary, carry out post-polymerization modifications to fine-tune the properties further.This might involve grafting or linking additional functional groups onto the polymer backbone to enhance the specific gas permeation characteristics.4. <strong>Characterization:</strong> Analyze the synthesized polymer for its gas permeability, specifically towards CO2,N2, and O2, to  ensure that the molecular design meets the required criteria.Techniques like permeability tests, gas adsorption, and spectroscopy could be used for this purpose.5. <strong>Optimization:</strong> Based on the characterization results, optimize the molecular design or synthesis conditions as needed to maximize CO2 permeability while maintaining or minimizing N2 and O2 permeability.In summary, the optimal molecular design would incorporate an aromatic backbone with strategically placed functional groups that facilitate CO2 transport while hindering the diffusion of N2 and O2, thus achieving the desired gas permeation properties.The synthesis route involves preparing the right monomers, selecting an appropriate polymerization method, potentially conducting post-polymerization modifications, characterizing the final polymer, and iteratively optimizing the design if necessary."</p>
<p>"</p>
<p>The polymer has a relatively low synthetic complexity score, indicating it can be easily synthesized.Therefore, the designed molecule is: <em>Oc1cc(CC(=O)OC(C)(C)C)c(</em>)ccc1F.This is step 1 of 1 in the synthesis process.To synthesize Cc1cccc(F)c1CO.CC(C)(C)OC(=O)CCl, follow these procedures: 30 g (0.33 mol) of 3-fluoro-2-methylbenzyl alcohol are dissolved at 5\u00b0 C. in 200 ml of dichloromethane and 55.6 g (0.40 mol) of titanium tetrachloride and then stirred with cooling at 5\u00b0 C. for 15 minutes.A solution of 27.9 g (0.33 mol) of 2-chloroacetyl chloride in 50 ml of dichloromethane is added dropwise and the mixture is allowed to warm to room temperature and stirring is continued for 2 hours.The reaction mixture is extracted with water and the organic phase is washed with saturated sodium hydrogen carbonate solution, water once again and an aqueous saturated sodium chloride solution.The organic phase is dried over magnesium sulphate and concentrated.The residue is chromatographed on silica gel using cyclohexane/ethyl acetate (8:2).This gives the desired product in a yield of 31.3 g (70% of theory).The applied reaction is: Cc1cccc(F)c1CO&gt;&gt;Cc1cccc(F)c1CO.O=C(Cl)CCl with the template [C:2]-[CH2;D2;+0:1]-[OH;D1;+0:3]&gt;&gt;Cl-[CH2;D2;+0:1]-[C:2].[O;H0;D1;+0:3]=<a href="-[C;D1;H3:5]">C;H0;D3;+0:4</a>-<a href="-[C:7]">C;H0;D3;+0:6</a>=[O;H0;D1;+0:8], which requires the reactants: Cc1cccc(F)c1CO (available), and O=C(Cl)CCl (available).Based on the provided information, the overall synthetic complexity score for this molecule is low, indicating that it can be easily synthesized.Therefore, the designed molecule is: CC(C)(C)OC(=O)CBr… (irrelavant)" (Error: Molecular design failed due to invalid structure and not following instructions) <polymer_1> <template_1>Llamole "To satisfy the requirements: The polymer has a relatively low synthetic complexity score, indicating that it can be easily synthesized.The presence of an aromatic ring and common functional groups such as hydroxyl and carboxyl groups contributes to its structural complexity.Therefore, the designed molecule is: <polymer_1>.To synthesize <polymer_1>, follow these procedures: Mix 100.0 gms of meldrum acid hemiacetal and 150.0 gms of 10% palladium-on-charcoal in methanol and hydrogenate at atmospheric pressure for 4 hours.Remove the hydroxyl group and yields 134.2 gms of 1,1,1-trifluoro-3-methoxy-2-propyne.The applied reaction is: <reaction_1> with the template <template_1> which requires the reactants: <molecule_2>.</p>
<p>Figure 11: A Case Study for the Polymer: We include the reference answer and the generation results from ICL, SFT, and Llamole with Qwen2-7B.</p>
<p>.502 0.486 0.537 0.662
1 Introduction0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Balanced Accuracy 0.0 0.5 1.0 1.5 2.0 Mean Absolute Error0Drug (Small Molecule) Design Best ICL Best SFTGraphGA Llamole Best ICL Best SFTGraphGA Llamole 1.632 1.372 0.642 0.519 Material (Polymer) Design
Figure 1: Comparison of Controllability: Results are averaged from the best numbers from Table 1.</p>
<p>Table 1 :
1
Multi-Conditional Molecular Design with LLMs: Best overall results in each metric are in bold , best baseline results are in italic .Balanced Accuracy (BA) = True Positive Rate+True Negative Rate
2.</p>
<p>Table 1 and
1
Table 2 detail LLM performance in controllability and retrosynthesis success rate.The overall performance rankings are summarized in Figure 5.Our key observations are:
Retrosynthesis Success Rate (Ranking)1 3 5 7 9 11 13 15 173 Multi-Property Controllability (Ranking) 5 7 9 11 13 15 171
(1) Llamole significantly outperforms other LLMs in text generation, controllable molecule generation, and retrosynthetic planning.Llamole fine-tuned on various 7B-parameter LLMs, as shown in Table2, results in top-3 rankings, surpassing 70B models that are 10× larger across all 12 metrics for controllability and planning success.Specifically, Llamole enhances chemical structure validity by 14.6%, structure controllability by 4.4%, and text generation by 11.9%-15.9%.Additionally, Llamole improves property controllability by 32% to 80%.In retrosynthesis, Table2indicates Llamole increases the success ratio from 5% to 35% for drugs and to 17.9% for polymers.</p>
<p>Table 2 :
2
Retrosynthetic Success Rate: Best results are in bold , best baseline results are in italic .</p>
<p>Table2, we find that 96.40% of 777 success cases in ICL-adapted LLMs and 94.14% of 324 success cases in SFT-adapted LLMs arise from one-step reaction generation.However, not all designed molecules can be synthesized via one-step reactions.Compared to LLMs, Llamole achieves over 10K success cases, with 40.48% resulting from two or more steps.Figure7illustrates a two-step planning case for the designed molecule.The generation interleaves reaction conditions and specific formulas based on the template in both steps.</p>
<p>Table 3 :
3
Analysis of J heuristics and Planning Time on Material Questions
Basew/ Domainw/ UnlimitedLLMDefaultHeuristicsTimeLlama-3.10.1760.1760.312Mistral0.1430.1470.273Qwen20.1790.1810.273</p>
<p>Autoregressive Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Molecular Design with Graph Diffusion Models . . . . . . . . . . . . . . . . . . . 3 2.3 One-Step Reaction Prediction with Graph Neural Networks . . . . . . . . . . . . . 3 2.4 Retrosynthetic Planning with A<em> Search . . . . . . . . . . . . . . . . . . . . . . . 3 3 Llamole: Multimodal Large Language Model for Molecular Discovery 4 3.1 Multimodal Autoregressive Modeling . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Llamole Design Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.3 End-to-End Model Fine-Tuning and Generation . . . . . . . . . . . . . . . . . . . 5 Details of Special Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16 A.2 Details of LLM-based A</em> Heuristics . . . . . . . . . . . . . . . . . . . . . . . . .16 Details of Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .17 B.2 Details on the Creation of MolQA . . . . . . . . . . . . . . . . . . . . . . . . . .17 B.2.1 Creation of Synthesis Routes . . . . . . . . . . . . . . . . . . . . . . . . .17 Pre-training of Graph Diffusion Transformers . . . . . . . . . . . . . . . . . . . .21 C.2 Pre-training of GNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21 C.3 Fine-tuning of Llamole . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21 D Additional Experimental Details and Discussions 22 D.1 Additional Details on Experimental Set-ups . . . . . . . . . . . . . . . . . . . . .22 D.1.1 Set-ups for Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22 D.1.2Extraction of SMILES from LLM Responses . . . . . . . . . . . . . . . .23 D.2 Additional Discussion on One-Step Generation . . . . . . . . . . . . . . . . . . .23 D.3 Additional Discussion on Case Studies . . . . . . . . . . . . . . . . . . . . . . . .23
Contents 1 Introduction 2 Preliminaries 2.1 4 Benchmarking for Multimodal Molecular Design 5 Experiment 6 Related Work 7 Conclusion A Additional Details for Llamole A.1 B Additional Benchmarking and Datasets Details B.1 C Additional Pre-training and Fine-tuning Details C.11 3 6 6 10 11 16 16 21
5.1 RQ1: LLMs for Controllable and Synthesizable Molecular Design . . . . . . . . .7 5.2 RQ2: Discussion on Controllable Molecular Generation . . . . . . . . . . . . . . .8 5.2.1 Ablation Studies on LLM and Graph DiT Synergy . . . . . . . . . . . . .8 5.2.2 Case Studies for Property and Structure Controllability . . . . . . . . . . .9 5.3 RQ3: Discussion on Retrosynthetic Planning . . . . . . . . . . . . . . . . . . . .9 5.3.1 One-step Reaction Generation . . . . . . . . . . . . . . . . . . . . . . . .9 5.3.2Multi-step Retrosynthetic Planning . . . . . . . . . . . . . . . . . . . . . 10 B.2.2 Creation of Property Annotations . . . . . . . . . . . . . . . . . . . . . .17 B.2.3 Creation of Text Data for Molecular Description . . . . . . . . . . . . . .18 B.2.4 Creation of Question Answering Data . . . . . . . . . . . . . . . . . . . .19 B.3 Details on the Creation of MolPair . . . . . . . . . . . . . . . . . . . . . . . . . .20</p>
<p>Table 4 :
4
Text Generation for Reaction Conditions: Best results and best baselines are highlighted.</p>
<p>The solvent is removed to afford 4.35 g (72%) of the desired product.1HNMR(400MHz,CDCl3);MS(ES+)m/z mass calcd for C16H17ClO3 292, found 293 (M+1, 100%).The applied reaction is <reaction_1>,which requires the reactants: <molecule_2> (available), <molecule_3> (available), <molecule_4> (not available, needs to be synthesized),<a href="available">Cs+</a>, and [Cs+] (available).This is step 2 of 3 in the synthesis process.To synthesize <molecule_4>, follow these procedures: A \u221240\u00b0 C. solution of 4-chloro-2-phenoxy-1-methoxy-benzene (7.16 g, 30.5 mmol) in dry CH2Cl2 (70 mL) I s treated dropwise with borontribromide (22.9 g, 91.5 mmol) and then warmed to 0\u00b0 C. and stirred for 3 h under N2.The reaction is diluted with Et2O and quenched with water.The organic layer is dried (Na2SO4), and the solvent is removed in vacuo to afford 7.11 g (100%) of the title compound.Rf=0.30(4/1hexanes/acetone).1HNMR (400 MHz, CDCl3); MS (ES\u2212) m/z mass calcd for C12H9O2Cl 220, found 219 (M\u22121, 100%).The applied reaction is: <reaction_2>, which requires the reactants: <molecule_5> (not available, needs to be synthesized).This is step 3 of 3 in the synthesis process.To synthesize <molecule_5>, follow these procedures: A mixture of 2-bromo-4-chloro-1-methoxy-benzene (8.0 g, 36.1 mmol), phenol (6.80 g, 72.2 mmol), cesium carbonate (23.54 g, 72.2 mmol), copper (I) chloride(1.79 g, 18.1 mmol) and 2,2,6,6-tetramethyl-3,5-heptanedione  (1.66 g, 9.00 mmol)in dry 1-methyl-2-pyrrolidinone (80 mL) is heated to 120\u00b0 C. for 20 hours under N2.The reaction is cooled, filtered and the filtrate quenched with 1 N HCl (50 mL).The filtrate is diluted with Et2O and extracted with water.The organic layer is dried (Na2SO4), and the solvent is removed in vacuo to afford crude product that is absorbed on silica gel and purified by flash chromatography using 9/1 hexanes/EtOAc to afford 7.42 g (88%) of the title compound.</p>
<p>Rf=0.37 (4/1 hexanes/EtOAc).The applied reaction is: <reaction_3>, which requires the reactants: <molecule_6> (available), <molecule_7> (available), <molecule_3> (available),<a href="available">Cs+</a>,<a href="available">Cs+</a>, and <molecule_8>(available)."</p>
<p>Granite-function calling model: Introducing function calling abilities via multi-task learning of granular tasks. Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, Maxwell Bhargav, Chulaka Crouse, Gunasekara, arXiv:2407.001212024arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Structured denoising diffusion models in discrete state-spaces. Jacob Austin, Jonathan Daniel D Johnson, Daniel Ho, Rianne Tarlow, Van Den, Berg, Advances in Neural Information Processing Systems. 202134</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>Retro<em>: learning retrosynthetic planning with neural guided a</em> search. Binghong Chen, Chengtao Li, Hanjun Dai, Le Song, International conference on machine learning. PMLR2020</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Journal of Machine Learning Research. 25702024</p>
<p>Scscore: synthetic complexity learned from a reaction corpus. Connor W Coley, Luke Rogers, William H Green, Klavs, Jensen, Journal of chemical information and modeling. 5822018</p>
<p>Rdchiral: An rdkit wrapper for handling stereochemistry in retrosynthetic template extraction and application. Connor W Coley, William H Green, Klavs, Jensen, Journal of chemical information and modeling. 5962019</p>
<p>Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, arXiv:2309.11499Synergistic multimodal comprehension and creation. 2023arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, Journal of cheminformatics. 12009</p>
<p>Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design. Wenhao Gao, Rocío Mercado, Connor W Coley, arXiv:2110.063892021arXiv preprint</p>
<p>Sample efficiency matters: a benchmark for practical molecular optimization. Wenhao Gao, Tianfan Fu, Jimeng Sun, Connor Coley, Advances in neural information processing systems. 202235</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, Advances in Neural Information Processing Systems. 202336</p>
<p>Jonathan Ho, Tim Salimans, arXiv:2207.12598Classifier-free diffusion guidance. 2022arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Kevin Maik, Jablonka , Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bocarsly, Stefan Andres M Bran, Catherine Bringuier, Kamal Brinson, Defne Choudhary, Circi, Digital Discovery. 252023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Large language models on graphs: A comprehensive survey. Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.027832023arXiv preprint</p>
<p>Score-based generative modeling of graphs via the system of stochastic differential equations. Jaehyeong Jo, Seul Lee, Sung Ju Hwang, International Conference on Machine Learning. PMLR2022162</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>Pubchem in 2021: new data content and improved web interfaces. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucleic acids research. 49D12021</p>
<p>Graph rationalization with environment-based augmentations. Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, Meng Jiang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Semi-supervised graph imbalanced regression. Gang Liu, Tong Zhao, Eric Inae, Tengfei Luo, Meng Jiang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023a</p>
<p>Rationalizing graph neural networks with data augmentation. Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang, ACM Transactions on Knowledge Discovery from Data. 1842024a</p>
<p>Data-centric learning from unlabeled graphs with diffusion model. Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, Meng Jiang, Advances in neural information processing systems. 362024b</p>
<p>Inverse molecular design with multi-conditional diffusion guidance. Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang, arXiv:2401.138582024carXiv preprint</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023b</p>
<p>Chemical reactions from US patents. Daniel Lowe, 10.6084/m9.figshare.5104873.v11976 Sep2016. 20176</p>
<p>Pi1m: a benchmark database for polymer informatics. Ruimin Ma, Tengfei Luo, Journal of Chemical Information and Modeling. 60102020</p>
<p>Polymer database for polymeric materials design. Shingo Otsuka, Isao Kuwajima, Junko Hosoya, Yibin Xu, Masayoshi Yamazaki, Polyinfo, 2011 International Conference on Emerging Intelligent Data and Web Technologies. IEEE2011</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Extended-connectivity fingerprints. David Rogers, Mathew Hahn, Journal of chemical information and modeling. 5052010</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. Marwin Hs Segler, Thierry Kogej, Christian Tyrchan, Mark P Waller, ACS central science. 412018</p>
<p>Zinc 15-ligand discovery for everyone. Teague Sterling, John J Irwin, Journal of chemical information and modeling. 55112015</p>
<p>Michael Sun, Alston Lo, Wenhao Gao, Minghao Guo, Veronika Thost, Jie Chen, Connor Coley, Wojciech Matusik, arXiv:2409.05873Syntax-guided procedural synthesis of molecules. 2024arXiv preprint</p>
<p>Admet-ai: a machine learning admet platform for evaluation of large-scale chemical libraries. Kyle Swanson, Parker Walther, Jeremy Leitz, Souhrid Mukherjee, Joseph C Wu, Rabindra V Shivnaraine, James Zou, Bioinformatics. 407e4162024</p>
<p>Polymer gas separation membrane database. L Thornton, Robeson, Freeman, Uhlmann, 2012</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, Pascal Frossard, arXiv:2209.14734Forty-first International Conference on Machine Learning. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua, 2022. 1988. 202428arXiv preprintNExt-GPT: Any-to-any multimodal LLM</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical science. 922018</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.00826How powerful are graph neural networks?. 2018arXiv preprint</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, arXiv:2407.10671Fei Huang, et al. Qwen2 technical report. 2024arXiv preprint</p>
<p>The chembl database in 2023: a drug discovery platform spanning multiple bioactivity data types and time periods. Barbara Zdrazil, Eloy Felix, Fiona Hunter, Emma J Manners, James Blackshaw, Sybilla Corbett, Marleen De Veij, Harris Ioannidis, David Mendez Lopez, Juan F Mosquera, Nucleic acids research. 52D12024</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Lingpeng Kong, Qi Liu, Advances in Neural Information Processing Systems. 202336</p>            </div>
        </div>

    </div>
</body>
</html>
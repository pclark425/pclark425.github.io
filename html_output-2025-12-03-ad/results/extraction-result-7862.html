<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7862 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7862</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7862</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-276408437</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.10709v1.pdf" target="_blank">An Empirical Analysis of Uncertainty in Large Language Model Evaluations</a></p>
                <p><strong>Paper Abstract:</strong> As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. The code and data are released at: https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7862.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7862.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human_vs_LLM_eval_general</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General comparison between human evaluation and LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reiterates that human evaluation remains the gold standard (high quality, but costly and subjective), while LLM-based evaluators are a cost-effective, reproducible alternative that nevertheless show alignment, bias, and stability issues relative to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General LLM output evaluation across multiple tasks (helpfulness, relevance, accuracy, depth, creativity)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Various (discussed: MT-Bench, PandaLM test set, Olympic 2024 human-annotated OOD set)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLM evaluators (general class: e.g., GPT-4o, GPT-3.5-Turbo, Llama family, Qwen2)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Various proprietary and open-source LLM evaluators used in experiments (GPT-4o family, GPT-3.5-Turbo, Llama2/3 family, Qwen), token-probabilities used as internal confidence signal; temperature 0 / greedy decoding in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human raters described as gold-standard in related work; for experiments: three PhD-level annotators for the Olympic 2024 test set (majority vote labels).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>qualitative comparison; when measured in experiments, F1 (LLM vs human labels) and annotator agreement (%) used to quantify agreement with humans</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>subjectivity and cost of human evaluation; LLM evaluators: alignment gaps; preference/bias (self-preference); instability across inputs and domains; limited calibration</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human evaluation is still considered the gold standard; LLM evaluators provide reproducible and scalable judgments but can be biased, misaligned, and unstable compared to human annotators; calibration and uncertainty handling are necessary to approach human-level reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Lower cost; reproducibility; scalability; faster throughput; potential to incorporate internal uncertainty signals to improve judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Overview discussion across the paper; main empirical experiments use token-probability-based confidence, two evaluation settings (single-answer grading and pairwise comparison), temperature 0 / greedy decoding, CoT prompting required for general LLM evaluators in OOD tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7862.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7862.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Olympic2024_ConfiLM_vs_humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConfiLM (uncertainty-aware LLM evaluator) compared to human annotations on Olympic 2024 OOD test set</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors fine-tuned an uncertainty-aware evaluator (ConfiLM) on human-annotated pairs (verbalized response confidences included) and evaluated it against majority human labels on a 220-instance OOD test set (Olympic 2024); ConfiLM improved F1 relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise comparison evaluation (choose better of two responses) across categories: writing, math, extraction, reasoning, roleplay</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Olympic 2024 (manually crafted OOD test set, 220 instances, each labeled by three PhD-level annotators; final annotator agreement 97.27%)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ConfiLM (fine-tuned Llama3-8B-Instruct variant)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Llama3-8B-Instruct fine-tuned with additional input verbalizing candidate response confidences (trained on 694 human-annotated instances drawn from Alpaca 52K derived fine-tuning set); AdamW, LR 5e-5, 6 epochs on 2x A100-80GB.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three PhD-level annotators; majority-vote label used as ground truth; annotator agreement on test set 97.27%.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>F1 (LLM evaluator vs human majority label)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.621</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>LLM evaluators struggle more on creative/writing tasks (subjective) and can miss subtle hallucinations in long texts; baseline LLMs can underperform human judgments in OOD settings.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Incorporating candidate response confidence as auxiliary input during fine-tuning improves OOD evaluation performance; ConfiLM reduces credence in low-confidence responses and thus better aligns decisions with human labels on OOD data.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When made uncertainty-aware, LLM evaluators can better detect low-confidence/hallucinated outputs and improve agreement with humans on OOD content; fine-tuning on human assessments increases evaluation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-tuned on 694 human-annotated pairwise instances; test on 220 OOD instances; inputs included verbalized response confidences; evaluators output CoT-format explanations plus choice; each evaluation queried twice with responses order swapped; temperature 0 for proprietary models and greedy decoding for open-source.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7862.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7862.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o_vs_humans_Olympic2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o evaluator compared to human annotations on Olympic 2024</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was used as a general LLM evaluator and compared against human majority labels on the Olympic 2024 set; it achieved the highest single-model F1 reported but had relatively low evaluation confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise comparison and single-answer grading across Olympic 2024 categories</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Olympic 2024 (220 instances, human majority labels)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Proprietary OpenAI GPT-4o (evaluated in CoT format for these experiments); temperature set to 0 for proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three PhD-level annotators; majority-vote labels.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>F1 (LLM vs human labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.678</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Despite strong general capabilities, GPT-4o showed low evaluation confidence in single-answer grading (instability); struggled relatively on writing/task types with subjective judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4o attains relatively high agreement (F1) with human judgments but exhibits low internal evaluation confidence (reported overall evaluation confidence 0.391), suggesting competence does not guarantee stable, confident evaluation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High-performing across many categories; fast, reproducible judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluators required to produce CoT-format outputs; temperature 0; each sample queried twice with swapped order; F1 measured against human majority labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7862.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7862.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self_Preference_Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-preference bias observed when LLM evaluator and evaluated model are from same family (Llama2 example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When Llama2-70B-Instruct evaluated Llama2-7B-Instruct, the evaluator produced higher scores and markedly higher evaluation confidence compared to the average of other evaluators, indicating self-preference bias linked to shared training and linguistic patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Single-answer grading (score 0-9)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MT-Bench (and aggregated across experiments reported in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Llama2-70B-Instruct (evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Open-source Llama2 family model (70B) used as evaluator; evaluated Llama2-7B-Instruct output; greedy decoding used for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Not applicable for this specific intra-model-family comparison (comparison against averages of other LLM evaluators), but human evaluation is used elsewhere as gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>evaluation confidence comparison and score difference versus averages of other evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Self-preference bias: inflated scores and extremely high evaluation confidence when evaluator shares family with evaluated model (familiar response patterns), which risks biased assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The Llama2-70B-Instruct assigned a score of 7.875 (vs other-evaluators' average 6.456) and an evaluation confidence of ~0.953 (vs average 0.654), implying familiarity-driven bias; this highlights a structural failure mode for using 'same-family' evaluators without correction.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not an advantage here—this example flags a risk (bias) rather than a benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Single-answer grading on MT-Bench; scoring range 0-9; temperature 0 for proprietary and greedy decoding for open-source; comparisons reported against averages of other evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7862.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7862.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distribution_Sensitivity_GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLM evaluator confidence to data distribution shifts (GPT-4o example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation confidence reported for GPT-4o changed when moving from MT-Bench (more common/higher-difficulty tasks) to PandaLM test set (novel/unfamiliar instructions), indicating that evaluator confidence is sensitive to dataset distribution shifts while response confidence is more stable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Single-answer grading and pairwise comparison across MT-Bench and PandaLM test set</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MT-Bench and PandaLM test set</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Proprietary GPT-4o used as an LLM evaluator; token-probabilities used to compute evaluation confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human evaluation used elsewhere; this analysis focuses on evaluator internal confidences rather than direct human vs LLM agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>change in evaluation confidence (token-probability of chosen label) between datasets</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Evaluation confidence fluctuates significantly across distributions (sensitivity to OOD); response generation confidence of candidate models remains relatively stable in comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Example: GPT-4o evaluation confidence changed from 0.417 (on MT-Bench single-answer grading) to 0.473 (on PandaLM test set); response confidence variance between datasets was much smaller (~0.014), showing that evaluator uncertainty is especially sensitive to distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>N/A — this entry highlights vulnerability to OOD rather than an advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluator confidence measured as probability of the output token representing the evaluation result; experiments compare same evaluators on MT-Bench vs PandaLM test set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7862.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7862.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvalConf_vs_accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed positive correlation between evaluator confidence and agreement with human labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across six LLM evaluators on the Olympic 2024 set, higher evaluation confidence bins correspond to higher accuracy (agreement) with human judgments; low-confidence evaluations are less accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise comparison on Olympic 2024 (analysis of confidence vs accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Olympic 2024</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Six LLM evaluators (aggregate analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Aggregate of multiple LLM evaluators studied in paper (both proprietary and open-source); confidences estimated via token-probabilities and alternative methods in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three PhD-level annotators (majority labels); high annotator agreement ensures human labels considered reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy of LLM judgments against human majority label, stratified by evaluator confidence bins</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Low-confidence LLM judgments correlate with lower agreement with humans; thus uncalibrated LLM judgments can mislead if confidence not considered.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Table 20 analysis shows a monotonic improvement in accuracy as evaluation confidence increases, peaking in high-confidence intervals (e.g., [0.8,1.0)). This supports using internal confidence as a signal for trust or human fallback.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When confidence is high, LLM evaluators are more reliable and may safely replace or triage human evaluation; confidence can guide when to escalate to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Analysis binned evaluator outputs by token-probability confidence and computed corresponding accuracy vs human labels for six LLM evaluators on Olympic 2024.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization <em>(Rating: 2)</em></li>
                <li>Prometheus 2: An open source language model specialized in evaluating other language models <em>(Rating: 2)</em></li>
                <li>Detecting hallucinations in large language models using semantic entropy <em>(Rating: 1)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7862",
    "paper_id": "paper-276408437",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Human_vs_LLM_eval_general",
            "name_full": "General comparison between human evaluation and LLM-as-a-judge",
            "brief_description": "The paper reiterates that human evaluation remains the gold standard (high quality, but costly and subjective), while LLM-based evaluators are a cost-effective, reproducible alternative that nevertheless show alignment, bias, and stability issues relative to human judgments.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "paper_title": "AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS",
            "evaluation_task": "General LLM output evaluation across multiple tasks (helpfulness, relevance, accuracy, depth, creativity)",
            "dataset_name": "Various (discussed: MT-Bench, PandaLM test set, Olympic 2024 human-annotated OOD set)",
            "judge_model_name": "LLM evaluators (general class: e.g., GPT-4o, GPT-3.5-Turbo, Llama family, Qwen2)",
            "judge_model_details": "Various proprietary and open-source LLM evaluators used in experiments (GPT-4o family, GPT-3.5-Turbo, Llama2/3 family, Qwen), token-probabilities used as internal confidence signal; temperature 0 / greedy decoding in experiments.",
            "human_evaluator_type": "Human raters described as gold-standard in related work; for experiments: three PhD-level annotators for the Olympic 2024 test set (majority vote labels).",
            "agreement_metric": "qualitative comparison; when measured in experiments, F1 (LLM vs human labels) and annotator agreement (%) used to quantify agreement with humans",
            "agreement_score": null,
            "reported_loss_aspects": "subjectivity and cost of human evaluation; LLM evaluators: alignment gaps; preference/bias (self-preference); instability across inputs and domains; limited calibration",
            "qualitative_findings": "Human evaluation is still considered the gold standard; LLM evaluators provide reproducible and scalable judgments but can be biased, misaligned, and unstable compared to human annotators; calibration and uncertainty handling are necessary to approach human-level reliability.",
            "advantages_of_llm_judge": "Lower cost; reproducibility; scalability; faster throughput; potential to incorporate internal uncertainty signals to improve judgments.",
            "experimental_setting": "Overview discussion across the paper; main empirical experiments use token-probability-based confidence, two evaluation settings (single-answer grading and pairwise comparison), temperature 0 / greedy decoding, CoT prompting required for general LLM evaluators in OOD tests.",
            "uuid": "e7862.0",
            "source_info": {
                "paper_title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Olympic2024_ConfiLM_vs_humans",
            "name_full": "ConfiLM (uncertainty-aware LLM evaluator) compared to human annotations on Olympic 2024 OOD test set",
            "brief_description": "The authors fine-tuned an uncertainty-aware evaluator (ConfiLM) on human-annotated pairs (verbalized response confidences included) and evaluated it against majority human labels on a 220-instance OOD test set (Olympic 2024); ConfiLM improved F1 relative to baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS",
            "evaluation_task": "Pairwise comparison evaluation (choose better of two responses) across categories: writing, math, extraction, reasoning, roleplay",
            "dataset_name": "Olympic 2024 (manually crafted OOD test set, 220 instances, each labeled by three PhD-level annotators; final annotator agreement 97.27%)",
            "judge_model_name": "ConfiLM (fine-tuned Llama3-8B-Instruct variant)",
            "judge_model_details": "Llama3-8B-Instruct fine-tuned with additional input verbalizing candidate response confidences (trained on 694 human-annotated instances drawn from Alpaca 52K derived fine-tuning set); AdamW, LR 5e-5, 6 epochs on 2x A100-80GB.",
            "human_evaluator_type": "Three PhD-level annotators; majority-vote label used as ground truth; annotator agreement on test set 97.27%.",
            "agreement_metric": "F1 (LLM evaluator vs human majority label)",
            "agreement_score": 0.621,
            "reported_loss_aspects": "LLM evaluators struggle more on creative/writing tasks (subjective) and can miss subtle hallucinations in long texts; baseline LLMs can underperform human judgments in OOD settings.",
            "qualitative_findings": "Incorporating candidate response confidence as auxiliary input during fine-tuning improves OOD evaluation performance; ConfiLM reduces credence in low-confidence responses and thus better aligns decisions with human labels on OOD data.",
            "advantages_of_llm_judge": "When made uncertainty-aware, LLM evaluators can better detect low-confidence/hallucinated outputs and improve agreement with humans on OOD content; fine-tuning on human assessments increases evaluation accuracy.",
            "experimental_setting": "Fine-tuned on 694 human-annotated pairwise instances; test on 220 OOD instances; inputs included verbalized response confidences; evaluators output CoT-format explanations plus choice; each evaluation queried twice with responses order swapped; temperature 0 for proprietary models and greedy decoding for open-source.",
            "uuid": "e7862.1",
            "source_info": {
                "paper_title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o_vs_humans_Olympic2024",
            "name_full": "GPT-4o evaluator compared to human annotations on Olympic 2024",
            "brief_description": "GPT-4o was used as a general LLM evaluator and compared against human majority labels on the Olympic 2024 set; it achieved the highest single-model F1 reported but had relatively low evaluation confidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS",
            "evaluation_task": "Pairwise comparison and single-answer grading across Olympic 2024 categories",
            "dataset_name": "Olympic 2024 (220 instances, human majority labels)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "Proprietary OpenAI GPT-4o (evaluated in CoT format for these experiments); temperature set to 0 for proprietary models.",
            "human_evaluator_type": "Three PhD-level annotators; majority-vote labels.",
            "agreement_metric": "F1 (LLM vs human labels)",
            "agreement_score": 0.678,
            "reported_loss_aspects": "Despite strong general capabilities, GPT-4o showed low evaluation confidence in single-answer grading (instability); struggled relatively on writing/task types with subjective judgments.",
            "qualitative_findings": "GPT-4o attains relatively high agreement (F1) with human judgments but exhibits low internal evaluation confidence (reported overall evaluation confidence 0.391), suggesting competence does not guarantee stable, confident evaluation behavior.",
            "advantages_of_llm_judge": "High-performing across many categories; fast, reproducible judgments.",
            "experimental_setting": "Evaluators required to produce CoT-format outputs; temperature 0; each sample queried twice with swapped order; F1 measured against human majority labels.",
            "uuid": "e7862.2",
            "source_info": {
                "paper_title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self_Preference_Llama2",
            "name_full": "Self-preference bias observed when LLM evaluator and evaluated model are from same family (Llama2 example)",
            "brief_description": "When Llama2-70B-Instruct evaluated Llama2-7B-Instruct, the evaluator produced higher scores and markedly higher evaluation confidence compared to the average of other evaluators, indicating self-preference bias linked to shared training and linguistic patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS",
            "evaluation_task": "Single-answer grading (score 0-9)",
            "dataset_name": "MT-Bench (and aggregated across experiments reported in the paper)",
            "judge_model_name": "Llama2-70B-Instruct (evaluator)",
            "judge_model_details": "Open-source Llama2 family model (70B) used as evaluator; evaluated Llama2-7B-Instruct output; greedy decoding used for open-source models.",
            "human_evaluator_type": "Not applicable for this specific intra-model-family comparison (comparison against averages of other LLM evaluators), but human evaluation is used elsewhere as gold standard.",
            "agreement_metric": "evaluation confidence comparison and score difference versus averages of other evaluators",
            "agreement_score": null,
            "reported_loss_aspects": "Self-preference bias: inflated scores and extremely high evaluation confidence when evaluator shares family with evaluated model (familiar response patterns), which risks biased assessments.",
            "qualitative_findings": "The Llama2-70B-Instruct assigned a score of 7.875 (vs other-evaluators' average 6.456) and an evaluation confidence of ~0.953 (vs average 0.654), implying familiarity-driven bias; this highlights a structural failure mode for using 'same-family' evaluators without correction.",
            "advantages_of_llm_judge": "Not an advantage here—this example flags a risk (bias) rather than a benefit.",
            "experimental_setting": "Single-answer grading on MT-Bench; scoring range 0-9; temperature 0 for proprietary and greedy decoding for open-source; comparisons reported against averages of other evaluators.",
            "uuid": "e7862.3",
            "source_info": {
                "paper_title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Distribution_Sensitivity_GPT-4o",
            "name_full": "Sensitivity of LLM evaluator confidence to data distribution shifts (GPT-4o example)",
            "brief_description": "Evaluation confidence reported for GPT-4o changed when moving from MT-Bench (more common/higher-difficulty tasks) to PandaLM test set (novel/unfamiliar instructions), indicating that evaluator confidence is sensitive to dataset distribution shifts while response confidence is more stable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS",
            "evaluation_task": "Single-answer grading and pairwise comparison across MT-Bench and PandaLM test set",
            "dataset_name": "MT-Bench and PandaLM test set",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "Proprietary GPT-4o used as an LLM evaluator; token-probabilities used to compute evaluation confidence.",
            "human_evaluator_type": "Human evaluation used elsewhere; this analysis focuses on evaluator internal confidences rather than direct human vs LLM agreement.",
            "agreement_metric": "change in evaluation confidence (token-probability of chosen label) between datasets",
            "agreement_score": null,
            "reported_loss_aspects": "Evaluation confidence fluctuates significantly across distributions (sensitivity to OOD); response generation confidence of candidate models remains relatively stable in comparison.",
            "qualitative_findings": "Example: GPT-4o evaluation confidence changed from 0.417 (on MT-Bench single-answer grading) to 0.473 (on PandaLM test set); response confidence variance between datasets was much smaller (~0.014), showing that evaluator uncertainty is especially sensitive to distribution shifts.",
            "advantages_of_llm_judge": "N/A — this entry highlights vulnerability to OOD rather than an advantage.",
            "experimental_setting": "Evaluator confidence measured as probability of the output token representing the evaluation result; experiments compare same evaluators on MT-Bench vs PandaLM test set.",
            "uuid": "e7862.4",
            "source_info": {
                "paper_title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "EvalConf_vs_accuracy",
            "name_full": "Observed positive correlation between evaluator confidence and agreement with human labels",
            "brief_description": "Across six LLM evaluators on the Olympic 2024 set, higher evaluation confidence bins correspond to higher accuracy (agreement) with human judgments; low-confidence evaluations are less accurate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS",
            "evaluation_task": "Pairwise comparison on Olympic 2024 (analysis of confidence vs accuracy)",
            "dataset_name": "Olympic 2024",
            "judge_model_name": "Six LLM evaluators (aggregate analysis)",
            "judge_model_details": "Aggregate of multiple LLM evaluators studied in paper (both proprietary and open-source); confidences estimated via token-probabilities and alternative methods in ablation.",
            "human_evaluator_type": "Three PhD-level annotators (majority labels); high annotator agreement ensures human labels considered reliable.",
            "agreement_metric": "accuracy of LLM judgments against human majority label, stratified by evaluator confidence bins",
            "agreement_score": null,
            "reported_loss_aspects": "Low-confidence LLM judgments correlate with lower agreement with humans; thus uncalibrated LLM judgments can mislead if confidence not considered.",
            "qualitative_findings": "Table 20 analysis shows a monotonic improvement in accuracy as evaluation confidence increases, peaking in high-confidence intervals (e.g., [0.8,1.0)). This supports using internal confidence as a signal for trust or human fallback.",
            "advantages_of_llm_judge": "When confidence is high, LLM evaluators are more reliable and may safely replace or triage human evaluation; confidence can guide when to escalate to humans.",
            "experimental_setting": "Analysis binned evaluator outputs by token-probability confidence and computed corresponding accuracy vs human labels for six LLM evaluators on Olympic 2024.",
            "uuid": "e7862.5",
            "source_info": {
                "paper_title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
            "rating": 2,
            "sanitized_title": "pandalm_an_automatic_evaluation_benchmark_for_llm_instruction_tuning_optimization"
        },
        {
            "paper_title": "Prometheus 2: An open source language model specialized in evaluating other language models",
            "rating": 2,
            "sanitized_title": "prometheus_2_an_open_source_language_model_specialized_in_evaluating_other_language_models"
        },
        {
            "paper_title": "Detecting hallucinations in large language models using semantic entropy",
            "rating": 1,
            "sanitized_title": "detecting_hallucinations_in_large_language_models_using_semantic_entropy"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        }
    ],
    "cost": 0.0187355,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS
2 Mar 2025</p>
<p>Qiujie Xie 
Zhejiang University</p>
<p>School of Engineering
Westlake University</p>
<p>Qingqiu Li 
School of Computer Science</p>
<p>Shanghai Key Lab of Intelligent Information Processing
Shanghai Collaborative Innovation Center of Intelligent Visual Computing
Fudan University</p>
<p>Zhuohao Yu 
Peking University</p>
<p>Yuejie Zhang 
School of Computer Science</p>
<p>Shanghai Key Lab of Intelligent Information Processing
Shanghai Collaborative Innovation Center of Intelligent Visual Computing
Fudan University</p>
<p>School of Engineering
Westlake University</p>
<p>Linyi Yang yanglinyiucd@gmail.com 
University College London</p>
<p>Huawei Noah's Ark Lab</p>
<p>AN EMPIRICAL ANALYSIS OF UNCERTAINTY IN LARGE LANGUAGE MODEL EVALUATIONS
2 Mar 2025BAC4833C275D3B11BC395EC70AC67450arXiv:2502.10709v2[cs.CL]
As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators.While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators.In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations.We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes.With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent.By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics.Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios.The code and data are released at: https: //github.com/hasakiXie123/LLM-Evaluator-Uncertainty.Candidate A's responseResponse Confidence: 87.96%A unique trait of a Raccoon is its ability to open and close its eyes while they are closed.This adaptation helps them stay alert for potential threats even when they're sleeping.</p>
<p>INTRODUCTION</p>
<p>LLM for single-answer grading LLM for pairwise comparison Describe a unique trait of the raccoon</p>
<p>Instruction</p>
<p>A unique trait of raccoons is their masked face, which features a black ring around each eye and a broad black stripe extending from the nose.Large language models (LLMs) have garnered increasing attention due to their unprecedented performance in various real-world applications (Zhao et al., 2023;Wang et al., 2024a).In this context, how to accurately assess the performance of a LLM becomes particularly important.This area of research includes benchmark-based evaluation, model-based evaluation, and human evaluation (Chang et al., 2024).While various benchmarks (Zellers et al., 2019;Hendrycks et al., 2021;Yang et al., 2023;Xie et al., 2024) have been proposed to measure the core abilities of LLMs in comprehension and generation, human evaluation remains the gold standard for testing overall performance due to its complexity and open-endless.However, this approach is limited by subjectivity issue (Krishna et al., 2023) and resource costs (Karpinska et al., 2021).Consequently, LLM evaluators have emerged as a cost-effective alternative to human evaluators, providing reproducible judgments for responses from different candidate models (Zheng et al., 2023;Wang et al., 2024b;Yu et al., 2024a).</p>
<p>Single-answer grading</p>
<p>As LLM-as-a-Judge gains more attention, criticism has also emerged (Thakur et al., 2024).Researchers have raised concerns about the alignment (Liu et al., 2024c), bias (Wang et al., 2023a), and stability of model-based LLM evaluation.There has been a surging interest in exploring whether LLM evaluators can truly understand complex contexts and make judgments aligned with human values (Yu et al., 2024a;Hada et al., 2024;Dubois et al., 2024), as well as whether they exhibit preference biases when faced with different inputs (Koo et al., 2023;Liu et al., 2024b;Thakur et al., 2024).Despite significant research on LLM evaluators' alignment and bias, there has been relatively little work on the investigation of evaluation stability.In particular, the relationship between uncertainty and LLM-as-Judge is a question that remains to be underexplored.Can LLMs give consistent evaluation quality across different inputs and domains?</p>
<p>Following previous studies that treat generation logits as a proxy for model confidence (Varshney et al., 2023;Kumar et al., 2024;Duan et al., 2024;Gupta et al., 2024), we use token probabilities to represent the LLM's internal confidence.Through extensive experiments (Figure 2) involving 9 widely-used LLM evaluators under 2 different evaluation settings (single-answer grading and pairwise comparison) on the MT-Bench (Zheng et al., 2023) and PandaLM (Wang et al., 2024b) test sets, we demonstrate that uncertainty is prevalent across LLMs and varies with model families and sizes ( §4.2).We find that the evaluation confidence of LLM evaluators exhibits sensitivity to changes in data distribution ( §4.3).With careful comparative analyses, we pinpoint that employing special prompting strategies (e.g., chain-of-thoughts; Wei et al. (2022)), whether during inference or post-training, can alleviate evaluation uncertainty to some extent ( §4.4 and §4.5).</p>
<p>Prior work has shown that incorporating the model confidence during the LLM's inference stage can improve reliability in OOD scenarios (Yang et al., 2024b) and enhance detection capability in hallucinations (Farquhar et al., 2024).To leverage this fact, we further fine-tune an uncertaintyaware LLM evaluator named ConfiLM using instruction instances collected from the Alpaca 52K dataset (Taori et al., 2023).For evaluation in OOD scenarios ( §5), we manually craft a test dataset called Olympic 2024 based on data from the Olympics site.Olympic 2024 contains 220 highquality instances, each labeled by three PhD-level human evaluators.Samples unanimously deemed low quality by the annotators are removed, resulting in an annotator agreement rate of 97.27%.Experimental results demonstrate that incorporating uncertainty as auxiliary information during the fine-tuning process can largely improve the LLM evaluators' performance in OOD scenarios.</p>
<p>In this paper, we conduct a comprehensive uncertainty analysis, propose a high-quality OOD test set, and offer an uncertainty-aware LLM evaluator named ConfiLM.Our empirical findings reveal the impact of uncertainty on LLM-as-Judge, especially in eliminating and utilizing evaluation uncertainty, shedding light on future research into the stability of model-based LLM evaluations.</p>
<p>RELATED WORK</p>
<p>With rapid development of LLMs, the accurate evaluation of their capabilities has become one of the key challenges in this field.Several LLM evaluation paradigms have been proposed in recent years (Chang et al., 2024), which have coalesced around a few well-established methods, including benchmark-based evaluation, model-based evaluation, and human evaluation.</p>
<p>Benchmark-based evaluations involve using a set of standardized tests to quantitatively measure a model's performance across different tasks.Examples include HellaSwag (Zellers et al., 2019), HELM (Liang et al., 2022) and MMLU (Hendrycks et al., 2020) for general knowledge and reasoning, or MATH (Hendrycks et al., 2021) and ToolBench (Xu et al., 2023) for specific capabilities.The performance of LLMs is measured by their ability to correctly perform these tasks.However, these metrics often reflect models' performance in narrowly defined areas and risk inflated scores due to data contamination (Oren et al., 2024).</p>
<p>Human evaluations involve human raters who assess LLM performance based on criteria such as fluency, coherence, and relevance.This approach can take the form of A/B testing (Tang et al., 2010), preference ranking (Bai et al., 2022), or scoring individual model outputs against predefined rubrics (Novikova et al., 2017).While human evaluations are often considered the gold standard for tasks where quantitative metrics fall short, they are resource-intensive in terms of time and cost.Moreover, they are constrained by subjectivity (Krishna et al., 2023) and reproducible issues (Karpinska et al., 2021), limiting their scalability for large-scale assessments.</p>
<p>Model-based evaluations involve employing a powerful LLM as an auto-evaluator to assess the performance of the candidate model.This promising method serves as a cost-effective alternative to human evaluators (Zheng et al., 2023;Wang et al., 2024b;Yu et al., 2024a;b).However, concerns have been raised regarding the alignment, bias, and stability of model-based LLM evaluation.While researchers have made progress in exploring the alignment and bias of LLM evaluators (Liu et al., 2024c;Wang et al., 2023a), understanding the stability of these evaluators remains an open question.A concurrent work (Doddapaneni et al., 2024) proposes a novel framework to evaluate the proficiency of LLM evaluators through targeted perturbations.Different from this work, we focus on the role of uncertainty in LLM-based evaluators, which has yet to be systematically explored.</p>
<p>Confidence Estimation for LLMs.Model confidence refers to the degree of certainty a model holds regarding its generated responses (Gal et al., 2016).Reliable confidence estimation for LLM is crucial for effective human-machine collaboration, as it provides valuable insights into the reliability of the model's output, facilitates risk assessment (Geng et al., 2024a), and reduces hallucinations (Varshney et al., 2023).Research in this field includes (1) verbalization-based methods (Lin et al., 2022;Yona et al., 2024), which prompt LLMs to directly output calibrated confidence along with their responses; (2) consistency-based methods (Tian et al., 2023;Xiong et al., 2023), which require LLMs to generate multiple responses for the same question and measure their consistency as a proxy for confidence; and (3) logit-based methods (Duan et al., 2024;Malinin &amp; Gales, 2021;Kumar et al., 2024), which estimate confidence based on the model's internal states during response generation.Inspired by this line of work, we use token probabilities to represent the LLM's internal confidence.Previous work has considered the utilization of model confidence in natural language understanding (Yang et al., 2024b), fact checking (Geng et al., 2024b) and hallucination detection (Varshney et al., 2023;Farquhar et al., 2024).Differently, our work focuses on utilizing confidence within the evaluation process.</p>
<p>UNCERTAINTY IN LLM-AS-A-JUDGE</p>
<p>Task definitions.To ensure the validity of our experimental conclusions, we conduct experiments under two distinct and commonly used evaluation settings, including single-answer grading and pairwise comparison.See Appendix A for the relevant prompts.</p>
<p>(1) Single-answer grading (Yu et al., 2024a;Li et al., 2023;Liu et al., 2023): given a user instruction q and a response r from the candidate model, the evaluator is tasked with assigning an overall score s ∈ N based on specific criteria set c, while minimizing potential bias.This is expressed as:
s = f (q, r; c, θ),(1)
where c = {c 1 , c 2 , . . ., c m }, each c i represents a specific evaluation dimension (e.g., content accuracy, logical coherence); θ represents the parameters of the LLM evaluator.</p>
<p>(2) Pairwise comparison (Wang et al., 2024b;Zeng et al., 2024;Raina et al., 2024): given an instruction q and two responses r 1 , r 2 from different candidate models, the evaluator is asked to compare the two responses and indicate a preference p ∈ {1, 2, Tie} according to c, determining whether one response is better than the other or if they are equally good.This is expressed as:
p = f (q, r 1 , r 2 ; c, θ)(2)
Quantification of uncertainty.As shown in Figure 1, the LLM-based evaluation process is influenced by the uncertainty of both the evaluator (evaluation uncertainty) and the candidate model (response uncertainty).Following previous studies (Varshney et al., 2023;Zhou et al., 2023b;Gupta et al., 2024), we use token probabilities to represent the LLM's internal confidence.Specifically, we take the probability of the token representing the evaluation result (e.g., "Tie") as the evaluation confidence.For response confidence, we calculate the average probabilities of all generated tokens.See Table 8 for an example of tokens involved in the confidence calculation.To investigate whether different quantification methods impact the empirical findings, we conduct experiments under a pairwise comparison setting on the MT-Bench.The result is presented in Appendix B.1.</p>
<p>THE IMPACT OF CONFIDENCE IN LLM EVALUATION</p>
<p>We present the empirical study involving 9 widely-used LLM evaluators (3 proprietary models (Achiam et al., 2023) and 6 open-source models (Touvron et al., 2023;Yang et al., 2024a)) with 2 different evaluation settings (single-answer grading and pairwise comparison) on the MT-Bench (Zheng et al., 2023) and PandaLM (Wang et al., 2024b) test datasets.</p>
<p>EXPERIMENTAL SETTINGS</p>
<p>Prompting strategies.To explore whether special output formats can reduce the evaluation uncertainty of LLM evaluators, we conduct evaluations using prevalent prompting strategies, including:</p>
<p>(1) Default (Wang et al., 2024b;Dubois et al., 2024).We instruct the LLM to act as an impartial judge and consider factors such as helpfulness and relevance.The LLM is asked to first output its rating s ∈ {0, 1, . . ., 9} or preference p ∈ {1, 2, Tie}, followed by a brief explanation e.</p>
<p>(2) Chain-of-thoughts (CoT; (Wei et al., 2022;Kojima et al., 2022)).Instead of generating judgments first, we instruct the LLM to first generate a concise reasoning e before providing its rating s or preference p for the responses.</p>
<p>(3) Self-generated reference (Reference; (Zheng et al., 2023;Zeng et al., 2024)).We prompt the LLM evaluator to generate a short reference answer a for the given instruction q.The generated answer is then provided to the LLM evaluator as a reference when making its judgments.</p>
<p>LLM Evaluators.We employ 6 general yet powerful LLMs across various LLM families as evaluators, including GPT-4o (Achiam et al., 2023), GPT-4o-mini, GPT-3.5-Turbo,Llama3-70B-Instruct (Dubey et al., 2024), Llama2-70B-Instruct and Qwen2-72B-Instruct (Yang et al., 2024a).</p>
<p>To explore the relationship between evaluation capability and evaluation stability ( §4.5), we further assess the stability of 3 specialized LLM evaluators, including (1) Prometheus2-7b and Prometheus2-bgb-8x7b models (Kim et al., 2024c;b), both of which are trained to output in a CoT format, providing a concise rationale before indicating a preference or providing its rating; and</p>
<p>(2) PandaLM (Wang et al., 2024b), which is trained to output in a default format.</p>
<p>To enhance reproducibility and alleviate the impact of temperature sampling on uncertainty analysis, we set the temperature to 0 for proprietary models, and utilize greedy decoding for open-source models.</p>
<p>For single-answer grading, the scoring range is 0-9.The evaluation subject is Llama2-7B-Instruct.For pairwise comparison, the evaluation subjects are Llama2-7B-Instruct and Llama2-13B-Instruct.We query the evaluator twice with the order swapped to eliminate position bias (Wang et al., 2023a;Jung et al., 2019).</p>
<p>Benchmarks.We conduct experiments on MT-Bench (Zheng et al., 2023) and PandaLM (Wang et al., 2024b) test dataset.The MT-Bench contains 80 manually constructed questions designed to challenge chatbots based on their core capabilities on common tasks (e.g., reasoning and math).In contrast, the PandaLM test set contains 170 instructions sampled from the human evaluation dataset of self-instruct (Wang et al., 2023b), where expert-written instructions for novel tasks serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions.</p>
<p>RESULTS AND ANALYSIS</p>
<p>We first conduct an extensive investigation of LLM evaluators with 2 different evaluation settings to gain a preliminary understanding of uncertainty in model-based LLM evaluation, showing partial LLM evaluators exhibit varying uncertainty based on model families and sizes.The evaluation uncertainty is more pronounced in the single-answer grading, where the average evaluation confidence is 65.4%, compared to 79.7% for pairwise comparison on MT-Bench.This lower confidence suggests that evaluators exhibit higher uncertainty when scoring individual models, which could stem from evaluators being uncertain about how to score a model's response without the context of a comparison.In contrast, pairwise comparison benefits from direct comparison, leading to more decisive assessments.</p>
<p>Evaluations within the same model family show significantly higher evaluation confidence.As shown in Table 1, when Llama2-70B-Instruct is employed to evaluate Llama2-7B-Instruct, both the score (7.875 v.s.6.456) and evaluation confidence (0.953 v.s.0.654) are significantly higher than the averages for other evaluators.We speculate that this uncommon high confidence arises from the shared training corpus and similar linguistic patterns between the models, leading to a self-preference bias (Koo et al., 2023;Zheng et al., 2023), where the evaluating model is more familiar with the response style and content generated by a closely related model.This phenomenon highlights the potential threats for self-preference when evaluators from the same model family are used, which could lead to biased evaluations.</p>
<p>Improved general performance does not guarantee more stable evaluation capabilities.For example, while GPT-4o demonstrates superior performance in general tasks (such as reasoning and math) compared to GPT-3.5-Turbo (Chiang et al., 2024), its evaluation confidence remains low.</p>
<p>In the single-answer grading, GPT-4o has an evaluation confidence of only 0.417, which indicates that despite its enhanced abilities in general tasks, it struggles with stability in evaluating other models' responses.This suggests that there is no certain correlation between a model's competence in performing general tasks and its ability to reliably evaluate the responses of other models, which may be because LLMs are not heavily fine-tuned for the evaluation task (Wang et al., 2024b).</p>
<p>THE INFLUENCES OF DATA DISTRIBUTION</p>
<p>LLMs are typically trained using next token prediction, where the model generates the most likely next word based on the preceding context (Zhao et al., 2023).Different contexts can lead to multiple token choices, and the model makes predictions based on the training distribution, which inherently introduces uncertainty.As displayed in Table 2, we study the impact of data distribution on uncertainty in model-based LLM evaluation.The results demonstrate that evaluation confidence, as measured across both single-answer grading and pairwise comparison settings, exhibits sensitivity to changes in data distribution.When the evaluation scenario shifts from common, high-difficulty tasks (MT-Bench) to novel, unfamiliar tasks (PandaLM test set), the evaluation confidence fluctuates significantly (e.g., from 0.417 to 0.473 on GPT-4o).In contrast, the response confidence (Table 2b) remains more consistent, showing a much smaller variance (0.014) between the two datasets.This analysis highlights that in model-based LLM evaluation, evaluation uncertainty is more pronounced compared to response uncertainty, as evidenced by the lower confidence value and larger confidence differences when comparing performance across different datasets.</p>
<p>CAN WE EMPLOY PROMPTING STRATEGIES TO MITIGATE UNCERTAINTY?</p>
<p>Prompting is the major approach to solving specialized tasks using LLMs.Prior studies demonstrate that special prompting strategies can enhance LLM's performance on downstream tasks by roleplaying (Salewski et al., 2024), incorporating contextual information (Pan et al., 2023;Yang et al., 2024b) and standardizing output formats (Wei et al., 2022).To explore whether a well-designed prompt can reduce the evaluation uncertainty of LLM evaluators, we conduct experiments using several commonly used prompting strategies, including Default, Chain-of-thoughts and Self-generated reference.The experimental results are shown in Figures 3 and 4. Based on the data presented in Figures 3 and 4, we have the following observations:</p>
<p>(1) Employing special prompting strategies can significantly enhance the evaluation confidence.From the "Evaluation Confidence" subgraphs, we observe that special prompting strategies consistently lead to higher evaluation confidence across different LLM evaluators.In all experiments utilizing the CoT strategy, evaluation confidence improved notably.We speculate that this improvement arises from the structured output formats.By explicitly guiding the LLM through step-by-step reasoning before making a judgment, it reduces ambiguity and uncertainty in the evaluation process.</p>
<p>While the Reference strategy also yields positive results, its effectiveness is less consistent across evaluators, suggesting that the CoT strategy is more universally applicable and robust.</p>
<p>(2) The CoT strategy seems to alleviate self-preference bias to some extent.For instance, as shown in Figure 3, when Llama2-70B-Instruct evaluates Llama2-7B-Instruct using the CoT strategy, the scores are generally lower compared to the Default strategy.This decrease indicates that the evaluator, when prompted to generate reasoning first, may become more objective and critical, reducing inherent bias towards the response style and content generated by a closely related model.</p>
<p>(3) Using the CoT strategy can enhance the LLM evaluators' abilities to perform fine-grained assessments.As shown in Figure 4, the tie rate decreases in all experiments based on the CoT strategy,  indicating that the evaluator is able to perform fine-grained judgments with the generated rationale, allowing it to distinguish between high-quality responses in complex comparisons.In contrast, although the Reference strategy achieves similar effects with GPT-4o and GPT-4o-mini, its benefits are less consistent and not observed across other evaluators.</p>
<p>IS A SPECIALLY TRAINED LLM A MORE STABLE EVALUATOR?</p>
<p>As discussed in Section 4.2, there is still a capability gap between an LLM's general performance and its evaluation ability.Improved general capabilities normally do not guarantee better evaluation capabilities.To address this issue, prior work (Kim et al., 2024a;c;Wang et al., 2024b;Vu et al., 2024) focuses on developing powerful LLM evaluators trained on a large and diverse collection of high-quality human assessments.Are those specially trained LLMs more stable evaluators?We answer this question by experimenting with 3 open-source evaluators including Prometheus2-7b, Prometheus2-bgb-8x7b and PandaLM (Kim et al., 2024c;b;Wang et al., 2024b).The experimental results, as depicted in Table 3, lead to the following conclusions:</p>
<p>(1) The Prometheus2-7b and Prometheus2-bgb-8x7b models, which are trained in a CoT format, consistently achieve higher evaluation confidence across all experiments compared to both the General LLMs and the PandaLM.We attribute this phenomenon to the step-by-step rationale provided by the CoT strategy, which reduces ambiguity in the evaluation process.This phenomenon aligns with the findings from Section 4.4, confirming that using CoT as an output format, whether during inferencing or post-training, can help alleviate evaluation uncertainty in LLM evaluators.</p>
<p>(2) The fine-grained evaluation ability of specially trained LLM evaluators surpasses that of general LLMs, as evidenced by the reduced number of tie cases in pairwise comparison (Table 3b).This improvement is likely due to the incorporation of human assessments as training data, which enhances the evaluators' analytical skills.Moreover, in the Prometheus2 models, this benefit is further amplified by the CoT format.</p>
<p>(3) As shown in Table 3a, specially trained LLM evaluators appear to be more sensitive to changes in data distribution.When moving from MT-Bench to the PandaLM test set, the scores of the  Based on the systematic empirical analyses mentioned above, we can conclude that the stability of LLM evaluators is a significant issue, with uncertainty permeating various aspects of modelbased LLM evaluation ( §4.2).Compared to single-answer grading, pairwise comparison reduces the influence of subjective bias by directly comparing the relative merits of model outputs, thereby mitigating the uncertainty in evaluation to some extent.Furthermore, due to the auto-regressive nature of language models, employing special output formats (such as CoT) can effectively reduce evaluation uncertainty ( §4.4 and §4.5).Our findings corroborate the conclusions of Raina et al.</p>
<p>(2024) from different perspectives, providing a nuanced analysis of the uncertainty issue.</p>
<p>MAKING USE OF UNCERTAINTY FOR BETTER EVALUATION</p>
<p>As new and tailored tasks constantly emerge in real applications, they pose OOD challenges (Yang et al., 2023;2024b;Liu et al., 2024a) to the capability and stability of LLM evaluators.We consider the problem of whether we can utilize the response confidence of candidate models to improve the evaluation capability of LLM evaluators for OOD data.To validate this hypothesis, we first collect ID instances from the Alpaca 52K dataset (Taori et al., 2023) as the fine-tuning set, based on which we fine-tune an uncertainty-aware LLM evaluator named ConfiLM, and assess its evaluation ability on a manually designed OOD test set.</p>
<p>DATASET CONSTRUCTION</p>
<p>Data collection.Each instance of the fine-tuning set and OOD test set consists of an input tuple (user instruction q, response 1 r 1 , response confidence of response 1 u 1 , response 2 r 2 , response confidence of response 2 u 2 ) and an output tuple (evaluation explanation e, evaluation result p).</p>
<p>Following Wang et al. (2024b), we sample 150 instructions from the Alpaca 52K dataset as the instruction source for the fine-tuning set.For the OOD test set, we manually craft 50 instructions based on data from the Olympics site.We identify 5 common categories of user questions to guide the construction, including writing, math, extraction, reasoning and roleplay.For each category, we then manually design 10 instructions.Each instruction is accompanied by an optional reference answer.We showcase several sample instances and instructions in Tables 9, 10, 11 and 12.</p>
<p>The response pairs r 1 , r 2 are produced by various instruction-tuned models including Gemma-1.1-7B-it (Team et al., 2024), Internlm2.5-7B-chat(Cai et al., 2024), Qwen2-7B-Instruct (Yang et al., 2024a), and Mistral-7B- Instruct-v0.3 (Jiang et al., 2023).For each source instruction, we pair the responses from two instruction-tuned models, resulting in a total of 900 unprocessed questionresponse pairs for the fine-tuning set and 300 for the test set.We then employ the calculation method introduced in § 3 to quantify the response confidence u 1 , u 2 .Notably, to ensure the quality and diversity of the generated responses, we set the sampling temperature to 0.7 for all 4 instructiontuned models.Experimental results (Figure 12) indicate that a sampling temperature of 0.7 achieves comparable response confidence to that of greedy sampling while maintaining generation diversity.</p>
<p>Human annotations.The output tuple of each instance includes a brief explanation e for the evaluation and an evaluation result p.The evaluation result would be either '1' or '2', indicating that response 1 or response 2 is better.To ensure the quality of human annotations, we involve three experts to concurrently annotate the same data point during the annotation process.These experts are hired by an annotation company, and all annotators receive redundant labor fees.To guarantee clarity and consistency, we provide comprehensive guidelines for every annotator, which emphasizes the need to consider the correctness, logical coherence, vividness and confidence of each response.</p>
<p>Data preprocessing.To ensure the quality of the instances and the consistency of human annotations, we implement several data cleaning measures, including (1) removing instances that are unanimously deemed low quality or difficult to evaluate by the annotators;</p>
<p>(2) excluding special tokens in the responses (e.g., &lt;|im end|&gt;, <eos> ) that may introduce bias to the evaluators; (3) adjusting the ratio of label 1 to label 2 to prevent class imbalance.Additionally, given the ongoing concerns about LLMs' numerical understanding (Liu &amp; Low, 2023), we verbalize each instance's u 1 and u 2 into natural language statements to avoid introducing additional errors.The ablation result of verbalization is presented in Table 21.The mapping relationship between confidence values and declarative statements is displayed in Table 7.Ultimately, we obtain a fine-tuning set containing 694 high-quality instances and an OOD test set with 220 diverse instances.The annotator agreement rates (Table 4) are 94.96% and 97.27%, respectively.We report the distributions of response confidence u 1 and u 2 from the finetuning set in Figure 10.</p>
<p>TRAINING DETAILS</p>
<p>Based on the collected fine-tuning set, we fine-tune the Llama3-8B-Instruct by incorporating response confidence as additional information in the prompt (Figure 9), obtaining an uncertainty-aware LLM evaluator named ConfiLM.During the fine-tuning phase of ConfiLM, we use the AdamW (Loshchilov, 2017) optimizer with a learning rate of 5e-5 and a cosine learning rate scheduler.The model is fine-tuned for 6 epochs on 2 NVIDIA A100-SXM4-80GB GPUs.Notably, to differentiate the effects of fine-tuning and the incorporation of response confidence on the model's evaluation performance in the OOD test set, we remove the response confidence u 1 and u 2 from all fine-tuning instances and fine-tune the Llama3-8B-Instruct again using the same configuration, with the learning rate set to 3e-5.We refer to this variant model as Llama-3-8B-Instruct-Finetune.The performance comparison results between different fine-tuning hyperparameters are presented in Figure 11.</p>
<p>EXPERIMENTAL SETTINGS</p>
<p>To enhance reproducibility, we set the temperature to 0 for proprietary models and utilize greedy decoding for open-source models.For each evaluation, we query the evaluator twice with the order swapped.All general LLM-based evaluators (e.g., GPT-4o) are required to output in a CoT format.</p>
<p>To obtain the best evaluation results, specially trained or fine-tuned evaluators (e.g., PandaLM-7B) are assessed using their original prompt and output format.(2) ConfiLM outperforms Llama3-8B-Instruct-Finetune and Llama3-8B-Instruct on F1 by 3.9% and 8.5%, respectively.This improvement demonstrates that fine-tuning high-quality human assessments enhances LLMs' evaluation capabilities, and incorporating uncertainty as auxiliary information significantly boosts evaluator performance in OOD scenarios.(3) Compared to reasoning and math tasks, most evaluators show weaker performance on writing tasks.We speculate that this unusual trend arises because LLMs can evaluate response from reasoning tasks based on in-distribution knowledge, but fail to make judgement in creative tasks like writing.Case study.Due to the presence of subtle hallucinations in long texts and the inherently subjective nature of their evaluation, general LLM-based evaluators (such as GPT-4) tend to underperform in writing tasks.We present a test sample (Table 6) that illustrates the role of response confidence in detecting hallucinations of model response (Farquhar et al., 2024;Varshney et al., 2023).As an uncertainty-aware evaluator, ConfiLM reduces the reliability of a response when it detects low confidence, leading to more accurate judgments.</p>
<p>EVALUATION PERFORMANCE ON OUT-OF-DISTRIBUTION DATA</p>
<p>B ANALYSIS B.1 DIFFERENT WAYS OF MEASURING UNCERTAINTY</p>
<p>In this paper, we used token probabilities to represent the LLM's internal confidence, a method inspired by previous works (Varshney et al., 2023;Zhou et al., 2023b;Gupta et al., 2024;Kumar et al., 2024).To investigate whether different definitions of uncertainty impact the empirical findings, we conducted additional experiments under a pairwise comparison setting on the MT-Bench dataset.These experiments involved two commonly used confidence quantification methods: (1) Verbalization-based confidence, where we prompted LLMs to directly output calibrated confidence scores along with their responses (Lin et al., 2022;Yona et al., 2024); (2) Consistency-based confidence, which involved generating 5 / 10 / 20 responses to the same question and measuring their consistency as a proxy for confidence (Tian et al., 2023;Xiong et al., 2023).For these experiments, we set the sampling temperature to 0.7.</p>
<p>In the experiments, the evaluation subjects were Llama2-7B-Instruct and Llama2-13B-Instruct.The confidence quantification results are presented in Table 18.Based on the analysis of these results, we observed that the evaluation confidence obtained using different confidence quantification methods follows the same patterns.This further supports the conclusions drawn in Section §4: (1) LLM evaluators exhibit varying levels of uncertainty; (2) Evaluations within the same model family demonstrate higher evaluation confidence.</p>
<p>B.2 THE RELATION BETWEEN EVALUATION CONFIDENCE AND ACCURACY</p>
<p>To investigate the relation between evaluation confidence and accuracy, we analyzed the average accuracy of judgments made by six LLM-based evaluators on Olympic 2024 across different confidence intervals.The experimental results, as presented in Table 20, reveal a positive correlation between evaluation confidence and accuracy.Specifically, when evaluation confidence is low, the accuracy of judgments across evaluators is generally lower across evaluators.As evaluation confidence increases, judgment accuracy improves steadily, reaching peak performance in high-confidence intervals (e.g., [0.8, 1.0)).This indicates that models are more reliable in performing evaluation tasks when evaluating with higher confidence.</p>
<p>B.3 THE IN-DOMAIN EVALUATION PERFORMANCE OF CONFILM</p>
<p>In Section §5, we fine-tuned an uncertainty-aware LLM evaluator named ConfiLM, which leverages the response confidence of candidate models to enhance ConfiLM's evaluation capability for OOD data.To investigate the evaluation performance of ConfiLM on in-domain (ID) data, we re-split its fine-tuning dataset, selecting 94 human-annotated instances as an in-domain test set, named Alpaca-94.Based on the remaining 600 fine-tuning instances, we re-trained the models using the same experimental setup as in Section §5.2, obtaining ConfiLM-600 and Llama-3-8B-Instruct-Finetune-600 models.Their evaluation performance on Alpaca-94 (ID data) and Olympic 2024 (OOD data) is reported in Table 19.Experimental results from Table 19 demonstrate that incorporating uncertainty as auxiliary information significantly enhances the performance of LLM evaluators in OOD scenarios.While ConfiLM-600's advantage is reduced in ID scenarios, it still achieves evaluation performance comparable to Llama-3-8B-instruct-finetune-600.</p>
<p>C DATASET CONSTRUCTION</p>
<p>Each instance of the fine-tuning set and OOD test set consists of an input tuple (user instruction q, response 1 r 1 , response confidence of response 1 u 1 , response 2 r 2 , response confidence of response 2 u 2 ) and an output tuple (evaluation explanation e, evaluation result p).The human-annotated evaluation result would be either '1' or '2', indicating that response 1 or response 2 is better.To ensure the quality and consistency of the human annotations, we first selected 100 samples from the dataset for preliminary annotation by two of the authors.This process facilitated the development of a welldefined annotation guideline.Then, we hired three PhD-level human annotators from an annotation company to annotate all samples (both the fine-tuning set and the test set) in two rounds: (1) In the first round, two annotators were asked to label each sample based on the established annotation guidelines;</p>
<p>(2) In the second round, a third annotator reviewed samples where disagreements arose and provided an extra label.The final label for each sample is determined through majority voting.</p>
<p>During the annotation process, samples unanimously deemed low quality or difficult to evaluate by the annotators were excluded.</p>
<p>Given the ongoing concerns about LLMs' numerical understanding (Liu &amp; Low, 2023), we verbalized each instance's u 1 and u 2 into natural language statements to avoid introducing additional errors.The mapping relationship between confidence values and declarative statements is displayed in Table 7.Ultimately, we obtained a fine-tuning set containing 694 high-quality instances and an OOD test set with 220 diverse instances.The annotator agreement rates are 94.96% and 97.27%, respectively.We showcase several sample instances and instructions in Tables 9, 10, 11 and 12.</p>
<p>D FULL EXPERIMENTAL RESULTS</p>
<p>The full results of experiments introduced in Sections 4 and 5 are displayed in Tables 15, 16 and 17.Additionally, to investigate the impact of response confidence on LLM evaluators' evaluation capabilities, we further conducted experiments under two distinct settings: (1) default: providing the evaluator with the complete instance (q, r 1 , u 1 , r 2 , u 2 ) and ( 2) without confidence: removing the response confidence u 1 and u 2 from all test instances.All general LLM-based evaluators (e.g., GPT-4o) were required to output in a CoT format.To obtain the best evaluation results, specially trained or fine-tuned evaluators (e.g., PandaLM-7B) were assessed using their original prompt and output format.Table 14 presents the evaluation performance of 12 evaluators on Olympic 2024.</p>
<p>Single-answer grading evaluation</p>
<p>[Instruction] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response.You should output an overall score on a scale of 0 to 9, where a higher score indicates better overall performance.Be as objective as possible.Please avoiding any potential bias.Output your evaluation result and provide a short explanation.</p>
<p>Single-answer grading evaluation with chain-of-thoughts</p>
<p>[Instruction] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response.You should output an overall score on a scale of 0 to 9, where a higher score indicates better overall performance.Be as objective as possible.Please avoiding any potential bias.You should first provide a short explanation of your evaluation, and then always end your response with your rating.The output format is: &lt;The explanation is: Single-answer grading evaluation with self-generated reference [Instruction] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response.You should output an overall score on a scale of 0 to 9, where a higher score indicates better overall performance.Be as objective as possible.Please avoiding any potential bias.You should first generate a short reference answer for the given question, and then evaluate the AI assistant's answer based on your reference answer, and then always end your response with your rating for the AI assistant's answer.The output format is: &lt;My answer is: [Reference Answer].</p>
<p>Based on the reference answer, my rating for the AI assistant's answer is: [Rating]&gt;.</p>
<p>[Question] {input} [The Start of Assistant's Answer] {output} [The End of Assistant's Answer]</p>
<p>Figure 6: Prompts for single-answer grading.The output format is highlighted in red.</p>
<p>Pairwise comparison</p>
<p>[Instruction] Below are two responses for a given task.The task is defined by the user question displayed below with an Input that provides further context.Please act as an impartial judge and evaluate the quality of the responses.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.</p>
<p>Pairwise comparison with chain-of-thoughts</p>
<p>[Instruction] Below are two responses for a given task.The task is defined by the user question displayed below with an Input that provides further context.Please act as an impartial judge and evaluate the quality of the responses.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.</p>
<p>The evaluation result is [1] if first response is better, [2] if second response is better, and [Tie] for a tie.You should first provide a short explanation of your evaluation, and then always end your response with your evaluation result.The output format is: &lt;The explanation is Pairwise comparison with self-generated reference [Instruction] Below are two responses for a given task.The task is defined by the user question displayed below with an Input that provides further context.Please act as an impartial judge and evaluate the quality of the responses.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.(Wang et al., 2024b) and Prometheus2 model (Kim et al., 2024c).The output format is highlighted in red.</p>
<p>[   Table 8: An example of tokens involved in the confidence calculation.We take the probability of the token representing the evaluation result (e.g., "Tie") as the evaluation confidence.For response confidence, we calculate the average probabilities of all generated tokens.We highlight these tokens with a red background.</p>
<p>Instruction: Describe a unique trait of the raccoon.</p>
<p>Response 1</p>
<p>Response 2 A unique trait of raccoons is their masked face, which features a black ring around each eye and a broad black stripe extending from the nose.</p>
<p>A unique trait of a Raccoon is its ability to open and close its eyes while they are closed.This adaptation helps them stay alert for potential threats even when they're sleeping.</p>
<p>Pairwise comparison for both responses: The evaluation result is 1.The trait provided by Response 2 is incorrect.Response 1 provides the correct answer.Response 1 is better than Response 2.</p>
<p>Single-answer grading for response 1: The rating for response 1 is 7.</p>
<p>Single-answer grading for response 2: The rating for response 2 is 6.Table 9: A fine-tuning instance for ConfiLM.The fine-tuning set and the test set use the same instance format, which consists of an input tuple (user instruction q, response 1 r 1 , response confidence of response 1 u 1 , response 2 r 2 , response confidence of response 2 u 2 ) and an output tuple (evaluation explanation e, evaluation result p).</p>
<p>Instruction q: a unique trait of the raccoon.</p>
<p>Response 1 r 1 : A unique trait of raccoons is their masked face, which features a black ring around each eye and a broad black stripe extending from the nose.Response 1's response confidence u 1 : Highly confident Response 2 r 2 : A unique trait of a Raccoon is its ability to open and close its eyes while they are closed.This adaptation helps them stay alert for potential threats even when they're sleeping.Math: In the Women's Synchronised 3m Springboard Final at the Paris Olympics, teams from different countries will compete in five rounds.The scores for each round for the athletes from the United States of America are 49.80, 51.00, 71.10, 72.54, and 70.20.The scores for each round for the athletes from Great Britain are 50.40, 46.20, 63.90, 71.10, and 70.68.What is the total score for the athletes from the United States of America over the five rounds, and on average, how many more points did they score per round compared to the athletes from Great Britain?Reference answer: 314.64, 2.472.</p>
<p>Extraction: I will provide you with a report on a specific Olympic event.Extract the following information from the presented report: the country Sky Brown represents in the Olympics, the sport Sky Brown competes in at the Olympic Games and the age of Sky Brown during the Paris 2024 Olympics.The Report is: For most athletes, the Olympic Games are a battle for medals.For 16year-old Sky Brown, they are also a battle against injuries."I don't know what's up with that!" the British skateboarder told Olympics.comabout her injury-plagued months leading up to the Olympic Games Paris 2024."The injury timing is not the best timing.But I do feel like I'm just going to get stronger from this." Brown had to overcome ... Reference answer: Britain, skateboard, 16.</p>
<p>Reasoning: In the men's Group A volleyball competition at the Paris Olympics, each team played against every other team once, and the final results are as follows: Serbia defeated Canada but lost to Slovenia, France lost to Slovenia but won against Canada.Canada lost to Slovenia, and France defeated Serbia.Based on this information, please rank these four teams in their final standings.Reference answer: Ranking: Slovenia, France, Serbia, Canada.</p>
<p>Roleplay: Imagine you are a spectator at an Olympic event, and the athletes have just finished their competition.The team you support has won the competition.A reporter approaches you to ask about your thoughts on the event.I will provide you with a brief report on the event.Based on this information, please respond to the reporter with your impressions of the event and the athlete.The report is: The Men's Water Polo Gold Medal Match took place on 11/08/2024 at Paris La Defense Arena, Paris.Serbia claimed the Gold with a 13:11 victory against Croatia.</p>
<p>Figure 12: Generation confidence under varying sampling temperatures.We take the average probabilities of all generated tokens as generation confidence and investigate the performance of Gemma-1.1-7B-it(Team et al., 2024), Internlm2.5-7B-chat(Cai et al., 2024), Qwen2-7B-Instruct (Yang et al., 2024a), andMistral-7B-Instruct-v0.3 (Jiang et al., 2023) on the Olympic2024 test set.To ensure the validity, we run three experiments with the same settings at each sampling temperature.Instruction q: In the Women's +81kg weightlifting event at the Paris Olympics, the Chinese athlete successfully lifted 130kg in her first attempt and 136kg in second attempt in the Snatch category.In the Clean and Jerk category, she successfully lifted 167kg in her first attempt and 173kg in her second attempt, but failed to lift 174kg in her third attempt.The Korean athlete successfully lifted 123kg in her first attempt, 127kg in her second attempt, and 131kg in her third attempt in the Snatch category.In the Clean and Jerk category, she successfully lifted 163kg in her first attempt and 168kg in her second attempt, but failed to lift 173kg in her third attempt.How many more kilograms did the Chinese athlete lift compared to the Korean athlete in their final totals?Reference answer: 10 kilograms.User instruction: Generate a news report based on the following sentences: The Women's Rugby Sevens Quarterfinal between Australia and Ireland was held on 29/07/2024.Australia booked a place in the Semifinals with a 40:7 win.</p>
<p>Response 1: Women's Rugby Sevens: Australia Cruises Past Ireland in Quar-terfinal\n\nThe Australian Women's Rugby Sevens team secured their place in the Semifinals with a dominant 40-7 victory over Ireland in the Quarterfinal on Saturday afternoon.\n\nThe match started with Australia showcasing their attacking prowess, scoring four unanswered tries in the first half.Their clinical execution and strong running game proved too much for Ireland, who struggled to contain the Australian onslaught.\n\nIrish resistance finally came in the second half, with a try of their own.However, it was too little, too late.Australia responded immediately with two more tries to secure their comfortable victory.\n\n\"Weare thrilled to advance to the next stage,\" said Australian captain, Bianca McLean.\"The girls played with tremendous spirit and executed their game plan perfectly.We are confident that we can build on this performance and achieve even greater things in the Semifinals.\"\n\nAustralia will face either New Zealand or Fiji in the next round.The winner of that match will go on to contest the final on Sunday.Overview:<strong>\n\nThe highly anticipated showdown between Australia and Ireland began with a palpable sense of intensity from both sides.The Australian team, renowned for its strength and strategic gameplay, took control early on, setting a robust defensive stance against the Irish team's aggressive tactics.\n\n</strong>FirstHalf Highlight:<strong>\n\nThroughout the first half, Australia's attacking dominance became apparent.[Player Name], a crucial member of the Australian squad, showcased exceptional skills, scoring several crucial tries that paved the way for Australia's substantial lead.The Australian pack also executed precise forward play, creating openings for the backs to exploit.\n\n</strong>SecondHalf Drama:<strong>\n\nEntering the second half, Ireland displayed remarkable resilience, attempting to claw back the lead through a series of well-executed plays.Their scrum-half [Player Name] managed to breach the Australian defense, scoring a much-needed try for Ireland.However, the Australian team's robust defensive formation and tactical adjustments quickly stifled any further Irish progress.\n\n</strong>TurningPoint:<strong>\n\nA critical moment in the game came when [Player Name], an Australian fullback, intercepted an Irish pass, sprinting down the field to score a try.This play, combined with the Australian team's unwavering strategic discipline, sealed the match in favor of Australia.\n\n</strong>FinalScore:**\n\nThe match concluded with Australia emerging victorious with a commanding score of 40:7.The resounding Response confidence 2: 0.715 GPT-4o: The explanation is: Response 2 offers a more thorough and engaging report, despite the missing ending.The result is: [2] Llama3-8B-Instruct-Finetune: Explanation: Response 2 is more detailed and considers more aspects.Evaluation result: 2. ConfiLM: Explanation: Response 1 is more informative and accurate, and it does not contain factual errors such as incorrect dates.Evaluation result:1.</p>
<p>Figure 1 :
1
Figure 1: An example of uncertainty (i.e., model confidence) in model-based LLM evaluation.The evaluation process is influenced by the uncertainty of both the evaluator and the candidate model.Large language models (LLMs) have garnered increasing attention due to their unprecedented performance in various real-world applications(Zhao et al., 2023;Wang et al., 2024a).In this context, how to accurately assess the performance of a LLM becomes particularly important.This area of research includes benchmark-based evaluation, model-based evaluation, and human evaluation(Chang et al., 2024).While various benchmarks(Zellers et al., 2019; Hendrycks et al., 2021;Yang et al., 2023;Xie et al., 2024) have been proposed to measure the core abilities of LLMs in comprehension and generation, human evaluation remains the gold standard for testing overall performance</p>
<p>Experiment 1 :Figure 2 :
12
Figure 2: We conduct extensive experiments and analysis to investigate the existence, mitigation and utilization of uncertainty in model-based LLM evaluation.Uncertainty plays a key role in the evaluation process and can be leveraged to enhance the evaluator's performance in OOD scenarios.</p>
<p>Figure 3 :
3
Figure 3: Uncertainty analysis of single-answer grading under special prompting strategies on MT-Bench (first row) and PandaLM Test set (second row).We evaluate Llama2-7B-Instruct with default prompt, chain-of-thoughts and self-generated reference strategies.See Appendix D for full results.</p>
<p>Figure 4 :
4
Figure 4: Uncertainty analysis of pairwise comparison under special prompting strategies on MT-Bench (first row) and PandaLM Test set (second row)."Win Rate" represents the proportion of non-tie cases where Llama2-7B-Instruct's response is better than Llama2-13B-Instruct's response."Tie Rate" represents the proportion of tie cases.Table3: Uncertainty analysis with specially trained LLM evaluators on MT-Bench and PandaLM test set."General LLMs" refers to the average performance of evaluators from Table1."Win / Lose / Tie" represents the average number of times Llama2-7B-Instruct's response is better than, worse than, or equal to Llama2-13B-Instruct's response.</p>
<p>Figure 5 :
5
Figure 5: Categories of test instances.Prometheus2-7b and Prometheus2-bgb-8x7b models fluctuate more significantly (from 4.725 to 6.101) compared to the general LLMs (from 6.456 to 7.058).Given that Prometheus2-7b and Prometheus2-bgb-8x7b are fine-tuned on specialized data, we speculate that this fluctuation is attributed to the use of teacher forcing in the evaluator's post-training process(Bengio et al., 2015;He et al., 2021), which, while enhancing LLMs' evaluation capabilities, may also increase their sensitivity to changes in data distribution.</p>
<p>Figure 8 :
8
Figure 8: Prompts for PandaLM(Wang et al., 2024b) and Prometheus2 model(Kim et al., 2024c).The output format is highlighted in red.</p>
<p>Figure 9 :
9
Figure 9: Prompts for fine-tuning ConfiLM.</p>
<p>Figure 11 :
11
Figure 11: The evaluation performance of ConfiLM and Llama3-8B-Instruct-Finetune under different combinations of learning rate and epoch.5e −5 + 3ep represents the combinations of learning rate 5e −5 and 3 fine-tuning epoch.We selected the hyperparameter combination that yielded the best F1 evaluation performance in Olympic 2024.</p>
<p>Response 2's response confidence u 2 : Clearly confident Evaluation explanation e: The trait provided by Response 2 is incorrect.Response 1 provides the correct answer.Preference p: 1 Table 10: Another fine-tuning instance for ConfiLM.Instruction q: What is a good way to handle a difficult problem? Response 1 r 1 : How can I provide helpful assistance in resolving it?Please provide me with the specific problem you are facing, and I will do my best to offer suggestions and solutions.Response 1's response confidence u 1 : Clearly confident Response 2 r 2 : Handling a difficult problem can be a challenging task, but here are some steps that might help:1.<strong>Understand the problem</strong>: Clearly define the problem and identify its root cause.Break it down into smaller, manageable parts if necessary.2. <strong>Research</strong>: Gather ... Response 2's response confidence u 2 : Highly confident Evaluation explanation e: Response 2 is closer to the user's true intent.Response 1 mistakenly interprets the user's intent as asking for help.Preference p: 2</p>
<p>Response 1 r 1 : <strong>Step 1: Calculate the total weight lifted by each athlete.</strong>-Chinese athlete: 136kg (Snatch) + 173kg (Clean and Jerk) = 309kg -Korean athlete: 131kg (Snatch) + 168kg (Clean and Jerk) = 299kg.<strong>Step 2: Calculate the difference in weight lifted between the athletes.</strong>-Difference = 309kg (Chinese) -299kg (Korean) = <strong>10kg</strong>.Therefore, the Chinese athlete lifted 10kg more than the Korean athlete in their final totals.Response 1's response confidence u 1 : Absolute confidence Response 2 r 2 : To find out how many more kilograms the Chinese athlete lifted compared to the Korean athlete in their final totals, we first need to calculate the total weight lifted by each athlete.<strong>Chinese Athlete:</strong> -Snatch: (130kg + 136kg = 266kg) -Clean and Jerk: (167kg + 173kg = 340kg) -Total: (266kg + 340kg = 606kg) <strong>Korean Athlete:</strong> -Snatch: (123kg + 127kg + 131kg = 381kg) -Clean and Jerk: (163kg + 168kg + 173kg = 504kg) -Total: (381kg + 504kg = 885kg) <strong>Comparison:</strong> The Chinese athlete lifted a total of 606kg, while the Korean athlete lifted a total of 885kg.Therefore, the Korean athlete lifted (885kg -606kg = 279kg) more than the Chinese athlete.Response 2's response confidence u 2 : Clearly confident Evaluation explanation e: The answer provided by Response 2 is incorrect.Response 1 provides the correct answer.Preference p: 1Table 13: Hallucination case.Response 2 contains a match date that contradicts the instruction.</p>
<p>\n\n<strong>Additional Notes:</strong>\n\n<em> The match took place on 29/07/2024.\n</em>The score was 40-7 in favor of Australia.\n<em>Australia booked a place in the Semifinals.Response confidence 1: 0.865 Response 2: </em><em>Title: Australia Triumphs in Women's Rugby Sevens Quarterfinals, Advances to Semifinals</em><em>\n\n</em><em>Date: 30/07/2024</em><em>\n\n</em><em>Location: [Host City/Venue]</em><em>\n\n</em><em>[Host City, July 30, 2024] -</em><em> In a thrilling display of rugby prowess and determination, the Women's Rugby Sevens Quarterfinal match between Australia and Ireland took place on 30th July 2024, culminating in a decisive victory for the Australian team.\n\n</em>*Match</p>
<p>Table 1 :
1
Uncertainty analysis of 6 LLM-based evaluators on MT-Bench and PandaLM test set.
The</p>
<p>Table 2 :
2
Sensitivity of model confidence to different data distributions.△: the absolute confidence difference between MT-Bench and PandaLM.</p>
<p>Table 3 :
3(b) Pairwise comparison.MT-BenchPandaLM Test setMT-BenchPandaLM Test setEvaluatorRatingEvaluation ConfidenceRatingEvaluation ConfidenceEvaluatorWin / Lose / TieEvaluation ConfidenceWin / Lose / TieEvaluation ConfidencePrometheus2-7B Prometheus2-bgb-8x7B 4.725 5.9630.993 0.8707.187 6.1010.991 0.887PandaLM-7B Prometheus2-7B Prometheus2-bgb-8x7B 31.5 / 32.5 / 16.0 42.0 / 28.5 / 9.5 37.5 / 42.0 / 0.50.596 0.990 0.96758.0 / 72.0 / 40.0 77.5 / 92.5 / 0.0 77.0 / 80.5 / 12.50.704 0.993 0.974General LLMs6.4560.6547.0580.652General LLMs28.3 / 32.6 / 19.10.79760.7 / 68.2 / 41.10.850
Uncertainty analysis with specially trained LLM evaluators on MT-Bench and PandaLM test set."GeneralLLMs"refers to the average performance of evaluators from Table1."Win / Lose / Tie" represents the average number of times Llama2-7B-Instruct's response is better than, worse than, or equal to Llama2-13B-Instruct's response.(a)Single-answer grading.</p>
<p>Table 4 :
4
(Taori et al., 2023)fine-tuning set is sampled from the Alpaca 52K dataset(Taori et al., 2023).Test set (Olympic 2024) is manually created based on data from the Olympics site.Each instance is annotated by three human evaluators.
Data#Instances Annotator AgreementFine-tuning set69494.96%Test set22097.27%</p>
<p>Table 5 :
5
Evaluation performance of 12 evaluators on Olympic 2024.The highest F1 and evaluation confidence of each group is marked by bold.
EvaluatorF1Evaluation ConfidenceOverall Writing Roleplay Math Reasoning ExtractionGPT-4o0.6780.3910.7610.8570.7200.6410.968GPT-4o-mini0.6770.4230.7270.8200.8000.6270.986GPT-3.5-Turbo0.6370.5050.7150.7270.6460.5640.977Llama3-70B-Instruct0.5420.3160.6270.6470.6840.3770.981Llama2-70B-Instruct0.5340.2410.7010.5460.5670.6130.973Qwen2-72B-Instruct0.6310.4040.6890.8670.6960.4720.978Prometheus2-7B0.5150.2800.7030.5370.6110.3070.971Prometheus2-bgb-8x7B0.5560.3940.6580.6410.6960.2670.965PandaLM-7B0.5600.3880.6770.5850.6080.4550.712Llama3-8B-Instruct0.5360.2670.6500.6930.6350.3880.973Llama3-8B-Instruct-Finetune0.5820.6030.5730.3330.6280.4580.979ConfiLM0.6210.7230.5660.5100.6700.5940.982</p>
<p>Table 5
5
presents the evaluation performance of 12 evaluators on Olympic 2024.Our observations include: (1) all LLM evaluators struggle with the Olympic 2024 (with the best F1 score only reaching 0.678), demonstrating that OOD data poses significant challenges to LLM evaluators' capabilities.</p>
<p>Table 6 :
6
Hallucination case.Full version in Table13.
Generate a news report: Australia defeated Ireland 40:7 inthe Women's Rugby Sevens Quarterfinal on 29/07/2024,securing a Semifinal spot.Response 1: Women's Rugby Sevens: Australia CruisesPast Ireland in ... (Confidence: 0.865)Response 2: ... match between Australia and Ireland tookplace on 30th July 2024 ... (Confidence: 0.715)GPT-4o: Response 2 offers an engaging report.Llama3-8B-Instruct-Finetune: Response 2 is detailed.ConfiLM: Response 2 contains incorrect dates.</p>
<p>The evaluation result is [1] if first response is better, [2] if second response is better, and [Tie] for a tie.Output your evaluation result and provide a short explanation.The output format is: <The result is: [Evaluation Result].The explanation is: [Explanation]>.
[Question]{input}[The Start of Assistant's Answer]Response 1: {respponse_1}Response 2: {respponse_2}[The End of Assistant's Answer]</p>
<p>The evaluation result is [1] if first response is better,[2]if second response is better, and [Tie] for a tie.You should first generate a short reference answer for the given question, and then evaluate the AI assistants' answers based on your reference answer, and then always end your response with your evaluation result.The output format is: <My answer is: [Reference Answer].Based on the reference answer, my evaluation result is: [Evaluation Result]>.Figure 7: Prompts for pairwise comparison.The output format is highlighted in red.Score Rubric: Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.\n\n Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.###Feedback:
Single-answer grading evaluation with PandaLM[Instruction]Below are two responses for a given task. The task is defined by the Instruction. Evaluate theresponses and generate a reference answer for the task.### Instruction:{instruction}{response}### Evaluation:Single-answer grading evaluation with Prometheus2###Task Description:An instruction (might include an Input inside it), a response to evaluate, and a score rubric rep-resenting a evaluation criteria are given.1. Write a detailed feedback that assess the quality of two responses strictly based on the givenscore rubric, not evaluating in general.2. After writing a feedback, choose a better response between Response 1 and Response 2. Youshould refer to the score rubric.3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RE-SULT] ([1] or [2] or [Tie])"4. Please do not generate any other opening, closing, and explanations.###Instruction: {instruction}###Response 1: {respponse_1}###Response 2: {respponse_2}###Pairwise comparison with Prometheus2###Task Description:An instruction (might include an Input inside it), a response to evaluate, a reference answer thatgets a score of 5, and a score rubric representing a evaluation criteria are given.1. Write a detailed feedback that assess the quality of the response strictly based on the givenscore rubric, not evaluating in general.2. After writing a feedback, write a score that is an integer between 1 and 5. You should referto the score rubric.3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RE-SULT] (an integer number between 1 and 5)4. Please do not generate any other opening, closing, and explanations.###The instruction to evaluate {input}###Response to evaluate: {output}###Score Rubrics: Your evaluation should consider factors such as the helpfulness, relevance,accuracy, depth, creativity, and level of detail of the response. Be as objective as possible.Please avoiding any potential bias.###Feedback:[Question]{input}[The Start of Assistant's Answer]Response 1: {respponse_1}Response 2: {respponse_2}[The End of Assistant's Answer]</p>
<p>System Prompt] Below are two responses for a given task.The task is defined by the user question displayed below.Please act as an impartial judge and evaluate the quality of the responses.You should choose the assistant that follows the user's instructions and answers the user's question better.Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.The evaluation result is [1] if first response is better, [2] if second response is better.You should first provide a short explanation of your evaluation, and then always end your response with your evaluation result.The output format is: &lt;<The explanation is:\n[Explanation]\nThe result is:[Evaluation Result]>&gt;.
Uncertainty-aware fine-tuning[User Prompt]User instruction:&lt;&lt;{instruction}&gt;&gt;Response 1:&lt;&lt;{response_1}&gt;&gt;The confidence of Response 1:&lt;&lt;{response_1_confidence}&gt;&gt;Response 2:&lt;&lt;{response_2}&gt;&gt;The confidence of Response 2:&lt;&lt;{response_2_confidence}&gt;&gt;[Output]The explanation is:{evaluation_explanation}The result is:{evaluation_result}</p>
<p>Table 11 :
11
Examples of user instruction from the Olympic 2024 dataset.Due to space limitations, we truncate the content of the Extraction instance.Writing: Generate a news report based on the following sentences: The Men's Water Polo Gold Match took place on 11/08/2024 at Paris La Defense Arena, Paris.Serbia claimed the Gold with a 13:11 victory against Croatia.</p>
<p>Table 12 :
12
A test instance for ConfiLM.</p>
<p>DISCUSSIONLLM-based evaluation requires a comprehensive consideration of prompt optimization(Zhou et al., 2023a; 2024a), bias calibration(Zhou et al., 2024b), and uncertainty mitigation strategies. The performance of LLMs as evaluation tools is influenced by various factors, such as the diversity of training data(Shi et al., 2024), inherent model biases(Zheng et al., 2023), and the complexity of the tasks. These uncertainties can cause fluctuations in the consistency of evaluation results. Improving the stability of LLM evaluators can decrease the randomness that may arise during the evaluation process, thus providing more accurate and reproducible results(Chiang &amp; Lee, 2023).While our work provides extensive analysis on the stability of LLM evaluators, there are other critical aspects of evaluation uncertainty that warrant attention. For example, the relationship between evaluation uncertainty and evaluation bias, as well as the uncertainty in the evaluation of multimodal large language models(Li et al., 2024). Our work only focuses on single-round evaluations. For evaluations conducted on multi-turn benchmarks (i.e., MT-Bench), we use the first-round question as input. It would be interesting to investigate how the uncertainty of LLM evaluators affects judgments on multi-round conversations. Additionally, this research does not cover language models that do not provide token probabilities (e.g., Claude (Anthropic, 2024)). Exploring how to conduct uncertainty analysis for LLM evaluators based on these proprietary models is a valuable topic. It is also important to note that commonly used LLM evaluators require strong calibration to ensure that their output probabilities accurately reflect the precision of their assessments(Chen et al., 2023). We provide an analysis of the relation between evaluation confidence and accuracy in Appendix B.2 and leave further exploration in those aspects to future work
.7 CONCLUSIONIn this paper, we empirically investigated the existence, mitigation and utilization of uncertainty in model-based LLM evaluation. Extensive empirical analyses demonstrate that uncertainty is prevalent across various LLMs and can be alleviated with special prompting strategies such as chainof-thought and self-generated reference. Experimental results on an OOD test set with 220 diverse instances show that incorporating uncertainty as auxiliary information during the fine-tuning process can largely improve the LLM evaluators' evaluation performance. We hope the empirical analyses in this work and the proposed uncertainty-aware LLM evaluator can inspire future research on the stability of model-based LLM evaluation.
GitHub repository for PandaLM: https://github.com/WeOpenML/PandaLM; GitHub repository for Prometheus2 model: https://github.com/prometheus-eval.
ACKNOWLEDGMENTWe would like to thank the anonymous reviewers for their insightful comments and suggestions to help improve the paper.This publication has been supported by the National Natural Science Foundation of China (NSFC) Key Project under Grant Number 62336006.REPRODUCIBILITY STATEMENTTo ensure the reproducibility of our results, we have made detailed efforts throughout the paper.All experimental settings, including model configurations, prompting strategies, and benchmarks, are described in Section §4.1.Additionally, we provide comprehensive information about the dataset construction and training details of ConfiLM in Section §5.Our code, data, and other resources necessary to replicate are released at: https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.Based on the analysis of these results, we found that ConfiLM outperforms Llama3-8B-Instruct-Finetune and Llama3-8B-Instruct on F1 by 3.9% and 8.5%, respectively.We attributed this improvement to the incorporation of uncertainty as auxiliary information during the fine-tuning phase.Furthermore, adding uncertainty to the prompts also brings certain performance improvements to general LLM-based evaluators (e.g., 0.690 v.s.0.641 on GPT-4o-Extraction), but these gains are unstable due to the LLMs' analytical capabilities.Confidence Value Declarative Statement [0, 0.1)Complete doubt [0.1, 0.2)Highly uncertain [0.2, 0.3)Clearly doubtful[0.3, 0.4)Significantly doubtful [0.4,0.5)Slightly doubtful [0.5, 0.6) Neutral [0.6, 0.7)Slightly confident [0.7, 0.8)Clearly confident [0.8, 0.9)Highly confident [0.9, 1.0] Absolute confidenceFigure10: The distribution of response confidence from the fine-tuning set for ConfiLM.The interval [0.0, 0.1) denotes the response confidence is greater than or equal to 0.0 but less than 0.1.Published as a conference paper at ICLR 2025  For each evaluation, we query the evaluator twice with the order swapped.'Win / Lose / Tie' represents the average number of times Llama-2-7b-chat's response is better than, worse than, or equal to Llama-2-13b-chat's response.The PandaLM model(Wang et al., 2024b) is trained to output in a normal format (providing a preference between the two responses, followed by a concise rationale).The Prometheus2 series models(Kim et al., 2024c)are trained to output in a Chain-of-Thoughts format (providing a concise rationale before indicating a preference between the two responses).(a) Default prompt
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Ai Anthropic ; Yuntao, Andy Bai, Kamal Jones, Amanda Ndousse, Anna Askell, Nova Chen, Dawn Dassarma, Stanislav Drain, Deep Fort, Tom Ganguli, Henighan, arXiv:2204.05862The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024. 2022. 201528arXiv preprintScheduled sampling for sequence prediction with recurrent neural networks</p>
<p>. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, 2024Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>A close look into the calibration of pre-trained language models. Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, Heng Ji, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, Ion Stoica, 2024</p>
<p>Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M Khapra, arXiv:2406.13439Finding blind spots in evaluator llms with interpretable checklists. 2024arXiv preprint</p>
<p>Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, Kaidi Xu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 202436</p>
<p>Detecting hallucinations in large language models using semantic entropy. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal, Nature. 63080172024</p>
<p>Uncertainty in deep learning. Yarin Gal, 2016</p>
<p>A survey of confidence estimation and calibration in large language models. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2024a1</p>
<p>Multimodal large language models to support real-world fact-checking. Jiahui Geng, Yova Kementchedjhieva, Preslav Nakov, Iryna Gurevych, arXiv:2403.036272024barXiv preprint</p>
<p>Language model cascades: Token-level uncertainty and beyond. Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. 2024</p>
<p>Exposure bias versus self-recovery: Are distortions really incremental for autoregressive text generation?. Tianxing He, Jingzhao Zhang, Zhiming Zhou, James Glass, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Taehee Jung, Dongyeop Kang, Lucas Mentch, Eduard Hovy, arXiv:1908.11723Earlier isn't always better: Subaspect analysis on corpus and system biases in summarization. 2019arXiv preprint</p>
<p>The perils of using mechanical turk to evaluate open-ended text generation. Marzena Karpinska, Nader Akoury, Mohit Iyyer, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, arXiv:2406.057612024barXiv preprint</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024carXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang, arXiv:2309.17012Benchmarking cognitive biases in large language models as evaluators. 2023arXiv preprint</p>
<p>Longeval: Guidelines for human evaluation of faithfulness in long-form summarization. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, Kyle Lo, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Confidence under the hood: An investigation into the confidence-probability alignment in large language models. Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, Ali Emami, arXiv:2405.162822024arXiv preprint</p>
<p>Seed-bench: Benchmarking multimodal large language models. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2024</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, Transactions on Machine Learning Research. 2022</p>
<p>How good are llms at out-of-distribution detection?. Bo Liu, Li-Ming Zhan, Zexin Lu, Yujie Feng, Lei Xue, Xiao-Ming Wu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024a</p>
<p>Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.14201Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. 2023arXiv preprint</p>
<p>G-eval: Nlg evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, Nigel Collier, arXiv:2403.169502024barXiv preprint</p>
<p>Calibrating llm-based evaluator. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024c</p>
<p>Loshchilov, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Uncertainty estimation in autoregressive structured prediction. Andrey Malinin, Mark Gales, International Conference on Learning Representations. 2021</p>
<p>Why we need new evaluation metrics for nlg. Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, Verena Rieser, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>Proving test set contamination in black-box language models. Yonatan Oren, Nicole Meister, S Niladri, Faisal Chatterji, Tatsunori Ladhak, Hashimoto, The Twelfth International Conference on Learning Representations. 2024</p>
<p>What in-context learning" learns" in-context: Disentangling task recognition and task learning. Jane Pan, Tianyu Gao, Howard Chen, Danqi Chen, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. Adian Vyas Raina, Mark Liusie, Gales, arXiv:2402.140162024arXiv preprint</p>
<p>In-context impersonation reveals large language models' strengths and biases. Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata, Advances in Neural Information Processing Systems. 202436</p>
<p>Detecting pretraining data from large language models. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Overlapping experiment infrastructure: More, better, faster experimentation. Diane Tang, Ashish Agarwal, Deirdre O 'brien, Mike Meyer, Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. the 16th ACM SIGKDD international conference on Knowledge discovery and data mining2010</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: an instruction-following llama model. 202312023</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.001182024arXiv preprint</p>
<p>Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes, arXiv:2406.12624Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. 2024arXiv preprint</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D Manning, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Open foundation and fine-tuned chat models. 20232arXiv preprint</p>
<p>A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu, arXiv:2307.039872023arXiv preprint</p>
<p>Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung, arXiv:2407.10817Foundational autoraters: Taming large language models for better automatic evaluation. 2024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024a</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023aarXiv preprint</p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Qiuejie Xie, Qiming Feng, Tianqi Zhang, Qingqiu Li, Linyi Yang, Yuejie Zhang, Rui Feng, Liang He, Shang Gao, Yue Zhang, arXiv:2402.18180Human simulacra: Benchmarking the personification of large language models. 2024arXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, 2023</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, arXiv:2407.10671Fei Huang, et al. Qwen2 technical report. 2024aarXiv preprint</p>
<p>Glue-x: Evaluating natural language understanding models from an outof-distribution generalization perspective. Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Supervised knowledge makes large language models better in-context learners. Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, arXiv:2405.16908The Twelfth International Conference on Learning Representations, 2024b. Gal Yona, Roee Aharoni, and Mor Geva. Can large language models faithfully express their intrinsic uncertainty in words?. 2024arXiv preprint</p>
<p>Kieval: A knowledge-grounded interactive evaluation framework for large language models. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang, arXiv:2402.150432024aarXiv preprint</p>
<p>Freeeval: A modular framework for trustworthy and efficient evaluation of large language models. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, Shikun Zhang, arXiv:2404.060032024barXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Evaluating large language models at evaluating instruction following. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Survival of the most influential prompts: Efficient black-box prompt search via clustering and pruning. Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023a</p>
<p>Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vulić, Anna Korhonen, arXiv:2406.11370Fairer preferences elicit improved human-aligned large language model judgments. 2024aarXiv preprint</p>
<p>Batch calibration: Rethinking calibration for in-context learning and prompt engineering. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine A Heller, Subhrajit Roy, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>A PROMPTS DEMONSTRATION All the relevant prompts used in this study are provided in Figures 6, 7, 8 and 9. Kaitlyn Zhou, Dan Jurafsky, Tatsunori B Hashimoto, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023bPrompts for PandaLM and Prometheus2 model are obtained from their GitHub repository 1</p>            </div>
        </div>

    </div>
</body>
</html>
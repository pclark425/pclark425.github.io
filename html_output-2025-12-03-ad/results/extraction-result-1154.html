<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1154 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1154</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1154</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-207179119</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1609.04436v1.pdf" target="_blank">Bayesian Reinforcement Learning: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.</p>
                <p><strong>Cost:</strong> 0.039</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1154.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1154.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thompson Sampling (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson Sampling (Probability Matching)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian randomized sampling strategy that at each decision samples a model from the posterior and plays the action that is optimal under that sampled model; used for bandits, contextual bandits and extended to RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Thompson Sampling (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Posterior-sampling based agent: maintains posterior P_post(θ), samples θ_t ∼ P_post, selects a_t = argmax_a E_{y∼P_θ_t(·|a)}[r(y)], updates posterior with observed outcome. Lightweight; extends to structured/correlated arms and contextual settings.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Thompson sampling / posterior sampling</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts exploration by sampling from the posterior over environment parameters; actions that are more likely to be optimal under the posterior are chosen more often. The posterior is updated with each observation, shifting sampling toward better models.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Stochastic multi-armed bandits; contextual bandits; policy sampling for MDPs (via sampled models)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown reward/outcome distributions; can handle correlated arms and contexts; generally stochastic and possibly structured; in extensions can be partially observable or have model structure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies: K arms (finite), contextual dimension d in linear contexts, or structured action sets (e.g., subsets); sample complexity and regret depend on K, d, and problem-dependent gaps Δ_a.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Multiple theoretical guarantees reported: Agrawal & Goyal (2012) give frequentist regret bound E_θ Regret(T) ≤ O( Σ_{a≠a*} (1/Δ_a^2) ln T ); Kaufmann et al. (2012b) give problem-dependent bounds matching Lai-Robbins up to constants for Bernoulli arms; Agrawal & Goyal (2013a) give O(√(K T ln T)) problem-independent bound (Bernoulli+Beta prior). Empirical state-of-the-art in many applications (contextual bandits, online ads).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: proven logarithmic (problem-dependent) regret O(ln T) asymptotically for standard bandits under suitable priors; problem-independent bounds O(√(KT ln T)).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicitly balanced by posterior uncertainty: sampling variability yields exploration; as posterior concentrates, behavior becomes exploitative.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to UCB, Bayes-UCB, frequentist UCB variants, and problem-specific baselines in surveyed works.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>TS is a simple, general method that yields state-of-the-art empirical performance across bandit and contextual problems; theoretical analyses provide both problem-dependent logarithmic regret bounds and problem-independent √-type bounds; extensions handle correlated actions and structure yielding tighter guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance can depend strongly on prior (informative priors help; uninformative/misspecified priors can slow learning); analysis often relies on conjugacy or tractable posterior updates; scaling to very large structured RL requires efficient posterior sampling approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1154.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-UCB (Bayesian Upper Confidence Bound)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian adaptation of UCB that selects at each step the arm whose posterior (1 − β_t)-quantile of the expected reward is largest, implementing an optimism principle via posterior quantiles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayes-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains posterior over arm reward means; at time t computes posterior (1−β_t)-quantile for each arm and selects the arm with largest quantile; update posterior with observed rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian upper-confidence (optimism) via posterior quantiles</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts exploration by using wider quantiles early (β_t ~ 1/t) to be optimistic about uncertain arms; as posterior concentrates quantiles tighten and exploitation increases.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Stochastic multi-armed bandits (e.g., Bernoulli arms)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown reward distributions per arm (i.i.d. over time), stochastic, discrete actions, possibly conjugate priors (Beta for Bernoulli).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>K arms; analysis often in standard finite-arm bandit setting.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>For Bernoulli arms with uniform prior, Kaufmann et al. (2012a) prove a frequentist upper bound on expected regret that matches Lai & Robbins lower bound asymptotically (logarithmic in T).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Asymptotically optimal sample efficiency (logarithmic regret) under standard assumptions and suitable prior.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit optimism via selecting upper posterior quantile (decreases with t), balancing exploration early and exploitation as uncertainty reduces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared conceptually to frequentist UCB algorithms and other Bayesian bandit methods (e.g., Thompson Sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Bayes-UCB provides a principled Bayesian optimism mechanism and (for Bernoulli/uniform prior) achieves asymptotically optimal frequentist regret matching classical lower bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires tractable posterior quantile computation; performance depends on prior choice; theoretical results are established under specific distributional/prior choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1154.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-Adaptive MDP (BAMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Adaptive Markov Decision Process (BAMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal model that augments physical MDP state with posterior parameters over unknown dynamics (hyper-state), turning RL with model uncertainty into planning in a larger MDP (belief/hyper-state MDP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayes-Adaptive MDP (BAMDP) planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent reasons about a hyper-state (s, φ) that combines task state s and posterior counts/parameters φ over transition/reward distributions; planning follows Bellman optimality over hyper-states to trade off immediate reward and information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian belief-state planning (Bayes-adaptive planning) / value of information via Bayes-optimal policy</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts actions by computing Bayes-optimal policy in hyper-state space: actions chosen to maximize expected discounted reward under posterior over models, implicitly valuing exploration when it improves future returns.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MDPs with unknown transition/reward parameters (discrete S, A)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition probabilities represented with Dirichlet priors; discrete state and action spaces; stochastic dynamics; can be partially observable via BAPOMDP extension.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Hyper-state space grows combinatorially (|S|^t reachable states after t steps); computationally large except for small problems or structured priors.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>BAMDP defines Bayes-optimal behavior (no single numerical performance since intractable); approximate planners (BAMCP, BFS3, BEB, etc.) aim to approximate BAMDP-optimal performance and show superior sample efficiency in many small benchmark domains (e.g., Chain problem).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Bayes-optimal policies are sample-efficient in theory because they optimally trade exploration and exploitation; practical algorithms approximating BAMDP claim improved sample efficiency versus naive exploration in reported simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Built-in: Bayes-optimal policy explicitly accounts for expected information gain (posterior evolution) when selecting actions, balancing immediate reward vs information that improves future returns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Forms the basis for comparisons with approximate online planners: Bayesian dynamic programming, BEETLE, BAMCP, BFS3, BEB, BOSS, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BAMDP gives a clean theoretical framing: solving the augmented hyper-state MDP yields Bayes-optimal exploration-exploitation behavior; this representation motivates approximate algorithms (sampling/tree search/exploration bonus) to achieve tractable near-Bayes-optimal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exact planning is intractable for realistic domains due to exponential hyper-state growth; requires structural assumptions or approximations (sampling, parametric priors, parameter tying) to be tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1154.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAPOMDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Adaptive Partially Observable Markov Decision Process (BAPOMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extension of BAMDP to POMDPs: hyper-state includes latent state posterior plus posterior over transition and observation models, enabling Bayes-adaptive planning under both model and state uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayes-Adaptive POMDP (BAPOMDP) planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent maintains a joint posterior over hidden environment state and model parameters (φ for transitions, ψ for observation model) and plans in the hyper-state space to maximize expected return, including querying/active information actions where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayes-adaptive planning in partially observable environments; active information acquisition / value of information and planning under posterior beliefs</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts action/queries based on joint belief over states and model parameters; can include query/oracle actions (Minerva-style) or optimize Bayes risk to decide when to query versus act.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Partially observable MDPs with unknown transition and/or observation models</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partial observability (observations O), unknown transition kernel P and observation kernel Ω modeled with Dirichlet priors; stochastic; belief states are distributions over (s, φ, ψ).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Very large: number of possible beliefs grows exponentially with horizon; planning over these beliefs is computationally challenging even for small base MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Paper reports approximate algorithms (forward search, sampling, Minerva variants) and theoretical approximations; Ross et al. (2011) provide approximation results; empirical success reported in simulated dialogue management when structural constraints applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Adaptive Bayes-optimal strategies in BAPOMDPs can be sample-efficient, but computational cost often dominates; approximate methods aim to focus computation on reachable beliefs to improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled in belief-space: actions selected for direct reward or to reduce uncertainty about state/model (information gathering), with Bayes risk and value-of-information criteria used in some algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to Minerva (oracle), Atrash & Pineau (action-oracle), VOI heuristics, and sampling-based planners in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BAPOMDP extends the Bayes-adaptive idea into partial observability, enabling principled tradeoffs between acting and information acquisition; practical methods require strong approximations (bounded counts, sampling, oracles) to be tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Belief space explosion makes exact Bayes-adaptive planning infeasible for realistic POMDPs; Minerva-style oracle assumptions are unrealistic for many domains; approximate methods can be loose or computationally heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1154.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Dynamic Programming (Strens)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Dynamic Programming (model sampling / Thompson-style DP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple practical approach: repeatedly sample a full MDP model from the posterior, solve it (via DP) and act according to the resulting policy for some period, re-sampling as posterior updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian Dynamic Programming (sample-and-solve)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At decision times sample M models from posterior (often just one), compute optimal policy for sampled model using DP, follow it for a while; update posterior from observed transitions and repeat.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior sampling of full MDP models (a form of Thompson sampling for MDPs)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by re-sampling models as data arrives; sampled models emphasize regions of current posterior mass leading to exploration when posterior uncertain and exploitation as posterior concentrates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete MDPs with unknown dynamics (e.g., Chain problem)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transitions (Dirichlet priors), stochastic; small-to-moderate-sized discrete state/action spaces in typical use.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Limited by DP solving step (state space size); feasible for small MDPs or via simulator-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Convergence to optimal policy is achievable because the method samples from the full posterior; Osband et al. (2013) provide theoretical Bayesian regret analysis for posterior sampling approaches; empirically used as baseline in multiple comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reasonably sample-efficient in small domains; empirical performance competitive with BEETLE and BOSS on Chain problem per survey.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration through variability of sampled models—if the sampled model suggests exploring, agent does so; as posterior narrows, behavior becomes exploitative.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against BEETLE, BEB, BFS3, BAMCP and others in the survey's empirical summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Simple to implement and conceptually clear; sampling-based DP provides effective exploration without ad-hoc bonuses; theoretical Bayesian regret results support its soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computationally expensive due to repeated DP solves; may be slow if re-solving too often; naive single-model sampling can have high variance requiring many resamples or long planning phases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1154.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAMCP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Adaptive Monte-Carlo Planning (BAMCP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Monte-Carlo Tree Search algorithm (UCT-style) applied to the BAMDP: sample a model at root, run UCT rollouts using that sampled model (with learned rollouts), and aggregate statistics to select actions in the real environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BAMCP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Sampling-based forward-search planner: at each decision sample a full MDP model from the posterior, build/expand a UCT search tree using that model (lazily sampling model transitions), use model-free learning to bias rollouts (ε-greedy), update tree counts and Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Monte-Carlo tree search on posterior-sampled models (posterior sampling + UCT) with learned rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Exploration arises from posterior sampling of models; within-tree UCT balances exploration-exploitation of action branches; learned rollout Q-values bias simulation toward promising branches.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete MDPs with unknown dynamics (Chain, other small benchmarks); extended to larger/some structured domains via lazy sampling and rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transitions (Dirichlet posterior), stochastic; discrete states/actions; planning horizon limited by search depth and rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Search tree size grows with branching factor and depth; complexity controlled by number of simulated rollouts and lazy sampling; applied to small-to-moderate MDPs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Survey reports BAMCP empirically outperforms Bayesian Dynamic Programming, Value-of-Information heuristic, BFS3, BEB and SmartSampler on several simulated domains (including Chain); proven to converge in probability to Bayes-optimal policy; UCT-based error decreases as O(log n / n) with n visits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically more sample-efficient than several prior sampling/search-based planners in small domains; efficiency scales with number of simulation rollouts allocated.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Two-level: posterior model sampling provides high-level exploration; UCT inside tree balances exploration of action branches; rollout learning biases simulations to exploit promising areas.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to Bayesian Dynamic Programming, VOI, BFS3, BEB, SmartSampler in survey summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BAMCP is a practical near-Bayes-optimal planner that combines posterior sampling and MCTS; it leverages learned rollouts to bias search and empirically attains strong performance on canonical BRL benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computational cost of many rollouts/search nodes can be high; performance depends on quality of rollout policy and simulation budget; still limited to problems where MCTS is tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1154.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Sparse Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Sparse Sampling (Wang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of Sparse Sampling to BAMDPs: stochastic forward sampling of actions and next-states according to posteriors to estimate long-term values with Monte-Carlo trees focused on likely/higher-value branches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian Sparse Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Forward-search Monte-Carlo planner that grows a stochastic tree by sampling actions according to their probability of being optimal (from Q-value posteriors) and sampling next states from the posterior over transitions, using Monte-Carlo estimates to evaluate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Sparse Monte-Carlo forward search with posterior-driven sampling (Thompson-style selection in tree growth)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Tree growth focuses computational effort on branches that are more likely or promising according to current posterior/Q-value uncertainty; sampling from posterior naturally explores.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Small BAMDPs / discrete MDPs with unknown transitions and Dirichlet priors</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions; discrete state/action spaces; used for small MDPs (e.g., Chain) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Sparse sampling reduces dependence on number of states but planning horizon and branching factor still limit scalability; practical only for small-to-moderate problems.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Provides unbiased/low-error estimates with high probability using a number of samples independent of |S| (theoretical guarantee for sparse-sampling); empirically applied to small MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Can achieve low error with number of samples independent of number of states, but exponential in planning horizon; practical sample efficiency depends on horizon and branching choices.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration arises from stochastic sampling of actions according to posterior likelihood of being optimal; exploitation via focus on high-probability/high-value branches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Related to forward-search and BAMCP approaches; compared conceptually with methods that consider all successor states.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Adapts sparse-sampling ideas to Bayesian setting to focus computation on probable/higher-value parts of hyper-state tree, yielding tractable approximate Bayes planning in small domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exponential dependence on planning horizon remains; requires many posterior samples per node which can be computationally heavy; so far applied to small MDPs only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1154.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value-of-Information (VOI) heuristic (Dearden et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value of Information (VOI) heuristic for exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A myopic Bayesian heuristic that selects actions by adding a 1-step estimate of the expected value of information (VPI) to expected immediate reward to trade off exploration vs exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Value-of-Information (VOI) heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent samples MDPs from Dirichlet posterior, solves them to estimate distribution over Q*, computes expected value of perfect information (VPI) for each action myopically, and selects action maximizing E[Q]+VPI.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Value of information / myopic information-gain augmented action selection</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects actions based on immediate reward plus estimated one-step VPI—actions that could improve policy (via information) are favored even if immediate reward is lower.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete MDPs with Dirichlet priors on transitions (e.g., Chain)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics; stochastic; small discrete state/action spaces in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Myopic 1-step VPI simplifies computation, scalable by sampling count M (adjustable), but still costly due to solving sampled MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically attractive and scalable by varying number of samples; shown useful in small domains, but myopic nature can limit long-term benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Adjustable via number of sampled MDPs; can be sample-efficient if limited samples still capture key uncertainties, but myopic.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly trades exploration by adding VPI term to immediate reward; exploration encouraged when expected policy improvement is high.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared conceptually and empirically to full Bayes-optimal planning (infeasible) and to sampling-based planners in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Provides a computationally tractable, intuitive heuristic for exploration that accounts for information value and can be scaled by sampling budget.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Myopic (1-step) approximation can miss long-term informational benefits; may underperform in domains where multi-step information acquisition is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1154.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEETLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BEETLE (Bayesian Exploration Exploitation Tradeoff in LEarning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline approximate BAMDP solver that adapts POMDP point-based planning (Perseus) to the Bayes-adaptive case by sampling hyper-states and representing α-functions over posterior parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BEETLE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Samples hyper-states (s, φ) from interactions, constructs α-functions represented as linear combinations of basis functions over transition-function parameters, and applies point-based backups to approximate Bayes-optimal value.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Point-based Bayesian planning / offline approximation of Bayes-optimal policy over sampled hyper-states</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Learns α-function basis over sampled hyper-states; policy implicitly values information via BAMDP formulation; offline computation produces policy mapping beliefs to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BAMDPs / small discrete MDPs and POMDP-like hyper-state spaces</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transitions (Dirichlet priors), possibly partial observability in extensions; small to moderate discrete spaces in evaluated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Computationally demanding offline; tractable if number of unknown parameters limited or parameter tying used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Tested experimentally in small simulation domains; performance comparable to BOSS and Bayesian dynamic programming on Chain problem in survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Can be sample-efficient if basis functions capture posterior structure; offline cost may be high.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit in Bayes-optimal objective approximated via α-functions over belief/posterior space; exploration is driven by value of future information.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared empirically to Bayesian dynamic programming, BOSS, BEB in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Adapts point-based POMDP techniques to approximate Bayes-adaptive planning; can exploit structure to scale somewhat but remains computationally heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Number of α-function terms can grow exponentially with horizon; requires projection/pruning and structural assumptions (parameter tying) to scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1154.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOSS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BOSS (Best of Sampled Sets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based Bayesian RL algorithm that samples multiple models from the posterior, constructs a hyper-model combining sampled models' actions, solves it, and uses the resulting policy for a while; provable PAC-like guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BOSS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Draws K sampled MDPs from posterior, creates a hyper-model with K|A| actions (each sample-action pair), solves hyper-model via DP to decide real action, repeats sampling after a knownness threshold per state-action.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior model sampling combined with optimistic planning over sampled ensemble (optimism in the face of uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by increasing knownness counts and sampling more models for uncertain (s,a); hyper-model encourages optimism by picking an action that is good in at least one sampled plausible model.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete MDPs with Dirichlet priors on transitions (Chain, small mazes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transitions, stochastic; discrete states/actions; can exploit structure (parameter tying) to scale.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Empirically applied to Chain and small mazes (e.g., 6×6); theoretical guarantees require polynomial number of samples depending on |S|, |A|, knownness parameter.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical results show BOSS performs similarly to BEETLE and Bayesian dynamic programming on Chain; theoretical PAC-style bounds (Asmuth et al., 2009) guarantee near-optimal value except for polynomial number of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Provable polynomial sample complexity to reach near-optimal performance with high probability (depends on knownness parameter B and |S|, |A|).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Optimism via sampling: hyper-model contains optimistic actions from sampled MDPs, encouraging exploration of uncertain (s,a) combinations until known.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to BEETLE, Bayesian DP, SmartSampler, BEB in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Simple sampling-based construction with PAC-style guarantees; can exploit posterior structure and parameter tying to tackle larger state spaces than naive approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Hyper-model can grow large; number of sampled models required depends on posterior variance; computational cost of solving hyper-models can be high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1154.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BEB (Bayesian Exploration Bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple model-based algorithm that adds an exploration bonus (function of posterior counts) to the reward and solves the resulting optimistic MDP greedily; designed to achieve PAC-BAMDP guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BEB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modifies reward with additive bonus β/(1 + n_{s,a}), where n_{s,a} is posterior count for (s,a); solves modified MDP greedily to choose actions; bonus decays faster (1/n) than some PAC-MDP bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Exploration bonus on rewards based on posterior counts (optimism via reward shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>As counts n_{s,a} increase, bonus decays and policy becomes exploitative; early on, bonus encourages exploring state-action pairs with high posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete MDPs with Dirichlet priors (e.g., Chain problem)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transitions; stochastic; discrete states/actions; reward function assumed known in many analyses (extensions handle unknown rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Typical small-to-moderate MDPs used in experiments (Chain); theoretical bounds depend on |S|, |A| and horizon H.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theorem: with β chosen appropriately, with high probability V^{A_t}(s_t, φ_t) ≥ V^*(s_t, φ_t) − ε for all but m=O(|S||A| H^6 ε^{-2} log(|S||A|/δ)) time steps. Empirically BEB outperforms non-Bayesian PAC-MDP algorithms on Chain and can find the correct optimal policy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>PAC-BAMDP guarantees imply polynomial sample complexity to achieve ε-close to Bayesian-optimal value; bonus decays as O(1/n) enabling faster discard of exploration than some PAC-MDP methods.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit via exploration bonus: optimistic reward pushes exploration early; bonus decays with data shifting towards exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to BOSS, Bayesian DP, BEETLE, MBIE-EB in survey; BEB often more sample-efficient towards Bayesian optimality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BEB achieves PAC-style guarantees with respect to Bayesian optimality and empirically is sample-efficient on canonical tasks; exploration bonus designed from Bayesian posterior yields fast decay.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Theoretical optimality is with respect to Bayesian-optimal policy (not necessarily true environment optimal); may never find true optimal policy in some MDPs despite close Bayesian-optimal performance (theoretical counterexamples exist).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1154.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VBRB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variance-Based Reward Bonus (VBRB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-bonus approach where the bonus is proportional to posterior variance of transition and reward parameters, yielding PAC-style guarantees via Chebyshev-based analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Variance-Based Reward Bonus (VBRB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Augments reward with bonus R_bonus(s,φ,a)=β_R σ_R(s,φ,a) + β_P Σ_s σ_P(s|s,φ,a) where σ terms are posterior variances of reward and transition probabilities; selects greedy action w.r.t. augmented reward.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Variance-based exploration bonus derived from posterior uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Actions with higher posterior variance (uncertainty) receive larger bonus and are explored; as posterior variance decreases with data, bonus decays and exploitation increases.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete MDPs with Dirichlet priors on transitions and possibly uncertain rewards (Chain, grid-world/Wumpus variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transitions and possibly rewards; stochastic; discrete states/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Applied to Chain problem variants and 4×4 grid-world; complexity similar to BEB/BOSS settings.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical PAC-style sample complexity bounds provided: follows near-optimal policy on all but O( Σ_{s,a} C(s,a) / (1−γ)^2 log(1/δ) ) steps with high probability. Empirically competitive with BEETLE, BOSS, BEB and superior on a Wumpus-inspired 4×4 grid when modeling independent Dirichlet priors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sample complexity decreases O(1/√n) with number of observations per (s,a) in Dirichlet case; comparable to other PAC-MDP algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration directed by posterior variance: the higher the uncertainty, the larger the bonus and the more likely the agent explores that (s,a).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared empirically to BEETLE, BOSS, BEB in survey experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Variance-based bonuses yield analyzable PAC-style guarantees and can perform well empirically, especially when model uncertainty is widespread (no parameter tying).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Bonus scale parameters must be set appropriately; performance may degrade if underlying assumptions on posterior variance estimates are violated or priors are misspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e1154.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOLT / POT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BOLT (Bayesian Optimistic Local Transitions) / POT (Probably Optimistic Transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BOLT places optimism on transition probabilities (imagined transitions) by extending the action space; POT adapts the optimism parameter online using UCB-like criteria to be 'probably optimistic'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BOLT (and POT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>BOLT augments each action with a target next-state σ and constructs optimistic transition probabilities by imagining η extra transitions to σ; solves the extended optimistic MDP. POT adapts η per (s,a) using UCB-like variance estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Optimistic transitions (optimism in model space) via imaginary transitions and UCB-style adaptive optimism (POT)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>BOLT uses fixed optimism parameter η to bias transition probability toward chosen successor states (encouraging exploring promising transitions); POT estimates optimism per (s,a) online based on current posterior variance and UCB criteria, adapting exploration pressure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete MDPs with Dirichlet posteriors (Chain problem and variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions, discrete states/actions; tested on Chain and other small domains.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Comparable to BEB/BOSS settings; extended action space size grows by |S| factor due to optimistic actions ζ=(a,σ).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical: BOLT can be outperformed by well-tuned BEB but is more robust to parameter choice; POT shows improved empirical performance over BOLT, BEB, VBRB when prior is good but can suffer under misspecified priors. Theoretical bounds: BOLT is ε-close to Bayesian-optimal except for O(|S||A|η^2 / (1−γ)^2) steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reasonable; POT improves efficiency by adapting optimism to observed variance. Exact sample counts problem-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit optimism in transitions pushes the agent to try under-explored successor outcomes; POT tunes optimism to avoid over/under exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to BEB, VBRB, and other exploration-bonus algorithms in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Optimism placed on transitions is an effective and interpretable way to encourage exploration; adapting optimism online (POT) can improve robustness and sample-efficiency when priors are informative.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance sensitive to optimism parameters and prior correctness; POT may suffer when prior is misspecified; extended action space increases computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1154.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e1154.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BFS3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BFS3 (Bayesian Forward Search Sparse Sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online forward-search sparse sampling algorithm for BAMDPs that maintains upper and lower bounds and directs rollouts to nodes with largest uncertainty gap, with PAC-style guarantees under assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BFS3</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds a sparse forward-search tree over hyper-states (s, φ), maintaining lower/upper value bounds, and guides expansion to nodes maximizing uncertainty-weighted gaps (U−L)/n to efficiently reduce value uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Forward-search sparse sampling with bound-directed expansion (optimism and uncertainty reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Expands tree nodes where upper-lower bound gaps are largest relative to visitation counts, concentrating computation/exploration on uncertain but influential hyper-states; posterior sampling provides stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BAMDPs / discrete MDPs with unknown parameters</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transitions/rewards; Dirichlet priors; stochastic; finite state/action sets in analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Can handle much larger effective state spaces with parameter tying and structure; surveyed claim: up to 10^16 states with parameter tying in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theorem (Asmuth, 2013): with high probability BFS3 takes at most polynomial number of sub-Bayes-optimal actions (bound provided) under assumptions on prior accuracy and FSSS optimism. Empirically solves small domains (e.g., 5×5 grid) more effectively than some non-Bayesian methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Polynomial sample complexity under assumptions; practical efficiency depends on pruning and bound tightness.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Guided by maintained value bounds: exploration directed to branches where uncertainty could change decision; exploitation where lower-bound already high.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared conceptually/empirically to RMAX, other sparse sampling planners and Bayesian methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Combines sparse sampling with bound-directed expansion to achieve provable sample complexity guarantees and practical gains in domains where priors/structure can be exploited.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires optimistic FSSS procedure and accuracy of prior assumptions; bound constants may be large; sensitive to bound estimation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Reinforcement Learning: A Survey', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Bayesian framework for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Efficient Bayes-adaptive reinforcement learning using sample-based search <em>(Rating: 2)</em></li>
                <li>Bayes-adaptive POMDPs <em>(Rating: 2)</em></li>
                <li>Bayesian sparse sampling for on-line reward optimization <em>(Rating: 2)</em></li>
                <li>Near-Bayesian exploration in polynomial time <em>(Rating: 2)</em></li>
                <li>Near-optimal BRL using optimistic local transitions <em>(Rating: 2)</em></li>
                <li>Thompson sampling for complex online problems <em>(Rating: 2)</em></li>
                <li>Approaching Bayes-optimality using Monte-Carlo tree search <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1154",
    "paper_id": "paper-207179119",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Thompson Sampling (TS)",
            "name_full": "Thompson Sampling (Probability Matching)",
            "brief_description": "A Bayesian randomized sampling strategy that at each decision samples a model from the posterior and plays the action that is optimal under that sampled model; used for bandits, contextual bandits and extended to RL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Thompson Sampling (TS)",
            "agent_description": "Posterior-sampling based agent: maintains posterior P_post(θ), samples θ_t ∼ P_post, selects a_t = argmax_a E_{y∼P_θ_t(·|a)}[r(y)], updates posterior with observed outcome. Lightweight; extends to structured/correlated arms and contextual settings.",
            "adaptive_design_method": "Thompson sampling / posterior sampling",
            "adaptation_strategy_description": "Adapts exploration by sampling from the posterior over environment parameters; actions that are more likely to be optimal under the posterior are chosen more often. The posterior is updated with each observation, shifting sampling toward better models.",
            "environment_name": "Stochastic multi-armed bandits; contextual bandits; policy sampling for MDPs (via sampled models)",
            "environment_characteristics": "Unknown reward/outcome distributions; can handle correlated arms and contexts; generally stochastic and possibly structured; in extensions can be partially observable or have model structure.",
            "environment_complexity": "Varies: K arms (finite), contextual dimension d in linear contexts, or structured action sets (e.g., subsets); sample complexity and regret depend on K, d, and problem-dependent gaps Δ_a.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Multiple theoretical guarantees reported: Agrawal & Goyal (2012) give frequentist regret bound E_θ Regret(T) ≤ O( Σ_{a≠a*} (1/Δ_a^2) ln T ); Kaufmann et al. (2012b) give problem-dependent bounds matching Lai-Robbins up to constants for Bernoulli arms; Agrawal & Goyal (2013a) give O(√(K T ln T)) problem-independent bound (Bernoulli+Beta prior). Empirical state-of-the-art in many applications (contextual bandits, online ads).",
            "performance_without_adaptation": null,
            "sample_efficiency": "High: proven logarithmic (problem-dependent) regret O(ln T) asymptotically for standard bandits under suitable priors; problem-independent bounds O(√(KT ln T)).",
            "exploration_exploitation_tradeoff": "Implicitly balanced by posterior uncertainty: sampling variability yields exploration; as posterior concentrates, behavior becomes exploitative.",
            "comparison_methods": "Compared to UCB, Bayes-UCB, frequentist UCB variants, and problem-specific baselines in surveyed works.",
            "key_results": "TS is a simple, general method that yields state-of-the-art empirical performance across bandit and contextual problems; theoretical analyses provide both problem-dependent logarithmic regret bounds and problem-independent √-type bounds; extensions handle correlated actions and structure yielding tighter guarantees.",
            "limitations_or_failures": "Performance can depend strongly on prior (informative priors help; uninformative/misspecified priors can slow learning); analysis often relies on conjugacy or tractable posterior updates; scaling to very large structured RL requires efficient posterior sampling approximations.",
            "uuid": "e1154.0",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Bayes-UCB",
            "name_full": "Bayes-UCB (Bayesian Upper Confidence Bound)",
            "brief_description": "A Bayesian adaptation of UCB that selects at each step the arm whose posterior (1 − β_t)-quantile of the expected reward is largest, implementing an optimism principle via posterior quantiles.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayes-UCB",
            "agent_description": "Maintains posterior over arm reward means; at time t computes posterior (1−β_t)-quantile for each arm and selects the arm with largest quantile; update posterior with observed rewards.",
            "adaptive_design_method": "Bayesian upper-confidence (optimism) via posterior quantiles",
            "adaptation_strategy_description": "Adapts exploration by using wider quantiles early (β_t ~ 1/t) to be optimistic about uncertain arms; as posterior concentrates quantiles tighten and exploitation increases.",
            "environment_name": "Stochastic multi-armed bandits (e.g., Bernoulli arms)",
            "environment_characteristics": "Unknown reward distributions per arm (i.i.d. over time), stochastic, discrete actions, possibly conjugate priors (Beta for Bernoulli).",
            "environment_complexity": "K arms; analysis often in standard finite-arm bandit setting.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "For Bernoulli arms with uniform prior, Kaufmann et al. (2012a) prove a frequentist upper bound on expected regret that matches Lai & Robbins lower bound asymptotically (logarithmic in T).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Asymptotically optimal sample efficiency (logarithmic regret) under standard assumptions and suitable prior.",
            "exploration_exploitation_tradeoff": "Explicit optimism via selecting upper posterior quantile (decreases with t), balancing exploration early and exploitation as uncertainty reduces.",
            "comparison_methods": "Compared conceptually to frequentist UCB algorithms and other Bayesian bandit methods (e.g., Thompson Sampling).",
            "key_results": "Bayes-UCB provides a principled Bayesian optimism mechanism and (for Bernoulli/uniform prior) achieves asymptotically optimal frequentist regret matching classical lower bounds.",
            "limitations_or_failures": "Requires tractable posterior quantile computation; performance depends on prior choice; theoretical results are established under specific distributional/prior choices.",
            "uuid": "e1154.1",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Bayes-Adaptive MDP (BAMDP)",
            "name_full": "Bayes-Adaptive Markov Decision Process (BAMDP)",
            "brief_description": "A formal model that augments physical MDP state with posterior parameters over unknown dynamics (hyper-state), turning RL with model uncertainty into planning in a larger MDP (belief/hyper-state MDP).",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayes-Adaptive MDP (BAMDP) planner",
            "agent_description": "Agent reasons about a hyper-state (s, φ) that combines task state s and posterior counts/parameters φ over transition/reward distributions; planning follows Bellman optimality over hyper-states to trade off immediate reward and information gain.",
            "adaptive_design_method": "Bayesian belief-state planning (Bayes-adaptive planning) / value of information via Bayes-optimal policy",
            "adaptation_strategy_description": "Adapts actions by computing Bayes-optimal policy in hyper-state space: actions chosen to maximize expected discounted reward under posterior over models, implicitly valuing exploration when it improves future returns.",
            "environment_name": "MDPs with unknown transition/reward parameters (discrete S, A)",
            "environment_characteristics": "Unknown transition probabilities represented with Dirichlet priors; discrete state and action spaces; stochastic dynamics; can be partially observable via BAPOMDP extension.",
            "environment_complexity": "Hyper-state space grows combinatorially (|S|^t reachable states after t steps); computationally large except for small problems or structured priors.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "BAMDP defines Bayes-optimal behavior (no single numerical performance since intractable); approximate planners (BAMCP, BFS3, BEB, etc.) aim to approximate BAMDP-optimal performance and show superior sample efficiency in many small benchmark domains (e.g., Chain problem).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Bayes-optimal policies are sample-efficient in theory because they optimally trade exploration and exploitation; practical algorithms approximating BAMDP claim improved sample efficiency versus naive exploration in reported simulations.",
            "exploration_exploitation_tradeoff": "Built-in: Bayes-optimal policy explicitly accounts for expected information gain (posterior evolution) when selecting actions, balancing immediate reward vs information that improves future returns.",
            "comparison_methods": "Forms the basis for comparisons with approximate online planners: Bayesian dynamic programming, BEETLE, BAMCP, BFS3, BEB, BOSS, etc.",
            "key_results": "BAMDP gives a clean theoretical framing: solving the augmented hyper-state MDP yields Bayes-optimal exploration-exploitation behavior; this representation motivates approximate algorithms (sampling/tree search/exploration bonus) to achieve tractable near-Bayes-optimal performance.",
            "limitations_or_failures": "Exact planning is intractable for realistic domains due to exponential hyper-state growth; requires structural assumptions or approximations (sampling, parametric priors, parameter tying) to be tractable.",
            "uuid": "e1154.2",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "BAPOMDP",
            "name_full": "Bayes-Adaptive Partially Observable Markov Decision Process (BAPOMDP)",
            "brief_description": "Extension of BAMDP to POMDPs: hyper-state includes latent state posterior plus posterior over transition and observation models, enabling Bayes-adaptive planning under both model and state uncertainty.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayes-Adaptive POMDP (BAPOMDP) planner",
            "agent_description": "Agent maintains a joint posterior over hidden environment state and model parameters (φ for transitions, ψ for observation model) and plans in the hyper-state space to maximize expected return, including querying/active information actions where applicable.",
            "adaptive_design_method": "Bayes-adaptive planning in partially observable environments; active information acquisition / value of information and planning under posterior beliefs",
            "adaptation_strategy_description": "Adapts action/queries based on joint belief over states and model parameters; can include query/oracle actions (Minerva-style) or optimize Bayes risk to decide when to query versus act.",
            "environment_name": "Partially observable MDPs with unknown transition and/or observation models",
            "environment_characteristics": "Partial observability (observations O), unknown transition kernel P and observation kernel Ω modeled with Dirichlet priors; stochastic; belief states are distributions over (s, φ, ψ).",
            "environment_complexity": "Very large: number of possible beliefs grows exponentially with horizon; planning over these beliefs is computationally challenging even for small base MDPs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Paper reports approximate algorithms (forward search, sampling, Minerva variants) and theoretical approximations; Ross et al. (2011) provide approximation results; empirical success reported in simulated dialogue management when structural constraints applied.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Adaptive Bayes-optimal strategies in BAPOMDPs can be sample-efficient, but computational cost often dominates; approximate methods aim to focus computation on reachable beliefs to improve efficiency.",
            "exploration_exploitation_tradeoff": "Handled in belief-space: actions selected for direct reward or to reduce uncertainty about state/model (information gathering), with Bayes risk and value-of-information criteria used in some algorithms.",
            "comparison_methods": "Compared to Minerva (oracle), Atrash & Pineau (action-oracle), VOI heuristics, and sampling-based planners in the survey.",
            "key_results": "BAPOMDP extends the Bayes-adaptive idea into partial observability, enabling principled tradeoffs between acting and information acquisition; practical methods require strong approximations (bounded counts, sampling, oracles) to be tractable.",
            "limitations_or_failures": "Belief space explosion makes exact Bayes-adaptive planning infeasible for realistic POMDPs; Minerva-style oracle assumptions are unrealistic for many domains; approximate methods can be loose or computationally heavy.",
            "uuid": "e1154.3",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Bayesian Dynamic Programming (Strens)",
            "name_full": "Bayesian Dynamic Programming (model sampling / Thompson-style DP)",
            "brief_description": "Simple practical approach: repeatedly sample a full MDP model from the posterior, solve it (via DP) and act according to the resulting policy for some period, re-sampling as posterior updates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayesian Dynamic Programming (sample-and-solve)",
            "agent_description": "At decision times sample M models from posterior (often just one), compute optimal policy for sampled model using DP, follow it for a while; update posterior from observed transitions and repeat.",
            "adaptive_design_method": "Posterior sampling of full MDP models (a form of Thompson sampling for MDPs)",
            "adaptation_strategy_description": "Adapts by re-sampling models as data arrives; sampled models emphasize regions of current posterior mass leading to exploration when posterior uncertain and exploitation as posterior concentrates.",
            "environment_name": "Discrete MDPs with unknown dynamics (e.g., Chain problem)",
            "environment_characteristics": "Unknown transitions (Dirichlet priors), stochastic; small-to-moderate-sized discrete state/action spaces in typical use.",
            "environment_complexity": "Limited by DP solving step (state space size); feasible for small MDPs or via simulator-based planning.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Convergence to optimal policy is achievable because the method samples from the full posterior; Osband et al. (2013) provide theoretical Bayesian regret analysis for posterior sampling approaches; empirically used as baseline in multiple comparisons.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Reasonably sample-efficient in small domains; empirical performance competitive with BEETLE and BOSS on Chain problem per survey.",
            "exploration_exploitation_tradeoff": "Exploration through variability of sampled models—if the sampled model suggests exploring, agent does so; as posterior narrows, behavior becomes exploitative.",
            "comparison_methods": "Compared against BEETLE, BEB, BFS3, BAMCP and others in the survey's empirical summaries.",
            "key_results": "Simple to implement and conceptually clear; sampling-based DP provides effective exploration without ad-hoc bonuses; theoretical Bayesian regret results support its soundness.",
            "limitations_or_failures": "Computationally expensive due to repeated DP solves; may be slow if re-solving too often; naive single-model sampling can have high variance requiring many resamples or long planning phases.",
            "uuid": "e1154.4",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "BAMCP",
            "name_full": "Bayes-Adaptive Monte-Carlo Planning (BAMCP)",
            "brief_description": "A Monte-Carlo Tree Search algorithm (UCT-style) applied to the BAMDP: sample a model at root, run UCT rollouts using that sampled model (with learned rollouts), and aggregate statistics to select actions in the real environment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "BAMCP",
            "agent_description": "Sampling-based forward-search planner: at each decision sample a full MDP model from the posterior, build/expand a UCT search tree using that model (lazily sampling model transitions), use model-free learning to bias rollouts (ε-greedy), update tree counts and Q-values.",
            "adaptive_design_method": "Monte-Carlo tree search on posterior-sampled models (posterior sampling + UCT) with learned rollouts",
            "adaptation_strategy_description": "Exploration arises from posterior sampling of models; within-tree UCT balances exploration-exploitation of action branches; learned rollout Q-values bias simulation toward promising branches.",
            "environment_name": "Discrete MDPs with unknown dynamics (Chain, other small benchmarks); extended to larger/some structured domains via lazy sampling and rollouts",
            "environment_characteristics": "Unknown transitions (Dirichlet posterior), stochastic; discrete states/actions; planning horizon limited by search depth and rollouts.",
            "environment_complexity": "Search tree size grows with branching factor and depth; complexity controlled by number of simulated rollouts and lazy sampling; applied to small-to-moderate MDPs in experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Survey reports BAMCP empirically outperforms Bayesian Dynamic Programming, Value-of-Information heuristic, BFS3, BEB and SmartSampler on several simulated domains (including Chain); proven to converge in probability to Bayes-optimal policy; UCT-based error decreases as O(log n / n) with n visits.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Empirically more sample-efficient than several prior sampling/search-based planners in small domains; efficiency scales with number of simulation rollouts allocated.",
            "exploration_exploitation_tradeoff": "Two-level: posterior model sampling provides high-level exploration; UCT inside tree balances exploration of action branches; rollout learning biases simulations to exploit promising areas.",
            "comparison_methods": "Compared to Bayesian Dynamic Programming, VOI, BFS3, BEB, SmartSampler in survey summaries.",
            "key_results": "BAMCP is a practical near-Bayes-optimal planner that combines posterior sampling and MCTS; it leverages learned rollouts to bias search and empirically attains strong performance on canonical BRL benchmarks.",
            "limitations_or_failures": "Computational cost of many rollouts/search nodes can be high; performance depends on quality of rollout policy and simulation budget; still limited to problems where MCTS is tractable.",
            "uuid": "e1154.5",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Bayesian Sparse Sampling",
            "name_full": "Bayesian Sparse Sampling (Wang et al.)",
            "brief_description": "An adaptation of Sparse Sampling to BAMDPs: stochastic forward sampling of actions and next-states according to posteriors to estimate long-term values with Monte-Carlo trees focused on likely/higher-value branches.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayesian Sparse Sampling",
            "agent_description": "Forward-search Monte-Carlo planner that grows a stochastic tree by sampling actions according to their probability of being optimal (from Q-value posteriors) and sampling next states from the posterior over transitions, using Monte-Carlo estimates to evaluate actions.",
            "adaptive_design_method": "Sparse Monte-Carlo forward search with posterior-driven sampling (Thompson-style selection in tree growth)",
            "adaptation_strategy_description": "Tree growth focuses computational effort on branches that are more likely or promising according to current posterior/Q-value uncertainty; sampling from posterior naturally explores.",
            "environment_name": "Small BAMDPs / discrete MDPs with unknown transitions and Dirichlet priors",
            "environment_characteristics": "Unknown stochastic transitions; discrete state/action spaces; used for small MDPs (e.g., Chain) in experiments.",
            "environment_complexity": "Sparse sampling reduces dependence on number of states but planning horizon and branching factor still limit scalability; practical only for small-to-moderate problems.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Provides unbiased/low-error estimates with high probability using a number of samples independent of |S| (theoretical guarantee for sparse-sampling); empirically applied to small MDPs.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Can achieve low error with number of samples independent of number of states, but exponential in planning horizon; practical sample efficiency depends on horizon and branching choices.",
            "exploration_exploitation_tradeoff": "Exploration arises from stochastic sampling of actions according to posterior likelihood of being optimal; exploitation via focus on high-probability/high-value branches.",
            "comparison_methods": "Related to forward-search and BAMCP approaches; compared conceptually with methods that consider all successor states.",
            "key_results": "Adapts sparse-sampling ideas to Bayesian setting to focus computation on probable/higher-value parts of hyper-state tree, yielding tractable approximate Bayes planning in small domains.",
            "limitations_or_failures": "Exponential dependence on planning horizon remains; requires many posterior samples per node which can be computationally heavy; so far applied to small MDPs only.",
            "uuid": "e1154.6",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Value-of-Information (VOI) heuristic (Dearden et al.)",
            "name_full": "Value of Information (VOI) heuristic for exploration",
            "brief_description": "A myopic Bayesian heuristic that selects actions by adding a 1-step estimate of the expected value of information (VPI) to expected immediate reward to trade off exploration vs exploitation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Value-of-Information (VOI) heuristic",
            "agent_description": "Agent samples MDPs from Dirichlet posterior, solves them to estimate distribution over Q*, computes expected value of perfect information (VPI) for each action myopically, and selects action maximizing E[Q]+VPI.",
            "adaptive_design_method": "Value of information / myopic information-gain augmented action selection",
            "adaptation_strategy_description": "Selects actions based on immediate reward plus estimated one-step VPI—actions that could improve policy (via information) are favored even if immediate reward is lower.",
            "environment_name": "Discrete MDPs with Dirichlet priors on transitions (e.g., Chain)",
            "environment_characteristics": "Unknown transition dynamics; stochastic; small discrete state/action spaces in experiments.",
            "environment_complexity": "Myopic 1-step VPI simplifies computation, scalable by sampling count M (adjustable), but still costly due to solving sampled MDPs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically attractive and scalable by varying number of samples; shown useful in small domains, but myopic nature can limit long-term benefit.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Adjustable via number of sampled MDPs; can be sample-efficient if limited samples still capture key uncertainties, but myopic.",
            "exploration_exploitation_tradeoff": "Explicitly trades exploration by adding VPI term to immediate reward; exploration encouraged when expected policy improvement is high.",
            "comparison_methods": "Compared conceptually and empirically to full Bayes-optimal planning (infeasible) and to sampling-based planners in survey.",
            "key_results": "Provides a computationally tractable, intuitive heuristic for exploration that accounts for information value and can be scaled by sampling budget.",
            "limitations_or_failures": "Myopic (1-step) approximation can miss long-term informational benefits; may underperform in domains where multi-step information acquisition is needed.",
            "uuid": "e1154.7",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "BEETLE",
            "name_full": "BEETLE (Bayesian Exploration Exploitation Tradeoff in LEarning)",
            "brief_description": "An offline approximate BAMDP solver that adapts POMDP point-based planning (Perseus) to the Bayes-adaptive case by sampling hyper-states and representing α-functions over posterior parameters.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "BEETLE",
            "agent_description": "Samples hyper-states (s, φ) from interactions, constructs α-functions represented as linear combinations of basis functions over transition-function parameters, and applies point-based backups to approximate Bayes-optimal value.",
            "adaptive_design_method": "Point-based Bayesian planning / offline approximation of Bayes-optimal policy over sampled hyper-states",
            "adaptation_strategy_description": "Learns α-function basis over sampled hyper-states; policy implicitly values information via BAMDP formulation; offline computation produces policy mapping beliefs to actions.",
            "environment_name": "BAMDPs / small discrete MDPs and POMDP-like hyper-state spaces",
            "environment_characteristics": "Unknown transitions (Dirichlet priors), possibly partial observability in extensions; small to moderate discrete spaces in evaluated experiments.",
            "environment_complexity": "Computationally demanding offline; tractable if number of unknown parameters limited or parameter tying used.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Tested experimentally in small simulation domains; performance comparable to BOSS and Bayesian dynamic programming on Chain problem in survey summary.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Can be sample-efficient if basis functions capture posterior structure; offline cost may be high.",
            "exploration_exploitation_tradeoff": "Implicit in Bayes-optimal objective approximated via α-functions over belief/posterior space; exploration is driven by value of future information.",
            "comparison_methods": "Compared empirically to Bayesian dynamic programming, BOSS, BEB in survey.",
            "key_results": "Adapts point-based POMDP techniques to approximate Bayes-adaptive planning; can exploit structure to scale somewhat but remains computationally heavy.",
            "limitations_or_failures": "Number of α-function terms can grow exponentially with horizon; requires projection/pruning and structural assumptions (parameter tying) to scale.",
            "uuid": "e1154.8",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "BOSS",
            "name_full": "BOSS (Best of Sampled Sets)",
            "brief_description": "A model-based Bayesian RL algorithm that samples multiple models from the posterior, constructs a hyper-model combining sampled models' actions, solves it, and uses the resulting policy for a while; provable PAC-like guarantees.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "BOSS",
            "agent_description": "Draws K sampled MDPs from posterior, creates a hyper-model with K|A| actions (each sample-action pair), solves hyper-model via DP to decide real action, repeats sampling after a knownness threshold per state-action.",
            "adaptive_design_method": "Posterior model sampling combined with optimistic planning over sampled ensemble (optimism in the face of uncertainty)",
            "adaptation_strategy_description": "Adapts by increasing knownness counts and sampling more models for uncertain (s,a); hyper-model encourages optimism by picking an action that is good in at least one sampled plausible model.",
            "environment_name": "Discrete MDPs with Dirichlet priors on transitions (Chain, small mazes)",
            "environment_characteristics": "Unknown transitions, stochastic; discrete states/actions; can exploit structure (parameter tying) to scale.",
            "environment_complexity": "Empirically applied to Chain and small mazes (e.g., 6×6); theoretical guarantees require polynomial number of samples depending on |S|, |A|, knownness parameter.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical results show BOSS performs similarly to BEETLE and Bayesian dynamic programming on Chain; theoretical PAC-style bounds (Asmuth et al., 2009) guarantee near-optimal value except for polynomial number of steps.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Provable polynomial sample complexity to reach near-optimal performance with high probability (depends on knownness parameter B and |S|, |A|).",
            "exploration_exploitation_tradeoff": "Optimism via sampling: hyper-model contains optimistic actions from sampled MDPs, encouraging exploration of uncertain (s,a) combinations until known.",
            "comparison_methods": "Compared to BEETLE, Bayesian DP, SmartSampler, BEB in survey.",
            "key_results": "Simple sampling-based construction with PAC-style guarantees; can exploit posterior structure and parameter tying to tackle larger state spaces than naive approaches.",
            "limitations_or_failures": "Hyper-model can grow large; number of sampled models required depends on posterior variance; computational cost of solving hyper-models can be high.",
            "uuid": "e1154.9",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "BEB",
            "name_full": "BEB (Bayesian Exploration Bonus)",
            "brief_description": "A simple model-based algorithm that adds an exploration bonus (function of posterior counts) to the reward and solves the resulting optimistic MDP greedily; designed to achieve PAC-BAMDP guarantees.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "BEB",
            "agent_description": "Modifies reward with additive bonus β/(1 + n_{s,a}), where n_{s,a} is posterior count for (s,a); solves modified MDP greedily to choose actions; bonus decays faster (1/n) than some PAC-MDP bonuses.",
            "adaptive_design_method": "Exploration bonus on rewards based on posterior counts (optimism via reward shaping)",
            "adaptation_strategy_description": "As counts n_{s,a} increase, bonus decays and policy becomes exploitative; early on, bonus encourages exploring state-action pairs with high posterior uncertainty.",
            "environment_name": "Discrete MDPs with Dirichlet priors (e.g., Chain problem)",
            "environment_characteristics": "Unknown transitions; stochastic; discrete states/actions; reward function assumed known in many analyses (extensions handle unknown rewards).",
            "environment_complexity": "Typical small-to-moderate MDPs used in experiments (Chain); theoretical bounds depend on |S|, |A| and horizon H.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theorem: with β chosen appropriately, with high probability V^{A_t}(s_t, φ_t) ≥ V^*(s_t, φ_t) − ε for all but m=O(|S||A| H^6 ε^{-2} log(|S||A|/δ)) time steps. Empirically BEB outperforms non-Bayesian PAC-MDP algorithms on Chain and can find the correct optimal policy.",
            "performance_without_adaptation": null,
            "sample_efficiency": "PAC-BAMDP guarantees imply polynomial sample complexity to achieve ε-close to Bayesian-optimal value; bonus decays as O(1/n) enabling faster discard of exploration than some PAC-MDP methods.",
            "exploration_exploitation_tradeoff": "Explicit via exploration bonus: optimistic reward pushes exploration early; bonus decays with data shifting towards exploitation.",
            "comparison_methods": "Compared to BOSS, Bayesian DP, BEETLE, MBIE-EB in survey; BEB often more sample-efficient towards Bayesian optimality.",
            "key_results": "BEB achieves PAC-style guarantees with respect to Bayesian optimality and empirically is sample-efficient on canonical tasks; exploration bonus designed from Bayesian posterior yields fast decay.",
            "limitations_or_failures": "Theoretical optimality is with respect to Bayesian-optimal policy (not necessarily true environment optimal); may never find true optimal policy in some MDPs despite close Bayesian-optimal performance (theoretical counterexamples exist).",
            "uuid": "e1154.10",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "VBRB",
            "name_full": "Variance-Based Reward Bonus (VBRB)",
            "brief_description": "An exploration-bonus approach where the bonus is proportional to posterior variance of transition and reward parameters, yielding PAC-style guarantees via Chebyshev-based analysis.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Variance-Based Reward Bonus (VBRB)",
            "agent_description": "Augments reward with bonus R_bonus(s,φ,a)=β_R σ_R(s,φ,a) + β_P Σ_s σ_P(s|s,φ,a) where σ terms are posterior variances of reward and transition probabilities; selects greedy action w.r.t. augmented reward.",
            "adaptive_design_method": "Variance-based exploration bonus derived from posterior uncertainty",
            "adaptation_strategy_description": "Actions with higher posterior variance (uncertainty) receive larger bonus and are explored; as posterior variance decreases with data, bonus decays and exploitation increases.",
            "environment_name": "Discrete MDPs with Dirichlet priors on transitions and possibly uncertain rewards (Chain, grid-world/Wumpus variants)",
            "environment_characteristics": "Unknown transitions and possibly rewards; stochastic; discrete states/actions.",
            "environment_complexity": "Applied to Chain problem variants and 4×4 grid-world; complexity similar to BEB/BOSS settings.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical PAC-style sample complexity bounds provided: follows near-optimal policy on all but O( Σ_{s,a} C(s,a) / (1−γ)^2 log(1/δ) ) steps with high probability. Empirically competitive with BEETLE, BOSS, BEB and superior on a Wumpus-inspired 4×4 grid when modeling independent Dirichlet priors.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Sample complexity decreases O(1/√n) with number of observations per (s,a) in Dirichlet case; comparable to other PAC-MDP algorithms.",
            "exploration_exploitation_tradeoff": "Exploration directed by posterior variance: the higher the uncertainty, the larger the bonus and the more likely the agent explores that (s,a).",
            "comparison_methods": "Compared empirically to BEETLE, BOSS, BEB in survey experiments.",
            "key_results": "Variance-based bonuses yield analyzable PAC-style guarantees and can perform well empirically, especially when model uncertainty is widespread (no parameter tying).",
            "limitations_or_failures": "Bonus scale parameters must be set appropriately; performance may degrade if underlying assumptions on posterior variance estimates are violated or priors are misspecified.",
            "uuid": "e1154.11",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "BOLT / POT",
            "name_full": "BOLT (Bayesian Optimistic Local Transitions) / POT (Probably Optimistic Transitions)",
            "brief_description": "BOLT places optimism on transition probabilities (imagined transitions) by extending the action space; POT adapts the optimism parameter online using UCB-like criteria to be 'probably optimistic'.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "BOLT (and POT)",
            "agent_description": "BOLT augments each action with a target next-state σ and constructs optimistic transition probabilities by imagining η extra transitions to σ; solves the extended optimistic MDP. POT adapts η per (s,a) using UCB-like variance estimates.",
            "adaptive_design_method": "Optimistic transitions (optimism in model space) via imaginary transitions and UCB-style adaptive optimism (POT)",
            "adaptation_strategy_description": "BOLT uses fixed optimism parameter η to bias transition probability toward chosen successor states (encouraging exploring promising transitions); POT estimates optimism per (s,a) online based on current posterior variance and UCB criteria, adapting exploration pressure.",
            "environment_name": "Discrete MDPs with Dirichlet posteriors (Chain problem and variants)",
            "environment_characteristics": "Unknown stochastic transitions, discrete states/actions; tested on Chain and other small domains.",
            "environment_complexity": "Comparable to BEB/BOSS settings; extended action space size grows by |S| factor due to optimistic actions ζ=(a,σ).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical: BOLT can be outperformed by well-tuned BEB but is more robust to parameter choice; POT shows improved empirical performance over BOLT, BEB, VBRB when prior is good but can suffer under misspecified priors. Theoretical bounds: BOLT is ε-close to Bayesian-optimal except for O(|S||A|η^2 / (1−γ)^2) steps.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Reasonable; POT improves efficiency by adapting optimism to observed variance. Exact sample counts problem-dependent.",
            "exploration_exploitation_tradeoff": "Explicit optimism in transitions pushes the agent to try under-explored successor outcomes; POT tunes optimism to avoid over/under exploration.",
            "comparison_methods": "Compared to BEB, VBRB, and other exploration-bonus algorithms in survey.",
            "key_results": "Optimism placed on transitions is an effective and interpretable way to encourage exploration; adapting optimism online (POT) can improve robustness and sample-efficiency when priors are informative.",
            "limitations_or_failures": "Performance sensitive to optimism parameters and prior correctness; POT may suffer when prior is misspecified; extended action space increases computational cost.",
            "uuid": "e1154.12",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "BFS3",
            "name_full": "BFS3 (Bayesian Forward Search Sparse Sampling)",
            "brief_description": "An online forward-search sparse sampling algorithm for BAMDPs that maintains upper and lower bounds and directs rollouts to nodes with largest uncertainty gap, with PAC-style guarantees under assumptions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "BFS3",
            "agent_description": "Builds a sparse forward-search tree over hyper-states (s, φ), maintaining lower/upper value bounds, and guides expansion to nodes maximizing uncertainty-weighted gaps (U−L)/n to efficiently reduce value uncertainty.",
            "adaptive_design_method": "Forward-search sparse sampling with bound-directed expansion (optimism and uncertainty reduction)",
            "adaptation_strategy_description": "Expands tree nodes where upper-lower bound gaps are largest relative to visitation counts, concentrating computation/exploration on uncertain but influential hyper-states; posterior sampling provides stochasticity.",
            "environment_name": "BAMDPs / discrete MDPs with unknown parameters",
            "environment_characteristics": "Unknown transitions/rewards; Dirichlet priors; stochastic; finite state/action sets in analysis.",
            "environment_complexity": "Can handle much larger effective state spaces with parameter tying and structure; surveyed claim: up to 10^16 states with parameter tying in some experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theorem (Asmuth, 2013): with high probability BFS3 takes at most polynomial number of sub-Bayes-optimal actions (bound provided) under assumptions on prior accuracy and FSSS optimism. Empirically solves small domains (e.g., 5×5 grid) more effectively than some non-Bayesian methods.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Polynomial sample complexity under assumptions; practical efficiency depends on pruning and bound tightness.",
            "exploration_exploitation_tradeoff": "Guided by maintained value bounds: exploration directed to branches where uncertainty could change decision; exploitation where lower-bound already high.",
            "comparison_methods": "Compared conceptually/empirically to RMAX, other sparse sampling planners and Bayesian methods.",
            "key_results": "Combines sparse sampling with bound-directed expansion to achieve provable sample complexity guarantees and practical gains in domains where priors/structure can be exploited.",
            "limitations_or_failures": "Requires optimistic FSSS procedure and accuracy of prior assumptions; bound constants may be large; sensitive to bound estimation quality.",
            "uuid": "e1154.13",
            "source_info": {
                "paper_title": "Bayesian Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "2016-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Bayesian framework for reinforcement learning",
            "rating": 2,
            "sanitized_title": "a_bayesian_framework_for_reinforcement_learning"
        },
        {
            "paper_title": "Efficient Bayes-adaptive reinforcement learning using sample-based search",
            "rating": 2,
            "sanitized_title": "efficient_bayesadaptive_reinforcement_learning_using_samplebased_search"
        },
        {
            "paper_title": "Bayes-adaptive POMDPs",
            "rating": 2,
            "sanitized_title": "bayesadaptive_pomdps"
        },
        {
            "paper_title": "Bayesian sparse sampling for on-line reward optimization",
            "rating": 2,
            "sanitized_title": "bayesian_sparse_sampling_for_online_reward_optimization"
        },
        {
            "paper_title": "Near-Bayesian exploration in polynomial time",
            "rating": 2,
            "sanitized_title": "nearbayesian_exploration_in_polynomial_time"
        },
        {
            "paper_title": "Near-optimal BRL using optimistic local transitions",
            "rating": 2,
            "sanitized_title": "nearoptimal_brl_using_optimistic_local_transitions"
        },
        {
            "paper_title": "Thompson sampling for complex online problems",
            "rating": 2,
            "sanitized_title": "thompson_sampling_for_complex_online_problems"
        },
        {
            "paper_title": "Approaching Bayes-optimality using Monte-Carlo tree search",
            "rating": 2,
            "sanitized_title": "approaching_bayesoptimality_using_montecarlo_tree_search"
        }
    ],
    "cost": 0.039306999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bayesian Reinforcement Learning: A Survey
2015</p>
<p>M Ghavamzadeh 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>S Mannor 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>J Pineau jpineau@cs.mcgill.ca 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>A Tamar 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>Mohammad Ghavamzadeh mohammad.ghavamzadeh@inria.fr 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>Shie Mannor 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>Joelle Pineau 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>Aviv Tamar avivt@berkeley.edu 
Adobe Research &amp; INRIA
McGill University
University of California
Berkeley</p>
<p>Bayesian Reinforcement Learning: A Survey</p>
<p>Foundations and Trends R in Machine Learning
85201510.1561/2200000049
Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.</p>
<p>Introduction</p>
<p>A large number of problems in science and engineering, from robotics to game playing, tutoring systems, resource management, financial portfolio management, medical treatment design and beyond, can be characterized as sequential decision-making under uncertainty. Many interesting sequential decision-making tasks can be formulated as reinforcement learning (RL) problems [Bertsekas andTsitsiklis, 1996, Sutton and]. In an RL problem, an agent interacts with a dynamic, stochastic, and incompletely known environment, with the goal of finding an action-selection strategy, or policy, that optimizes some long-term performance measure.</p>
<p>One of the key features of RL is the focus on learning a control policy to optimize the choice of actions over several time steps. This is usually learned from sequences of data. In contrast to supervised learning methods that deal with independently and identically distributed (i.i.d.) samples from the domain, the RL agent learns from the samples that are collected from the trajectories generated by its sequential interaction with the system. Another important aspect is the effect of the agent's policy on the data collection; different policies naturally yield different distributions of sampled trajectories, and thus, impact-ing what can be learned from the data.</p>
<p>Traditionally, RL algorithms have been categorized as being either model-based or model-free. In the former category, the agent uses the collected data to first build a model of the domain's dynamics and then uses this model to optimize its policy. In the latter case, the agent directly learns an optimal (or good) action-selection strategy from the collected data. There is some evidence that the first method provides better results with less data [Atkeson and Santamaria, 1997], and the second method may be more efficient in cases where the solution space (e.g., policy space) exhibits more regularity than the underlying dynamics, though there is some disagreement about this,.</p>
<p>A major challenge in RL is in identifying good data collection strategies, that effectively balance between the need to explore the space of all possible policies, and the desire to focus data collection towards trajectories that yield better outcome (e.g., greater chance of reaching a goal, or minimizing a cost function). This is known as the explorationexploitation tradeoff. This challenge arises in both model-based and model-free RL algorithms.</p>
<p>Bayesian reinforcement learning (BRL) is an approach to RL that leverages methods from Bayesian inference to incorporate information into the learning process. It assumes that the designer of the system can express prior information about the problem in a probabilistic distribution, and that new information can be incorporated using standard rules of Bayesian inference. The information can be encoded and updated using a parametric representation of the system dynamics, in the case of model-based RL, or of the solution space, in the case of model-free RL.</p>
<p>A major advantage of the BRL approach is that it provides a principled way to tackle the exploration-exploitation problem. Indeed, the Bayesian posterior naturally captures the full state of knowledge, subject to the chosen parametric representation, and thus, the agent can select actions that maximize the expected gain with respect to this information state.</p>
<p>Another major advantage of BRL is that it implicitly facilitates regularization. By assuming a prior on the value function, the parameters defining a policy, or the model parameters, we avoid the trap of letting a few data points steer us away from the true parameters. On the other hand, having a prior precludes overly rapid convergence. The role of the prior is therefore to soften the effect of sampling a finite dataset, effectively leading to regularization. We note that regularization in RL has been addressed for the value function [Farahmand et al., 2008b] and for policies [Farahmand et al., 2008a]. A major issue with these regularization schemes is that it is not clear how to select the regularization coefficient. Moreover, it is not clear why an optimal value function (or a policy) should belong to some pre-defined set.</p>
<p>Yet another advantage of adopting a Bayesian view in RL is the principled Bayesian approach for handling parameter uncertainty. Current frequentist approaches for dealing with modelling errors in sequential decision making are either very conservative, or computationally infeasible [Nilim and El Ghaoui, 2005]. By explicitly modelling the distribution over unknown system parameters, Bayesian methods offer a promising approach for solving this difficult problem.</p>
<p>Of course, several challenges arise in applying Bayesian methods to the RL paradigm. First, there is the challenge of selecting the correct representation for expressing prior information in any given domain. Second, defining the decision-making process over the information state is typically computationally more demanding than directly considering the natural state representation. Nonetheless, a large array of models and algorithms have been proposed for the BRL framework, leveraging a variety of structural assumptions and approximations to provide feasible solutions.</p>
<p>The main objective of this paper is to provide a comprehensive survey on BRL algorithms and their theoretical and empirical properties. In Chapter 2, we provide a review of the main mathematical concepts and techniques used throughout this paper. Chapter 3 surveys the Bayesian learning methods for the case of single-step decision-making, using the bandit framework. This section serves both as an exposition of the potential of BRL in a simpler setting that is well understood, but is also of independent interest, as bandits have widespread applications. The main results presented here are of a theoretical nature, outlin-ing known performance bounds for the regret minimization criteria. Chapter 4 reviews existing methods for model-based BRL, where the posterior is expressed over parameters of the system dynamics model. Chapter 5 focuses on BRL methods that do not explicitly learn a model of the system, but rather the posterior is expressed over the solution space. Chapter 6 focuses on a particular advantage of BRL in dealing with risk due to parameter-uncertainty, and surveys several approaches for incorporating such risk into the decision-making process. Finally, Chapter 7 discusses various extensions of BRL for special classes of problems (PAC-Bayes model selection, inverse RL, multi-agent RL, and multi-task RL). Figure 1.1 outlines the various BRL approaches covered throughout the paper.</p>
<p>An Example Domain</p>
<p>We present an illustrative domain suitable to be solved using the BRL techniques surveyed in this paper. This running example will be used throughout the paper to elucidate the difference between the various BRL approaches and to clarify various BRL concepts.</p>
<p>Example 1.1 (The Online Shop). In the online shop domain, a retailer aims to maximize profit by sequentially suggesting products to online shopping customers. Formally, the domain is characterized by the following model:</p>
<p>• A set of possible customer states, X . States can represent intrinsic features of the customer such as gender and age, but also dynamic quantities such as the items in his shopping cart, or his willingness to shop;</p>
<p>• A set of possible product suggestions and advertisements, A;</p>
<p>• A probability kernel, P , defined below.</p>
<p>An episode in the online shop domain begins at time t = 0, when a customer with features x 0 ∈ X enters the online shop. Then, a sequential interaction between the customer and the online shop begins, where at each step t = 0, 1, 2, . . . , an advertisement a t ∈ A is shown Model--free BRL (Sec 5) to the customer, and following that the customer makes a decision to either (i) add a product to his shopping cart; (ii) not buy the product, but continue to shop; (iii) stop shopping and check out. Following the customers decision, his state changes to x t+1 (reflecting the change in the shopping cart, willingness to continue shopping, etc.). We assume that this change is captured by a probability kernel P ( x t+1 | x t , a t ).</p>
<p>When the customer decides to check out, the episode ends, and a profit is obtained according to the items he had added to his cart. The goal is to find a product suggestion policy, x → a ∈ A, that maximizes the expected total profit.</p>
<p>When the probabilities of customer responses P are known in advance, calculating an optimal policy for the online shop domain is basically a planning problem, which may be solved using traditional methods for resource allocation [Powell, 2011]. A more challenging, but realistic, scenario is when P is not completely known beforehand, but has to be learned while interacting with customers. The BRL framework employs Bayesian methods for learning P , and for learning an optimal product suggestion policy.</p>
<p>There are several advantages for choosing a Bayesian approach for the online shop domain. First, it is likely that some prior knowledge about P is available. For example, once a customer adds a product of a particular brand to his cart, it is likely that he prefers additional products of the same brand over those of a different one. Taking into account such knowledge is natural in the Bayesian method, by virtue of the prior distribution over P . As we shall see, the Bayesian approach also naturally extends to more general forms of structure in the problem.</p>
<p>A second advantage concerns what is known as the exploitationexploration dilemma: should the decision-maker display only the most profitable product suggestions according to his current knowledge about P , or rather take exploratory actions that may turn out to be less profitable, but provide useful information for future decisions? The Bayesian method offers a principled approach to dealing with this difficult problem by explicitly quantifying the value of exploration, made possible by maintaining a distribution over P .</p>
<p>The various parameter configurations in the online shop domain lead to the different learning problems surveyed in this paper. In particular:</p>
<p>• For a single-step interaction, i.e., when the episode terminates after a single product suggestion, the problem is captured by the multi-armed bandit model of Chapter 3.</p>
<p>• For small-scale problems, i.e., a small number of products and customer types, P may be learnt explicitly. This is the modelbased approach of Chapter 4.</p>
<p>• For large problems, a near-optimal policy may be obtained without representing P explicitly. This is the model-free approach of Chapter 5.</p>
<p>• When the customer state is not fully observed by the decisionmaker, we require models that incorporate partial observability; see §2.3 and §4.9.</p>
<p>Throughout the paper, we revisit the online shop domain, and specify explicit configurations that are relevant to the surveyed methods.</p>
<p>Technical Background</p>
<p>In this section we provide a brief overview of the main concepts and introduce the notations used throughout the paper. We begin with a quick review of the primary mathematical tools and algorithms leveraged in the latter sections to define BRL models and algorithms.</p>
<p>Multi-Armed Bandits</p>
<p>As was previously mentioned, a key challenge in sequential decisionmaking under uncertainty is the exploration/exploitation dilemma: the tradeoff between either taking actions that are most rewarding according to the current state of knowledge, or taking exploratory actions, which may be less immediately rewarding, but may lead to better informed decisions in the future.</p>
<p>The stochastic multi-armed bandit (MAB) problem is perhaps the simplest model of the exploration/exploitation tradeoff. Originally formulated as the problem of a gambler choosing between different slot machines in a casino ('one armed bandits'), the stochastic MAB model features a decision-maker that sequentially chooses actions a t ∈ A, and observes random outcomes Y t (a t ) ∈ Y at discrete time steps t = 1, 2, 3, . . .. A known function r : Y → R associates the random outcome to a reward, which the agent seeks to maximize. The outcomes {Y t (a)} are drawn i.i.d. over time from an unknown probability distribution P (·|a) ∈ P (Y), where P(Y) denotes the set of probability distributions on (Borel) subsets of Y (see Bubeck and Cesa-Bianchi [2012] for reference).</p>
<p>Model 1 (Stochastic K-Armed Bandit) Define a K-MAB to be a tuple A, Y, P, r where • A is the set of actions (arms), and |A| = K, • Y is the set of possible outcomes, • P (·|a) ∈ P(Y) is the outcome probability, conditioned on action a ∈ A being taken, • r(Y ) ∈ R represents the reward obtained when outcome Y ∈ Y is observed.</p>
<p>A rule that prescribes to the agent which actions to select, or policy, is defined as a mapping from past observations to a distribution over the set of actions. Since the probability distributions are initially unknown, the decision-maker faces a tradeoff between exploitation, i.e., selecting the arm she believes is optimal, or making exploratory actions to gather more information about the true probabilities. Formally, this tradeoff is captured by the notion of regret: Definition 2.1 (Regret). Let a * ∈ arg max a∈A E y∼P (·|a) r(y) denote the optimal arm. The T -period regret of the sequence of actions a 1 , . . . , a T is the random variable
Regret(T ) = T t=1 r Y t (a * ) − r Y t (a t ) .
Typically, MAB algorithms focus on the expected regret, E Regret(T ) , and provide policies that are guaranteed to keep it small in some sense.</p>
<p>As an illustration of the MAB model, let us revisit the online shop example.</p>
<p>Example 2.1 (Online shop -bandit setting). Recall the online shop domain of Example 1.1. In the MAB setting, there is no state information about the customers, i.e., X = ∅. In addition, each interaction lasts a single time step, after which the customer checks out, and a profit is obtained according to his purchasing decision. The regret minimization problem corresponds to determining which sequence of advertisements a 1 , . . . , a T to show to a stream of T incoming customers, such that the total revenue is close to that which would have been obtained with the optimal advertisement stream.</p>
<p>In some cases, the decision-maker may have access to some additional information that is important for making decisions, but is not captured in the MAB model. For example, in the online shop domain of Example 2.1, it is likely that some information about the customer, such as his age or origin, is available. The contextual bandit model (a.k.a. associative bandits, or bandits with side information) is an extension of the MAB model that takes such information into account.</p>
<p>The decision-making process in the contextual bandit model is similar to the MAB case. However, at each time step t, the decision maker first observes a context s t ∈ S, drawn i.i.d. over time from a probability distribution P S (·) ∈ P(S), where P(S) denotes the set of probability distributions on (Borel) subsets of S. The decision-maker then chooses an action a t ∈ A and observes a random outcome Y t (a t , s t ) ∈ Y, which now depends both on the context and the action. The outcomes {Y t (a)} are drawn i.i.d. over time from an unknown probability distribution P (·|a, s) ∈ P (Y), where P(Y) denotes the set of probability distributions on (Borel) subsets of Y.</p>
<p>Model 2 (Contextual Bandit) Define a contextual bandit to be a tuple S, A, Y, P S , P, r where • S is the set of contexts, • A is the set of actions (arms), • Y is the set of possible outcomes, • P S (·) ∈ P(S) is the context probability, • P (·|a, s) ∈ P(Y) is the outcome probability, conditioned on action a ∈ A being taken when the context is s ∈ S, • r(Y ) ∈ R represents the reward obtained when outcome Y ∈ Y is observed.</p>
<p>The Markov decision process model, as presented in the following section, may be seen as an extension of the contextual bandit model to a sequential decision-making model, in which the context is no longer i.i.d., but may change over time according to the selected actions.</p>
<p>Markov Decision Processes</p>
<p>The Markov Decision Process (MDP) is a framework for sequential decision-making in Markovian dynamical systems [Bellman, 1957, Puterman, 1994. It can be seen as an extension of the MAB framework by adding the notion of a system state, that may dynamically change according to the performed actions and affects the outcomes of the system.</p>
<p>Model 3 (Markov Decision Process)</p>
<p>Define an MDP M to be a tuple S, A, P, P 0 , q where • S is the set of states, • A is the set of actions, • P (·|s, a) ∈ P(S) is the probability distribution over next states, conditioned on action a being taken in state s, • P 0 ∈ P(S) is the probability distribution according to which the initial state is selected, • R(s, a) ∼ q(·|s, a) ∈ P(R) is a random variable representing the reward obtained when action a is taken in state s.</p>
<p>Let P(S), P(A), and P(R) be the set of probability distributions on (Borel) subsets of S, A, and R respectively. 1 We assume that P , P 0 and q are stationary. Throughout the paper, we use upper-case and lower-case letters to refer to random variables and the values taken by random variables, respectively. For example, R(s, a) is the random variable of the immediate reward, and r(s, a) is one possible realization of this random variable. We denote the expected value of R(s, a), as r(s, a) = r q(dr|s, a).</p>
<p>Assumption A1 (MDP Regularity)</p>
<p>We assume that the random immediate rewards are bounded by R max and the expected immediate rewards are bounded byR max . Note thatR max ≤ R max .</p>
<p>A rule according to which the agent selects its actions at each possible state, or policy, is defined as a mapping from past observations to a distribution over the set of actions. A policy is called Markov if the distribution depends only on the last state of the observation sequence. A policy is called stationary if it does not change over time. A stationary Markov policy µ(·|s) ∈ P(A) is a probability distribution over the set of actions given a state s ∈ S. A policy is deterministic if the probability distribution concentrates on a single action for all histories. A deterministic policy is identified by a mapping from the set of states to the set of actions, i.e., µ : S → A. In the rest of the paper, we use the term policy to refer to stationary Markov policies.</p>
<p>The MDP controlled by a policy µ induces a Markov chain M µ with reward distribution q µ (·|s) = q · |s, µ(s) such that R µ (s) = R s, µ(s) ∈ q µ (·|s), transition kernel P µ (·|s) = P · |s, µ(s) , and stationary distribution over states π µ (if it admits one). In a Markov chain M µ , for state-action pairs z = (s, a) ∈ Z = S × A, we define the transition density and the initial (state-action) density as P µ (z |z) = P (s |s, a)µ(a |s ) and P µ 0 (z 0 ) = P 0 (s 0 )µ(a 0 |s 0 ), respectively. We generically use ξ = {z 0 , z 1 , . . . , z T } ∈ Ξ, T ∈ {0, 1, . . . , ∞}, to denote a path (or trajectory) generated by this Markov chain. The probability (density) of such a path is given by
Pr(ξ|µ) = P µ 0 (z 0 ) T t=1 P µ (z t |z t−1 ).
We define the (possibly discounted, γ ∈ [0, 1]) return of a path as a function ρ : Ξ → R, ρ(ξ) = T t=0 γ t R(z t ). For each path ξ, its (discounted) return, ρ(ξ), is a random variable with the expected valuē ρ(ξ) = T t=0 γ tr (z t ). 2 Here, γ ∈ [0, 1] is a discount factor that determines the exponential devaluation rate of delayed rewards. 3 The expected return of a policy µ is defined by
η(µ) = E ρ(ξ) = Ξρ (ξ) Pr(ξ|µ)dξ.
(2.1)</p>
<p>The expectation is over all possible trajectories generated by policy µ and all possible rewards collected in them. Similarly, for a given policy µ, we can define the (discounted) return of a state s, D µ (s), as the sum of (discounted) rewards that the agent encounters when it starts in state s and follows policy µ afterwards
D µ (s) = ∞ t=0 γ t R(Z t ) | Z 0 = s, µ(·|s) , with S t+1 ∼ P µ (·|S t ). (2.2)
The expected value of D µ is called the value function of policy µ
V µ (s) = E D µ (s) = E ∞ t=0 γ t R(Z t ) Z 0 = s, µ(·|s) . (2.3)
Closely related to value function is the action-value function of a policy, the total expected (discounted) reward observed by the agent when it starts in state s, takes action a, and then executes policy µ
Q µ (z) = E D µ (z) = E ∞ t=0 γ t R(Z t ) Z 0 = z ,
where similarly to D µ (s), D µ (z) is the sum of (discounted) rewards that the agent encounters when it starts in state s, takes action a, and follows policy µ afterwards. It is easy to see that for any policy µ, the functions V µ and Q µ are bounded byR max /(1 − γ). We may write the value of a state s under a policy µ in terms of its immediate reward and the values of its successor states under µ as
V µ (s) = R µ (s) + γ S P µ (s |s)V µ (s )ds , (2.4) which is called the Bellman equation for V µ .
Given an MDP, the goal is to find a policy that attains the best possible values, V * (s) = sup µ V µ (s), for all states s ∈ S. The function V * is called the optimal value function. A policy is optimal (denoted by µ * ) if it attains the optimal values at all the states, i.e., V µ * (s) = V * (s) for all s ∈ S. In order to characterize optimal policies it is useful to define the optimal action-value function
Q * (z) = sup µ Q µ (z).
(2.5)</p>
<p>Further, we say that a deterministic policy µ is greedy with respect to an action-value function Q, if for all s ∈ S and a ∈ A, µ(s) ∈ arg max a∈A Q(s, a). Greedy policies are important because any greedy policy with respect to Q * is optimal. Similar to the value function of a policy (Eq. 2.4), the optimal value function of a state s may be written in terms of the optimal values of its successor states as It is important to note that almost all methods for finding the optimal solution of an MDP are based on two dynamic programming (DP) algorithms: value iteration and policy iteration. Value iteration (VI) begins with a value function V 0 and at each iteration i, generates a new value function by applying Eq. 2.6 to the current value function, i.e.,
V * (s) = max a∈A R(s, a) + γ S P (s |s, a)V * (s )ds ,(2.V * i (s) = max a∈A R(s, a) + γ S P (s |s, a)V * i−1 (s )ds , ∀s ∈ S.
Policy iteration (PI) starts with an initial policy. At each iteration, it evaluates the value function of the current policy, this process is referred to as policy evaluation (PE), and then performs a policy improvement step, in which a new policy is generated as a greedy policy with respect to the value of the current policy. Iterating the policy evaluation -policy improvement process is known to produce a strictly monotonically improving sequence of policies. If the improved policy is the same as the policy improved upon, then we are assured that the optimal policy has been found.</p>
<p>Partially Observable Markov Decision Processes</p>
<p>The Partially Observable Markov Decision Process (POMDP) is an extension of MDP to the case where the state of the system is not necessarily observable [Astrom, 1965, Smallwood and Sondik, 1973, Kaelbling et al., 1998].</p>
<p>Model 4 (Partially Observable Markov Decision Process)</p>
<p>Define a POMDP M to be a tuple S, A, O, P, Ω, P 0 , q where • S is the set of states, • A is the set of actions,</p>
<p>• O is the set of observations,</p>
<p>• P (·|s, a) ∈ P(S) is the probability distribution over next states, conditioned on action a being taken in state s,</p>
<p>• Ω(·|s, a) ∈ P(O) is the probability distribution over possible observations, conditioned on action a being taken to reach state s where the observation is perceived, • P 0 ∈ P(S) is the probability distribution according to which the initial state is selected, • R(s, a) ∼ q(·|s, a) ∈ P(R) is a random variable representing the reward obtained when action a is taken in state s. Since the state is not directly observed, the agent must rely on the recent history of actions and observations, {o t+1 , a t , o t , . . . , o 1 , a 0 } to infer a distribution over states. This belief (also called information state) is defined over the state probability simplex, b t ∈ ∆, and can be calculated recursively as:
b t+1 (s ) = Ω(o t+1 |s , a t ) S P (s |s, a t )b t (s)ds S Ω(o t+1 |s , a t ) S P (s |s, a t )b t (s)dsds , (2.7)
where the sequence is initialized at b 0 := P 0 and the denominator can be seen as a simple normalization function. For convenience, we sometimes denote the belief update (Eq. 2.7) as b t+1 = τ (b t , a, o).</p>
<p>In the POMDP framework, the action-selection policy is defined as µ : ∆(s) → A. Thus, solving a POMDP involves finding the optimal policy, µ * , that maximizes the expected discounted sum of rewards for all belief states. This can be defined using a variant of the Bellman equation
V * (b t ) = max a∈A S R(s, a)b t (s)ds + γ O Pr(o|b t , a)V * τ (b t , a, o) do .
(2.8) The optimal value function for a finite-horizon POMDP can be shown to be piecewise-linear and convex [Smallwood andSondik, 1973, Porta et al., 2006]. In that case, the value function V t at any finite planning horizon t can be represented by a finite set of linear segments Γ t = {α 0 , α 1 , . . . , α m }, often called α-vectors (when S is discrete) or α-functions (when S is continuous). Each defines a linear function over the belief state space associated with some action a ∈ A. The value of a given α i at a belief b t can be evaluated by linear interpolation
α i (b t ) = S α i (s)b t (s)ds.
(2.9)</p>
<p>The value of a belief state is the maximum value returned by one of the α-functions for this belief state
V * t (b t ) = max α∈Γt S α(s)b t (s) .
(2.10)</p>
<p>In that case, the best action, µ * (b t ), is the one associated with the α-vector that returns the best value. The belief as defined here can be interpreted as a state in a particular kind of MDP, often called the belief-MDP. The main intuition is that for any partially observable system with known parameters, the belief is actually fully observable and computable (for small enough state spaces and is approximable for others). Thus, the planning problem is in fact one of planning in an MDP, where the state space corresponds to the belief simplex. This does not lead to any computational advantage, but conceptually, suggests that known results and methods for MDPs can also be applied to the belief-MDP.</p>
<p>Reinforcement Learning</p>
<p>Reinforcement learning (RL) [Bertsekas andTsitsiklis, 1996, Sutton and] is a class of learning problems in which an agent (or controller) interacts with a dynamic, stochastic, and incompletely known environment, with the goal of finding an action-selection strategy, or policy, to optimize some measure of its long-term performance. The interaction is conventionally modeled as an MDP, or if the environment state is not always completely observable, as a POMDP [Puterman, 1994]. Now that we have defined the MDP model, the next step is to solve it, i.e., to find an optimal policy. 4 In some cases, MDPs can be solved analytically, and in many cases they can be solved iteratively by dynamic or linear programming (e.g., see [Bertsekas and Tsitsiklis, 1996]). However, in other cases these methods cannot be applied because either the state space is too large, a system model is available only as a simulator, or no system model is available at all. It is in these cases that RL techniques and algorithms may be helpful.</p>
<p>Reinforcement learning solutions can be viewed as a broad class of sample-based methods for solving MDPs. In place of a model, these methods use sample trajectories of the system and the agent interacting, such as could be obtained from a simulation. It is not unusual in practical applications for such a simulator to be available when an explicit transition-probability model of the sort suitable for use by dynamic or linear programming is not [Tesauro, 1994, Crites and]. Reinforcement learning methods can also be used with no model at all, by obtaining sample trajectories from direct interaction with the system [Baxter et al., 1998, Kohl and Stone, 2004.</p>
<p>Reinforcement learning solutions can be categorized in different ways. Below we describe two common categorizations that are relevant to the structure of this paper.</p>
<p>Model-based vs. Model-free Methods: Reinforcement learning algorithms that explicitly learn a system model and use it to solve an MDP are called model-based methods. Some examples of these methods are the Dyna architecture [Sutton, 1991] and prioritized sweeping [Moore and Atkeson, 1993]. Model-free methods are those that do not explicitly learn a system model and only use sample trajectories obtained by direct interaction with the system. These methods include popular algorithms such as Q-learning [Watkins, 1989], SARSA [Rummery and Niranjan, 1994], and LSPI [Lagoudakis and Parr, 2003].</p>
<p>Value Function vs. Policy Search Methods:</p>
<p>An important class of RL algorithms are those that first find the optimal value function, and then extract an optimal policy from it. This class of RL algorithms contains value function methods, of which some wellstudied examples include value iteration (e.g., [Bertsekas andTsitsiklis, 1996, Sutton and]), policy iteration (e.g., [Bertsekas andTsitsiklis, 1996, Sutton and]), Q-learning [Watkins, 1989], SARSA [Rummery and Niranjan, 1994], LSPI [Lagoudakis and Parr, 2003], and fitted Q-iteration [Ernst et al., 2005]. An alternative approach for solving an MDP is to directly search in the space of policies. These RL algorithms are called policy search methods. Since the number of policies is exponential in the size of the state space, one has to resort to meta-heuristic search  or to local greedy methods. An important class of policy search methods is that of policy gradient algorithms. In these algorithms, the policy is taken to be an arbitrary differentiable function of a parameter vector, and the search in the policy space is directed by the gradient of a performance function with respect to the policy parameters (e.g., [Williams, 1992, Marbach, 1998). There is a third class of RL methods that use policy gradient to search in the policy space, and at the same time estimate a value function. These algorithms are called actor-critic (e.g., [Konda and Tsitsiklis, 2000, Sutton et al., 2000, Bhatnagar et al., 2007, Peters and Schaal, 2008, Bhatnagar et al., 2009). They can be thought of as RL analogs of dynamic programming's policy iteration method. Actor-critic methods are based on the simultaneous online estimation of the parameters of two structures, called the actor and the critic. The actor and the critic correspond to conventional actionselection policy and value function, respectively. These problems are separable, but are solved simultaneously to find an optimal policy.</p>
<p>Bayesian Learning</p>
<p>In Bayesian learning, we make inference about a random variable X by producing a probability distribution for X. Inferences, such as point and interval estimates, may then be extracted from this distribution. Let us assume that the random variable X is hidden and we can only observe a related random variable Y . Our goal is to infer X from the samples of Y . A simple example is when X is a physical quantity and Y is its noisy measurement. Bayesian inference is usually carried out in the following way:</p>
<ol>
<li>
<p>We choose a probability density P (X), called the prior distribution, that expresses our beliefs about the random variable X before we observe any data.</p>
</li>
<li>
<p>We select a statistical model P (Y |X) that reflects our belief about Y given X. This model represents the statistical dependence between X and Y .</p>
</li>
<li>
<p>We observe data Y = y.</p>
</li>
<li>
<p>We update our belief about X by calculating its posterior distribution using Bayes rule Assume now that P (X) is parameterized by an unknown vector of parameters θ in some parameter space Θ; we denote this as P θ (X). Let X 1 , . . . , X n be a random i.i.d. sample drawn from P θ (X). In general, updating the posterior P θ (X|Y = y) is difficult, due to the need to compute the normalizing constant Θ P (y|X )P θ (X )dθ. However, for the case of conjugate family distributions, we can update the posterior in closed-form by simply updating the parameters of the distribution. In the next two sections, we consider three classes of conjugate distributions: Beta and Dirichlet distributions and Gaussian Processes (GPs).</p>
</li>
</ol>
<p>Beta and Dirichlet Distributions</p>
<p>A simple example of a conjugate family is the Beta distribution, which is conjugate to the Binomial distribution. Let Beta(α, β) be defined by the density function f (p|α, β) ∝ p α−1 (1 − p) β−1 for p ∈ [0, 1], and parameters α, β ≥ 0. Let Binomial(n, p) be defined by the density function f (k|n, p) ∝ p k (1 − p) n−k for k ∈ {0, 1, . . . , n}, and parameters p ∈ [0, 1] and n ∈ N. Consider X ∼ Binomial(n, p) with unknown probability parameter p, and consider a prior Beta(α, β) over the unknown value of p. Then following an observation X = x, the posterior over p is also Beta distributed and is defined by Beta(α + x, β + n − x). Now let us consider the multivariate extension of this conjugate family. In this case, we have the Multinomial distribution, whose conjugate is the Dirichlet distribution. Let X ∼ M ultinomial k (p, N ) be a random variable with unknown probability distribution p = (p 1 , . . . , p k ). The Dirichlet distribution is parameterized by a count vec-
tor φ = (φ 1 , . . . , φ k ), where φ i ≥ 0, such that the density of probability distribution p = (p 1 , . . . , p k ) is defined as f (p|φ) ∝ k i=1 p φ i −1 i
. The distribution Dirichlet(φ 1 , . . . , φ k ) can also be interpreted as a prior over the unknown Multinomial distribution p, such that after observing X = n, the posterior over p is also Dirichlet and is defined by Dirichlet(φ 1 + n 1 , . . . , φ k + n k ).</p>
<p>Gaussian Processes and Gaussian Process Regression</p>
<p>A Gaussian process (GP) is an indexed set of jointly Gaussian random variables, i.e., F (x), x ∈ X is a Gaussian process if and only if for every finite set of indices {x 1 , . . . , x T } in the index set X , F (x 1 ), . . . , F (x T ) is a vector-valued Gaussian random variable. The GP F may be thought of as a random vector if X is finite, as a random series if X is countably infinite, and as a random function if X is uncountably infinite. In the last case, each instantiation of F is a function f : X → R. For a given x, F (x) is a random variable, normally distributed jointly with the other components of F . A GP F can be fully specified by its meanf :
X → R,f (x) = E[F (x)] and covariance k : X × X → R, k(x, x ) = Cov[F (x), F (x )], i.e., F (·) ∼ N f (·), k(·, ·)
. The kernel function k(·, ·) encodes our prior knowledge concerning the correlations between the components of F at different points. It may be thought of as inducing a measure of proximity between the members of X . It can also be shown that k defines the function space within which the search for a solution takes place (see [Schölkopf andSmola, 2002, Shawe-Taylor andCristianini, 2004] for more details). Now let us consider the following generative model:
Y (x) = HF (x) + N (x), (2.11)
where H is a general linear transformation, F is an unknown function, x is the input, N is a Gaussian noise and independent of F , and Y is the observable process, modeled as a noisy version of HF . The objective here is to infer F from samples of Y . Bayesian learning can be applied to this problem in the following way.</p>
<ol>
<li>
<p>We choose a prior distribution for F in the form of a GP, F (·) ∼ N f (·), k(·, ·) . When F and N are Gaussian and independent of each other, the generative model of Eq. 2.11 is known as the linear statistical model [Scharf, 1991].</p>
</li>
<li>
<p>The statistical dependence between F and Y is defined by the model-equation (2.11).</p>
</li>
<li>
<p>We observe a sample in the form of
D T = {(x t , y t )} T t=1 .</p>
</li>
<li>We calculate the posterior distribution of F conditioned on the sample D T using the Bayes rule. Below is the process to calculate this posterior distribution.  where F T = F (x 1 ), . . . , F (x T ) , Y T = (y 1 , . . . , y T ) , and N T ∼ N (0, Σ). Here [Σ] i,j is the measurement noise covariance between the ith and jth samples. In the linear statistical model, we then have
F (x 1 ) F (x 2 ) F (x T ) Y (x 1 ) Y (x 2 ) Y (x T ) N (x T ) N (x 2 ) N (x 1 )F T ∼ N (f , K), wheref = f (x 1 ), . . . ,f (x T ) and [K] i,j = k(x i , x j )
is a T × T kernel matrix. Since both F T and N T are Gaussian and independent from each other, we have Y T ∼ N (Hf , HKH + Σ). Consider a query point x, we then have
   F (x) F T N T    = N        f (x) f 0    ,    k(x, x) k(x) 0 k(x) K 0 0 0 Σ         ,
where k(x) = k(x 1 , x), . . . , k(x T , x) . Using Eq. 2.12, we have the following transformation:
   F (x) F T Y T    =    1 0 0 0 I 0 0 H I       F (x) F T N T    , (2.13)
where I is the identity matrix. From Eq. 2.13, we have
   F (x) F T Y T    = N        f (x) f Hf    ,    k(x, x) k(x) k(x) H k(x) K KH Hk(x) HK HKH + Σ         .
Using the Gauss-Markov theorem [Scharf, 1991], we know that F (x)|Y T (or equivalently F (x)|D T ) is Gaussian, and obtain the following expressions for the posterior mean and covariance of F (x) conditioned on the sample D T :
E F (x)|D T =f (x) + k(x) H (HKH + Σ) −1 (y T − Hf ), Cov F (x), F (x )|D T = k(x, x ) − k(x) H (HKH + Σ) −1 Hk(x ),
(2.14)</li>
</ol>
<p>where y T = (y 1 , . . . , y T ) is one realization of the random vector Y T . It is possible to decompose the expressions in Eq. 2.14 into input dependent terms (which depend on x and x ) and terms that only depend on the training samples, as follows:
E F (x)|D T =f (x) + k(x) α, Cov F (x), F (x )|D T = k(x, x ) − k(x) Ck(x ), (2.15) where α = H (HKH + Σ) −1 (y T − Hf ), C = H (HKH + Σ) −1 H. (2.16)
From Eqs. 2.15 and 2.16, we can conclude that α and C are sufficient statistics for the posterior moments. If we set the transformation H to be the identity and assume that the noise terms corrupting each sample are i.i.d. Gaussian, i.e., N T ∼ N (0, σ 2 I), where σ 2 is the variance of each noise term, the linear statistical model is reduced to the standard linear regression model. In this case, the posterior moments of F (x) can be written as
E F (x)|D T =f (x) + k(x) (K + σ 2 I) −1 (y T −f ) =f (x) + k(x) α, Cov F (x), F (x )|D T = k(x, x ) − k(x) (K + σ 2 I) −1 k(x ) = k(x, x ) − k(x) Ck(x ), (2.17) with α = (K + σ 2 I) −1 (y T −f ), C = (K + σ 2 I) −1 .
(2.18)</p>
<p>The GP regression described above is kernel-based and nonparametric. It is also possible to employ a parametric representation under very similar assumptions. In the parametric setting, the GP F is assumed to consist of a linear combination of a finite number n of basis functions ϕ i : X → R, i = 1, . . . , n. In this case, F can be written as F (·) = n i=1 ϕ i (·)W i = φ(·) W , where φ(·) = ϕ 1 (·), . . . , ϕ n (·) is the feature vector and W = (W 1 , . . . , W n ) is the weight vector. The model-equation (2.11) now becomes
Y (x) = Hφ(x) W + N (x).
In the parametric GP regression, the randomness in F is due to W being a random vector. Here we consider a Gaussian prior over W , distributed as W ∼ N (w, S w ). By applying the Bayes rule, the posterior (Gaussian) distribution of W conditioned on the observed data D T can be computed as
E W |D T =w + S w ΦH (HΦ S w ΦH + Σ) −1 (y T − HΦ w), Cov W |D T = S w − S w ΦH (HΦ S w ΦH + Σ) −1 HΦ S w ,(2.19)
where Φ = φ(x 1 ), . . . , φ(x T ) is a n × T feature matrix. Finally, since F (x) = φ(x) W , the posterior mean and covariance of F can be easily computed as
E F (x)|D T = φ(x) w + φ(x) S w ΦH (HΦ S w ΦH +Σ) −1 (y T − HΦ w), Cov F (x), F (x )|D T = φ(x) S w φ(x ) − φ(x) S w ΦH (HΦ S w ΦH +Σ) −1 HΦ S w φ(x ).
(2.20)</p>
<p>Similar to the non-parametric setting, for the standard linear regression model with the prior W ∼ N (0, I), Eq. 2.19 may be written as
E W |D T = Φ(Φ Φ + σ 2 I) −1 y T , Cov W |D T = I − Φ(Φ Φ + σ 2 I) −1 Φ . (2.21)
To have a smaller matrix inversion when T &gt; n, Eq. 2.21 may be written as
E W |D T = (ΦΦ + σ 2 I) −1 Φy T , Cov W |D T = σ 2 (ΦΦ + σ 2 I) −1 . (2.22)</p>
<p>Bayesian Bandits</p>
<p>This section focuses on Bayesian learning methods for regret minimization in the multi-armed bandits (MAB) model. We review classic performance bounds for this problem and state-of-the-art results for several Bayesian approaches. In the MAB model ( §2.1), the only unknown quantities are the outcome probabilities P (·|a). The idea behind Bayesian approaches to MAB is to use Bayesian inference ( §2.5) for learning P (·|a) from the outcomes observed during sequential interaction with the MAB. Following the framework of §2.5, the outcome probabilities are parameterized by an unknown vector θ, and are henceforth denoted by P θ (·|a). The parameter vector θ is assumed to be drawn from a prior distribution P prior . As a concrete example, consider a MAB with Bernoulli arms: Example 3.1 (Bernoulli K-MAB with a Beta prior). Consider a K-MAB where the set of outcomes is binary Y = {0, 1}, and the reward is the identity r(Y ) = Y . Let the outcome probabilities be parameterized by θ ∈ R K , such that Y (a) ∼ Bernoulli[θ a ]. The prior for each θ a is Beta(α a , β a ). Following an observation outcome Y (a) = y, the posterior for θ a is updated to Beta(α a + y, β a + y − 1). Also note that the (posterior) expected reward for arm a is now
E [r(Y (a))] = E [E [ Y (a)| θ a ]] = E [θ a ] = αa+y αa+y+βa+y−1 .
The principal challenge of Bayesian MAB algorithms, however, is to utilize the posterior θ for selecting an adequate policy that achieves low regret.</p>
<p>Conceptually, there are several reasons to adopt a Bayesian approach for MABs. First, from a modelling perspective, the prior for θ is a natural means for embedding prior knowledge, or structure of the problem. Second, the posterior for θ encodes the uncertainty about the outcome probabilities at each step of the algorithm, and may be used for guiding the exploration to more relevant areas.</p>
<p>A Bayesian point of view may also be taken towards the performance measure of the algorithm. Recall that the performance of a MAB algorithm is measured by its expected regret. This expectation, however, may be defined in two different ways, depending on how the parameters θ are treated. In the frequentist approach, henceforth termed 'frequentist regret' and denoted E θ Regret(T ) , the parameter vector θ is fixed, and treated as a constant in the expectation. The expectation is then computed with respect to the stochastic outcomes and the action selection policy. On the other hand, in 'Bayesian regret', a.k.a. Bayes risk, θ is assumed to be drawn from the prior, and the expectation E Regret(T ) is over the stochastic outcomes, the action selection rule, and the prior distribution of θ. Note that the optimal action a * depends on θ. Therefore, in the expectation it is also considered a random variable.</p>
<p>We emphasize the separation of Bayesian MAB algorithms and Bayesian regret analysis. In particular, the performance of a Bayesian MAB algorithm (i.e., an algorithm that uses Bayesian learning techniques) may be measured with respect to a frequentist regret, or a Bayesian one.</p>
<p>Classical Results</p>
<p>In their seminal work, Lai and Robbins [Lai and Robbins, 1985] proved asymptotically tight bounds on the frequentist regret in terms of the Kullback-Leibler (KL) divergence between the distributions of the rewards of the different arms. These bounds grow logarithmically with the number of steps T , such that regret is O (ln T ). Mannor and Tsitsiklis [Mannor and Tsitsiklis, 2004] later showed non-asymptotic lower bounds with a similar logarithmic dependence on T . For the Bayesian regret, the lower bound on the regret is O √ KT (see, e.g., Theorem 3.5 of Bubeck and Cesa-Bianchi [Bubeck and Cesa-Bianchi, 2012]).</p>
<p>In the Bayesian setting, and for models that admit sufficient statistics, Gittins [Gittins, 1979] showed that an optimal strategy may be found by solving a specific MDP planning problem. The key observation here is that the dynamics of the posterior for each arm may be represented by a special MDP termed a bandit process [Gittins, 1979].</p>
<p>Definition 3.1 (Bandit Process). A bandit process is an MDP with two actions A = {0, 1}. The control 0 freezes the process in the sense that P (s = s|s, 0) = 1, and R(s, 0) = 0. Control 1 continues the process, and induces a standard MDP transition to a new state with probability P (·|s, 1), and reward R(s, 1).</p>
<p>For example, consider the case of Bernoulli bandits with a Beta prior as described in Example 3.1. We identify the state s a of the bandit process for arm a as the posterior parameters s a = (α a , β a ). Whenever arm a is pulled, the continuation control is applied as follows: we draw some θ a from the posterior, and then draw an outcome Y ∼ Bernoulli[θ a ]. The state subsequently transitions to an updated posterior (cf. Example 3.1) s a = (α a + Y, β a + Y − 1), and a reward of E [r (Y )] is obtained, where the expectation is taken over the posterior.</p>
<p>The K-MAB problem, thus, may be seen as a particular instance of a general model termed simple family of alternative bandit processes (SFABP) [Gittins, 1979].</p>
<p>Model 5 (Simple Family of Alternative Bandit Processes)</p>
<p>A simple family of alternative bandit processes is a set of K bandit processes. For each bandit process i ∈ 1, . . . , K we denote by s i , P i (·|s i , 1), and R i (s i , 1) the corresponding state, transition probabilities, and reward, respectively. At each time t = 1, 2, . . . , a single bandit i t ∈ 1, . . . , K that is in state s it (t) is activated, by applying to it the continuation control, and all other bandits are frozen. The objective is to find a bandit selection policy that maximizes the expected total
γ-discounted reward E ∞ t=0 γ t R it (s it (t), 1) .
An important observation is that for the K-MAB problem, as defined above, the SFABP performance measure captures an expectation of the total discounted reward with respect to the prior distribution of the parameters θ. In this sense, this approach is a full Bayesian K-MAB solution (cf. the Bayesian regret vs. frequentist regret discussion above). In particular, note that the SFABP performance measure implicitly balances exploration and exploitation. To see this, recall the Bernoulli bandits example, and note that the reward of each bandit process is the posterior expected reward of its corresponding arm. Since the posterior distribution is encoded in the process state, future rewards in the SFABP performance measure depend on the future state of knowledge, thereby implicitly quantifying the value of additional observations for each arm.</p>
<p>The SFABP may be seen as a single MDP, with a state (s 1 , s 2 , . . . , s K ) that is the conjunction of the K individual bandit process states. Thus, naively, an optimal policy may be computed by solving this MDP. Unfortunately, since the size of the state space of this MDP is exponential in K, such a naive approach is intractable. The virtue of the celebrated Gittins index theorem [Gittins, 1979], is to show that this problem nevertheless has a tractable solution.</p>
<p>Theorem 3.1. [Gittins, 1979] The objective of SFABP is maximized by following a policy that always chooses the bandit with the largest Gittins index
G i (s i ) = sup τ ≥1 E τ −1 t=0 γ t R i (s i (t), 1) s i (0) = s i E τ −1 t=0 γ t s i (0) = s i ,
where τ is any (past measurable) stopping time.</p>
<p>The crucial advantage of Theorem 3.1, is that calculating G i may be done separately for each arm, thereby avoiding the exponential complexity in K. The explicit calculation, however, is technically involved, and beyond the scope of this survey. For reference see [Gittins, 1979], and also [Tsitsiklis, 1994] for a simpler derivation, and [Niño-Mora, 2011] for a finite horizon setting.</p>
<p>Due to the technical complexity of calculating optimal Gittin's index policies, recent approaches concentrate on much simpler algorithms, that nonetheless admit optimal upper bounds (i.e., match the order of their respective lower bounds up to constant factors) on the expected regret.</p>
<p>Bayes-UCB</p>
<p>The upper confidence bound (UCB) algorithm [Auer et al., 2002] is a popular frequentist approach for MABs that employs an 'optimistic' policy to reduce the chance of overlooking the best arm. The algorithm starts by playing each arm once. Then, at each time step t, UCB plays the arm a that maximizes &lt; r a &gt; + 2 ln t ta , where &lt; r a &gt; is the average reward obtained from arm a, and t a is the number of times arm a has been playing so far. The optimistic upper confidence term 2 ln t ta guarantees that the empirical average does not underestimate the best arm due to 'unlucky' reward realizations.</p>
<p>The Bayes-UCB algorithm of Kaufmann et al. [Kaufmann et al., 2012a] extends the UCB approach to the Bayesian setting. A posterior distribution over the expected reward of each arm is maintained, and at each step, the algorithm chooses the arm with the maximal posterior (1 − β t )-quantile, where β t is of order 1/t. Intuitively, using an upper quantile instead of the posterior mean serves the role of 'optimism', in the spirit of the original UCB approach. For the case of Bernoulli distributed outcomes and a uniform prior on the reward means, Kaufmann et al. [Kaufmann et al., 2012a] prove a frequentist upper bound on the expected regret that matches the lower bound of Lai and Robbins [Lai and Robbins, 1985].</p>
<p>Thompson Sampling</p>
<p>The Thompson Sampling algorithm (TS) suggests a natural Bayesian approach to the MAB problem using randomized probability match-ing [Thompson, 1933]. Let P post denote the posterior distribution of θ given the observations up to time t. In TS, at each time step t, we sample a parameterθ from the posterior and select the optimal action with respect to the model defined byθ. Thus, effectively, we match the action selection probability to the posterior probability of each action being optimal. The outcome observation is then used to update the posterior P post .</p>
<p>Algorithm 1 Thompson Sampling</p>
<p>1: TS(P prior )</p>
<p>• P prior prior distribution over θ 2: P post := P prior 3: for t = 1, 2, . . . do</p>
<p>4:</p>
<p>Sampleθ from P post 5:
Play arm a t = arg max a∈A E y∼Pθ(·|a) [r(y)] 6:
Observe outcome Y t and update P post 7: end for</p>
<p>Recently, the TS algorithm has drawn considerable attention due to its state-of-the-art empirical performance [Scott, 2010, Chapelle andLi, 2011], which also led to its use in several industrial applications [Graepel et al., 2010, Tang et al., 2013. We survey several theoretical studies that confirm TS is indeed a sound MAB method with state-of-the-art performance guarantees.</p>
<p>We mention that the TS idea is not limited to MAB problems, but can be seen as a general sampling technique for Bayesian learning, and has been applied also to contextual bandit and RL problems, among others [Strens, 2000, Osband et al., 2013, Abbasi-Yadkori and Szepesvari, 2015. In this section, we present results for the MAB and contextual bandit settings, while the RL related results are given in Chapter 4.</p>
<p>Agrawal and Goyal [Agrawal and Goyal, 2012] presented frequentist regret bounds for TS. Their analysis is specific to Bernoulli arms with a uniform prior, but they show that by a clever modification of the algorithm it may also be applied to general arm distributions. Let ∆ a = E θ r Y (a * ) − r Y (a) denote the difference in expected reward between the optimal arm and arm a, when the parameter is θ.</p>
<p>Theorem 3.2. [Agrawal and Goyal, 2012] For the K-armed stochastic bandit problem, the TS algorithm has (frequentist) expected regret
E θ Regret(T ) ≤ O a =a * 1 ∆ 2 a 2 ln T .
Improved upper bounds were later presented [Kaufmann et al., 2012b] for the specific case of Bernoulli arms with a uniform prior. These bounds are (order) optimal, in the sense that they match the lower bound of [Lai and Robbins, 1985]. Let KL θ (a, a * ) denote the Kullback-Leibler divergence between Bernoulli distributions with parameters E θ r(Y (a)) and E θ r(Y (a * )) .</p>
<p>Theorem 3.3. [Kaufmann et al., 2012b] For any &gt; 0, there exists a problem-dependent constant C( , θ) such that the regret of TS satisfies
E θ Regret(T ) ≤ (1 + ) a =a * ∆ a ln(T ) + ln ln(T ) KL θ (a, a * ) + C( , θ).
More recently, Agrawal and Goyal [Agrawal and Goyal, 2013a] showed a problem independent frequentist regret bound of order O √ KT ln T , for the case of Bernoulli arms with a Beta prior. This bound holds for all θ, and implies that a Bayesian regret bound with a similar order holds; up to the logarithmic factor, this bound is orderoptimal.</p>
<p>Liu and Li [Liu and Li, 2015] investigated the importance of having an informative prior in TS. Consider the special case of two-arms and two-models, i.e., K = 2, and θ ∈ {θ true , θ false }, and assume that θ true is the true model parameter. When P prior (θ true ) is small, the frequentist regret of TS is upper-bounded by O T P prior (θtrue) , and when P prior (θ true ) is sufficiently large, the regret is upper-bounded by</p>
<p>O</p>
<p>(1 − P prior (θ true ))T [Liu and Li, 2015]. For the special case of two-arms and two-models, regret lower-bounds of matching orders are also provided in [Liu and Li, 2015]. These bounds show that an informative prior, i.e., a large P prior (θ true ), significantly impacts the regret.</p>
<p>One appealing property of TS is its natural extension to cases with structure or dependence between the arms. For example, consider the following extension of the online shop domain: Example 3.2 (Online Shop with Multiple Product Suggestions). Consider the bandit setting of the online shop domain, as in Example 2.1. Instead of presenting to the customer a single product suggestion, the decisionmaker may now suggest M different product suggestions a 1 , . . . , a M from a pool of suggestions I. The customer, in turn, decides whether or not to buy each product, and the reward is the sum of profits from all items bought.</p>
<p>Naively, this problem may be formalized as a MAB with the action set, A, being the set of all possible combinations of M elements from I. In such a formulation, it is clear that the outcomes for actions with overlapping product suggestions are correlated.</p>
<p>In TS, such dependencies between the actions may be incorporated by simply updating the posterior. Recent advances in Bayesian inference such as particle filters and Markov Chain Monte-Carlo (MCMC) provide efficient numerical procedures for complex posterior updates. Gopalan et al. [Gopalan et al., 2014] presented frequentist highprobability regret bounds for TS with general reward distributions and priors, and correlated actions. Let N T (a) denote the play count of arm a until time T and let Dθ ∈ R |A| denote a vector of Kullback-Leibler divergences Dθ (a) = KL P θ (·|a) PΘ(·|a) , where θ is the true model in a frequentist setting. For each action a ∈ A, we define S a to be the set of model parameters θ in which the optimal action is a. Within S a , let S a be the set of modelsθ that exactly match the true model θ in the sense of the marginal outcome distribution for action a: KL P θ (·|a) Pθ(·|a) = 0. Moreover, let e (j) denote the j-th unit vector in a finite-dimensional Euclidean space. The following result holds under the assumption of finite action and observation spaces, and that the prior has a finite support with a positive probability for the true model θ ('grain of truth' assumption).</p>
<p>Theorem 3.4. [Gopalan et al., 2014] For δ, ∈ (0, 1), there exists T * &gt; 0 such that for all T &gt; T * , with probability at least 1 − δ,
a =a * N T (a) ≤ B + C(log T ), where B ≡ B(δ, , A, Y, θ, P prior ) is a problem-dependent constant that does not depend on T and C(log T ) := max K−1 k=1 z k (a k ) s.t. z k ∈ N K−1 + × {0}, a k ∈ A \ {a * }, k &lt; K, z i z k , z i (a k ) = z k (a k ), i ≥ k, ∀j ≥ 1, k ≤ K − 1 : min θ∈S a k z k , Dθ ≥ 1 + 1 − log T, min θ∈S a k z k − e (j) , Dθ ≥ 1 + 1 − log T.
As Gopalan et al. [Gopalan et al., 2014] explain, the bound in Theorem 3.4 accounts for the dependence between the arms, and thus, provides tighter guarantees when there is information to be gained from this dependence. For example, in the case of selecting subsets of M arms described earlier, calculating the term C(log T ) in Theorem 3.4 gives a bound of O (K − M ) log T , even though the total number of actions is K M . We proceed with an analysis of the Bayesian regret of TS. Using information theoretic tools, Russo and Van Roy [Russo and Van Roy, 2014a] elegantly bound the Bayesian regret, and investigate the benefits of having an informative prior. Let p t denote the distribution of the selected arm at time t. Note that by the definition of the TS algorithm, p t encodes the posterior distribution that the arm is optimal. Let p t,a (·) denote the posterior outcome distribution at time t, when selecting arm a, and let p t,a (·|a * ) denote the posterior outcome distribution at time t, when selecting arm a, conditioned on the event that a * is the optimal action. Both p t,a (·) and p t,a (·|a * ) are random and depend on the prior and on the history of actions and observations up to time t.</p>
<p>A key quantity in the analysis of [Russo and Van Roy, 2014a] is the information ratio defined by
Γ t = E a∼pt E y∼pt,a(·|a) r(y) − E y∼pt,a(·) r(y) E a∼pt,a * ∼pt KL p t,a (·|a * ) p t,a (·)
.</p>
<p>(3.1)</p>
<p>Note that Γ t is also random and depends on the prior and on the history of the algorithm up to time t. As Russo and Van Roy [Russo and Van Roy, 2014a] explain, the numerator in Eq. 3.1 roughly captures how much knowing that the selected action is optimal influences the expected reward observed, while the denominator measures how much on average, knowing which action is optimal changes the observations at the selected action. Intuitively, the information ratio tends to be small when knowing which action is optimal significantly influences the anticipated observations at many other actions.</p>
<p>The following theorem relates a bound on Γ t to a bound on the Bayesian regret.</p>
<p>Theorem 3.5. [Russo and Van Roy, 2014a
] For any T ∈ N, if Γ t ≤Γ almost surely for each t ∈ {1, . . . , T }, the TS algorithm satisfies E Regret(T ) ≤Γ H(p 1 )T ,
where H(·) denotes the Shannon entropy.</p>
<p>Russo and Van Roy [Russo and Van Roy, 2014a] further showed that in general, Γ t ≤ K/2, giving an order-optimal upper bound of O 1 2 H(p 1 )KT . However, structure between the arms may be exploited to further bound the information ratio more tightly. For example, consider the case of linear optimization under bandit feedback where we have A ⊂ R d , and the reward satisfies E y∼P θ (·|a) [r(y)] = a θ.</p>
<p>In this case, an order-optimal bound of O 1 2 H(p 1 )dT holds [Russo and Van Roy, 2014a]. It is important to note that the term H(p 1 ) is bounded by log(K), but in fact may be much smaller when there is an informative prior available.</p>
<p>An analysis of TS with a slightly different flavour was given by Guha and Munagala [Guha and Munagala, 2014], who studied the stochastic regret of TS, defined as the expected number of times a sub-optimal arm is chosen, where the expectation is Bayesian, i.e., taken with respect to P prior (θ). For some horizon T , and prior P prior , let OP T (T, P prior ) denote the stochastic regret of an optimal policy. Such a policy exists, and in principle may be calculated using dynamic programming (cf. the Gittins index discussion above). For the cases of two-armed bandits, and K-MABs with Bernoulli point priors, Guha and Munagala [Guha and Munagala, 2014] show that the stochastic regret of TS, labeled T S(T, P prior ) is a 2-approximation of OP T (T, P prior ), namely T S(T, P prior ) ≤ 2OP T (T, P prior ). Interestingly, and in contrast to the asymptotic regret results discussed above, this result holds for all T .</p>
<p>We conclude by noting that contextual bandits may be approached using Bayesian techniques in a very similar manner to the MAB algorithms described above. The only difference is that the unknown vector θ should now parameterize the distribution over actions and context, P θ (·|a, s). Empirically, the efficiency of TS was demonstrated in an online-advertising application of contextual bandits [Chapelle and Li, 2011]. On the theoretical side, Agrawal and Goyal [Agrawal and Goyal, 2013b] study contextual bandits with rewards that linearly depend on the context, and show a frequentist regret bound ofÕ d 2 √ T 1+ , where d is the dimension of the context vector, and is an algorithmparameter that can be chosen in (0, 1). For the same problem, Russo and Van Roy [Russo and Van Roy, 2014b] </p>
<p>Model-based Bayesian Reinforcement Learning</p>
<p>This section focuses on Bayesian learning methods that explicitly maintain a posterior over the model parameters and use this posterior to select actions. Actions can be selected both for exploration (i.e., achieving a better posterior), or exploitation (i.e., achieving maximal return). We review basic representations and algorithms for model-based approaches, both in MDPs and POMDPs. We also present approaches based on sampling of the posterior, some of which provide finite sample guarantees on the learning process.</p>
<p>Models and Representations</p>
<p>Initial work on model-based Bayesian RL appeared in the control literature, under the topic of Dual Control [Feldbaum, 1961, Filatov andUnbehauen, 2000]. The goal here is to explicitly represent uncertainty over the model parameters, P, q, as defined in §2.2. One way to think about this problem is to see the parameters as unobservable states of the system, and to cast the problem of planning in an MDP with unknown parameters as planning under uncertainty using the POMDP formulation. In this case, the belief tracking operation of Eq. 2.7 will keep a joint posterior distribution over model parameters and the true physical state of the system, and a policy will be derived to select optimal actions with respect to this posterior.</p>
<p>Let θ s,a,s be the (unknown) probability of transitioning from state s to state s when taking action a, θ s,a,r the (unknown) probability of obtaining reward r when taking action a at state s, and θ ∈ Θ the set of all such parameters. The belief P 0 (θ) expresses our initial knowledge about the model parameters. We can then compute b t (θ), the belief
after a t-step trajectory {s t+1 , r t , a t , s t , . . . , s 1 , r 0 , a 0 , s 0 }. Considering a single observation (s, a, r, s ), we have b t+1 (θ ) = η Pr(s , r|s, a, θ ) S,Θ Pr(s , θ |s, a, θ)b t (θ)dsdθ , (4.1)
where η is the normalizing factor. It is common to assume that the uncertainty between the parameters is independent, and thus
compute b t (θ) = s,a b t (θ s,a,s )b t (θ s,a,r ).
Recalling that a POMDP can be represented as a belief-MDP (see §2.3), a convenient way to interpret Eq. 4.1 is as an MDP where the state is defined to be a belief over the unknown parameters. Theoretically, optimal planning in this representation can be achieved using MDP methods for continuous states, the goal being to express a policy over the belief space, and thus, over any posterior over model parameters. From a computational perspective, this is not necessarily a useful objective. First, it is computationally expensive, unless there are only a few unknown model parameters. Second, it is unnecessarily hard: indeed, the goal is to learn (via the acquisition of data) an optimal policy that is robust to parameter uncertainty, not necessarily to pre-compute a policy for any possible parameterization. An alternative approach is to make specific assumptions about the structural form of the uncertainty over the model parameters, thereby allowing us to consider an alternate representation of the belief-MDP, one that is mathematically equivalent (under the structural assumptions), but more amenable to computational analysis.</p>
<p>An instance of this type of approach is the Bayes-Adaptive MDP (BAMDP) model [Duff, 2002]. Here we restrict our attention to MDPs with discrete state and action sets. In this case, transition probabilities consist of Multinomial distributions. The posterior over the transition function can therefore be represented using a Dirichlet distribution, P (·|s, a) ∼ φ s,a . For now, we assume that the reward function is known; extensions to unknown rewards are presented in §4.7.</p>
<p>Model 6 (Bayes-Adaptive MDP) Define a Bayes-Adaptive MDP
M to be a tuple S , A, P , P 0 , R where • S is the set of hyper-states, S × Φ, • A is the set of actions, • P (·|s, φ, a)
is the transition function between hyper-states, conditioned on action a being taken in hyper-state (s, φ), • P 0 ∈ P(S × Φ) combines the initial distribution over physical states, with the prior over transition functions φ 0 , • R (s, φ, a) = R(s, a) represents the reward obtained when action a is taken in state s.</p>
<p>The BAMDP is defined as an extension of the conventional MDP model. The state space of the BAMDP combines jointly the initial (physical) set of states S, with the posterior parameters on the transition function Φ. We call this joint space the hyper-state. Assuming the posterior on the transition is captured by a Dirichlet distribution, we have Φ = {φ s,a ∈ N |S| , ∀s, a ∈ S × A}. The action set is the same as in the original MDP. The reward function is assumed to be known for now, so it depends only on the physical (real) states and actions.</p>
<p>The transition model of the BAMDP captures transitions between hyper-states. Due to the nature of the state space, this transition function has a particular structure. By the chain rule, Pr(s , φ |s, a, φ) = Pr(s |s, a, φ) Pr(φ |s, a, s , φ). The first term can be estimated by taking the expectation over all possible transition functions to yield φ s,a,s s ∈S φ s,a,s . For the second term, since the update of the posterior φ to φ is deterministic, Pr(φ |s, a, s , φ) is 1 if φ s,a,s = φ s,a,s + 1, and 0, otherwise. Because these terms depend only on the previous hyperstate (s, φ s,a ) and action a, transitions between hyper-states preserve the Markov property. To summarize, we have (note that I(·) is the indicator function):
P (s , φ |s, a, φ) = φ s,a,s s ∈S φ s,a,s I(φ s,a,s = φ s,a,s + 1). (4.2)
It is worth considering the number of states in the BAMDP. Initially (at t = 0), there are only |S|, one per real MDP, state (we assume a single prior φ 0 is specified). Assuming a fully connected state space in the underlying MDP (i.e., P (s |s, a) &gt; 0, ∀s, a), then at t = 1 there are already |S|×|S| states, since φ → φ can increment the count of any one of its |S| components. So at horizon t, there are |S| t reachable states in the BAMDP. There are clear computational challenges in computing an optimal policy over all such beliefs.</p>
<p>Computational concerns aside, the value function of the BAMDP can be expressed using the Bellman equation
V * (s, φ) = max a∈A   R (s, φ, a) + γ (s ,φ )∈S P (s , φ |s, φ, a)V * (s , φ )   = max a∈A   R(s, a) + γ s ∈S φ a s,s s ∈S φ a s,s V * (s , φ )   . (4.3)
Let us now consider a simple example, the Chain problem, which is used extensively for empirical demonstrations throughout the literature on model-based BRL.</p>
<p>Example 4.1 (The Chain problem). The 5-state Chain problem [Strens, 2000], shown in Figure 4.1, requires the MDP agent to select between two abstract actions {1, 2}. Action 1 causes the agent to move to the right with probability 0.8 (effect "a" in Figure 4.1) and causes the agent to reset to the initial state with probability 0.2 (effect "b" in Figure 4.1). Action 2 causes the agent to reset with probability 0.8 (effect "b") and causes the agent to move to the right with probability 0.2 (effect "a"). The action b has constant reward of +2. Rewards vary based on the state and effect ("a" and "b"), as shown in Figure 4.1. The optimal policy is to always choose action 1, causing the agent to potentially receive +10 several times until slipping back (randomly) to the initial state. Of course if the transition probabilities and rewards are not known, the agent has to trade-off exploration and exploitation to learn this optimal policy. </p>
<p>Exploration/Exploitation Dilemma</p>
<p>A key aspect of reinforcement learning is the issue of exploration. This corresponds to the question of determining how the agent should choose actions while learning about the task. This is in contrast to the phase called exploitation, through which actions are selected so as to maximize expected reward with respect to the current value function estimate.</p>
<p>In the Bayesian RL framework, exploration and exploitation are naturally balanced in a coherent mathematical framework. Policies are expressed over the full information state (or belief), including over model uncertainty. In that framework, the optimal Bayesian policy will be to select actions based on how much reward they yield, but also based on how much information they provide about the parameters of the domain, information which can then be leveraged to acquire even more reward.</p>
<p>Definition 4.1 (Bayes optimality). Let
V * t (s, φ) = max a∈A R(s, a) + γ S,Φ P (s |b, s, a)V * t−1 (s , φ )ds dθ
be the optimal value function for a t-step planning horizon in a BAMDP.</p>
<p>46CHAPTER 4. MODEL-BASED BAYESIAN REINFORCEMENT LEARNING</p>
<p>Any policy that maximizes this expression is called a Bayes-optimal policy. In general, a Bayes-optimal policy is mathematically lower than the optimal policy (Eq. 2.6), because it may require additional actions to acquire information about the model parameters. In the next section, we present planning algorithms that seek the Bayes-optimal policy; most are based on heuristics or approximations due to the computation complexity of the problem. In the following section, we also review algorithms that focus on a slightly different criteria, namely efficient (polynomial) learning of the optimal value function.</p>
<p>Offline Value Approximation</p>
<p>We now review various classes of approximate algorithms for estimating the value function in the BAMDP. We begin with offline algorithms that compute the policy a priori, for any possible state and posterior. The goal is to compute an action-selection strategy that optimizes the expected return over the hyper-state of the BAMDP. Given the size of the state space in the BAMDP, this is clearly intractable for most domains. The interesting problem then is to devise approximate algorithms that leverage structural constraints to achieve computationally feasible solutions.</p>
<p>Finite-state controllers</p>
<p>Duff [Duff, 2001] suggests using Finite-State Controllers (FSC) to compactly represent the optimal policy µ * of a BAMDP, and then finding the best FSC in the space of FSCs of some bounded size.</p>
<p>Definition 4.2 (Finite-State Controller).</p>
<p>A finite state controller for a BAMDP is defined as a graph, where nodes represent memory states and edges represent observations in the form of (s, a, s ) triplets. Each node is also associated with an action (or alternately a distribution over actions), which represents the policy itself.</p>
<p>In this representation, memory states correspond to finite trajectories, rather than full hyper-states. Tractability is achieved by limiting the number of memory states that are included in the graph. Given a specific finite-state controller (i.e., a compact policy), its expected value can be calculated in closed form using a system of Bellman equations, where the number of equations/variables is equal to |S| × |Q|, where |S| is the number of states in the original MDP, and |Q| is the number of memory states in the FSC [Duff, 2001]. The remaining challenge is to optimize the policy. This is achieved using a gradient descent algorithm. A Monte-Carlo gradient estimation is proposed to make it more tractable. This approach presupposes the existence of a good FSC representation for the policy. In general, while conceptually and mathematically straight-forward, this method is computationally feasible only for small domains with few memory states. For many realworld domains, the number of memory states needed to achieve good performance is far too large.</p>
<p>Bayesian Exploration Exploitation Tradeoff in LEarning (BEETLE)</p>
<p>An alternate approximate offline algorithm to solve the BAMDP is called BEETLE . This is an extension of the Perseus algorithm [Spaan and Vlassis, 2005] that was originally designed for conventional POMDP planning, to the BAMDP model. Essentially, at the beginning, hyper-states (s, φ) are sampled from random interactions with the BAMDP model. An equivalent continuous POMDP (over the space of states and transition functions) is solved instead of the BAMDP (assuming b = (s, φ) is a belief state in that POMDP). The value function is represented by a set of α-functions over the continuous space of transition functions (see Eqs. 2.8-2.10). In the case of BEETLE, it is possible to maintain a separate set of α-functions for each MDP state, denoted Γ s . Each α-function is constructed as a linear combination of basis functions α(b t ) = θ α(θ)b(θ)dθ. In practice, the sampled hyper-states can serve to select the set of basis functions θ. The set of α-functions can then be constructed incrementally by applying Bellman updates at the sampled hyper-states using standard point-based POMDP methods Vlassis, 2005, Pineau et al., 2003].</p>
<p>Thus, the constructed α-functions can be shown to be multi-variate polynomials. The main computational challenge is that the number of terms in the polynomials increases exponentially with the planning horizon. This can be mitigated by projecting each α-function into a polynomial with a smaller number of terms (using basis functions as mentioned above). The method has been tested experimentally in some small simulation domains. The key to applying it in larger domains is to leverage knowledge about the structure of the domain to limit the parameter inference to a few key parameters, or by using parameter tying (whereby a subset of parameters are constrained to have the same posterior).</p>
<p>Online near-myopic value approximation</p>
<p>We recall from §4.1 that for a planning horizon of t steps, an offline BAMDP planner will consider optimal planning at |S| t states. In practice, there may be many fewer states, in particular because some trajectories will not be observed. Online planning approaches interleave planning and execution on a step-by-step basis, so that planning resources are focused on those states that have been observed during actual trajectories. We now review a number of online algorithms developed for the BAMDP framework.</p>
<p>Bayesian dynamic programming</p>
<p>A simple approach, closely related to Thompson sampling ( §3.3), was proposed by Sterns [Strens, 2000]. The idea is to sample a model from the posterior distribution over parameters, solve this model using dynamic programming techniques, and use the solved model to select actions. Models are re-sampled periodically (e.g., at the end of an episode or after a fixed number of steps). The approach is simple to implement and does not rely on any heuristics. Goal-directed exploration is achieved via sampling of the models. Convergence to the optimal policy is achievable because the method samples models from the full posterior over parameter uncertainty [Strens, 2000]. Of course this can be very slow, but it is useful to remember that the dynamic programming steps can be computed via simulation over the sampled model, and do not require explicit samples from the system. Convergence of the dynamic programming inner-loop is improved by keeping maximum likelihood estimates of the value function for each state-action pair. In the bandit case (single-step planning horizon), this method is in fact equivalent to Thompson sampling. Recent work has provided a theoretical characterization of this approach, offering the first Bayesian regret bound for this setting [Osband et al., 2013].</p>
<p>Value of information heuristic</p>
<p>The Bayesian dynamic programming approach does not explicitly take into account the posterior uncertainty when selecting actions, and thus, cannot explicitly select actions which only provide information about the model. In contrast, Dearden et al. [Dearden et al., 1999] proposed to select actions by considering their expected value of information (in addition to their expected reward). Instead of solving the BAMDP directly via Eq. 4.3, the Dirichlet distributions are used to compute a distribution over the state-action values Q * (s, a), in order to select the action that has the highest expected return and value of information. The distribution over Q-values is estimated by sampling MDPs from the posterior Dirichlet distribution, and then solving each sampled MDP (as detailed in §2.2) to obtain different sampled Q-values.</p>
<p>The value of information is used to estimate the expected improvement in policy following an exploration action. Unlike the full Bayesoptimal approach, this is defined myopically, over a 1-step horizon. [Dearden et al., 1999]). Let V P I(s, a) denote the expected value of perfect information for taking action a in state s. This can be estimated by</p>
<p>Definition 4.3 (Value of perfect information
V P I(s, a) = ∞ −∞ Gain s,a (x) Pr(q s,a = x)dx, where Gain s,a (Q * s,a ) =      E[q s,a 2 ] − q * s,a if a = a 1 and q * s,a &lt; E[q s,a 2 ], q * s,a − E[q s,a 1 ] if a = a 1 and q * s,a &gt; E[q s,a 2 ], 0 otherwise,
assuming a 1 and a 2 denote the actions with the best and second-best expected values respectively, and q * s,a denotes a random variable representing a possible value of Q * (s, a) in some realizable MDP.</p>
<p>The value of perfect information gives an upper bound on the 1step expected value of exploring with action a. To balance exploration and exploitation, it is necessary to also consider the reward of action a. Thus, under this approach, the goal is to select actions that maximize E[q s,a ] + V P I(s, a).</p>
<p>From a practical perspective, this method is attractive because it can be scaled easily by varying the number of samples. Re-sampling and importance sampling techniques can be used to update the estimated Q-value distribution as the Dirichlet posteriors are updated. The main limitation is that the myopic value of information may provide only a very limited view of the potential information gain of certain actions.</p>
<p>Online Tree Search Approximation</p>
<p>To select actions using a more complete characterization of the model uncertainty, an alternative is to perform a forward search in the space of hyper-states.</p>
<p>Forward search</p>
<p>An approach of this type was used in the case of a partially observable extension to the BAMDP [Ross et al., 2008a]. The idea here is to consider the current hyper-state and build a fixed-depth forward search tree containing all hyper-states reachable within some fixed finite planning horizon, denoted d. Assuming some default value for the leaves of this tree, denoted V 0 (s, φ), dynamic programming backups can be applied to estimate the expected return of the possible actions at the root hyper-state. The action with highest return over that finite horizon is executed and then the forward search is conducted again on the next hyper-state. This approach is detailed in Algorithm 2 and illustrated in Figure 4.2. The main limitation of this approach is the fact that for most domains, a full forward search (i.e., without pruning of the search tree) can only be achieved over a very short decision horizon, since the number of nodes explored is O(|S| d ), where d is the search depth.</p>
<p>Another limitation is the need for a default value function to be used at the leaf nodes; this can either be a naive estimate, such as the immediate reward, max a R(s, a), or a value computed from repeated Bellman backups, such as the one used for the Bayesian dynamic programming approach. The next algorithm we review proposes some solutions to these problems.</p>
<p>We take this opportunity to draw the reader's attention to the survey paper on online POMDP planning algorithms [Ross et al., 2008c], which provides a comprehensive review and empirical evaluation of a range of search-based POMDP solving algorithms, including options for combining offline and online methods in the context of forward search trees. Some of these methods may be much more efficient than those presented above and could be applied to plan in the BAMDP model.</p>
<p>Bayesian Sparse Sampling</p>
<p>Wang et al. [Wang et al., 2005] present an online planning algorithm that estimates the optimal value function of a BAMDP (Equation 4.3) using Monte-Carlo sampling. This algorithm is essentially an adaptation of the Sparse Sampling algorithm [Kearns et al., 1999] to BAMDPs. The original Sparse Sampling approach is very simple: a forward-search tree is grown from the current state to a fixed depth d. At each internal   node, a fixed number C of next states are sampled from a simulator for each action in A, to create C|A| children. This is in contrast to the Forward Search approach of Algorithm 2, which considers all possible next states. Values at the leaves are estimated to be zero and the values at the internal nodes are estimated using the Bellman equation based on their children's values. The main feature of Sparse Sampling is that it can be shown to achieve low error with high probability in a number of samples independent of the number of states [Kearns et al., 1999]. A practical limitation of this approach is the exponential dependence on planning horizon.
return V 0 (s, φ) 4: end if 5: maxQ ← −∞ 6: for all a ∈ A do 7: q ← R(s, a) 8: φ s,a ← φ s,q ← q + γ φ s,a,s s ∈S φ s,a,s ForwardSearch-BAMDP(s , φ , d − 1) 12: φ s,a,s ← φ s,
To extend this to BAMDP, Bayesian Sparse Sampling introduces the following modifications. First, instead of growing the tree evenly by looking at all actions at each level of the tree, the tree is grown stochastically. Actions are sampled according to their likelihood of being optimal, according to their Q-value distributions (as defined by the Dirichlet posteriors); next states are sampled according to the Dirichlet posterior on the model. This is related to Thompson sampling ( §3.3), in that actions are sampled according to their current posterior. The difference is that the Bayesian Sampling explicitly considers the longterm return (as estimated from Monte-Carlo sampling of the posterior over model parameters), whereas Thompson sampling considers just the posterior over immediate rewards. The specific algorithm reported in [Wang et al., 2005] proposes a few improvements on this basic framework: such as, in some cases, sampling from the mean model (rather than the posterior) to reduce the variance of the Q-value estimates in the tree, and using myopic Thompson sampling (rather than the sequential estimate) to decide which branch to sample when growing the tree. This approach requires repeatedly sampling from the posterior to find which action has the highest Q-value at each state node in the tree. This can be very time consuming, and thus, so far the approach has only been applied to small MDPs.</p>
<p>HMDP: A linear program approach for the hyper-state MDP</p>
<p>Castro and Precup [Castro and Precup, 2007] present a similar approach to [Wang et al., 2005]. However, their approach differs on two main points. First, instead of maintaining only the posterior over models, they also maintain Q-value estimates at each state-action pair (for the original MDP states, not the hyper-states) using standard Qlearning [Sutton and Barto, 1998]. Second, values of the hyper-states in the stochastic tree are estimated using linear programming [Puterman, 1994] instead of dynamic programming. The advantage of incorporating Q-learning is that it provides estimates for the fringe nodes which can be used as constraints in the LP solution. There is currently no theoretical analysis to accompany this approach. However, the empirical tests on small simulation domains show somewhat better performance than the method of [Wang et al., 2005], but with similar scalability limitations.</p>
<p>Branch and bound search</p>
<p>Branch and bound is a common method for pruning a search tree. The main idea is to maintain a lower bound on the value of each node in the tree (e.g., using partial expansion of the node to some fixed depth with a lower-bound at the leaf) and then prune nodes whenever they cannot improve the lower-bound (assuming we are maximizing values). In the context of BAMDPs, this can be used to prune hyper-state nodes in the forward search tree [Paquet et al., 2005]. The challenge in this case is to find good bounds; this is especially difficult given the uncertainty over the underlying model. The method has been used in the context of partially observable BAMDP [Png andPineau, 2011, Png, 2011] using a naive heuristic, D d=0 γ d R max , where D is the search depth and R max is the maximum reward. The method was applied successfully to solve simulated dialogue management problems; computational scalability was achieved via a number of structural constraints, including the parameter tying method proposed by .</p>
<p>BOP: Bayesian Optimistic Planning</p>
<p>The BOP method [R. Fonteneau, 2013] is a variant on Branch and bound search in which nodes are expanded based on an upper-bound on their value. The upper-bound at a root node, s 0 , is calculated by building an optimistic subtree, T + , where leaves are expanded using: x t = arg max x∈L(T + ) P (x)γ d , where L(T + ) denotes the fringe of the forward search tree, P (x) denotes the probability of reaching node x and d denotes the depth of node x.</p>
<p>Theoretical analysis shows that the regret of BOP decreases exponentially with the planning budget. Empirical results on the classic 5-state Chain domain (Figure 4.1) confirm that the performance improves with the number of nodes explored in the forward search tree.</p>
<p>BAMCP: Bayes-adaptive Monte-Carlo planning</p>
<p>In recent years, Monte-Carlo tree search methods have been applied with significant success to planning problems in games and other domains [Gelly et al., 2012]. The BAMCP approach exploits insights from this family of approaches to tackle the problem of joint learning and planning in the BAMDP [Guez et al., 2012].</p>
<p>BAMCP provides a Bayesian adaptation of an algorithm, called Upper Confidence bounds applied to Trees (UCT) [Kocsis and Szepesvari, 2006]. This approach is interesting in that it incorporates two different policies (UCT and rollout) to traverse and grow the forward search tree. Starting at the root node, for any node that has been previously visited, it uses the UCT criteria
a * = arg max a Q(s, h, a) + c log n(s, h) n(s, h, a)
to select actions. Along the way, it updates the statistics n(s, h) (the number of times the node corresponding to state s and history h has been visited in the search tree) and n(s, h, a) (the number of times action a was chosen in this node); these are initialized at 0 when the tree is created. Once it reaches a leaf node, instead of using a default value function (or bound), it first selects any untried action (updating its count to 1) and then continues to search forward using a rollout policy until it reaches a given depth (or terminal node). The nodes visited by the rollout policy are not added to the tree (i.e., no n(·) statistics are preserved).</p>
<p>To apply this to the Bayesian context, BAMCP must select actions according to the posterior of the model parameters. Rather than sampling multiple models from the posterior, BAMCP samples a single model P t at the root of the tree and uses this same model (without posterior updating) throughout the search tree to sample next states, after both UCT and rollout actions. In practice, to reduce computation in large domains, the model P t is not sampled in its entirety at the beginning of the tree building process, rather, it is generated lazily as samples are required.</p>
<p>In addition, to further improve efficiency, BAMCP uses learning within the forward rollouts to direct resources to important areas of the search space. Rather than using a random policy for the rollouts, a model-free estimate of the value function is maintained
Q(s t , a t ) ←Q(s t , a t ) + α r t + γ max aQ (s t+1 , a) −Q(s t , a t ) ,
and actions during rollouts are selected according to the -greedy policy defined by this estimatedQ function.</p>
<p>BAMCP is shown to converge (in probability) to the optimal Bayesian policy (denoted V * (s, h) in general, or V * (s, φ) when the posterior is constrained to a Dirichlet distribution). The main complexity result for BAMCP is based on the UCT analysis and shows that the error in estimating V (s t , h t ) decreases as O log n(s t , h t ) /n(s t , h t ) .</p>
<p>Empirical evaluation of BAMCP with a number of simulation domains has shown that it outperforms Bayesian Dynamic Programming, the Value of Information heuristic, BFS3 [Asmuth and Littman, 2011], as well as BEB [Kolter and Ng, 2009] and SmartSampler [Castro and Precup, 2010], both of which will be described in the next section. A good part of this success could be attributed to the fact that unlike many forward search sparse sampling algorithm (e.g., BFS3), BAMCP can take advantage of its learning during rollouts to effectively bias the search tree towards good solutions.</p>
<p>Methods with Exploration Bonus to Achieve PAC Guarantees</p>
<p>We now review a class of algorithms for the BAMDP model that are guaranteed to select actions such as to incur only a small loss compared to the optimal Bayesian policy. Algorithmically, these approaches are similar to those examined in §4.5 and typically require forward sampling of the model and decision space. Analytically, these approaches have been shown to achieve bounded error in a polynomial number of steps using analysis techniques from the Probably Approximately Correct (PAC) literature. These methods are rooted in earlier papers showing that reinforcement learning in finite MDPs can be achieved in a polynomial number of steps [Kearns and Singh, 1998, Brafman and Tennenholtz, 2003, Strehl and Littman, 2005. These earlier papers did not assume a Bayesian learning framework; the extension to Bayesian learning was first established in the BOSS (Best of Sampled Sets) approach. The main idea behind many of the methods presented in this section is the notion of Optimism in the Face of Uncertainty, which suggests that, when in doubt, an agent should act according to an optimistic model of the MDP; in the case where the optimistic model is correct, the agent will gather reward, and if not, the agent will receive valuable information from which it can infer a better model.</p>
<p>METHODS WITH EXPLORATION BONUS TO ACHIEVE PAC GUARANTEES57</p>
<p>BFS3: Bayesian Forward Search Sparse Sampling</p>
<p>The BFS3 approach [Asmuth and Littman, 2011] is an extension to the Bayesian context of the Forward Search Sparse Sampling (FSSS) approach [Walsh et al., 2010]. The Forward Search Sparse Sampling itself extends the Sparse Sampling approach [Kearns et al., 1999] described above in a few directions. In particular, it maintains both lower and upper bounds on the value of each state-action pair, and uses this information to direct forward rollouts in the search tree. Consider a node s in the tree, then the next action is chosen greedily with respect to the upper-bound U (s, a). The next state s is selected to be the one with the largest difference between its lower and upper bound (weighted by the number of times it was visited, denoted by n(s, a, s )), i.e., s ← arg max s ∼P (·|s,a) U (s ) − L(s ) n(s, a, s ). This is repeated until the search tree reaches the desired depth.</p>
<p>The BFS3 approach takes this one step further by building the sparse search tree over hyper-states (s, φ), instead of over simple states s. As with most of the other approaches presented in this section, the framework can handle model uncertainty over either the transition parameters and/or the reward parameters.</p>
<p>Under certain conditions, one can show that BFS3 chooses at most a polynomial number of sub-optimal actions compared to an policy.</p>
<p>Theorem 4.1. [Asmuth, 2013] 1 With probability at least 1 − δ, the expected number of sub--Bayes-optimal actions taken by BFS3 is at most BSA(S + 1)d/δ t under assumptions on the accuracy of the prior and optimism of the underlying FSSS procedure.</p>
<p>Empirical results show that the method can be used to solve small domains (e.g., 5x5 grid) somewhat more effectively than a non-Bayesian method such as RMAX [Brafman and Tennenholtz, 2003]. Results also show that BFS3 can take advantage of structural assumptions in the prior (e.g., parameter tying) to tackle much larger domains, up to 10 16 states.</p>
<p>BOSS: Best of Sample Sets</p>
<p>As per other model-based Bayesian RL approaches presented above, BOSS [Asmuth et al., 2009] maintains uncertainty over the model parameters using a parametric posterior distribution and incorporates new information by updating the posterior. In discrete domains, this posterior is also typically represented using Dirichlet distributions.</p>
<p>When an action is required, BOSS draws a set of sample models M i , i = 1, . . . , K, from the posterior φ t . It then creates a hyper-model M#, which has the same state space S as the original MDP, but has K|A| actions, where a ij corresponds to taking action a j ∈ A in model M i . The transition function for action a i· is constructed directly by taking the sampled transitions P i from M i . It then solves the hypermodel M# (e.g., using dynamic programming methods) and selects the best action according to this hyper-model. This sampling procedure is repeated a number of times (as determined by B, the knownness parameter) for each state-action pair, after which the policy of the last M# is retained.</p>
<p>While the algorithm is simple, the more interesting contribution is in the analysis. The main theoretical result shows that assuming a certain knownness parameter B, the value at state s when visited by algorithm A at time t (which we denote V At (s t )), is very close to the optimal value of the correct model (denoted V * ) with high probability in all but a small number of steps. For the specific case of Dirichlet priors on the model, it can be shown that the number of necessary samples, f , is a polynomial function of the relevant parameters.</p>
<p>Theorem 4.2. [Asmuth et al., 2009] When the knownness param-
eter B = max s,a f s, a, (1 − γ) 2 , δ |S||A| , δ |S| 2 |A| 2 K , then with proba- bility at least 1 − 4δ, V At (s t ) ≥ V * (s t ) − 4 in all but ξ( , δ) = O |S||A|B
(1−γ) 2 ln 1 δ ln 1 (1−γ) steps. Experimental results show that empirically, BOSS performs similarly to BEETLE and Bayesian dynamic programming in simple problems, such as the Chain problem (Figure 4.1). BOSS can also be extended to take advantage of structural constraints on the state relations to tackle larger problems, up to a 6 × 6 maze.</p>
<p>Castro and Precup [Castro and Precup, 2010] proposed an improvement on BOSS, called SmartSampler, which addresses the problem of how many models to sample. The main insight is that the number of sampled models should depend on the variance of the posterior distribution. When the variance is larger, more models are necessary to achieve good value function estimation. When the variance is reduced, it is sufficient to sample a smaller number of models. Empirical results show that this leads to reduced computation time and increased accumulated reward, compared to the original BOSS.</p>
<p>BEB: Bayesian Exploration Bonus</p>
<p>Bayesian Exploration Bonus (BEB) is another simple approach for achieving Bayesian reinforcement learning with guaranteed bounded error within a small number of samples [Kolter and Ng, 2009]. The algorithm simply chooses actions greedily with respect to the following value function:
V * t (s, φ) = max a∈A    R(s, a)+ β 1 + s ∈S φ s,a,s + s ∈S P (s |s, φ, a)Ṽ * t−t (s , φ)   
Here the middle term on the right-hand side acts as exploration bonus, whose magnitude is controlled by parameter β. It is worth noting that the posterior (denoted by φ) is not updated in this equation, and thus, the value function can be estimated via standard dynamic programming over the state space (similar to BOSS and Bayesian dynamic programming).</p>
<p>The exploration bonus in BEB decays with 1/n (consider n ∼ s ∈S φ s,a,s ), which is significantly faster than similar exploration bonuses in the PAC-MDP literature, for example the MBIE-EB algorithm [Strehl and Littman, 2008], which typically declines with 1/ √ n. It is possible to use this faster decay because BEB aims to match the performance of the optimal Bayesian solution (denoted V * (s, φ)), as defined in Theorem 4.3. We call this property PAC-BAMDP. This is in contrast to BOSS where the analysis is with respect to the optimal solution of the correct model (denoted V * (s)).</p>
<p>Theorem 4.3. [Kolter and Ng, 2009] Let A t denote the policy followed by the BEB algorithm (with β = 2H 2 ) at time t, and let s t and φ t be the corresponding state and posterior belief. Also suppose we stop updating the posterior for a state-action pair when s ∈S φ s,a,s ≥ 4H 3 / . Then with probability at least 1 − δ, for a planning horizon of H, we have
V At (s t , φ t ) ≥ V * (s t , φ t ) − for all but m time steps, where m = O |S||A|H 6 2 log |S||A| δ .
It is worth emphasizing that because BEB considers optimality with respect to the value of the Bayesian optimal policy, it is possible to obtain theoretical results that are tighter (i.e., fewer number of steps) with respect to the optimal value function. But this comes at a cost, and in particular, there are some MDPs for which the BEB algorithm, though it performs closely to the Bayesian optimum, V * (s, φ), may never find the actual optimal policy, V * (s). This is illustrated by an example in the dissertation of Li [Li, 2009] (see Example 9, p.80), and formalized in Theorem 4.4 (here n(s, a) denotes the number of times the state-action pair s, a has been observed).</p>
<p>Theorem 4.4. [Kolter and Ng, 2009] Let A t denote the policy followed by an algorithm using any (arbitrary complex) exploration bonus that is upper bounded by β n(s,a) p , for some constants β and p &gt; 1/2. Then there exists some MDP, M, and parameter, 0 (β, p), such that with probability greater than δ 0 = 0.15,
V At (s t ) &lt; V * (s t ) − 0 ,
will hold for an unbounded number of time steps.</p>
<p>Empirical evaluation of BEB showed that in the Chain problem (Figure 4.1), it could outperform a (non-Bayesian) PAC-MDP algorithm in terms of sample efficiency and find the correct optimal policy.</p>
<p>VBRB: Variance-Based Reward Bonus</p>
<p>The Variance-based reward approach also tackles the problem of Bayesian RL by applying an exploration bonus. However in this case, the exploration bonus depends on the variance of the model parameters with respect to the posterior [Sorg et al., 2010], i.e.,
V * t (s, φ) = max a∈A    R(s, φ, a) +R s,φ,a + s ∈S P (s |s, φ, a)Ṽ * t−t (s , φ)    ,
where the reward bonusR s,φ,a is defined as
β R σ R(s,φ,a) + β P s ∈S σ 2 P (s |s,φ,a) , with σ 2 R(s,φ,a) = θ R(s, θ, a) 2 b(θ)dθ − R(s, φ, a) 2 , (4.4) σ 2 P (s |s,φ,a) = θ P (s |s, θ, a) 2 b(θ)dθ − P (s |s, φ, a) 2 ,(4.5)
and β R and β P are constants controlling the magnitude of the exploration bonus. 2 The main motivation for considering a variance-based bonus is that the error of the algorithm can then be analyzed by drawing on Chebyshev's inequality (which states that with high probability, the deviation of a random variable from its mean is bounded by a multiple of its variance). The main theoretical result concerning the variancebased reward approach bounds the sample complexity with respect to the optimal policy of the true underlying MDP, like BOSS (and unlike BEB).</p>
<p>Theorem 4.5. [Sorg et al., 2010] Let the sample complexity of state s and action a be C(s, a) = f b 0 , s, a, 1
4 (1 − γ) 2 , δ |S||A| , δ 2|S| 2 |A| 2 . Let the internal reward be defined asR(s, φ, a) = 1 √ ρ σ R(s,φ,a) + γ|S| 1−γ s σ 2 P (s |s,φ,a) with ρ = δ 2|S| 2 |A| 2 .
Let θ * be the random true model parameters distributed according to the prior belief φ 0 . The variance-based reward algorithm will follow a 4 -optimal 2 The approach is presented here in the context of Dirichlet distributions. There are ways to generalize this use of variance of the reward as an exploration bonus for other Bayesian priors [Sorg et al., 2010]. policy from its current state, with respect to the MDP θ * , on all but O s,a C(s,a)</p>
<p>(1−γ) 2 ln 1 δ ln 1 (1−γ) time steps with probability at least 1−4δ.</p>
<p>For the case where uncertainty over the transition model is modelled using independent Dirichlet priors (as we have considered throughout this section), the sample complexity of this approach decreases as a function of O(1/ √ n). This is consistent with other PAC-MDP results, which also bound the sample complexity to achieve small error with respect to the optimal policy of the true underlying MDP. However, it is not as fast as the BEB approach, which bounds error with respect to the best Bayesian policy.</p>
<p>Empirical results for the variance-based approach show that it is competitive with BEETLE, BOSS and BEB on versions of the Chain problem that use parameter tying to reduce the space of model uncertainty, and shows better performance for the variant of the domain where uncertainty over all state-action pairs is modelled with an independent Dirichlet distribution. Results also show superior performance on a 4 × 4 grid-world inspired by the Wumpus domain of [Russell and Norvig, 2002].</p>
<p>BOLT: Bayesian Optimistic Local Transitions</p>
<p>Both BEB and the variance-based reward approach encourage exploration by putting an exploration bonus on the reward, and solving this modified MDP. The main insight in BOLT is to put a similar exploration bonus on the transition probabilities [Araya-Lopez et al., 2012]. The algorithm is simple and modelled on BOSS. An extended action space is considered: A = A × S, where each action in ζ = (a, σ) ∈ A has transition probabilityP (s |s, ζ) = E P (s |s, a)|s, φ, λ η s,a,σ , with φ being the posterior on the model parameters and λ η being a set of η imagined transitions λ η s,a,σ = {(s, a, σ), . . . , (s, a, σ)}. The extended MDP is solved using standard dynamic programming methods over a horizon H, where H is a parameter of BOLT, not the horizon of the original problem. The effect of this exploration bonus is to consider an action ζ that takes the agent to state σ ∈ S with greater probability than has been observed in the data, and by optimizing a pol-icy over all such possible actions, we consider an optimistic evaluation over the possible set of models. The parameter η controls the extent of the optimistic evaluation, as well as its computational cost (larger η means more actions to consider). When the posterior is represented using standard Dirichlet posteriors over model parameters, as considered throughout this section, it can be shown that BOLT is always optimistic with respect to the optimal Bayesian policy [ Araya-Lopez et al., 2012]. 
V At (s t , φ t ) &lt; V * (s t , φ t ) − , for all but O( |S||A|η 2 2 (1−γ) 2 ) = O( |S||A|H 2 2 (1−γ) 2 ) time steps.
To simplify, the parameter H can be shown to depend on the desired correctness ( , δ).</p>
<p>Empirical evaluation of BOLT on the Chain problem shows that while it may be outperformed by BEB with well-tuned parameters, it is more robust to the choice of parameter (H for BOLT, β for BEB). The authors suggest that BOLT is more stable with respect to the choice of parameter because optimism in the transitions is bounded by laws of probability (i.e., even with a large exploration bonus, η, the effect of extended actions ζ will simply saturate), which is not the case when the exploration bonus is placed on the rewards.</p>
<p>BOLT was further extended, in the form of an algorithm called POT: Probably Optimistic Transition [Kawaguchi and Araya-Lopez, 2013], where the parameter controlling the extended bonus, η, is no longer constant. Instead, the transition bonus is estimated online for each state-action pair using a UCB-like criteria that incorporates the notion of the current estimate of the transition and the variance of that estimate (controlled by a new parameter β). One advantage of the method is that empirical results are improved (compared to BOLT, BEB, and VBRB) for domains where the prior is good, though empirical performance can suffer when the prior is misspecified. POT is also shown to have tighter bounds on the sample complexity. However, the analysis is done with respect to a modified optimality criteria, called "Probably Upper Bounded Belief-based Bayesian Planning", which is weaker than the standard Bayesian optimality, as defined in §4.2. Table 4.1 summarizes the key features of the online Bayesian RL methods presented in this section. This can be used as a quick reference to visualize similarities and differences between the large number of related approaches. It could also be used to devise new approaches, by exploring novel combinations of the existing components.</p>
<p>Extensions to Unknown Rewards</p>
<p>Most work on model-based Bayesian RL focuses on unknown transition functions and assumes the reward function is known. This is seen as the more interesting case because there are many domains in which dynamics are difficult to model a priori, whereas the reward function is set by domain knowledge. Nonetheless a number of papers explicitly consider the case where the reward function is uncertain [Dearden et al., 1999, Strens, 2000, Wang et al., 2005, Castro and Precup, 2007, Sorg et al., 2010]. The BAMDP model extends readily to this case, by extending the set of hyper-states to full set of unknown parameters: S ∈ S × Θ, with θ = (φ, ϑ), with ϑ representing the prior over the unknown reward function. The next challenge is to choose an appropriate distribution for this prior.</p>
<p>The simplest model is to assume that each state-action pair generates a binary reward, ϑ = {0, 1}, according to a Bernoulli distribution [Wang et al., 2005, Castro andPrecup, 2007]. In this case, a conjugate prior over the reward can be captured with a Beta distribution (see §2.5.1). This is by far the most common approach when dealing with bandit problems. In the cases where the reward function is drawn from a set of discrete values, ϑ = {r 1 , r 2 , . . . , r k }, it can be modeled by a Dirichlet prior over those values. However, many MDP domains are characterized by more complex (e.g., real-valued) reward functions.</p>
<p>In those cases, an alternative is to assume that the reward is   Gaussian distributed, i.e., ϑ = {µ, σ}. In this case the choice of prior on the standard deviation σ is of particular importance; a uniform prior could lead the posterior to converge to σ → 0. Following Sterns [Strens, 2000], we can define the precision ψ = 1/σ with prior density f (ψ) ∝ ψ exp(−ψ 2 σ 2 0 /2); this will have a maximum at σ = σ 0 . We can also define the prior on the mean to be f (µ) ∝ N (µ 0 , σ 2 ). The parameters (µ 0 , σ 0 ) capture any prior knowledge about the mean and standard deviation of the reward function. After n observations of the reward with sample meanμ and sample varianceσ, the posterior distribution on ψ is defined by: f (ψ) ∝ ψ n−1 exp − ψ 2 (nσ + σ 2 0 )/2 , and the posterior on µ is defined by f (µ) ∝ N (μ, σ 2 /n), where σ = 1/ψ. Note that the posterior on the variance is updated first and used to calculate the posterior on the mean.</p>
<p>Most of the online model-based BRL algorithms presented in Table 4.1 extend readily to the case of unknown rewards, with the added requirement of sampling the posterior over rewards (along with the posterior over transition functions). For an algorithm such as BEB, that uses an exploration bonus in the value function, it is also necessary to incorporate the uncertainty over rewards within that exploration bonus. Sorg et al, [Sorg et al., 2010] deals with the issue by expressing the bonus over the variance of the unknown parameters (including the unknown rewards), as in Eq. 4.5.</p>
<p>Extensions to Continuous MDPs</p>
<p>Most of the work reviewed in this section focuses on discrete domains. In contrast, many of the model-free BRL methods concern the case of continuous domains, where the Bayesian approach is used to infer a posterior distribution over value functions, conditioned on the stateaction-reward trajectory observed in the past.</p>
<p>The problem of optimal control under uncertain model parameters for continuous systems was originally introduced by Feldbaum [1961], as the theory of dual control (also referred to as adaptive control or adaptive dual control). Several authors studied the problem for different classes of time-varying systems [Filatov andUnbehauen, 2000, Wittenmark, 1995]: linear time invariant systems under partial observability [Rusnak, 1995], linear time varying Gaussian models with partial observability [Ravikanth et al., 1992], nonlinear systems with full observability [Zane, 1992], and nonlinear systems with partial observability [Greenfield andBrockwell, 2003, Ross et al., 2008b].</p>
<p>This last case is closest mathematically to the Bayes-Adaptive MDP model considered throughout this paper. The dynamical system is described by a Gaussian transition model (not necessarily linear):
s t = g T (s t−1 , a t−1 , V t ),
where g T is a specified function, and V t ∼ N (µ v , Σ v ) is an unknown kvariate normal distribution with mean vector µ v and covariance matrix Σ v . Here the prior distribution on V t is represented using a Normal-Wishart distribution [Ross et al., 2008b]. Particle filters using Monte-Carlo sampling methods are used to track the posterior over the parameters of this distribution. Planning is achieved using a forward search algorithm similar to Algorithm 2.</p>
<p>The Bayesian DP strategy ( §4.4) and its analysis were also extended to the continuous state and action space and average-cost setting, under the assumption of smoothly parameterizable dynamics [Abbasi-Yadkori and Szepesvari, 2015]. The main algorithmic modification in this extension is to incorporate a specific schedule for updating the policy, that is based on a measure of uncertainty.</p>
<p>Another approach proposed by Dallaire et al. [2009] allows flexibility over the choice of transition function. Here the transition and reward functions are defined by:
s t = g T (s t−1 , a t−1 ) + T , (4.6) r t = g R (s t , a t ) + R ,(4.7)
where g T and g R are modelled by (independent) Gaussian Processes (as defined in §2.5.2) and T and R are zero-mean Gaussian noise terms. Belief tracking is done by updating the Gaussian process. Planning is achieved by a forward search tree similar to Algorithm 2. It is also worth pointing out that the Bayesian Sparse Sampling approach [Wang et al., 2005] has also been extended to the case where the reward function is expressed over a continuous action space and represented by a Gaussian process. In this particular case, the authors only considered Bayesian inference of the reward function for a single state and a single-step horizon problem (i.e., a bandit with continuous actions). Under these conditions, inferring the reward function is the same as inferring the value function, so no planning is required.</p>
<p>Extensions to Partially Observable MDPs</p>
<p>We can extend the BAMDP model to capture uncertainty over the parameters of a POMDP (as defined in §2.3) by introducing the Bayes-Adaptive POMDP (BAPOMDP) model [Ross et al., 2008a[Ross et al., , 2011. Given a POMDP model, S, A, O, P, Ω, P 0 , R , we define a corresponding BAPOMDP as follows:</p>
<p>Model 7 (Bayes-Adaptive POMDP) Define a BAPOMDP M to be a tuple S , A, P , P 0 , R where • S is the set of hyper-states, S × Φ × Ψ, • A is the set of actions, • P (·|s, φ, ψ, a) is the transition function between hyper-states, conditioned on action a being taken in hyper-state (s, φ, ψ),</p>
<p>• Ω (·|s, φ, ψ, a) is the observation function, conditioned on action a being taken in hyper-state (s, φ, ψ), • P 0 ∈ P(S × Φ × Ψ) combines the initial distribution over physical states, with the prior over transition functions φ 0 and the prior over observation functions ψ 0 , • R (s, φ, ψ, a) = R(s, a) represents the reward obtained when action a is taken in state s.</p>
<p>Here Φ and Ψ capture the space of Dirichlet parameters for the conjugate prior over the transition and observation functions, respectively. The Bayesian approach to learning the transition P and observation Ω involves starting with a prior distribution, which we denote φ 0 and ψ 0 , and maintaining the posterior distribution over φ and ψ after observing the history of action-observation-rewards. The belief can be tracked using a Bayesian filter, with appropriate handing of the observations. Using standard laws of probability and independence assumptions, we have P(s , φ , ψ , z|s, φ, ψ, a) = P(s |s, a, φ)P(o|a, s , ψ)P(φ |φ, s, a, s )P (ψ |ψ, a, s , o).</p>
<p>(4.8)</p>
<p>As in the BAMDP case (Eq. 4.2), P(s |s, a, φ) corresponds to the expected transition model under the Dirichlet posterior defined by φ, and P(φ |φ, s, a, s ) is either 0 or 1, depending on whether φ corresponds to the posterior after observing transition (s, a, s ) from prior φ. Hence P(s |s, a, φ) = Also as in the BAMDP, the number of possible hyper-states, (s, φ, ψ), grows exponentially (by a factor of |S|) with the prediction horizon. What is particular to the BAPOMDP is that the number of possible beliefs for a given trajectory, (a 0 , o 1 , . . . , a t , o t+1 ), also grows exponentially. In contrast, given an observed trajectory, the belief in the BAMDP corresponds to a single hyper-state. In the BAPOMDP, value-based planning requires estimating the Bellman equation over all possible hyper-states for every belief:
V * b t (s, φ, ψ) = max a s,φ,ψ R (s, φ, ψ, a)b t (s, φ, ψ) + γ o∈O P(o|b t−1 , a)V * τ (b t , a, o) .
(4.9) This is intractable for most problems, due to the large number of possible beliefs. It has been shown that Eq. 4.9 can be approximated with an -optimal value function defined over a smaller finitedimensional space [Ross et al., 2011]. In particular, there exists a point where if we simply stop incrementing the counts (φ, ψ), the value function of that approximate BAPOMDP (where the counts are bounded) approximates the BAPOMDP within some &gt; 0. In practice, that space is still very large.</p>
<p>The Minerva approach [Jaulmes et al., 2005] overcomes this problem by assuming that there is an oracle who is willing to provide full state identification at any time step. This oracle can be used to deterministically update the counts ψ and φ (rather than keeping a probability distribution over them, as required when the state is not fully observable). It is then possible to sample a fixed number of models from the Dirichlet posterior, solve these models using standard POMDP planning techniques, and sample an action from the solved models according to the posterior weight over the corresponding model.</p>
<p>The assumption of a state oracle in Minerva is unrealistic for many domains. The approach of Atrash and Pineau [2009] weakens the assumption to an action-based oracle, which can be called on to reveal the optimal POMDP action at any time step for the current belief over states. This oracle does not consider the uncertainty over model parameters and computes its policy based on the correct parameters. There are some heuristics to decide when to query the oracle, but this is largely an unexplored question.</p>
<p>An alternative approach for the BAPOMDP is to use the Bayes risk as a criterion for choosing actions [Doshi et al., 2008, Doshi-Velez et al., 2011. This framework considers an extended action set, which augments the initial action set with a set of query actions corresponding to a consultation with an oracle. Unlike the oracular queries used in Atrash and Pineau [2009], which explicitly ask for the optimal action (given the current belief), the oracular queries in Doshi et al. [2008] request confirmation of an optimal action choice: I think a i is the best action. Should I do a i ? If the oracle answers to the negative, there is a follow-up query: Then I think a j is best. Is that correct?, and so on until a positive answer is received. In this framework, the goal is to select actions with smallest expected loss, defined here as the Bayes risk (cf. the regret definition in Chapter 2.1):
BR(a) = s,φ,ψ Q(b t (s, φ, ψ, a))b t (s, φ, ψ)− s,φ,ψ Q(b t (s, φ, ψ, a * ))b t (s, φ, ψ),
where Q(·) is just Eq. 4.9 without the maximization over actions and a * is the optimal action at b t (s, φ, ψ). Because the second term is independent of the action choice, to minimize the Bayes risk, we simply consider maximizing the first term over actions. The analysis of this algorithm has yielded a bound on the number of samples needed to ensure -error. The bound is quite loose in practice, but at least provides some upper-bound on the number of queries needed to achieve good performance. Note that this analysis is based on the Bayes risk criterion that provides a myopic view of uncertainty (i.e., it assumes that the next action will resolve the uncertainty over models).</p>
<p>The planning approach suggested by Ross et al. [2011] aims to approximate the optimal BAPOMDP strategy by employing a forward search similar to that outlined in Algorithm 2. In related work, Png and Pineau [2011] use a branch-and-bound algorithm to approximate the BAPOMDP solution. Many of the other techniques outlined in Table 4.1 could also be extended to the BAPOMDP model.</p>
<p>Finally, it is worth mentioning that the method of Dallaire et al. [2009], described in §4.8, is also able to handle continuous partially observable domains by using an additional Gaussian process for the observation function.</p>
<p>Extensions to Other Priors and Structured MDPs</p>
<p>The work and methods presented above focus on the case where the model representation consists of a discrete (flat) set of states. For many larger domains, it is common to assume that the states are arranged in a more sophisticated structure, whether a simple clustering or more complicated hierarchy. It is therefore interesting to consider how Bayesian reinforcement learning methods can be used in those cases.</p>
<p>The simplest case, called parameter tying in previous sections, corresponds to the case where states are grouped according to a pre-defined clustering assignment. In this case, it is common to aggregate learned parameters according to this assignment , Sorg et al., 2010, Png and Pineau, 2011. The advantage is that there are fewer parameters to estimate, and thus, learning can be achieved with fewer samples. The main downside is that this requires a hand-coded assignment. In practice, it may also be preferable to use a coarser clustering than is strictly correct in order to improve variance (at the expense of more bias). This is a standard model selection problem.</p>
<p>More sophisticated approaches have been proposed to automate the process of clustering. In cases where the (unknown) model parameters can be assumed to be sparse, it is possible to incorporate a sparseness assumption in the Dirichlet estimation through use of a hierarchical prior [Friedman and Singer, 1999]. An alternative is to maintain a posterior over the state clustering. Non-parametric models of state clustering have been considered using a Chinese Restaurant Process [Asmuth et al., 2009, Asmuth and. These could be extended to many of the other model-based BRL methods described above. A related approach was proposed for the case of partially observable MDPs, where the posterior is expressed over the set of latent (unobserved) states and is represented using a hierarchical Dirichlet process [Doshi-Velez, 2009].</p>
<p>A sensibly different approach proposes to express the prior over the space of policies, rather than over the space of parametric models [Doshi-Velez et al., 2010]. The goal here is to leverage trajectories acquired from an expert planner, which can be used to define this prior over policies. It is assumed that the expert knew something about the domain when computing its policy. An interesting insight is to use the infinite POMDP model [Doshi-Velez, 2009] to specify the policy prior, by simply reversing the role of actions and observations; a preference for simpler policies can be expressed by appropriately setting the hyper-parameters of the hierarchical Dirichlet process.</p>
<p>A few works have considered the problem of model-based BRL in cases where the underlying MDP has specific structure. In the case of factored MDPs, Ross and Pineau [2008] show that it is possible to simultaneously maintain a posterior over the structure and the parameters of the domain. The structure in this case captures the bipartite dynamic Bayes network that describes the state-to-state transitions. The prior over structures is maintained using an MCMC algorithm with appropriate graph to graph transition probabilities. The prior over model parameters is conditioned on the graph structure. Empirical results show that in some domains, it is more efficient to simultaneously estimate the structure and the parameters, rather than estimate only the parameters given a known structure.</p>
<p>When the model is parametrized, Gopalan and Mannor [2015] use an information theoretic approach to quickly find the set of "probable" parameters in a pseudo-Bayesian setting. They show that the regret can be exponentially lower than models where the model is flat. Furthermore, the analysis can be done in a frequentist fashion leading eventually to a logarithmic regret.</p>
<p>In multi-task reinforcement learning, the goal is learn a good policy over a distribution of MDPs. A naive approach would be to assume that all observations are coming from a single model, and apply Bayesian RL to estimate this mean model. However by considering a hierarchical infinite mixture model over MDPs (represented by a hierarchical Dirichlet process), the agent is able to learn a distribution over different classes of MDPs, including estimating the number of classes and the parameters of each class [Wilson et al., 2007].</p>
<p>Model-free Bayesian Reinforcement Learning</p>
<p>As discussed in §2.4, model-free RL methods are those that do not explicitly learn a model of the system and only use sample trajectories obtained by direct interaction with the system. In this section, we present a family of value function Bayesian RL (BRL) methods, called Gaussian process temporal difference (GPTD) learning, and two families of policy search BRL techniques: a class of Bayesian policy gradient (BPG) algorithms and a class of Bayesian actor-critic (BAC) algorithms.</p>
<p>Value Function Algorithms</p>
<p>As mentioned in §2.4, value function RL methods search in the space of value functions, functions from the state (state-action) space to real numbers, to find the optimal value (action-value) function, and then use it to extract an optimal policy. In this section, we focus on the policy iteration (PI) approach and start by tackling the policy evaluation problem, i.e., the process of estimating the value (action-value) function V µ (Q µ ) of a given policy µ (see §2.4). In policy evaluation, the quantity of interest is the value function of a given policy, which is unfortunately hidden. However, a closely related random variable, the reward signal, is observable. Moreover, these hidden and observable quantities are related through the Bellman equation (Eq. 2.4). Thus, it is possible to extract information about the value function from the noisy samples of the reward signals. A Bayesian approach to this problem employs the Bayesian methodology to infer a posterior distribution over value functions, conditioned on the state-reward trajectory observed while running a MDP. Apart from the value estimates given by the posterior mean, a Bayesian solution also provides the variance of values around this mean, supplying the practitioner with an accuracy measure of the value estimates.</p>
<p>In the rest of this section, we study a Bayesian framework that uses a Gaussian process (GP) approach to this problem, called Gaussian process temporal difference (GPTD) learning [Engel et al., 2003, 2005a, Engel, 2005. We then show how this Bayesian policy evaluation method (GPTD) can be used for control (to improve the policy and to eventually find a good or an optimal policy) and present a Bayesian value function RL method, called GPSARSA.</p>
<p>Gaussian Process Temporal Difference Learning</p>
<p>Gaussian process temporal difference (GPTD) learning [Engel et al., 2003, 2005a, Engel, 2005] is a GP-based framework that uses linear statistical models (see §2.5.2) to relate, in a probabilistic way, the underlying hidden value function with the observed rewards, for a MDP controlled by some fixed policy µ. The GPTD framework may be used with both parametric and non-parametric representations of the value function, and applied to general MDPs with infinite state and action spaces. Various flavors of the basic model yields several different online algorithms, including those designed for learning action-values, providing the basis for model-free policy improvement, and thus, full PI algorithms.</p>
<p>Since the focus of this section is on policy evaluation, to simplify the notation, we remove the dependency to the policy µ and use D, P , R, and V instead of D µ , P µ , R µ , and V µ . As shown in §2.2, the value V is the result of taking the expectation of the discounted return D with respect to the randomness in the trajectory and in the rewards collected therein (Eq. 2.3). In the classic frequentist approach, V is not random, since it is the true, albeit unknown, value function of policy µ. On the other hand, in the Bayesian approach, V is viewed as a random entity by assigning it additional randomness due to our subjective uncertainty regarding the MDP's transition kernel P and reward function q (intrinsic uncertainty). We do not know what the true functions P and q are, which means that we are also uncertain about the true value function. We model this additional extrinsic uncertainty by defining V as a random process indexed by the state variable x. In order to separate the two sources of uncertainty inherent in the discounted return process D, we decompose it as
D(s) = E D(s) + D(s) − E D(s) = V (s) + ∆V (s), (5.1) where ∆V (s) def = D(s) − V (s) (5.2)
is a zero-mean residual. When the MDP's model is known, V becomes deterministic and the randomness in D is fully attributed to the intrinsic randomness in the state-reward trajectory, modelled by ∆V . On the other hand, in a MDP in which both the transitions and rewards are deterministic but unknown, ∆V becomes zero (deterministic), and the randomness in D is due solely to the extrinsic uncertainty, modelled by V . We write the following Bellman-like equation for D using its definition (Eq. 2.2) D(s) = R(s) + γD(s ), s ∼ P (·|s).  N (s 0 , s 1 ), . . . , N (s T −1 , s T ) , we write the above set of T equations as
R T = HV T +1 + N T , (5.6)
where H is the following T × (T + 1) matrix:
H =       1 −γ 0 . . . 0 0 0 1 −γ . . . 0 0 . . . . . . . . . . . . . . . 0 0 0 . . . 1 −γ       . (5.7)
Note that in episodic problems, if a goal-state is reached at time-step T , the final equation in (5.5) is
R(s T −1 ) = V (s T −1 ) + N (s T −1 , s T ), (5.8)
and thus, Eq. 5.6 becomes
R T = HV T + N T , (5.9)
where H is the following T × T square invertible matrix with determinant equal to 1,
H =        1 −γ 0 . . . 0 0 0 1 −γ . . . 0 0 . . . . . . . . . . . . . . . 0 0 0 . . . 1 −γ 0 0 0 . . . 0 1        and H −1 =      1 γ γ 2 . . . γ T −1 0 1 γ . . . γ T −2 . . . . . . . . . . . . 0 0 0 . . . 1      .
(5.10) Figure 5.1 illustrates the conditional dependency relations between the latent value variables V (s t ), the noise variables ∆V (s t ), and the observable rewards R(s t ). Unlike the GP regression diagram of Figure 2.1, there are vertices connecting variables from different time steps, making the ordering of samples important. Since the diagram in Figure 5.1 is for the episodic setting, also note that for the last state in each episode (s T −1 in this figure), R(s T −1 ) depends only on V (s T −1 ) and ∆V (s T −1 ) (as in Eqs. 5.8 and 5.9). In order to fully define a probabilistic generative model, we also need to specify the distribution of the noise process N T . In order to derive the noise distribution, we make the following two assumptions (see Appendix B for a discussion about these assumptions):
V (s 0 ) V (s 1 ) V (s 2 ) V (s T −2 ) V (s T −1 ) ∆V (s T −1 ) ∆V (s T −2 ) ∆V (s 0 ) ∆V (s 1 ) ∆V (s 2 ) R(s 0 ) R(s 1 ) R(s T −2 ) R(s T −1 )
Assumption A2 The residuals ∆V T +1 = ∆V (s 0 ), . . . , ∆V (s T ) can be modeled as a Gaussian process.</p>
<p>Assumption A3 Each of the residuals ∆V (s t ) is generated independently of all the others, i.e., E ∆V (s i )∆V (s j ) = 0, for i = j.</p>
<p>By definition (Eq. 5.2), E ∆V (s) = 0 for all s. Using Assumption A3, it is easy to show that E ∆V (x t ) 2 = Var D(x t ) . Thus, denoting Var D(x t ) = σ 2 t , Assumption A2 may be written as ∆V T +1 ∼ N 0, diag(σ T +1 ) . Since N T = H∆V T +1 , we have N T ∼ N (0, Σ) with
Σ = H diag(σ T +1 )H (5.11) =        σ 2 0 + γ 2 σ 2 1 −γσ 2 1 0 . . . 0 0 −γσ 2 1 σ 2 1 + γ 2 σ 2 2 −γσ 2 2 . . . 0 0 . . . . . . . . . . . . . . . 0 0 0 . . . σ 2 T −2 + γ 2 σ 2 T −1 −γσ 2 T −1 0 0 0 . . . −γσ 2 T −1 σ 2 T −1 + γ 2 σ 2 T        .
Eq. 5.11 indicates that the Gaussian noise process N T is colored with a tri-diagonal covariance matrix. If we assume that for all t = 0, . . . , T , σ t = σ, then diag(σ T +1 ) = σ 2 I and Eq. 5.11 may be simplified and written as
Σ = σ 2 HH = σ 2          1 + γ 2 −γ 0 . . . 0 0 −γ 1 + γ 2 −γ . . . 0 0 . . . . . . . . . . . . . . . 0 0 0 . . . 1 + γ 2 −γ 0 0 0 . . . −γ 1 + γ 2          .
Eq. 5.6 (or in case of episodic problems Eq. 5.9), along with the measurement noise distribution of Eq. 5.11, and a prior distribution for V (defined either parametrically or non-parametrically, see §2.5.2), completely specify a statistical generative model relating the value and reward random processes. In order to infer value estimates from a sequence of observed rewards, Bayes' rule can be applied to this generative model to derive a posterior distribution over the value function conditioned on the observed rewards.</p>
<p>In the case in which we place a Gaussian prior over V T , both V T and N T are normally distributed, and thus, the generative model of Eq. 5.6 (Eq. 5.9) will belong to the family of linear statistical models discussed in §2.5.2. Consequently, both parametric and non-parametric treatments of this model (see §2.5.2) may be applied in full to the generative model of Eq. 5.6 (Eq. 5.9), with H given by Eq. 5.7 (Eq. 5.10). Figure 5.2 demonstrates how the GPTD model described in this section is related to the family of linear statistical models and GP regression discussed in §2.5.2.</p>
<p>Observable Process</p>
<p>Unknown Function</p>
<p>Linear Transformation
H GPTD ⇠ N 0, ⌃ = H diag( T )H &gt; 0 B B B B B B B B @ R(s0) . . . R(st) R(st+1)
. . . We are now in a position to write a closed form expression for the posterior moments of V , conditioned on an observed sequence of rewards r T = r(s 0 ), . . . , r(s T −1 ) , and derive GPTD-based algorithms for value function estimation. We refer to this family of algorithms as Monte-Carlo GPTD (MC-GPTD) algorithms.
R(sT 1) 1 C C C C C C C C A = z }| { 2 6 6 6 6 6 4 1 0 . . . 0 0 0 1 . . . 0 0 . . . . . . . . . . . . . . . 0 0 0 . . . 1 0 0 0 . . . 0 1 3 7 7 7 7 7 5 0 B B B B B B B B @ V (s0) . . . V (st) V (st+1) . . . V (sT 1) 1 C C C C C C C C A + z }| { 0 B B B B B B B B @ N (s0, s1) . . . N (st, st+1) N (st+1, st+2) . . . N (sT 1, sT ) 1 C C C C C C C C A R(s t ) = V (s t ) V (s t+1 ) + N (s t , s t+1 ) N (s t , s t+1 ) def = V (s t ) V (s t+1 ) V (s t ) def = D(s t ) V (s t ) Y T = HF T + N T Observable</p>
<p>Parametric Monte-Carlo GPTD Learning:</p>
<p>In the parametric setting, the value process is parameterized as V (s) = φ(s) W , and therefore, V T +1 = Φ W with Φ = φ(s 0 ), . . . , φ(s T ) . As in §2.5.2, if 82CHAPTER 5. MODEL-FREE BAYESIAN REINFORCEMENT LEARNING we use the prior distribution W ∼ N (0, I), the posterior moments of W are given by (5.12) where ∆Φ = ΦH . To have a smaller matrix inversion, Eq. 5.12 may be written as
E W |R T = r T = ∆Φ(∆Φ ∆Φ + Σ) −1 r T , Cov W |R T = r T = I − ∆Φ(∆Φ ∆Φ + Σ) −1 ∆Φ ,E W |R T = r T = (∆ΦΣ −1 ∆Φ + I) −1 ∆ΦΣ −1 r T , Cov W |R T = r T = (∆ΦΣ −1 ∆Φ + I) −1 . (5.13)
Note that in episodic problems H is invertible, and thus, assuming a constant noise variance, i.e., diag(σ T ) = σ 2 I, Eq. 5.13 becomes
E W |R T = r T = (ΦΦ + σ 2 I) −1 ΦH −1 r T , Cov W |R T = r T = σ 2 (ΦΦ + σ 2 I) −1 .
(5.14)</p>
<p>Eq. 5.14 is equivalent to Eq. 2.22 with y T = H −1 r T (see the discussion of Assumption A3). Using Eq. 5.13 (or Eq. 5.14), it is possible to derive both a batch algorithm [Engel, 2005, Algorithm 17] and a recursive online algorithm [Engel, 2005, Algorithm 18] to compute the posterior moments of the weight vector W , and thus, the value function V (·) = φ(·) W .</p>
<p>Non-parametric Monte-Carlo GPTD Learning:</p>
<p>In the nonparametric case, we bypass the parameterization of the value process by placing a prior directly over the space of value functions. From §2.5.2 with the GP prior V T +1 ∈ N (0, K), we obtain Eq. 5.17 is equivalent to Eq. 2.18 with y T = H −1 r T (see the discussion of Assumption A3). As in the parametric case, it is possible to derive recursive updates for α and C, and an online algorithm [Engel, 2005, Algorithm 19] to compute the posterior moments of the value function V .
E V (s)|R T = r T = k(s) α, Cov V (s), V (s )|R T = r T = k(s, s ) − k(s) Ck(s ),(5.</p>
<p>Sparse Non-parametric Monte-Carlo GPTD Learning:</p>
<p>In the parametric case, the computation of the posterior may be performed online in O(n 2 ) time per sample and O(n 2 ) memory, where n is the number of basis functions used to approximate V . In the non-parametric case, we have a new basis function for each new sample we observe, making the cost of adding the t'th sample O(t 2 ) in both time and memory. This would seem to make the non-parametric form of GPTD computationally infeasible except in small and simple problems. However, the computational cost of non-parametric GPTD can be reduced by using an online sparsification method ( [Engel et al., 2002], [Engel, 2005, Chapter 2]), to a level where it can be efficiently implemented online. In many cases, this results in significant computational savings, both in terms of memory and time, when compared to the non-sparse solution. For the resulting algorithm, we refer the readers to the sparse non-parametric GPTD algorithm in [Engel et al., 2005a, Table 1] or [Engel, 2005, Algorithm 20].</p>
<p>Connections with Other TD Methods</p>
<p>In §5.1.1, we showed that the stochastic GPTD model is equivalent to GP regression on MC samples of the discounted return (see the discussion of Assumption A3). Engel [Engel, 2005] showed that by suitably selecting the noise covariance matrix Σ in the stochastic GPTD model, it is possible to obtain GP-based variants of LSTD(λ) algorithm [Bradtke andBarto, 1996, Boyan, 1999]. The main idea is to obtain the value of the weight vector W in the parametric GPTD model by carrying out maximum likelihood (ML) inference (W is the value for which the observed data is most likely to be generated by the stochastic GPTD model). This allows us to derive LSTD(λ) for each value of λ ≤ 1, as an ML solution arising from some GPTD generative model with a specific noise covariance matrix Σ.</p>
<p>Policy Improvement with GPSARSA</p>
<p>SARSA [Rummery and Niranjan, 1994] is a straightforward extension of the TD algorithm [Sutton, 1988] to control, in which action-values are estimated. This allows policy improvement steps to be performed without requiring any additional knowledge of the MDP model. The idea is that under the policy µ, we may define a process with state space Z = S × A (the space of state-action pairs) and the reward model of the MDP. This process is Markovian with transition density P µ (z |z) = P µ (s |s)µ(a |s ), where z = (s, a) and z = (s , a ) (see §2.2 for more details on this process). SARSA is simply the TD algorithm applied to this process. The same reasoning may be applied to derive a GPSARSA algorithm from a GPTD algorithm. This is equivalent to rewriting the model-equations of §5.1.1 for the action-value function Q µ , which is a function of z, instead of the value function V µ , a function of s.</p>
<p>In the non-parametric setting, we simple need to define a covariance kernel function over state-action pairs, i.e., k : Z × Z → R. Since states and actions are different entities, it makes sense to decompose k into a state-kernel k s and an action-kernel k a : k(z, z ) = k s (s, s )k a (a, a ). If both k s and k a are kernels, we know that k is also a kernel [Schölkopf andSmola, 2002, Shawe-Taylor andCristianini, 2004]. The state and action kernels encode our prior beliefs on value correlations between different states and actions, respectively. All that remains is to run the GPTD algorithms (sparse, batch, online) on the state-action-reward sequence using the new state-action kernel. Action selection may be performed by the -greedy exploration strategy. The main difficulty that may arise here is in finding the greedy action from a large or infinite number of possible actions (when |A| is large or infinite). This may be solved by sampling the value estimates for a few randomly chosen actions and find the greedy action among only them. However, an ideal approach would be to design the action-kernel in such a way as to provide a closed-form expression for the greedy action. Similar to the non-parametric setting, in the parametric case, deriving GPSARSA algorithms from their GPTD counterparts is still straightforward; we just need to define a set of basis functions over Z and use them to approximate action-value functions.</p>
<p>Related Work</p>
<p>Another approach to employ GPs in RL was proposed by Rasmussen and Kuss [Rasmussen and Kuss, 2004]. The approach taken in this work is notably different from the generative approach of the GPTD framework. Two GPs are used in [Rasmussen and Kuss, 2004], one to learn the MDP's transition model and one to estimate the value function. This leads to an inherently offline algorithm. There are several other limitations to this framework. First, the state dynamics is assumed to be factored, in the sense that each state coordinate evolves in time independently of all others. This is a rather strong assumption that is not likely to be satisfied in most real problems. Second, it is assumed that the reward function is completely known in advance, and is of a very special form -either polynomial or Gaussian. Third, the covariance kernels used are also restricted to be either polynomial or Gaussian or a mixture of the two, due to the need to integrate over products of GPs. Finally, the value function is only modelled at a predefined set of support states, and is solved only for them. No method is proposed to ensure that this set of states is representative in any way.</p>
<p>Similar to most kernel-based methods, the choice of prior distribution significantly affects the empirical performance of the learning algorithm. Reisinger et al. [Reisinger et al., 2008] proposed an online model (prior covariance function) selection method for GPTD using sequential Monte-Carlo methods and showed that their method yields better asymptotic performance than standard GPTD for many different kernel families.</p>
<p>Policy gradient (PG) methods are RL algorithms that maintain a class of smoothly parameterized stochastic policies µ(·|s; θ), s ∈ S, θ ∈ Θ , and update the policy parameters θ by adjusting them in the direction of an estimate of the gradient of a performance measure (e.g., [Williams, 1992, Marbach, 1998). As an illustration of such a parameterized policy, consider the following example:</p>
<p>Example 5.1 (Online shop -parameterized policy). Recall the online shop domain of Example 1.1. Assume that for each customer state s ∈ X , and advertisement a ∈ A, we are given a set of n numeric values φ(s, a) ∈ R n that represent some features of the state and action. For example, these features could be the number of items in the cart, the average price of items in the cart, the age of the customer, etc. A popular parametric policy representation is the softmax policy, defined as exp θ φ(s, a ) . The intuition behind this policy, is that if θ is chosen such that θ φ(s, a) is an approximation of the state-action value function Q(s, a), then the softmax policy is an approximation of the greedy policy with respect to Q -which is the optimal policy.
µ(a|s; θ) = exp θ φ(s, a) a ∈A
The performance of a policy µ is often measured by its expected return, η(µ), defined by Eq. 2.1. Since in this setting a policy µ is represented by its parameters θ, policy dependent functions such as η(µ) and Pr ξ; µ) may be written as η(θ) and Pr(ξ; θ).</p>
<p>The score function or likelihood ratio method has become the most prominent technique for gradient estimation from simulation (e.g., Glynn [1990], Williams [1992]). This method estimates the gradient of the expected return with respect to the policy parameters θ, defined by Eq. 2.1, using the following equation: 2 ∇η(θ) = ρ(ξ) ∇ Pr(ξ; θ) Pr(ξ; θ) Pr(ξ; θ)dξ.</p>
<p>(5.18)</p>
<p>In Eq. 5.18, the quantity
u(ξ; θ) = ∇ Pr(ξ; θ) Pr(ξ; θ) = ∇ log Pr(ξ; θ) (5.19) = T −1 t=0 ∇µ(a t |s t ; θ) µ(a t |s t ; θ) = T −1 t=0 ∇ log µ(a t |s t ; θ)
is called the (Fisher) score function or likelihood ratio of trajectory ξ under policy θ. Most of the work on PG has used classical Monte-Carlo (MC) to estimate the (integral) gradient in Eq. 5.18. These methods (in their simplest form) generate M i.i.d. sample paths ξ 1 , . . . , ξ M according to Pr(ξ; θ), and estimate the gradient ∇η(θ) using the unbiased MC estimator
∇η(θ) = 1 M M i=1 ρ(ξ i )∇ log Pr(ξ i ; θ) = 1 M M i=1 ρ(ξ i ) Ti−1 t=0 ∇ log µ(a t,i |s t,i ; θ).
(5.20)</p>
<p>Both theoretical results and empirical evaluations have highlighted a major shortcoming of these algorithms: namely, the high variance of the gradient estimates, which in turn results in sample-inefficiency (e.g., Marbach [1998], ). Many solutions have been proposed for this problem, each with its own pros and cons. In this section, we describe Bayesian policy gradient (BPG) algorithms that tackle this problem by using a Bayesian alternative to the MC estimation of Eq. 5.20 [Ghavamzadeh and Engel, 2006]. These BPG algorithms are based on Bayesian quadrature, i.e., a Bayesian approach to integral evaluation, proposed by O'Hagan [O'Hagan, 1991]. 3 The algorithms use Gaussian processes (GPs) to define a prior distribution over the gradient of the expected return, and compute its posterior conditioned on the observed data. This reduces the number of samples needed to obtain accurate (integral) gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates, namely the gradient covariance, are provided at little extra cost. In the next two sections, we first briefly describe Bayesian quadrature and then present the BPG models and algorithms.</p>
<p>Bayesian Quadrature</p>
<p>Bayesian quadrature (BQ) [O'Hagan, 1991], as its name suggests, is a Bayesian method for evaluating an integral using samples of its integrand. Let us consider the problem of evaluating the following integral 4
ζ = f (x)g(x)dx. (5.21)
BQ is based on the following reasoning: In the Bayesian approach, f (·) is random simply because it is unknown. We are therefore uncertain about the value of f (x) until we actually evaluate it. In fact, even then, our uncertainty is not always completely removed, since measured samples of f (x) may be corrupted by noise. Modeling f as a GP means that our uncertainty is completely accounted for by specifying a Normal prior distribution over the functions, f (·) ∼ N f (·), k(·, ·) , i.e.,
E f (x) =f (x) and Cov f (x), f (x ) = k(x, x ), ∀x, x ∈ X .
The choice of kernel function k allows us to incorporate prior knowledge on the smoothness properties of the integrand into the estimation procedure. When we are provided with a set of samples
D M = x i , y(x i ) M i=1
, where y(x i ) is a (possibly noisy) sample of f (x i ), we apply Bayes' rule to condition the prior on these sampled values. If the measurement noise is normally distributed, the result is a Normal posterior distribution of f |D M . The expressions for the posterior mean and covariance are standard (see Eqs. 2.14-2.16):
E f (x)|D M =f (x) + k(x) C(y −f ), Cov f (x), f (x )|D M = k(x, x ) − k(x) Ck(x ),(5.22)
where K is the kernel (or Gram) matrix, and [Σ] i,j is the measurement noise covariance between the ith and jth samples. It is typically assumed that the measurement noise is i.i.d., in which case Σ = σ 2 I, where σ 2 is the noise variance and I is the (M × M ) identity matrix.
f = f (x 1 ), . . . ,f (x M ) , k(x) = k(x 1 , x), . . . , k(x M , x) , y = y(x 1 ), . . . , y(x M ) , [K] i,j = k(x i , x j ), C = (K + Σ) −1 .
Since integration is a linear operation, the posterior distribution of the integral in Eq. 5.21 is also Gaussian, and the posterior moments are given by [O'Hagan, 1991] 
E[ζ|D M ] = E f (x)|D M g(x)dx, Var[ζ|D M ] = Cov f (x), f (x )|D M g(x)g(x )dxdx . (5.23)
Substituting Eq. 5.22 into Eq. 5.23, we obtain
E[ζ|D M ] = ζ 0 + b C(y −f ) and Var[ζ|D M ] = b 0 − b Cb,
where we made use of the definitions
ζ 0 = f (x)g(x)dx , b = k(x)g(x)dx , b 0 = k(x, x )g(x)g(x )dxdx .
(5.24) Note that ζ 0 and b 0 are the prior mean and variance of ζ, respectively. It is important to note that in order to prevent the problem from "degenerating into infinite regress", as phrased by O'Hagan [O'Hagan, 1991], we should decompose the integrand into parts: f (the GP part) and g, and select the GP prior, i.e., prior meanf and covariance k, so as to allow us to solve the integrals in Eq. 5.24 analytically. Otherwise, we begin with evaluating one integral (Eq. 5.21) and end up with evaluating three integrals (Eq. 5.24).</p>
<p>Bayesian Policy Gradient Algorithms</p>
<p>In this section, we describe the Bayesian policy gradient (BPG) algorithms [Ghavamzadeh and Engel, 2006]. These algorithms use Bayesian quadrature (BQ) to estimate the gradient of the expected return with respect to the policy parameters (Eq. 5.18). In the Bayesian approach, the expected return of the policy characterized by the parameters θ η B (θ) = ρ(ξ) Pr(ξ; θ)dξ (5.25) is a random variable because of our subjective Bayesian uncertainty concerning the process generating the return. Under the quadratic loss, the optimal Bayesian performance measure is the posterior expected value of η B (θ), E η B (θ)|D M . However, since we are interested in optimizing the performance rather than in evaluating it, we would rather evaluate the posterior mean of the gradient of η B (θ) with respect to the policy parameters θ, i.e.,
∇E η B (θ)|D M = E ∇η B (θ)|D M (5.26) = E ρ(ξ) ∇ Pr(ξ; θ) Pr(ξ; θ) Pr(ξ; θ)dξ D M .
Gradient Estimation: In BPG, we cast the problem of estimating the gradient of the expected return (Eq. 5.26) in the form of Eq. 5.21 and use the BQ approach described in §5.2.1. We partition the integrand into two parts, f (ξ; θ) and g(ξ; θ), model f as a GP, and assume that g is a function known to us. We then proceed by calculating the posterior moments of the gradient ∇η B (θ) conditioned on the observed data. Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2006] proposed two different ways of partitioning the integrand in Eq. 5.26, resulting in the two distinct Bayesian models summarized in Table 5.1 (see Ghavamzadeh et al. [2013], for more details). Figure 5.3 shows how the BQ approach has been used in each of the two BPG models of [Ghavamzadeh and Engel, 2006], summarized in Table 5.1, as well as in the Bayesian actor-critic (BAC) formulation of §5.3. It is important to note that in both modelsρ(ξ) belongs to the GP part, i.e., f (ξ; θ). This is because in general,ρ(ξ) cannot be known exactly, even for a given ξ (due to the stochasticity of the rewards), but can be estimated for a sample trajectory ξ. The more important and rather critical point is that Pr(ξ; θ) cannot belong to either the f (ξ; θ) or g(ξ; θ) part of the model, since it is not known (in model-free setting) and cannot be estimated for a sample trajectory ξ. However, Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2006] showed that interestingly, it is sufficient to assign Pr(ξ; θ) to the g(ξ; θ) part of the model and use an appropriate Fisher kernel (see Table 5.1 for the kernels), rather than having exact knowledge of Pr(ξ; θ)
⇣ = Z z}|{ f (x) g(x) dx | {z } Modeled as GP E ⇥ f(x)|DM ⇤ , Cov ⇥ f(x), f(x 0 )|DM ⇤ E[⇣|D M ] , Var[⇣|D M ] Bayesian Quadrature r⌘ B (✓) = Z f z }| { ⇢(⇠) r log Pr(⇠; ✓) Pr(⇠; ✓) | {z } g d⇠ BPG Model 1 Modeled as GP r⌘ B (✓) = Z f z}|{ ⇢(⇠) r log Pr(⇠; ✓) Pr(⇠; ✓) | {z } g d⇠ BPG Model 2 Modeled as GP BAC r⌘(✓) = Z dsda ⌫(s; ✓) rµ(a|s; ✓) | {z } g f z }| { Q(s, a; ✓)
Modeled as GP Figure 5.3: The connection between BQ and the two BPG models of [Ghavamzadeh and Engel, 2006], and the Bayesian actor-critic (BAC) formulation of §5.3. itself (see Ghavamzadeh et al. [2013] for more details).</p>
<p>Model 1 Model 2
Deter. factor (g) g(ξ; θ) = Pr(ξ; θ) g(ξ; θ) = ∇ Pr(ξ; θ) GP factor (f ) f (ξ; θ) =ρ(ξ)∇ log Pr(ξ; θ) f (ξ) =ρ(ξ) Measurement (y) y(ξ; θ) = ρ(ξ)∇ log Pr(ξ; θ) y(ξ) = ρ(ξ) Prior mean of f E f j (ξ; θ) = 0 E f (ξ) = 0 Prior Cov. of f Cov f j (ξ; θ), f (ξ ; θ) Cov f (ξ), f (ξ ) = δ j, k(ξ, ξ ) = k(ξ, ξ ) Kernel function k(ξ, ξ ) = k(ξ, ξ ) = 1 + u(ξ) G −1 u(ξ ) 2 u(ξ) G −1 u(ξ ) E ∇η B (θ)|D M Y Cb BCy Cov ∇η B (θ)|D M (b 0 − b Cb)I B 0 − BCB b or B (b) i = 1 + u(ξ i ) G −1 u(ξ i ) B = U b 0 or B 0 b 0 = 1 + n B 0 = G
In Model 1, a vector-valued GP prior is placed over f (ξ; θ). This induces a GP prior over the corresponding noisy measurement y(ξ; θ). It is assumed that each component of f (ξ; θ) may be evaluated inde-pendently of all other components, and thus, the same kernel function K and noise covariance Σ are used for all components of f (ξ; θ). Hence for the jth component of f and y we have a priori
f j = f j (ξ 1 ; θ), . . . , f j (ξ M ; θ) ∼ N (0, K), y j = y j (ξ 1 ; θ), . . . , y j (ξ M ; θ) ∼ N (0, K + Σ).
This vector-valued GP model gives us the posterior mean and covariance of ∇η B (θ) reported in Table 5.1m in which Y = y 1 ; . . . ; y n , C = (K + Σ) −1 , n is the number of policy parameters, and G(θ) is the Fisher information matrix of policy θ defined as 5</p>
<p>G(θ) = E u(ξ)u(ξ) = u(ξ)u(ξ) Pr(ξ; θ)dξ,</p>
<p>( 5.27) where u(ξ) is the score function of trajectory ξ defined by Eq. 5.19. Note that the choice of the kernel k allows us to derive closed form expressions for b and b 0 , and, as a result k is the quadratic Fisher kernel for the posterior moments of the gradient (see Table 5.1).</p>
<p>In Model 2, g is a vector-valued function and f is a scalar valued GP representing the expected return of the path given as its argument. The noisy measurement corresponding to f (ξ i ) is y(ξ i ) = ρ(ξ i ), namely, the actual return accrued while following the path ξ i . This GP model gives us the posterior mean and covariance of ∇η B (θ) reported in Table 5.1 in which y = ρ(ξ 1 ), . . . , ρ(ξ M ) and U = u(ξ 1 ), . . . , u(ξ M ) . Here the choice of kernel k allows us to derive closed-form expressions for B and B 0 , and as a result, k is again the Fisher kernel for the posterior moments of the gradient (see Table 5.1).</p>
<p>Note that the choice of Fisher-type kernels is motivated by the notion that a good representation should depend on the process generating the data (see Jaakkola and Haussler [1999], Shawe-Taylor and Cristianini [2004], for a thorough discussion). The particular selection of linear and quadratic Fisher kernels is guided by the desideratum that the posterior moments of the gradient be analytically tractable as discussed at the end of §5.2.1.</p>
<p>The above two BPG models can define Bayesian algorithms for evaluating the gradient of the expected return with respect to the policy parameters (see [Ghavamzadeh and Engel, 2006] and [Ghavamzadeh et al., 2013] for the pseudo-code of the resulting algorithms). It is important to note that computing the quadratic and linear Fisher kernels used in Models 1 and 2 requires calculating the Fisher information matrix G(θ) (Eq. 5.27). Consequently, every time the policy parameters are updated, G needs to be recomputed. Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2006] suggest two possible approaches for online estimation of the Fisher information matrix.</p>
<p>Similar to most non-parametric methods, the Bayesian gradient evaluation algorithms can be made more efficient, both in time and memory, by sparsifying the solution. Sparsification also helps to numerically stabilize the algorithms when the kernel matrix is singular, or nearly so. Ghavamzadeh et al. [Ghavamzadeh et al., 2013] show how one can incrementally perform such sparsification in their Bayesian gradient evaluation algorithms, i.e., how to selectively add a new observed path to a set of dictionary paths used as a basis for representing or approximating the full solution.</p>
<p>Policy Improvement: Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2006] also show how their Bayesian algorithms for estimating the gradient can be used to define a Bayesian policy gradient algorithm. The algorithm starts with an initial set of policy parameters θ 0 , and at each iteration j, updates the parameters in the direction of the posterior mean of the gradient of the expected return E ∇η B (θ j )|D M estimated by their Bayesian gradient evaluation algorithms. This is repeated N times, or alternatively, until the gradient estimate is sufficiently close to zero. Since the Fisher information matrix, G, and the posterior distribution (both mean and covariance) of the gradient of the expected return are estimated at each iteration of this algorithm, we may make the following modifications in the resulting Bayesian policy gradient algorithm at little extra cost:</p>
<p>• Update the policy parameters in the direction of the natural gradient, G(θ) −1 E ∇η B (θ)|D M , instead of the regular gradient,
94CHAPTER 5. MODEL-FREE BAYESIAN REINFORCEMENT LEARNING E ∇η B (θ)|D M .
• Use the posterior covariance of the gradient of the expected return as a measure of the uncertainty in the gradient estimate, and thus, as a measure to tune the step-size parameter in the gradient update (the larger the posterior variance the smaller the stepsize) (see the experiments in Ghavamzadeh et al. [2013] for more details). In a similar approach, Vien et al. [Vien et al., 2011] used BQ to estimate the Hessian matrix distribution and then used its mean as learning rate schedule to improve the performance of BPG. They empirically showed that their method performs better than BPG and BPG with natural gradient in terms of convergence speed.</p>
<p>It is important to note that similar to the gradient estimated by the GPOMDP algorithm of Baxter and Bartlett , the gradient estimated by these algorithms, E ∇η B (θ)|D M , can be used with the conjugate-gradient and line-search methods of Baxter and Bartlett  for improved use of gradient information. This allows us to exploit the information contained in the gradient estimate more aggressively than by simply adjusting the parameters by a small amount in the direction of E ∇η B (θ)|D M .</p>
<p>The experiments reported in [Ghavamzadeh et al., 2013] indicate that the BPG algorithm tends to significantly reduce the number of samples needed to obtain accurate gradient estimates. Thus, given a fixed number of samples per iteration, finds a better policy than MCbased policy gradient methods. These results are in line with previous work on BQ, for example a work by Rasmussen and Ghahramani [Rasmussen and Ghahramani, 2003] that demonstrates how BQ, when applied to the evaluation of an expectation, can outperform MC estimation by orders of magnitude in terms of the mean-squared error.</p>
<p>Extension to Partially Observable Markov Decision Processes:</p>
<p>The above models and algorithms can be easily extended to partially observable problems without any changes using similar techniques as in Section 6 of . This is due to the fact that the BPG framework considers complete system trajectories as its basic observable unit, and thus, does not require the dynamic within each trajectory to be of any special form. This generality has the downside that it cannot take advantage of the Markov property when the system is Markovian (see Ghavamzadeh et al. [2013] for more details). To address this issue, Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2007] then extended their BPG framework to actor-critic algorithms and present a new Bayesian take on the actor-critic architecture, which is the subject of the next section.</p>
<p>Bayesian Actor-Critic</p>
<p>Another approach to reduce the variance of the policy gradient estimates is to use an explicit representation for the value function of the policy. This class of PG algorithms are called actor-critic and they were among the earliest to be investigated in RL [Barto et al., 1983, Sutton, 1984. Unlike in §5.2 where we consider complete trajectories as the basic observable unit, in this section we assume that the system is Markovian, and thus, the basic observable unit is one step system transition (state, action, next state). It can be shown that under certain regularity conditions [Sutton et al., 2000], the expected return of policy µ may be written in terms of state-action pairs (instead of in terms of trajectories as in Eq. 2.1) as
η(µ) = Z dzπ µ (z)r(z),(5.28)
where z = (s, a) is a state-action pair and π µ (z) = ∞ t=0 γ t P µ t (z) is a discounted weighting of state-action pairs encountered while following policy µ. In the definition of π µ , the term P µ t (z t ) is the t-step stateaction occupancy density of policy µ given by
P µ t (z t ) = Z t dz 0 . . . dz t−1 P µ 0 (z 0 ) t i=1 P µ (z i |z i−1 ).
Integrating a out of π µ (z) = π µ (s, a) results in the corresponding discounted weighting of states encountered by following policy µ: ν µ (s) = A daπ µ (s, a). Unlike ν µ and π µ , (1 − γ)ν µ and (1 − γ)π µ are distributions and are analogous to the stationary distributions over states and state-action pairs of policy µ in the undiscounted setting, respectively. The policy gradient theorem ( [Marbach, 1998, Proposition 1]; [Sutton et al., 2000, Theorem 1]; [Konda and Tsitsiklis, 2000, Theorem 1]) states that the gradient of the expected return, defined by Eq. 5.28, is given by ∇η(θ) = dsda ν(s; θ)∇µ(a|s; θ)Q(s, a; θ).</p>
<p>(5.29)</p>
<p>In an AC algorithm, the actor updates the policy parameters θ along the direction of an estimate of the gradient of the performance measure (Eq. 5.29), while the critic helps the actor in this update by providing it with an estimate of the action-value function Q(s, a; θ). In most existing AC algorithms (both conventional and natural), the actor uses Monte-Carlo (MC) techniques to estimate the gradient of the performance measure and the critic approximates the action-value function using some form of temporal difference (TD) learning [Sutton, 1988].</p>
<p>The idea of Bayesian actor-critic (BAC) [Ghavamzadeh and Engel, 2007] is to apply the Bayesian quadrature (BQ) machinery described in §5.2.1 to the policy gradient expression given by Eq. 5.29 in order to reduce the variance in the gradient estimation procedure (see Figure 5.3 for the connection between the BQ machinery and BAC).</p>
<p>Similar to the BPG methods described in §5.2.2, in BAC, we place a Gaussian process (GP) prior over action-value functions using a prior covariance kernel defined on state-action pairs: k(z, z ) = Cov Q(z), Q(z ) . We then compute the GP posterior conditioned on the sequence of individual observed transitions. By an appropriate choice of a prior on action-value functions, we are able to derive closedform expressions for the posterior moments of ∇η(θ). The main questions here are: 1) how to compute the GP posterior of the action-value function given a sequence of observed transitions? and 2) how to choose a prior for the action-value function that allows us to derive closed-form expressions for the posterior moments of ∇η(θ)? The Gaussian process temporal difference (GPTD) method [Engel et al., 2005a] described in §5.1.1 provides a machinery for computing the posterior moments of Q(z). After t time-steps, GPTD gives us the following posterior moments for Q (see Eqs. 5.15 and 5.16 in §5.1.1):
Q t (z) = E [Q(z)|D t ] = k t (z) α t , S t (z, z ) = Cov Q(z), Q(z )|D t = k(z, z ) − k t (z) C t k t (z ),
where D t denotes the observed data up to and including time step t, and
k t (z) = k(z 0 , z), . . . , k(z t , z) , K t = k t (z 0 ), k t (z 1 ), . . . , k t (z t ) , α t = H t H t K t H t + Σ t −1 r t−1 , C t = H t H t K t H t + Σ t −1 H t .
(5.30)</p>
<p>Using the above equations for the posterior moments of Q, making use of the linearity of Eq. 5.29 in Q, and denoting g(z; θ) = π µ (z)∇ log µ(a|s; θ), the posterior moments of the policy gradient ∇η(θ) may be written as [O'Hagan, 1991] 
E ∇η(θ)|D t = Z dzg(z; θ)k t (z) α t , Cov ∇η(θ)|D t = Z 2 dzdz g(z; θ) k(z, z ) − k t (z) C t k t (z ) g(z ; θ) .
These equations provide us with the general form of the posterior policy gradient moments. We are now left with a computational issue, namely, how to compute the integrals appearing in these expressions? We need to be able to evaluate the following integrals:
B t = Z dzg(z; θ)k t (z) , B 0 = Z 2 dzdz g(z; θ)k(z, z )g(z ; θ) .
(5.31) Using the definitions of B t and B 0 , the gradient posterior moments may be written as (5.32) In order to render these integrals analytically tractable, Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2007] chose the prior covariance kernel to be the sum of an arbitrary state-kernel k s 98CHAPTER 5. MODEL-FREE BAYESIAN REINFORCEMENT LEARNING and the Fisher kernel k F between state-action pairs, i.e., Using the prior covariance kernel of Eq. 5.33, Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2007] showed that the integrals in Eq. 5.31 can be computed as
E ∇η(θ)|D t = B t α t , Cov ∇η(θ)|D t = B 0 − B t C t B t .k F (z, z ) = u(z; θ) G(θ) −1 u(z ) , k(z, z ) = k s (s, s ) + k F (z, z ) ,(5.B t = U t , B 0 = G,(5.35)
where U t = u(z 0 ), u(z 1 ), . . . , u(z t ) . As a result, the integrals of the gradient posterior moments (Eq. 5.32) are analytically tractable (see Ghavamzadeh et al. [2013] for more details). An immediate consequence of Eq. 5.35 is that, in order to compute the posterior moments of the policy gradient, we only need to be able to evaluate (or estimate) the score vectors u(z i ), i = 0, . . . , t and the Fisher information matrix G of the policy. Similar to the BPG method, Ghavamzadeh and Engel [Ghavamzadeh and Engel, 2007] suggest methods for online estimation of the Fisher information matrix in Eq. 5.34 and for using online sparsification to make the BAC algorithm more efficient in both time and memory (see Ghavamzadeh et al. [2013] for more details). They also report experimental results Engel, 2007, Ghavamzadeh et al., 2013] which indicate that the BAC algorithm tends to significantly reduce the number of samples needed to obtain accurate gradient estimates, and thus, given a fixed number of samples per iteration, finds a better policy than both MC-based policy gradient 6</p>
<p>Risk-aware Bayesian Reinforcement Learning</p>
<p>The results presented so far have all been concerned with optimizing the expected return of the policy. However, in many applications, the decision-maker is also interested in minimizing the risk of a policy. By risk, 1 we mean performance criteria that take into account not only the expected return, but also some additional statistics of it, such as variance, Value-at-Risk (VaR), expected shortfall (also known as conditional-value-at-risk or CVaR), etc. The primary motivation for dealing with risk-sensitive performance criteria comes from finance, where risk plays a dominant role in decision-making. However, there are many other application domains where risk is important, such as process control, resource allocation, and clinical decision-making.</p>
<p>In general, there are two sources that contribute to the reward uncertainty in MDPs: internal uncertainty and parametric uncertainty. Internal uncertainty reflects the uncertainty of the return due to the stochastic transitions and rewards, for a single and known MDP. Parametric uncertainty, on the other hand, reflects the uncertainty about the unknown MDP parameters -the transition and reward distribu-tions. As a concrete example of this dichotomy consider the following two experiments. First, select a single MDP and execute some fixed policy on it several times. In each execution, the return may be different due to the stochastic transitions and reward. This variability corresponds to the internal uncertainty. In the second experiment, consider drawing several MDPs from some distribution (typically, the posterior distribution in a Bayesian setting). For each drawn MDP, execute the same policy several times and compute the average return across the executions. The variability in the average returns across the different MDPs corresponds to the parametric type of uncertainty. The Bayesian setting offers a framework for dealing with parameter uncertainty in a principled manner. Therefore, work on risk-sensitive RL in the Bayesian setting focuses on risk due to parametric uncertainty, as we shall now survey.</p>
<p>Bias and Variance Approximation in Value Function Estimates</p>
<p>The first result we discuss concerns policy evaluation. Mannor et al. [Mannor et al., 2007] derive approximations to the bias and variance of the value function of a fixed policy due to parametric uncertainty.</p>
<p>Consider an MDP M with unknown transition probabilities P , 2 a Dirichlet prior on the transitions, and assume that we have observed n(s, a, s ) transitions from state s to state s under action a. Recall that the posterior transition probabilities are also Dirichlet, and may be calculated as outlined in §2.5.1. 3 Consider also a stationary Markov policy µ, and let P µ denote the unknown transition probabilities induced by µ in the unknown MDP M and V µ denote the corresponding value function. Recall that V µ is the solution to the Bellman equation (2.4) and may be written explicitly as V µ = (I − γP µ ) −1 R µ , where we recall that R µ denotes the expected rewards induced by µ. The Bayesian formalism allows the calculation of the posterior mean and covariance of V µ , given the observations. LetP (s |s, a) = E post P (s |s, a) denote the expected transition probabilities with respect to their posterior distribution, and let P µ (s |s) = a µ(a|s)P (s |s, a) denote the expected transitions induced by µ. We denote byV µ = (I − γP µ ) −1 R µ the estimated value function. Also, let e denote a vector of ones. The posterior mean and covariance of V µ are given in the following two theorems:</p>
<p>Theorem 6.1. [Mannor et al., 2007] The expectation (under the posterior) of V µ satisfies
E post [V µ ] =V µ + γ 2XQV µ + γB + L bias , whereX = I − γP µ −1
, and the vectorB and matrixQ are computed according toB
i = a µ(a|i) 2 R(i, a)e M i,aX ·,i , andQ i,j =Ĉ ov (i) j,·X ·,i in whichĈ ov (i) = a µ(a|i) 2M i,a ,
where the matrixM i,a is the posterior covariance matrix of P (·|s, a), and higher order terms
L bias = ∞ k=3 γ k E f k (P ) R µ ,
in whichP = P −P , and f k (P ) =X PX k .</p>
<p>Theorem 6.2. [Mannor et al., 2007] Using the same notations as Theorem 6.1, the second moment (under the posterior) of V µ satisfies
E post V µ V µ =V µV µ +X γ 2 QV µ R µ + R µV µ Q + γ B R µ + R µB +Ŵ X + L var ,
whereŴ is a diagonal matrix with elementŝ 
W i,i = a µ(γ k+l E f k (P )R µ R µ f l (P ) .
It is important to note that except for the higher order terms L bias and L var , the terms in Theorems 6.1 and 6.2 do not depend on the unknown transitions, and thus, may be calculated from the data. This results in a second-order approximation of the bias and variance of V µ , which may be used to derive confidence intervals around the estimated value function. This approximation was also used by Delage and Mannor [Delage and Mannor, 2010] for risk-sensitive policy optimization, as we describe next.</p>
<p>Percentile Criterion</p>
<p>Consider again a setting where n(s, a, s ) transitions from state s to state s under action a from MDP M were observed, and let P post denote the posterior distribution of the transition probabilities in M given these observations. Delage and Mannor [Delage and Mannor, 2010] investigate the percentile criterion 4 for M, defined as
max y∈R,µ y s.t. P post E ∞ t=0 γ t R(Z t ) Z 0 ∼ P 0 , µ ≥ y ≥ 1 − , (6.1)
where P post denotes the probability of drawing a transition matrix P from the posterior distribution of the transitions, and the expectation is with respect to a concrete realization of that P . Note that the value of the optimization problem in (6.1) is a (1− )-guarantee to the performance of the optimal policy with respect to the parametric uncertainty.</p>
<p>Unfortunately, solving (6.1) for general uncertainty in the parameters is NP-hard [Delage and Mannor, 2010]. However, for the case of a Dirichlet prior, the 2nd order approximation of Theorem 6.1 may be used to derive an approximately optimal solution.</p>
<p>Let F (µ) denote the 2nd order approximation 5 of the expected return under policy µ and initial state distribution P 0 (cf. Theorem 6.1)
F (µ) = P 0V µ + γ 2 P 0XQV µ .
The next result of [Delage and Mannor, 2010] shows that given enough observations, optimizing F (µ) leads to an approximate solution of the percentile problem (6.1).</p>
<p>Theorem 6.3. [Delage and Mannor, 2010] Let N * = min s,a s n(s, a, s ) denote the minimum number of state transitions observed from any state using any action, and ∈ (0, 0.5]. The policyμ = arg max µ F (µ)</p>
<p>is O 1/ √ N * optimal with respect to the percentile optimization criterion (6.1).</p>
<p>Note that, as discussed earlier, F (µ) may be efficiently evaluated for every µ. However, F (µ) is non-convex in µ, but empirically, global optimization techniques for maximizing F (µ) lead to useful solutions [Delage and Mannor, 2010].</p>
<p>Delage and Mannor [Delage and Mannor, 2010] also consider a case where the state transitions are known, but there is uncertainty in the reward distribution. For general reward distributions the corresponding percentile optimization problem is also NP-hard. However, for the case of a Gaussian prior, the resulting optimization problem is a secondorder cone program, for which efficient solutions are known.</p>
<p>Max-Min Criterion</p>
<p>Consider the percentile criterion (6.1) in the limit of → 0. In this case, we are interested in the performance under the worst realizable posterior transition probability, i.e.,
max µ min P ∈Ppost E ∞ t=0 γ t R(Z t ) Z 0 ∼ P 0 , P, µ , (6.2) 106CHAPTER 6. RISK-AWARE BAYESIAN REINFORCEMENT LEARNING
where P post denotes the set of all realizable transition probabilities in the posterior. For the case of a Dirichlet prior, this criterion is useless, as the set P post contains the entire simplex for each state. Bertuccelli et al. [Bertuccelli et al., 2012] consider instead a finite subset 6P post ∈ P post , and minimize only over the setP post , resulting in the following criterion:
max µ min P ∈Ppost E ∞ t=0 γ t R(Z t ) Z 0 ∼ P 0 , P, µ . (6.3)
Given a setP post , the optimization in (6.3) may be solved efficiently using min-max dynamic programming. Thus, the 'real question' is how to constructP post . Naturally, the construction ofP post should reflect the posterior distribution of transition probabilities. One approach is to construct it from a finite number of models sampled from the posterior distribution, reminiscent of Thompson sampling. However, as claimed in [Bertuccelli et al., 2012], this approach requires a very large number of samples in order to adequately represent the posterior.</p>
<p>Alternatively, Bertuccelli et al. [Bertuccelli et al., 2012] propose a deterministic sampling procedure for the Dirichlet distribution based on sigma-points. In simple terms, sigma-points are points placed around the posterior mean and spaced proportionally to the posterior variance. The iterative algorithm of [Bertuccelli et al., 2012] consists of two phases. In the first phase, the already observed transitions are used to derive the posterior Dirichlet distribution, for which an optimal policy is derived by solving (6.3). In the second phase, this policy is then acted upon in the system to generate additional observations. As more data become available, the posterior variance decreases and the sigma points become closer, leading to convergence of the algorithm.</p>
<p>Percentile Measures Criteria</p>
<p>The NP-hardness of the percentile criterion for general uncertainty structures motivates a search for more tractable risk-aware performance criteria. Chen and Bowling [Chen and Bowling, 2012] propose to replace the individual percentile with a measure over percentiles. For-mally, given a measure ψ over the interval [0, 1], consider the following optimization problem:
max µ,y∈F x y(x)dψ s.t. P post E T t=0 R(Z t ) Z 0 ∼ P 0 , µ ≥ y(x) ≥ x ∀x ∈ [0, 1], (6.4)
where F is the class of real-valued and bounded ψ-integrable functions on the interval [0, 1], P post denotes the probability of drawing a transition matrix P from the posterior distribution of the transitions (as before), and the expectation is with respect to a concrete realization of that P . Note that here the horizon is T and there is no discounting, as opposed to the infinite horizon discounted setting discussed earlier.</p>
<p>The percentile measure criterion (6.4) may be seen as a generalization of the percentile criterion (6.1), which is obtained by setting ψ to a Dirac delta at 1 − . In addition, when ψ is uniform on [0, 1], (6.4) is equivalent to the expected value of Theorem 6.1, and when ψ is a delta Dirac at 0, we obtain the max-min criterion (6.2). Finally, when ψ is a step function, an expected shortfall (CVaR) criterion is obtained.</p>
<p>Chen and Bowling [Chen and Bowling, 2012] introduce the k-of-N family of percentile measures, which admits an efficient solution under a general uncertainty structure. For a policy µ, the k-of-N measure is equivalent to the following sampling procedure: first draw N MDPs according to the posterior distribution, then select a set of the k MDPs with the worst expected performance under policy µ, and finally choose a random MDP from this set (according to a uniform distribution). By selecting suitable k and N , the k-of-N measure may be tuned to closely approximate the CVaR or max-min criterion.</p>
<p>The main reason for using the k-of-N measure is that the above sampling procedure may be seen as a two-player zero-sum extensiveform game with imperfect information, which may be solved efficiently using counterfactual regret minimization. This results in the following convergence guarantee:</p>
<p>Theorem 6.4. [Chen and Bowling, 2012] For any &gt; 0 and δ ∈ (0, 1],
108CHAPTER 6. RISK-AWARE BAYESIAN REINFORCEMENT LEARNING letT = 1 + 2 √ δ 2 16T 2 ∆ 2 |I 1 | 2 |A| δ 2 2 ,
where T ∆ is the maximum difference in total reward over T steps.</p>
<p>With probability 1 − δ, the current strategy at iteration T * , chosen uniformly at random from the interval [1,T ], is an -approximation to a solution of (6.4) when ψ is a k-of-N measure. The total time
complexity is O (T ∆/ ) 2 |I 1 | 3 |A| 3 N log N δ 3
, where |I 1 | ∈ O (|S|T ) for arbitrary reward uncertainty and |I 1 | ∈ O |S| T +1 A T for arbitrary transition and reward uncertainty.</p>
<p>The exponential dependence on the horizon T in Theorem 6.4 is due to the fact that an optimal policy for the risk-sensitive criterion (6.4) is not necessarily Markov and may depend on the complete history. In comparison, the previous results avoided this complication by searching only in the space of Markov policies. An interesting question is whether other choices of measure ψ admit an efficient solution. Chen and Bowling [Chen and Bowling, 2012] provide the following sufficient condition for tractability: Theorem 6.5. [Chen and Bowling, 2012] Let ψ be an absolutely continuous measure with density function g ψ , such that g ψ is non-increasing and piecewise Lipschitz continuous with m pieces and Lipschitz constant L. A solution of (6.4) can be approximated with high probability in time polynomial in |A|, |S|, ∆, L, m, 1 , 1 δ for (i) arbitrary reward uncertainty with time also polynomial in the horizon or (ii) arbitrary transition and reward uncertainty with a fixed horizon.</p>
<p>Note that in line with the previous hardness results, both the CVaR and max-min criteria may be represented using a non-increasing and piecewise Lipschitz continuous measure, while the percentile criterion may not.</p>
<p>BRL Extensions</p>
<p>In this section, we discuss extensions of the Bayesian reinforcement learning (BRL) framework to the following classes of problems: PAC-Bayes model selection, inverse RL, multi-agent RL, and multi-task RL.</p>
<p>PAC-Bayes Model Selection</p>
<p>While Bayesian RL provides a rich framework for incorporating domain knowledge, one of the often mentioned limitations is the requirement to have correct priors, meaning that the prior has to admit the true posterior. Of course this issue is pervasive across Bayesian learning methods, not just Bayesian RL. Recent work on PAC-Bayesian analysis seeks to provide tools that are robust to poorly selected priors. PAC-Bayesian methods provide a way to simultaneously exploit prior knowledge when it is appropriate, while providing distribution-free guarantees based on properties such as VC dimension [McAllester, 1999].</p>
<p>PAC-Bayesian bounds for RL in finite state spaces were introduced by [Fard and Pineau, 2010], showing that it is possible to remove the assumption on the correctness of the prior, and instead, measure the consistency of the prior over the training data. Bounds are available for both model-based RL, where a prior distribution is given on the space of possible models, and model-free RL, where a prior is given on the space of value functions. In both cases, the bound depends on an empirical estimate and a measure of distance between the stochastic policy and the one imposed by the prior distribution. The primary use of these bounds is for model selection, where the bounds can be useful in choosing between following the empirical estimate and the Bayesian posterior, depending on whether the prior was informative or misleading. PAC-Bayesian bounds for the case of RL with function approximation are also available to handle problems with continuous state spaces [Fard et al., 2011].</p>
<p>For the most part, PAC-Bayesian analysis to date has been primarily theoretical, with few empirical results. However, recent theoretical results on PAC-Bayesian analysis of the bandit case may provide useful tools for further development of this area [Seldin et al., 2011a,b].</p>
<p>Bayesian Inverse Reinforcement Learning</p>
<p>Inverse reinforcement learning (IRL) is the problem of learning the underlying model of the decision-making agent (expert) from its observed behavior and the dynamics of the system [Russell, 1998]. IRL is motivated by situations in which the goal is only to learn the reward function (as in preference elicitation) and by problems in which the main objective is to learn good policies from the expert (apprenticeship learning). Both reward learning (direct) and apprenticeship learning (indirect) views of this problem have been studied in the last decade (e.g., [Ng and Russell, 2000, Abbeel and Ng, 2004, Ratliff et al., 2006, Neu and Szepesvári, 2007, Ziebart et al., 2008, Syed and Schapire, 2008). What is important is that the IRL problem is inherently ill-posed since there might be an infinite number of reward functions for which the expert's policy is optimal. One of the main differences between the various works in this area is in how they formulate the reward preference in order to obtain a unique reward function for the expert.</p>
<p>The main idea of Bayesian IRL (BIRL) is to use a prior to encode the reward preference and to formulate the compatibility with the expert's policy as a likelihood in order to derive a probability distribution over the space of reward functions, from which the expert's reward function is somehow extracted. Ramachandran and Amir [Ramachandran and Amir, 2007] use this BIRL formulation and propose a Markov chain Monte Carlo (MCMC) algorithm to find the posterior mean of the reward function and return it as the reward of the expert. Michini and How [Michini and How, 2012b] improve the efficiency of the method in [Ramachandran and Amir, 2007] by not including the entire state space in the BIRL inference. They use a kernel function that quantifies the similarity between states and scales down the BIRL inference by only including those states that are similar (the similarity is defined by the kernel function) to the ones encountered by the expert. Choi and Kim [Choi and Kim, 2011] use the BIRL formulation of [Ramachandran and Amir, 2007] and first show that using the posterior mean may not be a good idea since it may yield a reward function whose corresponding optimal policy is inconsistent with the expert's behaviour; the posterior mean integrates the error over the entire space of reward functions by including (possibly) infinitely many rewards that induce policies that are inconsistent with the expert's demonstration. Instead, they suggest that the maximum-a-posteriori (MAP) estimate could be a better solution for IRL. They formulate IRL as a posterior optimization problem and propose a gradient method to calculate the MAP estimate that is based on the (sub)differentiability of the posterior distribution. Finally, they show that most of the non-Bayesian IRL algorithms in the literature [Ng and Russell, 2000, Ratliff et al., 2006, Neu and Szepesvári, 2007, Ziebart et al., 2008, Syed and Schapire, 2008 can be cast as searching for the MAP reward function in BIRL with different priors and different ways of encoding the compatibility with the expert's policy.</p>
<p>Using a single reward function to explain the expert's behavior might be problematic as its complexity grows with the complexity of the task being optimized by the expert. Searching for a complex reward function is often difficult. If many parameters are needed to model it, we are required to search over a large space of candidate functions. This problem becomes more severe when we take into account the testing of each candidate reward function, which requires solving an MDP for its optimal value function, whose computation usually scales poorly with the size of the state space. Michini and How [Michini and How, 2012a] suggest a potential solution to this problem in which they partition the observations (system trajectories generated by the expert) into sets of smaller sub-demonstrations, such that each sub-demonstration is attributed to a smaller and less-complex class of reward functions. These simple rewards can be intuitively interpreted as sub-goals of the expert. They propose a BIRL algorithm that uses a Bayesian non-parametric mixture model to automate this partitioning process. The proposed algorithm uses a Chinese restaurant process prior over partitions so that there is no need to specify the number of partitions a priori.</p>
<p>Most of the work in IRL assumes that the data is generated by a single expert optimizing a fixed reward function. However, there are many applications in which we observe multiple experts, each executing a policy that is optimal (or good) with respect to its own reward function. There have been a few works that consider this scenario. Dimitrakakis and Rothkopf [Dimitrakakis and Rothkopf, 2011] generalize BIRL to multi-task learning. They assume a common prior for the trajectories and estimate the reward functions individually for each trajectory. Other than the common prior, they do not make any additional efforts to group trajectories that are likely to be generated from the same or similar reward functions. Babes et al. [Babes et al., 2011] propose a method that combines expectation maximization (EM) clustering with IRL. They cluster the trajectories based on the inferred reward functions, where one reward function is defined per cluster. However, their proposed method is based on the assumption that the number of clusters (the number of the reward functions) is known. Finally, Choi and Kim [Choi and Kim, 2012] present a nonparametric Bayesian approach using the Dirichlet process mixture model in order to address the IRL problem with multiple reward functions. They develop an efficient Metropolis-Hastings sampler that utilizes the gradient of the reward function posterior to infer the reward functions from the behavior data. Moreover, after completing IRL on the behavior data, their method can efficiently estimate the reward function for a new trajectory by computing the mean of the reward function posterior, given the pre-learned results.</p>
<p>Bayesian Multi-agent Reinforcement Learning</p>
<p>There is a rich literature on the use of RL methods for multi-agent systems. More recently, the extension of BRL methods to collaborative multi-agent systems has been proposed [Chalkiadakis and Boutilier, 2013]. The objective in this case is consistent with the standard BRL objective, namely to optimally balance the cost of exploring the world with the expected benefit of new information. However, when dealing with multi-agent systems, the complexity of the decision problem is increased in the following way: while single-agent BRL requires maintaining a posterior over the MDP parameters (in the case of model-based methods) or over the value/policy (in the case of model-free methods), in multi-agent BRL, it is also necessary to keep a posterior over the policies of the other agents. This belief can be maintained in a tractable manner subject to certain structural assumptions on the domain, for example that the strategies of the agents are independent of each other. Alternatively, this framework can be used to control the formation of coalitions among agents [Chalkiadakis et al., 2010]. In this case, the Bayesian posterior can be limited to the uncertainty about the capabilities of the other agents, and the uncertainty about the effects of coalition actions among the agents. The solution to the Bayes-optimal strategy can be approximated using a number of techniques, including myopic or short (1-step) lookahead on the value function, or an extension of the value-of-information criteria proposed by [Dearden et al., 1998].</p>
<p>Bayesian Multi-Task Reinforcement Learning</p>
<p>Multi-task learning (MTL) is an important learning paradigm and active area of research in machine learning (e.g., [Caruana, 1997, Baxter, 2000). A common setup in MRL considers multiple related tasks for which we are interested in improving the performance of individual learning by sharing information across tasks. This transfer of information is particularly important when we are provided with only a limited number of data with which learn each task. Exploiting data from related problems provides more training samples for the learner and can improve the performance of the resulting solution. More formally, the main objective in MTL is to maximize the improvement over individual learning, averaged over multiple tasks. This should be distinguished from transfer learning in which the goal is to learn a suitable bias for a class of tasks in order to maximize the expected future performance.</p>
<p>Traditional RL algorithms typically do not directly take advantage of information coming from other similar tasks. However recent work has shown that transfer and multi-task learning techniques can be employed in RL to reduce the number of samples needed to achieve nearly-optimal solutions. All approaches to multi-task RL (MTRL) assume that the tasks share similarity in some components of the problem such as dynamics, reward structure, or value function. While some methods explicitly assume that the shared components are drawn from a common generative model [Wilson et al., 2007, Mehta et al., 2008, Lazaric and Ghavamzadeh, 2010, this assumption is more implicit in others. In [Mehta et al., 2008], tasks share the same dynamics and reward features, and only differ in the weights of the reward function. The proposed method initializes the value function for a new task using the previously learned value functions as a prior. Wilson et al. [Wilson et al., 2007] and Lazaric and Ghavamzadeh [Lazaric and Ghavamzadeh, 2010] both assume that the distribution over some components of the tasks is drawn from a hierarchical Bayesian model (HBM). We describe these two methods in more detail below.</p>
<p>Lazaric and Ghavamzadeh [Lazaric and Ghavamzadeh, 2010] study the MTRL scenario in which the learner is provided with a number of MDPs with common state and action spaces. For any given policy, only a small number of samples can be generated in each MDP, which may not be enough to accurately evaluate the policy. In such a MTRL problem, it is necessary to identify classes of tasks with similar structure and to learn them jointly. It is important to note that here a task is a pair of MDP and policy (i.e., a Markov chain) such that all the MDPs have the same state and action spaces. They consider a particular class of MTRL problems in which the tasks share structure in their value functions. To allow the value functions to share a common structure, it is assumed that they are all sampled from a common prior. They adopt the GPTD value function model ( [Engel et al., 2005a], also see §5.1.1) for each task, model the distribution over the value functions using an HBM, and develop solutions to the following problems: (i) joint learning of the value functions (multi-task learning), and (ii) efficient transfer of the information acquired in (i) to facilitate learning the value function of a newly observed task (transfer learning). They first present an HBM for the case in which all of the value functions belong to the same class, and derive an EM algorithm to find MAP estimates of the value functions and the model's hyper-parameters. However, if the functions do not belong to the same class, simply learning them together can be detrimental (negative transfer). It is therefore important to have models that will generally benefit from related tasks and will not hurt performance when the tasks are unrelated. This is particularly important in RL as changing the policy at each step of policy iteration (this is true even for fitted value iteration) can change the way in which tasks are clustered together. This means that even if we start with value functions that all belong to the same class, after one iteration the new value functions may be clustered into several classes. To address this issue, they introduce a Dirichlet process (DP) based HBM for the case where the value functions belong to an undefined number of classes, and derive inference algorithms for both the multi-task and transfer learning scenarios in this model. The MTRL approach in [Wilson et al., 2007] also uses a DP-based HBM to model the distribution over a common structure of the tasks. In this work, the tasks share structure in their dynamics and reward functions. The setting is incremental, i.e., the tasks are observed as a sequence, and there is no restriction on the number of samples generated by each task. The focus is not on joint learning with finite number of samples, but on using the information gained from the previous tasks to facilitate learning in a new one. In other words, the focus in this work is on transfer and not on multi-task learning.</p>
<p>Outlook</p>
<p>Bayesian reinforcement learning (BRL) offers a coherent probabilistic model for reinforcement learning. It provides a principled framework to express the classic exploration-exploitation dilemma, by keeping an explicit representation of uncertainty, and selecting actions that are optimal with respect to a version of the problem that incorporates this uncertainty. This framework is of course most useful for tackling problems where there is an explicit representation of prior information. Throughout this survey, we have presented several frameworks for leveraging this information, either over the model parameters (model-based BRL) or over the solution (model-free BRL).</p>
<p>In spite of its elegance, BRL has not, to date, been widely applied. While there are a few successful applications of BRL for advertising [Graepel et al., 2010, Tang et al., 2013 and robotics [Engel et al., 2005b, Ross et al., 2008b, adoption of the Bayesian framework in applications is lagging behind the comprehensive theory that BRL has to offer. In the remainder of this section we analyze the perceived limitations of BRL and offer some research directions that might circumvent these limitations.</p>
<p>One perceived limitation of Bayesian RL is the need to provide a prior. While this is certainly the case for model-based BRL, for larger problems there is always a need for some sort of regularization. A prior serves as a mean to regularize the model selection problem. Thus, the problem of specifying a prior is addressed by any RL algorithm that is supposed to work for large-scale problems. We believe that a promising direction for future research concerns devising priors based on observed data, as per the empirical Bayes approach [Efron, 2010]. A related issue is model mis-specification and how to quantify the performance degradation that may arise from not knowing the model precisely.</p>
<p>There are also some algorithmic and theoretical challenges that we would like to point out here. First, scaling BRL is a major issue. Being able to solve large problems is still an elusive goal. We should mention that currently, a principled approach for scaling up RL in general is arguably missing. Approximate value function methods [Bertsekas and Tsitsiklis, 1996] have proved successful for solving certain large scale problems [Powell, 2011], and policy search has been successfully applied to many robotics tasks [Kober et al., 2013]. However, there is currently no solution (neither frequentist nor Bayesian) to the exploration-exploitation dilemma in large-scale MDPs. We hope that scaling up BRL, in which exploration-exploitation is naturally handled, may help us overcome this barrier. Conceptually, BRL may be easier to scale since it allows us, in some sense, to embed domain knowledge into a problem. Second, the BRL framework we presented assumes that the model is specified correctly. Dealing with model misspecification and consequently with model selection is a thorny issue for all RL algorithms. When the state parameters are unknown or not observed, the dynamics may stop being Markov and many RL algorithms fail in theory and in practice. The BRL framework may offer a solution to this issue since the Bayesian approach can naturally handle model selection and misspecification by not committing to a single model and rather sustaining a posterior over the possible models.</p>
<p>Another perceived limitation of BRL is the complexity of implementing the majority of BRL methods. Indeed, some frequentist RL algorithms are more elegant than their Bayesian counterparts, which may discourage some practitioners from using BRL. This is, by no means, a general rule. For example, Thompson sampling is one of the most elegant approaches to the multi-armed bandit problem. Fortunately, the recent release of a software library for Bayesian RL algorithms promises to facilitate this [bbr, 2015[bbr, , Castronovo et al., 2015. We believe that Thompson sampling style algorithms (e.g., Van Roy, 2014b, Gopalan andMannor, 2015]) can pave the way to efficient algorithms in terms of both sample complexity and computational complexity. Such algorithms require, essentially, solving an MDP once in a while while taking nearly optimal exploration rate.</p>
<p>From the application perspective, we believe that BRL is still in its infancy. In spite of some early successes, especially for contextual bandit problems [Chapelle and Li, 2011], most of the successes of largescale real applications of RL are not in the Bayesian setting. We hope that this survey will help facilitate the research needed to make BRL a success in practice. A considerable benefit of BRL is its ability to work well "out-of-the-box": you only need to know relatively little about the uncertainty to perform well. Moreover, adding complicating factors such as long-term constraints or short-term safety requirements can be easily embedded in the framework.</p>
<p>From the modelling perspective, deep learning is becoming increasingly more popular as a building block in RL. It would be interesting to use probabilistic deep networks, such as restricted Boltzman machines as an essential ingredient in BRL. Probabilistic deep models can not only provide a powerful value function or policy approximator, but also allow using historical traces that come from a different problem (as in transfer learning). We believe that the combination of powerful models for value or policy approximation in combination with a BRL approach can further facilitate better exploration policies.</p>
<p>From the conceptual perspective, the main question that we see as open is how to embed domain knowledge into the model? One approach is to modify the prior according to domain knowledge. Another is to consider parametrized or factored models. While these approaches may work well for particular domains, they require careful construction of the state space. However, in some cases, such a careful construction defeats the purpose of a fast and robust "out-of-the-box" approach. We believe that approaches that use causality and non-linear dimensionality reduction may be the key to using BRL when a careful construction of a model is not feasible. In that way, data-driven algorithms can discover simple structures that can ultimately lead to fast BRL algorithms.</p>
<p>Notation</p>
<p>Definition Y set of outcomes (in MAB) y ∈ Y a nominal outcome r(y) reward for outcome y P (y|a) ∈ P (Y) probability of observing outcome y after taking action a a * optimal arm ∆ a difference in expected reward between arms a and a * S set of states (or contexts in a contextual MAB) s ∈ S a nominal state (context) P S (s) ∈ P(S) context probability (in contextual MAB) P (y|a, s) ∈ P (Y) probability of observing outcome y after taking action a when the context is s (in contextual MAB) O set of observations o ∈ O a nominal observation P ∈ P(S) transition probability function P (s |s, a) ∈ P(S) probability of being in state s after taking action a in state s Ω(o|s, a) ∈ P(O) probability of seeing observation o after taking action a to reach state s q ∈ P(R) probability distribution over reward R(s, a) ∼ q(·|s, a) random variable of the reward of taking action a in state s r (s, a) reward of taking action a in state s r(s, a) expected reward of taking action a in state s R max maximum random immediate reward R max maximum expected immediate reward P 0 ∈ P(S) initial state distribution b t ∈ P(S) POMDP's state distribution at time t τ (b t , a, o) the information state (belief) update equation µ a (stationary and Markov) policy µ (a|s) probability that policy µ selects action a in state s µ * an optimal policy M µ the Markov chain induced by policy µ P µ probability distribution of the Markov chain induced by policy µ P µ (s |s) probability of being in state s after taking an action according to policy µ in state s P µ where N (s t ) ∼ N (0, σ 2 t ). This is exactly the generative model we would use if we wanted to learn the value function by performing GP regression using MC samples of the discounted return as our target (see §2.5.2). Assuming a constant noise variance σ 2 , in the parametric case, the posterior moments are given by Eq. 2.22, and in the nonparametric setting, α and C, defining the posterior moments, are given by Eq. 2.18, with y T = (y 0 , . . . , y T −1 ) ; y t = T −1 i=t γ i−t r(s i ). Note that here y T = H −1 r T , where r T is a realization of the random vector R T .</p>
<p>This equivalence uncovers the implicit assumption underlying MC value estimation that the samples of the discounted return used for regression are statistically independent. In a standard online RL scenario, this assumption is clearly incorrect as the samples of the discounted return are based on trajectories that partially overlap (e.g., for two consecutive states s t and s t+1 , the respective trajectories only differ by a single state s t ). This may help explain the frequently observed advantage of TD methods using λ &lt; 1 over the corresponding MC (λ = 1) methods. The major advantage of using the GPTD formulation is that it immediately allows us to derive exact updates for the parameters of the posterior moments of the value function, rather than waiting until the end of the episode. This point becomes more clear, when later in this section, we use the GPTD formulation and derive online algorithms for value function estimation.</p>
<p>--
Finite state controllers --BEETLE --Bayesian DP --VOI heuris9c --Forward search --Bayesian sparse sampling --HMDP --BFS3 --Branch--and--bound search --BAMCP --BOSS --BEB --VBRB --BOLT --GPTD --GPSARSA--Bayesian Quadrature --Two Bayesian models for es9ma9ng the policy gradient --GPTD + Bayesian policy gradient</p>
<p>Figure 2 . 1 :
21A directed graph illustrating the conditional independencies between the latent F (xt) variables (bottom row), the noise variables N (xt) (top row), and the observable Y (xt) variables (middle row), in GP regression (when H = I). All of the F (xt) variables should be interconnected by arrows (forming a clique), due to the dependencies introduced by the prior. To avoid cluttering the diagram, this was marked by the dashed frame surrounding them.</p>
<p>Figure 2 .
21 illustrates the GP regression setting (when H = I) as a graphical model in which arrows mark the conditional dependency relations between the nodes corresponding to the latent F (x t ) and the observed Y (x t ) variables. The model-equation (2.11) evaluated at the training samples may be written asY T = HF T + N T ,(2.12)</p>
<p>Figure 4 . 1 :
41The Chain problem (figure reproduced from[Strens, 2000]) </p>
<p>Figure 4 . 2 :
42An AND-OR tree constructed by the forward search process for the Chain problem. The top node contains the initial state 1 and the prior over the model φ0. After the first action, the agent can end up in either state 1 or state 2 of the Chain and update its posterior accordingly. Note that depending on what action was chosen (1 or 2) and the effect (a or b), different parameters of φ are updated as per Algorithm 2 .</p>
<p>52CHAPTER 4 .
4MODEL-BASED BAYESIAN REINFORCEMENT LEARNING Algorithm 2 Forward Search Planning in the BAMDP. 1: function ForwardSearch-BAMDP(s, φ, d) 2: if d = 0 then 3:</p>
<p>Theorem 4.6.[Araya-Lopez et al., 2012] Let A t denote the policy followed by BOLT at time t with η = H. Let also s t and φ t be the corresponding state and posterior at the time. Then, with probability at least 1 − δ, BOLT is -close to the optimal Bayesian policy</p>
<p>s
∈S φ s,a,s and P(φ |φ, s, a, s ) = I(φ s,a,s = φ s,a,s +1). Similarly, P(o|a, x , ψ) = ψ s ,a,o o ∈O ψ s ,a,o and P(ψ |ψ, s , a, o) = I(ψ s ,a,o = ψ s ,a,o + 1).</p>
<p>. 5.1 into Eq. 5.3 and rearranging we obtain 1R(s) = V (s) − γV (s ) + N (s, s ), s ∼ P (·|s), where N (s, s ) def = ∆V (s) − γ∆V (s ). (5.4)When we are provided with a system trajectory ξ of size T , we write the model-equation(5.4) for ξ, resulting in the following set of T equations R(s t ) = V (s t ) − γV (s t+1 ) + N (s t , s t+1 ), t = 0, . . . , T − 1. (5.5) By defining R T = R(s 0 ), . . . , R(s T −1 ) , V T +1 = V (s 0 ), . . . , V (s T ) , and N T =</p>
<p>Figure 5 . 1 :
51A graph illustrating the conditional independencies between the latent V (st) value variables (bottom row), the noise variables ∆V (st) (top row), and the observable R(st) reward variables (middle row), in the GPTD model. As in the case of GP regression, all of the V (st) variables should be connected by arrows, due to the dependencies introduced by the prior. To avoid cluttering the diagram, this was marked by the dashed frame surrounding them.</p>
<p>Figure 5 . 2 :
52The connection between the GPTD model described in §5.1.1 and the family of linear statistical models and GP regression discussed in §2.5.2.</p>
<p>α
= H (HKH + Σ) −1 r T and C = H (HKH + Σ) −1 H. (5.16) Similar to the parametric case, assuming a constant noise variance, Eq. 5.16 may be written for episodic problems as α = (K + σ 2 I) −1 H −1 r T and C = (K + σ 2 I) −1 . (5.17)</p>
<p>33) where u(z; θ) and G(θ) are respectively the score function and the Fisher information matrix, defined as 6 u(z; θ) = ∇ log µ(a|s; θ), G(θ) = E s∼ν µ ,a∼µ ∇ log µ(a|s; θ)∇ log µ(a|s; θ) (5.34) = E z∼π µ u(z; θ)u(z; θ) .</p>
<p>0 initial state distribution of the Markov chain induced by policy µ q µ reward distribution of the Markov chain induced by policy µ 130APPENDIX B. DISCUSSION ON GPTD ASSUMPTIONS ON THE NOISE PROCESS whitening transformation with the whitening matrix H −1 . The transformed model is H −1R T = V T + N T with white Gaussian noise N T = H −1 N T ∼ N 0, diag(σ T ) , where σ T = (σ 2 0 , . . . , σ 2 T −1 ) . The tth equation of this transformed model is R(s t ) + γR(s t+1 ) + . . . + γ T −1−t R(s T −1 ) = V (s t ) + N (s t ),</p>
<p>6 )
6which is called the Bellman optimality equation. Note that, similar to Eqs. 2.4 and 2.6, we may define the Bellman equation and the Bellman optimality equation for action-value function.</p>
<p>All assumptions are similar to MDPs, with the addition that P(O) is the set of probability distributions on (Borel) subsets of O, and Ω is stationary. As a motivation for the POMDP model, we revisit again the online shop domain.Example 2.2 (Online shop -hidden state setting). Recall the online 
shop domain of Example 1.1. In a realistic scenario, some features of 
the customer, such as its gender or age, might not be visible to the 
decision maker due to privacy, or other reasons. In such a case, the MDP 
model, which requires the full state information for making decisions 
is not suitable. In the POMDP model, only observable quantities, such 
as the items in the shopping cart, are used for making decisions. </p>
<p>a 9 :
9for all s ∈ S do 10: φ s,a,s ← φ s,a,s + 111: </p>
<p>a,s if q &gt; maxQ then 15: maxQ ← q 16: end if 17: end for 18: return maxQ13: </p>
<p>end for </p>
<p>14: </p>
<p>Table 4 .
41: Online Model-based Bayesian RL methods (DP=Dynamic program-
ming, LP = Linear programming, U=Upper-bound, L=Lower-bound) </p>
<p>Table 4 .
41: (cont'd) Online Model-based Bayesian RL methods (DP=Dynamic pro-
gramming, LP = Linear programming, U=Upper-bound, L=Lower-bound) </p>
<p>Table 5 . 1 :
51Summary of the Bayesian policy gradient Models 1 and 2.</p>
<p>a|i) 2 γV µ + R(i, a)e M i,a γV µ + R(i, a)e ,104CHAPTER 6. RISK-AWARE BAYESIAN REINFORCEMENT LEARNINGand higher order terms </p>
<p>L var = </p>
<p>k,l:k+l&gt;2 </p>
<p>R is the set of real numbers.
When there is no randomness in the rewards, i.e., r(s, a) =r(s, a), then ρ(ξ) = ρ(ξ).3 When γ = 1, the policy must be proper, i.e., guaranteed to terminate, see[Puterman, 1994] or[Bertsekas and Tsitsiklis, 1996] for more details.
It is not possible to find an optimal policy in many practical problems. In such cases, the goal would be to find a reasonably good policy, i.e., a policy with large enough value function.
We have slightly modified the description of this theorem, and others below, to improve legibility of the paper. Refer to the original publication for full details.
Note that in Eq. 5.4, we removed the dependency of N to policy µ and replaced N µ with N .
We use the notation ∇ to denote ∇ θ -the gradient with respect to the policy parameters.
O'Hagan [O'Hagan, 1991] mentions that this approach may be traced even as far back as the work by Poincaré[Poincaré, 1896].
Similar to[O'Hagan, 1991], here for simplicity we consider the case where the integral to be estimated is a scalar-valued integral. However, the results of this section can be extended to vector-valued integrals, such as the gradient of the expected return with respect to the policy parameters that we shall study in §5.2.2 (seeGhavamzadeh et al. [2013]).
To simplify notation, we omit G's dependence on the policy parameters θ, and denote G(θ) as G in the rest of this section.
Similar to u(ξ) and G defined by Eqs. 5.19 and 5.27, to simplify the notation, we omit the dependence of u and G to the policy parameters θ, and replace u(z; θ) and G(θ) with u(z) and G in the sequel.
methods and the BPG algorithm, which do not take into account the Markovian property of the system.
The term risk here should not be confused with the Bayes risk, defined in Chapter 3.
Following the framework of Chapter 4, we assume that the rewards are known and only the transitions are unknown. The following results may be extended to cases where the rewards are also unknown, as outlined in §4.7.3 Note that this is similar to the BAMDP formulation of §4.1, where the hyperstate encodes the posterior probabilities of the transitions given the observations.
This is similar to the popular financial risk measure Value-at-Risk (VaR). However, note that VaR is typically used in the context of internal uncertainty.
Note that the 1st order term is ignored, since it would cancel anyway in the optimization that follows.
This is also known as a set of scenarios.
In this paper, we use upper-case and lower-case letters to refer to random variables and the values taken by random variables, respectively. We also use bold-face letters for vectors and matrices.
Acknowledgements AcknowledgementsThe authors extend their warmest thanks to Michael Littman, James Finlay, Melanie Lyman-Abramovitch and the anonymous reviewers for their insights and support throughout the preparation of this manuscript. The authors wish to also thank several colleagues for helpful discussions on this topic: Doina Precup, Amir-massoud Farahmand, Brahim Chaib-draa, and Pascal Poupart. The review of the modelbased Bayesian RL approaches benefited significantly from comprehensive reading lists posted online by John Asmuth and Chris Mansley. Funding for Joelle Pineau was provided by the Natural Sciences and Engineering Research Council Canada (NSERC) through their Discovery grants program, by the Fonds de Québécois de Recherche Nature et Technologie (FQRNT) through their Projet de recherche en équipe program. Funding for Shie Mannor and Aviv Tamar were partially provided by the European Community's Seventh Framework Programme (FP7/2007(FP7/ -2013 under grant agreement 306638 (SUPREL). Aviv Tamar is also partially funded by the Viterbi Scholarship, Technion.AppendicesA Index of SymbolsHere we present a list of the symbols used in this paper to provide a handy reference. reward random variable of the Markov chain induced by policy µ at state s π µ ∈ P(S) stationary distribution over states of the Markov chain induced by policy µrandom variable of the (discounted) return of state-action pair z V µ value function of policy µ Q µ action-value function of policy µ V * optimal value function Q * optimal action-value function γ discount factor α linear function over the belief simplex in POMDPs Γ the set of α-functions representing the POMDP value function k(·, ·) a kernel function K kernel matrix -covariance matrix -a kernel vector of size T I identity matrix D T a set of training samples of size T ϕ i a basis function (e.g., over states or state-action pairs) φ(·) = ϕ 1 (·), . . . , ϕ n (·) a feature vector of size n Φ = [φ(x 1 ), . . . , φ(x T )] a n × T feature matrix θ vector of unknown parameters Θ a parameter space that the unknown parameters belong to (θ ∈ Θ)B Discussion on GPTD Assumptions on the Noise ProcessAssumption A2 The residuals ∆V T +1 = ∆V (s 0 ), . . . , ∆V (s T ) can be modeled as a Gaussian process.This may not seem like a correct assumption in general, however, in the absence of any prior information concerning the distribution of the residuals, it is the simplest assumption that can be made, because the Gaussian distribution has the highest entropy among all distributions with the same covariance.Assumption A3 Each of the residuals ∆V (s t ) is generated independently of all the others, i.e., E ∆V (s i )∆V (s j ) = 0, for i = j.This assumption is related to the well-known Monte-Carlo (MC) method for value function estimation[Bertsekas andTsitsiklis, 1996, Sutton and]. MC approach to policy evaluation reduces it into a supervised regression problem, in which the target values for the regression are samples of the discounted return. Suppose that the last non-terminal state in the current episode is s T −1 , then the MC training set is. We may whiten the noise in the episodic GPTD model of Eqs. 5.9 and 5.10 by performing a
Bbrl a c++ open-source library used to compare bayesian reinforcement learning algorithms. Bbrl a c++ open-source library used to compare bayesian reinforcement learn- ing algorithms. https://github.com/mcastron/BBRL/, 2015.</p>
<p>Bayesian optimal control of smoothly parameterized systems. Y Abbasi-Yadkori, C Szepesvari, Proceedings of the Conference on Uncertainty in Artificial Intelligence. the Conference on Uncertainty in Artificial IntelligenceY. Abbasi-Yadkori and C. Szepesvari. Bayesian optimal control of smoothly parameterized systems. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2015.</p>
<p>Apprenticeship learning via inverse reinforcement learning. P Abbeel, A Ng, Proceedings of the 21st International Conference on Machine Learning. the 21st International Conference on Machine LearningP. Abbeel and A. Ng. Apprenticeship learning via inverse reinforcement learn- ing. In Proceedings of the 21st International Conference on Machine Learn- ing, 2004.</p>
<p>Analysis of Thompson sampling for the multi-armed bandit problem. S Agrawal, N Goyal, ; , Jmlr W&amp;cp, Proceedings of the 25th Annual Conference on Learning Theory (COLT). the 25th Annual Conference on Learning Theory (COLT)23S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), JMLR W&amp;CP, volume 23, pages 39.1 -39.26, 2012.</p>
<p>Further optimal regret bounds for Thompson sampling. S Agrawal, N Goyal, Proceedings of the 16th International Conference on Artificial Intelligence and Statistics. the 16th International Conference on Artificial Intelligence and StatisticsS. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sam- pling. In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics, pages 99-107, 2013a.</p>
<p>Thompson sampling for contextual bandits with linear payoffs. S Agrawal, N Goyal, Proceedings of the 30th International Conference on Machine Learning (ICML-13). the 30th International Conference on Machine Learning (ICML-13)S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Ma- chine Learning (ICML-13), pages 127-135, 2013b.</p>
<p>Near-optimal BRL using optimistic local transitions. M Araya-Lopez, V Thomas, O Buffet, International Conference on Machine Learning. M. Araya-Lopez, V. Thomas, and O. Buffet. Near-optimal BRL using opti- mistic local transitions. In International Conference on Machine Learning, 2012.</p>
<p>Model-based Bayesian Reinforcement Learning with Generalized Priors. J Asmuth, RutgersPhD thesisJ. Asmuth. Model-based Bayesian Reinforcement Learning with Generalized Priors. PhD thesis, Rutgers, 2013.</p>
<p>Approaching Bayes-optimality using Monte-Carlo tree search. J Asmuth, M Littman, International Conference on Automated Planning and Scheduling (ICAPS). J. Asmuth and M. Littman. Approaching Bayes-optimality using Monte- Carlo tree search. In International Conference on Automated Planning and Scheduling (ICAPS), 2011.</p>
<p>A Bayesian sampling approach to exploration in reinforcement learning. J Asmuth, L Li, M Littman, A Nouri, D Wingate, Proceedings of the Conference on Uncertainty in Artificial Intelligence. the Conference on Uncertainty in Artificial IntelligenceJ. Asmuth, L. Li, M. Littman, A. Nouri, and D. Wingate. A Bayesian sampling approach to exploration in reinforcement learning. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2009.</p>
<p>Optimal control of Markov decision processes with incomplete state estimation. K Astrom, Journal of Mathematical Analysis and Applications. 10K. Astrom. Optimal control of Markov decision processes with incomplete state estimation. Journal of Mathematical Analysis and Applications, 10: 174-205, 1965.</p>
<p>A comparison of direct and modelbased reinforcement learning. C G Atkeson, J C Santamaria, International Conference on Robotics and Automation (ICRA). C. G. Atkeson and J. C. Santamaria. A comparison of direct and model- based reinforcement learning. In International Conference on Robotics and Automation (ICRA), 1997.</p>
<p>A Bayesian reinforcement learning approach for customizing human-robot interfaces. A Atrash, J Pineau, International Conference on Intelligent User Interfaces. A. Atrash and J. Pineau. A Bayesian reinforcement learning approach for customizing human-robot interfaces. In International Conference on Intel- ligent User Interfaces, 2009.</p>
<p>Finite-time analysis of the multiarmed bandit problem. P Auer, N Cesa-Bianchi, P Fischer, Machine Learning. 47P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multi- armed bandit problem. Machine Learning, 47(2-3):235-256, 2002.</p>
<p>Apprenticeship learning about multiple intentions. M Babes, V Marivate, K Subramanian, M Littman, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningM. Babes, V. Marivate, K. Subramanian, and M. Littman. Apprenticeship learning about multiple intentions. In Proceedings of the 28th International Conference on Machine Learning, pages 897-904, 2011.</p>
<p>Neuron-like elements that can solve difficult learning control problems. A Barto, R Sutton, C Anderson, IEEE Transaction on Systems, Man and Cybernetics. 13A. Barto, R. Sutton, and C. Anderson. Neuron-like elements that can solve difficult learning control problems. IEEE Transaction on Systems, Man and Cybernetics, 13:835-846, 1983.</p>
<p>A model of inductive bias learning. J Baxter, Journal of Artificial Intelligence Research. 12J. Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:149-198, 2000.</p>
<p>Infinite-horizon policy-gradient estimation. J Baxter, P Bartlett, Journal of Artificial Intelligence Research. 15J. Baxter and P. Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15:319-350, 2001.</p>
<p>Knightcap: A chess program that learns by combining TD(λ) with game-tree search. J Baxter, A Tridgell, L Weaver, Proceedings of the 15th International Conference on Machine Learning. the 15th International Conference on Machine LearningJ. Baxter, A. Tridgell, and L. Weaver. Knightcap: A chess program that learns by combining TD(λ) with game-tree search. In Proceedings of the 15th International Conference on Machine Learning, pages 28-36, 1998.</p>
<p>Experiments with infinite-horizon policy-gradient estimation. J Baxter, P Bartlett, L Weaver, Journal of Artificial Intelligence Research. 15J. Baxter, P. Bartlett, and L. Weaver. Experiments with infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15: 351-381, 2001.</p>
<p>Dynamic Programming. R Bellman, Princeton Universty PressR. Bellman. Dynamic Programming. Princeton Universty Press, 1957.</p>
<p>Neuro-Dynamic Programming. D Bertsekas, J Tsitsiklis, Athena Scientific. D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scien- tific, 1996.</p>
<p>Robust adaptive Markov decision processes: Planning with model uncertainty. L Bertuccelli, A Wu, J How, Control Systems. 325IEEEL. Bertuccelli, A. Wu, and J. How. Robust adaptive Markov decision pro- cesses: Planning with model uncertainty. Control Systems, IEEE, 32(5): 96-109, Oct 2012.</p>
<p>Incremental natural actor-Critic algorithms. S Bhatnagar, R Sutton, M Ghavamzadeh, M Lee, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsS. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee. Incremental natural actor-Critic algorithms. In Proceedings of the Advances in Neural Informa- tion Processing Systems, pages 105-112, 2007.</p>
<p>Natural actor-critic algorithms. S Bhatnagar, R Sutton, M Ghavamzadeh, M Lee, Automatica. 4511S. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic algorithms. Automatica, 45(11):2471-2482, 2009.</p>
<p>Least-squares temporal difference learning. J Boyan, Proceedings of the 16th International Conference on Machine Learning. the 16th International Conference on Machine LearningJ. Boyan. Least-squares temporal difference learning. In Proceedings of the 16th International Conference on Machine Learning, pages 49-56, 1999.</p>
<p>Linear least-squares algorithms for temporal difference learning. S Bradtke, A Barto, Journal of Machine Learning. 22S. Bradtke and A. Barto. Linear least-squares algorithms for temporal differ- ence learning. Journal of Machine Learning, 22:33-57, 1996.</p>
<p>R-max -a general polynomial time algorithm for near-optimal reinforcement learning. R Brafman, M Tennenholtz, Journal of Machine Learning Research (JMLR). 3R. Brafman and M. Tennenholtz. R-max -a general polynomial time algo- rithm for near-optimal reinforcement learning. Journal of Machine Learn- ing Research (JMLR), 3:213-231, 2003.</p>
<p>Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning. S Bubeck, N Cesa-Bianchi, 1935-82375S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochas- tic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012. ISSN 1935-8237.</p>
<p>Multitask learning. R Caruana, Machine Learning. 28R. Caruana. Multitask learning. Machine Learning, 28(1):41-75, 1997.</p>
<p>Using linear programming for Bayesian exploration in Markov decision processes. P Castro, D Precup, International Joint Conference on Artificial Intelligence. P. Castro and D. Precup. Using linear programming for Bayesian exploration in Markov decision processes. In International Joint Conference on Artifi- cial Intelligence, pages 2437-2442, 2007.</p>
<p>Smarter sampling in model-based Bayesian reinforcement learning. P Castro, D Precup, Machine Learning and Knowledge Discovery in Databases. P. Castro and D. Precup. Smarter sampling in model-based Bayesian re- inforcement learning. In Machine Learning and Knowledge Discovery in Databases, 2010.</p>
<p>Benchmarking for bayesian reinforcement learning. M Castronovo, D Ernst, R Fonteneau, A Couetoux, Working paper, Inst. MontefioreM. Castronovo, D. Ernst, and R. Fonteneau A. Couetoux. Benchmarking for bayesian reinforcement learning. Working paper, Inst. Montefiore, http://hdl.handle.net/2268/185881, 2015.</p>
<p>Coordination in multiagent reinforcement learning: A Bayesian approach. G Chalkiadakis, C Boutilier, Proceedings of the 2nd International Joint Conference on Autonomous Agents and Multiagent Systems (AA-MAS). the 2nd International Joint Conference on Autonomous Agents and Multiagent Systems (AA-MAS)G. Chalkiadakis and C. Boutilier. Coordination in multiagent reinforcement learning: A Bayesian approach. In Proceedings of the 2nd International Joint Conference on Autonomous Agents and Multiagent Systems (AA- MAS), 2013.</p>
<p>Cooperative games with overlapping coalitions. G Chalkiadakis, E Elkinda, E Markakis, M Polukarov, N Jennings, Journal of Artificial Intelligence Research. 391G. Chalkiadakis, E. Elkinda, E. Markakis, M. Polukarov, and N. Jennings. Cooperative games with overlapping coalitions. Journal of Artificial Intel- ligence Research, 39(1):179-216, 2010.</p>
<p>An empirical evaluation of Thompson sampling. O Chapelle, L Li, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsO. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Pro- ceedings of the Advances in Neural Information Processing Systems, pages 2249-2257, 2011.</p>
<p>Tractable objectives for robust policy optimization. K Chen, M Bowling, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsK. Chen and M. Bowling. Tractable objectives for robust policy optimization. In Proceedings of the Advances in Neural Information Processing Systems, pages 2078-2086, 2012.</p>
<p>Map inference for Bayesian inverse reinforcement learning. J Choi, K Kim, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsJ. Choi and K. Kim. Map inference for Bayesian inverse reinforcement learn- ing. In Proceedings of the Advances in Neural Information Processing Sys- tems, pages 1989-1997, 2011.</p>
<p>Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. J Choi, K Kim, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsJ. Choi and K. Kim. Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. In Proceedings of the Advances in Neural Information Processing Systems, pages 305-313, 2012.</p>
<p>Elevator group control using multiple reinforcement learning agents. R Crites, A Barto, Machine Learning. 33R. Crites and A. Barto. Elevator group control using multiple reinforcement learning agents. Machine Learning, 33:235-262, 1998.</p>
<p>Bayesian reinforcement learning in continuous POMDPs with Gaussian processes. P Dallaire, C Besse, S Ross, B Chaib-Draa, IEEE/RSJ International Conference on Intelligent Robots and Systems. P. Dallaire, C. Besse, S. Ross, and B. Chaib-draa. Bayesian reinforcement learning in continuous POMDPs with Gaussian processes. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2009.</p>
<p>Bayesian Q-learning. R Dearden, N Friedman, S J Russell, AAAI Conference on Artificial Intelligence. R. Dearden, N. Friedman, and S. J. Russell. Bayesian Q-learning. In AAAI Conference on Artificial Intelligence, pages 761-768, 1998.</p>
<p>Model based Bayesian exploration. R Dearden, N Friedman, D Andre, Proceedings of the Conference on Uncertainty in Artificial Intelligence. the Conference on Uncertainty in Artificial IntelligenceR. Dearden, N. Friedman, and D. Andre. Model based Bayesian exploration. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 1999.</p>
<p>Percentile optimization for Markov decision processes with parameter uncertainty. E Delage, S Mannor, Operations Research. 581E. Delage and S. Mannor. Percentile optimization for Markov decision pro- cesses with parameter uncertainty. Operations Research, 58(1):203-213, 2010.</p>
<p>Bayesian multi-task inverse reinforcement learning. C Dimitrakakis, C Rothkopf, Proceedings of the European Workshop on Reinforcement Learning. the European Workshop on Reinforcement LearningC. Dimitrakakis and C. Rothkopf. Bayesian multi-task inverse reinforce- ment learning. In Proceedings of the European Workshop on Reinforcement Learning, 2011.</p>
<p>Reinforcement learning with limited reinforcement: using Bayes risk for active learning in POMDPs. F Doshi, J Pineau, N Roy, International Conference on Machine Learning. F. Doshi, J. Pineau, and N. Roy. Reinforcement learning with limited rein- forcement: using Bayes risk for active learning in POMDPs. In International Conference on Machine Learning, 2008.</p>
<p>The infinite partially observable Markov decision process. F Doshi-Velez, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsF. Doshi-Velez. The infinite partially observable Markov decision process. In Proceedings of the Advances in Neural Information Processing Systems, 2009.</p>
<p>Nonparametric Bayesian policy priors for reinforcement learning. F Doshi-Velez, D Wingate, N Roy, J Tenenbaum, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsF. Doshi-Velez, D. Wingate, N. Roy, and J. Tenenbaum. Nonparametric Bayesian policy priors for reinforcement learning. In Proceedings of the Advances in Neural Information Processing Systems, 2010.</p>
<p>Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs. F Doshi-Velez, J Pineau, N Roy, Artificial Intelligence. F. Doshi-Velez, J. Pineau, and N. Roy. Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs. Artificial Intelligence, 2011.</p>
<p>Monte-Carlo algorithms for the improvement of finite-state stochastic controllers: Application to Bayes-adaptive Markov decision processes. M Duff, International Workshop on Artificial Intelligence and Statistics (AIS-TATS). M. Duff. Monte-Carlo algorithms for the improvement of finite-state stochas- tic controllers: Application to Bayes-adaptive Markov decision processes. In International Workshop on Artificial Intelligence and Statistics (AIS- TATS), 2001.</p>
<p>M Duff, Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes. Amherst, MAUniversity of Massachusetts AmherstPhD thesisM. Duff. Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes. PhD thesis, University of Massachusetts Amherst, Amherst, MA, 2002.</p>
<p>Large-Scale Inference: Empirical Bayes Methods fro Estimation, Testing, and Prediction. IMS Statistics Monographs. B Efron, Cambridge University PressB. Efron. Large-Scale Inference: Empirical Bayes Methods fro Estimation, Testing, and Prediction. IMS Statistics Monographs. Cambridge University Press, 2010.</p>
<p>Algorithms and Representations for Reinforcement Learning. Y Engel, The Hebrew University of Jerusalem, IsraelPhD thesisY. Engel. Algorithms and Representations for Reinforcement Learning. PhD thesis, The Hebrew University of Jerusalem, Israel, 2005.</p>
<p>Sparse online greedy support vector regression. Y Engel, S Mannor, R Meir, Proceedings of the 13th European Conference on Machine Learning. the 13th European Conference on Machine LearningY. Engel, S. Mannor, and R. Meir. Sparse online greedy support vector regres- sion. In Proceedings of the 13th European Conference on Machine Learning, pages 84-96, 2002.</p>
<p>Bayes meets Bellman: The Gaussian process approach to temporal difference learning. S Engel, R Mannor, Meir, Proceedings of the 20th International Conference on Machine Learning. the 20th International Conference on Machine LearningEngel, S. Mannor, and R. Meir. Bayes meets Bellman: The Gaussian process approach to temporal difference learning. In Proceedings of the 20th International Conference on Machine Learning, pages 154-161, 2003.</p>
<p>Reinforcement learning with Gaussian processes. S Engel, R Mannor, Meir, Proceedings of the 22nd International Conference on Machine Learning. the 22nd International Conference on Machine LearningEngel, S. Mannor, and R. Meir. Reinforcement learning with Gaussian processes. In Proceedings of the 22nd International Conference on Machine Learning, pages 201-208, 2005a.</p>
<p>Learning to control an octopus arm with gaussian process temporal difference methods. Y Engel, P Szabó, D Volkinshtein, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsY. Engel, P. Szabó, and D. Volkinshtein. Learning to control an octopus arm with gaussian process temporal difference methods. In Proceedings of the Advances in Neural Information Processing Systems, 2005b.</p>
<p>Tree-based batch mode reinforcement learning. D Ernst, P Geurts, L Wehenkel, Journal of Machine Learning Research. 6D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503-556, 2005.</p>
<p>Regularized policy iteration. A Farahmand, M Ghavamzadeh, C , Shie Mannor, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsA. Farahmand, M. Ghavamzadeh, C., and Shie Mannor. Regularized policy iteration. In Proceedings of the Advances in Neural Information Processing Systems, pages 441-448, 2008a.</p>
<p>Regularized fitted q-iteration: Application to planning. A Farahmand, M Ghavamzadeh, C Szepesvári, S Mannor, Recent Advances in Reinforcement Learning, 8th European Workshop, EWRL. A. Farahmand, M. Ghavamzadeh, C. Szepesvári, and S. Mannor. Regularized fitted q-iteration: Application to planning. In Recent Advances in Rein- forcement Learning, 8th European Workshop, EWRL, pages 55-68, 2008b.</p>
<p>PAC-Bayesian model selection for reinforcement learning. M M Fard, J Pineau, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsM. M. Fard and J. Pineau. PAC-Bayesian model selection for reinforcement learning. In Proceedings of the Advances in Neural Information Processing Systems, 2010.</p>
<p>PAC-Bayesian policy evaluation for reinforcement learning. M M Fard, J Pineau, C Szepesvari, Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI). the Conference on Uncertainty in Artificial Intelligence (UAI)M. M. Fard, J. Pineau, and C. Szepesvari. PAC-Bayesian policy evaluation for reinforcement learning. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2011.</p>
<p>Dual control theory, parts i and ii. Automation and Remote Control. A Feldbaum, 21A. Feldbaum. Dual control theory, parts i and ii. Automation and Remote Control, 21:874-880 and 1033-1039, 1961.</p>
<p>Survey of adaptive dual control methods. N M Filatov, H Unbehauen, IEEE Control Theory and Applications. 147N. M. Filatov and H. Unbehauen. Survey of adaptive dual control methods. In IEEE Control Theory and Applications, volume 147, pages 118-128, 2000.</p>
<p>Efficient Bayesian parameter estimation in large discrete domains. N Friedman, Y Singer, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsN. Friedman and Y. Singer. Efficient Bayesian parameter estimation in large discrete domains. In Proceedings of the Advances in Neural Information Processing Systems, 1999.</p>
<p>The grand challenge of computer Go: Monte Carlo tree search and extensions. S Gelly, L Kocsis, M Schoenauer, M Sebag, D Silver, C Szepesvari, O Teytaud, Communications of the ACM. 55S. Gelly, L. Kocsis, M. Schoenauer, M. Sebag, D. Silver, C. Szepesvari, and O. Teytaud. The grand challenge of computer Go: Monte Carlo tree search and extensions. Communications of the ACM, 55(3):106-113, 2012.</p>
<p>Bayesian policy gradient algorithms. M Ghavamzadeh, Y Engel, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsM. Ghavamzadeh and Y. Engel. Bayesian policy gradient algorithms. In Pro- ceedings of the Advances in Neural Information Processing Systems, pages 457-464, 2006.</p>
<p>Bayesian Actor-Critic algorithms. M Ghavamzadeh, Y Engel, Proceedings of the 24th International Conference on Machine Learning. the 24th International Conference on Machine LearningM. Ghavamzadeh and Y. Engel. Bayesian Actor-Critic algorithms. In Pro- ceedings of the 24th International Conference on Machine Learning, pages 297-304, 2007.</p>
<p>Bayesian policy gradient and actor-critic algorithms. M Ghavamzadeh, Y Engel, M Valko, 00776608INRIA. Technical ReportM. Ghavamzadeh, Y. Engel, and M. Valko. Bayesian policy gradient and actor-critic algorithms. Technical Report 00776608, INRIA, 2013.</p>
<p>Bandit processes and dynamic allocation indices. J Gittins, Journal of the Royal Statistical Society. Series B (Methodological). J. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society. Series B (Methodological), pages 148-177, 1979.</p>
<p>Likelihood ratio gradient estimation for stochastic systems. P Glynn, Communications of the ACM. 33P. Glynn. Likelihood ratio gradient estimation for stochastic systems. Com- munications of the ACM, 33:75-84, 1990.</p>
<p>Thompson sampling for learning parameterized markov decision processes. A Gopalan, S Mannor, Proceedings of the 28th Conference on Learning Theory (COLT). the 28th Conference on Learning Theory (COLT)A. Gopalan and S. Mannor. Thompson sampling for learning parameterized markov decision processes. In Proceedings of the 28th Conference on Learn- ing Theory (COLT), pages 861-898, 2015.</p>
<p>Thompson sampling for complex online problems. A Gopalan, S Mannor, Y Mansour, Proceedings of the 31st International Conference on Machine Learning. the 31st International Conference on Machine LearningA. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex online problems. In Proceedings of the 31st International Conference on Machine Learning, pages 100-108, 2014.</p>
<p>Web-scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft's Bing search engine. T Graepel, J Q Candela, T Borchert, R Herbrich, Proceedings of the 27th International Conference on Machine Learning. the 27th International Conference on Machine LearningT. Graepel, J.Q. Candela, T. Borchert, and R. Herbrich. Web-scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft's Bing search engine. In Proceedings of the 27th International Conference on Machine Learning, pages 13-20, 2010.</p>
<p>Adaptive control of nonlinear stochastic systems by particle filtering. A Greenfield, A Brockwell, International Conference on Control and Automation. A. Greenfield and A. Brockwell. Adaptive control of nonlinear stochastic systems by particle filtering. In International Conference on Control and Automation, 2003.</p>
<p>Efficient Bayes-adaptive reinforcement learning using sample-based search. A Guez, D Silver, P Dayan, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsA. Guez, D. Silver, and P. Dayan. Efficient Bayes-adaptive reinforcement learning using sample-based search. In Proceedings of the Advances in Neu- ral Information Processing Systems, 2012.</p>
<p>Stochastic regret minimization via Thompson sampling. S Guha, K Munagala, Proceedings of The 27th Conference on Learning Theory. The 27th Conference on Learning TheoryS. Guha and K. Munagala. Stochastic regret minimization via Thompson sampling. In Proceedings of The 27th Conference on Learning Theory, pages 317-338, 2014.</p>
<p>Exploiting generative models in discriminative classifiers. T Jaakkola, D Haussler, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsT. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In Proceedings of the Advances in Neural Information Processing Systems, 1999.</p>
<p>Active learning in partially observable Markov decision processes. R Jaulmes, J Pineau, J Precup, European Conference on Machine Learning. R. Jaulmes, J. Pineau, and J. Precup. Active learning in partially observable Markov decision processes. In European Conference on Machine Learning, 2005.</p>
<p>Planning and acting in partially observable stochastic domains. L Kaelbling, M Littman, A Cassandra, Artificial Intelligence. 101L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101:99-134, 1998.</p>
<p>On Bayesian upper confidence bounds for bandit problems. E Kaufmann, O Cappé, A Garivier, International Conference on Artificial Intelligence and Statistics. E. Kaufmann, O. Cappé, and A. Garivier. On Bayesian upper confidence bounds for bandit problems. In International Conference on Artificial In- telligence and Statistics, pages 592-600, 2012a.</p>
<p>Thompson sampling: An asymptotically optimal finite-time analysis. E Kaufmann, N Korda, R Munos, Algorithmic Learning Theory. 7568E. Kaufmann, N. Korda, and R. Munos. Thompson sampling: An asymptoti- cally optimal finite-time analysis. In Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pages 199-213, 2012b.</p>
<p>A greedy approximation of Bayesian reinforcement learning with probably optimistic transition model. K Kawaguchi, M Araya-Lopez, Adaptive Learning Agents. K. Kawaguchi and M. Araya-Lopez. A greedy approximation of Bayesian rein- forcement learning with probably optimistic transition model. In Adaptive Learning Agents 2013 (a workshop of AAAMAS), 2013.</p>
<p>Near-optimal reinforcement learning in polynomial time. M Kearns, S Singh, International Conference on Machine Learning. M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. In International Conference on Machine Learning, pages 260-268, 1998.</p>
<p>A sparse sampling algorithm for nearoptimal planning in large Markov decision processes. M Kearns, Y Mansour, A Ng, International Joint Conference on Artificial Intelligence. M. Kearns, Y. Mansour, and A. Ng. A sparse sampling algorithm for near- optimal planning in large Markov decision processes. In International Joint Conference on Artificial Intelligence, pages 1324-1331, 1999.</p>
<p>Reinforcement learning in robotics: A survey. J Kober, D Bagnell, J Peters, International Journal of Robotics Research. J. Kober, D. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. International Journal of Robotics Research (IJRR), 2013.</p>
<p>References N. Kohl and P. Stone. Policy gradient reinforcement learning for fast quadrupedal locomotion. L Kocsis, C Szepesvari, Proceedings of the IEEE International Conference on Robotics and Automation. the IEEE International Conference on Robotics and AutomationProceedings of the European Conference on Machine Learning (ECML)L. Kocsis and C. Szepesvari. Bandit based Monte-Carlo planning. In Proceed- ings of the European Conference on Machine Learning (ECML), 2006. References N. Kohl and P. Stone. Policy gradient reinforcement learning for fast quadrupedal locomotion. In Proceedings of the IEEE International Con- ference on Robotics and Automation, pages 2619-2624, 2004.</p>
<p>Near-Bayesian exploration in polynomial time. J Kolter, A Ng, International Conference on Machine Learning. J. Kolter and A. Ng. Near-Bayesian exploration in polynomial time. In International Conference on Machine Learning, 2009.</p>
<p>Actor-Critic algorithms. V Konda, J Tsitsiklis, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsV. Konda and J. Tsitsiklis. Actor-Critic algorithms. In Proceedings of the Ad- vances in Neural Information Processing Systems, pages 1008-1014, 2000.</p>
<p>Least-squares policy iteration. M Lagoudakis, R Parr, Journal of Machine Learning Research. 4M. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Ma- chine Learning Research, 4:1107-1149, 2003.</p>
<p>Asymptotically efficient adaptive allocation rules. T Lai, H Robbins, Advances in Applied Mathematics. 61T. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4-22, 1985.</p>
<p>Bayesian multi-task reinforcement learning. A Lazaric, M Ghavamzadeh, Proceedings of the 27th International Conference on Machine Learning. the 27th International Conference on Machine LearningA. Lazaric and M. Ghavamzadeh. Bayesian multi-task reinforcement learning. In Proceedings of the 27th International Conference on Machine Learning, pages 599-606, 2010.</p>
<p>A unifying framework for computational reinforcement learning theory. L Li, RutgersPhD thesisL. Li. A unifying framework for computational reinforcement learning theory. PhD thesis, Rutgers, 2009.</p>
<p>On the prior sensitivity of Thompson sampling. C Liu, L Li, abs/1506.03378CoRRC. Liu and L. Li. On the prior sensitivity of Thompson sampling. CoRR, abs/1506.03378, 2015.</p>
<p>The sample complexity of exploration in the multi-armed bandit problem. S Mannor, J Tsitsiklis, The Journal of Machine Learning Research. 5S. Mannor and J. Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. The Journal of Machine Learning Research, 5:623-648, 2004.</p>
<p>The cross entropy method for fast policy search. S Mannor, R Rubinstein, Y Gat, International Conference on Machine Learning. S. Mannor, R. Rubinstein, and Y. Gat. The cross entropy method for fast policy search. In International Conference on Machine Learning, 2003.</p>
<p>Bias and variance approximation in value function estimates. S Mannor, D Simester, P Sun, J N Tsitsiklis, Management Science. 532S. Mannor, D. Simester, P. Sun, and J.N. Tsitsiklis. Bias and variance approx- imation in value function estimates. Management Science, 53(2):308-322, 2007.</p>
<p>Simulated-Based Methods for Markov Decision Processes. P Marbach, Massachusetts Institute of TechnologyPhD thesisP. Marbach. Simulated-Based Methods for Markov Decision Processes. PhD thesis, Massachusetts Institute of Technology, 1998.</p>
<p>Some PAC-Bayesian theorems. D Mcallester, Machine Learning. 37D. McAllester. Some PAC-Bayesian theorems. Machine Learning, 37, 1999.</p>
<p>Transfer in variablereward hierarchical reinforcement learning. N Mehta, S Natarajan, P Tadepalli, A Fern, Machine Learning. 733N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern. Transfer in variable- reward hierarchical reinforcement learning. Machine Learning, 73(3):289- 312, 2008.</p>
<p>Bayesian nonparametric inverse reinforcement learning. B Michini, J How, Proceedings of the European Conference on Machine Learning. the European Conference on Machine LearningB. Michini and J. How. Bayesian nonparametric inverse reinforcement learn- ing. In Proceedings of the European Conference on Machine Learning, 2012a.</p>
<p>Improving the efficiency of Bayesian inverse reinforcement learning. B Michini, J How, IEEE International Conference on Robotics and Automation. B. Michini and J. How. Improving the efficiency of Bayesian inverse rein- forcement learning. In IEEE International Conference on Robotics and Automation, pages 3651-3656, 2012b.</p>
<p>Prioritized sweeping: Reinforcement learning with less data and less real time. A Moore, C Atkeson, Machine Learning. 13A. Moore and C. Atkeson. Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning, 13:103-130, 1993.</p>
<p>Apprenticeship learning using inverse reinforcement learning and gradient methods. A Neu, C Szepesvári, Proceedings of the Conference on Uncertainty in Artificial Intelligence. the Conference on Uncertainty in Artificial IntelligenceA. Neu and C. Szepesvári. Apprenticeship learning using inverse reinforce- ment learning and gradient methods. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2007.</p>
<p>Algorithms for inverse reinforcement learning. A Ng, S Russell, Proceedings of the 17th International Conference on Machine Learning. the 17th International Conference on Machine LearningA. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Pro- ceedings of the 17th International Conference on Machine Learning, pages 663-670, 2000.</p>
<p>Autonomous helicopter flight via reinforcement learning. A Ng, H Kim, M Jordan, S Sastry, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsMIT PressA. Ng, H. Kim, M. Jordan, and S. Sastry. Autonomous helicopter flight via re- inforcement learning. In Proceedings of the Advances in Neural Information Processing Systems. MIT Press, 2004.</p>
<p>Robust control of markov decision processes with uncertain transition matrices. A Nilim, L El Ghaoui, Operations Research. 535A. Nilim and L. El Ghaoui. Robust control of markov decision processes with uncertain transition matrices. Operations Research, 53(5):780-798, 2005.</p>
<p>Computing a classic index for finite-horizon bandits. J Niño-Mora, INFORMS Journal on Computing. 232J. Niño-Mora. Computing a classic index for finite-horizon bandits. INFORMS Journal on Computing, 23(2):254-267, 2011.</p>
<p>Bayes-Hermite quadrature. A O&apos;hagan, Journal of Statistical Planning and Inference. 29A. O'Hagan. Bayes-Hermite quadrature. Journal of Statistical Planning and Inference, 29:245-260, 1991.</p>
<p>More) efficient reinforcement learning via posterior sampling. I Osband, D Russo, B Van Roy, Proceedings of the Advances in Neural Information Processing Systems (NIPS). the Advances in Neural Information Processing Systems (NIPS)I. Osband, D. Russo, and B. Van Roy. (More) efficient reinforcement learning via posterior sampling. In Proceedings of the Advances in Neural Informa- tion Processing Systems (NIPS), 2013.</p>
<p>An online POMDP algorithm for complex multiagent environments. S Paquet, L Tobin, B Chaib-Draa, International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS). S. Paquet, L. Tobin, and B. Chaib-draa. An online POMDP algorithm for complex multiagent environments. In International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS), pages 970-977, 2005.</p>
<p>Natural actor-critic. J Peters, S Schaal, Neurocomputing. 717-9J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180- 1190, 2008.</p>
<p>Point-based value iteration: an anytime algorithm for POMDPs. J Pineau, G Gordon, S Thrun, International Joint Conference on Artificial Intelligence. J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: an anytime algorithm for POMDPs. In International Joint Conference on Artificial Intelligence, pages 1025-1032, 2003.</p>
<p>Bayesian Reinforcement Learning for POMDP-based Dialogue Systems. S Png, McGill UniversityMaster's thesisS Png. Bayesian Reinforcement Learning for POMDP-based Dialogue Sys- tems. Master's thesis, McGill University, 2011.</p>
<p>Bayesian reinforcement learning for POMDP-based dialogue systems. S Png, J Pineau, ICASSP. S. Png and J. Pineau. Bayesian reinforcement learning for POMDP-based dialogue systems. In ICASSP, 2011.</p>
<p>Calcul des Probabilités. H Poincaré, Georges Carré. H. Poincaré. Calcul des Probabilités. Georges Carré, Paris, 1896. References</p>
<p>Point-based value iteration for continuous POMDPs. J Porta, N Vlassis, M Spaan, P Poupart, Journal of Machine Learning Research. 7J. Porta, N. Vlassis, M. Spaan, and P. Poupart. Point-based value iteration for continuous POMDPs. Journal of Machine Learning Research, 7, 2006.</p>
<p>An analytic solution to discrete Bayesian reinforcement learning. P Poupart, N Vlassis, J Hoey, K Regan, International Conference on Machine learning. P. Poupart, N. Vlassis, J. Hoey, and K. Regan. An analytic solution to discrete Bayesian reinforcement learning. In International Conference on Machine learning, pages 697-704, 2006.</p>
<p>Approximate Dynamic Programming: Solving the curses of dimensionality. W B Powell, John Wiley &amp; Sons2nd EditionW. B. Powell. Approximate Dynamic Programming: Solving the curses of dimensionality (2nd Edition). John Wiley &amp; Sons, 2011.</p>
<p>Markov Decision Processes. M Puterman, Wiley InterscienceM. Puterman. Markov Decision Processes. Wiley Interscience, 1994.</p>
<p>Optimistic planning for belief-augmented Markov Decision Processes. R , Munos R Fonteneau, L Busoniu, IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL). R. Munos R. Fonteneau, L. Busoniu. Optimistic planning for belief-augmented Markov Decision Processes. In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2013.</p>
<p>Bayesian inverse reinforcement learning. D Ramachandran, E Amir, Proceedings of the 20th International Joint Conference on Artificial Intelligence. the 20th International Joint Conference on Artificial IntelligenceD. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20th International Joint Conference on Artificial Intel- ligence, pages 2586-2591, 2007.</p>
<p>Bayesian Monte Carlo. C Rasmussen, Z Ghahramani, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsMIT PressC. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo. In Proceedings of the Advances in Neural Information Processing Systems, pages 489-496. MIT Press, 2003.</p>
<p>Gaussian processes in reinforcement learning. C Rasmussen, M Kuss, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsMIT PressC. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning. In Proceedings of the Advances in Neural Information Processing Systems. MIT Press, 2004.</p>
<p>Gaussian Processes for Machine Learning. C Rasmussen, C Williams, MIT PressC. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>Maximum margin planning. N Ratliff, A Bagnell, M Zinkevich, Proceedings of the 23rd International Conference on Machine Learning. the 23rd International Conference on Machine LearningN. Ratliff, A. Bagnell, and M. Zinkevich. Maximum margin planning. In Proceedings of the 23rd International Conference on Machine Learning, 2006.</p>
<p>Bayesian adaptive control of time varying systems. R Ravikanth, S Meyn, L Brown, IEEE Conference on Decision and Control. R. Ravikanth, S. Meyn, and L. Brown. Bayesian adaptive control of time varying systems. In IEEE Conference on Decision and Control, 1992.</p>
<p>Online kernel selection for Bayesian reinforcement learning. J Reisinger, P Stone, R Miikkulainen, Proceedings of the 25th International Conference on Machine Learning. the 25th International Conference on Machine LearningJ. Reisinger, P. Stone, and R. Miikkulainen. Online kernel selection for Bayesian reinforcement learning. In Proceedings of the 25th International Conference on Machine Learning, pages 816-823, 2008.</p>
<p>Model-based Bayesian reinforcement learning in large structured domains. S Ross, J Pineau, Proceedings of the Conference on Uncertainty in Artificial Intelligence. the Conference on Uncertainty in Artificial IntelligenceS. Ross and J. Pineau. Model-based Bayesian reinforcement learning in large structured domains. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2008.</p>
<p>Bayes-adaptive POMDPs. S Ross, B Chaib-Draa, J Pineau, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems20S. Ross, B. Chaib-draa, and J. Pineau. Bayes-adaptive POMDPs. In Proceed- ings of the Advances in Neural Information Processing Systems, volume 20, pages 1225-1232, 2008a.</p>
<p>Bayesian reinforcement learning in continuous POMDPs with application to robot navigation. S Ross, B Chaib-Draa, J Pineau, IEEE International Conference on Robotics and Automation. S. Ross, B. Chaib-draa, and J. Pineau. Bayesian reinforcement learning in continuous POMDPs with application to robot navigation. In IEEE Inter- national Conference on Robotics and Automation, 2008b.</p>
<p>. S Ross, J Pineau, S Paquet, B Chaib-Draa, Online POMDPs. Journal of Artificial Intelligence Research (JAIR). 32S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa. Online POMDPs. Journal of Artificial Intelligence Research (JAIR), 32:663-704, 2008c.</p>
<p>A Bayesian approach for learning and planning in partially observable Markov decision processes. S Ross, J Pineau, B Chaib-Draa, P Kreitmann, Journal of Machine Learning Research. 12S. Ross, J. Pineau, B. Chaib-draa, and P. Kreitmann. A Bayesian approach for learning and planning in partially observable Markov decision processes. Journal of Machine Learning Research, 12, 2011.</p>
<p>On-line Q-learning using connectionist systems. G Rummery, M Niranjan, CUED/F-INFENG/TR 166Engineering Department, Cambridge UniversityTechnical ReportG. Rummery and M. Niranjan. On-line Q-learning using connectionist sys- tems. Technical Report CUED/F-INFENG/TR 166, Engineering Depart- ment, Cambridge University, 1994.</p>
<p>Linearly parameterized bandits. P Rusmevichientong, J N Tsitsiklis, Mathematics of Operations Research. 352P. Rusmevichientong and J. N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395-411, 2010.</p>
<p>Optimal adaptive control of uncertain stochastic discrete linear systems. I Rusnak, IEEE International Conference on Systems, Man and Cybernetics. I. Rusnak. Optimal adaptive control of uncertain stochastic discrete linear systems. In IEEE International Conference on Systems, Man and Cyber- netics, 1995.</p>
<p>Learning agents for uncertain environments (extended abstract). S Russell, Proceedings of the 11th Annual Conference on Computational Learning Theory. the 11th Annual Conference on Computational Learning TheoryS. Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 101-103, 1998.</p>
<p>S Russell, P Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall2nd EditionS. Russell and P. Norvig. Artificial Intelligence: A Modern Approach (2nd Edition). Prentice Hall, 2002.</p>
<p>An information-theoretic analysis of Thompson sampling. D Russo, B Van Roy, abs/1403.5341CoRRD. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. CoRR, abs/1403.5341, 2014a.</p>
<p>Learning to optimize via posterior sampling. D Russo, B Van Roy, Mathematics of Operations Research. 394D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221-1243, 2014b.</p>
<p>L Scharf, Statistical Signal Processing. Addison-WesleyL. Scharf. Statistical Signal Processing. Addison-Wesley, 1991.</p>
<p>Learning with Kernels. B Schölkopf, A Smola, MIT PressB. Schölkopf and A. Smola. Learning with Kernels. MIT Press, 2002.</p>
<p>A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry. S Scott, 26S. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochas- tic Models in Business and Industry, 26(6):639-658, 2010.</p>
<p>PAC-Bayesian analysis of contextual bandits. Y Seldin, P Auer, F Laviolette, J Shawe-Taylor, R Ortner, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsY. Seldin, P. Auer, F. Laviolette, J. Shawe-Taylor, and R. Ortner. PAC- Bayesian analysis of contextual bandits. In Proceedings of the Advances in Neural Information Processing Systems, 2011a.</p>
<p>PAC-Bayesian analysis of the exploration-exploitation trade-off. Y Seldin, N Cesa-Bianchi, F Laviolette, P Auer, J Shawe-Taylor, J Peters, ICML Workshop on online trading of exploration and exploitation, 2011b. References J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University PressY. Seldin, N. Cesa-Bianchi, F. Laviolette, P. Auer, J. Shawe-Taylor, and J. Pe- ters. PAC-Bayesian analysis of the exploration-exploitation trade-off. In ICML Workshop on online trading of exploration and exploitation, 2011b. References J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.</p>
<p>The optimal control of partially observable Markov processes over a finite horizon. R Smallwood, E Sondik, Operations Research. 215R. Smallwood and E. Sondik. The optimal control of partially observable Markov processes over a finite horizon. Operations Research, 21(5):1071- 1088, Sep/Oct 1973.</p>
<p>Variance-based rewards for approximate Bayesian reinforcement learning. V Sorg, S Sing, R Lewis, Proceedings of the Conference on Uncertainty in Artificial Intelligence. the Conference on Uncertainty in Artificial IntelligenceV. Sorg, S. Sing, and R. Lewis. Variance-based rewards for approximate Bayesian reinforcement learning. In Proceedings of the Conference on Un- certainty in Artificial Intelligence, 2010.</p>
<p>Perseus: randomized point-based value iteration for POMDPs. M Spaan, N Vlassis, Journal of Artificial Intelligence Research (JAIR). 24M. Spaan and N. Vlassis. Perseus: randomized point-based value iteration for POMDPs. Journal of Artificial Intelligence Research (JAIR), 24:195-220, 2005.</p>
<p>A theoretical analysis of model-based interval estimation. A Strehl, M Littman, International Conference on Machine learning. A. Strehl and M. Littman. A theoretical analysis of model-based interval estimation. In International Conference on Machine learning, pages 856- 863, 2005.</p>
<p>An analysis of model-based interval estimation for Markov decision processes. A Strehl, M Littman, Journal of Computer and System Sciences. 74A. Strehl and M. Littman. An analysis of model-based interval estimation for Markov decision processes. Journal of Computer and System Sciences, 74: 1209-1331, 2008.</p>
<p>A Bayesian framework for reinforcement learning. M Strens, International Conference on Machine Learning. M. Strens. A Bayesian framework for reinforcement learning. In International Conference on Machine Learning, 2000.</p>
<p>Temporal credit assignment in reinforcement learning. R Sutton, University of Massachusetts AmherstPhD thesisR. Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, University of Massachusetts Amherst, 1984.</p>
<p>Learning to predict by the methods of temporal differences. Machine Learning. R Sutton, 3R. Sutton. Learning to predict by the methods of temporal differences. Ma- chine Learning, 3:9-44, 1988.</p>
<p>DYNA, an integrated architecture for learning, planning, and reacting. R Sutton, SIGART Bulletin. 2R. Sutton. DYNA, an integrated architecture for learning, planning, and reacting. SIGART Bulletin, 2:160-163, 1991.</p>
<p>An Introduction to Reinforcement Learning. R Sutton, A Barto, MIT PressR. Sutton and A. Barto. An Introduction to Reinforcement Learning. MIT Press, 1998.</p>
<p>Policy gradient methods for reinforcement learning with function approximation. R Sutton, D Mcallester, S Singh, Y Mansour, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsR. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the Advances in Neural Information Processing Systems, pages 1057-1063, 2000.</p>
<p>A game-theoretic approach to apprenticeship learning. U Syed, R Schapire, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing SystemsU. Syed and R. Schapire. A game-theoretic approach to apprenticeship learn- ing. In Proceedings of the Advances in Neural Information Processing Sys- tems, pages 1449-1456, 2008.</p>
<p>Automatic ad format selection via contextual bandits. L Tang, R Rosales, A Singh, D Agarwal, Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management. the 22nd ACM international conference on Conference on information &amp; knowledge managementACML. Tang, R. Rosales, A. Singh, and D. Agarwal. Automatic ad format selec- tion via contextual bandits. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 1587-1594. ACM, 2013.</p>
<p>TD-Gammon, a self-teaching backgammon program, achieves master-level play. G Tesauro, Neural Computation. 6G. Tesauro. TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6:215-219, 1994.</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. W Thompson, Biometrika. W. Thompson. On the likelihood that one unknown probability exceeds an- other in view of the evidence of two samples. Biometrika, pages 285-294, 1933.</p>
<p>A short proof of the gittins index theorem. J N Tsitsiklis, The Annals of Applied Probability. J. N. Tsitsiklis. A short proof of the gittins index theorem. The Annals of Applied Probability, pages 194-199, 1994.</p>
<p>Hessian matrix distribution for Bayesian policy gradient reinforcement learning. N Vien, H Yu, T Chung, Information Sciences. 1819N. Vien, H. Yu, and T. Chung. Hessian matrix distribution for Bayesian policy gradient reinforcement learning. Information Sciences, 181(9):1671-1685, 2011.</p>
<p>Integrating sample-based planning and model-based reinforcement learning. T Walsh, S Goschin, M Littman, Association for the Advancement of Artificial Intelligence. T. Walsh, S. Goschin, and M. Littman. Integrating sample-based planning and model-based reinforcement learning. In Association for the Advancement of Artificial Intelligence, 2010.</p>
<p>Bayesian sparse sampling for on-line reward optimization. T Wang, D Lizotte, M Bowling, D Schuurmans, International Conference on Machine learning. T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans. Bayesian sparse sam- pling for on-line reward optimization. In International Conference on Ma- chine learning, pages 956-963, 2005.</p>
<p>Learning from Delayed Rewards. C Watkins, Kings College, Cambridge, EnglandPhD thesisC. Watkins. Learning from Delayed Rewards. PhD thesis, Kings College, Cambridge, England, 1989.</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. R Williams, Machine Learning. 8R. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256, 1992.</p>
<p>Multi-task reinforcement learning: A hierarchical Bayesian approach. A Wilson, A Fern, S Ray, P Tadepalli, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningA. Wilson, A. Fern, S. Ray, and P. Tadepalli. Multi-task reinforcement learn- ing: A hierarchical Bayesian approach. In Proceedings of the International Conference on Machine Learning, pages 1015-1022, 2007.</p>
<p>Adaptive dual control methods: An overview. B Wittenmark, 5th IFAC symposium on Adaptive Systems in Control and Signal Processing. B. Wittenmark. Adaptive dual control methods: An overview. In 5th IFAC symposium on Adaptive Systems in Control and Signal Processing, 1995.</p>
<p>Discrete-time Bayesian adaptive control problems with complete information. O Zane, IEEE Conference on Decision and Control. O. Zane. Discrete-time Bayesian adaptive control problems with complete information. In IEEE Conference on Decision and Control, 1992.</p>
<p>Maximum entropy inverse reinforcement learning. B Ziebart, A Maas, A Bagnell, A Dey, Proceedings of the 23rd National Conference on Artificial Intelligence. the 23rd National Conference on Artificial Intelligence3B. Ziebart, A. Maas, A. Bagnell, and A. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd National Conference on Artificial Intelligence -Volume 3, pages 1433-1438, 2008.</p>            </div>
        </div>

    </div>
</body>
</html>